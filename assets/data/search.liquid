[
  
  {
    "title"    : "Bedrock Ã  l&#39;AWS Summit Paris 2023",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, aws, summit, paris, 2023",
    "url"      : "/2023/04/20/aws-summit-paris-2023.html",
    "date"     : "April 20, 2023",
    "excerpt"  : "Lâ€™AWS Summit Paris 2023 sâ€™est dÃ©roulÃ© le 4 avril. Câ€™Ã©tait pour nous lâ€™occasion de dÃ©couvrir les derniÃ¨res innovations au cÅ“ur des services AWS, comme la solution dâ€™IA dâ€™aide au dÃ©veloppement nommÃ©e CodeWhisperer. De plus, Pascal Martin, Principal ...",
  "content"  : "Lâ€™AWS Summit Paris 2023 sâ€™est dÃ©roulÃ© le 4 avril. Câ€™Ã©tait pour nous lâ€™occasion de dÃ©couvrir les derniÃ¨res innovations au cÅ“ur des services AWS, comme la solution dâ€™IA dâ€™aide au dÃ©veloppement nommÃ©e CodeWhisperer. De plus, Pascal Martin, Principal Engineer, y assistait aussi en tant que speaker pour partager notre expÃ©rience en conception et maintenance de SystÃ¨mes DistribuÃ©s.\nEn plus des deux points prÃ©cÃ©demment citÃ©s, nous verrons aussi comment eTF1 sâ€™est prÃ©parÃ© pour la Coupe du Monde de la FIFA 2022, ou de souverainetÃ© et de son application chez AWS.\n\nÃ€ vos cÃ´tÃ©s pour les grands moments : AWS, TF1 et la Coupe du Monde de la FIFA 2022\n\nConfÃ©rence prÃ©sentÃ©e par :\n\n  Imane Zeroual - Senior Technical Account Manager, AWS\n  Djamel Arichi, Head of Managed Services and Support, eTF1\n  Ali Oubabiz, Head of Digital Infrastructure, eTF1\n  Remy Pinsonneau, Architecte, eTF1\n\n\n\n\neTF1 partage son retour dâ€™expÃ©rience sur la Coupe du Monde de foot 2022 et les dÃ©fis surmontÃ©s pour que leur plateforme de replay myTF1 propose une parfaite expÃ©rience utilisateur tout au long de lâ€™Ã©vÃ©nement.\n\nLa prÃ©sentation, coanimÃ©e par Imane, Senior Technical Manager de chez AWS, permet aussi dâ€™en apprendre un peu plus sur le programme IEM dâ€™accompagnement de clients AWS lors dâ€™Ã©vÃ©nements critiques. Nous avons dâ€™ailleurs dÃ©jÃ  exploitÃ© ce programme chez Bedrock Streaming.\nChallenge technique pour les Ã©quipes eTF1, la Coupe du Monde de football 2022 a battu plusieurs records de la plateforme, dont des pics Ã  plus de 2,4 Millions dâ€™utilisateurs simultanÃ©s. Lâ€™Ã©vÃ©nement a Ã©tÃ© prÃ©parÃ© en collaboration avec les Ã©quipes dâ€™AWS pour adapter les infrastructures Ã  recevoir de fortes charges.\n\nTrois points critiques identifiÃ©s :\n\n  Authent/backend, les millions dâ€™utilisateurs vont sâ€™authentifier dans une fenÃªtre de 15 minutes.\n  Delivery vidÃ©o, tout au long de lâ€™Ã©vÃ©nement une forte charge, constante, est attendue.\n  PublicitÃ©, pic de charge trÃ¨s important mais ponctuel.\n\n\nDes scÃ©narios de tests de performances ont Ã©tÃ© effectuÃ©s Ã  lâ€™aide de K6, pour chacun des points. La prÃ©production a servi dâ€™environnement de test, avant dâ€™effectuer une validation finale sur la production. Du travail a Ã©tÃ© Ã©galement rÃ©alisÃ© sur les services AWS : par exemple, les tables DynamoDB ont Ã©tÃ© basculÃ©es en OnDemand afin de profiter de lâ€™Ã©lasticitÃ© plus rapide du service, malgrÃ© les coÃ»ts supplÃ©mentaires, comparÃ© au mode provisionnÃ©.\nAu niveau des clusters Kubernetes, les applications ont Ã©tÃ© redimensionnÃ©es Ã  la hausse (mÃ©moire, cpu, HPA) pour anticiper les pics de charge et ne pas seulement se reposer sur du scaling rÃ©actif.\n\nLors de la compÃ©tition, une War Room Ã©tait ouverte suivant lâ€™importance des matchs. Elle Ã©tait composÃ©e dâ€™intervenants AWS grÃ¢ce au programme IEM, de personnels techniques eTF1 et de membres du service management pour pouvoir rÃ©agir en cas dâ€™imprÃ©vus. \nLa War Room a dâ€™ailleurs Ã©tÃ© mise Ã  contribution puisque la plateforme Ã  subi des attaques DDOS pendant certains matchs. Le CDN Cloudfront et WAF ont permis de les contenir.\n\nChez Bedrock Streaming, nous Ã©tions curieux de ce retour dâ€™expÃ©rience : nous avons prÃ©parÃ© ce mÃªme type dâ€™Ã©vÃ©nement lors de lâ€™Euro de football 2020. Les dÃ©fis Ã  surmonter sont les mÃªmes que ceux que nous avions rencontrÃ©s et nous sommes arrivÃ©s Ã  des conclusions similaires dans nos choix techniques. Nous avions dâ€™ailleurs dÃ©veloppÃ© un outil pour rÃ©pondre au problÃ¨me de scalabilitÃ© dans kubernetes durant lâ€™Euro 2020 et que nous utilisons toujours aujourdâ€™hui, un article de blog Ã  ce sujet est disponible ici.\n\nComment bien dÃ©buter avec Amazon CodeWhisperer\n\nConfÃ©rence prÃ©sentÃ©e par :\n\n  SÃ©bastien Butreau, Senior Partner Solutions Architect, AWS\n  SÃ©bastien Grazzini, Principal Solutions Architect, AWS\n\n\n\n\nAmazon annonce une sortie grand public, prochaine, de son assistant de dÃ©veloppement par IA CodeWhisperer (update: depuis, CodeWhisperer est passÃ© GA).\nNous avons eu droit Ã  une dÃ©monstration de lâ€™outil. En quelques minutes et seulement Ã  lâ€™aide de quelques commentaires, les deux prÃ©sentateurs ont produit un script python capable de prendre en entrÃ©e un rÃ©pertoire de photos et donner en sortie un JSON qui, pour chaque photo, donnait le nom de la cÃ©lÃ©britÃ© prÃ©sente dessus.\n\nChez Bedrock Streaming, nous pensons quâ€™il est trÃ¨s important de suivre ce nouveau tournant que prend lâ€™aide au dÃ©veloppement via lâ€™IA depuis quelques mois. Nous prÃ©voyons de tester lors de nos journÃ©es R&amp;amp;D Github Copilot et Amazon CodeWhisperer.\n\nLâ€™outil dâ€™Amazon a quelques atouts, notamment la fonctionnalitÃ© de suivi des rÃ©fÃ©rences qui permet de savoir si du code proposÃ© est similaire Ã  du code utilisÃ© pour lâ€™apprentissage et peut-Ãªtre protÃ©gÃ© par une licence incompatible avec notre usage. De plus, lâ€™intÃ©gration du SDK Amazon est assez poussÃ©e et cela prend tout son sens, notamment lors du dÃ©veloppement pour des lambdas AWS oÃ¹ lâ€™outil semble trÃ¨s performant.\n\nBienvenue dans le Monde Merveilleux des SystÃ¨mes DistribuÃ©s\n\nCette annÃ©e encore, nous avons eu la chance de pouvoir partager notre expÃ©rience, lors dâ€™une confÃ©rence donnÃ©e par Pascal, Principal Engineer et AWS Hero : Â« Bienvenue dans le Monde Merveilleux des SystÃ¨mes DistribuÃ©s Â»\n\n\n\nPourquoi sâ€™embÃªter avec des SystÃ¨mes distribuÃ©s ? Comment en tirer profit ? Quels dangers ? ScalabilitÃ©, coordination et rÃ©silience : trois grands axes pour ce talk, basÃ© sur lâ€™expÃ©rience acquise par les Ã©quipes Bedrock, tant infra que devs, depuis plusieurs annÃ©es.\n\nEn tant que speaker, pouvoir partager avec notre communautÃ© est toujours aussi agrÃ©able ! Et, dans le public, il Ã©tait assez intÃ©ressant dâ€™entendre les rÃ©actions de nos voisins lorsque Pascal racontait certaines anecdotes ou prÃ©sentait certains concepts. Les problÃ©matiques que nous rencontrons dans nos mÃ©tiers, nous sommes nombreux Ã  les rencontrer, et câ€™est tout lâ€™intÃ©rÃªt des Ã©vÃ©nements comme AWS Summit : apprendre les uns des autres !\n\nCette prÃ©sentation nâ€™a malheureusement pas Ã©tÃ© enregistrÃ©e lors du Summit, mais Pascal lâ€™a redonnÃ©e depuis Ã  MixIT, oÃ¹ elle a Ã©tÃ© enregistrÃ©e â€“ et les vidÃ©os devraient Ãªtre bientÃ´t publiÃ©es ;-)\n\nLa souverainetÃ© des donnÃ©es chez AWS\n\nUne des confÃ©rences portait sur les thÃ¨mes de la SouverainetÃ© dans le Cloud AWS et du RÃ¨glement europÃ©en GÃ©nÃ©ral sur la Protection des DonnÃ©es (RGPD). Lors de cette prÃ©sentation, Stephan Hadinger (Directeur de la Technologie chez AWS) a exposÃ© le cadre de ce rÃ¨glement et sa mise en application au sein de lâ€™infrastructure AWS. Câ€™est cette partie qui Ã©tait, dâ€™aprÃ¨s nous, la plus intÃ©ressante, Ã©tant donnÃ©e sa dimension technique.\n\nRGPD est un regroupement de rÃ¨gles qui rÃ©gissent et protÃ¨gent les droits des rÃ©sidents dâ€™Union EuropÃ©enne. Il porte sur le respect de la confidentialitÃ© et la protection des donnÃ©es personnelles. Toute entreprise exerÃ§ant dans lâ€™UE y est soumise. Dans le cas prÃ©sent, la RGPD couvre Ã  la fois les clients AWS (comme Bedrock) et les utilisateurs finaux (comme les utilisateurs des services Bedrock).\n\nChez AWS, la SouverainetÃ© est synonyme dâ€™autonomie stratÃ©gique et sâ€™exprime de la faÃ§on suivante :\n\n  la possession des donnÃ©es clients : tous les clients AWS ont le contrÃ´le de leurs donnÃ©es et applications, et nous verrons comment ;\n  le choix de la localisation des donnÃ©es, via la possibilitÃ© dâ€™hÃ©berger lâ€™intÃ©gralitÃ© des donnÃ©es sur le territoire de son choix, en France par exemple ;\n  lâ€™accÃ¨s au meilleur de la technologie, qui favorise lâ€™innovation ;\n  et la possibilitÃ© de changer de solution (pas de lock-in).\n\n\nLes clients sont les seuls possesseurs de leurs donnÃ©es, ils en ont le contrÃ´le total : AWS nâ€™a aucun droit dâ€™usage des donnÃ©es de leurs clients. De plus, AWS nâ€™a pas accÃ¨s aux donnÃ©es et ne dÃ©place pas (gÃ©ographiquement) les donnÃ©es de ses clients.\n\nLâ€™implÃ©mentation technique de ces concepts repose, entre autres, sur le chiffrement systÃ©matique des donnÃ©es. AWS Nitro est une des briques dâ€™architecture qui en est responsable pour les EC2 (depuis 2013 pour la partie rÃ©seau). Nitro permet le chiffrement de toute la chaÃ®ne de donnÃ©es (rÃ©seau, volumes de stockage) et comprend plusieurs composants :\n\n  Carte Nitro dÃ©diÃ©e au Ã©changes externes (rÃ©seau + accÃ¨s aux EBS, stockage persistant)\n  Carte Nitro pour le stockage local (stockage temporaire attachÃ© Ã  lâ€™hÃ´te)\n  Hyperviseur Nitro (il sâ€™agit dâ€™un hyperviseur basÃ© sur linux KVM, mais grandement modifiÃ© pour les besoins, pas de sshd, pas de systemd, pas de couche rÃ©seau)\n  Puce de sÃ©curitÃ© Nitro (qui empÃªche le client dâ€™avoir accÃ¨s aux composants de lâ€™hÃ´te, procÃ¨de Ã  la mise Ã  jour des firmwares des composants du serveur et gÃ¨re le sÃ©cure boot afin de contrÃ´ler lâ€™Ã©tat des firmwares des composants avant de dÃ©marrer lâ€™hÃ´te).\n\n\n\n\nAu delÃ  du chiffrement dont il est principalement question ici, Nitro permet aussi de grandement augmenter les performances des EC2 en limitant lâ€™impact de lâ€™hyperviseur sur le CPU utilisÃ© par les clients. Dans le cas dâ€™une virtualisation classique, toutes les tÃ¢ches listÃ©es ci-dessus sont effectuÃ©es par le processeur lui-mÃªme, grignotant ainsi de la puissance des machines. Ici, Nitro permet de dÃ©charger le CPU de ces tÃ¢ches en le rendant ainsi dÃ©diÃ© aux EC2.\n\nAWS utilise aussi des solutions telles que Key Management Service (KMS) pour chiffrer les donnÃ©es de plus dâ€™une centaine de ses services. Il sâ€™agit lÃ  aussi dâ€™un systÃ¨me de protection des donnÃ©es des utilisateurs : seul lâ€™opÃ©rateur possÃ©dant la clÃ© de chiffrement est capable de lire les donnÃ©es de ces services. \nUne version Ã©tendue de KMS est mÃªme disponible pour les clients les plus soucieux de la protection de leurs donnÃ©es : External Key Stores. XKS est un dispositif physique pouvant Ãªtre hÃ©bergÃ© en dehors des locaux dâ€™AWS. Il est mÃªme capable de se â€œdÃ©fendreâ€ contre les attaques physiques en procÃ©dant Ã  lâ€™effacement des clÃ©s lors dâ€™une tentative dâ€™intrusion physique. Il sâ€™agit probablement de lâ€™ultime implÃ©mentation de sÃ©curitÃ© et de souverainetÃ© chez AWS.\n\n\n\nTout au long de cette confÃ©rence, on a bien senti que le but dâ€™AWS, afin de respecter les donnÃ©es de ses usagers, Ã©tait de faire en sorte de ne pas pouvoir accÃ©der aux donnÃ©es de ses clients.\n\nLe mot de la fin\n\nLâ€™AWS Summit comportait plus dâ€™une centaine de sessions et nous avons juste eu lâ€™occasion dâ€™effleurer le contenu proposÃ© lors de cette journÃ©e.\n\nNous avons commencÃ© Ã  migrer vers Le Cloud en 2018 et notre premier AWS Summit Paris Ã©tait en 2019 â€“ nous y avions dâ€™ailleurs dÃ©jÃ  parlÃ© de cette migration au cours dâ€™un autre Ã©vÃ©nement.\n\nDepuis, que de chemin parcouru ! Cette annÃ©e, nous pensions moins Ã  Kubernetes, Ã  DynamoDB ou aux optimisations de coÃ»ts, sur lesquels nous avons bien bossÃ© depuis 2019. Notre attention Ã©tait plus attirÃ©e vers des sujets que nous avons commencÃ© Ã  travailler plus rÃ©cemment et oÃ¹ nous avons encore des challenges majeurs, comme les approches pleinement serverless ;-)\n"
} ,
  
  {
    "title"    : "Bedrock au Kubernetes Community Days France 2023",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, rex",
    "url"      : "/2023/04/03/kubernetes-community-days.html",
    "date"     : "April 3, 2023",
    "excerpt"  : "La premiÃ¨re Ã©dition de KCD (Kubernetes Community Days) en France sâ€™est dÃ©roulÃ©e le 7 mars au Centre Pompidou et rassemblant prÃ¨s de 1000 participants pour une belle journÃ©e de confÃ©rences.\n\n\n\nKCD a rassemblÃ© les communautÃ©s tech franÃ§aises pour ce...",
  "content"  : "La premiÃ¨re Ã©dition de KCD (Kubernetes Community Days) en France sâ€™est dÃ©roulÃ©e le 7 mars au Centre Pompidou et rassemblant prÃ¨s de 1000 participants pour une belle journÃ©e de confÃ©rences.\n\n\n\nKCD a rassemblÃ© les communautÃ©s tech franÃ§aises pour cette journÃ©e de partage dâ€™expertise et de retours dâ€™expÃ©rience autour de Kubernetes et des technologies Cloud Native et DevOps.\n\nSolomon Hykes, son acolyte JÃ©rome Petazzoni et lâ€™Ã‰ducation Nationale ont prÃ©sentÃ© la keynote dâ€™ouverture.\nCette premiÃ¨re keynote a permis dâ€™introduire le projet Santorin du MinistÃ¨re de lâ€™Ã‰ducation. Câ€™est un systÃ¨me dâ€™aide Ã  la correction et Ã  la notation pour lequel ils utilisent 3 clusters afin dâ€™analyser 5 millions de copies.\n\n\n  \n\nSolomon Hykes &amp;amp; JÃ©rÃ´me Petazzoni\n\n\nDes grands acteurs de la tech en France tels que Scaleway, OVHCloud, Shadow, eTF1, Back Market, vpTech, Doctolib, Deezer, Carrefour et lâ€™Ã‰ducation Nationale Ã©taient prÃ©sents pour rapporter leurs expÃ©riences. \nLes trois salles nommÃ©es aux couleurs du drapeau franÃ§ais Ã©taient disponibles tout au long de la journÃ©e pour accueillir la quarantaine de confÃ©rences organisÃ©es par KCD.\n\nLa plus-value dâ€™un portail dÃ©veloppeur chez Back Market \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  Sami Farhat - Backend Engineer\n\n\nBack Market, entreprise franÃ§aise de commerce Ã©lectronique, est venue nous parler de leur implÃ©mentation \ndâ€™un â€œDevPortalâ€ basÃ© sur le projet Backstage.io.\n\n\n\nLa crÃ©ation de ce portail dÃ©veloppeur Ã  Ã©tÃ© initiÃ© en 2021 suite au projet de mise Ã  lâ€™Ã©chelle et de passage en microservices de leur applications.\n\nInitialement, chaque nouveau service Ã©tait crÃ©Ã© manuellement et nÃ©cessitait du travail dans les Ã©quipes dâ€™infrastructure.\nEn plus de demander du travail lors de leur crÃ©ation, les services nâ€™Ã©taient donc pas systÃ©matiquement crÃ©Ã©s avec les mÃªmes bases de codes et pouvaient diffÃ©rer dans leurs implÃ©mentation.\n\nLe but Ã©tait donc dâ€™obtenir une vue centralisÃ©e sur les projets et de permettre aux dÃ©veloppeurs de crÃ©er de nouveaux services eux-mÃªmes.\n\n\n\nLa crÃ©ation de ce portail Ã  Ã©galement permis Ã  Back Market dâ€™initier lâ€™utilisation dâ€™un modÃ¨le pour la crÃ©ation de services, ainsi dâ€™uniformiser les architectures et de faciliter le passage en microservices.\n\nIls ont Ã©galement implÃ©mentÃ© une vue relationnelle concernant les projets et les Ã©quipes qui y sont associÃ©es.\n\n\n\nEnfin, pour trouver les projets prioritaires pour la migration en microservices, ils ont crÃ©Ã© une vue nommÃ©e Coupling scores :\n\n\n\nCâ€™est une vue qui permet dâ€™obtenir la liste des applications monolithiques avec le taux de couplage le plus Ã©levÃ©.\n\nLe replay de cette confÃ©rence est disponible ici.\n\nVPC dans k8s : Pas aussi simple que Ã§a en a lâ€™air \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  Louis Portay - IngÃ©nieur DevOps Kapsule Scaleway\n\n\nAu tour de Scaleway qui nous ont prÃ©sentÃ© comment ils ont implÃ©mentÃ© les VPC privÃ©s dans le service Kaspule, leur Kubernetes managÃ©.\nCâ€™est un besoin qui sâ€™est prÃ©sentÃ© afin dâ€™Ã©viter que les Ã©changes inter-noeuds transitent via le rÃ©seau public.\n\n\n\nPour utiliser le rÃ©seau privÃ© dans Kaspule, ils ont ajoutÃ© une interface nommÃ©e â€œkapsule0â€ sur les instances utilisÃ©es dans la crÃ©ation du cluster. Cette interface est ensuite attachÃ©e Ã  Cilium dans le cluster.\n\n\n\nCette fonctionnalitÃ© est actuellement en bÃªta, elle sera bientÃ´t disponible de maniÃ¨re rÃ©gionale.\n\nParmi les implÃ©mentations futures, Scaleway prÃ©voit de proposer la possibilitÃ© de retirer lâ€™interface rÃ©seau publique afin que tous les Ã©changes entre Kubelet et le Control Plane passent Ã©galement via le rÃ©seau privÃ©.\n\nLe replay de cette confÃ©rence est disponible ici.\n\nKubernetes the not so hard Veepee way \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  LoÃ¯c Blot - Lead SRE Veepee\n  MickaÃ«l Todorovic - Tribe Lead SRE Veepee\n\n\nLoÃ¯c et MickaÃ«l de Veepee sont venus prÃ©senter lâ€™Ã©volution des infrastructures utilisÃ©es par lâ€™entreprise.\nInitialement, avant 2019, ils comptaient plus de 10 000 machines virtuelles dans leur parc.\nCes machines hÃ©bergaient les applications de Veepee via diverses technologies :\n\n  Swarm\n  Rancher\n  Hashicorp Nomad\n  Docker compose\n  LXC\n\n\nEn 2019, pour anticiper la gestion de la vente des tickets pour le concert de CÃ©line Dion, ils ont choisi de migrer leurs services sur des clusters Kubernetes.\nLa premiÃ¨re infrastructure Ã©tait managÃ©e via Ansible, ils utilisaient Traefik et un cert manager home made.\n\n\n\nAujourdâ€™hui, ils fournissent un produit Container as a Service nommÃ© Starfish. Câ€™est un outil quâ€™ils ont Ã©crit en Go et qui permet de gÃ©rer les applications des Ã©quipes Veepee. Ils utilisent Ã©galement Gitlab et ArgoCD.\n\nLe replay de cette confÃ©rence est disponible ici.\n\nConclusion\n\nLa majoritÃ© de confÃ©rences auxquelles jâ€™ai assistÃ© Ã©taient des retours dâ€™expÃ©rience. Câ€™Ã©tait particuliÃ¨rement intÃ©ressant car en plus de la prÃ©sentation dâ€™une technologie, nous avons un retour dÃ©taillÃ© sur lâ€™usage de cette derniÃ¨re.\n\nMerci Ã  tout les speakers pour leur partage de connaissances et aux organisateurs de KCD France.\n"
} ,
  
  {
    "title"    : "Twitch: du streaming mais pas en lit de pierre #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/twitch-streaming",
    "date"     : "March 31, 2023",
    "excerpt"  : "Twitch: du streaming mais pas en lit de pierre.\nPrÃ©sentÃ© par Quentin GILLIE.\n",
  "content"  : "Twitch: du streaming mais pas en lit de pierre.\nPrÃ©sentÃ© par Quentin GILLIE.\n"
} ,
  
  {
    "title"    : "REX-Shape Up, un LFT dont vous Ãªtes les hÃ©ros #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/rex-shape-up",
    "date"     : "March 31, 2023",
    "excerpt"  : "REX-Shape Up, un LFT dont vous Ãªtes les hÃ©ros. \nPrÃ©sentÃ© par Pierre-Thomas GUILLOT.\n",
  "content"  : "REX-Shape Up, un LFT dont vous Ãªtes les hÃ©ros. \nPrÃ©sentÃ© par Pierre-Thomas GUILLOT.\n"
} ,
  
  {
    "title"    : "Retour ConfÃ©rence Vue Amsterdam 2023",
    "category" : "",
    "tags"     : " node, Node, vue, vuex, pinia, vite, Vitest, TypeScript, developer, javascript",
    "url"      : "/2023/03/31/retour-vue-amsterdam-2023.html",
    "date"     : "March 31, 2023",
    "excerpt"  : "Câ€™est dans le Theater Amsterdam que se sont dÃ©roulÃ©s ces deux jours de VueJS Amsterdam, Ã©vÃ©nement faisant partie de la JSWorld Conference, durant toute la semaine.\n\nDe nombreux sponsors Ã©taient lÃ  pour lâ€™occasion, ainsi que des Ã©coles comme VueMas...",
  "content"  : "Câ€™est dans le Theater Amsterdam que se sont dÃ©roulÃ©s ces deux jours de VueJS Amsterdam, Ã©vÃ©nement faisant partie de la JSWorld Conference, durant toute la semaine.\n\nDe nombreux sponsors Ã©taient lÃ  pour lâ€™occasion, ainsi que des Ã©coles comme VueMastery ou VueSchool (proposant une toute nouvelle certification Vue), et des partenaires plus connus comme Storyblok ou Nuxt Labs.\n\n\n\nLâ€™ambiance Ã©tait au rendez-vous dÃ¨s le dÃ©but avec une confÃ©rence en musique, avec Tim Benniks guitare en main !\n\nNous avons ensuite pu profiter de confÃ©rences aussi nombreuses que variÃ©es. De lâ€™accessibilitÃ© Ã  la gestion de lâ€™interface en Vue du leader mondial du transport de marchandises Maersk, en passant par les tests et le guide ultime pour publier un package NPM !\n\nSans oublier la grande famille Nuxt qui Ã©tait (presque) au complet !\n\nLâ€™ensemble des confÃ©rences est visible sur la chaÃ®ne youtube du JSWorld Conference.\n\nState of Vuenion\n\nEvan You (crÃ©ateur de Vue et Vite) a prÃ©sentÃ© un Ã©tat des lieux et des derniÃ¨res avancÃ©es de Vue, Ã©paulÃ© par Alex Kyriakidis (fondateur de VueSchool).\n\n\n\nCommenÃ§ant par un retour sur les nombreuses nouveautÃ©s de ces trois derniÃ¨res annÃ©es, pour les plus connues Vue 3, Vite, Vitest et Pinia, Evan a dâ€™abord fait un focus sur la position de Vue 3 en tant que version par dÃ©faut depuis le 7 fÃ©vrier 2022.\n\nIl a aussi Ã©voquÃ© les avancÃ©es relatives Ã  Vue 2.7 (dont lâ€™intÃ©gration de la Composition API, du v-bind CSS, etc.) permettant de rapprocher les expÃ©riences dÃ©veloppeurs entre Vue 2 et Vue 3.\n\nEvan a ensuite prÃ©sentÃ© les derniers travaux sur la version core de Vue en version 3. Principalement orientÃ©s sur des amÃ©liorations concernant la facilitÃ© dâ€™utilisation, lâ€™accÃ©lÃ©ration des tests (avec Vitest) ainsi que la vitesse de build (avec Rollup).\n\nEnfin, Evan a prÃ©sentÃ© les projets Ã  venir pour le core. La disparition de la Reactivity Transform jugÃ©e trop risquÃ©e, lâ€™amÃ©lioration du server side rendering et la crÃ©ation dâ€™un â€œvapor modeâ€, une nouvelle maniÃ¨re de compiler les Single Files Components en optimisant lâ€™utilisation dâ€™un Virtual DOM. Ce dernier permet dâ€™approcher la vitesse de compilation dâ€™une application en JS vanilla, en gardant la puissance du framework !\n\n\n  ğŸ“º ConfÃ©rence sur Youtube\n\n\nLa fin de vie de Vue 2\n\n\n\nAprÃ¨s avoir prÃ©sentÃ© lâ€™Ã©tat de lâ€™Ã©cosystÃ¨me qui gravite autour de Vue et Vite lors de sa confÃ©rence â€œState of the Vuenion 2023â€, Evan You a terminÃ© en dÃ©voilant la date de fin de vie de Vue 2 au 31 dÃ©cembre 2023.\n\nAprÃ¨s cette date, la version arrÃªtera de recevoir des mises Ã  jour ; il sera nÃ©anmoins possible de bÃ©nÃ©ficier de mise Ã  jour payante, bien quâ€™il soit vivement conseillÃ© de migrer vers la derniÃ¨re version du framework.\n\nUne page dans la documentation a Ã©tÃ© mise en place, prÃ©sentant les options qui sâ€™offrent aux dÃ©veloppeurs et aux organismes qui nâ€™ont pas encore migrÃ© leurs applications.\n\nIl Ã©tait une foisâ€¦ Histoire\n\n\n\nGuillaume Chau, un des membres de lâ€™Ã©quipe de dÃ©veloppement de Vue, a prÃ©sentÃ© lâ€™outil Histoire durant une petite demi-heure.\n\nLâ€™outil est dans la mÃªme veine que Storybook. La plupart du temps, il sert Ã  afficher et documenter des composants dâ€™un design system en complÃ¨te isolation.\n\nContrairement Ã  dâ€™autres outils, Histoire est pensÃ© pour sâ€™intÃ©grer parfaitement dans son environnement de dÃ©veloppement, de faÃ§on Ã  ce que lâ€™Ã©criture des â€œstoriesâ€ sâ€™apparente le plus possible Ã  lâ€™Ã©criture et Ã  lâ€™utilisation native de composants Vue.\n\nHistoire utilise Vite ce qui lui permet de sâ€™intÃ©grer dans un projet qui lâ€™utilise dÃ©jÃ  avec trÃ¨s peu de configurations supplÃ©mentaires.\n\n\n  ğŸ“º ConfÃ©rence sur Youtube\n\n\nUne autre histoire deâ€¦ migration !\n\n\n\nLa sociÃ©tÃ© Maersk, spÃ©cialiste dans la logistique des transports, nous a prÃ©sentÃ© son application destinÃ©e entre autres, Ã  la gestion des conteneurs maritimes. Une occasion pour nous faire part de leur processus de migration de Vue 2 vers Vue 3 !\n\nRÃ©alisant la mÃªme migration de version Ã  Bedrock Streaming sur la partie backoffice, nous constatons que nous partageons beaucoup de similitudes !\n\nVous trouverez un article dÃ©diÃ© sur la migration Vue 2 vers Vue 3 Ã  Bedrock en suivant ce lien ! ğŸ‰\n\n\n  ğŸ“º ConfÃ©rence sur Youtube\n\n\nâ€œLetâ€™s Build A Virtual DOMâ€\n\n\n\nCertaines confÃ©rences Ã©taient aussi lâ€™occasion de rappeler des fondamentaux. Beaucoup de dÃ©veloppeurs sont conscients que Vue utilise un systÃ¨me de DOM virtuel pour gÃ©nÃ©rer ses pages mais peu savent vraiment ce que cela signifie.\n\nLa confÃ©rence de Marc Backes a permis de dÃ©mystifier cela en dÃ©veloppant sur scÃ¨ne un DOM virtuel simple Ã  travers plusieurs cas pratiques.\n\nCe dernier a dâ€™ailleurs publiÃ© le code Ã©crit sur son Github : https://github.com/themarcba/vue-vdom.\n\n\n  ğŸ“º ConfÃ©rence sur Youtube\n\n\nLe guide complet du packaging des librairies\n\n\n\nCette confÃ©rence Ã©tait prÃ©sentÃ©e par Bjorn Lu (core team member de Astro, Vite et Svelte), et expliquait comment crÃ©er un package dâ€™une librairie et le publier â€œpresqueâ€ sans peine.\n\nEn prenant lâ€™exemple dâ€™une librairie extrÃªmement simple, proposant une fonction dâ€™addition, Bjorn a parcouru les Ã©tapes de la crÃ©ation de ce package progressivement, en prenant en compte le fonctionnement dâ€™ESModules, lâ€™ajout du typage, puis le support de CommonJS pour les utilisations sur des anciennes versions de node et de lâ€™export en parallÃ¨le des version ESM et CJS.\n\nIl prÃ©sente ensuite les outils de build les plus courants ainsi que quelques outsiders, puis prend lâ€™exemple de Tsup pour montrer une commande de build.\n\nUne solution intÃ©ressante pour typer utilisant JSDoc et simplifiant beaucoup les Ã©tapes du build completera sa confÃ©rence, \npour enfin terminer par une sÃ©rie dâ€™outils et de â€œdo and donâ€™tâ€ trÃ¨s pratiques dont publint.dev fait parti.\n\n\n  ğŸ“º ConfÃ©rence sur Youtube\n\n\nConclusion\n\n\n\nCe sÃ©jour Ã  Amsterdam pour assister Ã  cette confÃ©rence de deux jours a Ã©tÃ© enrichissant Ã  bien des Ã©gards.\nNon seulement la ville est belle et agrÃ©able Ã  visiter, mais la dÃ©couverte dâ€™outils prometteurs rÃ©pondant Ã  nos besoins a Ã©galement Ã©tÃ© un vÃ©ritable apport pour notre entreprise.\n\nPour en dÃ©couvrir plus\n\n\n  Site VueJS Amsterdam\n  Gallerie photos\n  Replay des confÃ©rences sur Youtube\n\n"
} ,
  
  {
    "title"    : "Montres bracelets, le guide pratique de l&#39;amateur d&#39;horlogerie #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/guide-pratique-amateur-horlogerie",
    "date"     : "March 31, 2023",
    "excerpt"  : "Montres bracelets, le guide pratique de lâ€™amateur dâ€™horlogerie.\nPrÃ©sentÃ© par Rafi PANOYAN.\n",
  "content"  : "Montres bracelets, le guide pratique de lâ€™amateur dâ€™horlogerie.\nPrÃ©sentÃ© par Rafi PANOYAN.\n"
} ,
  
  {
    "title"    : "Comment gÃ©rer des journÃ©es de 35h #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-gerer-des-journees-de-35h",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment gÃ©rer des journÃ©es de 35h\nPrÃ©sentÃ© par Sylvain GOUGOUZIAN.\n",
  "content"  : "Comment gÃ©rer des journÃ©es de 35h\nPrÃ©sentÃ© par Sylvain GOUGOUZIAN.\n"
} ,
  
  {
    "title"    : "Comment (enfin) sortir vos side projects #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-enfin-sortir-vos-side-projects",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment (enfin) sortir vos side projects.\nPrÃ©sentÃ© par Thomas JARRAND.\n",
  "content"  : "Comment (enfin) sortir vos side projects.\nPrÃ©sentÃ© par Thomas JARRAND.\n"
} ,
  
  {
    "title"    : "Comment j&#39;ai rÃ©ussi Ã  capturer la couleur et quelle est sa signification ? #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/capture-et-signification-de-la-couleur",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment jâ€™ai rÃ©ussi Ã  capturer la couleur et quelle est sa signification ?\nPrÃ©sentÃ© par Hugo DETANG.\n",
  "content"  : "Comment jâ€™ai rÃ©ussi Ã  capturer la couleur et quelle est sa signification ?\nPrÃ©sentÃ© par Hugo DETANG.\n"
} ,
  
  {
    "title"    : "Couleur, Typographie, Logo: Analyse dune charte graphique #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/analyse-charte-graphique",
    "date"     : "March 31, 2023",
    "excerpt"  : "Couleur, Typographie, Logo: Analyse dune charte graphique. \nPrÃ©sentÃ© par Sylvain MASSON.\n",
  "content"  : "Couleur, Typographie, Logo: Analyse dune charte graphique. \nPrÃ©sentÃ© par Sylvain MASSON.\n"
} ,
  
  {
    "title"    : "De Node.js 10 Ã  Node.js 18, nous avons rattrapÃ© 8 ans de retard et de dette technique",
    "category" : "",
    "tags"     : " node, Node.js, vue, Vue.js, vuex, pinia, vite, Vite.js, Vitest, TypeScript, developer retention, migration",
    "url"      : "/2023/03/25/de-node-js-10-a-node-js-18-nous-avons-rattrape-8-ans-de-retard-et-de-dette-technique-et-seule-une-approche-progressive-et-incrementale-etait-viable.html",
    "date"     : "March 25, 2023",
    "excerpt"  : "Difficile de faire Ã©voluer des applications et amÃ©liorer une stack si lâ€™ensemble est basÃ© sur une version obsolÃ¨te de Node.jsâ€¦ Dans cet article, nous verrons comment nous avons rÃ©ussi Ã  migrer vers une version rÃ©cente et maintenue de Node.js grÃ¢ce...",
  "content"  : "Difficile de faire Ã©voluer des applications et amÃ©liorer une stack si lâ€™ensemble est basÃ© sur une version obsolÃ¨te de Node.jsâ€¦ Dans cet article, nous verrons comment nous avons rÃ©ussi Ã  migrer vers une version rÃ©cente et maintenue de Node.js grÃ¢ce Ã  une approche progressive et incrÃ©mentale.\n\n\n  Contexte gÃ©nÃ©ral et fonctionnel\n  Contexte technique\n  Objectif\n  Une premiÃ¨re stratÃ©gie problÃ©matique : la mÃ©thode â€œrhinocÃ©rosâ€ ğŸ¦\n  La stratÃ©gie gagnante : une migration progressive ğŸ“¶    \n      Motivation\n      Plan dâ€™action\n    \n  \n  DifficultÃ©s rencontrÃ©es    \n      Non dÃ©coupage des Ã©tapes de migration\n      MÃ©connaissance de Typescript\n      Suppression prÃ©cipitÃ©e de librairies obsolÃ¨tes\n      Non anticipation de la complexitÃ© liÃ©e Ã  certaines dÃ©pendances\n      Entretien des applications legacy en mÃªme temps\n    \n  \n  Autres avantages    \n      Uniformisation des technologies au sein de la sociÃ©tÃ©\n      AttractivitÃ© et rÃ©tention des dÃ©veloppeurs\n    \n  \n  Conclusion\n\n\nContexte gÃ©nÃ©ral et fonctionnel\n\nBedrock streaming est une co-entreprise (joint-venture) crÃ©Ã©e en 2020 par M6 Group et RTL Group, permettant Ã  7 diffuseurs et sociÃ©tÃ©s de mÃ©dias dans 5 pays dâ€™Europe de divertir 45 millions dâ€™utilisateurs chaque jour, sur tous les Ã©crans.\n\nPour gÃ©rer tous leurs utilisateurs ainsi que leurs contenus, notamment vidÃ©os, les clients de Bedrock Streaming accÃ¨dent chacun Ã  une constellation dâ€™applications au sein dâ€™un back-office centralisÃ© (appelÃ© BO par la suite).\n\nContexte technique\n\nDe part sa conception initiale, le BO est une application monorepo. Elle fournit (Ã  elle-mÃªme donc), des donnÃ©es via une API Symfony 4 (PHP 7.4), consommÃ©es uniquement par :\n\n\n  des applications Vue.js 1 et Vue.js 2 gÃ©rÃ©es par la team backend (qui historiquement maintient le frontend de quelques applications) ;\n  des applications Vue.js 2 gÃ©rÃ©es par la team frontend.\n\n\nLe tout, dans un environnement Node.js 10.\n\nObjectif\n\nNode.js 10 est arrivÃ© en fin de vie le 30 avril 2021. Il nâ€™est donc plus maintenu, que ce soit en terme de fonctionnalitÃ©s ou en terme de sÃ©curitÃ©. Naturellement, toutes les dÃ©pendances JS migrent progressivement vers un support des versions de Node.js supÃ©rieures, et abandonnent le support de cette version 10 devenue obsolÃ¨te.\n\nIl sâ€™agit donc de migrer la version de Node.js vers une version supÃ©rieure, dans lâ€™idÃ©al LTS afin de se prÃ©munir dâ€™une obsolescence prÃ©maturÃ©e. Dans un premier temps, Node.js 12.\n\nVoici plusieurs raisons qui poussent Ã  migrer Node.js :\n\n\n  Nouvelles fonctionnalitÃ©s (e.g. nouvelle implÃ©mentation pour lâ€™ES6 Module Support expÃ©rimental, source : https://nodejs.medium.com/announcing-a-new-experimental-modules-1be8d2d6c2ff ) ;\n  Abandon de fonctionnalitÃ©s dÃ©faillantes (e.g. via dÃ©prÃ©ciation) ;\n  Performance (e.g. mise Ã  jour de V8 engine, source : https://nodejs.medium.com/introducing-node-js-12-76c41a1b3f3f ) ;\n  SÃ©curitÃ© (e.g. mise Ã  jour de TLS, source : https://nodejs.medium.com/introducing-node-js-12-76c41a1b3f3f ) ;\n  Ã‰volutions des dÃ©pendances externes. (e.g. Cypress qui abandonne les versions de Node.js non maintenues et qui requiert Node.js 14, 16 ou 18+, source : https://docs.cypress.io/guides/references/changelog#12-0-0).\n\n\nUne premiÃ¨re stratÃ©gie problÃ©matique : la mÃ©thode â€œrhinocÃ©rosâ€ ğŸ¦\n\nLa dÃ©cision a Ã©tÃ© prise de migrer le repository de Node.js 10 vers Node.js 12 en dÃ©but dâ€™annÃ©e 2021.\n\nEmpiriquement, cette mÃ©thode a montrÃ© plusieurs limites :\n\n\n  mÃªme si la compilation semblait bien se dÃ©rouler, des erreurs apparaissaient au moment de lâ€™affichage de lâ€™UI â¡ Il semblait donc nÃ©cessaire de parcourir lâ€™intÃ©gralitÃ© des Ã©crans afin de dÃ©celer toutes les anomalies possibles â¡ Le travail de la QA Ã©tait alors consÃ©quent ;\n  mÃªme lorsquâ€™une anomalie est corrigÃ©e, une nouvelle peut apparaitre â¡ Il fallait re-parcourir les Ã©crans concernÃ©s (par exemple, aprÃ¨s avoir corrigÃ© une anomalie qui empÃªche lâ€™apparition dâ€™une modale, de nouvelles anomalies peuvent Ãªtre dÃ©celÃ©es au niveau des fonctionnalitÃ©s que permet cette modale) â¡ Le travail de la QA augmentait de faÃ§on exponentielle au fil des corrections dâ€™anomalies ;\n  des dizaines voire centaines de dÃ©pendances dans le projet Ã©taient dÃ©pendantes de Node.js 10 sans Ãªtre encore compatibles avec Node.js 12 â¡ Il sâ€™agissait donc de faire le point sur celles-ci, pour trouver des Ã©quivalents compatibles.\n\n\nAprÃ¨s plusieurs mois, bien que bon nombre dâ€™anomalies avaient pu Ãªtre corrigÃ©es, la situation stagnait et la fin ne semblait pas plus proche quâ€™au dÃ©but.\n\nLes raisons de lâ€™Ã©chec :\n\n\n  Lâ€™anciennetÃ© de certaines applications. Certaines dâ€™entre elles avaient plus de 8 ans dâ€™existence. En nâ€™ayant subi que quelques corrections seulement. Les connaissances fonctionnelles et techniques sâ€™Ã©taient donc estompÃ©es naturellement, en raison dâ€™une absence de documentation (autant fonctionnelle que technique). Il sâ€™agit lÃ  des dettes fonctionnelle et technique. Lorsquâ€™elles sont lÃ , elles sont relativement simples Ã  identifier. Mais câ€™est dÃ©jÃ  trop tardâ€¦ ;\n  Lâ€™absence de mise Ã  jour des technologies. Certaines technologies devenues obsolÃ¨tes (jQuery 1.9, Vue.js 1, Bootstrap 2.3) imposait non plus un refactor liÃ© Ã  une migration, mais une vÃ©ritable refonte ;\n  Lâ€™absence de tests. La couverture de tests Ã©tait alors faible voire nulle. Migrer sans rÃ©gression relevait alors dâ€™une chance non maitrisable ;\n  La faÃ§on dont la migration a Ã©tÃ© lancÃ©e Ã©tait trop tÃ©mÃ©raire : câ€™est la mÃ©thode rhinocÃ©ros.\n    \n      crÃ©ation dâ€™une nouvelle branche (et dâ€™une PR pour cette branche)\n      suppression de Node.js 10 et installation de Node.js 12\n      correction de toutes les anomalies qui apparaissent !\n    \n  \n\n\nCe fonctionnement peut marcher pour des pÃ©rimÃ¨tres techniques plus petits ou du moins dont les contours sont prÃ©cisÃ©ment marquÃ©s ;\n\nLâ€™organisation en Ã©quipe devenait compliquÃ©e. Au fur et Ã  mesure des dÃ©couvertes des anomalies au sein dâ€™une seule et unique PR, il devenait difficile de suivre tous les sujets, sans dÃ©coupage prÃ©cis et rigoureux.\n\nFace Ã  cette situation, dont les dÃ©veloppeurs et testeurs ne semblaient plus voir le bout, il a Ã©tÃ© dÃ©cidÃ© dâ€™employer une autre stratÃ©gie.\n\nLa stratÃ©gie gagnante : une migration progressive ğŸ“¶\n\nDe part un essoufflement des dÃ©veloppeurs et une nouvelle Ã©nergie insufflÃ©e par des dÃ©parts et arrivÃ©es dans lâ€™Ã©quipe, une nouvelle stratÃ©gie a Ã©mergÃ©. Face Ã  lâ€™Ã©chec de la premiÃ¨re, il a Ã©tÃ© proposÃ© plus simplement de partir sur des bases saines, afin de migrer les applications sur des fondations plus solides car maitrisÃ©es.\n\nPlus techniquement, cela sâ€™est traduit par :\n\n\n  CrÃ©ation dâ€™un nouveau rÃ©pertoire modern-apps/ dans le monorepo.\n  Mise en place dâ€™une architecture basÃ©e sur Node.js 16 (Oui oui, Node.js 16 directement ! Il sâ€™agissait de la version LTS en cours en date de dÃ©but 2022.) dans ce rÃ©pertoire seulement.\n  Migration des applications du BO, une par une, vers une stack plus moderne. En date de dÃ©but 2023, cette migration est toujours en cours.\n\n\nMotivation\n\nLa motivation Ã©tait principalement portÃ©e par :\n\n\n  une volontÃ© forte dâ€™abandonner des outils et technologies vieillissantes voire obsolÃ¨tes ;\n  une pression engendrÃ©e par lâ€™Ã©volution rapide des technologies :\n    \n      Node.js sort une version LTS tous les ans ;\n      Vue.js 3 venait de sortir et lâ€™effort des dÃ©veloppeurs du framework allait se porter plutÃ´t sur cette version que sur la version 2.\n    \n  \n  une pression engendrÃ©e par les autres Ã©quipes de la sociÃ©tÃ© qui, elles, Ã©taient Ã  jour (pour certaines), dont celle qui proposait des outils JS et TS dont lâ€™Ã©quipe pourrait avoir lâ€™usage, comme par exemple une librairie de configuration pour eslint couplÃ© Ã  vue ;\n  une excitation liÃ©e Ã  lâ€™utilisation dâ€™une stack rÃ©cente et de cutting-edge tools.\n\n\nPlan dâ€™action\n\nCette page blanche a nÃ©cessitÃ© un plan dâ€™action que voici :\n\n\n  CrÃ©ation dâ€™une application simplissime en guise de PoC, afin de montrer la viabilitÃ© dâ€™un travail sous Node.js 16 dans une sous-partie du projet en parallÃ¨le dâ€™un travail toujours actif sous Node.js 10 dans le reste du projet.\n  Mise en place dâ€™une certaines DX vis-Ã -vis des linters et formatters notamment (ainsi que dâ€™extensions dâ€™IDE), par lâ€™application de rÃ¨gles simples mais strictes, qui Ã©vitent aux dÃ©veloppeurs les tÃ¢ches sans plus-value, comme ajuster manuellement lâ€™indentation ou ajouter les points-virgules.\n  Migration des librairies internes au monorepo.\n  Migration du design system, ainsi que des outils affÃ©rents (Storybook).\n  Migration dâ€™une premiÃ¨re application, la plus simple possible. Lâ€™objectif Ã©tait alors de se rendre compte trÃ¨s concrÃ¨tement des Ã©tapes de migration dâ€™une application, afin dâ€™en tirer une documentation exploitable pour les futures applications. Il en est ressorti que la majeure partie du travail consistait Ã  refactor le code avec les nouvelles technologies choisies, en lâ€™occurrence :\n    \n      Vue.js 3 et sa Composition API (framework JS),\n      Vite (serveur de dev et de build),\n      Pinia (global state management),\n      Vitest (framework de test unitaire),\n      Cypress dans ses derniÃ¨res versions (framework de test end-to-end)\n      aussi et surtout Typescript (langage de programmation, sur-couche Ã  JS).\n    \n  \n  Migration du processus de build et dâ€™intÃ©gration aux templates backend (via notamment une extension Twig implÃ©mentÃ©e par nos soins, ViteAppExtension.php)\n  Mise en place dâ€™une CI pour ces nouvelles applications, calquÃ©e sur celle des anciennes applications : linting, tests pour celles qui en avaient, dÃ©ploiement en preview, etc.\n\n\nEn quelques mois seulement, il a Ã©tÃ© possible dâ€™obtenir un rÃ©sultat concret. Le rÃ©pertoire modern-apps/ a Ã©tÃ© initiÃ© en fÃ©vrier 2022, et dÃ¨s avril de la mÃªme annÃ©e, une premiÃ¨re application migrÃ©e Ã©tait livrÃ©e en production. Et cela, avec un seul dÃ©veloppeur Ã  plein temps sur le sujet.\n\nDifficultÃ©s rencontrÃ©es\n\nCette seconde stratÃ©gie nâ€™a bien sÃ»r pas Ã©tÃ© sans encombre. Voici les principales difficultÃ©s rencontrÃ©es, dont lâ€™Ã©quipe a su se prÃ©munir au fil du temps.\n\nNon dÃ©coupage des Ã©tapes de migration\n\nLors de la migration dâ€™une des premiÃ¨res applications dont la complexitÃ© Ã©tait lÃ©gÃ¨rement supÃ©rieure aux prÃ©cÃ©dentes, nous nous sommes retrouvÃ©s embourbÃ©s dans une multitude de bugs techniques et fonctionnels. En effet, migrer implique plusieurs changements qui nâ€™ont pas nÃ©cessairement de rapport les uns avec les autres :\n\n\n  ajouter des types TS\n  migrer la librairie de Global State Management de vuex vers pinia\n  migrer la Global API de Vue (de new Vue() vers createApp())\n  migrer de lâ€™Options API vers la Composition API de Vue\n  etc.\n\n\nSi tous ces changements sont opÃ©rÃ©s en mÃªme temps, comment rÃ©agir lors de lâ€™apparition dâ€™une anomalie ? Comment traquer efficacement cette anomalie ?\n\n\n  Solution adoptÃ©e\n\n  Nous avons dÃ©cidÃ© de dÃ©couper plus finement nos dÃ©veloppements. Une PR doit concerner un pÃ©rimÃ¨tre rÃ©duit et bien dÃ©fini. Par exemple, la PR de migration de la librairie de Global State Management ne doit comporter que des modifications Ã  ce sujet, et doit fournir une application fonctionnelle dont les tests passent.\n\n\nMÃ©connaissance de Typescript\n\n\n  TypeScript is a strongly typed programming language that builds on JavaScript, giving you better tooling at any scale.\n\n\nSource : https://www.typescriptlang.org/\n\nCe langage de programmation, bien que son adoption parmi les dÃ©veloppeurs JS explose, sâ€™est avÃ©rÃ© une complÃ¨te nouveautÃ© dans lâ€™Ã©quipe. Il peut Ãªtre tentant dâ€™Ã©crire des any partout, ou de supprimer le strict modeâ€¦\n\n\n  Solution adoptÃ©e\n\n  Nous avons dÃ©cidÃ© dâ€™intÃ©grer TS progressivement sans se mettre trop de pression quant Ã  lâ€™intÃ©gralitÃ© du typage de nos applications. Typescript permet justement cette intÃ©gration progressive aux projets.\n\n  Un trÃ¨s gros progrÃ¨s a aussi Ã©tÃ© rÃ©alisÃ© grÃ¢ce Ã  la gÃ©nÃ©ration automatique des types TS Ã  partir de lâ€™API (grÃ¢ce Ã  lâ€™introspection system de GraphQL). Les donnÃ©es reÃ§ues du backend se voyaient alors avoir une structure directement exploitable dans le frontend.\n\n\nSuppression prÃ©cipitÃ©e de librairies obsolÃ¨tes\n\nLors du dÃ©coupage des Ã©tapes de migration, une problÃ©matique est apparue. Par exemple, si nous souhaitons migrer de vuex vers pinia dans un second temps, comment faire pour que lâ€™application reste fonctionnelle avec vuex dans le premier temps ?\n\n\n  Solution adoptÃ©e\n\n  Nous avons dÃ©cidÃ© de conserver certaines librairies, le temps de la migration des applications. Il peut Ãªtre tentant de vouloir supprimer immÃ©diatement ce qui nous semble obsolÃ¨te, mais ces Ã©lÃ©ments ne seront vraiment obsolÃ¨tes que lorsque toutes les applications seront migrÃ©es ; mais pas le temps quâ€™elles le soient.\n\n\nNon anticipation de la complexitÃ© liÃ©e Ã  certaines dÃ©pendances\n\nBien que cet aspect nâ€™Ã©tait pas une surprise, certaines librairies ont apportÃ© plus de difficultÃ©s que dâ€™autres lors de la migration. Par exemple, lâ€™intÃ©gration de Vue 3 et la Composition API impliquait la montÃ©e de version de vee-validate, un librairie de validation de formulaire. Il sâ€™est avÃ©rÃ© que lâ€™implÃ©mentation imposÃ©e Ã©tait radicalement diffÃ©rente de la version prÃ©cÃ©dente (compatible avec Vue 2 et lâ€™Options API), moins intuitive et plus complexe.\n\n\n  Solution adoptÃ©e\n\n  Ce cas de figure nâ€™est pas vraiment impressionnant car nous nous y attendions. Nous avons dÃ©cidÃ© dans un premier temps dâ€™effectuer une certaine veille technique, afin de remettre en cause le choix initial de cette librairie. Il sâ€™est avÃ©rÃ© que nous lâ€™avons conservÃ©e, ce qui amenait dans un second temps une montÃ©e en compÃ©tence quant Ã  lâ€™utilisation de celle-ci, en vue de son intÃ©gration.\n\n\nEntretien des applications legacy en mÃªme temps\n\nUne application donnÃ©e pouvait se retrouver dâ€™une part en cours de migration, et dâ€™autre part devoir recevoir une Ã©volution ou une correction dâ€™anomalie.\n\n\n  Solution adoptÃ©e\n\n  Le choix et lâ€™ordre des applications Ã  migrer a Ã©tÃ© choisi en fonction des prioritÃ©s en cours. Nous avons choisi de migrer en premier les applications qui ne subissaient que trÃ¨s peu de modifications. Par la suite, et encore aujourdâ€™hui, nous livrons en production rapidement chaque application migrÃ©e, afin de ne pas avoir Ã  maintenir plusieurs versions en mÃªme temps (la version legacy Ã©tant tout de mÃªme conservÃ©e le temps de sâ€™assurer que la version moderne tourne correctement en production auprÃ¨s des clients). Dans les trÃ¨s rares cas oÃ¹ une application en cours de migration devait recevoir une Ã©volution ou une correction dâ€™anomalie, nous la traitions dans les 2 versions.\n\n\nAutres avantages\n\nUniformisation des technologies au sein de la sociÃ©tÃ©\n\nAu sein de Bedrock, le back-office nâ€™est pas la seule application. Il existe aussi des applications frontend sur les mÃªmes technologies pour adresser lâ€™Ã©cran web ou les tÃ©lÃ©visions connectÃ©es. Bien que le framework utilisÃ© pour celles-ci soit React.js et non Vue.js, lâ€™outillage peut Ãªtre uniformisÃ© entre les projets et les Ã©quipes. La migration a permis de prÃ©parer le terrain pour mettre en place ces outils : TypeScript, PNPM, etc.\n\nAttractivitÃ© et rÃ©tention des dÃ©veloppeurs\n\nCette migration gÃ©nÃ©rale permet de mettre en place une stack rÃ©solument plus moderne et dâ€™utiliser des outils et technologies plus rÃ©cents. Nâ€™est-ce pas lÃ  un argument fort pour attirer des nouveaux dÃ©veloppeurs et retenir ceux dÃ©jÃ  en place ? Dans lâ€™Ã©quipe, plusieurs personnes ont Ã©mis des doutes sur leur volontÃ© de rester dans la sociÃ©tÃ© si la dÃ©cision de migrer, et donc dâ€™intÃ©grer des technologies plus Ã  jour, nâ€™avait pas Ã©tÃ© prise. En date de dÃ©but 2023, il fait peu de doutes que les projets en Vue 3 sont plus attractifs que les projets en Vue 2â€¦\n\nConclusion\n\nEn fin de compte, cette approche progressive et incrÃ©mentale, toujours en cours, permet de maintenir dans un rÃ©pertoire bien dÃ©fini une stack rÃ©cente dont les mises Ã  jour sont simples car petites. Par exemple, nous avons rÃ©cemment migrÃ© de Node.js 16 vers Node.js 18â€¦ en quelques jours !\n\nCette grande aventure, toujours en cours, nous a permis de vraiment prendre conscience quâ€™il faut entretenir certes les applications mais aussi les versions des frameworks et outils ! Utiliser un nouvel outil ou une nouvelle technologie est un choix fort quâ€™il faut Ãªtre capable dâ€™assumer dans le temps.\n\nIl peut paraitre frustrant dâ€™entretenir des outils, sans gagner en performance ni en productivitÃ© mais seulement pour ne pas devenir obsolÃ¨te. Mettre lâ€™accent sur ces points, tout en sachant bien jauger jusquâ€™oÃ¹ doivent aller ces upgrades, est la marque dâ€™un certain professionnalisme.\n\nIl est vrai que dans lâ€™immÃ©diat, la valeur ajoutÃ©e pour le client est modÃ©rÃ©e : les gains restent trÃ¨s techniques, notamment en termes de stabilitÃ© et de performances. Ce nâ€™est que plus tard que les gains se feront concrÃ¨tement sentir : plus dâ€™efficacitÃ© et de productivitÃ© pour les Ã©volutions, et plus de fiabilitÃ©.\n\nIl est aussi important de savoir reconnaitre quâ€™une technologie utilisÃ©e (parfois avec fiertÃ© Ã  ses dÃ©buts) est devenue obsolÃ¨te, et quâ€™il faut sâ€™en dÃ©barrasser pendant quâ€™il est encore temps.\n"
} ,
  
  {
    "title"    : "La gamification contre le legacy",
    "category" : "",
    "tags"     : " infra, legacy, retour d'expÃ©rience",
    "url"      : "/2023/03/20/La-gamification-contre-le-legacy.html",
    "date"     : "March 20, 2023",
    "excerpt"  : "Ce que vous ne voulez pas voir dans vos backlogsâ€¦\n\nElles sont lÃ , tapies dans lâ€™ombre de la colonne â€œTo doâ€ de vos backlogs, attendant que leur heure vienne. Ã€ chaque backlog refinement, vous vous demandez sâ€™il ne faut pas tout simplement les annu...",
  "content"  : "Ce que vous ne voulez pas voir dans vos backlogsâ€¦\n\nElles sont lÃ , tapies dans lâ€™ombre de la colonne â€œTo doâ€ de vos backlogs, attendant que leur heure vienne. Ã€ chaque backlog refinement, vous vous demandez sâ€™il ne faut pas tout simplement les annuler, puisque personne ne les prend en chargeâ€¦ De quoi parle-t-on ? De ces user stories qui existent dans le backlog de chaque Ã©quipe technique, pour traiter â€œun jourâ€ un sujet legacy. Ces petits aides-mÃ©moire de sujets â€œÃ  ne pas oublierâ€ qui nous poursuivent mais ne sont que peu souvent traitÃ©s, faute de priorisation.\n\n\nUn exemple de backlog legacy\n\n\nClean de code mort, montÃ©es de versions de layers Terraform, projets de refactoring jamais dÃ©butÃ©sâ€¦ autant de sujets pÃ©nibles Ã  traiter qui nÃ©cessitent du tempsâ€¦ et de la rÃ©silience. Parce que bien souvent, dÃ©buter lâ€™un de ces sujets revient Ã  sâ€™attaquer Ã  toutes les dÃ©pendances liÃ©es, Ã  gÃ©rer tous les impacts. Et parce quâ€™il sâ€™agit aussi de tÃ¢ches redondantes, non-automatisables, nâ€™apportant quasiment aucune valeur business immÃ©diatement mesurable.. Du â€œrunâ€, pur et simple. Dans le Slack de Bedrock, il y a un emoji tout trouvÃ© pour ce type de tÃ¢che : \n\nBien sÃ»r, on parvient parfois Ã  dÃ©gager du temps pour sâ€™atteler Ã  ces user stories. Mais il faut souvent plus dâ€™un sprint pour en venir Ã  bout, et lâ€™Ã©quipe en charge de leur rÃ©alisation peut rapidement se dÃ©courager devant lâ€™ampleur et le caractÃ¨re rÃ©pÃ©titif de la tÃ¢che.\n\nNos Ã©quipes Ops et DevOps sont responsables de 23 repositories Terraform. Lorsquâ€™il a Ã©tÃ© nÃ©cessaire dâ€™upgrader tous nos layers en version 1.x, nous nous sommes dâ€™abord donnÃ© pour consigne que chaque personne qui tombait sur un layer obsolÃ¨te devait le mettre Ã  jour avant de poursuivre son travail. Oui mais voilÃ , mettre Ã  jour un layer Ã§a ne se fait pas en deux minutes, et bien souvent on refuse dâ€™abandonner ce sur quoi on travaillait jusquâ€™alors pour mettre Ã  jour sa version de Terraform. La consigne a alors Ã©voluÃ© : pour chaque layer Ã  mettre Ã  jour, on crÃ©Ã© une US en colonne â€œto doâ€â€¦ Vous voyez oÃ¹ lâ€™on veut en venir ? ğŸ˜\n\nPour tenter de venir Ã  bout de ces sujets legacy que lâ€™on traÃ®ne comme des boulets, nous avons mis en place depuis octobre 2022 les â€œJeudis du funâ€, dont lâ€™organisation est prise en charge par la facilitatrice agile et la Project Manager Officer (PMO) du service Infrastructure (autrices de cet article).\n\n\nLogo de la 1Ã¨re Ã©dition du â€œjeudi du funâ€\n\n\nÃ‰radiquer en gamifiant, challengeant, sâ€™entraidant.\n\nLâ€™idÃ©e est simple : faire travailler ensemble, sur une journÃ©e, les cinq Ã©quipes de la verticale (trois Ã©quipes de SysAdmins et deux Ã©quipes DevOps) pour faire avancer un sujet legacy. Au cours de cette journÃ©e, les profils et les membres dâ€™Ã©quipe seront mixÃ©s, afin de ne pas travailler avec les mÃªmes collÃ¨gues quâ€™au quotidien. Leads, principal engineer, seniors et juniors : tout le monde participe Ã  la corvÃ©e !\n\nIl est difficile de convoquer 25 personnes sur une journÃ©e en leur disant que la journÃ©e est banalisÃ©e pour traiter des tÃ¢ches pÃ©nibles. Elles viendraient Ã  reculons. Deux axes ont Ã©tÃ© choisis pour faire de ces journÃ©es des journÃ©es â€œparticuliÃ¨resâ€ :\n\n\n  Gamifier certains moments clÃ©s de la journÃ©e : la dÃ©couverte du sujet, la composition des Ã©quipes, la remise en jambe du dÃ©but dâ€™aprÃ¨s-midiâ€¦\n  Challenger les participants pour ne pas simplement leur demander de traiter du legacy, mais bien dâ€™Ãªtre la meilleure Ã©quipe pour traiter du legacy. Celle qui ira le plus loin, qui en fera le plus.\n  (Un troisiÃ¨me axe, plus convivial, est choisi pour la fin de journÃ©e : partager un verre tous ensemble.)\n\n\n\nLe jeu de dÃ©couverte du sujet de la 3Ã¨me Ã©dition : Ansible\n\n\nLors de la 1Ã¨re Ã©dition, en octobre 2022, nous avons proposÃ© aux Ã©quipes un grand thÃ¨me : le repository â€œSysAdmin/Terraformâ€, centre nÃ©vralgique du travail de lâ€™Infra. Il y a beaucoup Ã  faire : les fameux upgrades de layers, du refactoring de code pour industrialiser nos process, des PRs ouvertes et restÃ©es en suspens depuis de nombreux moisâ€¦ chacun peut y trouver son compte. Chacune des six Ã©quipes composÃ©es ce jour-lÃ  disposait de dix minutes pour dÃ©finir Ã  quel chantier elle sâ€™attaquerait durant la journÃ©e. A lâ€™issue de ces dix minutes, le reprÃ©sentant de lâ€™Ã©quipe devait prÃ©senter aux autres le sujet choisi et lâ€™indicateur qui permettrait de juger si le travail a Ã©tÃ© accompli ou non, en fin de journÃ©e. Lâ€™Ã©quipe ayant proposÃ© le sujet le plus ambitieux sâ€™est vue attribuer des points bonus, rentrant en compte pour le calcul du score final.\n\nPour la seconde Ã©dition le mois suivant, le sujet Ã©tait imposÃ© : toutes les Ã©quipes avaient pour objectif de mieux sÃ©curiser les secrets contenus dans le code Bedrock. Lâ€™Ã©quipe qui en traiterait le plus grand nombre lâ€™emporterait.\n\nLors de la derniÃ¨re Ã©dition, en fÃ©vrier dernier, la compÃ©tition reposait Ã©galement sur le nombre de points gagnÃ©s par chaque Ã©quipe en fin de journÃ©e. Nous avons attribuÃ© un nombre de points Ã  chaque tÃ¢che pouvant Ãªtre traitÃ©e dans la journÃ©e, en fonction de sa complexitÃ© et/ou de sa prioritÃ©. Chaque Ã©quipe pouvait sâ€™organiser librement : choisir plusieurs petites tÃ¢ches ou deux plus importantesâ€¦\n\nBien sÃ»r, pour que la compÃ©tition soit totale, chaque Ã©dition du jeudi du fun se termine par une remise de prix : distribution de goodies, de cartes â€œbonusâ€ ou â€œmalusâ€ valables dans nos â€œvraisâ€ sprints, de gourmandisesâ€¦ il faut que la rÃ©compense soit rÃ©elle pour que les participants se prennent au jeu.\n\n\nExemple de lot pouvant Ãªtre remportÃ© lors du â€œJeudi du funâ€\n\n\nLe risque avec la compÃ©tition, câ€™est de se laisser dÃ©border : gagner coÃ»te que coÃ»te, ajouter des points Ã  son compteur en faisant du â€œquick &amp;amp; dirtyâ€. Jusquâ€™Ã  prÃ©sent, la compÃ©tition dans la verticale Infra est restÃ©e bon enfant : les Ã©quipes se dÃ©fient entre elles tout au long de la journÃ©e, des points â€œbonusâ€ sont rÃ©clamÃ©s aux organisatrices au moindre prÃ©texteâ€¦ mais personne ne perd de vue lâ€™objectif principal : venir Ã  bout du sujet.\n\nLes Jeudis du Fun reposent donc sur le challenge et le jeu. Mais nous avions sous-estimÃ© un autre axe nous permettant de faire de ces journÃ©es un succÃ¨s : lâ€™entraide. A chaque Ã©dition, les retours les plus enthousiastes portent sur le fait de passer une journÃ©e Ã  travailler en cross-team. SysAdmins et DevOps apprennent les uns des autres, les juniors ont lâ€™occasion de former des leadsâ€¦ et chacun Ã©largit son spectre de compÃ©tences. Au-delÃ  du fait de venir Ã  bout de sujets legacy, lâ€™Ã©mulation engendrÃ©e par ces journÃ©es justifie Ã  elle-seule leur organisation.\n\nEt puis, quitte Ã  faire des jeudis du fun des journÃ©es particuliÃ¨res, autant y aller franchement : certains membres de nos Ã©quipes nâ€™hÃ©sitent pas Ã  venir dÃ©guisÃ©s pour ajouter une dose de fun. Vous avez croisÃ© une licorne, Pikachu ou un plombier dans lâ€™open space de Bedrock ? Aucun doute, câ€™Ã©tait un jeudi ! Un dress code a mÃªme Ã©tÃ© dÃ©fini lors de lâ€™Ã©dition de fÃ©vrier 2023.\n\nItÃ©rer, et corriger nos erreurs Ã  chaque Ã©dition\n\nTrois Ã©ditions du â€œjeudi du funâ€ ont Ã©tÃ© organisÃ©es jusquâ€™Ã  prÃ©sent. Ã€ la fin de chaque Ã©dition, les organisatrices recueillent le feed-back des participantes et participants, afin de corriger ce qui doit lâ€™Ãªtre et de capitaliser sur ce qui a marchÃ©. Voici le premier bilan que nous pouvons en tirer.\n\nDe lâ€™importance du choix du sujet\n\nLe succÃ¨s de la journÃ©e repose sur le choix du sujet. En choisissant un sujet fÃ©dÃ©rateur, comme lors de notre premiÃ¨re Ã©dition, et en laissant le soin Ã  chaque Ã©quipe de dÃ©finir quel chantier elle souhaitait mener, nous partions gagnantes. Le repo Sysadmin/Terraform sur lequel nous avons travaillÃ© lors de cette journÃ©e est un point de douleur pour lâ€™ensemble de nos Ã©quipes : chacun des participants a compris lâ€™intÃ©rÃªt de jouer le jeu et de retrousser ses manches. Les Ã©quipes ont mÃªme eu du mal Ã  clÃ´turer la journÃ©e, car elles voulaient finir ce quâ€™elles avaient commencÃ©.\n\n\nAu cours de la 1Ã¨re journÃ©e du â€œJeudi du funâ€\n\n\nLors de la seconde Ã©dition en revanche, le sujet de cette Ã©dition a mis la journÃ©e en pÃ©ril. Nous avions demandÃ© aux Ã©quipes dâ€™ajouter un niveau de sÃ©curitÃ© Ã  lâ€™ensemble des secrets contenus dans la codebase de Bedrock. Cela a suscitÃ© quelques difficultÃ©s :\n\n  Tout dâ€™abord, il sâ€™agissait de trouver une mÃ©thode pour identifier tous les secrets concernÃ©s. Toutes les Ã©quipes du jeudi du fun ont alors planchÃ© sur ce sujet, en utilisant des mÃ©thodes et outils diffÃ©rents. Au final, nous ne sommes parvenus que tardivement (2h aprÃ¨s le lancement de la journÃ©e) Ã  nous mettre dâ€™accord sur une mÃ©thodologie. Autant de temps perdu que nous aurions pu consacrer au cÅ“ur du sujet, la sÃ©curisation des secrets.\n  En nous attaquant Ã  lâ€™ensemble des secrets de Bedrock, nous touchions forcÃ©ment Ã  des repositories projets dont nous ne sommes pas les code owners. Ce nâ€™est pas une vÃ©ritable difficultÃ© en soi, puisquâ€™au quotidien, nous intervenons frÃ©quemment dans ces repos projets pour accompagner les Ã©quipes devs. En revanche, lâ€™ajout dâ€™un niveau de sÃ©curitÃ© supplÃ©mentaire sur des secrets implique de pouvoir tester, puis de merger nos modifications. Impossible de rÃ©aliser ces actions sans les Ã©quipes back et front responsables des projets, ou sans impacter leur travail. Notre pÃ©rimÃ¨tre dâ€™intervention lors de cette journÃ©e Ã  Ã©tÃ© considÃ©rablement limitÃ©.\n\n\nLa complexitÃ© du sujet et le constat de notre incapacitÃ© Ã  avancer lors de cette journÃ©e ont rapidement conduit Ã  un dÃ©couragement des troupes. Nous sommes tout de mÃªme ressortis de cette Ã©dition avec des points positifs :\n\n  Une meilleure visibilitÃ© sur le pÃ©rimÃ¨tre de sÃ©curisation Ã  couvrir, en dÃ©finissant le nombre de secrets concernÃ©s,\n  Un workflow visant Ã  dÃ©tecter Ã  lâ€™avenir tout nouveau secret concernÃ©\n  â€¦ et la nÃ©cessitÃ© de mieux dÃ©finir les guidelines pour le choix du sujet !\n\n\nEntendu pendant la 2nde Ã©dition du jeudi du fun ğŸ˜…\n\n\n  ğŸ‘§ğŸ» : â€œAlors, quâ€™est-ce que tu fais de beau ?â€\n\n  ğŸ‘¦ : â€œJe souffreâ€\n\n\nCes guidelines nous ont aidÃ© Ã  dÃ©finir le choix de la thÃ©matique de la 3Ã¨me Ã©dition du jeudi du fun. Le sujet devait rÃ©pondre Ã  ces critÃ¨res :\n\n  ÃŠtre rÃ©alisable en une journÃ©e,\n  Permettre de terminer / accÃ©lÃ©rer un projet ou dâ€™Ã©radiquer du legacy,\n  ÃŠtre dans le pÃ©rimÃ¨tre dont lâ€™infra est le code owner,\n  Et Ãªtre â€œmorcelableâ€ en sous-pÃ©rimÃ¨tres, un pour chaque Ã©quipe.\n  Enfin, lâ€™avancÃ©e du sujet doit Ãªtre mesurable.\n\n\nPour lâ€™Ã©dition de fÃ©vrier 2023, nous avons donc â€œjouÃ©â€ avec la migration Ansible en cours de rÃ©alisation dans lâ€™une de nos Ã©quipes de SysAdmins. 45 rÃ´les Ansible restaient Ã  migrer vers notre nouveau template Ansible, utilisÃ© pour dÃ©ployer nos machines on-prem : il y a du travail pour tout le monde, câ€™est parti !\n\nEt finalement, est-ce que Ã§a marche ?\n\nAprÃ¨s trois Ã©ditions, il nous semble nÃ©cessaire de prendre un peu de recul pour analyser si ces journÃ©es portent leur fruit. Les Ã©quipes sont ravies de travailler ensemble, certes, mais lâ€™objectif principal est-il rempli ? Les jeudis du fun permettent-ils de venir Ã  bout de sujets legacy ?\n\nLa premiÃ¨re Ã©dition a fortement contribuÃ© Ã  Ã©radiquer du legacy : nous avons mis Ã  jour la quasi-totalitÃ© des layers Terraform, nous avons mergÃ© ou fermÃ© lâ€™entiÃ¨retÃ© des PRs, et nous avons initiÃ© des travaux de rework. Cependant, nous nâ€™avions pas dÃ©fini dâ€™indicateurs de rÃ©ussite assez fiables lors de cette premiÃ¨re itÃ©ration pour quantifier rÃ©ellement le travail accompli. Si toute la Verticale partage le sentiment dâ€™avoir avancÃ© lors de cette journÃ©e, nous ne savons pas le mesurer finement.\n\n\nCapture dâ€™Ã©cran du repo sysadmin/terraform au cours de la 1Ã¨re Ã©dition du â€œJeudi du funâ€\n\n\nPour pallier cette difficultÃ©, nous avions dÃ©fini un indicateur de suivi trÃ¨s simple pour la seconde Ã©dition du jeudi du fun : nombre de secrets Ã  traiter / nombre de secrets traitÃ©s. Ainsi, nous savons que, lors de cette (difficile) journÃ©e, nous avons traitÃ© environ un quart du pÃ©rimÃ¨tre.\n\nAu lancement de la 3Ã¨me Ã©dition du jeudi du fun, nous avions 45 rÃ´les Ã  migrer vers notre nouveau template Ansible. Ã€ lâ€™issue de cette journÃ©e, lâ€™Ã©quipe responsable du sujet nâ€™en avait plus que 10 Ã  traiter. La mutualisation de nos forces a portÃ© ses fruits !\n\nInsuffisants lors de la premiÃ¨re Ã©dition, les indicateurs de suivi mis en place dans les Ã©ditions suivantes sont cruciaux pour Ã©valuer le ROI de ces journÃ©es de travail â€œparticuliÃ¨resâ€.\n\nLes coulisses du jeudi du fun\n\nLes jeudis du fun sont organisÃ©s par deux personnes au sein de la verticale infra. Si les sÃ©ances de prÃ©paration de cette journÃ©e (qui dÃ©butent environ 3 semaines avant la tenue de lâ€™Ã©vÃ©nement) sont source de beaucoup de rires, il nâ€™empÃªche quâ€™elles doivent Ã©galement rÃ©pondre Ã  certaines problÃ©matiques.\n\nSâ€™adapter aux habitudes de travail de chacun\n\nEn premier lieu, nous devons organiser une journÃ©e Ã  laquelle tous les membres de nos Ã©quipes puissent prendre part, quâ€™ils soient au bureau ou en tÃ©lÃ©travail. Tous les moments de la journÃ©e doivent tenir compte de cet Ã©lÃ©ment, quâ€™il sâ€™agisse des phases de travail en petits groupes, des sessions en plÃ©niÃ¨re (25 personnes) comme le lancement de la journÃ©e, la remise des prix ou les diffÃ©rents jeux qui ponctuent ces jeudis.\n\nLes phases de travail en Ã©quipe sont les plus simples Ã  gÃ©rer : nos Ã©quipes ont dÃ©jÃ  lâ€™habitude au quotidien de travailler avec des collÃ¨gues Ã  distance. Tout le monde se connecte sur une room de visioconfÃ©rence, et le tour est jouÃ©.\n\n\nTeam mixte prÃ©sentiel / distanciel lors du 1er â€œjeudi du funâ€\n\n\nLes moments en plÃ©niÃ¨re sont en revanche plus dÃ©licats Ã  gÃ©rer, car le brouhaha dâ€™une vingtaine de personnes rassemblÃ©es dans une mÃªme piÃ¨ce reste difficilement audible pour les personnes Ã  distance. Un prochain challenge pourrait Ãªtre dâ€™organiser un jeudi du fun 100% distanciel.\n\nIl est Ã©galement nÃ©cessaire de tenir compte de la faÃ§on de travailler de chacun : si certaines personnes sont capables de travailler en faisant fi du bruit dâ€™un open space, dâ€™autres ont besoin de plus de calme. Ã€ chaque Ã©dition, nous tentons dâ€™organiser le jeudi du fun sous diffÃ©rentes formes, pour tenir compte des besoins de chacun, mais nous nâ€™avons pas encore trouvÃ© la solution idÃ©ale.\n\nLors de la premiÃ¨re Ã©dition, nous Ã©tions tous rassemblÃ©s dans le mÃªme open space, sans dispositif particulier pour les personnes ayant besoin dâ€™un environnement silencieux, et cette journÃ©e leur a Ã©tÃ© difficile Ã  supporter. De nombreuses autres Ã©quipes de Bedrock avec qui nous partageons dâ€™habitude cet open space Ã©taient en dÃ©placement ce jour-lÃ , ce qui a nÃ©anmoins permis de limiter nos nuisances sonores Ã  notre seule verticale.\n\nPour la seconde Ã©dition, nous avions rÃ©servÃ© un open space dans les locaux de Bedrock pour ne pas prendre le risque de dÃ©ranger les autres Ã©quipes : lâ€™ambiance y a Ã©tÃ© dâ€™autant plus conviviale mais nâ€™a apportÃ© aucun mieux aux personnes ayant besoin de tranquillitÃ© pour travailler.\n\nLors de notre derniÃ¨re Ã©dition, nous avons tentÃ© une approche hybride : la plupart des Ã©quipes Ã©taient rassemblÃ©es dans un mÃªme open space, et pour les personnes ayant besoin de sâ€™isoler, une salle de rÃ©union avait Ã©tÃ© rÃ©servÃ©e pour lâ€™occasion. Il semble que cette organisation a apportÃ© un mieux pour les personnes souffrant du bruit avec un Ã©cueil cependant : elles Ã©taient isolÃ©es des autres Ã©quipes tout au long de la journÃ©e, et le jeudi du fun repose (aussi) sur lâ€™Ã©mulation collectiveâ€¦\n\nLes autres limites de lâ€™organisation\n\nAu fil des Ã©ditions, nous avons rencontrÃ©, en tant quâ€™organisatrices, deux autres limites.\n\nLa premiÃ¨re touche au choix du sujet. Si la dÃ©finition de la thÃ©matique de la premiÃ¨re journÃ©e a Ã©tÃ© Ã©vidente car le repository sysadmin/terraform est source de complaintes quotidiennes, trÃ¨s vite, nous avons eu besoin dâ€™aide pour dÃ©finir les sujets des Ã©ditions suivantes.  \nEn effet, il est difficile pour nous dâ€™apprÃ©hender un sujet dans sa globalitÃ© : y aura-tâ€™il du travail pour chaque Ã©quipe ? Le sujet est-il accessible pour tous nos profils, sans montÃ©e en compÃ©tence prÃ©alable ? Quelles sont concrÃ¨tement les actions Ã  conduire pour venir Ã  bout dâ€™un sujet ? Pour pallier Ã  ce problÃ¨me, nous avons rÃ©alisÃ© un tour de passe-passe : lâ€™Ã©quipe qui remporte le jeudi du fun gagne le droit de dÃ©finir avec nous le sujet de lâ€™Ã©dition suivante. Et Ã§a fonctionne ! Les gagnants participent avec plaisir au choix du prochain sujet de torture de fun !\n\nLa seconde limite concerne la rÃ©currence de lâ€™Ã©vÃ©nement. Initialement, nous avions prÃ©vu dâ€™organiser un jeudi du fun par mois, pour venir Ã  bout rapidement de nos sujets legacy. AprÃ¨s les deux premiÃ¨res Ã©ditions (organisÃ©es en octobre et novembre 2022), nous nous sommes aperÃ§ues que nous perdrions le fun de cette journÃ©e si elle revenait trop frÃ©quemment. Pour que cet Ã©vÃ©nement reste une journÃ©e de travail particuliÃ¨re Ã  laquelle les personnes participent avec plaisir, nous avons fait le choix dâ€™opter pour un format trimestriel.\n\nNext steps et prochains dÃ©fis\n\nDâ€™autres amÃ©liorations restent Ã  apporter, notamment autour de la gestion du reste Ã  faire. Comment finir correctement les travaux initiÃ©s dans cette journÃ©e, afin de ne pas crÃ©er de nouvelles user stories legacy ? Ce point est tout aussi important que celui sur le travail accompli au cours de ces journÃ©es. Entamer un rework et le laisser en chantier gÃ©nÃ¨re au moins autant de frustration que le manque de temps pour traiter du legacy.\n\nNÃ©anmoins, aprÃ¨s trois Ã©ditions du jeudi du fun, il nous semblait important de partager notre expÃ©rience, ne serait-ce que pour convaincre des Ã©quipes de devs de Bedrock de venir jouer avec nous lors dâ€™une prochaine Ã©dition !\n\n\nLes participants du Jeudi du fun\n\n\n\n\nPour vous donner un aperÃ§u de comment se dÃ©roulent ces fameux jeudis, voici grosso modo le programme dâ€™une journÃ©e :\n\n\n  \n    â° 9h00 Petit dÃ©jeuner convivial (car câ€™est trÃ¨s important de commencer une telle journÃ©e en prenant des forces)\n  \n  \n    â° 9h30 DÃ©but officiel de la journÃ©e : on se retrouve en plÃ©niÃ¨re, dans une grande salle de rÃ©union, avec tous les participants et on (rÃ©)explique le contexte de la journÃ©e ainsi que le programme. \nOn commence avec un petit jeu (5 minutes maximum) qui sert Ã  deviner le sujet du jour. Les sujets sont toujours gardÃ©s secrets jusquâ€™au lancement de la journÃ©e, ce qui donne lieu Ã  toutes sortes dâ€™hypothÃ¨ses les jours qui prÃ©cÃ¨dent (â€œOui, oui, bien sÃ»r on va recoder toute notre plateforme dans un autre langage jeudiâ€).On fait monter la pression !  \nLâ€™objectif de ce premier jeu est dâ€™Ã©nergiser un maximum nos collÃ¨gues et de leur permettre de commencer Ã  se projeter sur ce quâ€™ils vont pouvoir y faire. Le jeu change Ã  chaque fois, pour garder un effet de surprise. \nEnsuite, vient le temps de rÃ©vÃ©ler la constitution des Ã©quipes qui changent elles aussi Ã  chaque Ã©dition afin de permettre Ã  chaque personne de cÃ´toyer de nouveaux collÃ¨gues.\n  \n  \n    â° 10h00 Les Ã©quipes partent travailler sur le sujet du jour, Ã  leurs postes de travail\n  \n  \n    â° 12h30 - 13h30 DÃ©jeuner\n  \n\n\n\n\n\n  \n    â° 13h30 Jeu de reprise (facultatif) : on se retrouve autour dâ€™un blind test ou un gartic phone, histoire de passer un bon moment et de se remettre en jambe pour lâ€™aprÃ¨s-midi. Câ€™est un court moment de team building qui est trÃ¨s apprÃ©ciÃ© la plupart du temps (sauf lorsque les Ã©quipes ne veulent pas perdre un minute pour venir Ã  bout de leur objectif !)\n  \n  \n    â° 14h00 Les Ã©quipes reprennent le travail initiÃ© le matin et essayent de finir un maximum de choses\n  \n  \n    â° 17h30 On se retrouve en plÃ©niÃ¨re pour le dÃ©brief de la journÃ©e : on fait le point sur le travail accompli, le dÃ©compte des points gagnÃ©s par chaque Ã©quipe et on fait le fameux podium ainsi que la remise des prix. \nOn rÃ©cupÃ¨re Ã  chaud les premiers retours des participants.\n  \n  \n    â° 18h00 Le verre de lâ€™amitiÃ©\n  \n\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #19",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2023/03/13/bedrock-dev-facts-19.html",
    "date"     : "March 13, 2023",
    "excerpt"  : "La fin de lâ€™hiver approche, il est temps de faire un bilan ! Quelles bÃªtises le froid aura-t-il apportÃ©es parmi les devs ? â„ï¸\n\nLa confiance ğŸ¤\n\n\n\nMieux quâ€™un readme\n\n\n  Quand le mec tâ€™explique la solution, et finit par :\n\n  â€œEnfin Ã§a câ€™est si mon c...",
  "content"  : "La fin de lâ€™hiver approche, il est temps de faire un bilan ! Quelles bÃªtises le froid aura-t-il apportÃ©es parmi les devs ? â„ï¸\n\nLa confiance ğŸ¤\n\n\n\nMieux quâ€™un readme\n\n\n  Quand le mec tâ€™explique la solution, et finit par :\n\n  â€œEnfin Ã§a câ€™est si mon code a bien continuÃ© dâ€™Ãªtre copiÃ© collÃ© partoutâ€\n\n\nUne Ã©thique, moi ?\n\n\n  Moi je peux mettre du code dÃ©gueulasse un peu partout, câ€™est pas un problÃ¨me !\n\n\nLe Socrate des temps modernes\n\n\n  La vie est un Spike\n\n\nOn apprend de ses erreurs\n\n\n  Set up a reminder â€œ@myself ne jamais dire â€˜je finis aujourdâ€™huiâ€™â€ in this channel at 9h45AM every day.\n\n\nEn tout bien tout honneur â¤ï¸\n\n\n  Ah bah go, mets-moi en dur si tu veux.\n\n\nGIT 101\n\n\n  Jâ€™en ai connu certains, Ã  chaque fois quâ€™ils avaient un conflit sur leur branche, ils supprimaient le repo avant de le re-cloner\n\n\nTen seconds before disaster\n\n\n  Le cache, câ€™est nul !\n\n\nshoeHasHole: boolean ğŸ‘Ÿ\n\n\n  A : â€˜tin jâ€™ai un trou dans ma chaussure\n\n  B : Tu es sÃ»r que câ€™est pas un false ?\n\n\nOn a tous un env de test. Certains ont aussi un env de prod.\n\n\n\nPeur de rien, sauf dâ€™une choseâ€¦\n\n\n  A : Ã‡a finit en devs qui se reconvertissent Boulanger Ã§a.\n\n  B : Certes, mais lâ€™inverse est vrai aussi, il arrive que des Boulangers se reconvertissent aprÃ¨s Ãªtre devenus allergiques Ã  la Farine.\n\n  A : Câ€™est pour Ã§a que je ne me reconvertirai pas en Barmanâ€¦ trop peurâ€¦\n\n\nLa confiance, second Ã©pisode ğŸ¤\n\n\n  Câ€™est pas nâ€™importe quoi, juste un peu yolo !\n\n\nComme de lâ€™eau de roche trouble !\n\n\n  Franchement, je trouve Ã§a clair ! Mais je comprends pas..\n\n\nDe rares gÃ©nies ğŸ’¡\n\n\n  On est peut-Ãªtre des lumiÃ¨res, mais Ã§a ne veut pas dire quâ€™on est tous allumÃ©s !\n\n\nCassage de prod dans 3â€¦ 2â€¦ 1â€¦\n\n\n  Jâ€™le sens bien lÃ . Jâ€™le sens bien bien bien.\n\n\nItâ€™s not a bug, itâ€™s a feature\n\n\n  Jâ€™ai vÃ©rifiÃ©, le bug marchait bien.\n\n\nToujours lire les petites lignes ğŸ”\n\n\n  Tout* marche du coup !\n\n  (*pour lâ€™instant)\n\n\nMiaou ğŸ±ğŸ“ˆ\n\n\n  A : Je thÃ©orise que le chat ne miaule devant la porte que pour savoir sâ€™il pourrait passer quand il aura envie.\n\n  B : Ouah ton chat il fait du monitoring de la porte !\n\n\nFacile comme tout !\n\n\n  TKT ! tu mets ton JSON dans le yaml et Ã§a ira !\n\n\nUne grande histoire dâ€™amour, Ã©pisode 1\n\n\n  Moi, jâ€™adore le JSON\n\n\nUne grande histoire dâ€™amour, Ã©pisode 2\n\n\n  Le mail et le DNS câ€™est ma grande passion\n\n\nPartir comme un roi ğŸ‘‘\n\n\n\nLes grandes questions de la vie ğŸ¥\n\n\n  Dâ€™ailleurs câ€™est LinkedIn ou pain au linked ?\n\n\nLâ€™humour pour les nuls\n\n\n  A : Pouffe de rire\n\n  B : Tout va bien ?\n\n  A : DÃ©solÃ©, je viens de relire ma vanne\n\n\nLes progrÃ¨s de lâ€™IA ğŸ¤–\n\n\n  A : Alors B, câ€™est quoi le format de date php de la constante de format â€˜câ€™ ?\n\n  B : Tu mâ€™as pris pour chatGPT ou quoi ?\n\n\nUn stagiaire en dÃ©tresse\n\n\n  TLDR: Ã€ lâ€™aide svp\n\n\nLa sÃ©curitÃ© pour les nuls\n\n\n  Brian is in the Keychain.\n\n\nPromis dans le contexte câ€™est vrai\n\n\n  On doit afficher des ronds, alors câ€™est mieux sâ€™ils nous envoient des carrÃ©s.\n\n\nğŸ˜³\n\n\n  (Au pire, si on a la main sur une regexp, câ€™est dÃ©jÃ  plus quâ€™il nâ€™en faut pour me faire rÃªver)\n\n\nLa sÃ©lection naturelle\n\n\n  Je suis dâ€™accord que lÃ  il y a un bug, mais câ€™est un bug parce que je suis con !\n\n\nğŸµ\n\n\n  Je suis en train de me rappeler de mon weekend, et spoiler mettre du rhum dans son thÃ© ce nâ€™est pas une bonne idÃ©e.\n\n\nError : Task completed successfully\n\n\n\nLa confiance, 3.0 ğŸ¤\n\n\n  Coucou, aujourdâ€™hui, je pÃ¨te la reco (en prod), mais câ€™est sous contrÃ´le.\n\n\nUn instant de rÃ©alisme\n\n\n  Personnellement, je sais pas ce que je fous en dÃ©veloppeur !\n\n\nThomas the train ğŸš†\n\n\n\nTurlututu chapeau pointu !\n\n\n  On aurait dÃ» dire câ€™est â€œchapeau perchÃ©â€.\n\n\nMieux quâ€™un rappel automatique ğŸ¤¯\n\n\n  A : Du coup, tu as envoyÃ© un mail ?\n\n  B : Pas encore non ! Jâ€™attendais dâ€™y penser !\n\n\n"
} ,
  
  {
    "title"    : "Why is Transit Gateway service not right for us?",
    "category" : "",
    "tags"     : " on-premise, cloud, aws, network",
    "url"      : "/2023/03/02/aws_transit_gateway.html",
    "date"     : "March 2, 2023",
    "excerpt"  : "Managing the network of many interconnected AWS accounts can quickly lead to having a messy network architecture.\nTransit Gateway (TGW) service seems to be the way out of this. So how do you know if TGW is right for you?\n\nThis blog post will intro...",
  "content"  : "Managing the network of many interconnected AWS accounts can quickly lead to having a messy network architecture.\nTransit Gateway (TGW) service seems to be the way out of this. So how do you know if TGW is right for you?\n\nThis blog post will introduce how the service works and explain why we chose not to carry on with our migration to AWS Transit Gateway.\n\nTransit Gatewayâ€™s backstory\n\nTransit Gateway is a network transit hub that connects multiple VPCs and On-Premises sites to allows control traffic between them.\nIt was created to provide a new approach of network implementation on AWS and to make network administration smoother.\n\nVPC peering is a point-to-point connection between 2 VPCs.\nIt is a great example of complex network management because it adds a new topology to the network architecture.\nOn this diagram you can see an example of VPC peering usage. Itâ€™s not that messy yet but at scale it will be.\n\n\n\nBy acting as a â€œcloud routerâ€, TGW centralizes network connections and takes control of packet forwarding between VPCs.\n\n\n\nVPC peerings are not required anymore, we go back to a simpler star network topology thanks to Transit Gateway which really does address the complexity and restrictions of VPC peerings.\nAt that point, TGW seems to be the perfect answer for a simpler network architecture.\n\nWhat about TGW at Bedrock?\n\nWe operate more than 20 different AWS accounts for our customersâ€™ platforms. Each account has a VPC with at least 3 private and 3 public subnets. We also manage AWS accounts for internal tools like ECR repositories, monitoring tools and shared s3 buckets. We configured Site-to-Site VPNs from On-Premises infrastructure to all the VPCs in these accounts.\n\nFrom the creation of new AWS accounts to deploying the tenantsâ€™ platform, onboarding a new customer requires a lot of work and time.\n\nConfiguring VPCs Site-to-Site VPN is one of the steps that requires a lot of work. This is why we were interested in Transit Gateway at first.\n\nProof of concept\n\nWe created a production like Proof of Concept infrastructure using three AWS accounts, two different regions, multiple VPCs and a single Site-to-Site VPN from TGW to On-Premises firewall.\n\nHow did we test TGW?\n\nWe started by trying to split routing domains.\nCentralizing network connections also means (with correct ACLs or Security Groups) that VPCs can reach all other VPCs. We want to control that.\nTransit Gateway attachments read their routes in the TGW route table they are associated to. This is how we manage routing domains.\nWe create a Transit Gateway routing table and create routes for target networks.\nTGW attachments are able to propagate routes in a route table if we want to. But because of routing domains, we canâ€™t use that option and we have to add routes manually (attachments only read routes in the route table).\n\nThen we tested Transit Gateway peering.\nTGW is a regional service, this means that we need to have a TGW for each active AWS region. We use TGW peering to interconnect them.\nWe expected to have some way to propagate routes dynamically in the Transit Gateway peering route table. But it is not possible.\n\nThe last thing we tested is migrating from VPC Site-to-Site VPN to TGW VPN.\nBecause of the amount of VPC Site-to-Site VPN we have, it was important for us to know if we could get a minimal down time on On-Premises to VPC connections when migrating to the Transit Gateway VPN.\nThis process requires a lot of time because routes have to be deleted and created manually on each side.\n\nEven if we noticed some pain points, tests went well. So we decided to initiate the migration to the Transit Gateway service.\n\nWhy did we choose to rollback?\n\nEverything was okay at first, we successfully migrated two VPC Site-to-Site VPN to our Transit Gateway VPN.\n\nBut then previous pain points became barriers:\n\n  creating and managing routing domains is possible, but makes it impossible to use dynamic route propagation\n  there is not option to propagate routes in VPC route table, they all have to be created manually\n  data transfer cost is too high (and multiplied by the number of region on which you deployed TGW if your packets go through all these regions)\n  migrating to Transit Gateway requires a planned maintenance because there is a network downtime\n\n\nWe took some time to talk about what to do next and concluded that migrating to Transit Gateway will just move the complexity of configuring VPC Site-to-Site VPNs to configuring TGW attachments and routes.\n\nAWS support did not suggest enough solutions to the problems we faced, so we decided to rollback to VPC Site-to-Site VPNs.\n"
} ,
  
  {
    "title"    : "Projet XState",
    "category" : "",
    "tags"     : " xstate, lyonjs, meetup, react, javascript, conference",
    "url"      : "/2023/02/08/projet-xstate.html",
    "date"     : "February 8, 2023",
    "excerpt"  : "Dans une application frontend moderne, la gestion dâ€™Ã©tat est un Ã©lÃ©ment central de son bon fonctionnement. MalgrÃ© les nombreuses librairies disponibles (Redux, MobX, Recoilâ€¦), cette tache reste complexe Ã  rÃ©aliser et il est facile de perdre le con...",
  "content"  : "Dans une application frontend moderne, la gestion dâ€™Ã©tat est un Ã©lÃ©ment central de son bon fonctionnement. MalgrÃ© les nombreuses librairies disponibles (Redux, MobX, Recoilâ€¦), cette tache reste complexe Ã  rÃ©aliser et il est facile de perdre le contrÃ´le.\n\nDans lâ€™objectif de rester maitre de son application, je vous propose de dÃ©couvrir XState, une librairie reposant sur le concept de machine Ã  Ã©tats. Si lâ€™outil ne fait pas tout, le concept de machine Ã  Ã©tat aide grandement Ã  concevoir une application rÃ©siliente.\n\nPour prÃ©senter au mieux les concepts, la thÃ©orie sera suivie de pratique au travers dâ€™un live coding.\n"
} ,
  
  {
    "title"    : "The time I tried to build a Second Brain #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/the-time-i-tried-to-build-a-second-brain",
    "date"     : "January 27, 2023",
    "excerpt"  : "The time I tried to build a Second Brain\nPrÃ©sentÃ© par Sylvain ZOCCARATO.\n",
  "content"  : "The time I tried to build a Second Brain\nPrÃ©sentÃ© par Sylvain ZOCCARATO.\n"
} ,
  
  {
    "title"    : "Projet XState #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/projet-xstate",
    "date"     : "January 27, 2023",
    "excerpt"  : "Projet XState.\nPrÃ©sentÃ© par Maxime BLANC.\n",
  "content"  : "Projet XState.\nPrÃ©sentÃ© par Maxime BLANC.\n"
} ,
  
  {
    "title"    : "Quâ€™est-ce que lâ€™oignon dans le Web ? #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/loignon-dans-le-web",
    "date"     : "January 27, 2023",
    "excerpt"  : "Quâ€™est-ce que lâ€™oignon dans le Web ?\nPrÃ©sentÃ© par Etienne Doyon.\n",
  "content"  : "Quâ€™est-ce que lâ€™oignon dans le Web ?\nPrÃ©sentÃ© par Etienne Doyon.\n"
} ,
  
  {
    "title"    : "La philo et les livres : mes compagnons de route pour les dÃ©fis sportifs #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-philo-et-les-livres",
    "date"     : "January 27, 2023",
    "excerpt"  : "La philo et les livres : mes compagnons de route pour les dÃ©fis sportifs\nPrÃ©sentÃ© par Chiara PETTINELLI.\n",
  "content"  : "La philo et les livres : mes compagnons de route pour les dÃ©fis sportifs\nPrÃ©sentÃ© par Chiara PETTINELLI.\n"
} ,
  
  {
    "title"    : "La culture Hip-Hop #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-culture-hip-hop",
    "date"     : "January 27, 2023",
    "excerpt"  : "La culture Hip-Hop\nPrÃ©sentÃ© par Pascal HALTER.\n",
  "content"  : "La culture Hip-Hop\nPrÃ©sentÃ© par Pascal HALTER.\n"
} ,
  
  {
    "title"    : "La photographie de paysages nocturnes #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/photographie-de-paysages-nocturnes",
    "date"     : "January 27, 2023",
    "excerpt"  : "La photographie de paysages nocturnes\nPrÃ©sentÃ© par Camille NIEL.\n",
  "content"  : "La photographie de paysages nocturnes\nPrÃ©sentÃ© par Camille NIEL.\n"
} ,
  
  {
    "title"    : "Le festival de cannes de sa naissance Ã  aujourdâ€™hui #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/festival-de-cannes-de-sa-naissance-a-aujourdhui",
    "date"     : "January 27, 2023",
    "excerpt"  : "Le festival de cannes de sa naissance Ã  aujourdâ€™hui\nPrÃ©sentÃ© par Maxime LE MOAL.\n",
  "content"  : "Le festival de cannes de sa naissance Ã  aujourdâ€™hui\nPrÃ©sentÃ© par Maxime LE MOAL.\n"
} ,
  
  {
    "title"    : "A journey into connected TVs industrialisation process, Part 1",
    "category" : "",
    "tags"     : " ",
    "url"      : "/2023/01/10/bedrock-app-launcher.html",
    "date"     : "January 10, 2023",
    "excerpt"  : "At Bedrock, we build and run streaming applications on a wide variety of OTT devices (more than 60 different ecosystems). While testing and experimenting is easy on web and mobile devices, even for non-developers, itâ€™s not as easy for Connected TV...",
  "content"  : "At Bedrock, we build and run streaming applications on a wide variety of OTT devices (more than 60 different ecosystems). While testing and experimenting is easy on web and mobile devices, even for non-developers, itâ€™s not as easy for Connected TV (CTV). In this article, youâ€™ll discover how all of our employees can now access testing and pre-release environments on TV devices, with ease and without any technical knowledge.\n\nBedrock TvJS Project\n\nHow does it work ?\n\nTo address the growing number of CTVs vendors in the market, we have a one-and-only monorepo project named â€œTVJSâ€. It is a React application which we can deploy almost everywhere almost anywhere with the same code, UI and UX. The magic part? There isnâ€™t much manufacturer-specific code in that application, most of those particularities are handled by our homemade JS library named PELO (Platform Easy Life Officer). For non-French readers, â€œpÃ©loâ€ is a Lyon/Grenoble city slang to designate â€œsomeoneâ€.\n\n\n\nIn a few words, PELO is a set of libraries showing a unified front API for TV developers, so they donâ€™t have to keep in mind every TV specific details and custom APIs (like lifecycle, keyboard, storage handling, and more). PELO also provides several CLI tools allowing the use of proprietary manufacturer SDKs, with a common shared API.\n\nThere are at least two ways to deploy a TV application:\n\n  A fully packaged solution, where all application files and resources are stored on the TV. Everytime you want to update it, application you have to go through the manufacturer QA process. Doing so, you can develop either a web application that will run through the TVâ€™s Web Engine, or a native TV application.\n  The hosted solution, where the TV packaged application only redirects to a web application that you are responsible for. It grants much more flexibility, and delivery speed, as deployment and propagation of a new version are almost instantaneous.\n\n\nWe chose the second way as we are addressing a big number of devices and need all the flexibility we can have for deployments â€“ and, sadly, for rollbacks too. Therefore, we host and deploy our CTV applications like any other website and we control the TV Browser Engine to navigate to specific domain names.\n\nThree teams are working on this project, on the same repository:\n\n  a team dedicated to Core features (like catalog, user lifecycle)\n  a team dedicated to Player features (video playback and advertising)\n  and a team supporting legacy devices\n\n\nDeveloper teams are supported by a QA team. It is responsible for functional quality assurance on Pull Requests and pre-releases. Quality assurance designates any processes to ensure a service meets its quality requirements in terms of experience, stability, â€¦\n\nDevelop &amp;amp; release process\n\nWe do our maximum to ensure the best quality of service and experience of what we deliver to our customers and their end-users. We have a strong culture of automated testing &amp;amp; tech reviewing which allows us to deliver almost without a sweatâ€¦ Still, at our scale, missing a bug means a bad experience for thousands or millions of people! And thatâ€™s something we wonâ€™t accept without a fight!\n\n\n  One of Bedrockâ€™s Values is: ROCK-SOLID, ALWAYS\n\n\nAs a consequence, we also have dedicated QA teams testing our work for a subset of device models and versions, before it is being merged to the main codebase, and before going to production as part of a release. They are doing so by connecting TVs to specific environments that are deployed on-demand: previews and staging.\n\nLetâ€™s show off a little bit: at the beginning of 2022, thanks to the TVJS project, we were able to deploy production code to 7 manufacturers, and 38 device versions, meaning 266 combinations to check before launching a release into production! And these numbers are ever increasing!\n\nMy wish: make testing environments easily accessible\n\nWe love showing-off a bit over the applications we deliver on such a huge number of device models, but that doesnâ€™t go with ease nor without pain.\n\nTesting a specific environment on a device was not possible for non-project members (other teams, support, business &amp;amp; product teams, managers â€¦). Starting a preview or a staging application requires a deep understanding of the project, proprietary SDKs (even with our PELO CLI), shell, Git commands and advanced knowledge of how devices work in Developer Mode. This was a major issue: it causes interruptions for developers, slows delivery down, reduces our Time To Market.\n\nQA teams assigned to the project know its basics, they can use PELO CLI and proprietary SDKs, but cannot debug issues they may encounter with such tools: they have to ask developers to take actions for them (as this is not their core job). Using those tools is also time-consuming and time is of the essence when running quality checks while preparing a release.\n\nMany teams also want to start environments by themselves, to test their own developments on back-end services, to investigate when a customer creates a support ticket, â€¦\nThe most important of them are Video teams, responsible for video encoding, transcoding and packaging: they are constantly testing new streams and features, and need a way to test their content by themselves, without asking around for a TVJS developer.\n\nOur answer: The Launcher App !\n\nWhat does it do ?\n\nIâ€™ve developed a TV application to quickly and efficiently start a specific environment. Using the TV remote, people can select the wanted environment and be redirected to it instantly, having the app like they would with the specific app installed.\n\nTyping long texts is painful for TV users. So, when selecting the preview environment, it shows another set of options where users can input a specific PR number. A background process will ask our Github if it knows the PR number, if it is deployed on the selected customer/manufacturer and will pre-fill the branch name. If not specified, it will default back to our master preview that is updated whenever we merge code to the master branch.\n\n\n\nTechnical Architecture\n\nThe launcher is part of the TVJS monorepo, developed using React and re-using modules and packages for UI and Navigation allowing it to have minimum maintenance cost.\n\nFor the first iterations of development of the launcher, I hosted it on AWS Amplify, but the Core team quickly integrated it back to a regular production deployment process we have at Bedrock.\n\nAn automatic process builds the javascript bundle and assets and sends everything to AWS S3. The launcher will then be served through Fastly CDN. We build and deploy a unique launcher per compatible manufacturer on their own domain names (as-of-writing, Samsung Tizen, LG webOS and Hisense). For security reasons, those Fastly services are only accessible from our office networks.\n\n\n\nUnreliability of launcher app installation\n\nIâ€™m proud of this launcher and it is already saving loads of time for our QA teams ! They love it, as it helps them focus on their primary role: ensuring service &amp;amp; experience quality. Still, installing the launcher application on every device in our office is a huge amount of work! And, unfortunately, not a persistent one.\n\nTo develop and test apps on live devices, we need to set them in â€œDeveloper Modeâ€. And each manufacturer has its own way, more or less time-consuming. Worse, whenever Developer Mode expires, all applications installed during this time are uninstalled from the device! Which means we have to install the launcher again after a brief period of time.\n\n  Tizen Developer Mode\n  LG webOS Developer Mode\n\n\nThat period of time varies. For Samsung Tizen, weâ€™re not absolutely sure, but itâ€™s almost a month. For LG webOS, it is 50 hours if you donâ€™t extend the Developer Mode or if you connect another TV with the same Developer Account.\n\nSpecifically for LG, I did set up a CRON that automatically extends the Developer Mode, but sometimes it is being disconnected without reasonâ€¦ Or a mishandling by team members can cause the CRON to fail.\n\n\n\n\n\nTherefore, we arenâ€™t 100% sure the launcher application will be up and ready on all the office devices when work begins in the morning, which means developers will have to manually re-install the launcher when asked by another Bedrock employee. It generates frustration for both QA and developers as they are wasting precious time to re-install the launcher.\n\nDonâ€™t worry though, I already have a couple of ideas to ensure the installation becomes reliable! Iâ€™ll talk more about these it in a future article.\n\nConclusion\n\nAny Bedrock employee can now start an office CTV, use the launcher app, select customer and environment, hit Letâ€™s Go and access the environment they need to work!\n\nWhat we started to measure, and hopefully weâ€™ll have more refined metrics over the next months, is the time QA teams are gaining per day. They needed an average of 15 minutes to start up a TV, set up the Developer Mode, and install the wanted app through CLI. They are validating 5 PRs per day, on 2 different devices at minimum, they almost gain one hour per day. That means our Time To Market is faster, and our QA teams have more time to do exploratory testing as well as refining their tests and writing more automated tests. Something that is not as measurable as time, is the enhanced peace of mind for them to go to work every morning knowing they have a tool designed for them to focus on their core work.\n\nThis has improved the QA team overall velocity! And it makes the whole project more accessible for any employee. However, there is still room for improvement regarding launcher deployment and stability over time, and this is something I will cover in our next article.\n\nI hope you liked this article and it helped you if youâ€™re trying to achieve something similar!\n"
} ,
  
  {
    "title"    : "How Micro-Services changed our caching architecture",
    "category" : "",
    "tags"     : " on-premise, cloud, cdn, varnish, aws, cloud, fastly, varnish-operator, cloudfront, alb",
    "url"      : "/2022/12/23/varnish-operator.html",
    "date"     : "December 23, 2022",
    "excerpt"  : "At Bedrock we use Cloudfront or Fastly for two different reason. To protect our applications from potential Distributed Denial of Service Attack. And to provide a layer of cache in front of our applications. No need to go down to the app for an ea...",
  "content"  : "At Bedrock we use Cloudfront or Fastly for two different reason. To protect our applications from potential Distributed Denial of Service Attack. And to provide a layer of cache in front of our applications. No need to go down to the app for an easily cacheable response.\n\n\n\n\n\nAt least that is what we thought in 2018 when we were migrating from on premise to the Cloud.\n\nAt that time we had a Varnish instance caching everything at the border  of our on premise infrastructure. All the applications were running either on virtual machines or on bare metal servers. Those applications were mostly called by the end-userâ€™s browser. Whenever an application called another application it did it through Varnish.\n\nThis is ideal if applications are mostly called from the outside world. The Varnish instance caches all cacheable content, and it does not cost too much time as it was in the same Data Center.\n\n\n\n\nIn 2023, we think otherwise. We have now a KOps managed Kubernetes cluster running on EC2 spot instances in private subnets at AWS. As we migrated to the cloud we also embarked on the journey of splitting monolith into smaller more manageable microservices.\n\nWith less monoliths the Bedrock product is more resilient and easier to scale but it changes the topologies of network calls. Before there were far more calls coming from the internet from end-users browsers. Now with the new architecture coming into place inter-app requests have increased.\n\nOne solution would be to directly call the ingress of the applications, staying inside the cluster but without the benefit of caching as it is handled by the CDN. This would lead to unsustainable increase in CPU usage, and probably very little gain in terms of response time.\n\nA better solution for us would be to have the caching of CDN inside the cluster. This would enable us to have fast response time and little to no increase in CPU usage.\n\nEnter Varnish-Operator\n\nWe tested the project IBM/Varnish-Operator. This project allows us to create Custom Resources for Kubernetes handled by the Varnish-Operator. This object is called a VarnishCluster, the configuration is pretty simple to get started. This enables us to have a caching layer, between the Ingress-Controller and the Application.\n\n\n\n\n\nVarnishCluster also uses Varnish Configuration Language (VCL) which we are pretty familiar with since we use Varnish On-Premise since 2015, and developers use it regularly to configure Fastly distribution.\n\nBy adding cache using VarnishCluster to an application that is not fully cacheable, we almost divided itâ€™s average response time by two. It is not a surprise as inter api calls used to look like the following graph:\n\n\n\n\nWe changed parameters in the application after adding VarnishCluster so that it calls other app inside the cluster like in the following graph:\n\n\n\n\nA few details\n\nBefore I wrap this up, here are a few details about the implementations.\n\nAs you will be able to read in the Varnish documentation:\n\n  â€œBy default Varnish will use 100 megabytes of malloc(3) storage for caching objects, if you want to cache more than that, you should look at the -s argument.â€\n\n\nSo if you give many Gigs of memory to your Varnish container it wonâ€™t be attributed to the Varnish process. You can set it with the argument -s storage=malloc,&amp;lt;Number&amp;gt;.\n\nAs we use only Spot nodes that can be terminated by AWS at any moment with only 2 minutes notice, we want to give more resilience to our Varnish Clusters pod as cache is stored in RAM memory.\nYou lose all your cache at each restart of the Varnish Container.\n\nWe configured podAntiAffinity between application pods and VarnishClustersâ€™ to avoid scheduling those pods on the same node and be vulnerable to reclaims.\n\nWe added a podDisruptionBudget to avoid losing all our pods at the same time. We also customized the VCL a bit to make Varnish serve stale content in case our application is unreachable.\n\nWe also added a Prometheus Service Monitor to make sure all Varnish metrics would be scraped by Victoria Metrics.\n\nIn the Future\n\nIn next versions we would like to add the possibility to configure PriorityClass of VarnishClusters pod. PriorityClasses are used to order workloads priority.\nIn a context of scaling and of scarcity of resources, the scheduler will evict pods of lower priority to make room for the pod it is trying to schedule.\n\nFor now our VarnishClusterâ€™s pods have the PriorityClass by default but it is more critical than any other applications as it holds a cache in its memory.\n\nAlso we do not have logs of Varnish. We would like to be able to stream VarnishLog content into Loki. This would be super useful to debug and to investigate if we ever encounter bugs or unexpected behaviors.\n\nConclusion\n\nAverage response time going down, red bar is when we pushed it in production\n\n\n\nWith the generalization of microservices, Bedrock needed to rethink its architecture to optimize not only for browser to API calls but also for more API to API usage. By adding VarnishCluster in front of our applications and calling them directly from inside the cluster we improved significantly the Bedrock product.\n\nThe Github project is still young and lacks important features, we hope with this article to help draw attention to this project and potential contributors.\n\n\n\n"
} ,
  
  {
    "title"    : "Nos retours sur l&#39;HAProxyConf Paris 2022",
    "category" : "",
    "tags"     : " haproxy, haproxyconf, conference",
    "url"      : "/2022/12/23/haproxyconf-paris-2022.html",
    "date"     : "December 23, 2022",
    "excerpt"  : "Bedrock Ã©tait prÃ©sent lors de la ConfÃ©rence HAProxy qui se dÃ©roulait Ã  Paris en novembre 2022 : en tant que speaker, avec la prÃ©sentation de Vincent Gallissot, mais aussi en tant que spectateur. Cet article relate les points forts qui nous ont mar...",
  "content"  : "Bedrock Ã©tait prÃ©sent lors de la ConfÃ©rence HAProxy qui se dÃ©roulait Ã  Paris en novembre 2022 : en tant que speaker, avec la prÃ©sentation de Vincent Gallissot, mais aussi en tant que spectateur. Cet article relate les points forts qui nous ont marquÃ©s.\n\nLa prÃ©sentation de Vincent Gallissot, Lead Cloud Architect chez Bedrock, mettait en valeur lâ€™usage dâ€™HAProxy en tant que brique essentielle de notre infrastructure. Chez Bedrock, nous dÃ©veloppons et maintenons une plateforme de streaming qui a Ã©tÃ© migrÃ©e dans le Cloud en 2019. Cette prÃ©sentation Ã©tait grandement inspirÃ©e de lâ€™article intitulÃ© â€œScaling Bedrock video delivery to 50 million usersâ€, dans lequel vous trouverez plÃ©thore dâ€™informations concernant nos utilisations dâ€™HAProxy.\n\n\n\nSommaire\n\n\n  Ce que des millions de requÃªtes par seconde signifient en termes de coÃ»t et dâ€™Ã©conomie dâ€™Ã©nergie\n  Un outil pour les gouverner tous\n  Vous reprendrez bien un peu de pÃ©taoctets?\n\n\nCe que des millions de requÃªtes par seconde signifient en termes de coÃ»t et dâ€™Ã©conomie dâ€™Ã©nergie.\n\nLa keynote dâ€™ouverture avait pour orateur Willy Tarreau, le Lead Developer dâ€™HAProxy.\nAu travers dâ€™une dÃ©monstration concrÃ¨te mÃ©langeant software et hardware, lâ€™objectif Ã©tait de :\n\n  transmettre lâ€™idÃ©e quâ€™ajouter une brique logicielle dans un systÃ¨me ne le dÃ©grade pas pour autant, bien au contraire\n  sensibiliser lâ€™audience quant Ã  la consommation dâ€™Ã©nergie de nos systÃ¨mes\n\n\nContexte technique et premiÃ¨res amÃ©liorations\n\nPour ce premier cas dâ€™Ã©tude, Willy Tarreau nous prÃ©sente le cas dâ€™un service de vente en ligne.\n\nLa stack technique est composÃ©e de PHP / pgSQL (NodeJS + Symfony) et les images sont stockÃ©es en base de donnÃ©es. Câ€™est cette architecture qui sera mise Ã  lâ€™Ã©preuve lors des tests de charge Ã  venir.\n\nDans un premier temps, plusieurs amÃ©liorations (sans HAProxy) sont proposÃ©es. Il peut sâ€™agir dâ€™un simple rappel, voir dâ€™un pro-tip dâ€™architecture pour les plus novices : Les images en base de donnÃ©es, câ€™est une mauvaise idÃ©e.\n\nEn les dÃ©plaÃ§ant vers un CDN, le systÃ¨me peut rapidement et simplement doubler ses performances, la base de donnÃ©es Ã©tant un goulot dâ€™Ã©tranglement. La taille des pages peut Ãªtre optimisÃ©e via lâ€™activation de lâ€™option http â€œgzipâ€. Les informations de sessions sont elles aussi enregistrÃ©es en base de donnÃ©es. Afin dâ€™amÃ©liorer les performances, il est possible dâ€™ajouter du caching via des outils tels que Memcache.\n\nSuite Ã  cela, une premiÃ¨re amÃ©lioration dâ€™architecture serait dâ€™ajouter un NLB (Network Load Balancer) en amont du systÃ¨me qui distribuerait les requÃªtes entrantes vers plusieurs unitÃ©s de calculs.\n\n\n\nSchÃ©ma dâ€™architecture, premiÃ¨re version\n\nDans le cas prÃ©sent, les requÃªtes entrantes sont distribuÃ©es de faÃ§on alÃ©atoire entre les diffÃ©rentes unitÃ©s de traitement. Chacun de ces backends se connectant Ã  la mÃªme et unique base de donnÃ©es.\nLe benchmark ci-dessous (efficacitÃ©, au sens nombre de requÃªtes traitÃ©es en fonction du nombre dâ€™unitÃ©s de calcul), ne montre pas une croissance linÃ©aire. Il sâ€™agit dâ€™une courbe tendant vers une pente nulle (voir nÃ©gative pour les plus grosses architectures).\n\n\n\nGraphique reprÃ©sentant lâ€™efficacitÃ© du systÃ¨me en fonction du nombre de backends\n\nComment expliquer que cette architecture ne scale pas linÃ©airement ?\n\nMalgrÃ© les amÃ©liorations apportÃ©es pour les sessions grÃ¢ce au cache, il subsiste encore un problÃ¨me.\n\nLe NLB est un composant qui ne fait que rÃ©partir la charge sans tenir compte de lâ€™historique des requÃªtes. En effet, celui-ci va distribuer la charge dâ€™entrÃ©e alÃ©atoirement vers les backends.\nChaque backend reÃ§oit des requÃªtes provenant de nâ€™importe quel utilisateur impliquant alors un cache-miss trÃ¨s Ã©levÃ© : lâ€™utilisateur est rarement trouvÃ© dans le cache, ce qui gÃ©nÃ¨re une requÃªte supplÃ©mentaire en base de donnÃ©es et dÃ©grade les performances en plus de consommer inutilement des ressources.\n\nEt si nous ajoutons HAProxy Ã  notre systÃ¨me ?\n\nCâ€™est ici quâ€™entre en jeu HAProxy en remplaÃ§ant le NLB. Pour cela, pas besoin dâ€™un foudre de guerre en termes de ressources.\n\nLes tests ont Ã©tÃ© effectuÃ©s sur une machine ARM Breadbee cadencÃ©e Ã  1 GHz et possÃ©dant 64 Mo de RAM. Nous verrons Ã©galement par la suite quâ€™on pourrait mÃªme se passer dâ€™une machine supplÃ©mentaire.\n\nLe but dâ€™HAProxy est de spÃ©cialiser les caches des backends et plus globalement de forcer les sessions utilisateurs vers les mÃªmes backends.\n\nPour cela, HAProxy effectue une inspection de la couche 7 du trafic et renvoie toutes les requÃªtes dâ€™un mÃªme utilisateur sur une mÃªme machine en rÃ©duisant ainsi les cache-miss aux seuls cas des nouveaux clients se connectant Ã  la plateforme. Ainsi, le nombre dâ€™appels Ã  la base de donnÃ©es pour rÃ©cupÃ©rer les informations de session est drastiquement rÃ©duit, la majoritÃ© dâ€™entre elles Ã©tant stockÃ©es en cache.\n\nAutre fonctionnalitÃ© de taille : HAProxy limite le nombre de requÃªtes faites en parallÃ¨le sur un mÃªme backend, ce qui limite les locks de processus et les temps dâ€™attente. Ceci a pour consÃ©quence directe de rÃ©duire la consommation CPU.\n\nCes deux amÃ©liorations permettent Ã  lâ€™application de scaler de faÃ§on beaucoup plus linÃ©aire, tout en rÃ©duisant les consommations CPU et Ã©nergÃ©tiques inutiles. Globalement, les performances initiales sont largement dÃ©passÃ©es avec deux fois moins de backends.\n\nA partir de quand est-il intÃ©ressant de franchir le pas ?\n\nMaintenant que les bÃ©nÃ©fices dâ€™HAProxy ont Ã©tÃ© prÃ©sentÃ©s, la prochaine Ã©tape est de se demander : quand est-ce quâ€™on se lance ? La question est considÃ©rÃ©e en termes de performance, mais aussi sous un angle pÃ©cunier.\nSi HAProxy peut Ãªtre intÃ©grÃ© sans augmenter les coÃ»ts du systÃ¨me, câ€™est encore mieux.\n\nAjouter HAProxy dans un systÃ¨me composÃ© dâ€™un seul backend nâ€™apporte pas de bÃ©nÃ©fice : il nâ€™y a pas de load-balancing possible. Avec deux backends, si on divise le besoin de processing par deux, nous nâ€™avons plus quâ€™un seul backend et donc pas de load-balancing possible.\nCâ€™est en fait Ã  partir de 4 backends que lâ€™ajout dâ€™un HAProxy en entrÃ©e devient intÃ©ressant :\n\n  en retirant 2 serveurs de nos backends en conservant une puissance Ã©quivalente (cf les tests ci-dessus)\n  et en recyclant un des deux backends retirÃ©s en hÃ´te pour HAProxy\nEn fin de compte, pour une mÃªme puissance de traitement, un backend est retirÃ© ce qui permet de rÃ©duire les coÃ»ts de fonctionnement. Ce principe sâ€™applique Ã©galement sur un grand nombre de backends.\n\n\nCâ€™est lÃ  que prend tout son sens lâ€™expression qui avait Ã©tÃ© utilisÃ©e pour conclure cette keynote : â€œHAProxy is a free software running on free hardwareâ€.\n\nChez Bedrock, nous appliquons aussi ces diffÃ©rentes techniques de Consistent Hashing en entrÃ©e de notre CDN vidÃ©o. Nos caches vidÃ©os sont spÃ©cialisÃ©s et chaque utilisateur est redirigÃ© vers un unique backend lors de la lecture dâ€™une vidÃ©o.\nPour en savoir plus, vous pouvez consulter notre article au sujet du Consistent Hashing.\n\nUn outil pour les gouverner tous\n\nDans notre activitÃ© en informatique, nous sommes amenÃ©s Ã  dÃ©livrer de plus en plus rapidement des applications, des mises Ã  jour, etcâ€¦ Nous avons donc adoptÃ© la philosophie DevOps et tout un panel dâ€™outils autour de celle-ci afin de sÃ©curiser, monitorer et automatiser chaque Ã©tape de nos pipelines de livraison.\n\nLe cas de figure du load balancing est intÃ©ressant dans ce type dâ€™organisation, il est essentiel dâ€™exposer de nouvelles applications sur les environnements de production mais Ã©tant donnÃ© que la maÃ®trise de cet outil requiert une comprÃ©hension du rÃ©seau, la responsabilitÃ© incombe souvent Ã  lâ€™Ã©quipe Ops de le gÃ©rer.\n\nVous souhaitez mieux gÃ©rer votre flotte HAProxy ?\n\nAnjelko Iharos, directeur de lâ€™ingÃ©nierie Ã  HAProxy Technologies nous a prÃ©sentÃ© leur nouvel outil dâ€™automatisation : HAProxy Fusion Control Plane, packagÃ© dans la version entreprise de HAProxy.\n\nCelui-ci va amener une nouvelle interface enrichie afin de gÃ©rer toutes les instances HAProxy et les outils gravitant autour de ces derniÃ¨res.\n\nOn peut citer :\n\n  La possibilitÃ© pour les dÃ©veloppeurs de router eux-mÃªme leurs applications sans avoir besoin dâ€™un Ops dans leurs pipelines de CI via lâ€™API Fusion.\n  GÃ©rer les WAF de HAProxy de maniÃ¨re centralisÃ©e et rÃ©percuter cette configuration sur un ensemble de clusters/instances.\n  Permettre aux Ops de gÃ©rer la structure de leurs load balancers, ajouter de nouvelles instances, gÃ©rer les certificats SSL, le tuning des performances depuis un seul point dâ€™entrÃ©e.\n\n\nEst-ce rÃ©silient ?\n\nFusion Control Plane est livrÃ© avec tout un set de features intÃ©ressantes pour assurer sa maintenabilitÃ© et sa rÃ©silience :\n\n  Une pleine observabilitÃ© avec une application unifiÃ©e de rÃ©cupÃ©ration de logs, mÃ©triques et rapports dans la mÃªme interface. Lâ€™export de ces data est possible, notamment pour les transposer dans un dashboard tiers (Grafana, par exemple).\n  Un systÃ¨me de RBAC permettant de mieux gÃ©rer les pÃ©rimÃ¨tres de chacune des Ã©quipes dans le control plane.\n  La gestion centralisÃ©e de la configuration, la validation des configurations et le bot management. La partie WAF est packagÃ©e avec OWASP (communautÃ© publiant des recommandations pour la sÃ©curisation des applications web) ModSecurity Core Rule Set (CRS) pour la dÃ©tection des vulnÃ©rabilitÃ©s. Dans le cadre dâ€™un cluster un systÃ¨me de failover automatique avec auto-Ã©lection du leader (Ã  la maniÃ¨re de GOSSIP avec Consul).\n\n\nUne vue de lâ€™avenir ?\n\nAujourdâ€™hui, Fusion Control Plane limite son scope Ã  HAProxy Entreprise et Community Edition, les IngressController ne sont pour le moment pas encore supportÃ©s.\n\nIl nâ€™est pas encore pleinement compatible avec les features offertes par AWS (Gestion des ASG et de Route53) mais câ€™est en cours de dÃ©veloppement chez HAProxy Technologies.\n\nLe produit semble prometteur et intÃ©ressant. Les possibilitÃ©s quâ€™il nous offre pour laisser la main aux dÃ©veloppeurs sur la mise en place de routes vers leurs applications cÃ´tÃ© on-premise est vraiment un gros plus, mais il nous manque pour le moment le support de lâ€™IngressController HAProxy utilisÃ© sur nos cluster Kubernetes, ce qui nous empÃªche dâ€™en profiter au maximum.\n\nVous reprendrez bien un peu de pÃ©taoctets ?\n\nChez Bedrock, un Ã©lÃ©ment central de notre mÃ©tier est de fournir de la vidÃ©o Ã  nos utilisateurs. (Incroyable pour une boite qui fait de la VOD hein? ğŸ˜€).\n\nPour ce faire nous avons nos propres serveurs CDN hÃ©bergÃ©s sur Paris, en complÃ©ment des CDN publics comme Cloudfront ou Fastly. Cette annÃ©e nous avons servis plusieurs centaines de PB de donnÃ©es via nos serveurs et nous espÃ©rons pouvoir au moins doubler ce trafic lâ€™annÃ©e prochaine !\n\nNotre architecture CDN est constituÃ©e dâ€™un logiciel appelÃ© LBCDN qui â€œload-balanceâ€ la charge sur les CDN, on-prem et publics, en redirigeant un utilisateur vers un serveur CDN spÃ©cifique.\nNos serveurs en eux-mÃªmes sont basÃ©s sur Nginx avec une configuration assez simple en direct IO sur de gros SSD.\n\nLa HAproxy conf 2022 nous a pas mal inspirÃ©s pour rÃ©pondre Ã  nos problÃ©matiques avec ces deux confÃ©rences :\n\n  Boost your web apps with HAProxy and Varnish, by JÃ©rÃ©my Lecour CTO of Evolix:Video\n  Was That really HAProxy, by Ricardo Nabinger Sanchez performance engineer at Taghos: Video\n\n\nCes deux prÃ©sentations font Ã©tat dâ€™une architecture sur les CDN intÃ©ressante oÃ¹ HAProxy est utilisÃ© pour mettre â€œen sandwichâ€ lâ€™outil (ou les outils) faisant fonction de CDN.\nLâ€™architecture prÃ©sentÃ©e semble permettre une configuration bien plus fine que ce que nous avons actuellement avec seulement Nginx.\n\nPar exemple, sur nos CDN on-prem nous devons aujourdâ€™hui utiliser une astuce pour que Nginx puisse dynamiquement aller rÃ©soudre le nom de domaine du backend sur lequel il source ses fichiers. Cela est dÃ©jÃ  un peu dommage de ne pas avoir de mÃ©canisme disponible nativement. De plus, ce mÃ©canisme est difficile Ã  coupler avec dâ€™autres permettant dâ€™avoir du fail-over par exemple.\n\nCâ€™est ici quâ€™HAProxy pourrait intervenir pour rÃ©soudre notre problÃ©matique car il nous permet dâ€™avoir du fail over et des tests plus fins sur lâ€™Ã©tat de santÃ© des backends.\n\nDe plus, nous sommes en train de tester une solution de second-tier de CDN qui, du fait de la complexitÃ© ajoutÃ©e Ã  notre architecture de CDN, profiterait beaucoup dâ€™une plus grande finesse de configuration.\n\nâ€œMais attends, tu nâ€™as parlÃ© que de HAProxy en backend lÃ , tu triches un peu non? Câ€™est pas un sandwich câ€™est une tartine de HAProxy lÃ !â€\nTout Ã  fait, notre cas dâ€™usage actuel nâ€™a pas forcÃ©ment besoin dâ€™un HAProxy en frontal de Nginx.\n\nMAIS!\n\nCâ€™est lÃ  que les confÃ©rences sont intÃ©ressantes car elles montrent que lâ€™on peut mixer les backends.\nDans la confÃ©rence prÃ©sentÃ©e par Ricardo, lâ€™utilisation de deux backends (Varnish et hyper-cache) sur un mÃªme serveur est permise par un HAProxy. Cela permet de profiter de la complÃ©mentaritÃ© de ces services.\nDans notre cas, nous nâ€™avons pas besoin de cela mais une autre confÃ©rence nous a mis la puce Ã  lâ€™oreille : Writing HAProxy Filters in Rust, by Aleksandr Orlenko.\nCela pourrait nous permettre, avec un HAProxy en frontal, dâ€™agrÃ©ger plus finement les mesures de performances du serveur afin dâ€™optimiser lâ€™usage de ses ressources, ou dÃ©porter une partie du trafic sur un serveur moins chargÃ©, ou encore de rÃ©cupÃ©rer une partie des traitements actuellement effectuÃ©s par le LBCDN.\n\nAjouter cette fonctionnalitÃ© serait la belle cerise au kirsch au sommet de ce sandwich de HAProxy.\n\n\n\nâ€œIl est bizarre ton sandwichâ€\n\nâ€œBon dâ€™accord, câ€™est plutÃ´t un gÃ¢teau Ã  Ã©tages.â€\n\nâ€œOk câ€™est mieux, mais je prÃ©fÃ¨re les macarons de la HAProxy Conf 2022 quand mÃªme.â€\n\nA une prochaine fois !\n\nLa HAProxyConf, câ€™Ã©tait deux jours de confÃ©rences avec des orateurs venus de tous les coins du globe.\nUne belle occasion pour nous dâ€™en apprendre plus sur un outil que nous utilisons quotidiennement chez Bedrock.\nDans cet article, nous nâ€™avons pas pu faire mention de tout ce qui nous a intÃ©ressÃ©. Nous pourrions notamment citer les trÃ¨s intÃ©ressantes confÃ©rences au sujet de :\n\n  Docker et leur utilisation de lâ€™outil Keda\n  Ou encore de SoundCloud et leurs mesures anti-DDOS\n\n\nCette confÃ©rence Ã©tait aussi lâ€™occasion dâ€™Ã©changer avec lâ€™Ã©quipe HAProxy autour de sujets techniques qui nous concernent, de voir que nous utilisions dÃ©jÃ  certaines bonnes pratiques, mais aussi que nous avions de quoi nous amÃ©liorer.\n\nSuite Ã  cette confÃ©rence, câ€™est HAProxy Fusion que nous attendons le plus. Fusion sâ€™annonce comme lâ€™outil idÃ©al pour manager une flotte dâ€™HAProxy. Jusquâ€™Ã  prÃ©sent, nous devions utiliser une solution maison HSDO, fonctionnelle, mais trÃ¨s probablement moins bien intÃ©grÃ©e quâ€™un outil directement fourni par HAProxy.\n"
} ,
  
  {
    "title"    : "How many DynamoDB RCU and WCU should we reserve to achieve maximum cost reductions, when our workloads are changing all the time?",
    "category" : "",
    "tags"     : " aws, dynamodb, finops",
    "url"      : "/2022/11/22/dynamodb-reservations.html",
    "date"     : "November 22, 2022",
    "excerpt"  : "Many of the microservices in our VOD and Replay platform use DynamoDB as their database.\nPerformance is very good if the data is architected for it, scalability is reasonably fast, and the serverless aspect offloads a lot of the administration and...",
  "content"  : "Many of the microservices in our VOD and Replay platform use DynamoDB as their database.\nPerformance is very good if the data is architected for it, scalability is reasonably fast, and the serverless aspect offloads a lot of the administration and hosting work. Whether itâ€™s performance, resilience or time-to-market, DynamoDB helps us achieve our business goals.\n\nThat said, when we spend several hundred thousand dollars on DynamoDB every year, any optimization is good for us!\n\nWith DynamoDB, committing to a certain capacity for a year can help reduce costs â€“ up to 50% savings on that capacity. But how do we know how much to reserve when traffic on our platform varies throughout the day?\n\n\n\nTable of Contents\n\n\n  DynamoDB: a not always obvious cost model!\n  How many WCUs and RCUs do we consume?\n  In theory: how much should we reserve, to achieve maximum savings?\n  In practice: letâ€™s calculate how much to reserve!\n  Finally, letâ€™s create those reservations!\n  After reserving, viewing the costs\n  Conclusion\n\n\nDynamoDB: a not always obvious cost model!\n\nTo skip all the theory about how DynamoDB is priced and WCUs, RCU, on-demand and provisionned billing modes, click hereâ€¦\n\n\n  DynamoDB is serverless1!\n\n\nBut, as with many AWS services, you have to think for a while before you really understand DynamoDB costsâ€¦\n\nOut of scope costs\n\nWe pay for the volume of data stored, the volume of data backed up. These costs are outside the scope of this article and I wonâ€™t talk about them again today. They are not zero, however, and can even be a significant part of your bill â€“ for example, if you store large data for a long time2 in DynamoDB. Something you probably shouldnâ€™t do!\n\nWCUs and RCUs\n\nEach DynamoDB table can be configured in either on-demand or provisioned billing mode.\n\nIn the second case, we pay for RCUs (Read Capacity Units) and WCUs (Write Capacity Units), depending on the capacity we provision for each table.\nReservations only matter for these RCUs and WCUs, in purple in the screenshot below.\n\n\n\nOver the past year, our WCU and RCU costs in provisioned mode represent about half of our DynamoDB costs.\nStorage and backups have costs that we consider negligible today.\n\nAnd, from a financial standpoint, we work with far too many pay-per-request tables3 for my taste.\n\nThe documentation will tell you more, but in very broad terms:\n\n\n  One WCU is consumed when writing one line of data. Or for each 1 KB block written.\n  One RCU is consumed to read one line of data. Or for each 4 KB block read.\n  In eventually-consistent read mode, only 1/2 RCU is consumed to read one line of data. Or for each 4 KB block.\n  Transactional mode costs twice as much.\n\n\nAs you can imagine, the first optimization is to store only what is necessary and to request DynamoDB in the way that best meets the needs of the application, including consistency and costs. Developing a data schema that efficiently meets the needs of the application is crucial. I highly recommend you read Alex DeBrieâ€™s very good â€œThe DynamoDB Bookâ€! Financial optimization based on reservations should â€“ and can â€“ only come afterwards, when usage patterns have been dealt with.\n\nThe on-demand / pay-per-request mode\n\nIn on-demand mode, we theoretically donâ€™t have to worry about scalability, DynamoDB handles it for us4.\n\nIn this mode, we pay for each RCU and WCU we consume. If we donâ€™t use DynamoDB, we donâ€™t pay. If we use DynamoDB, we pay.\nThe counterpart is that RCUs and WCUs are more expensive in this mode than in the one presented below.\n\nThis mode is therefore very practical, in my opinion, in two cases:\n\n\n  In an environment where we only perform a few queries from time to time (dev, staging).\n  For tables that are usually not used much, but receive large and sudden peaks of requests at certain times.\n\n\nThis mode is not adapted, especially because costs are too high:\n\n\n  For tables where consumption is stable or varies slowly. Typically, tables for which usage follows our daily traffic wave, which is gentle enough on most applications for a reactive auto-scaling mechanism to meet our needs.\n\n\nProvisioned mode\n\nIn provisioned mode, we configure how many RCUs and WCUs we want and we pay for that number of RCUs and WCUs â€“ no matter if we consume them or not.\nThis billing mode is therefore less flexible than on-demand. On the other hand, RCUs and WCUs are less expensive.\n\nIn provisioned mode, we can set up an auto-scaler on the RCUs and WCUs of the tables that need it. It will dynamically reconfigure the provisioned RCUs and WCUs for those tables, to approximate the actual usage. With an auto-scaler, we can pay as close as possible to our actual consumption, at the provisioned price, which is lower than the on-demand one.\nHowever, scale-out is not instantaneous: it takes several minutes to detect it needs to act, and then up to several minutes (especially on a large table) to do so. Also, scale-in can only be triggered around once per hour. For more detailed information, read the documentation and the quota page.\n\nThis mode is especially recommended, in my opinion and considering our workloads:\n\n\n  As often as possible, since each RCU and WCU costs much less than in on-demand mode.\n\n\nThis mode is not suitable:\n\n\n  On tables where consumption varies very abruptly.\n\n\nIn provisioned mode, reservations\n\nBy agreeing to pay for a certain amount of RCU and WCU for one year (or even three years in some regions), these RCU and WCU become even cheaper: up to ~50%5 cheaper than in default-provisioned mode.\nReserving capacity is a great way to considerably reduce the cost of read/write operations on DynamoDB!\n\nReservations lock us for one year. We will pay for the reserved RCUs and WCUs, whether we use them or not.\nIt is therefore important to calculate correctly the reservations to be made.\n\nAlso, we pay a part of the total yearly amount at the beginning of the commitment (= â€œupfrontâ€), which means we must be able to invest a certain amount in advance.\nThe other part is spread over all the months of the commitment period.\n\nAs a consequence, the big question, to which the rest of this document tries to answer, is: â€œhow many RCU and WCU should we reserve to keep our costs as low as possible?â€\nWhen our consumption varies throughout the day, this calculation is pretty funÂ ;-)\n\nReservations are global to an AWS account, or even to all accounts on a consolidated bill6.\n\nâ†’ Reserved pricing is documented on the page of â€œprovisionedâ€ pricing.\nâ†’ You can also read this whitepaper.\n\nHow many WCUs and RCUs do we consume?\n\nFor the rest of our reasoning and this article, we only count the consumption in provisioned mode (and exclude on-demand), since thatâ€™s where we can play with reservations.\nAlso, we count provisioned WCU and RCU and not what is actually consumed â€“ so beware of any potential waste.\n\nOn the DynamoDB Web Console home screen, we can see, for an account and a region, how many WCUs and RCUs are provisioned at the current time:\n\n\n\nBut these numbers only give a view at a given instant, in a single AWS account and in a single region.\nWe deploy our platform across dozens of accounts and multiple regions, with traffic that changes throughout the day, so this is not enough.\n\nTable WCUs/RCUs\n\nFor a global view of all tables in an account in a region, we can query Cloudwatch Metrics, analyzing the ProvisionedWriteCapacityUnits or ProvisionedReadCapacityUnits metrics:\n\n\n\nThe Stacked Area view shows, at any given time, the total WCUs (or RCUs) provisioned for all of our tables, in an account and a region.\n\nWCU/RCU of GSI\n\nWe also need to count the WCUs/RCUs of the Global Secondary Indexes â€“ and these are different metrics! Or, at least, the metrics are shown in a different category in the Cloudwatch web console.\n\n\n\nSo, in totalâ€¦\n\nTo get the total, you have to consider this metric for the tables and for the Global Secondary Indexes! In the Cloudwatch console, you have to search in two categories.\nGraphing it all :\n\n\n\nOf course, this is to be looked at for WCUs, but also for RCUs, following exactly the same principle.\nAnd, again, weâ€™re working in multiple accounts and regions.\n\nIn theory: how much should we reserve, to achieve maximum savings?\n\nOnce we know how much capacity weâ€™re actually using, we can move on to reservations.\n\nBut the calculation would be far too easy if our usage was flat!\nIn reality, thanks to auto-scaling, our provisioned capacity follows our usual traffic pattern: a wave.\n\nAnd, two things:\n\n\n  if we reserve more than we provision, weâ€™ll waste money.\n  if we reserve less than we provision, we wonâ€™t save as much as we could.\n\n\nReserve at the bottom of the wave\n\nA first idea is to reserve the lowest value we provision throughout the day: what we provision at the bottom of our traffic wave, at night.\n\nIn this case, we are not wasting money, as we always provision 100% or more of our reservation.\nBut we are probably minimizing our savings, since we are provisioning more than the reservation, all day long.\n\nReserve at the top of the wave\n\nA second idea, kind of the opposite, is to reserve the highest value we provision throughout the day.\nThis way, we will never pay the full rate for any WCU/RCU.\n\nBut, in this case, we will be wasting a lot of money, since all day long we will be provisioning less than our reservation.\nThis is a bad idea.\n\nReserve â€œin the middleâ€, thanks to careful calculations\n\nNow, the real solution: calculate the right value:\n\n\n  Less than the highest value, to minimize waste.\n  And more than the lowest value, to optimize savings.\n\n\nIn practice: letâ€™s calculate how much to reserve!\n\nManipulating metrics in Cloudwatch, for visualization, may be acceptable, although we rarely do it since we use other stacks for our metrics. And aggregating metrics from multiple accounts should be feasible (we havenâ€™t tried it).\nBut for calculations, it is not enough.\n\nExporting metrics\n\nAs a first step, we exported the metrics visualized above, to be able to manipulate them in another tool â€“ in a spreadsheet, for example.\nTo export these metrics from Cloudwatch, we can query its API. We need to do this for all accounts and for each table, which is complicated to do manually.\n\nTo simplify the task, we started working with a script that exports this data to a CSV file.\nSpecifically, this script exports one data point per hour: the number of WCUs or RCUs actually provisioned during that hour.\n\nRunning this script for a representative week, we have enough data to calculate the ideal reservations.\n\n\n  ğŸ—“ï¸ Representative week?\nOf course, we have to be careful to choose the week we focus on.\nIf we work with data from a week with a huge unexplained peak of traffic, the results of our calculation will fit that week, but not so much to the rest of the year!\n\n\nA Google Spreadsheet calculation\n\nImporting this data into a Google Spreadsheet, we get two columns: a date+time and a number of WCUs.\nAnd this is for each one-hour range during an entire week:\n\n\n\n\n  â„¹ï¸ Only twelve hours\nHere, I only reproduce twelve rows corresponding to twelve hours, but keep in mind that there are actually 168 rows in my spreadsheet: one row per hour, 24 hours per day, for 7 days.\nAlso, the values used for this article are all simulated, to avoid sharing sensitive information, but they scrupulously respect the shape of our traffic and usage wave.\n\n\nThe next step is to integrate the cost of these WCUs.\nEasy anough, we multiply the number of WCUs by the cost of a WCU in Paris, i.e. $0.000772.\nAnd the sum of the cost of each line gives us the total cost, without reservation:\n\n\n\nThe calculations, on an assumption\n\nNow, letâ€™s assume, for the time being, that we reserve 25,000 WCUs:\n\n\n  The upfront, each hour, is $5.07991.\n  And, each hour, we also have to pay $3.82500 for this capacity, since the upfront is only partial.\n\n\nIn addition:\n\n\n  During some hours, when we consume less than 25,000 WCU, we will not pay anything extra.\n  During some other hours, when we consume more than 25,000 WCU, we will have to pay a supplement, at the full provisioned rate.\n\n\nAdding these data, we obtain a different hourly cost, often lower than the one determined above.\nAnd, therefore, we get a lower total cost as well:\n\n\n\nWith this hypothesis of a 25,000 WCU reservation, over these twelve hours, we would pay 135 dollars instead of 229 dollars without reservation.\nWe would then realize 40.96% savings!\n\nThe calculations, until we find the right value\n\nOf course, during the hours when we consume less than 25,000 WCU, we are wasting capacity: we are paying for it, without using it.\n\n\n\nThe goal of the game is to find the right number of WCUs to reserve: we want to reduce the total cost as much as possible, maximizing the percentage of savings.\n\nTo do so, we try different values for the number of WCUs reserved, until we find the one that maximizes the percentage of savings:\n\n\n\nHereâ€™s the same thing as a graph:\n\n\n\nHere, over these twelve hours, the optimal approach would be to reserve 23,000 WCU.\n\n\n  ğŸ’ª Getting real: an entire week\nIn reality, we perform exactly the same calculation and we follow this very same logic, on 168 lines of data, corresponding to a representative week.\n\n\nEasier calculations?\n\nThe first year we tried to reserve capacity, we quickly wrote a script to collect the data from Cloudwatch and export it as CSV.\n\nWe still havenâ€™t, after three or four years now, written a program that would perform the calculations based on this data to come up with the right value for the number of WCUs or RCUs to reserve.\nAs a matter of facts, copying and pasting data from the CSV export to a spreadsheet only takes a minute, we reuse the same year after year, and its visual aspect is nice!\n\nAlso, we only do these calculations and reservations twice a year, so we donâ€™t spend too much time working on this, while still refining more often than once each year.\nEach time, the process takes two of us7 about two hours, or one day per year in totalâ€¦ And the most time-consuming part is talking to our colleagues who are heavy DynamoDB users, and asking them â€œare you planning to reduce the consumption of your project over the coming year?â€\n\nFinally, letâ€™s create those reservations!\n\nWe calculated how many WCUs and how many RCUs we should reserve to achieve the best possible savings, hoping the week we chose to base our calculations on was actually a representative week.\n\nA commitment: be carefulâ€¦\n\nA reservation commits us to pay for a year, whether we use this capacity or not.\n\nSo, itâ€™s always a good idea to take a moment to validate with our colleagues that they are not planning to use less DynamoDB in the near future.\nOf course, the answer is often partly â€œit dependsâ€, since usage depends on new projects as well as on the traffic on our platforms, but if we can already anticipate the next planned optimizations, itâ€™s always a good thing.\n\nIn November 2022, we can only open DynamoDB reservations for one year if we work in the AWS Paris region.\nOther regions (us-east-1 for example) allow reservations for three years, which means more substantial savings. On the other hand, would we be willing to commit for three years and lose a major advantage of The Cloud, its flexibility?\n\nWhich account to reserve on?\n\nThe documentation says (emphasis mine):\n\n\n  If you have multiple accounts linked with consolidated billing, reserved capacity units purchased either at the payer account level or linked account level are shared with all accounts connected to the payer account.\nReserved capacity is applied first to the account that purchased it and then any unused capacity is applied to other linked accounts.\n\n\nWe have configured our AWS accounts to have a single payer account.\nWe have decided to make all our reservations in this account and they are applied to the child accounts without discrimination.\nThis applies to DynamoDB but also to RDS, EC2, Elasticacheâ€¦\n\nReserving!\n\nTo reserve, we go through the AWS DynamoDB Web console, in our payer account, in the region where these reservations will be used.\n\nOn this screen, you can see how many WCUs and RCUs we have already reserved.\nSince we make several reservations during the year, the reservations already in progress are to be subtracted from the values calculated above!\n\n\n\nTo create a new reservation, click on â€œPurchase reserved capacityâ€ and fill in the formÂ ;-)\n\n\n\nAfter reserving, viewing the costs\n\nOnce the reservations are made, in AWS Cost Explorer, the upfront cost is clearly visible.\nIt is charged at once, on the day we opened the reservation:\n\n\n\nTo have a daily view of WCU/RCU costs (reserved + provisioned in addition to reservations), remember to fill in â€œShow costs as: Amortized costsâ€ to smooth the monthly price of reservations over all days of the month:\n\n\n\n\n  Reservations and one payer account\nSince reservations, which cover the bulk of our DynamoDB costs, are made on our payer account, the bulk of our DynamoDB costs go back to this accountâ€¦ And not to the tenant/environment accounts.\nGood luck tracking costs and allocating them to projects and teams ğŸ’ª\n\n\nConclusion\n\nWe work with DynamoDB a lot, for several dozen microservices, and we face several types of infrastructure costs: on-demand reads/writes, provisioned reads/writes, storage, backups.\nIn exchange for a loss of flexibility and through reservations that commit us for a year, AWS allows us to reduce the cost of provisioned reads/writes.\n\nDetermining how much to reserve, in the face of a constantly changing load, is not easy.\nWe need to have a certain vision on the evolution of usage, over a year, and must accept to lose flexibility.\nAnd we need to find the right values to reserve for read and write capacity.\n\nWith three or four years of hindsight, by making reservations twice a year and by following the method detailed in this article, we realize savings of about 30% to 35% on our read and write capacity in provisioned mode.\nOn our scale, this saving represents several tens of thousands of dollars per year â€“ which is great, considering we only spend a few hours working on this every six months!\n\n\n\n  \n    \n      DynamoDB is one of the most serverless services we use and I like it a lot. Still, there are a few admin tasks left in our hands. Typically, we have to specify the capacity we need and configure an auto-scaler. We also have to enable encryption, backups, to setup permissions â€“ and to check all this is done, for all tables, managed by many teams.Â &amp;#8617;\n    \n    \n      If you do store a lot of data for a long time in DynamoDB, take a look at Standard-IA, it might help you reduce costs.Â &amp;#8617;\n    \n    \n      Why do we use pay-per-request so much? Well, in short, because this mode is more flexible than the provisioned one, and several of our projects are willing to pay much more in exchange for this flexibility.Â &amp;#8617;\n    \n    \n      DynamoDB in on-demand mode and scalability: in practice, AWS hides whatâ€™s going on, but doesnâ€™t scale to infinity instantly either.Â &amp;#8617;\n    \n    \n      50% is kind of the maximum possible saving we can achieve if our usage is flat and we reserve exactly what we provision. Flat usage might be what you see on your applications, but itâ€™s not how our platform works!Â &amp;#8617;\n    \n    \n      At Bedrock, we have a dedicated billing account â€“ a â€œpayer accountâ€ â€“ that aggregates costs from all our other accounts. Reservations are also shared amongst all (whitelisted) accounts that have a shared payer account.Â &amp;#8617;\n    \n    \n      For these kind of calculations and reservations, we usually work in pair, as this involves large amounts of money. Lowering risk of doing a costly mistake is quite a good idea.Â &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Ce que nous retenons de la droidcon London 2022",
    "category" : "",
    "tags"     : " android, droidcon, conference",
    "url"      : "/2022/11/22/droidcon-london-2022.html",
    "date"     : "November 22, 2022",
    "excerpt"  : "La communautÃ© Android a apportÃ© le soleil sur Londres les 27 et 28 octobre 2022. La droidcon London a rÃ©uni plus de 1400 dÃ©veloppeurs autour de lâ€™Ã©cosystÃ¨me Android, de ses outils et enjeux actuels. Jetpack Compose, Ã©videmment, mais aussi Gradle, ...",
  "content"  : "La communautÃ© Android a apportÃ© le soleil sur Londres les 27 et 28 octobre 2022. La droidcon London a rÃ©uni plus de 1400 dÃ©veloppeurs autour de lâ€™Ã©cosystÃ¨me Android, de ses outils et enjeux actuels. Jetpack Compose, Ã©videmment, mais aussi Gradle, modularisation, optimisation et autres sujets plus divers ont Ã©tÃ© abordÃ©s lors de ce rendez-vous incontournable pour la communautÃ©.\n\n\n\n\n  Ã‡a compile ? - Rafi Panoyan    \n      Vous reprendrez bien un peu de Gradle Enterprise ?\n      Dessine-moi un module\n      Trucs et astuces\n    \n  \n  Design the world - Damien Cuny    \n      To Compose\n      Design System\n      Vers lâ€™infini et au-delÃ \n    \n  \n  La gestion des erreurs - David Yim    \n      VÃ©rification des entrÃ©es\n      Le type Either\n      Kotlin Result\n      Conclusion\n    \n  \n  Ã€ la prochaine !\n\n\nÃ‡a compile ? - Rafi Panoyan\n\nLes sujets de compilation ont tenu une place trÃ¨s importante lors de cette Ã©dition de la droidcon Londres 2022. \nQuâ€™il sâ€™agisse dâ€™optimiser ses temps de compilation, de repenser la crÃ©ation de modules et des dÃ©pendances entre eux, de factoriser les logiques des scripts de compilation, \nnous avons eu une emphase claire sur lâ€™importance dâ€™adresser ces sujets.\n\nVous reprendrez bien un peu de Gradle Enterprise ?\n\nNelson Osacky, qui travaille chez Gradle, a prÃ©sentÃ© tous les outils que la formule Gradle Entreprise met Ã  disposition des dÃ©veloppeurs pour analyser en dÃ©tail les compilations.\n\nVous voulez vÃ©rifier que la compilation incrÃ©mentale est bien appliquÃ©e partout oÃ¹ cela est possible ? Un script permet de comparer, dans des conditions reproductibles, \nles entrÃ©es et sorties de vos builds, et analyse les tÃ¢ches empÃªchant ce mÃ©canisme central dans la rÃ©duction des temps de compilation.\n\nVous voulez vous assurer que Gradle est bien capable de retrouver le cache de vos tÃ¢ches sur un mÃªme poste ou bien depuis le cloud ? \nLÃ  aussi des outils vous permettent dâ€™identifier prÃ©cisemment les points qui ne tirent pas parti de ces mÃ©canismes.\n\nOn regrettera que ces outils soient disponibles uniquement pour la formule payante de Gradle. Cependant, les scans Gradle sont, eux,\ngratuits et illimitÃ©s, et permettent tout de mÃªme de mesurer et comparer des compilations et ainsi suivre lâ€™impact des diffÃ©rentes optimisations que vous pourriez apporter.\n\nDessine-moi un module\n\nLa modularisation ayant un impact sur les temps de compilation, plusieurs confÃ©rences ont abordÃ© ce sujet trÃ¨s en vogue dans la communautÃ© Android.\n\nUn point de vue intÃ©ressant de Josef Raska nous invite Ã  nous poser la question de la pertinence de modulariser selon le contexte. \nNe pas suivre une tendance mais se poser la question de lâ€™utilitÃ© dâ€™un nouveau module, et encore plus de ses dÃ©pendances avec les autres modules. \nVoilÃ  des propos qui invitent Ã  mesurer concrÃ¨tement lâ€™impact du chantier de la modularisation dans nos applications.\n\nAinsi, si on peut penser que modulariser permet de rÃ©duire les temps de compilation (en tirant parti de la parallÃ©lisation des tÃ¢ches par exemple), \nun chemin de dÃ©pendances trop long entre le module initial et la dÃ©pendance la plus profonde va entraÃ®ner une augmentation du temps de compilation.\n\nVigilance, donc, sur les â€œhubs de dÃ©pendancesâ€ (ces dÃ©pendances dont beaucoup de modules ont besoin, et qui ont besoin de beaucoup de modules).\n\n\n  \n  1. Hub de dÃ©pendances\n\n\n\nDe la mÃªme maniÃ¨re, un chemin de dÃ©pendances de trop grande profondeur ne permettra pas de tirer parti de la parallÃ©lisation des tÃ¢ches de compilation.\nSur le schÃ©ma ci-dessous, on peut voir quâ€™un chemin de profondeur 4 existe pour aller du module applicatif vers le module le plus bas dans la hiÃ©rarchie.\n\n\n  \n  2. Profondeur de dÃ©pendances\n\n\n\nJosef Raska propose le schÃ©ma suivant avec un dÃ©coupage API/implÃ©mentation afin de rÃ©duire au maximum cette profondeur, et ainsi compiler plus rapidement.\n\n\n  \n  3. Profondeur de dÃ©pendances rÃ©duite\n\n\n\nAndroid Studio et son analyse de dÃ©pendances peut Ãªtre trÃ¨s utile pour vÃ©rifier et mesurer cela.\nJosef Raska a dâ€™ailleurs crÃ©Ã© un plugin Gradle afin de spÃ©cifier ces rÃ¨gles Ã  lâ€™echelle dâ€™un projet et de sâ€™assurer quâ€™elles soient respectÃ©es : modules-graph-assert.\n\nTrucs et astuces\n\nAprÃ¨s ces conseils trÃ¨s avisÃ©s mais structurellement chronophages Ã  mettre en place (surtout sur de gros projets dÃ©jÃ  crÃ©Ã©s), dâ€™autres confÃ©renciers se sont plutÃ´t tournÃ©s vers les â€œquick-winâ€. Des changements peu coÃ»teux, aux gains plus modestes mais qui sâ€™additionnent, il en existe quelques-uns.\n\nAinsi, si Gradle nous permet dâ€™activer des fonctionnalitÃ©s de caching (org.gradle.unsafe.configuration-cache=true pour gagner du temps lors de la phase de configuration par exemple), il est aussi possible de dÃ©sactiver des fonctionnalitÃ©s du plugin Android si elles ne nous sont pas utiles.\n\nVoici une petite liste des propriÃ©tÃ©s qui sont activÃ©es par dÃ©faut, mÃªme lorsquâ€™elles ne sont pas utilisÃ©es dans les modules :\nandroid {\n  buildFeatures {\n    buildConfig false\n    aidl false\n    renderScript false\n    resValues false\n    shaders false\n  }\n}\n\n\nSi vous nâ€™utilisez pas les valeurs liÃ©es Ã  la configuration de votre compilation, ne gÃ©nÃ©rez pas de BuildConfig.\nSi vous nâ€™avez pas de resources dans votre module, dÃ©sactivez la gÃ©nÃ©ration de resValue !\n\nRetrouvez ici la liste de ces fonctionnalitÃ©s, leur utilitÃ© et leurs valeurs par dÃ©faut : BuildFeatures.\n\nDesign the world - Damien Cuny\n\nIl y a un peu plus dâ€™un an sortait la version 1.0 de Jetpack Compose, le nouveau toolkit dÃ©claratif pour la crÃ©ation dâ€™interfaces Android. Dâ€™autre part, le design system Material Design 3 vient de sortir en version stable et son implÃ©mentation Compose Material sont Ã©galement disponibles.\nAvec tout cela, le design a, cette annÃ©e encore, tenu une place de choix dans lâ€™agenda de cette droidcon 2022 Ã  Londres.\nMais comment utiliser tout cela correctement ? Comment sâ€™en servir pour implÃ©menter un design system personnalisÃ© ? Jusquâ€™oÃ¹ peut-on aller ?\nAutant de questions auxquelles ont tentÃ© de rÃ©pondre les nombreuses prÃ©sentations sur le sujet.\n\nTo Compose\n\nCompose facilite beaucoup de choses dans lâ€™implÃ©mentation et le maintien dâ€™interfaces sur Android. Cependant, cela nÃ©cessite de rÃ©apprendre Ã  faire certaines choses que lâ€™on maÃ®trise dÃ©jÃ  avec le systÃ¨me de View.\nDessiner dans un canvas en est une, et Himanshu Singh dans sa prÃ©sentation â€œComposing in your canvasâ€, nous montre les piÃ¨ges Ã  Ã©viter pour rÃ©aliser cela avec Compose.\n\nLa recomposition peut Ã©galement Ãªtre source de problÃ¨mes et de latences si Compose est mal utilisÃ©. Dans sa prÃ©sentation â€œUnderstanding recomposition performance pitfallsâ€, Jossi Wolf et Andrei Shikov nous donnent, Ã  partir dâ€™un exemple concret, les meilleures astuces pour lâ€™utiliser Ã  bon escient.\n\nDesign System\n\nEn faisant le parallÃ¨le avec la saga Ã©pique de JRR Tolkien, Daniel Beleza, dans sa prÃ©sentation â€œOne design system to rule them allâ€, nous explique comment il a rÃ©ussi, tout en se passant de Material design, Ã  unifier et automatiser son propre design system.\nCela demande, Ã©videmment, une collaboration totale de la part de lâ€™Ã©quipe de design, mais une fois cette intÃ©gration faite, les bÃ©nÃ©fices et lâ€™autonomie se ressentent de part et dâ€™autre.\nDes outils tel que Figma, Kotlin Poet ou des plugins Android Studio custom lui ont permis dâ€™automatiser ensuite ce processus.\n\nMaterial Design est un design system. Il a lâ€™avantage dâ€™Ãªtre bien documentÃ©, uniforme et rÃ©guliÃ¨rement enrichi. De plus, il est dÃ©jÃ  implÃ©mentÃ© dans lâ€™ancien systÃ¨me de View Android et plus rÃ©cemment dans Jetpack Compose avec Compose Material.\n\nUne des diffÃ©rences majeures entre Compose et le systÃ¨me de View sur Android est son dÃ©coupage. Dans Compose, Material nâ€™est implÃ©mentÃ© et nâ€™apparaÃ®t que dans la partie la plus hautes alors que dans le systÃ¨me de View, son implÃ©mentation est rÃ©partie dans toutes les couches de la librairie.\nIl est donc assez complexe de se passer de Material avec le systÃ¨me de View mais cela est complÃ©tement envisageable, voire recommandÃ©, dans certains cas avec Compose.\n\n\n\nPour illustrer cela Sebastiano Poggi (la moitiÃ© de Coding with the italians) est venu nous prÃ©senter, dans â€œCompose beyond Materialâ€, les questions Ã  se poser avant de se lancer dans son design system et comment le package Foundation de Compose peut nous aider.\n\nPour terminer il nous donne de nombreux conseils concrets sur lâ€™implÃ©mentation de composants sans Material. Le principal, rejoint la prÃ©sentation dâ€™introduction de cette droidcon, â€œThe Silver Bullet Syndrome Directorâ€™s Cut - Complexity Strikes Back!â€, un bon design system est un design system qui correspond Ã  nos besoins et qui y rÃ©pond le plus simplement possible.\n\nVers lâ€™infini et au-delÃ \n\nChris Bane et Nacho Lopez dans leur prÃ©sentation â€œBranching out Jetpack Composeâ€, nous ont racontÃ© comment lâ€™aventure du passage Ã  Compose sâ€™est dÃ©roulÃ©e chez Twitter, une des premiÃ¨res apps Ã  lâ€™adopter.\nAvec une code base aussi consÃ©quente (plus de 1000 modules, dont 300 pour le design, rÃ©partis sur plus de 30 Ã©quipes), ils ont dÃ» progressivement convaincre les Ã©quipes, les former et les accompagner.\nLa question de continuer Ã  utiliser Material Design sâ€™est Ã©galement posÃ©e chez eux. Ils lâ€™ont dans un premier temps conservÃ© pour faciliter le passage sur Compose, pour finalement le retirer complÃ¨tement en se basant, eux aussi, sur le package Foundation.\nLeur prÃ©sentation rÃ©sume bien lâ€™ensemble des Ã©tapes et des questions par lesquelles ils sont passÃ©s pour accomplir cette transition.\n\nAfin de remettre les choses en perspective, Ash Davies nous rappelle que Compose est un simple pattern de dÃ©veloppement multiplateforme. De ce fait, il peux Ãªtre appliquÃ© Ã  autre chose quâ€™Ã  de lâ€™UI comme le propose Jetpack Compose. Il nous explique dans â€œDemystifying Molecule: Running Your Own Compositions For Fun And Profitâ€, comment lâ€™appliquer Ã  la couche domaine dâ€™un projet pour le â€œFunâ€.\n\nLa gestion des erreurs - David Yim\n\nLa gestion des erreurs a Ã©tÃ© le sujet de plusieurs prÃ©sentations Ã  la droidcon. Ces prÃ©sentations avaient pour objectif de servir de piqÃ»re de rappel sur lâ€™importance de bien prendre en compte ce problÃ¨me concernant tous les dÃ©veloppeurs. Aujourdâ€™hui, nous avons tous les outils pour gÃ©rer facilement nos erreurs. Cependant, par paresse et comme nous prÃ©fÃ©rons penser de maniÃ¨re positive, nous ne pensons souvent quâ€™aux cas de succÃ¨s et les cas dâ€™erreurs sont souvent brouillons voire ne sont mÃªme pas spÃ©cifiÃ©s.\n\nLes speakers mâ€™ont marquÃ© avec un exemple de mauvaise gestion dâ€™erreur qui a coÃ»tÃ© plusieurs centaines de milliers de dollars. Lâ€™exemple parlait dâ€™une faille sur le site japonais de 7-Eleven, une chaÃ®ne de supÃ©rettes, dont elle a Ã©tÃ© victime. Dans la base de donnÃ©es de ce projet, les dÃ©veloppeurs ont ajoutÃ© un champ â€œdate de naissanceâ€ comme nullable. Plus tard, ce champ est devenu non-nullable. Par paresse, le dÃ©veloppeur qui a rendu ce champ non-nullable a mis par dÃ©faut un 1er janvier 2019 sur cette date lorsquâ€™elle nâ€™Ã©tait pas renseignÃ©e, simplement pour satisfaire son compilateur. Le problÃ¨me est que ce champ fut plus tard utilisÃ© dans la fonctionnalitÃ© de mot de passe oubliÃ© du site. En utilisant la date par dÃ©faut du 1er janvier 2019, un hacker a pu rÃ©cupÃ©rer des comptes utilisateurs et voler des informations bancaires. Cet exemple mâ€™a marquÃ© par lâ€™habitude que nous avons en tant que dÃ©veloppeur de nous soucier que de satisfaire notre compilateur plutÃ´t que de vraiment discuter de solutions rÃ©flÃ©chies Ã  nos problÃ¨mes techniques.\n\nPlusieurs mÃ©thodes de gestion des erreurs existent et les speakers en ont prÃ©sentÃ©s quelques-unes.\n\nVÃ©rification des entrÃ©es\n\nLâ€™une des mÃ©thode pour Ãªtre certain de ne pas avoir de problÃ¨me est de vÃ©rifier les donnÃ©es que lâ€™on reÃ§oit. Prenons un exemple simple :\n\ndata class User(val email: String)\n\n\nRien nâ€™empÃªche dâ€™instancier cette classe de la maniÃ¨re suivante :\n\nval user = User(email = &quot;&quot;)\n\n\nCela peut crÃ©er des problÃ¨mes par le futur, alors quâ€™il y a un moyen dâ€™Ã©viter cela\n\nclass Email(value: String) {\n    val value: String =\n        if (value.isEmpty() || !Regex(EMAIL_FORMAT).matches(value)) {\n            throw Exception(&quot;Email format is not correct&quot;)\n        } else {\n            value\n        }\n}\n\ndata class User(private val email: Email)\n\n\nCette mÃ©thode peut paraÃ®tre un peu exagÃ©rÃ©e dans cet exemple. Mais dans un contexte oÃ¹ la classe User serait utilisÃ©e par un grand nombre dâ€™Ã©quipes et que les rÃ¨gles mÃ©tier de lâ€™Email serait complexe, cette mÃ©thode prendrait tout son sens pour Ã©viter dâ€™avoir de mauvaises surprises !\n\nLe type Either\n\nLe type Either est un moyen de diffÃ©rencier les cas de succÃ¨s des cas dâ€™erreurs. Il est disponible dans la librairie Arrow ou facilement reproduisible :\n\nsealed class Either&amp;lt;out A, out B&amp;gt; {\n    class Left&amp;lt;A&amp;gt;(val value: A): Either&amp;lt;A, Nothing&amp;gt;()\n    class Right&amp;lt;B&amp;gt;(val value: B): Either&amp;lt;Nothing, B&amp;gt;()\n}\n\n\nLâ€™utilisation de ce type est quâ€™il est soit un type A, soit un type B. On peut ainsi dÃ©finir par exemple que le A est un cas de succÃ¨s et que le type B est un cas dâ€™erreur.\n\ndata class User(val name: String)\n\nvar either: Either&amp;lt;User, Exception&amp;gt;\n\nwhen (either) {\n    is Either.Left -&amp;gt; {\n        println(&quot;The user is called ${(either as Either.Left&amp;lt;User&amp;gt;).value.name}&quot;)\n    }\n    is Either.Right -&amp;gt; {\n        (either as Either.Right&amp;lt;Exception&amp;gt;).value.printStackTrace()\n    }\n}\n\n\nGrÃ¢ce Ã  ce type, on peut par exemple savoir si un appel Ã  une API a rÃ©ussi ou non, ce qui nous permet de gÃ©rer plus facilement nos cas dâ€™erreurs.\n\nKotlin Result\n\nLa classe Result est similaire au type Either et a pour avantage dâ€™Ãªtre directement inclue dans Kotlin et que lâ€™on nâ€™a pas Ã  se synchroniser pour savoir si le cÃ´tÃ© gauche est le cas de succÃ¨s ou dâ€™erreur.\n\ndata class User(val name: String)\n\nvar result: Result&amp;lt;User&amp;gt;\n\nwhen {\n    result.isSuccess -&amp;gt; {\n        println(&quot;The user is called ${result.getOrNull()?.name}&quot;)\n    }\n    result.isFailure -&amp;gt; {\n        result.exceptionOrNull()?.printStackTrace()\n    }\n}\n\nOU\n\nresult.onSuccess { user -&amp;gt;\n    println(&quot;The user is called ${user.name}&quot;)\n}.onFailure { throwable -&amp;gt;\n    throwable.printStackTrace()\n}\n\n\nConclusion\n\nPlusieurs mÃ©thodes existent pour prendre en compte nos cas dâ€™erreurs. Laquelle est la meilleure ? Eh bien vous vous y attendez sÃ»rement, mais Ã§a dÃ©pend ! On choisira une mÃ©thode ou une autre selon ce qui nous arrange par rapport Ã  la situation, nos choix dâ€™outils techniques ou nos effectifs. Lâ€™important Ã©tant de prendre en compte ces cas dâ€™erreurs et de ne pas laisser leur rÃ©solution au hasard. Les cas dâ€™erreurs ne sont en fait que dâ€™autres usecases de lâ€™utilisateur et souvent ne sont pas des edgecase. Ils mÃ©ritent donc dâ€™Ãªtre tout autant rÃ©flÃ©chis et spÃ©cifiÃ©s que les cas de succÃ¨s !\n\nÃ€ la prochaine !\n\nIl est toujours intÃ©ressant de mesurer lâ€™engouement pour tel ou tel sujet dans la communautÃ© Android en analysant les prÃ©sentations lors des diffÃ©rentes confÃ©rences technologiques.\n\nSans aucun doute, cette droidcon Ã©tait sous le signe de Jetpack Compose, qui bÃ©nÃ©ficie dâ€™un suivi et dâ€™un engagement fort de Google et de toute la communautÃ©.\nTout lâ€™enjeu ici est de rester au contact des innovations et de lâ€™Ã©volution de la plateforme Android, et Jetpack Compose offre un dÃ©fi que nous avons commencÃ© Ã  relever chez Bedrock.\n\nNous attendons avec impatience de voir oÃ¹ va Android, et avons Ã  coeur de participer Ã  cette aventure qui nous lie tous !\n\n\n"
} ,
  
  {
    "title"    : "Ce que nous avons retenu de la SymfonyCon - Disneyland Paris 2022",
    "category" : "",
    "tags"     : " conferences, backend, symfony, php",
    "url"      : "/2022/11/17/symfonycon.html",
    "date"     : "November 17, 2022",
    "excerpt"  : "En cette fin dâ€™annÃ©e, une petite Ã©quipe de chez Bedrock a assistÃ© au grand retour de la SymfonyCon 2022 aprÃ¨s 3 ans dâ€™absence. \nNous avons eu la chance de dÃ©couvrir les nouveautÃ©s liÃ©es Ã  Symfony 6.2 et dâ€™assister aux confÃ©rences sur de nombreux s...",
  "content"  : "En cette fin dâ€™annÃ©e, une petite Ã©quipe de chez Bedrock a assistÃ© au grand retour de la SymfonyCon 2022 aprÃ¨s 3 ans dâ€™absence. \nNous avons eu la chance de dÃ©couvrir les nouveautÃ©s liÃ©es Ã  Symfony 6.2 et dâ€™assister aux confÃ©rences sur de nombreux sujets techs Ã  Disneyland.\nLa keynote prÃ©sentÃ©e par Fabien Potencier, le crÃ©ateur de Symfony, nous donne un avant-goÃ»t des nouvelles fonctionnalitÃ©s qui seront prÃ©sentes dans la future version de Symfony.\nAu programme, un nouveau composant Webhooks ainsi que lâ€™Ã©volution du composant Mailer.\n\nUnleashing the power of lazy objects in PHP\n\nNicolas Grekas, habituÃ© de la scÃ¨ne PHP et membre de la Symfony Core Team, nous a prÃ©sentÃ© les diffÃ©rentes faÃ§ons dâ€™utiliser les lazy objects en PHP et plus spÃ©cifiquement Ã  lâ€™aide de Symfony.Â \n\nLes lazy objects sont des objets instanciÃ©s vides qui peuplent leurs propriÃ©tÃ©s eux-mÃªmes seulement quand ils sont utilisÃ©s.\nIls sont utiles lors de lâ€™instanciation de lourds objets peu appelÃ©s, cela permet de faire du lazy loading.\n\nNicolas nous a aussi prÃ©sentÃ© deux nouveaux traits prochainement disponibles sur Symfony 6.2 permettant de travailler avec ces objets.\nLes VirtualInheritanceProxies et les GhostsObjects sont deux nouvelles possibilitÃ©s pour implÃ©menter plus facilement des lazy objects en plus du ValueHolder.\n\nAdvanced Git magic\n\nPauline Vos nous reprÃ©sente GIT en dehors de son usage quotidien, comment aller plus loin que les pull, commit, push et merge habituels, elle nous livre donc une prÃ©sentation sur une mÃ©thode de debug en utilisant GIT.\n\nRetour dans un premier temps sur lâ€™importance des commits dit â€œatomicâ€ avec comme rÃ¨gles :\n\n  Chaque commit se rÃ©sume Ã  un fix ou une feature\n  Chaque commit doit fonctionner (tous les tests doivent passer)\n  Chaque message et description doivent Ãªtre clairs et concis\n\n\nPauline introduit ensuite la commande git rebase -i qui permet un rebase interactif servant notamment Ã  rÃ©Ã©crire notre historique.\n\n\n\nVient ensuite lâ€™utilisation de la commande git reflog, commande avec laquelle nous pouvons obtenir le dÃ©tail des commandes lancÃ©es sur la branche, elle peut de ce fait Ãªtre utile pour rÃ©parer une erreur.\n\nComment utiliser toutes ces commandes GIT pour debugger ?\nUne dÃ©monstration de la commande git bisect et de toutes ses options qui permettent dâ€™identifier le commit qui a introduit le bug en faisant une recherche dichotomique.\n\nPauline pousse la rÃ©flexion plus loin en alliant la commande git bisect avec un script de debug ou un test unitaire.\n\nâ€œWrite it, push it and find where it breaksâ€\n\nAfin de tirer parti de cette mÃ©thode, il est important que chaque commit soit fonctionnel, tous les tests doivent donc passer.\n\nVous pouvez Ã©galement retrouver un article de Pauline Ã  ce sujet sur son blog personnel.\n\nSchrÃ¶dingerâ€™s SQL - The SQL inside the Doctrine box\n\nAu dÃ©but de sa confÃ©rence Claudio Zizza insiste sur le fait de bien connaÃ®tre nos bases de donnÃ©es et le langage utilisÃ© pour les requÃªter, il Ã©voque notamment les diffÃ©rences entre MySQL et PostgreSQL. Ensuite, il nous a parlÃ© de Doctrine ORM, des fonctions que nous apprÃ©cions tant, car elles nous facilitent la vie. \nPuis, il a rappelÃ© lâ€™importance de savoir faire des requÃªtes SQL mÃªme si nous utilisons Doctrine. Comprendre le SQL peut nous permettre dâ€™optimiser notre utilisation de Doctrine et donc de mieux apprÃ©hender son fonctionnement. \nIl insiste aussi sur le fait que SQL â€œseulâ€ est bien plus puissant que DQL (Doctrine Query Language). \nPour terminer, Claudio nous a donnÃ© quelques recommandations de lecture pour apprendre le SQL ainsi quâ€™un site permettant de tester nos requÃªtes.\n\nAdvanced Test Driven Development\n\nDurant cette confÃ©rence, Diego Aguiar, dÃ©veloppeur de chez SymfonyCasts nous rappelle ce quâ€™est le TDD (Test Driven Development), son histoire, les diffÃ©rentes techniques ainsi que des astuces afin de nous dÃ©bloquer et bien sÃ»r les cas oÃ¹ il ne semble pas utile dâ€™utiliser le TDD.\n\nÃ€ retenir, le TDD est une discipline, il faut beaucoup dâ€™entraÃ®nement et rÃ©pÃ©ter continuellement ces exercices.\n\n\n\nâ€œFake it till you make itâ€, il sâ€™agit dâ€™abord dâ€™Ã©crire son code de test en utilisant par exemple des assertions, des tests avec diffÃ©rentes sorties et ensuite de produire le code qui va rÃ©soudre ces tests.\n\n\n\nâ€œ Write your test code, produce it and repeat.â€\n\nPourquoi nous retrouvons nous parfois bloquÃ©s ?\n\n  Les tests Ã©crits sont peut-Ãªtre faux\n  Les tests ne sont pas assez segmentÃ©s\n  Le code Ã©crit est peut-Ãªtre trop spÃ©cifique\n\n\nComment se dÃ©bloquer ?\n\n  Continuer et trouver un test plus simple\n  Refactoriser le code en production qui met en difficultÃ©\n  Ã‰crire les diffÃ©rents use-cases\n  Passer outre les tests un instant\n\n\nLes cas les plus favorables au TDD sont les nouvelles fonctionnalitÃ©s qui nâ€™ont pas de lien avec du code legacy. En ce qui concerne les cas non pertinent au TDD, nous retrouvons les cas de configuration, de dÃ©couverte de code et de requÃªtes.\n\nPour conclure, Diego nous rappelle que le TDD est bien Ã©videmment plus un outil quâ€™une rÃ¨gle.\n\nDynamic Validation With Symfony\n\nTout en se basant sur les Ã©volutions des PokÃ©mon, Marion Hurteau a introduit le principe de validation dynamique. Par exemple, vÃ©rifier que le nom de notre PokÃ©mon contient bien 10 caractÃ¨res, ou encore les diffÃ©rentes rÃ¨gles dâ€™Ã©volution en fonction du type de PokÃ©mon. \nÃ€ lâ€™aide dâ€™exemples de code qui sont disponibles sur son repo Git, elle a passÃ© en revue les faÃ§ons dâ€™implÃ©menter des validations Ã  lâ€™aide du composant Symfony Validator.\nAu fil de sa prÃ©sentation, la complexitÃ© des contraintes croÃ®t ce qui permet de voir un Ã©ventail de possibilitÃ©s.\n\nFrom monolith to decoupledâ€¦wait, why is that one getting bigger?!?\nLors de cette confÃ©rence, Shawna Spoor est venue nous parler de comment dÃ©couper un monolithe en une multitude de microservices grÃ¢ce au â€œStrangler Fig Patâ€. Elle a commencÃ© par nous rappeler les avantages et les inconvÃ©nients des microservices comparÃ© Ã  un monolithe.\n\nSuite Ã  cela, elle nous a donnÃ© les diffÃ©rentes Ã©tapes pour dÃ©couper une application monolithe en micro-services et cela sans jamais arrÃªter le dÃ©veloppement de nouvelles features:\n\n  Choisir une fonctionnalitÃ© qui peut Ãªtre dÃ©coupÃ©e\n  CrÃ©Ã© le nouveau Service\n  DÃ©placer le trafic vers le nouveau service\n  Recommencer jusquâ€™Ã  la disparition du monolithe\n\n\nOn peut rÃ©sumer ce pattern via lâ€™image ci-dessous, le tronc reprÃ©sente le monolithe et les branches qui lâ€™Ã©tranglent lentement correspondent aux micro-services.\n\n\nFrom a legacy Monolith to a Symfony Service Oriented Architecture with zero downtime\nLors de la confÃ©rence prÃ©sentÃ©e par ClÃ©ment Bertillon, nous avons pu voir comment son Ã©quipe a transformÃ© leur ancienne application monolithe composÃ©e de milliers de fichiers PHP en un monorepo dÃ©composÃ© en micro-services en utilisant le Strangler Fig pattern et cela sans aucune rupture de service ni arrÃªter le dÃ©veloppement de nouvelles features.\n\nDe maniÃ¨re trÃ¨s simplifiÃ©e, ils ont installÃ© Symfony, mis le code legacy dans un dossier Ã  la racine du projet, le routeur symfony permet dâ€™accÃ©der au nouveau micro-service tout en redirigeant vers le legacy si aucun contrÃ´leur nâ€™a Ã©tÃ© trouvÃ©. Il a conclu avec les rÃ¨gles dâ€™or et comment analyser les performances via Blackfire.\nCes deux confÃ©rences sur le Strangler fig paterne, mon permis de mettre en place un micro projet dans un de nos projets, tout en le cloisonnant du code parent (rÃ¨gles dâ€™or vÃ©rifiÃ© grÃ¢ce Ã  lâ€™outil prÃ©sentÃ© deptrac). Ce principe nous permettra de le transformer en micro service trÃ¨s facilement.\n\nPHPStan: Advanced Types\nCette confÃ©rence centrÃ©e sur lâ€™outil dâ€™analyse statique de code : PHPStan, a Ã©tÃ© prÃ©sentÃ©e par son crÃ©ateur OndÅ™ej Mirtes.\nIl a commencÃ© par nous rappeler quelle est la diffÃ©rence entre un langage compilÃ© et un langage interprÃ©tÃ©, le premier ne se compile pas sâ€™il y a des erreurs alors que le second ne plante quâ€™Ã  lâ€™exÃ©cution. Le but de PHPStan est de nous aider Ã  identifier toutes les erreurs sans avoir besoin dâ€™exÃ©cuter le code.\n\n\n\nCet outil analyse toutes les fonctions, les propriÃ©tÃ©s, le typage PHP, mais aussi la PHPDoc. OndÅ™ej nous a ensuite parlÃ© de tous les types PHPStan avec des exemples, en voici quelques un qui ont marquÃ© notre attention :\n\n  non-empty-array, non-empty-string\n  literal-string\n  integer-range, integer-mask, integer-maskof\n  conditional return types, union types, intersection types â€¦\n\n\nIl finit en nous rappelant que lâ€™utilisation de @var est une mauvaise pratique et quâ€™il vaut mieux renforcer le typage quitte Ã  modifier la documentation des vendor via les Stub files.\n\nGNAP: The future of OAuth\n[Slides]\n\nRobin Chalas @chalas_r nous a prÃ©sentÃ© GNAP (Grant Negotiation and Authorization Protocol) : une initiative pour dÃ©velopper la prochaine gÃ©nÃ©ration de protocoles dâ€™autorisation.\nPour mieux comprendre les enjeux, nous sommes repartis de lâ€™historique dâ€™oauth, ses Ã©volutions et ses Ã©cueils. Le constat Ã©tant que mÃªme si de nombreux problÃ¨mes connus ont Ã©tÃ© rÃ©solus, aujourdâ€™hui, pour bien utiliser OAuth 2, il faut lire une douzaine de RFC et sâ€™assurer quâ€™elles soient pertinentes pour les diffÃ©rents cas dâ€™utilisation.\n\nLâ€™augmentation de la complexitÃ© du protocole dÃ©grade lâ€™expÃ©rience du dÃ©veloppeur, ce qui va Ã  lâ€™encontre de son objectif principal qui est la simplicitÃ© pour les dÃ©veloppeurs de clients.\n\nGNAP (prononcÃ© â€œnapâ€) est une complÃ¨te rÃ©Ã©criture afin de rÃ©pondre aux besoins en sÃ©curitÃ© des applications modernesÂ :\n\n  PensÃ© pour tous les clients, plateformes (pas uniquement web, possibilitÃ©s de deeplinking mobile par exemple)\n  les interactions sont un concept clÃ©\n  Du chiffrement partout et des mÃ©canismes de rotation extensibles\n  Plusieurs Access Tokens / grant request\n  Gestion de lâ€™identitÃ© intÃ©grÃ©e\n  Plus developer friendly\n  Pas rÃ©trocompatible avec OAUTH2\n\n\nCe protocole est toujours Ã  lâ€™Ã©tat de brouillon, le groupe de travail a Ã©tÃ© montÃ© en octobre 2020 et lors du dernier rassemblement (nov. 2022), aucune modification du protocole nâ€™a Ã©tÃ© actÃ©e. Le speaker conclut sur la nÃ©cessitÃ© de commencer Ã  travailler sur lâ€™implÃ©mentation de ce protocole dans lâ€™Ã©cosystÃ¨me PHP afin de supporter ce nouveau standard dont la finalisation ne devrait plus tarder. \nPour aller plus loin https://oauth.xyz/\n\nA self-training journey to the certification Symfony\nCette confÃ©rence traite de la mÃ©thodologie et des bonnes pratiques pour obtenir la fameuse certification Symfony.\nEn effet, la confÃ©renciÃ¨re, Camille Jouan, nous prÃ©sente sa maniÃ¨re de prÃ©parer lâ€™examen.\nElle commence par Ã©noncer son plan dâ€™action :\n\n  Rassembler un maximum dâ€™informations (Symfony doc, site pour la prÃ©paration Ã  la certification, etc)\n  Organiser un plan autour du quoi/comment/pourquoi\n  Faire une timeline avec les Ã©tapes prÃ©vues\n  Se fixer un objectif dans le temps\n\n\nLâ€™idÃ©e est dâ€™accepter que cela ne sera pas parfait et que des retouches vont y Ãªtre apportÃ©es\n\nPour Camille, lâ€™idÃ©al serait de pouvoir faire un test blanc au bout dâ€™un certain temps de prÃ©paration sans â€œgrandes convictionsâ€ : le but Ã©tant de se familiariser avec lâ€™exercice.\nSi les fonds sont disponibles, un training est proposÃ© par Sensiolabs.\nEn fonction de cet examen blanc, ajuster son plan, se concentrer sur des parties qui doivent Ãªtre approfondies.\nUn autre point sur lequel la confÃ©renciÃ¨re a insistÃ© est le monitoring : rÃ©guliÃ¨rement faire un bilan sur son avancÃ©e pour sâ€™adapter.\nDes outils comme Trello, Excel, Google permettent dâ€™en avoir une vision globale.\n\nIl est important de parler de ce projet autour de soi, notamment auprÃ¨s de ses proches pour avoir du soutien, mais Ã©galement auprÃ¨s de son entreprise qui peut Ã©ventuellement proposer une subvention et ou un amÃ©nagement du temps de travail.\n\nElle conclut son intervention par un dernier conseil : cette mÃ©thodologie est adaptable Ã  la vie quotidienne et peut Ãªtre utile dans dâ€™autres situations.\n\nNotre retour dâ€™expÃ©rience\nCette nouvelle Ã©dition de la SymfonyCon nous a permis de dÃ©couvrir ou dâ€™approfondir certaines connaissances. \nNous pouvons aussi nous rendre compte de notre travail quotidien et prendre du recul sur celui-ci.\nCette expÃ©rience anglophone Ã©tait trÃ¨s enrichissante et les confÃ©rences proposÃ©es Ã©taient variÃ©es.\nIl y avait de la rÃ©solution de problÃ¨mes techniques, des retours dâ€™expÃ©riences ou encore de la tÃ©lÃ©mÃ©trie.\n\n\n"
} ,
  
  {
    "title"    : "Un onboarding facilitÃ© grÃ¢ce Ã  la revue de code!",
    "category" : "",
    "tags"     : " team",
    "url"      : "/2022/11/15/onboarding-revue-code.html",
    "date"     : "November 15, 2022",
    "excerpt"  : "Au sein des Ã©quipes de dÃ©veloppement, une activitÃ© bien connue est celle de la revue de code, et \nplus \nprÃ©cisÃ©ment de la revue du delta du code. Il sâ€™agit de lâ€™inspection par nos \npairs du code proposÃ© par nos soins, qui se trouve ainsi commentÃ© ...",
  "content"  : "Au sein des Ã©quipes de dÃ©veloppement, une activitÃ© bien connue est celle de la revue de code, et \nplus \nprÃ©cisÃ©ment de la revue du delta du code. Il sâ€™agit de lâ€™inspection par nos \npairs du code proposÃ© par nos soins, qui se trouve ainsi commentÃ© pour rÃ©pondre aux \nexigences de qualitÃ© de lâ€™Ã©quipe et du projet.\n\nLes risques dâ€™incomprÃ©hensions inhÃ©rents Ã  la communication Ã©crite, de malentendus\nou encore les remarques malheureuses peuvent rendre cet exercice redoutÃ© tant par les \nrelecteurs et relectrices \nque \npar celles et ceux dont le code est relu.\n\nAvant dâ€™arriver chez Bedrock, jâ€™Ã©tais un peu \ninquiÃ¨te. Je savais dÃ©jÃ  que les 9 personnes de ma future Ã©quipe font tou(te)s de la revue de code. \nComment Ã©changerons-nous? Saurai-je faire â€œbonne impressionâ€ via mes commentaires \nsur leur code ?\n\nEn arrivant, jâ€™ai Ã©tÃ© trÃ¨s \nagrÃ©ablement surprise de dÃ©couvrir que lâ€™Ã©quipe applique un \nstandard, celui des conventions de commentaires, ou conventional comments. GrÃ¢ce Ã  cela, mon \nonboarding a Ã©tÃ© grandement facilitÃ© et jâ€™ai dÃ©couvert une faÃ§on plus efficace dâ€™Ã©crire mes \ncommentaires !\n\nDisclaimer : cet article est inspirÃ© de ma confÃ©rence â€œRevue de code : on nâ€™est pas \nvenu-e-s pour\nsouffrir !â€ donnÃ©e Ã  lâ€™occasion du meet-up anniversaire Duchess chez Dataiku en 2022 et au Forum\nPHP 2022.\n\n\n\nPetit rappel : pourquoi faisons-nous des revues de code ?\n\nPasser en revue le code proposÃ© par ses co-Ã©quipier(e)s est largement rÃ©pandu dans les \nÃ©quipes de dÃ©veloppeurs et dÃ©veloppeuses. Bien sÃ»r, la qualitÃ© du code en elle-mÃªme se trouve \namÃ©liorÃ©e \ncar chacun apporte un regard neuf sur ce qui est proposÃ©, mais ce nâ€™est pas tout. La \nrevue de code est Ã©galement une faÃ§on de nous tenir informÃ©(e) de \nlâ€™implÃ©mentation de nouvelles features, dâ€™apprendre autant du mÃ©tier que de la \ntechnique et enfin, dâ€™apprendre Ã  travailler ensemble.\n\n\n\nVoici une \npetite liste non exhaustive de lâ€™intÃ©rÃªt de la revue de code :\n\n\n  AmÃ©liorer la qualitÃ© et la lisibilitÃ© du code grÃ¢ce aux remarques de toutes les\npersonnes de lâ€™Ã©quipe\n  Appliquer les standards adoptÃ©s par lâ€™Ã©quipe (et les apprendre !)\n  DÃ©tecter et corriger les Ã©ventuels bugs fonctionnels\n  Favoriser la collaboration en Ã©quipe\n  Former les dÃ©veloppeurs et dÃ©veloppeuses au fur et Ã  mesure des remarques\n  Partager les responsabilitÃ©s : en approuvant une pull request ou une merge request, nous\nsommes responsables en tant quâ€™Ã©quipe du code ajoutÃ©/modifiÃ© au tronc commun !\n\n\nâ€¦Et parfois, on souffre\n\n\n\nMais parfois, ce nâ€™est pas tout rose. Les commentaires quâ€™on laisse peuvent vexer. On \npeut nous-mÃªme Ãªtre vexÃ©. Car certains jours, on peut manquer dâ€™empathie. On peut avoir \nlâ€™impression dâ€™Ãªtre plus compÃ©tent(e) en \ncritiquant les autres, on veut se rassurer en se montrant plus qualifiÃ©(e). On peut Ã©galement \nÃªtre habituÃ©(e) Ã  une culture de la compÃ©tition, nous poussant ainsi Ã  faire des remarques dÃ©sagrÃ©ables Ã  nos pairs.\n\nComment le formatage de commentaire a-t-il amÃ©liorÃ© mon arrivÃ©e dans lâ€™Ã©quipe ?\n\nLa standardisation des commentaires a Ã©normÃ©ment amÃ©liorÃ© mon intÃ©gration dans \nlâ€™Ã©quipe. En effet, grÃ¢ce Ã  cela :\n\n\n  Jâ€™ai pu rapidement me rendre compte de ce qui Ã©tait bloquant / non bloquant et ainsi me \nconcentrer sur les actions essentielles et prioritaires Ã  mener;\n  je nâ€™ai pas eu Ã  me poser de questions sur le ton employÃ© par mes collÃ¨gues ni sur leur \nintention;\n  jâ€™ai pu rapidement faire moi-mÃªme des revues de code sans craindre dâ€™Ãªtre mal comprise;\n  jâ€™ai eu des retours qui mâ€™ont permis de progresser sur la connaissance du fonctionnel et \ndes standards de la team.\n\n\nAmÃ©liorer sa posture\n\nAvant de parler du standard, je vous propose de nous interroger sur \nnotre posture en tant que dÃ©veloppeur et dÃ©veloppeuse. Recevoir ou donner des commentaires, ce \nnâ€™est pas aisÃ© pour tous. Notre ego peut interfÃ©rer et dÃ©grader la qualitÃ© de nos \nÃ©changes avec nos collÃ¨gues. Aussi, avant de chercher Ã  formater nos commentaires, nous pouvons \nnous interroger sur leur contenu.\n\n\n\nLâ€™Egoless Programming, proposÃ© par Gerald Weinberg en 1971 dans son livre The Psychology of \nComputer Programming, prÃ©sente une dizaine de commandements pour nous \naider Ã  progresser.\n\nLe principe est le suivant : rÃ©duire au minimum les facteurs personnels lors des interactions \navec ses pairs, pour favoriser le travail en Ã©quipe et produire le maximum de qualitÃ©.\n\n\n  Critiquez le code au lieu des personnes,\n  Soyez factuels sur le code,\n  Nâ€™attaquez jamais les personnes.\n\n\nJe vous recommande de regarder cette excellente confÃ©rence sur lâ€™egoless programming, oÃ¹ Olivier \nThelu prend le temps de revenir sur tous les concepts :\n\n\nLes 10 commandements de la programmation sans &amp;eacute;go - Olivier Thelu - MiXiT 2022 from MiXiT on Vimeo.\n\nUne autre excellente confÃ©rence de Kim LaÃ¯ Trinh, lead dÃ©veloppeur, et son â€œAuto-critique de la \nrevue de code\nbienveillanteâ€.\n\n\n\nFormatez vos commentaires !\n\nUne fois quâ€™on a adoptÃ© une posture qui nous aide Ã  mieux recevoir et donner des commentaires dans le cadre de nos revues de code, on peut rÃ©flÃ©chir Ã  la faÃ§on dont on les formate.\n\nGrÃ¢ce au standard des conventional comments, nous disposons dâ€™une convention pour Ã©crire des \ncommentaires clairs et visuels et limiter les incomprÃ©hensions. Chacun de nous est invitÃ© \nÃ  rÃ©flÃ©chir Ã  lâ€™intention de son commentaire avant de lâ€™Ã©crire.\n\nPar exemple, avec ce commentaire qui peut prÃªter Ã  confusion (le OMG qui signifie â€œOh my godâ€ \npeut Ãªtre autant interprÃ©tÃ© comme quelque chose de nÃ©gatif que de positif, notamment ici puisque \nnous nâ€™avons pas le contexte ğŸ˜ˆ) :\n\n\n\nCe commentaire peut Ãªtre prÃ©fixÃ© par praise, ce qui signifie Ã©loge. Cela change radicalement \nle ton du commentaire.\n\n\n\nVoici un autre exemple laconique : Poubelle.\n\n\n\nCelui-ci peut Ãªtre amÃ©liorÃ© en Ã©tant prÃ©fixÃ© par lâ€™Ã©tiquette nitpick, qui signifie \nâ€œtatillonnerâ€, ce qui diminue Ã©galement son ton dramatique. De plus, lâ€™urgence peut Ãªtre \nindiquÃ©e (ici, non-bloquant) et le contexte est dÃ©crit et peut Ãªtre exploitÃ© grÃ¢ce Ã  un patch \nproposant un code de remplacement.\n\n\n\nLa comprÃ©hension du commentaire est facilitÃ©e par lâ€™effort fourni pour ajouter le maximum de\ncontexte possible. On gagne en lisibilitÃ© grÃ¢ce Ã  la catÃ©gorisation (Ã©tiquette), qui nous permet\nÃ©galement dâ€™immÃ©diatement savoir de quoi on parle. Par exemple, une question ne sera pas lue\nde la mÃªme faÃ§on quâ€™une suggestion !\n\nLa contextualisation permet de savoir si on traite le retour immÃ©diatement ou si on ouvre une\nnouvelle pull request plus tard, pour rÃ©medier au point soulevÃ©. On limite ainsi les\nquiproquos ou les pertes de temps sur des actions non prioritaires.\nEt surtout, on limite les mauvaises impressions sur le ton employÃ©.\n\nDescription du standard\n\n&amp;lt;label&amp;gt; [decorations]: &amp;lt;subject&amp;gt;\n\n[discussion]\n\n\n\n  Ã©tiquette (label) : â€œÃ©tiquetteâ€ pour signifier de quel genre de commentaire il sâ€™agit\n  sujet (subject) : le commentaire en lui-mÃªme\n  contexte supplÃ©mentaire (decorations) (optionnel)  : labels supplÃ©mentaires pour donner plus dâ€™indications (entre parenthÃ¨ses, sÃ©parÃ©s par des virgules).\nExemple : non-blocking, blocking, test â€¦\n  discussion (optional) : contexte, raisonnement ou tout autre Ã©lÃ©ment pour aider Ã  \ncomprendre le Â« pourquoi Â» et les Â« prochaines Ã©tapes Â» pour rÃ©soudre le commentaire\n\n\nLes labels\n\nVoici la liste de labels ou Ã©tiquettes, extraits du standard :\n\n\n  praise\n  nitpick\n  suggestion\n  issue\n  todo\n  question\n  thought\n  chore\n  typo\n  polish\n  quibble\n\n\nLâ€™Ã©quipe est, bien entendu, libre de choisir ou dâ€™inventer ses labels ! Chez nous, le choix a Ã©tÃ© \nfait de respecter le standard tel quâ€™il est proposÃ©, mais cela pourrait Ãªtre rediscutÃ© si besoin.\n\nVoici quelques dÃ©finitions (pour le reste, se rÃ©fÃ©rer au site du standard):\n\nPraise (Ã©loge)\n\nGrÃ¢ce Ã  ce commentaire, on souligne quelque chose de positif, on encourage la personne. Bien \nentendu, pas de second degrÃ© !\n\nSuggestion (suggestion)\n\nLes suggestions sont la majoritÃ© des commentaires quâ€™on laisse, en gÃ©nÃ©ral. Il sâ€™agit \ndâ€™amÃ©liorations Ã  apporter au sujet actuel. On cherchera Ã  Ãªtre explicite et clair,\n\n\n  expliquer en quoi il sâ€™agit dâ€™une amÃ©lioration;\n  utiliser des patchs;\n  utiliser des dÃ©corations blocking ou non-blocking.\n\n\nIssue (problÃ¨me)\n\nGrÃ¢ce aux issues, on met en Ã©vidence des problÃ¨mes spÃ©cifiques. IdÃ©alement, on couple ce \ncommentaire avec une Suggestion.\n\nThought (pensÃ©e)\n\nLes pensÃ©es sont des idÃ©es qui surgissent lors de la relecture du code. Celles-ci ne sont pas \nbloquantes par nature, mais sont extrÃªmement prÃ©cieuses, car elles peuvent conduire Ã  des \npossibilitÃ©s de mentorat.\n\nAppropriez-vous la mÃ©thode !\n\nBien entendu, vous nâ€™Ãªtes pas obligÃ© dâ€™utiliser toute la liste de labels proposÃ©e par le \nstandard. Vous pouvez en choisir quelques uns, ou bien carrÃ©ment vous en inspirer et crÃ©er les \nvÃ´tres. Câ€™est le choix quâ€™ont fait Camille et son Ã©quipe, quâ€™elle dÃ©crit dans cet excellent \narticle \nsur les conventional Comments et lâ€™utilisation des emojis.\n\nAinsi, lâ€™Ã©quipe a portÃ© son choix sur une liste dâ€™Ã©tiquettes illustrÃ©es par des emojis, qui \ntraduisent Ã  fois lâ€™intention du commentaire et son urgence.\nVoici quelques exemples, tirÃ©s de lâ€™article :\n\nğŸ¥œ peanuts\nâ“ question\nğŸ’¬ discussion\nğŸš¨ alerte\nğŸš« no-go\nğŸ‘ bravo\nâš ï¸ warning\nâ˜ ï¸ bad idea\nâœ¨ magic\nğŸ”¥ burn-it-all\n\nQuelques autres bonnes pratiques dâ€™onboarding\n\nBien entendu, il y a plein dâ€™autres faÃ§ons dâ€™aider vos nouveaux dÃ©veloppeurs ou\nnouvelles dÃ©veloppeuses Ã  dÃ©couvrir le code. Voici quelques autres idÃ©es :\n\n\n  \n    On peut se familiariser avec le workflow dâ€™une publication de pull request ou de merge \nrequest en faisant une petite modification (ajouter son nom dans un fichier, par exemple ?);\n  \n  \n    on peut Ãªtre accompagnÃ©(e) dâ€™un â€œbuddyâ€ qui nous est assignÃ© Ã  lâ€™arrivÃ©e dans lâ€™entreprise avec \nqui on fait les premiÃ¨res revues de code en direct, et pas par Ã©crit.\n  \n\n\nUne derniÃ¨re bonne pratique trÃ¨s largement rÃ©pandue : si les Ã©changes par commentaires sont trop \nnombreux sur une mÃªme pull request, pourquoi ne pas se retrouver directement et rÃ©soudre en pair \nles \npoints discutÃ©s ?\n"
} ,
  
  {
    "title"    : "Forum PHP 2022 - Lâ€™Ã©lÃ©phant bleu nâ€™a pas peur de la souris aux grandes oreilles",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2022/10/28/forum-php-afup-2022.html",
    "date"     : "October 28, 2022",
    "excerpt"  : "Pour cette Ã©dition du Forum PHP qui sâ€™est dÃ©roulÃ©e Ã  Disneyland Paris, Bedrock a vu les choses en grand : premiÃ¨re fois\nsponsor Or sur un forum et pas moins de 32 Bedrockien(ne)s prÃ©sent(e)s dont 4 en tant que confÃ©rencier(e)s !\nCette annÃ©e encore...",
  "content"  : "Pour cette Ã©dition du Forum PHP qui sâ€™est dÃ©roulÃ©e Ã  Disneyland Paris, Bedrock a vu les choses en grand : premiÃ¨re fois\nsponsor Or sur un forum et pas moins de 32 Bedrockien(ne)s prÃ©sent(e)s dont 4 en tant que confÃ©rencier(e)s !\nCette annÃ©e encore, le forum a Ã©tÃ© un moment privilÃ©giÃ© pour les Ã©changes, le partage et vous avez Ã©tÃ© nombreux(ses) Ã \nvenir nous rencontrer sur notre stand et nous avons Ã©tÃ© ravis de pouvoir Ã©changer avec vous.\n\nLa richesse et la diversitÃ© des confÃ©rences ont fait de cette Ã©dition une grande rÃ©ussite. Nous ne pouvons\nmalheureusement pas aborder toutes les confÃ©rences dans cet article, mais voici une sÃ©lection de 10 dâ€™entre elles.\n\n\n\nBedrock au Forum PHP\n\nGrande premiÃ¨re pour nous cette fois-ci : Bedrock avait un stand sur le forum PHP !\nNotre Ã©quipe technique, ainsi que des membres de lâ€™Ã©quipe RH a participÃ© Ã  la prÃ©sentation de\nlâ€™entreprise. Nous avons pu, tous ensemble, vous accueillir sur le stand pour rÃ©pondre aux questions sur notre activitÃ©,\nnotre organisation et la vie dans lâ€™entreprise.\n\nDe leurs propres mots :\n\n\n  â€œNotre prÃ©sence au forum fut une rÃ©ussite collective, qui nous a permis de nous faire connaÃ®tre et mettre en lumiÃ¨re BEDROCK en tant que structure Ã  part entiÃ¨re, et non plus en tant que Â« M6 Web Â».\nNos Ã©quipes Ã©taient ravies que Bedrock soit sponsor et ont pu se challenger sur de rÃ©els â€œcasses-tÃªtesâ€ et non pas sur leur code.\nDurant ces deux journÃ©es, le stand Bedrock a Ã©tÃ© un point de rendez-vous agrÃ©able pour nos Ã©quipes et pour les autres participants curieux dâ€™apprendre Ã  nous connaÃ®tre.\nCette participation, au-delÃ  dâ€™Ãªtre trÃ¨s formatrice pour la communautÃ© Bedrock, a mÃªme parfois Ã©tÃ© assimilÃ©e Ã  un Â« team building Â» pour reprendre les termes de nos collaborateurs, leur permettant de se retrouver dans un autre environnement et de participer Ã  lâ€™attractivitÃ© de leur entreprise.\nLes Ã©quipes Backend &amp;amp; RH Ã©taient fiÃ¨res de reprÃ©senter Bedrock en participant Ã  la vie du stand et de rÃ©pondre aux questions des passants.\n\n\n\n\nLes confÃ©rences quâ€™on retient\n\nJour 1\n\nThe PHP Foundation: The past, the present, and the future - Sebastian BERGMANN, Roman PRONSKIY\n\n\n  â€œPHP câ€™est simple, suffit de dÃ©gommer un mec, et Ã§a nâ€™existe plusâ€¦â€\n                                  \t- Chuck Norris -\n\n\nCette vision Ã  peine exagÃ©rÃ©e de lâ€™ami Chuck Ã©tait assez proche de la rÃ©alitÃ© jusquâ€™Ã  lâ€™annÃ©e derniÃ¨re. En effet,\njusque-lÃ , lâ€™Ã©crasante majoritÃ© de la connaissance internet et des implÃ©mentations des features de PHP reposaient sur\nles Ã©paules de Nikita Popov et Dmitry Stogov. Lesquels faisaient cela de maniÃ¨re non-professionnelle, comme un hobby,\nalors que PHP est aujourdâ€™hui utilisÃ© pour environ 70% des sites internets.\n\nTout ceci est rÃ©sumÃ© dans cet article (en anglais).\n\nCe qui nous amÃ¨ne Ã  un principe bien connu et trÃ¨s problÃ©matique de bon nombre dâ€™entreprises : le Bus Factor.\n\nBus Factor\n\nCe principe identifie le nombre de personnes qui doivent Ãªtre renversÃ©es par un bus pour que la connaissance dâ€™un\nprojet disparaisse et que ce dernier soit rÃ©ellement mis en pÃ©ril. AppliquÃ© Ã  PHP, cela signifie que si deux\npersonnes avaient Ã©tÃ© renversÃ©es par un bus ou avaient croisÃ© le chemin de Chuck, lâ€™ensemble de la communautÃ© PHP\naurait bien transpirÃ©.\n\nLa PHP Foundation\n\nEn 2021, Nikita Popov annonce son souhait de rÃ©duire drastiquement sa contribution au langage, et câ€™est lÃ  que\nla PHP Fundation est fondÃ©e autour de personnalitÃ©s comme Sebastian BERGMAN et\nRoman PRONSKIY qui ont animÃ© cette confÃ©rence Ã  laquelle nous avons eu la chance dâ€™assister.\n\nLâ€™objectif est simple : rÃ©colter des fonds pour pouvoir rÃ©munÃ©rer des personnes qui font Ã©voluer le langage et\nrÃ©partir la connaissance dudit langage entre un maximum de personnes pour minimiser le Bus Factor\n\nLâ€™aspect juridique et financier de la chose est dÃ©lÃ©guÃ© Ã  une structure\nnommÃ©e Open Collective, dont lâ€™activitÃ© est 100% transparente.\nLes derniers chiffres font ainsi valoir quâ€™aprÃ¨s dÃ©ductions des charges, il reste cette annÃ©e environ 580 000$ pour la\nfondation.\n\nLes fonds proviennent majoritairement de donations rÃ©partis comme suit :\n\n  77% provenant dâ€™entreprises (JetBrains, Livesport, Symfonyâ€¦)\n  23% provenant dâ€™individus\n\n\nParmi les actuels core contributeurs on retrouve :\n\n  Arnaud Le Blanc\n  Derick Rethans\n  George P. Banyard\n  Ilija Tovilo\n  Jakub Zelenka\n  MÃ¡tÃ© Kocsis\n\n\nLe futur de PHP ?\n\nAvec cette stratÃ©gie de contractualisation des core-contributeurs et le souci de partager la connaissance, on est en\ndroit dâ€™espÃ©rer que le risque de mise en pÃ©ril du langage soit fortement rÃ©duit. Qui plus est, lâ€™arrivÃ©e prochaine de\nla version 8.2 de PHP semble montrer que la nouvelle Ã©quipe a su faire avancer le projet, pour notre plus grand plaisir.\n\nWatch the clock - Andreas HEIGHL\n\nDans cette confÃ©rence, Andreas aborde la douloureuse problÃ©matique du temps en PHP\net plus prÃ©cisÃ©ment le cauchemar que sont les tests sur la notion de temps rÃ©el.\n\nAfin de rÃ©pondre Ã  cette problÃ©matique, il Ã©voque et explique en dÃ©tail tout le cheminement intellectuel pour la mise en\nplace de la PSR-20. Elle donnera la possibilitÃ©, grÃ¢ce Ã  une â€œsimpleâ€ interface, de\ngÃ©rer plus facilement le temps rÃ©el et les tests associÃ©s. Malheureusement, la PSR-20 est actuellement encore Ã  lâ€™Ã©tat\nde brouillon et ne sera probablement pas mise Ã  disposition avant longtemps.\n\nAfin de nous faire patienter, Andreas a dÃ©veloppÃ© sa propre implÃ©mentation de lâ€™interface Clock qui sera proposÃ©e dans\nla PSR-20 : https://packagist.org/packages/stella-maris/clock.\n\nComment Ãªtre bien onboardÃ© en tant que dÃ©veloppeuse junior reconvertie ? - AmÃ©lie ABDALLAH\n\nAmÃ©lie, alternante et en reconversion, a donnÃ© sa premiÃ¨re confÃ©rence pour faire un retour dâ€™expÃ©rience des onboardings\nquâ€™elle a vÃ©cu dans ses 2 premiÃ¨res entreprises.\n\nPour sa premiÃ¨re expÃ©rience en tant que dÃ©veloppeuse elle se retrouve Ã  devoir coder dÃ¨s le premier jour. Une fois\npassÃ©e lâ€™euphorie, je rappelle quâ€™elle est en reconversion et sur-motivÃ©e, elle se rend vite compte de tout ce qui ne\nva pas :\n\n\n  personne pour la former que ce soit techniquement ou concernant le mÃ©tier\n  aucune prÃ©sentation des autres Ã©quipes\n  aucun accompagnement de la part des supÃ©rieurs hiÃ©rarchiques\n  seule sur un projet oÃ¹ les rÃ¨gles mÃ©tiers semblent complexes\n\n\nElle se sent rapidement perdue et son moral et sa motivation tombent en flÃ¨che.\n\nConcernant sa seconde entreprise, câ€™est tout lâ€™inverse :\n\n\n  pas de code pour les alternants/reconverti(e)s avant 2 semaines\n  un systÃ¨me de marrainage/parrainage\n  un repo avec des exercices et bonnes pratiques pour progresser\n  une rencontre avec toutes les Ã©quipes pour se connaÃ®tre mais surtout pour bien comprendre les diffÃ©rents mÃ©tiers de ses collÃ¨gues\n\n\nAmÃ©lie conclut en incitant toutes les entreprises Ã  envisager des candidat(e)s en reconversion qui, Ã  la condition\ndâ€™Ãªtre bien accompagnÃ©(e)s, seront sur-motivÃ©(e)s et plein(e)s dâ€™Ã©nergie.\n\nCela nous permet de faire lâ€™Ã©tat des lieux de notre processus dâ€™onboarding. MÃªme si on peut trouver des pistes\ndâ€™amÃ©lioration, comme par exemple avoir une prÃ©sentation des autres Ã©quipes et diffÃ©rents mÃ©tiers bien plus tÃ´t, on\nconstate que nous avons dÃ©jÃ  mis en place beaucoup de bonnes pratiques :\n\n\n  calendrier dâ€™onboarding clair avec installation de la machine, prÃ©sentation de lâ€™Ã©quipe rejoint par la nouvelle personne, des locaux, des projets, etc.\n  systÃ¨me de marrainage/parrainage,\n  des formations disponibles sur les diffÃ©rentes technologies utilisÃ©es\n  ce quâ€™on appelle la Bedrock Academy, qui donne une vision globale des mÃ©tiers de lâ€™entreprise\n  un rapport dâ€™Ã©tonnement prÃ©sentÃ© par le nouvel arrivant qui nous permet dâ€™amÃ©liorer notre onboarding en continu\n\n\nFFI : De nouveaux horizons pour PHP - Pierre PELISSET\n\nChez Karafun, Pierre Pelisset nous prÃ©sente un moyen de dÃ©passer les frontiÃ¨res habituelles de PHP.\n\nLa mise en place de bar Ã  karaokÃ© nÃ©cessite en effet de manipuler du matÃ©riel spÃ©cifique. La transmission de donnÃ©es par\nUSB reste le moyen le plus simple de faire communiquer des systÃ¨mes, mais nativement rien nâ€™existe en PHP. Jusquâ€™Ã \nprÃ©sent les Ã©quipes de Karafun utilisaient un script Python avec PySerial pour rÃ©pondre Ã  ce besoin.\n\nDepuis PHP 7.4, il est possible dâ€™utiliser les Foreign Function Interface (FFI)\npour appeler directement des libraires C compilÃ©es depuis le code PHP. En se basant sur les bonnes librairies, il a Ã©tÃ©\nainsi possible pour les Ã©quipes de Karafun dâ€™utiliser PHP dans la totalitÃ© de leur stack, et de remplacer leurs scripts\nPython.\n\nPierre nous prÃ©sente ensuite la librairie php-termios quâ€™il a implÃ©mentÃ©e\npour permettre dâ€™utiliser Termios, une librairie de manipulation de terminal POSIX Ã©crite en C, depuis PHP.\n\nLâ€™interfaÃ§age reste facile tant quâ€™il sâ€™agit de types de donnÃ©es simples (comme des entiers), mais devient beaucoup plus\ncomplexe lorsquâ€™il sâ€™agit de manipuler des structures de donnÃ©es avancÃ©es (chaÃ®nes de caractÃ¨res, objets, â€¦).\n\nIdÃ©alement, il faut crÃ©er des classes PHP ressemblant Ã  la librairie C (mÃªmes noms de fonction, de constante, â€¦), afin\nde faciliter sa manipulation.\n\nLâ€™utilisation des FFI introduit de nouvelles difficultÃ©s liÃ©es gÃ©nÃ©ralement aux programmes compilÃ©s. Ainsi, dans le cas\nde Termios, les constantes utilisÃ©es varient selon la plateforme (Linux, MacOS, â€¦). Il a donc fallu utiliser quelques\nastuces pour les copier dynamiquement dans le code PHP. De mÃªme, pour distribuer le binaire compilÃ©, il faut prendre en\ncompte lâ€™architecture CPU de la machine exÃ©cutant le code.\n\nOpenTelemetry : vers un standard pour surveiller vos applications - Benoit Viguier\n\nLors de sa confÃ©rence, Benoit Viguier (dÃ©veloppeur chez Platform.sh au sein de lâ€™Ã©quipe de Blackfire.io) est venu parler\ndâ€™un standard de monitoring soutenu par la Cloud Native Computing Foundation : OpenTelemetry. Il a dâ€™abord commencÃ© par\nnous rappeler pourquoi nous faisons du monitoring.\n\nLe monitoring nous permet de savoir si nos services fonctionnent correctement (conforme au SLA|SLO), et si cela nâ€™est\npas le cas, de savoir pourquoi cela ne fonctionne pas. Pour faire cela, il nous a prÃ©sentÃ© les diffÃ©rentes solutions de\nmonitoring qui existent, de la plus simple Ã  mettre en place (Analytics), au plus|trop dÃ©taillÃ© (logs) mais aussi via\ndes metrics qui eux demandent une plus grande intÃ©gration.\n\nEnsuite vient la prÃ©sentation des promesses de ce nouveau standard (OpenTelemetry) et sa volontÃ© dâ€™uniformiser les trois\npiliers du monitoring : logs, metrics et les traces, avec la volontÃ© de rendre interopÃ©rable ces donnÃ©es collectÃ©es avec\nnâ€™importe quel service et ce quâ€™importe le langage.\n\n\n\nAfin de permettre cette interopÃ©rabilitÃ© pour la collecte, le traitement et lâ€™envoi des donnÃ©es entre nos applications\net nos APMs, OpenTelemetry propose lâ€™utilisation dâ€™un collecteur. Ce collecteur possÃ¨de trois composants par lesquels\nnotre donnÃ©e va transiter :\n\n\n  Receiver - GÃ¨re la rÃ©cupÃ©ration des donnÃ©es dans le collecteur. Il fonctionne aussi bien avec une mÃ©canique de push que de pull et supporte nativement les protocoles HTTP et gRPC.\n  Processor - Permet le traitement des donnÃ©es avant lâ€™envoi aux diffÃ©rents outils de monitoring.\n  Exporter - Envoie les donnÃ©es (via push ou pull) aux outils de monitoring.\n\n\nConcernant lâ€™utilisation du standard OpenTelemetry en PHP, il existe aujourdâ€™hui un SDK qui, cependant, ne possÃ¨de\npas beaucoup de fonctionnalitÃ©s.\n\nMalgrÃ© son jeune Ã¢ge (release v1.0 en 2021), OpenTelemetry a encore de beaux jours devant lui si la contribution et le\nsoutien de la communautÃ© continuent. Il est important de rappeler que le standard nâ€™est pas encore finalisÃ©, par exemple\nla fonctionnalitÃ© â€œloggingâ€ est encore en cours de dÃ©veloppement et nâ€™est disponible quâ€™en â€œdraftâ€.\n\nJour 2\n\nTypage en PHP, comment Ã§a fonctionne ? - George BANYARD\n\nLors de cette dissection du typage PHP, George Banyard, dÃ©veloppeur salariÃ© de la PHP Foundation, nous a expliquÃ© (ou du\nmoins tentÃ©) comment fonctionne le typage PHP grÃ¢ce Ã  des formules mathÃ©matiques. Mais avant de nous faire peur avec les\nformules, il nous dâ€™abord rappelÃ© les diffÃ©rents types existants et futurs (PHP 8.1 et 8.2) et comment ils sont\nreprÃ©sentÃ©s en C (zend_type, _zval_struct, _zend_class_entry)\n\n\n  Types Primitifs\n  Types dÃ©finis en espace utilisateur (classe, interfaces, enum)\n  Types LittÃ©raux (false, true)\n  Type callable\n  \n    \n      \n        \n          Types ComposÃ©s (A&amp;amp;B, A\n          B, Forme Normale Disjonctive)\n        \n      \n    \n  \n  Alias de Type (PHP 8.2)\n\n\nEnsuite, il nous a expliquÃ© le principe de substitution de Liskov de diffÃ©rente maniÃ¨re.\n\n  Si Ï†(x) est une propriÃ©tÃ© dÃ©montrable pour tout objet x de\ntype T, alors Ï†(y) est vraie pour tout objet y de type S tel que\nS est un sous-type de T\n\n\nMathÃ©matique, en C mais aussi en image :\n\n\n\nEn version trÃ¨s simplifiÃ©e, la substitution de Liskov permet de substituer un type par un autre type sâ€™il est\nmieux-disant. Câ€™est par exemple sur ce principe que sont fondÃ©es la co-variance et la contra-variance en PHP.\n\nAprÃ¨s toutes ces informations, George nous a fait rÃªver avec les nouveautÃ©s qui pourraient arriver dans notre langage prÃ©fÃ©rÃ© :\n\n\n  La possibilitÃ© de dÃ©finir nos alias de type (numeric qui serait int ou float)\n  Pouvoir dÃ©finir un typage pour les paramÃ¨tres et le retour des callable directement dans la fonction : foo(fn&amp;lt;int,string&amp;gt;:bool $callable) {}\n  Le paramÃ¨tre in-out qui permet de vÃ©rifier que le type ne change pas entre lâ€™entrÃ©e et la sortie dâ€™une fonction, utile notamment dans le cas de passage par rÃ©fÃ©rence\n  Pouvoir crÃ©er des types gÃ©nÃ©riques (par exemple class Collection&amp;lt;string&amp;gt;)\n\n\nVous pouvez retrouver les slides de la confÃ©rence ici\n\nProtÃ©ger votre application avec lâ€™en-tÃªte HTTP de sÃ©curitÃ© â€œContent Security Policyâ€ - Laurent BRUNET\n\nDurant ce talk, Laurent BRUNET nous a rappelÃ© lâ€™importance de sÃ©curiser les sites internet en nous parlant des attaques\nles plus utilisÃ©es et dont on pourrait se dÃ©fendre en exploitant correctement le header de rÃ©ponse Content-Security-Policy (aka. CSP).\n\nConcrÃ¨tement, le header CSP est dÃ©livrÃ© par le serveur en mÃªme temps que le HTML dâ€™une page web et contraint les\nnavigateurs Ã  sâ€™assurer que le contenu de la page respecte les rÃ¨gles de sÃ©curitÃ© dÃ©finies grÃ¢ce Ã  de nombreuses directives.\n\nIl existe 2 variantes de ce header qui peuvent Ãªtre utilisÃ©es conjointement :\n\n\n  une forme bloquante qui empÃªchera tous contenus illicites dâ€™Ãªtre chargÃ©s par le navigateur\nContent-Security-Policy: &amp;lt;directive1&amp;gt; ;  &amp;lt;directive2&amp;gt; ;  &amp;lt;directiveN&amp;gt;\n  une forme non-bloquante qui permet uniquement aux dÃ©veloppeurs dâ€™Ãªtre avertis si du contenu illicite est prÃ©sent sur la page\nContent-Security-Policy-Report-Only: &amp;lt;directive1&amp;gt; ;  &amp;lt;directive2&amp;gt; ;  &amp;lt;directiveN&amp;gt;\n\n\nEt voici quelques unes des failles de sÃ©curitÃ© Ã©voquÃ©es durant ce talk, chacune accompagnÃ©e dâ€™une des directives permettant de sâ€™en protÃ©ger :\n\n\n  on peut se protÃ©ger des attaques XSS (e.g. injection de scripts dans une page web permettant dâ€™exploiter les accÃ¨s dâ€™un utilisateur) en whitelistant les scripts autorisÃ©s Ã  sâ€™exÃ©cuter dans votre application avec script-src &#39;nonce-MonIDDeScript&#39; &#39;sha256-MonHashDeScript&#39; &#39;strict-dynamic&#39; https: &#39;unsafe-inline&#39; *.example.com ;\n  le Clickjacking (e.g. injection dâ€™iframe invisible dans laquelle lâ€™utilisateur va cliquer sans le savoir) peut Ãªtre facilement contrÃ© en whitelistant vos iframes et celles de vos partenaires avec frame-ancestors &#39;self&#39; https://example.com ;\n  la faille Magecart (e.g. rÃ©cupÃ©ration des donnÃ©es bancaires en les envoyant vers un nom de domaine pirate difficile Ã  dÃ©tecter) disparaÃ®tra en whitelistant les noms de domaines utilisÃ©s dans des appels AJAX avec connect-src &#39;self&#39; https://example.com ;\n\n\nSi le sujet vous intÃ©resse, nâ€™hÃ©sitez pas Ã  consulter le talk de Laurent dÃ¨s quâ€™il sera disponible en replay. En\nattendant, profitez de la documentation du MDN et sachez\nque Google fournit un service en ligne pour inspecter et valider les CSP de vos\npages web.\n\nTester Ã  travers OpenAPI, ou comment valider votre documentation ! - StÃ©phane Hulard\n\nStÃ©phane Hulard a commencÃ© sa confÃ©rence par nous rappeler ce quâ€™est OpenAPI : il sâ€™agit\ndâ€™une initiative qui vise Ã  normaliser et standardiser la description dâ€™APIs. Cela sert Ã  lâ€™interopÃ©rabilitÃ©,\nlâ€™automatisation et la fiabilitÃ©.\n\nIl faut voir la documentation comme une spÃ©cification de notre API. â€œUne documentation nâ€™a de sens que si elle reflÃ¨te\nlâ€™Ã©tat actuel de lâ€™application.â€ Rien de mieux donc que dâ€™intÃ©grer la validation de notre documentation par rapport Ã \nnotre code, et inversement : que notre documentation valide notre code !\n\n\n\nthephpleague nous propose une solution pour faire Ã§a : openapi-psr7-validator.\nCe paquet peut valider les messages PSR-7 par rapport aux spÃ©cifications OpenAPI (3.0.x) exprimÃ©es en YAML ou JSON.\n\nCe paquet se base sur les PHP Standards Recommendations (PSR) qui sont des textes dÃ©crivant une maniÃ¨re commune de\nrÃ©soudre un problÃ¨me spÃ©cifique. De cette faÃ§on, les projets qui suivent ces recommandations auront une excellente\ninteropÃ©rabilitÃ© en suivant les mÃªmes recommandations et contrats.\n\nStÃ©phane nous parle ensuite de la librairie Raven quâ€™il vient de publier. Raven a\npour but de faciliter la validation au travers de la documentation, de sâ€™appuyer sur de vraies donnÃ©es pour valider les\nrequÃªtes et rÃ©ponses ainsi que valider le comportement de lâ€™API testÃ©e. La librairie nâ€™en est quâ€™Ã  ses dÃ©buts, quelques\nissues sont remontÃ©es sur le repository, nâ€™hÃ©sitez pas Ã  contribuer !\n\nStÃ©phane a mis Ã  disposition les slides de sa confÃ©rence ici\n\nFrankenPHP, dans les entrailles de lâ€™interprÃ©teur PHP, de machines virtuelles et des threads - KÃ©vin DUNGLAS\n\nChez Bedrock, nous utilisons majoritairement nginx et php-fpm pour servir nos applications. Nous avons aussi fait des\nessais avec Road Runner comme alternative.\n\nDurant cette confÃ©rence, KÃ©vin DUNGLAS nous a prÃ©sentÃ© une nouvelle alternative Ã©crite en Go et nommÃ©e frankenphp.\nChouette effet dâ€™annonce au passage, FrankenPHP a Ã©tÃ© ouvert au public en direct durant la confÃ©rence.\n\nEn plus dâ€™avoir un logo trÃ¨s mignon, ce nouveau serveur dâ€™application PHP promet un gain de performance en gardant en\nmÃ©moire lâ€™application chargÃ©e. Autre argument intÃ©ressant, la facilitÃ© dâ€™installation et dâ€™utilisation.\n\nBien quâ€™il a plusieurs fois prÃ©cisÃ© que ce nâ€™Ã©tait pas prÃªt pour la production, cette nouvelle approche semble\nprometteuse. Il est probable que nous ne tardions pas Ã  lâ€™essayer pour voir sâ€™il est possible de gagner en performance.\n\nLes confÃ©renciers Bedrock\n\n\n\nCette annÃ©e, ce ne sont pas moins de 4 prÃ©sentations qui Ã©taient donnÃ©es par des personnes de chez Bedrock.\n\nVous pourrez les trouver trÃ¨s bientÃ´t en replay. Les liens seront partagÃ©s sur ce blog et les vidÃ©os disponibles\nsur la chaÃ®ne youtube de lâ€™afup.\n\nEn attendant, et pour rappel, il sâ€™agissait des confÃ©rences suivantes :\n\nComprenez comment PHP fonctionne, vos applications marcheront mieux - Pascal MARTIN\n\nÃ€ lâ€™Ã©chelle Ã  laquelle nous travaillons, avec des millions de personnes sur nos plateformes tous les jours, nous dÃ©couvrons, atteignons, voire dÃ©passons rÃ©guliÃ¨rement des limites de lâ€™approche traditionnelle de PHP et de php-fpm. Au cours de ce talk, Pascal souhaitait partager notre expÃ©rience de travail avec PHP, sur des sujets souvent peu connus par les dÃ©veloppeurs et dÃ©veloppeuses, pour aider le public Ã  crÃ©er des applications qui rÃ©pondent mieux aux attentes de leur public.\n\n\n  Pour exÃ©cuter du code, PHP consomme du processeur et de la mÃ©moire. Quand une requÃªte HTTP arrive, un processus php-fpm lui est dÃ©diÃ©. Mais ces ressources sont limitÃ©es. Et, mÃªme dans le Cloud ou en serverless, scaler prend du temps et les coÃ»ts sâ€™envolent !\n\n\n\n  Savez-vous combien de CPU et de RAM votre application rÃ©clame ? Et pendant quelle durÃ©e ? Si non ou sans comprendre Â« pourquoi Â», difficile de dÃ©velopper efficacement et de dimensionner un hÃ©bergement pÃ©renne ! Peut-Ãªtre que Ã§a marcheâ€¦ Sur votre poste. Ou pendant un moment, en gaspillant de lâ€™argent et des ressources. Mais lâ€™expÃ©rience prouve que, tÃ´t ou tard, ces questions vous rattraperont.\n\n\n\n  Cycle de vie de PHP, communication entre nginx et php-fpm, approche shared-nothing, compilation et cache dâ€™opcodes, gestion interne de la mÃ©moire ou mÃªme architecture logicielle et debuggingâ€¦ Pour quâ€™une application rÃ©ponde aux attentes de son public, nous devons comprendre comment PHP fonctionne !\n\n\nSauve un-e dÃ©v, Ã©cris une doc ! - Sarah HAÃM-LUBCZANSKI\n\n\n  Vous Ãªtes dÃ©veloppeur ou dÃ©veloppeuse PHP : vous aimez programmer, rÃ©flÃ©chir. Vous aimez crÃ©er des applications ou des bibliothÃ¨ques de qualitÃ©. Mais pourquoi personne ne les utilise ? Parce que votre documentation nâ€™est pas Ã  la hauteur !\n\n\n\n  Justement : je suis Technical Writer et mon mÃ©tier est de vous aider Ã  valoriser votre logiciel auprÃ¨s de ses utilisateurs et utilisatrices, Ã  travers une bonne doc. Comprenons comment architecturer, concevoir et rÃ©diger votre contenu. DÃ©couvrons les outils qui vous procureront une aide prÃ©cieuse. Enfin, facilitons sa mise Ã  jour pour quâ€™elle soit pÃ©renne.\n\n\n\n  DorÃ©navant, vous saurez identifier les passages obligÃ©s et ceux oÃ¹ vous pouvez gagner du temps.\n\n\nRevue de code : on nâ€™est pas venu pour souffrir ! - Anne-Laure DE BOISSIEU\n\n\n  Jâ€™ai rejoint ma nouvelle Ã©quipe il y a 6 mois, avec une apprÃ©hension. Comment allais-je vivre les revues de code par des collÃ¨gues que je ne connais pas encore ? IncomprÃ©hensions, malentendus : la communication Ã©crite rend cet exercice trÃ¨s dÃ©licat. Vous avez Ã©tÃ© blessÃ©-e par un commentaire ? Etait-il vraiment mal intentionnÃ© ? Vous avez blessÃ© quelquâ€™un sans le vouloir, Ã  cause dâ€™une tournure maladroite ?\n\n\n\n  Dans mon Ã©quipe, jâ€™ai dÃ©couvert un cadre qui mâ€™a permis de me sentir bien accueillie dÃ¨s mon arrivÃ©e. En adoptant une posture et une convention bien adaptÃ©e, on peut largement diminuer le risque de mal se comprendre. Non seulement on communique mieux, mais on amÃ©liore la qualitÃ© globale du projet.\n\n\n\n  Vous nâ€™aurez plus aucune raison de souffrir !\n\n\nBFF, notre best friend forever pour faire plein dâ€™applications frontend ? - Valentin CLARAS\n\n\n  Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application Ã©tant dÃ©ployÃ©e sur de nombreux appareils (ordinateur, mobile, set top box, tv connectÃ©e, consoles de jeux, tv stick etc â€¦). Il Ã©tait devenu trÃ¨s difficile de gÃ©rer la crÃ©ation et lâ€™Ã©volution de ces nombreuses applications qui requÃªtaient et formataient chacune elles-mÃªmes les donnÃ©es dont elles avaient besoin.\n\n\n\n  Pour cela, en 2018, nous avons dÃ©cidÃ© de nous lancer dans la crÃ©ation dâ€™un Back For Front afin dâ€™unifier et faciliter les interactions backend et frontend. Cette confÃ©rence fut lâ€™occasion de passer en revue :\n  \n    les concepts du Back For Front\n    lâ€™architecture api-gateway et micro service mise en place\n    les gains fonctionnels et la vÃ©locitÃ© gagnÃ©e\n    les diffÃ©rents mÃ©canismes dÃ©veloppÃ©s pour absorber les importants pics de charge (rÃ©silience, circuit breaker, fallbacks etc.)\n    les impacts techniques et organisationnels dâ€™une telle architecture\n  \n\n\n\n  Aujourdâ€™hui notre API Gateway BFF opÃ¨re 92 frontends dÃ©livrant 1.5 milliards de vidÃ©os par an pour 45 millions dâ€™utilisateurs actifs (MaU).\nVous pourrez trouver un complÃ©ment dâ€™informations au sujet de notre BFF dans la suite dâ€™articles dÃ©diÃ©.\n\n\nPour conclure\n\nNous sommes revenus la tÃªte pleine de nouvelles idÃ©es. Ces deux jours de confÃ©rences nous ont permis de montrer le savoir faire prÃ©sent chez Bedrock et nous avons aussi pu nous inspirer des connaissances dâ€™autres personnes. Les formats variÃ©s rÃ©pondaient aux goÃ»ts de chacun(e) et ont rendu ce forum unique.\n\nLes nombreuses activitÃ©s proposÃ©es entre chaque confÃ©rence permettaient dâ€™Ã©changer entre pairs et comme toujours la communautÃ© a Ã©tÃ© mise Ã  lâ€™honneur avec la construction dâ€™une fresque LEGO reprÃ©sentant tous les logos des antennes de lâ€™AFUP.\n\nMerci Ã  tou(te)s les confÃ©rencier(e)s pour leur travail incroyable et merci lâ€™AFUP pour lâ€™organisation de ce superbe Ã©vÃ¨nement !\n\n\n\nVivement lâ€™annÃ©e prochaine !\n"
} ,
  
  {
    "title"    : "Sauve un-e dÃ©v, Ã©cris une doc !",
    "category" : "",
    "tags"     : " conference, afup, forumphp, doc",
    "url"      : "/2022/10/13/sauve-une-dev-ecris-une-doc.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Vous Ãªtes dÃ©veloppeur ou dÃ©veloppeuse PHP : vous aimez programmer, rÃ©flÃ©chir. Vous aimez crÃ©er des applications ou des bibliothÃ¨ques de qualitÃ©. Mais pourquoi personne ne les utilise ? Parce que votre documentation nâ€™est pas Ã  la hauteur !\n\nJustem...",
  "content"  : "Vous Ãªtes dÃ©veloppeur ou dÃ©veloppeuse PHP : vous aimez programmer, rÃ©flÃ©chir. Vous aimez crÃ©er des applications ou des bibliothÃ¨ques de qualitÃ©. Mais pourquoi personne ne les utilise ? Parce que votre documentation nâ€™est pas Ã  la hauteur !\n\nJustement : je suis Technical Writer et mon mÃ©tier est de vous aider Ã  valoriser votre logiciel auprÃ¨s de ses utilisateurs et utilisatrices, Ã  travers une bonne doc. Comprenons comment architecturer, concevoir et rÃ©diger votre contenu. DÃ©couvrons les outils qui vous procurerons une aide prÃ©cieuse. Enfin, facilitons sa mise Ã  jour pour quâ€™elle soit pÃ©renne.\n\nDorÃ©navant, vous saurez identifier les passages obligÃ©s et ceux oÃ¹ vous pouvez gagner du temps.\n"
} ,
  
  {
    "title"    : "Revue de code : on nâ€™est pas venu pour souffrirÂ !",
    "category" : "",
    "tags"     : " conference, afup, forumphp, revue",
    "url"      : "/2022/10/13/revue-de-code-on-n-est-pas-venu-pour-souffrir.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Jâ€™ai rejoint ma nouvelle Ã©quipe il y a 6 mois, avec une apprÃ©hension. Comment allais-je vivre les revues de code par des collÃ¨gues que je ne connais pas encore ? IncomprÃ©hensions, malentendus : la communication Ã©crite rend cet exercice trÃ¨s dÃ©lica...",
  "content"  : "Jâ€™ai rejoint ma nouvelle Ã©quipe il y a 6 mois, avec une apprÃ©hension. Comment allais-je vivre les revues de code par des collÃ¨gues que je ne connais pas encore ? IncomprÃ©hensions, malentendus : la communication Ã©crite rend cet exercice trÃ¨s dÃ©licat. Vous avez Ã©tÃ© blessÃ©-e par un commentaire ? Etait-il vraiment mal intentionnÃ© ? Vous avez blessÃ© quelquâ€™un sans le vouloir, Ã  cause dâ€™une tournure maladroite ?\n\nDans mon Ã©quipe, jâ€™ai dÃ©couvert un cadre qui mâ€™a permis de me sentir bien accueillie dÃ¨s mon arrivÃ©e. En adoptant une posture et une convention bien adaptÃ©e, on peut largement diminuer le risque de mal se comprendre. Non seulement on communique mieux, mais on amÃ©liore la qualitÃ© globale du projet.\n\nVous nâ€™aurez plus aucune raison de souffrir !\n"
} ,
  
  {
    "title"    : "Comprenez comment PHP fonctionne, vos applications marcheront mieux, Forum PHP 2022",
    "category" : "",
    "tags"     : " conference, afup, forumphp, php",
    "url"      : "/2022/10/13/comprenez-comment-php-fonctionne-vos-applications-marcheront-mieux.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Pour exÃ©cuter du code, PHP consomme du processeur et de la mÃ©moire. Quand une requÃªte HTTP arrive, un processus php-fpm lui est dÃ©diÃ©. Mais ces ressources sont limitÃ©es. Et, mÃªme dans Le Cloud ou en serverless, scaler prend du temps et les coÃ»ts s...",
  "content"  : "Pour exÃ©cuter du code, PHP consomme du processeur et de la mÃ©moire. Quand une requÃªte HTTP arrive, un processus php-fpm lui est dÃ©diÃ©. Mais ces ressources sont limitÃ©es. Et, mÃªme dans Le Cloud ou en serverless, scaler prend du temps et les coÃ»ts sâ€™envolent !\n\nSavez-vous combien de CPU et de RAM votre application rÃ©clame ? Et pendant quelle durÃ©e ? Si non ou sans comprendre Â« pourquoi Â», difficile de dÃ©velopper efficacement et de dimensionner un hÃ©bergement pÃ©renne ! Peut-Ãªtre que Ã§a marcheâ€¦ Sur votre poste. Ou pendant un moment, en gaspillant de lâ€™argent et des ressources. Mais lâ€™expÃ©rience prouve que, tÃ´t ou tard, ces questions vous rattraperont.\n\nCycle de vie de PHP, communication entre nginx et php-fpm, approche shared-nothing, compilation et cache dâ€™opcodes, gestion interne de la mÃ©moire ou mÃªme architecture logicielle et debuggingâ€¦ Pour quâ€™une application rÃ©ponde aux attentes de son public, nous devons comprendre comment PHP fonctionne !\n"
} ,
  
  {
    "title"    : "BFF, notre best friend forever pour faire plein dâ€™applications frontendÂ ?",
    "category" : "",
    "tags"     : " conference, afup, forumphp, php, bff",
    "url"      : "/2022/10/13/bff-notre-best-friend-forever-pour-faire-plein-d-applications-frontend.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application Ã©tant dÃ©ployÃ©e sur de nombreux appareils (ordinateur, mobile, set top box, tv connectÃ©, consoles de jeux, t...",
  "content"  : "Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application Ã©tant dÃ©ployÃ©e sur de nombreux appareils (ordinateur, mobile, set top box, tv connectÃ©, consoles de jeux, tv stick etc â€¦). Il Ã©tait devenu trÃ¨s difficile de gÃ©rer la crÃ©ation et lâ€™Ã©volution de ces nombreuses applications qui requÃªtaient et formataient chacune elles-mÃªmes les donnÃ©es dont elles avaient besoin.\n\nPour cela, en 2018, nous avons dÃ©cidÃ© de nous lancer dans la crÃ©ation dâ€™un Back For Front afin dâ€™unifier et faciliter les interactions backend et frontend. Au cours de cette confÃ©rence nous passerons en revue :\n\n\n  les concepts du back for front\n  lâ€™architecture api-gateway et micro service mise en place\n  Les gains fonctionnels et la vÃ©locitÃ©s gagnÃ©e\n  les diffÃ©rents mÃ©canismes dÃ©veloppÃ©s pour absorber les importants pic de charge (rÃ©silience, circuit breaker, fallbacks etc.)\n  les impacts techniques et organisationnels dâ€™une telle architecture\n\n\nAujourdâ€™hui notre API Gateway BFF opÃ¨re 92 frontends dÃ©livrant 1.5 milliards de vidÃ©os par an pour 45 millions dâ€™utilisateurs actifs (MaU). Venez dÃ©couvrir notre retour dâ€™expÃ©rience sur la mise en place dâ€™un tel projet.\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #18",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/10/07/bedrock-dev-facts-18.html",
    "date"     : "October 7, 2022",
    "excerpt"  : "Câ€™est maintenant lâ€™automne ğŸğŸƒ, on vous propose les devfacts de cette fin dâ€™Ã©tÃ© et il y a du lourd !\n\nCogito ergo sum\n\n\n  Le seul truc que je â€œsaisâ€ câ€™est â€œon paye 200 par mois pour avoir un direct connectâ€ ; mais peut-Ãªtre que je sais que je ne sa...",
  "content"  : "Câ€™est maintenant lâ€™automne ğŸğŸƒ, on vous propose les devfacts de cette fin dâ€™Ã©tÃ© et il y a du lourd !\n\nCogito ergo sum\n\n\n  Le seul truc que je â€œsaisâ€ câ€™est â€œon paye 200 par mois pour avoir un direct connectâ€ ; mais peut-Ãªtre que je sais que je ne sais rien.\n\n\nğŸ‘‹ğŸ”¥ğŸ™ŠğŸ‰\n\n\n  Je te fais confiance, je te laisserai mettre des emoji sur ma tombe\n\n\nLa prioritÃ© câ€™est important\n\n\n  Câ€™est plus important que des trucs moins importants.\n\n\nBiafine nÃ©cessaire\n\nUn QA voulant faire une vanne au morning\n\n  Le QA: â€œQuâ€™est-ce qui a deux lettres et qui marche pas?â€\n\n  RÃ©ponse du dev: â€œLa QA ?â€\n\n\n3615 croissants\n\nParlant dâ€™une Ã©volution majeure\n\n  Est-ce que câ€™est OK pour vous pour quâ€™on MEP aujourdâ€™hui ou pas ?\n\n  Ã€ priori pas de contre-indications. Peut-Ãªtre acheter directement une boulangerie ?\n\n\nSugar !\n\n\n  Câ€™est du sucre syntaxique\n\n  Quand il y a trop de sucre, tu risques du diabÃ¨te\n\n  Surtout que le diabÃ¨te syntaxique, câ€™est le pire\n\n\nLA metric\n\n\n  Je crois quâ€™on est bon, on a pas plus dâ€™erreur que dâ€™user.\n\n\nLe verre Ã  moitiÃ© plein\n\n\n  Du coup Ã§a a cassÃ© je suppose?\n\n  Ouais, enfin pas totalement, presque un succÃ¨s.\n\n\nLa montagne Ã§a vous gagne\n\nEn parlant de son ascension du Ventoux Ã  vÃ©lo:\n\n\n  â€œMoi je pensais vraiment que jâ€™Ã©tais prÃªtâ€\n\n\nMais pourquoi ?\n\nEssayant de comprendre pourquoi une commande met beaucoup de temps sur sa machine.\n\n\n  Je suis sur master, branche Ã  jour, yarn rÃ©alisÃ© juste avant, et je pense pas que mon PC soit particuliÃ¨rement atteint de maladie congÃ©nitale\n(Enfin, jâ€™espÃ¨re pas?)\n\n\nRead me please\n\n\n  Tout est dans le README (que les gens liront pas de toute faÃ§on mais bon).\n\n\nLes Ã©toiles â­ï¸\n\n\n  ÃŠtre responsable dâ€™un incident, câ€™est pas marrant câ€™est sÃ»r. \nMais Ã§a forme !\nCasser la prod et la rÃ©parer Ã§a donne une Ã©toile sur le maillot, et je peux te dire quâ€™on est plusieurs ici Ã  avoir plus dâ€™Ã©toiles que le drapeau amÃ©ricain.\n\n\nReview main dans la main\n\n\n  La peer review, câ€™est la meilleure des reviews.\n\n\nPatterns\n\n\n  A : â€œJe pense que je vais devoir faire un singleton pour Ã§aâ€¦â€\n\n  B : â€œSinon, tu devrais pouvoir aussi faire des multitons ?â€\n\n  C : â€œOn appelle Ã§a une boÃ®te de conserve.â€\n\n\nà¶¸\n\n\n  Ã‡a devait Ãªtre en base64 des Maya\n\n\nTime is relative\n\n\n  Je pense que la premiÃ¨re fois ce sera long â€¦et aprÃ¨s ce ne sera pas long, mais ce sera long quand mÃªme.\n\n\n:bim:\n\n\n  Alors lâ€™intelligence elle existe dÃ©jÃ , elle sâ€™appelle vous-mÃªmeâ€¦ mais actuellement elle nâ€™est pas disponible.\n\n\nâ˜•ï¸\n\n\n  Tu peux mettre autant de code brouette dans tes pull request que de sucre dans ton cafÃ© : Lâ€™important, câ€™est que Ã§a soit bien diluÃ©. \nMoi, je ne sucre pas mon cafÃ©.\n\n\nğŸ”\n\n\n  Normalement jâ€™utilise un vrai IDE. Mais je ne sais pas monter le zoom sur PHPStorm\n\n\nEncore une histoire de date\n\n\n  Câ€™est une fois par semaine, les releases mensuelles ?\n\n"
} ,
  
  {
    "title"    : "API Platform Conference 2022",
    "category" : "",
    "tags"     : " conferences, backend, api, php",
    "url"      : "/2022/10/07/api-platform-conference-2022.html",
    "date"     : "October 7, 2022",
    "excerpt"  : "En cette pÃ©riode de rentrÃ©e, Bedrock participait Ã  lâ€™API Platform Conference 2022, oÃ¹ nous avons eu le plaisir dâ€™assister Ã  une partie des confÃ©rences proposÃ©es. Un grand merci Ã  toutes les personnes chez Les-Tilleuls.coop pour lâ€™organisation de c...",
  "content"  : "En cette pÃ©riode de rentrÃ©e, Bedrock participait Ã  lâ€™API Platform Conference 2022, oÃ¹ nous avons eu le plaisir dâ€™assister Ã  une partie des confÃ©rences proposÃ©es. Un grand merci Ã  toutes les personnes chez Les-Tilleuls.coop pour lâ€™organisation de cet Ã©vÃ¨nement !\nPour cette seconde Ã©dition, le programme Ã©tait rÃ©parti sur deux jours, les 15 &amp;amp; 16 septembre 2022.\n\nEn introduction Ã  cette confÃ©rence, KÃ©vin Dunglas, crÃ©ateur dâ€™API Platform, a mis en ligne la version 3.0.0 du framework en nous prÃ©sentant certaines nouvelles fonctionnalitÃ©s dÃ©veloppÃ©es telles que le support natif de XDebug. Il a profitÃ© de lâ€™occasion pour prÃ©senter un petit historique dâ€™API Platform.\n\nDomain-driven design with API Platform 3\n\nLors de cette confÃ©rence, Robin Chalas et Mathias Arlaud nous ont parlÃ© de lâ€™utilisation dâ€™API Platform dans le cadre du Domain Driven Development et de lâ€™Architecture Hexagonale.\nLes prÃ©sentateurs ont commencÃ© cette prÃ©sentation par plusieurs rappels et prÃ©sentations sur des sujets comme :\n\n  Domain Driven Design\n  Structures hexagonales\n  CQRS\n\n\nCes rappels ont permis dâ€™enchainer sur lâ€™utilisation du framework dans ce contexte Ã  travers un exemple de projet DDD utilisant API Platform 3 et suivant lâ€™architecture hexagonale.\n\nIls expliquent comment implÃ©menter API Platform dans notre code en dÃ©taillant plusieurs points :\n\n  Lâ€™implÃ©mentation des providers cÃ´tÃ© query\n  Lâ€™implÃ©mentation des processors cÃ´tÃ© command\n  Le systÃ¨me dâ€™opÃ©ration\n  Les providers/processors qui appellent lâ€™app via les bus\n\n\nComment Alice Gardenâ€™s gÃ¨re-t-elle son code mÃ©tier via les Ã©vÃ¨nements\n\nNous avons pu assister Ã  la confÃ©rence â€œComment Alice Gardenâ€™s gÃ¨re-t-elle le code mÃ©tier via des Ã©vÃ¨nementsâ€¯?â€ proposÃ©e par leur technical architect Nicolas Lemahieu. Tout dâ€™abord, il nous a prÃ©sentÃ© le contexte de son entreprise Alice Gardenâ€™s qui fait de la vente de mobilier dâ€™extÃ©rieur. En se basant sur la stack technique dÃ©jÃ  prÃ©sente : Symfony, RabbitMQ, MariaDB et sans tout refondre, comment faire pour mieux gÃ©rer le code mÃ©tier actuellement Ã©parpillÃ© un peu partout dans le code.\n\nIls utilisaient beaucoup de subscribers Doctrine, ce qui entraÃ®ne plusieurs problÃ¨mes :\n\n  Des subscribers nombreux = plus de logique au flush\n  Code plus difficile Ã  maintenir et Ã  comprendre\n  Lâ€™augmentation du risque de boucles infinies implique que chaque changement entraÃ®ne des boucles sur le UnitOfWork\n  Duplication de code\n  Code fortement couplÃ© Ã  Doctrine et manque de typage\n  Du cÃ´tÃ© du profiler Symfony, cela devient compliquÃ© aussi dÃ¨s quâ€™on commence Ã  en avoir beaucoup\n  Les tests sont compliquÃ©s :\n    \n      Unitaires quasi impossibles,\n      Fonctionnels possibles, mais demandent beaucoup de ressources en temps et donc dâ€™argent\n    \n  \n\n\nNicolas Lemahieu a donc prÃ©sentÃ© les diffÃ©rentes solutions envisagÃ©es ainsi que leurs avantages et inconvÃ©nients :\n\n  Domain Driven Development : sÃ©paration trÃ¨s nette du mÃ©tier et de lâ€™infra, mais demande beaucoup trop dâ€™effort Ã  mettre en place, car aucune correspondance avec les bundles dÃ©jÃ  existants (risque de rÃ©gression trop haut, coÃ»t de dÃ©veloppement trop grandâ€¦)\n  Garder les Ã©vÃ¨nements et sâ€™affranchir de Doctrine : câ€™est la solution qui a Ã©tÃ© retenue parce quâ€™elle permettait de rÃ©utiliser un maximum de lâ€™existant\n\n\nPuis, nous avons pu apprendre comment implÃ©menter cette solution :\n\n  CrÃ©ation dâ€™une abstraction supplÃ©mentaire â€œBusinessObjectâ€ : nouveau dossier Business dans src oÃ¹ :\n    \n      EntitÃ© = objet mÃ©tier\n      MÃ©thodes des entitÃ©s = rÃ¨gles mÃ©tier\n      Toutes les interfaces entitÃ© Ã©tendent BusinessObjectInterface\n    \n  \n  Classes abstraites dans Business\n  ImplÃ©mentation dâ€™events custom pour chaque objet mÃ©tier et par type dâ€™event\n  Event provider : fournit les Ã©vÃ¨nements qui sont mis dans une collection puis tag dans les services\n\n\nEn conclusion, chez Alice Gardenâ€™s, ils ont rÃ©ussi Ã  nâ€™avoir quâ€™un seul Doctrine Subscriber, donc une seule boucle de UnitOfWork et des tests facilitÃ©s, car câ€™est seulement du PHP. La dÃ©pendance Ã  Doctrine est Ã©liminÃ©e et il suffira de dÃ©placer le provider pour adapter le code Ã  une autre infrastructure.\n\nRÃ©utiliser et partager vos opÃ©rations personnalisÃ©es avec API Platform\n\nGrÃ¢ce Ã  Hubert Lenoir et JÃ©rÃ©my JarriÃ© de lâ€™entreprise SensioLabs, nous avons pu apprendre Ã  rÃ©utiliser et partager les opÃ©rations avec API Platform. Ils utilisent 3 API REST faites avec API Platform v3. Des opÃ©rations gÃ©nÃ©riques comme â€œlikerâ€ peuvent sâ€™appliquer sur des ressources diffÃ©rentes (articles, photos, pages, etc.), on peut donc rÃ©utiliser du code.\n\nLes principes pour faire cette utilisation gÃ©nÃ©rique de code sont simples :\n\n  Un seul contrÃ´leur pour plusieurs ressources = un contrÃ´leur de comportement\n  Une interface pour que les entitÃ©s puissent adopter ce comportement\n  Trait pour les mÃ©thodes du comportement (1 Ã  n comportements, donc classe abstraite ou interface impossible)\n  Ajouter des services intermÃ©diaires\n\n\nIl survient un seul problÃ¨me avec ce pattern : la duplication des annotations API Platform. La solution est dâ€™ajouter des dÃ©corations (design pattern decorator) sur les mÃ©tadatas dâ€™API Platform. Il est donc simple de crÃ©er des comportements indÃ©pendants des ressources qui pourront Ãªtre facilement rÃ©utilisÃ©s. Ce projet Ã©tait encore Ã  lâ€™Ã©tat de POC, mais JÃ©rÃ©my et Hubert allaient mettre Ã  jour la version plus aboutie sur le repository GitHub.\n\nLa revue de code est un art\n\nSmaine Milianni a proposÃ© une confÃ©rence sur les conseils Ã  suivre afin dâ€™effectuer une revue de code. Nous avons pu rÃ©aliser quâ€™au sein de Bedrock nous appliquions dÃ©jÃ  de nombreux principes.\n\nNous appliquons dÃ©jÃ  :\n\n  Un template de PR pour dÃ©crire les caractÃ©ristiques du bug ou de lâ€™US :\n    \n      Quoi, pourquoi, comment, comment tester, etc.\n    \n  \n  Des noms de commits explicites\n  La bienveillance\n  Le challenge du code des autres personnes\n  La connaissance des nombreux concepts de code (SOLID, KISSâ€¦)\n  Faire du pair review ou mob review\n  Un request bot dÃ©jÃ  en place\n  Tester et ne pas se fier uniquement Ã  la lecture du code\n\n\nNous avons aussi pu prendre du recul et noter des conseils Ã  appliquer. Chacun a pu transmettre ses idÃ©es Ã  son Ã©quipe. Le rappel quâ€™une revue câ€™est aussi souligner le positif et pas seulement challenger le code. Cette confÃ©rence est trÃ¨s concrÃ¨te et facilement applicable Ã  Bedrock.\n\nFighting impostor syndrome: a practical handbook\n\nLors de cette confÃ©rence, Marine Gandy commence sa prÃ©sentation par dÃ©finir ce que le syndrome de lâ€™imposteur est : une peur de lâ€™Ã©chec, la crainte que quelquâ€™un dise que nous ne sommes pas capables, mais aussi le sentiment de ne pas mÃ©riter de rÃ©ussir.\nElle nous explique que ce terme Ã©tait tout dâ€™abord attribuÃ© exclusivement aux femmes, mais quâ€™aprÃ¨s une Ã©tude montrant que 70% de la population Ã©tait touchÃ©e, il se serait gÃ©nÃ©ralisÃ© Ã  tous les genres.\nMarine Gandy nous Ã©nonce que la tech est trÃ¨s touchÃ©e par ce phÃ©nomÃ¨ne pour plusieurs raisons comme le fait quâ€™il y ait beaucoup de renouveau dans ce corps de mÃ©tier et que nous avons vite lâ€™impression de retourner Ã  nos dÃ©buts lorsque nous changeons de techno, crÃ©ant ainsi un sentiment dâ€™instabilitÃ©. \nDans ce contexte, Marine nous parle de lâ€™effet Julien Lepers en prenant pour exemple le fait de se trouver dans une Ã©quipe de personnes bien plus expÃ©rimentÃ©es que nous oÃ¹ la situation influe sur la personne.\nPour finir, la confÃ©renciÃ¨re nous prÃ©sente plusieurs pistes Ã  suivre pour Ã©viter ou minimiser ce genre de sentiment :\n\n  ArrÃªter de se comparer aux autres\n  Se challenger sur de nouveaux domaines pour se rendre compte quâ€™on peut toujours apprendre\n  Travailler sur ses faiblesses pour permettre de se sentir plus compÃ©tent\n\n\nMon combat contre lâ€™arachnophobie\n\nJÃ©rÃ´me Tanghe, par son arachnophobie, nous a expliquÃ© comment il est arrivÃ© Ã  contribuer Ã  API Platform afin dâ€™ajouter une option pour cacher la mascotte Webby. Et câ€™est ce dont il nous a parlÃ©, Ã  savoir comment bien dÃ©marrer sa premiÃ¨re pull request pour contribuer au logiciel libre. La premiÃ¨re Ã©tape Ã©tant de trouver le bon repository qui nous conviendrait parmi les projets existants. Dans le but dâ€™identifier un sujet sur lequel contribuer, il ne faut pas hÃ©siter Ã  utiliser la fonctionnalitÃ© des tags sur les issues, par exemple le tag hacktoberfest dans le cadre dâ€™API Platform. Une fois le sujet trouvÃ©, il faut maintenant identifier la branche de base Ã  partir de laquelle faire sa pull request, cela peut sâ€™agir dâ€™une version spÃ©cifique choisie ou mÃªme de la branche principale. La contribution au logiciel libre ou Ã  lâ€™open source ne passe pas uniquement par des pull requests uniquement basÃ©s sur le code. Il est Ã©galement possible de tester les prÃ©versions (release-candidate), signaler des bugs, faire des suggestions, amÃ©liorer la documentation ou encore rÃ©diger des traductions. Enfin, il est conseillÃ© de prendre en compte chaque retour sur dâ€™autres pull requests, cela permet notamment de dÃ©couvrir les principes et standards du projet.\n\nPourquoi je nâ€™utilise pas API Platform\n\nFrÃ©dÃ©ric Bouchery, sâ€™est dÃ©cidÃ©ment perdu en se retrouvant Ã  prÃ©senter cette confÃ©rence Ã  lâ€™API Platform conference 2022. MalgrÃ© tout, cela lui a permis de nous partager son introspection : Mais au fait, pourquoi il ne sâ€™en sert pas ?\nDans cette premiÃ¨re partie de sa confÃ©rence, FrÃ©dÃ©ric nâ€™hÃ©site pas Ã  utiliser beaucoup de sarcasme. Il nous explique quâ€™il ne sâ€™en sert pas, car API Platform est Ã©crit en PHP et pour lui, câ€™est une technologie vieillissante qui ne devrait pas tarder Ã  rejoindre Cobolt. Ã‰galement parce quâ€™API Platform utilise Symfony, alors que tout le monde le sait trÃ¨s bien, enfin surtout les Google Trend, Laravel est plus utilisÃ© dans le monde. De plus, FrÃ©dÃ©ric nâ€™aime pas la magie et API Platform en est rempli : sÃ©rialisation et dÃ©sÃ©rialisation Ã  tout va alors que lui est capable de faire une API en seulement quelques lignes avec du PHP pur sans artifice. Enfin, il reproche Ã  API Platform de devenir compliquÃ© Ã  utiliser si le projet qui se base dessus est complexe, trop de personnalisation et de configurations doivent Ãªtre effectuÃ©es. \nDans cette deuxiÃ¨me partie, FrÃ©dÃ©ric fait tomber son masque sarcastique et dÃ©cide de revenir sur les points quâ€™il a abordÃ©s prÃ©cÃ©demment :\n\n  Les tendances concernant un langage ne sont pas des bons indicateurs, en effet imaginer le futur ou la mort de PHP via des statistiques dont certaines basÃ©es sur lâ€™opinion publique nâ€™est pas une bonne faÃ§on de faire\n  PHP, câ€™est aujourdâ€™hui 75% des sites du monde entier et 54% parmi le top 1000 des sites internet frÃ©quemment utilisÃ©s\n  MalgrÃ© tout, il nous conseille de ne pas Ãªtre mono technologique non plus. Sâ€™intÃ©resser Ã  dâ€™autres langages est une bonne chose\n  API Plateform a quand mÃªme de bonnes performances : 99% de rÃ©ponses avec une moyenne de 91â€‰ms sur 5 000 requÃªtes comparÃ©es Ã  son code PHP pur avec une moyenne de 21â€‰ms pour 5 000 requÃªtes Ã©galement sachant que son code ne prend pas en compte la sÃ©curitÃ©\n  Le framework Laravel utilisant des composants Symfony, il est dÃ¨s lors difficile de les comparer. MÃªme si factuellement Laravel est plus utilisÃ© dans le monde, on nâ€™utilise pas du Laravel ou du Symfony pour les mÃªmes raisons et câ€™est une bonne chose que les deux coexistent ensemble\n  Il pensait quâ€™avec la complexitÃ© de ses projets, il Ã©tait trop dur de passer Ã  API Platform, mais il sâ€™est rendu compte que ce nâ€™Ã©tait pas nÃ©cessairement vrai\n\n\nEn conclusion de sa confÃ©rence et sans aucun sarcasme, FrÃ©dÃ©ric nâ€™hÃ©site pas Ã  se livrer Ã  nous et finit par nous dire quâ€™il va finalement utiliser API Platform 3 pour un projet.\n\nWhatâ€™s New in Caddy, the webserver of API Platform\n\nFrancis Lavoie nous a prÃ©sentÃ© plusieurs nouveautÃ©s dans Caddy, un webserver Ã©crit en Go ayant beaucoup de fonctionnalitÃ©s activÃ©es par dÃ©faut et fourni avec API Platform dans lâ€™installation de base.\n\nParmi les nouveautÃ©s prÃ©sentÃ©es, il nous a notamment parlÃ© dâ€™amÃ©liorations au niveau des request matchers avec des matchers rÃ©utilisables, des expressions et des fonctions. Il a ensuite parlÃ© dâ€™une gestion native de Authelia permettant de dÃ©lÃ©guer facilement lâ€™authentification depuis la configuration du serveur.\nEnfin, la directive file_server peut maintenant servir des fichiers provenant dâ€™autres sources que le systÃ¨me de fichiers local, par exemple depuis un bucket S3.\n\nCertaines fonctionnalitÃ©s sont dÃ©sormais activÃ©es par dÃ©faut comme HTTP/3 et la suppression du log des headers dâ€™authentification oÃ¹ il est Ã©galement possible de crÃ©er des filtres pour retirer dâ€™autres informations.\n\nWebAuthn : se dÃ©barrasser des mots de passe. DÃ©finitivement.\n\nFlorent Morselli nous fait une proposition : il est de plus en plus possible aujourdâ€™hui de se passer complÃ¨tement des mots de passe.\n\nIl a commencÃ© par nous rappeler les problÃ¨mes rÃ©currents : mots de passe trop faibles et/ou trop courts, rÃ©utilisation sur plusieurs sites, la multiplication des fuites de bases de donnÃ©es, etc.\n\nLa solution proposÃ©e : WebAuthn, un standard dâ€™authentification multifacteur permettant dâ€™identifier les utilisateurs via des donnÃ©es biomÃ©triques, des clÃ©s physiques ou sans aucune information aprÃ¨s la premiÃ¨re authentification sur un appareil.\n\nCÃ´tÃ© implÃ©mentation, Florent nous a prÃ©sentÃ© deux projets : un bundle Symfony pour le cÃ´tÃ© backend et un composant Symfony UX pour le frontend.\n\nPHP WebSockets, or how to communicate with clients in real-time\n\nHabituellement connue pour faire des confÃ©rences sur Git, Pauline Vos nous a fait une dÃ©mo en live de lâ€™utilisation des WebSockets en PHP.\n\nElle a commencÃ© par une rapide explication de diffÃ©rents protocoles de communication en temps rÃ©el existant : WebRTC chez Google, Mercure chez Symfony et Livewire chez Laravel.\nLes WebSockets Ã©tant de simples tunnels Ã  donnÃ©es, ces protocoles permettent de les enrichir de diverses fonctionnalitÃ©s : identification, structures de messages, reconnexion auto, etc.\n\nVient ensuite la dÃ©mo qui consistait en une mini webapp de tombola en ligne. Elle a Ã©tÃ© dÃ©coupÃ©e en diffÃ©rentes Ã©tapes (prÃ©parÃ©es dans des branches Git) avec, pour chaque Ã©tape, une prÃ©sentation du code et des tests en live via lâ€™outil WebSocketKing.\nPour lâ€™Ã©tape finale, un QR code a Ã©tÃ© affichÃ© Ã  lâ€™Ã©cran pour permettre aux spectateurs et spectatrices de participer en live. Le hasard a voulu que le nom tirÃ© soit Antoine Bluchet, le contributeur principal dâ€™API Platform !\n\nComment (re)mettre la tech au service du bien commun ?\n\nPour conclure ces deux jours de confÃ©rences intenses en savoir et en Ã©motions, animÃ© par GrÃ©gory CopinÂ : HÃ©lÃ¨ne Marchois, Paul Andrieux et KÃ©vin Dunglas nous ont proposÃ© une excellente table ronde riche en idÃ©es et porteuse (dâ€™un peu) dâ€™espoir. Le sujet Ã©tant de savoir sâ€™il est possible de faire Ã©voluer la tech dans le but de rejoindre les objectifs de dÃ©part du logiciel libre.\n\nLâ€™apparition du mouvement du logiciel libre puis celle du web se sont bÃ¢ties sur de grands espoirs et de beaux objectifsÂ : Ã©mancipation des individus, partage des connaissances Ã  lâ€™Ã©chelle planÃ©taire, libertÃ© dâ€™expression, constructions de bien commun appartenant Ã  toutes et tous et maintenues collectivement. Malheureusement, force est de constater que le web comme le logiciel libre ont Ã©tÃ© dÃ©tournÃ©s de leurs objectifs de base et que les idÃ©auxÂ quâ€™ils portaient ont Ã©tÃ© bien mis Ã  malÂ : surveillance de masse, capitalisation de ces biens communs et prÃ©carisation des individus et des libertÃ©s.\n\nKÃ©vin nous explique ensuite la diffÃ©rence entre logiciel libre et open source : API Platform est un logiciel libre plus quâ€™open source, mÃªme si techniquement, câ€™est les deux. Historiquement, le logiciel libre est apparu dans le but de crÃ©er un bien commun pour lâ€™humanitÃ© et sâ€™est Ã©largi avec, notamment, la notion de commons via WikipÃ©dia. La diffÃ©rence avec lâ€™open source est que si le code est disponible, ce nâ€™est pas uniquement pour bÃ¢tir tout et nâ€™importe quoi avec, mais câ€™est un code qui porte des valeurs et qui a pour but de faire en sorte que tout le monde sans distinction puisse facilement crÃ©er de nouveaux outils qui puissent Ãªtre partagÃ©s, qui appartiennent Ã  un ensemble de personnes et qui vont socialiser le travail qui est rÃ©alisÃ© en commun lÃ -dessus. Le but du logiciel libre Ã  la base, câ€™est de faire en sorte que ces valeurs de transparence, de dÃ©mocraties, de partage de connaissance sâ€™Ã©tendent via le logiciel Ã  lâ€™ensemble de la sociÃ©tÃ©. Donc si vous aussi, vous voulez utiliser un logiciel libre, la condition est que, vous aussi, vous devez faire quelque chose qui sert lâ€™humanitÃ© : crÃ©er un bien commun et mettre aussi Ã  disposition le code source du logiciel. En lâ€™occurrence, API Platform, est une licence permissive, câ€™est-Ã -dire quâ€™il est possible de faire tout et nâ€™importe quoi avec, mais ce nâ€™est pas le cas pour le logiciel Mercure par exemple, oÃ¹ si vous lâ€™utilisez et le modifiez, vous Ãªtes obligÃ© de redistribuer les Ã©lÃ©ments.\n\nQuant Ã  lâ€™open source, câ€™est une initiative qui est arrivÃ©e bien aprÃ¨s le logiciel libre et est une offensive de multinationale de la technologie qui veut dÃ©politiser le mouvement du logiciel libre. Le point de dÃ©part Ã©tant que, techniquement, câ€™est trÃ¨s intÃ©ressant de crÃ©er du logiciel ensemble, de partager les coÃ»ts de maintenance entre diffÃ©rentes entreprises ou personnes et câ€™est surtout trÃ¨s intÃ©ressant dâ€™avoir accÃ¨s au secret de fabrication pour les choses qui ont peu de valeur ajoutÃ©e. Mais lâ€™objectif final Ã©tant de capitaliser, faire du business et capter la valeur sur ce qui a une trÃ¨s forte valeur ajoutÃ©e. Par exemple, pour macOS, toutes les briques de bases sont complÃ¨tement libres, dÃ©veloppÃ©es par une communautÃ© de personne, dâ€™entreprise et essentiellement beaucoup de bÃ©nÃ©voles et dâ€™hobbyiste. Et dans ce cas-lÃ , ce qui a une extrÃªme valeur ajoutÃ©e, câ€™est lâ€™UI au-dessus du matÃ©riel ou encore les jolis outils qui coÃ»tent une fortune. Ce qui permet Ã  Apple dâ€™Ãªtre la boite la plus riche du monde en rÃ©utilisant le travail de personnes qui nâ€™ont pas fait Ã§a pour macOS Ã  la base.\n\nLes trois personnes intervenantes reprÃ©sentant chacune une SCOP, la table ronde sâ€™est ensuite naturellement tournÃ©e vers le lien entre le logiciel libre et le mouvement coopÃ©ratif. Le lien Ã©tant la vision politique du logiciel libre via son socle de valeurÂ : libertÃ©, transparence, gouvernance partagÃ©e et coopÃ©ration. On retrouve cet esprit de transparence, de fonctionnement dÃ©mocratique et de fonctionnement par coopÃ©ration Ã  lâ€™intÃ©rieur de la SCOP et entre les diffÃ©rentes SCOP.\nSâ€™en est ajoutÃ©e la question du sens par rapport Ã  son travail. Effectivement, le logiciel libre, comme le mouvement coopÃ©ratif, redonne du sens, principalement car cela ouvre le champ des possibles en vue des enjeux climatiques et sociaux actuels. MÃªme si lâ€™on vit dans une sociÃ©tÃ© qui est rÃ©gie par le profit, la compÃ©tition fÃ©roce et le pouvoir, il existe des possibilitÃ©s de sâ€™organiser autrement et qui fonctionne quand mÃªme Ã  une Ã©chelle consÃ©quente, bien quâ€™encore insuffisante. Des actions individuelles existent et sont possibles. Pour cela, nous vous recommandons de regarder la confÃ©rence dâ€™HÃ©lÃ¨ne Ã  lâ€™API Platform Conference de lâ€™annÃ©e derniÃ¨re quâ€™elle rÃ©sume et Ã©toffe lors de cette table ronde.\n\nEt bien sÃ»r, quand cela sera possible, nous vous encourageons fortement de regarder le replay de cette confÃ©rence (sur la chaine des Tilleuls) qui redonne un peu dâ€™espoir quant aux futurs des organisations dÃ©mocratiques de nos mÃ©tiers.\n\nConclusion\nMerci Ã  toutes et tous les speakers, Ã  API Platform ainsi quâ€™aux Tilleuls-coop pour cet Ã©vÃ¨nement ! Nous avons pu en apprendre plus sur API Platform et revenir la tÃªte pleine dâ€™idÃ©es pour nos projets futurs et prÃ©sents ! Ã€ lâ€™annÃ©e prochaine peut-Ãªtre !\n"
} ,
  
  {
    "title"    : "Turn off your fracking notifications #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/turn-off-your-notifications",
    "date"     : "September 30, 2022",
    "excerpt"  : "Turn off your fracking notifications.\nPrÃ©sentÃ© par Fabien Dumas.\n",
  "content"  : "Turn off your fracking notifications.\nPrÃ©sentÃ© par Fabien Dumas.\n"
} ,
  
  {
    "title"    : "Nearby interaction, Airtags or how your iPhone shares your location #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/nearby-interaction-airtags",
    "date"     : "September 30, 2022",
    "excerpt"  : "DÃ©couvrez comment votre iPhone partage votre localisation.\nPrÃ©sentÃ© par Oleksandr Balystky.\n",
  "content"  : "DÃ©couvrez comment votre iPhone partage votre localisation.\nPrÃ©sentÃ© par Oleksandr Balystky.\n"
} ,
  
  {
    "title"    : "La facilitation spectacle : Entre artifices et intention #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-facilitation-spectacle",
    "date"     : "September 30, 2022",
    "excerpt"  : "La facilitation spectacle : Entre artifices et intention.\nPrÃ©sentÃ© par Camille Cousin &amp;amp; Marie-AndrÃ©e Jolibois.\n",
  "content"  : "La facilitation spectacle : Entre artifices et intention.\nPrÃ©sentÃ© par Camille Cousin &amp;amp; Marie-AndrÃ©e Jolibois.\n"
} ,
  
  {
    "title"    : "Courir: la voi(e/x) du fondeur #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/courir-la-voie-du-fondeur",
    "date"     : "September 30, 2022",
    "excerpt"  : "Courir: la voi(e/x) du fondeur\nPrÃ©sentÃ© par Thomas Sontag.\n",
  "content"  : "Courir: la voi(e/x) du fondeur\nPrÃ©sentÃ© par Thomas Sontag.\n"
} ,
  
  {
    "title"    : "Comment cloner Shazam ! #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-cloner-shazam",
    "date"     : "September 30, 2022",
    "excerpt"  : "DÃ©couvrez comment cloner Shazam!\nPrÃ©sentÃ© par Moustapha Agack.\n",
  "content"  : "DÃ©couvrez comment cloner Shazam!\nPrÃ©sentÃ© par Moustapha Agack.\n"
} ,
  
  {
    "title"    : "Chaos engineering dans le frontend #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/chaos-engineering-frontend",
    "date"     : "September 30, 2022",
    "excerpt"  : "DÃ©couvrez comment faire du chaos engineering dans le frontend.\nPrÃ©sentÃ© par Thibaud Courtoison.\n",
  "content"  : "DÃ©couvrez comment faire du chaos engineering dans le frontend.\nPrÃ©sentÃ© par Thibaud Courtoison.\n"
} ,
  
  {
    "title"    : "Bullet journal #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/bullet-journal",
    "date"     : "September 30, 2022",
    "excerpt"  : "Bullet Journal.\nPrÃ©sentÃ© par BÃ©nÃ©dicte Garcia.\n",
  "content"  : "Bullet Journal.\nPrÃ©sentÃ© par BÃ©nÃ©dicte Garcia.\n"
} ,
  
  {
    "title"    : "Subtitles, open captions, closed captions, SDH, oh my!",
    "category" : "",
    "tags"     : " streaming, subtitles, captions, video, player",
    "url"      : "/2022/09/20/captioning-in-the-streaming-world.html",
    "date"     : "September 20, 2022",
    "excerpt"  : "Subtitles, open captions, closed captions, SDH, oh my!\n\nWait, what? Subtitles and captions are not the same? Have you noticed the popular Â« cc Â» logo in a player you use, standing for Â« Closed Captions Â» but you have never heard of Open Captions? ...",
  "content"  : "Subtitles, open captions, closed captions, SDH, oh my!\n\nWait, what? Subtitles and captions are not the same? Have you noticed the popular Â« cc Â» logo in a player you use, standing for Â« Closed Captions Â» but you have never heard of Open Captions? In this article we are going to dive into the different textual representations used in the streaming world.\n\nSubtitles vs Captions: a matter of accessibility\n\nSince both terms are often mixed up, in this post we are going to explain in details the different types of subtitles and captions.\n\nSubtitles exist in order to help the viewer understand the spoken language in the content being watched, assuming that the viewer can hear. This is the important part. You can think of subtitles as the closest translation of what is being said, textually represented on the screen.\n\nClosed Captions\n\nClosed Captions on the other hand, assume that the viewer is deaf (or hard of hearing) hence cannot understand what is being said, regardless of the spoken language. For this reason and contrary to subtitles, it will describe spoken dialogues as well as all important audio information such as music, sounds, speaker information when it makes sense (for example narrated content). In terms of appearance, closed captions are usually white text on a black background. An important note is that they are not supported through digital connections such as HDMI.\n\nSubtitles and closed captions are separate files that provide information for the receiver to decode. Theyâ€™re not part of the stream and both can be turned off.\n\nSubtitles for the Deaf and Hard of Hearing\n\nSubtitles for the Deaf and Hard of Hearing, known as SDH, are a combination between subtitles and closed captions. They can be in the same language of the video original audio and bring some additional non-spoken information (speaker identification, sound effects, etc.) and/or be translated. This makes the content accessible for the deaf and hard of hearing who can read and understand foreign languages.\n\nOpen Captions\n\nThe most important difference between Closed and Open Captions is that Open Captions are always visible and cannot be turned off. Think of them as Â« burned Â» in the video stream: they are not a separate file. Because of this, quality and readability may be affected. Open captions are widely used on social media for retention. Since there is a high chance that the end user is scrolling through content without sound, ensuring the display of text on the video helps catch and retain attention. In other cases where closed captions cannot be used for example if you have no control on the media player that will play your file, you may provide open captions to be sure to display a textual translation. The downside might be that part of the audience dislikes the superfluous text burned in the stream.\n\nForced subtitles\n\nThere is often a misconception around forced subtitles (sometimes referred as forced narratives) as they are mistaken with open captions. The name Â« forced Â» might suggest that they are burned in the video stream like open captions but there is a difference. Forced subtitles are actually distributed in a separate file and, despite their name, are not necessarily displayed. On our platform, if a subtitle or closed captions track is selected by the user, forced subtitles will not show up. We will come back to this later.\n\nActually, forced subtitles are a text representation of a communication element like a spoken dialogue, specify a character ID that are not described in the original (or dubbed) audio stream. A common example would be to translate alien language. Despite watching a movie in your native (or any language that does not require you to activate subtitles), you would not be able to understand. Thatâ€™s where forced subtitles come into play and ensure that you have a textual representation of what is being said even if you set subtitles to off, hence the Â« forced Â» attribute.\n\nHowever, imagine you are French and watch a Spanish show for instance. If you set the subtitles to French in order to be able to understand the content, forced subtitles wonâ€™t show up since you already have a textual representation of the content. Same goes for closed captions: if set to off, forced subtitles will display, if any. Otherwise, they wonâ€™t show up. To ensure better user experience, forced subtitles content should be included in all other tracks (regular subtitles, SDH, CC).\n\nIf available, forced subtitles should be displayed in the preferred language set up by the user.\n\nTo sum up, here is a table comparing the different technologies:\n\n\n  \n    \n      Â \n      Subtitles\n      SDH\n      Closed Captions\n      Open Captions\n      Forced Subtitles\n    \n  \n  \n    \n      Can be turned off\n      âœ”ï¸\n      âœ”ï¸\n      âœ”ï¸\n      Â \n      *\n    \n    \n      Appearance\n      Varies\n      Varies\n      Usually white text / black background\n      Varies\n      Varies\n    \n    \n      Position\n      Bottom third, centered\n      Bottom third, centered\n      Varies\n      Varies\n      Bottom third, centered\n    \n    \n      HDMI Supported\n      âœ”ï¸\n      âœ”ï¸\n      Â \n      âœ”ï¸\n      âœ”ï¸\n    \n    \n      Describes music and sounds\n      Â \n      âœ”ï¸\n      âœ”ï¸\n      âœ”ï¸\n      Â \n    \n    \n      Describes speaker ID\n      Â \n      âœ”ï¸\n      âœ”ï¸\n      âœ”ï¸\n      Â \n    \n    \n      Available in source language\n      Â \n      âœ”ï¸\n      âœ”ï¸\n      âœ”ï¸\n      âœ”ï¸\n    \n  \n\n\n*: Forced subtitles do not show up in the track selection tool making them impossible to turn on or off. However, the business rules of the media platform will take care of displaying them, if needed.\n"
} ,
  
  {
    "title"    : "Monitoring at scale with Victoria Metrics",
    "category" : "",
    "tags"     : " k8s, kubernetes, monitoring, prometheus, scaling, victoriametrics, cardinality",
    "url"      : "/2022/09/06/monitoring-at-scale-with-victoriametrics.html",
    "date"     : "September 6, 2022",
    "excerpt"  : "Monitoring at Bedrock :\nAt Bedrock Streaming, a large part of our applications are hosted on Kubernetes clusters, others use the EC2 service from AWS and a small part are hosted on â€œOnPremiseâ€ servers.\n\nFrom 2018 until January 2022, we used Promet...",
  "content"  : "Monitoring at Bedrock :\nAt Bedrock Streaming, a large part of our applications are hosted on Kubernetes clusters, others use the EC2 service from AWS and a small part are hosted on â€œOnPremiseâ€ servers.\n\nFrom 2018 until January 2022, we used Prometheus to monitor all these platforms, because Prometheus met all our needs: keeping control over our monitoring solution and supporting service discovery, which is essential in environments such as Kubernetes or AWS EC2. Prometheus also supports custom exporters we developed internally.\n\nOver the years, our business has grown significantly, so the load on our platforms has increased. Indirectly, the load on our Prometheus instances has also increased, to the point where certain limitations have become too much for us. This is why we have changed our monitoring/alerting stack.\n\nLimits of Prometheus\nPrometheus does not have a native High Availability mode: to have high availability, we had to duplicate our Prometheus instances. This implies that our targets were â€œscrappedâ€ by all our Prometheus instances (same for our rules and records).\nTo avoid this, we had to use sharding, but this made the infrastructure more complex. More information on this subject in this documentation from the Prometheus operator\n\nPrometheus is not designed to store metrics on a long-term basis, as mentioned in the documentation :\n\n&amp;gt; Prometheusâ€™s local storage is not intended to be durable long-term storage; external solutions offer extended retention and data durability.\n\nPrometheusâ€™s local storage is not intended to be durable long-term storage; external solutions offer extended retention and data durability.\nWe worked around this limitation by using Victoria Metrics (VMCluster) as a LongTermStorage via the remote_write protocol\n\nAll processes (scrapping, ingest, storage, etc.) were, until now, managed in the same â€œprometheusâ€ instance, which implied a less flexible and vertical scaling only (since recently a Prometheus agent is available for the â€œscrappingâ€ part).\n\nThe RAM and CPU usage of a Prometheus instance is correlated to the number of metrics (and their cardinality) it has to manage. In our case, several Prometheus instances consumed more than 64 GB of RAM and 26 CPUs each, in order to absorb our peak loads. In a Kubernetes cluster, this high resources consumption can cause problems, especially for scheduling.\n\nThe Write-Ahead Log (WAL) system can cause rather slow restarts if the Prometheus instance runs out of RAM and can cause the Prometheus instance to hang for varying lengths of time. During the replay of the WAL, Prometheus doesnâ€™t scrape anything, thus there is no alerting and no way of knowing if something is going on.\n\nThe cardinality of metrics\nWhen our Kubernetes clusters manage a large number of pods, a constraint quickly appears: cardinality.\n\n&amp;gt; The cardinality of a metric is the number of TimeSeries of that metric with single-valued labels.\n\n\n\nIn the example above, the status_code label has a cardinality of 5, app has a cardinality of 2 and the overall cardinality of the server_reponses metric is 10.\n\nIn this example, any Prometheus instance can handle this cardinality, but if you add for example the label pod_name or client_IP (or both) to the server_reponses metric, the cardinality increases for each different clients calls and for each pod.\n\nYou should read the excellent article from â€œRobust Perceptionâ€ for more details on this subject.\n\nAt Bedrock the high cardinality metrics come from our HAProxy ingress. For our needs, we retrieve several labels like the name of the ingress pod as well as its IP address, but more importantly the name and IP address of the destination pod. In a cluster that can grow to more than 15,000 pods, the combination of unique labels (cardinality) is very significant for some of our ingress metrics.\n\nWe found that Prometheus performed poorly when we had multiple metrics with high cardinalities (&amp;gt; 100,000), and resulted in over-consumption of RAM.\n\nDuring a high load event, Prometheus could consume up to 200 GB of RAM before being OOMKilled. When this happened, we would go completely blind as we had no metrics or alerting.\nThis also impacts us on scalability in our Kubernetes clusters, as we use CustomMetrics very heavily in HPAs to scale the number of pods in our applications.\n\nRAM and CPU consumption of our prometheus instances (the red lines represent the reboots of our instances, we also see a loss of metrics)\n\n\n\nPrometheus is still a good solution, which has served its purpose well for several years, but we have reached its limits in our production environments.\n\nReplacing Prometheus?\n\nWe spent time optimizing Prometheus to absorb the amount of metrics and their cardinality, in particular by either directly removing high cardinality metrics if they were totally unused, or by removing the labels of certain metrics that caused high cardinalities.\nWe have also optimized the Prometheus configuration directly, as well as the maximum IOPS of our EBS. The RAM and CPU consumption of Prometheus is linked to the number of metrics to manage and their cardinality. But we always have more traffic and therefore always more pods in our clusters: we should have perpetually increased Prometheus instances resources. This was a problem for scalability and costs.\n\nCan we replace a critical tool like this? What are our short, medium and long term needs? How can we optimize costs? And especially in what timeframe?\nThe emergency of recent incidents forced us to exclude solutions such as Thanos and Cortex. Testing these solutions completely would have required too much time, which we did not have.\n\nIt is also important to consider that we were already using Victoria Metrics, but only for the Long Term Storage part, without any problems.\nCould replacing Prometheus with a stack based entirely on Victoria Metrics overcome the limitations we had with Prometheus?\nHigh availability and fault tolerance is well-supported, their documentation explains how to manage this.\n\nManaging long-term data is possible, as we were already doing it.\nVictoria Metrics is built around a set of microservices. Each one is built in order to serve a specific job, and each supports vertical and especially horizontal scaling (with sharding). A very important point when used in a Kubernetes environment.\n\nIn addition, Victoria Metrics seemed to handle high cardinality metrics better (see article on this subject). It is also possible to do rate limiting on the number of Time Series to be ingested:\n\nCPU and RAM consumption is lower with better performance than with Prometheus and even other TSDBs, several comparative articles on this subject have already been published:\n\n  Remote Write Storage Wars\n  Prometheus vs VictoriaMetrics benchmark on node_exporter metrics\n  When size matters â€” benchmarking VictoriaMetrics vs Timescale and InfluxDB\n  Comparing Thanos to VictoriaMetrics cluster\n\n\nWe also wanted to keep the Prometheus language: PromQL in order to keep our Grafana dashboards and all our Prometheus alerts. Even though Victoria Metrics offers its own MetricsQL language, it is perfectly compatible with PromQL.\n\nYou can see the main features of Victoria Metrics as well as various case studies in their documentation.\n\nPOC of Victoria Metrics\nWe wanted to validate the performance and consumption of a stack entirely based on Victoria Metrics, the results were really encouraging.\n\nTest environment :\n\n  1500 web app pods\n  250 Haproxy Ingress pods (metric with high cardinality enabled)\n  3700 scrapped targets\n\n\nComparative table between Prometheus and Victoria Metrics :\n\n\n  \n    \n      Â \n      Prometheus\n      Victoria Metrics\n    \n  \n  \n    \n      CPU consumption\n      26\n      8\n    \n    \n      RAM consumption\n      30Go\n      11Go\n    \n    \n      New TimeSeries / min\n      50K\n      6.5M\n    \n    \n      Max active TimeSeries\n      7M\n      91M\n    \n    \n      Max cardinality\n      4 metrics &amp;gt; 100K\n      10+ metrics &amp;gt; 1M\n    \n  \n\n\nGraph on the CPU consumption of Victoria Metrics components\n\n\nNumber of active â€œTimeSeriesâ€ in Victoria Metrics\n\n\nOur benchmark persuaded us to use Victoria Metrics as a replacement for Prometheus.\n\nImplementation of Victoria Metrics :\nWe used the official victoria-metrics-k8s-stack Helm chart which is based on an operator. This chart Helm permits to deploy a complete monitoring and alerting stack in a Kubernetes cluster.\n\nA VMCluster (Insert, Select, Storage) is deployed to manage access to metrics. The collection of metrics (push/pull) from exporters in Prometheus format is handled by the VMagent. Its configuration is done in the form of a Prometheus configuration file. It is able to :\n\n  Manage the relabeling of metrics.\n  Temporarily store the metrics it has collected if the VMCluster is unavailable or not able to send the metrics to the VMCluster.\n  Limit the cardinality of metrics.\n\n\nOne of the advantages of using this Helm chart is that it will deploy essential components to properly monitor a Kubernetes cluster such as Kube-state-metrics or prometheus-node-exporter, but also scraping configurations for services such as Kubelet, KubeApiServer, KubeControllerManager, KubeDNS, KubeEtcd, KubeScheduler, KubeProxy\n\nAlerting is also managed via a VMAlert component, which will execute the alerting and recording rules set by VictoriaMetrics. Notifications are managed by an Alertmanager which is also deployable via this chart.\n\nOne of the advantages of using this Helm chart is that it will deploy essential components to properly monitor a Kubernetes cluster such as Kube-state-metrics or prometheus-node-exporter, but also scraping configurations for services such as Kubelet, KubeApiServer, KubeControllerManager, KubeDNS, KubeEtcd, KubeScheduler, KubeProxy\n\nThis is what our monitoring and alerting stack based on this Helm chart looks like.\n\n\nResumption of the history\nWe wanted to keep historical metrics of our Kubernetes clusters. Victoria Metrics provides a tool to manage the export and import of data from different TSDB: vmctl.\n\nIn order not to overload our monitoring stack, we splitted the exports into smaller or larger time ranges, depending on the history of the cluster. For clusters with little activity and therefore few metrics, exports/imports were split day by day, for others we had to use smaller time slots.\nA home-made bash script launched several kubernetes jobs simultaneously and took care of restarting one of them as soon as another one ended.\n\nBelow an extract of the definition of our Kubernetes job with the arguments we used to do our history transfer by time range:\n\n      containers:\n      - name: vmctl\n        image: victoriametrics/vmctl\n        resources:\n          requests:\n            cpu: &quot;1&quot;\n        args:\n          - vm-native\n          - --vm-native-src-addr=http://victoria-metrics-cluster-vmselect.monitoring.svc.cluster.local.:8481/select/001/prometheus\n          - --vm-native-dst-addr=http://vminsert-victoria-metrics-k8s-stack.monitoring.svc.cluster.local.:8480/insert/000/prometheus\n          - --vm-native-filter-match={__name__!~&quot;_vm.*&quot;}\n          - --vm-native-filter-time-start=&quot;{ { start } }&quot;\n          - --vm-native-filter-time-end=&quot;{ { end } }&quot;\n      restartPolicy: Never\n\n\nFeedback after months of use\nSince we have been using our new monitoring stack, we have encountered a few bugs (as with all solutions).\nMost of the time, these were not impactful, except for one that caused us a production incident.\nWe had an overconsumption of RAM of VMStorage which was fixed in version 1.76. I would like to highlight the responsiveness of the VictoriaMetrics team, whether on slack or on GitHub: I have had several discussions with them on various subjects, and they have always been reactive\n\nVictoria Metrics regularly releases new versions, including performance improvements and new features. The changelog will give you an idea of the latest improvements and their frequency.\n\nVictoria Metrics has an Enterprise version that adds some features, including one that we are interested in but have not yet tested: downsampling.\nWe have configured a one-year retention for each of our Kubernetes clusters, and on some clusters thatâ€™s mean more than 7 TB of data per VMStorage.\n\nThe downsampling allows you to configure how many metrics you want to keep per time interval.\n\nIn this example: -downsampling.period=24h:10s,1w:30s,30d:1m,360d:5m, (assuming we collect metrics every 5 seconds) we only keep:\n\n  one measurement point every 10 seconds beyond 24 hours (instead of one point every 5 seconds)\n  one measurement point every 30 seconds beyond 7 days\n  one measurement point every minute beyond 30 days\n  one measurement point every 5 minutes beyond one year\n\n\nIt is rarely necessary to keep all the measurements of our metrics on such a long scale, when we want to retrieve measurements that are several months old, it is usually to see a trend and not all the measurements.\nWith this option, we could greatly reduce the storage used by our metrics.\n\nConclusion\nThrough this article, you have discovered why and how we migrated our monitoring stack of our Kubernetes clusters at Bedrock from Prometheus to Victoria Metrics.\n\nThis was an important and critical subject for us, as monitoring is a critical need.\nNow our monitoring stack, based entirely on Victoria Metrics, is robust and capable of absorbing large load peaks.\n\nHere are some indicators of the victoria metrics stack performance of one of our Kubernetes clusters during last 6 months:\n\n  active time series: up to 39 million (average 7.4M)\n  total number of datapoints: 12 trillion\n  ingestion rate : up to 1.3 million new samples per second (average 227K)\n  churn rate : up to 117 Million new time series per day (average 30.6 Million)\n  disk usage (data + index): 15 TB\n  sample rate : up to 4.99M (average 343K)\n  scrape target : up to 49K (average 4.4K)\n\n\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Is machine learning a unicorn hiding a series of if and else?",
    "category" : "",
    "tags"     : " machine learning, Data Science",
    "url"      : "/2022/09/05/machine-learning-if-else.html",
    "date"     : "September 5, 2022",
    "excerpt"  : "Recently, a colleague asked me:\n\n\n  All good with your if and else machine learning system?\n\n\nIt was a joke but this one made me think.\n\nThis is a running gag: machine learning is only a series of if and else.\n\n\n\nBeyond the joke, it is true?\n\nYes!...",
  "content"  : "Recently, a colleague asked me:\n\n\n  All good with your if and else machine learning system?\n\n\nIt was a joke but this one made me think.\n\nThis is a running gag: machine learning is only a series of if and else.\n\n\n\nBeyond the joke, it is true?\n\nYes! â€¦and no. As always, it depends.\n\nQuick answer: Machine learning is a bunch of mathematical and statistical operations. Sometimes, the operations you use can be translated into if and else clauses, and sometimes not. But you never write the series of if and else yourself.\n\nA recap of machine learning\nThe idea of machine learning is: you have some data, and you apply an algorithm to these to detect a pattern. You put this pattern into a function.\n\n\n\nThen, youâ€™ll be able to use this function on new data to extract new information.\n\nA decision tree with a series of if and else\n\nThere are different types of machine learning. If you decide to build a decision tree (a famous way to do machine learning) to know the form of a diamond, youâ€™ll get something like that:\n\n\n\nIf you translate it with code, youâ€™ll get something like that:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    if size is high:\n        return red plate\n    else:\n        return grey pentagon\n\n\nThen, yes, you can see that here, you have a series of if and else.\n\nAnd decision trees are used a lot in machine learning. Most of the time, you donâ€™t use decision trees directly but forests of decision trees in the Random Forest algorithm or a series of decision trees in the Gradient Boosted Trees algorithm.\n\nBut, many algorithms in machine learning are just the generation of plain mathematical formulas\n\nLetâ€™s take another famous way to do machine learning: a neural network. What youâ€™ll get at the end is more something like that:\n\na*10+b*15+c*16+20â€¦\n\n\nThen, the process doesnâ€™t try to find a series of if and else, but a mathematical formula.\n\nI would like to finish with a last example: recommender systems. There are many ways to build a recommendation system. One which is well known is matrix factorization.\n\nMatrix factorization, what?\n\nI wonâ€™t explain deeply what it is about, but as a sum up, itâ€™s a manipulation of matrices. It comes from linear algebra.\nHere is a definition: Matrix decomposition.\n\nThe result is something like that:\n\nVector A * Vector B\n\n\nAs a result, yes, you have types of machine learning that will generate a series of if and else. But, you have also plenty of algorithms that try to find the variables of an equation or vectors.\n\nYou never write the series of if and else yourself\n\nLetâ€™s go back to the decision tree. As youâ€™ve seen previously, the result could be translated as a series of if and else.\n\nBut, you donâ€™t write directly this code. You generate it usingâ€¦ mathematical operations. Yes, again!\n\nAs an example, you can get the result of a decision tree using an optimisation algorithm with the Shannon Entropy formula:\n\n\n\nLetâ€™s suppose you want to guess the form (pentagon or plate) of a diamond according to its attributes. You have three diamonds:\n\n\n  \n    \n      carat\n      size\n      form\n    \n  \n  \n    \n      high\n      small\n      plate\n    \n    \n      low\n      high\n      plate\n    \n    \n      low\n      small\n      pentagon\n    \n  \n\n\nThe process is the following:\n\n  The data is split randomly: a random if statement is created like if carat is high\n  The process checks if it helps to generate a more accurate view of the data: by doing this if, are the data separated correctly? Do we have pentagons mostly from one side and plates from another?\n\n\nTo be able to know if the data are separated correctly, the Shannon entropy formula is used\n\n\n  if yes, the process keeps the if carat is high\n  if not, it generates another one\n\n\nThen, by keeping the if you get something like that:\n\n\n\nThe translation with a code is:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    #The process doesn&#39;t know yet how to handle that\n\n\nNote that you have a branch (below low) with a plate and a pentagon. It corresponds to the else where the process doesnâ€™t know what to put yet.\n\n\n  So, the data below low is split randomly: another if is created\n  The process checks if it helps to generate a more accurate view of the data\n\n\n\n  if yes, the process keeps the new if\n  if not, it generates another one\n\n\nBy keeping the new if, you get another branch:\n\n\n\nThe translation with a code is:\nif size is high:\n    return red plate\nelse:\n    return grey pentagon\n\n\nAt the end, you get a final tree decision:\n\n\nwith a final code:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    if size is high:\n        return red plate\n    else:\n        return grey pentagon\n\n\nOf course, in real life, data are more complicated and the process must iterate a lot until getting the perfect tree. The process used is an optimisation algorithm. This is the part called learning in machine learning.\n\nMathematical optimization [â€¦] is the selection of a best element, with regard to some criterion, from some set of available alternatives (definition from Wikipedia)\n\nIf you want to know how the Shannon entropy works with mathematical formulas, youâ€™ve got this article: Classification in machine learning - Example of Decision Tree with Shannon Entropy\n\nThen as a result, yes, you can have machine learning algorithms that will build a series of if and else. But to generate it, youâ€™ll use mathematical operations.\n\nNote that for other algorithms such as the matrix factorisation or neural networks, you donâ€™t use a process with the Shannon entropy formula, but other optimisation algorithms that donâ€™t generate a series of if and else but, as previously seen, vectors or formulas.\n\nSo why do we sometimes say that machine learning is a bunch of if and else statements?\n\nTo my opinion, because of expert systems. They are the ancestors of machine learning in artificial intelligence.\n\nArtificial intelligence is a way to simulate human cognitive abilities. In the history of artificial intelligence, people thought that they would be able to target that with expert systems. These are big series of hardcoded rules and thenâ€¦ of if and else.\n\nConclusion\nTo conclude, most of the time, machine learning is not a series of if and else. Itâ€™s just mathematics and for some techniques, they are very old. Iâ€™m thinking of linear regressions or Bayesian probabilities. These were used long before the existence of computers.\n\nPhoto of the unicorn by Stephen Leonardi on Unsplash\n"
} ,
  
  {
    "title"    : "Using a circuit breaker to spare the API we are calling",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, resiliency, circuit-breaker",
    "url"      : "/2022/09/02/backend-circuit-breaker.html",
    "date"     : "September 2, 2022",
    "excerpt"  : "Hi! Weâ€™re going to start our fourth article about Bedrockâ€™s API gateway.\nToday we will talk about the circuit breaker pattern, what it is, and how weâ€™re using it.\n\nThe Circuit Breaker Pattern\n\nWith this pattern, our API Gateway detects errors when...",
  "content"  : "Hi! Weâ€™re going to start our fourth article about Bedrockâ€™s API gateway.\nToday we will talk about the circuit breaker pattern, what it is, and how weâ€™re using it.\n\nThe Circuit Breaker Pattern\n\nWith this pattern, our API Gateway detects errors when calling its dependencies.\nIt will stop calling them if a given threshold (ratio of errors) is crossed.\n\nThe circuit breaker allows us to spare the dependencies in difficulty, but also avoid taking time to do something that will most likely fail.\n\nYouâ€™ll find a more detailed explanation about the circuit breaker on Martin FOWLERâ€™s blog.\n\nWhere to use it?\n\nAs soon as a service call is not mandatory for our BFF to answer something that a frontend application can read, then we can use the circuit breaker pattern.\n\nIf an API cannot handle a sudden increase in traffic (for example: itâ€™s not scaling fast enough or its database starts to throttle), itâ€™s better to stop calling it temporarily.\nWhen the right timeouts are configured, an API throttling will result in an error, as seen in the previous article\n\nHere are some examples:\n\nVideo progress information\n\nDisplaying a video progress bar is useful for end users, but itâ€™s better to not display this information instead of risking the entire page to not be displayed!\nIf the service that stores video viewing sessions is (slowing) down, we can stop asking for this information and stop displaying the video progress bar.\n\n\n\nUser geolocation\n\nThe geolocation service allows us to know where the end user is in the world. Based on this information we lock some area restricted contents.\nIf this service goes down for some reason, we will stop calling it, and instead use a default area matching the area of our customer as itâ€™s the majority case.\n\nImplementation and configuration\n\nSo far weâ€™re only using the circuit breaker pattern with HTTP calls.\nThis is made possible thanks to the Ganesha library, and its Guzzle middleware.\n\nThe Guzzle middleware is created as a service within the Symfony service definitions.\nItâ€™s then injected into our HttpClientFactory that will handle the creation of all the different clients.\nThe responsibility of using the circuit breaker falls on each service that will create a http client.\n\nAckintosh\\Ganesha\\GuzzleMiddleware:\n    factory: [&#39;@...Infrastructure\\HttpClient\\CircuitBreaker\\CircuitBreakerMiddlewareFactory&#39;, &#39;buildWithRateStrategy&#39;]\n    arguments:\n        $timeWindow: 60\n        $failureRateThreshold: 40\n        $minimumRequests: 10\n        $intervalToHalfOpen: 60\n\n\nMonitoring the circuit breaker\n\nAt Bedrock, weâ€™re used to monitor everything. The circuit breaker makes no exception to this rule.\nUsually we store time spent and response code for every outgoing http call.\nTo see when the circuit breaker is open, we catch the ganeshaâ€™s RejectedException to save a dedicated 666 http status.\n\nThis allows us to look for the exact number of calls avoided.\nBelow lies an example of a monitoring chart showing some errors happening during a usual night.\n\n\n\nWe also have to query slower services that often trigger our circuit breaker because they cannot answer in the short timeout we impose.\nThereafter, the same monitoring chart including such services.\n\n\n\nGoing further\n\nSo far, we have identified two areas for improvements described below.\n\nDifferent configurations\n\nWeâ€™re only using a single configuration for the circuit breaker.\nWe should allow each service to choose from a named list of configurations when creating a client, similarly to the different guzzle configuration we are using.\nThe main obstacle is a lack of hindsight which prevent us to have fine-tuned values.\nThis is something that will definitively be improved over time as we monitor over long period.\n\nStaled cache when the circuit breaker is open\n\nFor many editorial contents, weâ€™re using a staled cache version of the data as a fallback.\nTo do so, weâ€™re using another guzzle middleware.\n\nSadly, the two middlewares donâ€™t work together. We have to chose which one to use based on the criticality of the content and the API behind. \nThis is something that we aim at solving with a bit of R&amp;amp;D.\n\nConclusion\n\nIn todayâ€™s post weâ€™ve seen our usage of the circuit breaker pattern.\nIt allows us to spare the services we are calling, and avoid slowing us down in case of throttling.\n\nNext time, we will talk about our ultimate layer of protection to ensure the BFF always responds something readable to frontend applications.\n\nFrom the same series\n\n\n  Whatâ€™s a BFF\n  Handling API failures in a gateway\n  Whatâ€™s an error, and handling connexion to multiple APIs\n  Using a circuit breaker\n\n"
} ,
  
  {
    "title"    : "Prescaling pods in Kubernetes, we open source our solution",
    "category" : "",
    "tags"     : " k8s, kubernetes, pods, prometheus, scaling, hpa, resiliency, go, prescaling, opensource",
    "url"      : "/2022/09/01/kubernetes-prescaling-we-open-source-our-solution.html",
    "date"     : "September 1, 2022",
    "excerpt"  : "Previously we discussed how we manage the load of our Kubernetes clusters and how we can anticipate our needs with prescaling. Today, we are here to share our solution that we have reworked and open sourced! \n\n\nAt Bedrock Streaming, we provide str...",
  "content"  : "Previously we discussed how we manage the load of our Kubernetes clusters and how we can anticipate our needs with prescaling. Today, we are here to share our solution that we have reworked and open sourced! \n\n\nAt Bedrock Streaming, we provide streaming platforms to our customers (6play, Salto, Videoland and many others), we have a good knowledge of the daily load peaks and we know in advance the programs that are likely to generate a lot of traffic. We can therefore rely not only on reactive scaling, which has its limits (cf. prescaling article) but also on prescaling.\n\n&amp;gt; Prescaling consists of increasing the number of critical application pods in our clusters in advance in order to be ready to face a sudden traffic peak.\n\nInitially, we developed an in-house solution in Python for a simple reason: it was the language that most people in the team knew. Since we had time to test our solution, we thought it would be great to share it with everyone. But to do so, we had to make some adjustments.\n\nWe rewrote everything in go\n\nMany open source projects we use are written in Golang. In addition, the DevOps/Cloud world is mostly focused on Go. So, we decided to rewrite our prescaling solution in Go in order to make our teams more skilled in this language. The other goal was to make it cloud agnostic. In the Python version, we had an API part that stored prescaling events in a DynamoDB table, which made the solution dependent on AWS. Since prescaling is Kubernetes oriented, we had thought in the first versions in Python to store these events in Custom Resources (CRD) but due to lack of time, we did not implement it. We took advantage of the redesign to implement it and remove the dependency with AWS DynamoDB.\n\nWe also wanted to simplify the project. In the first versions, we had two bricks: one containing the exporter and another the API. We merged the two applications into one monolith. The API is CRUD and can handle CRD events.\n\nHere we go, we open source it\n\nThe great moment has come. Our prescaling solution is now available on GitHub in its alpha version: https://github.com/BedrockStreaming/prescaling-exporter.\n\nThis is the version we currently use in all our clusters. Letâ€™s quickly see how to implement the solution (you can find more details in the repo README).\n\nThe prescaling-exporter is distributed with helm charts in order to install it in kubernetes cluster.\n\nPrerequisites\n\nThe following bricks must be installed in the k8s cluster:\n\n  Prometheus Stack or Victoria Metrics Stack\n  Prometheus Adapter\n\n\nIt is possible to use another metrics stack but we do not provide an example at this time.\n\nClone the repo and run the following command with Helm3:\n\nhelm install prescaling-exporter ./helm/prescaling-exporter -n prescaling-exporter --create-namespace\n\n\nItâ€™s required to add the following configuration to Prometheus adapter:\n\n- &quot;metricsQuery&quot;: &quot;avg(&amp;lt;&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;})&quot;\n    &quot;name&quot;:\n      &quot;as&quot;: &quot;prescale_metric&quot;\n    &quot;resources&quot;:\n      &quot;overrides&quot;:\n        &quot;namespace&quot;:\n          &quot;resource&quot;: &quot;namespace&quot;\n    &quot;seriesQuery&quot;: &quot;prescale_metric&quot;\n\n\nDaily prescaling event\n\nWe have chosen to manage the configuration of daily events directly on the HPA (HorizontalPodAutoscaler) of the applications. Here is how to activate it, through annotations:\n\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: &quot;&quot;\n  annotations:\n    annotations.scaling.exporter.replica.min: &quot;&quot;\n    annotations.scaling.exporter.time.end: &quot;&quot;\n    annotations.scaling.exporter.time.start: &quot;&quot;\nspec:\n  metrics:\n  - type: External\n    external:\n      metricName: &quot;prescaling_metric&quot;\n      metricSelector:\n          matchLabels:\n            deployment: &quot;&quot;\n      targetValue: 10\n\n\nWe are able to control the start and end time of the prescaling and the minimum number of pods we want during this window. Please note that if the number of pods we want for prescaling is less than the current number of pods, the solution will not downscale the application and the HPA will continue to behave as usual.\n\nOne-time events\n\nWe can also record one-off events. For example, at Bedrock Streaming, during an important soccer match, we will record a special event in a Custom Resource Definition. \nOne-time events allow to prescale all applications having annotations on their HPA by multiplying their prescaling minimum replicas (annotations.scaling.exporter.replica.min) by the multiplier of the event in question.\n\nTo record a one-time event, an OpenAPI UI (formerly known as Swagger) is exposed by the prescaling exporter at the url /swagger/index.html. We can also register a new event from here or directly by making an api call to the following address /api/v1/events/.\n\n\n\nWhatâ€™s next?\n\nWe will continue to improve the solution. For example, we are thinking about removing annotations on HPAs and replacing them with a new dedicated CRD.\n\nAll contributions are welcome, donâ€™t hesitate to come and exchange with us on GitHub if you want to use the solution, we would be delighted.\n\n\n\nAuthors: JÃ©rÃ©my Planckeel, Valentin Chabrier\n"
} ,
  
  {
    "title"    : "BFF&#39;s error definition, and handling connections to multiple API",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, error, timout, retry, slo, guzzle",
    "url"      : "/2022/08/25/backend-errors-connections.html",
    "date"     : "August 25, 2022",
    "excerpt"  : "A quick sidetrack in our series about Bedrockâ€™s API gateway.\nThis piece defines what are we talking about when we say â€œan errorâ€, and explains how we handle the numerous connections to services we are calling.\n\nDefinition\n\nIn the previous article,...",
  "content"  : "A quick sidetrack in our series about Bedrockâ€™s API gateway.\nThis piece defines what are we talking about when we say â€œan errorâ€, and explains how we handle the numerous connections to services we are calling.\n\nDefinition\n\nIn the previous article, weâ€™ve seen how we handle errors.\nThis was mainly from a business point of view, and how itâ€™s done in our domain.\n\nBut what is â€œan errorâ€?\n\nThis term is a bit generic, and the definition will be too: an error is anything unexpected by the application.\n\nIn our context of an API Gateway, we are restricting this to the services we are calling.\n\nThis can be, but not exhaustively, a service not responding because:\n\n  itâ€™s offline;\n  itâ€™s taking too much time to answer;\n  itâ€™s responding with a 5** error (when talking about an API);\n  itâ€™s giving us an invalid or unexpected content.\n\n\nWhat are the consequences of those errors?\n\nThe first issue is: we wonâ€™t be able to display some part of the application as intended.\nWeâ€™ve talked about this previously already.\n\nThe second error, more insidious, is that it can slow down our BFF terribly.\n\nThe BFF response time is, on average, equals to the slowest service the BFF is calling.\nIf a service that usually responds in 200ms starts slowing down to an average response time of 1s and also times out half the time, it will increase the BFF response time to 1,5s (1s average, and 50% retry).\n\nThatâ€™s why we must be careful when configuring those timeouts.\nThe BFF exposes a response-time Service Level Objective (SLO), and frontend applications will cut any connection that takes too long.\nLosing some parts of the responses is better than slowing the BFF down to a point where frontend wonâ€™t get any response at all.\n\nHow are we mitigating the errors?\n\nFor any remote service, we configure short timeouts, and retry when we must.\nA short timeout is a timeout that usually match the SLO of the called services, and that will match 99% of our calls.\nWhen the SLO of the called service is higher than ours, we use a shorter timeout and accept that a larger parts of the calls will be cut.\nThe values are tailored according to our usages.\nWe use our monitoring to adapt those values in order to reduce the number of errors, while minimizing the impact on the BFF response time.\nWe are also constantly challenging our colleagues to improve the average response time of their services that we are calling.\n\nThe choice of using retries is based on the information criticality.\nFor example, retrieving the userâ€™s previous viewing sessions, is important for his/her experience, so weâ€™re using a retry here.\nOn the opposite, analytics are less important, so we donâ€™t use any retry there.\n\n    app.http_client_configs.best_effort:\n        retry: 1\n        timeout: 0.6\n        connect_timeout: 0.1\n    app.http_client_configs.fast_fail:\n        retry: 0\n        timeout: 0.6\n        connect_timeout: 0.1\n    app.http_client_configs.long_fail:\n        retry: 0\n        timeout: 1\n        connect_timeout: 0.1\n    app.http_client_configs.reliant:\n        retry: 2\n        timeout: 60\n        connect_timeout: 0.1\n\nAbove, you can see the yaml configuration our Symfony application uses to build its Guzzle clients.\n\nEach configuration can cascade onto the clients, making variants available for our Symfony services.\n\nBelow lies a Symfony configuration example:\n\n  We have an interface BFF\\Domain\\Content\\Repository from the domain for a content repository.\n  The interface is linked to an implementation BFF\\Infra\\HttpContentClient inside the infrastructure.\n  The implementation is built with variants (best_effort and fast_fail) from a factory using the matching Guzzle configurations.\n  Other services use a chosen repository according to their needs and criticality.\n\n\n    # Service definition with its aliases.\n    BFF\\Domain\\Content\\Repository: &#39;@BFF\\Domain\\Content\\Repository.fast_fail&#39;\n    BFF\\Domain\\Content\\Repository.best_effort: &#39;@BFF\\Infra\\HttpContentClient.best_effort&#39;\n    BFF\\Domain\\Content\\Repository.fast_fail: &#39;@BFF\\Infra\\HttpContentClient.fast_fail&#39;\n\n    # Concrete implementations\n    BFF\\Infra\\HttpContentClient.best_effort:\n        class: &#39;BFF\\Infra\\HttpContentClient&#39;\n        factory: [&#39;@BFF\\Infra\\ContentClientFactory&#39;, &#39;create&#39;]\n        bind:\n            $clientConfig: &#39;%app.http_client_configs.best_effort%&#39;\n    BFF\\Infra\\HttpContentClient.fast_fail:\n        class: &#39;BFF\\Infra\\HttpContentClient&#39;\n        factory: [&#39;@BFF\\Infra\\ContentClientFactory&#39;, &#39;create&#39;]\n        bind:\n            $clientConfig: &#39;%app.http_client_configs.fast_fail%&#39;\n\n    # Other services using the Repository\n    BFF\\Domain\\Navigation\\NavBarResolver:\n        $content: &#39;@BFF\\Domain\\Content\\Repository.best_effort&#39;\n\n    BFF\\Domain\\Layout\\BlockResolver:\n        $content: &#39;@BFF\\Domain\\Content\\Repository.fast_fail&#39;\n\n\nThis is an over simplified example as we have more layers and wrappers used for things like caching, monitoring, logging, etc.\n\nConclusion\n\nIn this article, weâ€™ve clarified what an error is, and explained that we cannot generalize the configuration and usage of our APIs. Timeouts and retries, especially, must be tailored depending on the criticality of each call.\n\nThis was a deviation on the road to our next article, where we will talk about monitoring the errors and stopping calls to failing APIs by implementing the circuit-breaker pattern.\n\nFrom the same series\n\n\n  Whatâ€™s a BFF\n  Handling API failures in a gateway\n  Whatâ€™s an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n"
} ,
  
  {
    "title"    : "Les spikes : quand, comment, pour quoi faire ?",
    "category" : "",
    "tags"     : " spike, methodologie, cytron, tech",
    "url"      : "/how-to-spike",
    "date"     : "August 23, 2022",
    "excerpt"  : "Câ€™est une histoire bien connue, dans la vie de nâ€™importe quel dÃ©veloppeur : un ticket arrive dans le backlog, dÃ©crivant une problÃ©matique relativement complexe. Câ€™est parfois une question de technologie inconnue, ou parfois simplement un chantier ...",
  "content"  : "Câ€™est une histoire bien connue, dans la vie de nâ€™importe quel dÃ©veloppeur : un ticket arrive dans le backlog, dÃ©crivant une problÃ©matique relativement complexe. Câ€™est parfois une question de technologie inconnue, ou parfois simplement un chantier un peu trapu. Je pense que toutes les Ã©quipes ont, au moins une fois dans leur vie, fait face Ã  ce genre de tÃ¢che impossible : câ€™est lâ€™occasion des regards dÃ©sespÃ©rÃ©s, alors quâ€™un junior se lamente en disant Â« Mais par oÃ¹ est-ce quâ€™il faut commencer ? Â». Et câ€™est lÃ  quâ€™on rÃ©pond : Â« Essaye de faire un spike Â».\n\nFaire un spike ? Quelle excellente idÃ©e ! Encore faudrait-il savoir ce quâ€™est un spike, comment Ã§a marche, et Ã  quoi Ã§a sert.\n\nJe vous propose donc ensemble de voir dans cet article : quâ€™est-ce quâ€™un spike, quand lâ€™utiliser, et comment considÃ©rer quâ€™il est rÃ©ussi ?\n\nspike â€“help ğŸ“š\n\nSi je devais citer Wikipedia, je dirais quâ€™un Spike, câ€™est â€œune mÃ©thode de dÃ©veloppement de produit, dÃ©rivÃ©e de lâ€™extrÃªme programming, et qui cherche Ã  crÃ©er le code le plus simple possible pour obtenir des solutions potentiellesâ€.\n\nEn gros, le but dâ€™un spike, câ€™est de rÃ©pondre Ã  la question â€œComment on fait ?â€ avec un prototype de code rÃ©alisÃ© grÃ¢ce Ã  une sÃ©rie de petites Ã©tapes simples. Un spike nâ€™est pas une formule magique qui va vous permettre de rÃ©aliser la tÃ¢che impossible que votre client vous a donnÃ©. En revanche, le spike va vous permettre de savoir si la tÃ¢che impossible ou compliquÃ©e Ã  premiÃ¨re vue est en fait possible, et si oui, comment.\nIl arrivera Ã©galement que votre spike vous permette de constater quâ€™une tÃ¢che donnÃ©e peut Ãªtre rÃ©alisÃ©e de plusieurs maniÃ¨res : que ce soit en passant par des librairies diffÃ©rentes, avec une implÃ©mentation changeante, ou autre chose encore. Dans ces cas, le spike va Ã©galement vous servir Ã  essayer ces diffÃ©rentes possibilitÃ©s, et Ã  choisir celle qui est la plus appropriÃ©e !\n\nLe moyen le plus simple est de procÃ©der morceau par morceau. Alors je vous propose quâ€™on sâ€™y mette maintenant, et quâ€™on regarde quoi faire !\n\nâ€œkowalski, analysis !â€ ğŸ“Š\n\nAvant toute chose, il faut savoir exactement ce que vous souhaitez faire. Rien ne sert de mettre la charrue avant les bÅ“ufs.\n\nSi ce nâ€™est pas fait, Ã©crivez noir sur blanc les lignes exactes qui vont dÃ©finir votre tÃ¢che comme finie. Que ce soit connecter votre utilisateur de faÃ§on sÃ©curisÃ©e, afficher une vidÃ©o sans heurt, ou juste avoir une page qui clignote en blanc et bleu, il faut que vous ayez une liste de bullet points, qui dÃ©finit prÃ©cisÃ©ment ce que vous voulez faire.\n\nVotre objectif final est de rÃ©aliser tout ce que vous avez sur cette liste : strictement rien de moins, mais aussi strictement rien de plus ! Pas de demande implicite de type â€œAh mais je voulais aussi que lâ€™image soit visible en noir et blancâ€ : si ce nâ€™est pas sur la liste, ce nâ€™est pas Ã  faire.\n\nCette liste peut Ãªtre Ã©crite selon votre format favori : un cahier des charges, une sÃ©rie de directives Gherkin, lâ€™important câ€™est quâ€™elle soit Ã©crite, claire et prÃ©cise. En dâ€™autres termes, vous dÃ©finissez ici votre propre cahier des charges.\n\nLe rÃ©sultat final doit donc Ãªtre quelque chose dans ce style :\n\nAs a client\nI want to see my product in 3 dimensions\nSo that I can know what it looks like\n\nAs a client\nI want to be able to rotate my product using the arrow keys\nSo that I can check it out entirely\n\nAs a client\nI want to be able to zoom on my product\nSo that I can see even the smallest details\n\n\nUne fois que vous savez quoi faire, on peut vraiment commencer Ã  mettre la main dans le code !\n\n// TODO : make the code below work ğŸ’»\n\nStop. LÃ¢chez tout.\n\nJe vous vois dÃ©jÃ , votre liste de points en main, Ã  tenter de la faire rentrer dans votre gros projet Ã  grands coups de burin, de vous gratter la tÃªte Ã  comprendre pourquoi Ã§a ne rentre pas, et quâ€™est-ce qui a bien pu casser, cette fois.\n\nUn peu de calme : le but dâ€™un spike nâ€™est pas de faire tout fonctionner, pas du tout. Prenez de la distance, et on va y aller en douceur.\n\nPour commencer, isolez une partie de votre projet et de vos points objectifs. Il existe plusieurs moyens de sâ€™y prendre : crÃ©er un nouveau projet, crÃ©er une nouvelle page avec seulement quelques composants, dÃ©charger votre backendâ€¦ On veut un environnement le plus propre possible.\nBeaucoup de projets sont vieux, et si mal conÃ§us quâ€™il aurait fallu les jeter au bout de deux ans. On cherche ici Ã  se dÃ©tacher au maximum de cette dette technique.\n\nNâ€™hÃ©sitez pas Ã  utiliser des mocks, des faux appels et rÃ©sultats au reste de votre application :  en simulant comment se comporte le reste de votre projet sans vÃ©ritablement y faire appel, vous diminuez au maximum votre marge dâ€™erreur, et vous assurez que vous contrÃ´lez la moindre information qui transite par votre code.\n\nMaintenant seulement, vous pouvez prendre votre clavier, et coder. Regardez comment implÃ©menter chacun de ces points dans votre code propre de maniÃ¨re Ã©purÃ©e.\nÃ‡a fonctionne du premier coup ? GÃ©nial, notez comment vous avez fait ! Ã‡a ne marche pas ? Dommage, mais ce nâ€™est pas une raison pour Ctrl+Z et recommencer. Notez bien ce qui nâ€™a pas marchÃ©, avant de retenter ! Si Ã§a ne marche toujours pas au bout de 2/3 essais, pas de soucis, nâ€™hÃ©sitez pas Ã  laisser ce point de cÃ´tÃ© et passer Ã  un autre. Mais Ã©crivez tout, car cela va vous servir trÃ¨s bientÃ´t !\n\nif(bug == true) { delete(bug); console.log(â€œIt works !â€); } ğŸ¤–\n\nIl peut cependant arriver que, parfois, tous vos efforts ne mÃ¨nent Ã  rien. Vous avez dÃ©jÃ  passÃ© plusieurs jours sur les diffÃ©rents sujets du spike, et vous nâ€™avez pas encore identifiÃ© de solution pour faire fonctionner le tout.\nDans ce cas-lÃ , pas de panique ! Il sâ€™agit Ã©galement dâ€™un des objectifs du spike. AprÃ¨s tout, si vous nâ€™avez pas pu rÃ©aliser votre objectif dans un cadre rÃ©duit, il est bien probable que vous nâ€™auriez jamais pu le faire fonctionner dans votre projet lui-mÃªme.\n\nLes mÃªmes points quâ€™indiquÃ©s ci-dessus continuent de sâ€™appliquer : notez ce que vous avez tentÃ© et les soucis rencontrÃ©s avec chaque implÃ©mentation. Puis, continuez le processus dÃ©taillÃ© ici : ce nâ€™est pas parce que votre code nâ€™as pas fonctionnÃ© quâ€™il ne doit surtout pas Ãªtre prÃ©sentÃ©. Peut-Ãªtre un de vos collÃ¨gues trouvera-t-il la ligne qui vous manque, ou le point-virgule que vous avez oubliÃ© : mais peut-Ãªtre aussi quâ€™il vous aidera Ã  comprendre ensemble pourquoi la solution ne fonctionne pas dans votre cadre.\nEt puis, vous pourrez alors vous poser la question : est-ce quâ€™il faut bien faire comprendre que la tÃ¢che demandÃ©e est irrÃ©alisable, ou est-ce quâ€™il faut prÃ©voir un chantier pour rÃ©ussir Ã  trouver un moyen de remplir la requÃªte ?\n\nLâ€™instant doc ğŸ“\n\nUne fois que vous avez terminÃ© de coder, il est temps pour vous de poser votre IDE, et de sortir votre outil de documentation favori : Confluence, Jira, que sais-je. \nPuis, Ã©crivez un compte-rendu de votre aventure. PrÃ©sentez lâ€™origine de votre spike (Le Pourquoi), ce que vous avez tentÃ© (Le Comment). Expliquez ce qui a marchÃ© et ce qui nâ€™a pas marchÃ© : cela vous servira lorsque vous implÃ©menterez vraiment la feature !\nEnfin, Ã©crivez Ã©galement les Ã©tapes quâ€™il faudrait suivre pour terminer la feature : ajoutez un maximum de dÃ©tails techniques. Ce sera autant de problÃ©matiques en moins pour le pauvre dev qui va rÃ©cupÃ©rer les US aprÃ¨s vous.\n\nJe vous suggÃ¨re donc de faire un plan de ce type :\n\n\n  ProblÃ¨me - Expliquez ici lâ€™Ã©tat initial. Quâ€™est-ce qui Ã©tait demandÃ© ? Pourquoi avoir choisi de faire un spike ? Quel en est lâ€™objectif ?\n  Observations  - Indiquez lÃ  vos rÃ©flexions et le code que vous avez produit. Expliquez ce que vous avez tentÃ©, les problÃ¨mes rencontrÃ©s et les solutions Ã©tablies, vos pistes de rÃ©flexion. Nâ€™hÃ©sitez surtout pas Ã  dÃ©tailler !\n  Actions - Enfin, dÃ©taillez dans cette derniÃ¨re partie ce quâ€™il restera Ã  faire afin de transformer ce spike en une feature fonctionnelle. Quels bugs corriger ? Quels points nâ€™ont pas encore Ã©tÃ© rÃ©alisÃ©s, et comment faire pour les rÃ©aliser ?\n\n\nPour la derniÃ¨re Ã©tape, je vous conseille de rÃ©aliser un tableau dâ€™actions SMART afin de dÃ©finir au mieux les tÃ¢ches Ã  rÃ©aliser.\nLe principe SMART suppose quâ€™une tÃ¢che doit Ãªtre composÃ©es des cinq caractÃ©ristiques suivantes afin dâ€™Ãªtre pertinente :\n\n  Elle doit Ãªtre SpÃ©cifique, afin que lâ€™objectif soit clair et concis (Quâ€™est-ce que je dois faire ? Exemple de rÃ©ponse : Â« Il faut que lâ€™image dâ€™un objet soit en 3D Â»)\n  Elle doit Ãªtre Mesurable, pour dÃ©finir un objectif quantifiable (Quant est-ce que ma tÃ¢che sera finie ? Exemple de rÃ©ponse : Â« Il faut que je puisse faire tourner lâ€™image avec les flÃ¨ches gauches et droites du clavier  Â»))\n  Elle doit Ãªtre Atteignable, sans demander de dÃ©crocher les Ã©toiles (Comment rÃ©aliser ma tÃ¢che ? Exemple de rÃ©ponse : Â« Utiliser la mÃ©thode Get3D de la librairie Easy3D Â»))\n  Elle doit Ãªtre RÃ©aliste au sujet en cours, donc nÃ©cessaire Ã  lâ€™accomplissement final (Est-ce quâ€™il est pertinent de prendre du temps pour faire Ã§a ? Exemple de rÃ©ponse : Â« Afin que notre client puisse voir lâ€™avant et lâ€™arriÃ¨re de nos produits Â»)\n  Elle doit Ãªtre dÃ©finie de faÃ§on Temporelle, afin de ne pas pouvoir sâ€™Ã©terniser (Pour quand ma tÃ¢che doit-elle Ãªtre rÃ©alisÃ©e ? Exemple de rÃ©ponse : Â« A rÃ©aliser avant que la feature soit considÃ©rÃ©e terminÃ©e Â»)\n\n\nDans le cas oÃ¹ une des tÃ¢ches que vous avez devisÃ© ne peut pas rÃ©pondre Ã  un de ces cinq points, alors il est probable quâ€™elle ne soit pas suffisamment prÃ©cise : peut-Ãªtre la tÃ¢che manque-t-elle de cadre ou de contexte, ou le temps nÃ©cessaire pour la rÃ©aliser ne peut que difficilement Ãªtre justifiÃ©. Je vous invite alors Ã  la supprimer, ou Ã  la fusionner avec une autre jusquâ€™Ã  enfin pouvoir rÃ©pondre Ã  ces cinq questions !\n\nBien entendu, nâ€™hÃ©sitez pas Ã  modifier le plan de cette documentation comme vous lâ€™entendez : vous Ãªtes celui qui allez lâ€™utiliser, aprÃ¨s tout !\n\nLa doc est finie ? Il ne reste plus que deux Ã©tapes, puis on pourra enfin considÃ©rer ce spike comme fini !\n\nPresentation_Spike.ppt ğŸ¬\n\nAvant de pouvoir clÃ´turer ce spike, il serait bien dâ€™avoir des retours extÃ©rieurs. Pour Ã§a, rien de mieux que de le prÃ©senter Ã  votre Ã©quipe !\nOrganisez ensemble une rÃ©union, pas trÃ¨s longue. Au sein de mon Ã©quipe, une demi-heure suffit. Il vous faudra peut-Ãªtre un peu moins ou un peu plus de temps.\n\nUtilisez cette prÃ©sentation afin de montrer, Ã©tape par Ã©tape, ce que vous avez rÃ©alisÃ©. Rappelez tout dâ€™abord les objectifs du spike, avant dâ€™expliquer votre analyse du problÃ¨me et les objectifs que vous avez identifiÃ©s. Puis, prÃ©sentez les diffÃ©rentes implÃ©mentations que vous avez tentÃ©es, avant de conclure en montrant votre documentation et en expliquant les tÃ¢ches qui restent Ã  accomplir pour rÃ©aliser la feature objectif.\n\nIl est trÃ¨s important que vous ne prÃ©sentiez pas uniquement le code que vous avez rÃ©ussi Ã  faire fonctionner, mais aussi vos tentatives Ã©chouÃ©es, et ce pour plusieurs raisons. Tout dâ€™abord, il est tout Ã  fait possible quâ€™un de vos collÃ¨gues, en voyant votre prÃ©sentation, rÃ©alise une de vos erreurs et vous lâ€™indique. Mais surtout, si quelquâ€™un dâ€™autre que vous rÃ©cupÃ¨re une des tÃ¢ches restantes, il risque de tenter les mÃªmes pistes que vous, et rencontrer les mÃªmes problÃ©matiques que vous !\n\nUne fois votre prÃ©sentation terminÃ©e, dÃ©battez avec le reste de votre Ã©quipe. Sâ€™ils sont dâ€™accord avec vous sur le plan dâ€™action que vous avez Ã©tabli grÃ¢ce Ã  votre tableau SMART, il vous reste une toute derniÃ¨re Ã©tape Ã  accomplir !\n\nâ€œHappily ever afterâ€¦â€ ğŸ’­\n\nMaintenant que tous vos coÃ©quipiers ont pu constater et valider votre travail, il ne vous reste plus quâ€™Ã  acter la mise en place : et pour Ã§a, rien de mieux que, aux cÃ´tÃ©s de votre Product Owner (Ou de lâ€™Ã©quivalent dans votre Ã©quipe) de crÃ©er des tÃ¢ches, User Story, post-its, ou quoi que ce soit, pour que les Ã©tapes restantes soient visibles et accessibles par tous !\n\nNâ€™hÃ©sitez pas Ã  le guider pour ajouter encore une fois des dÃ©tails techniques dans ces US ou tÃ¢ches : vous avez rÃ©alisÃ© lâ€™analyse, il serait dommage de ne pas lâ€™utiliser, et ce sera autant de temps gagnÃ© pour votre Ã©quipe. Tant que vous y Ãªtes, pensez aussi Ã  ajouter un lien vers votre documentation, ou vers une vidÃ©o de votre prÃ©sentationâ€¦ Plus il y aura de dÃ©tails, mieux Ã§a sera !\n\nIl est Ã©galement possible, comme indiquÃ© plus haut, que la tÃ¢che qui a entraÃ®nÃ© la rÃ©alisation de ce spike se dÃ©couvre Ãªtre impossible Ã  implÃ©menter. Il sâ€™agit lÃ  Ã©galement dâ€™un point Ã  faire avec votre Product Owner, afin de dÃ©cider ensemble de la procÃ©dure Ã  suivre : peut-Ãªtre faudra-t-il redÃ©finir les critÃ¨res dâ€™acceptation, ou bien laisser tomber complÃ¨tement cette idÃ©e.\n\nreturn 0;\n\nVous avez fini votre spike ! Ce qui Ã©tait Ã  lâ€™origine une tÃ¢che complexe, confuse ou impossible Ã  prÃ©voir, est dÃ©sormais divisÃ©e en une sÃ©rie dâ€™Ã©tapes, qui sera dÃ©sormais bien plus aisÃ©e Ã  rÃ©aliser pour votre Ã©quipe. Alors, satisfait ?\n"
} ,
  
  {
    "title"    : "Handling dependencies failures in an API gateway",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, resiliency",
    "url"      : "/2022/08/12/backend-fallbacks.html",
    "date"     : "August 12, 2022",
    "excerpt"  : "Welcome to our second article about the backend architecture and its api gateway.\nIn the first part, we talked about the BFF and all services it depends on.\nToday weâ€™re going to take a look at what to do when one of them (or many), fails to respon...",
  "content"  : "Welcome to our second article about the backend architecture and its api gateway.\nIn the first part, we talked about the BFF and all services it depends on.\nToday weâ€™re going to take a look at what to do when one of them (or many), fails to respond.\n\nService dependencies\n\nAs seen previously, the BFF uses multiple data sources and services to create a full layout.\n\nThose services are used to gather the contents to be displayed in the application:\n\n  getting user personalisation data;\n  advertising and analytics configuration;\n  asking if the user has some authorizations.\n\n\nIf we donâ€™t want our BFF to become one giant SPOF (1), we need to be resilient to the death (2) of those dependencies, any of them, at any time!\nYou must keep in mind that our top priority is to always be able to answer something readable to the frontend applications.\n\nDDD\n\nFirst thing first, we are using a DDD (3) approach for our modeling.\nThis means that we focus on the business, as described by our Product Owner. We try not to worry about the various implementation of our backendâ€™s friends and their different services.\n\nA picture is always easier to understand.\n\n\n\nAbove, we can see that when a user ask for a layout A, we are looking to resolve who is A.\nFrom the domain point of view, the page collection is only an interface.\n\nIn the picture below, we see the â€œPage collection implem (Infra)â€.\nItâ€™s a layer implementing the interface defined in the domain. It uses multiple clients that call the services behind.\nItâ€™s its responsibility to chose which service to look on for the page.\n\n\n\nDDD is a too large subjects to be perfectly defined in this article. If you want to dig deeper into it, there are multiple great reads, feel free to check them out!\nNow, how does this help us?\n\nHandling failures\n\nFailures handling is done by the middle layer seen in the previous example.\nIts goal is to catch error (4), and convert them to something expected and defined by the interface.\n\nThat said, its responsibility is not to know what the expected answer is. To do that, we use the domain.\n\nLetâ€™s see with a small code sample.\n\nNote: The following example is not a real use-case, but itâ€™s representative and simple enough to illustrate how it works.\n\nIn the code below, we see a class that represents the subscribing status of a user, which has two properties:\n\n  hasAccess controls whether the user can read protected contents;\n  isSubscribed is used in analytics, and to show subscription pages.\n\n\n&amp;lt;?php\nfinal class SubscribeStatus\n{\n    private function __construct(\n        public readonly bool $hasAccess,\n        public readonly bool $isSubscribed,\n    ) {\n    }\n\n    public static function createAnonymous(): self\n    {\n        return new self(false, false);\n    }\n\n    public static function createSubscribed(): self\n    {\n        return new self(true, true);\n    }\n}\n\n\nTo create such an object, we use either one of the two static functions, depending on the status we get from the subscriptions API.\nThis is done in the middle layer, but the business is kept in the domain.\n\nTo handle the failure, we add a new named constructor, dedicated to this specific case.\n\n    public static function createUnknown(): self\n    {\n        return new self(true, false);\n    }\n\n\nWhen an error happens and we canâ€™t retrieve the user subscription status, we now have a fallback option.\nWith this fallback option, the user will:\n\n  be able to access any content, itâ€™s better to let an anonymous user access a content it should not, that blocking a paying customer;\n  still be reported as not subscribed and will see all available offers.\n\n\nMost of the time, the answer is even simpler than this one.\n\nAnother example would be userâ€™s viewing statuses. If we canâ€™t retrieve them, we donâ€™t display any progress bar.\nUsers wonâ€™t be able to tell if they have seen a content, but they will still be able to navigate the application.\n\nInfrastructure solution, the stale cache\n\nIn some cases, the above solution doesnâ€™t work.\nFor example, contents information cannot be replaced by default values. If we donâ€™t know about a video or a program, we cannot guess what it is.\n\nLuckily, we can rely on the stale cache.\nStale cache is an old cache entry which is expired. When the cache finds such entry, it usually ignores it and asks for a new version of the response.\nIn case of failure, we can use the available staled version.\n\n\n\nThe limitation is that a response must have been cached at least once, in order to have a staled version.\n\nWhen there is no stale cache, we donâ€™t display the content (5).\n\nSo far, we are only using it with http implementation:\n\n  called API must answers with stale-if-error cache directive, it allows for the response to be used while stale when an error happens;\n  called API can answer with stale-while-revalidate cache directive, for better performances;\n  calling API can query with max-stale cache directive, to use stale response see the mdn for more on those headers;\n  on the client side, we are using the Kevinrob/guzzle-cache-middleware to do the job.\n\n\nFor an entry cached for up to 10 minutes (answered with max-age), we allow up to 4 hours of stale cache (with stale-if-error).\nSince we are using a shared cache, we are using max-stale when querying, with a random value up to 1 hour.\nThis makes most requests use the last stale response while one of them ask for a fresher response.\nThose values are chosen according to our platform usages where peak visitor last for about 2 to 3 hours at night.\n\nWe plan to expand its usage to other kinds of cached entries, such as manually saved data, and database queries.\n\nConclusion\n\nIn todayâ€™s post, we have seen how we handle the loss of our dependencies by anticipating their potential failures and preparing default acceptable behaviours.\n\nNext time, we will see how we can spare some traffic on those dependencies when theyâ€™re struggling with traffic.\n\nNotes\n\n  SPOF, as single point of failure since all frontend applications have to rely on the BFF, I cannot resist linking this excellent xkcd.\n  By â€œdeathâ€, we mean anything unexpected. It can be a 500 error code, a timeout, a wrong content. We will talk a bit more about this in the next article.\n  DDD, as domain driven design, you can read more about it on Martin FOWLERâ€™s website.\n  Throwing errors is still allowed, but restricted to domain exceptions, and must be specified in the methodâ€™s declaration in the interface (i.e. via a comment).\n  There will be a dedicated article on partial rendering.\n\n\nFrom the same series\n\n\n  Whatâ€™s a BFF\n  Handling API failures in a gateway\n  Whatâ€™s an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  ğŸ‡ºğŸ‡¸ Encrypt AWS AMIs: one way to do it wrong\n  ğŸ‡«ğŸ‡· Bedrock Ã  la kubecon 2022 (4 articles)\n\n"
} ,
  
  {
    "title"    : "How to ingest 400GB of logs per hour?",
    "category" : "",
    "tags"     : " onprem, cdn, logs, aws, cloud, nginx, vector, lambda, s3, glue, athena",
    "url"      : "/2022/08/08/private-cdn-logs.html",
    "date"     : "August 8, 2022",
    "excerpt"  : "Bedrock Streaming is a company that sells a white labeled streaming and live platform. Our customers are media groups, TV channels, and streaming companies. Our goal is to deliver a state-of-the-art streaming platform to our customers.\n\nTo achieve...",
  "content"  : "Bedrock Streaming is a company that sells a white labeled streaming and live platform. Our customers are media groups, TV channels, and streaming companies. Our goal is to deliver a state-of-the-art streaming platform to our customers.\n\nTo achieve this goal, we have our own Content Delivery Network (CDN), made of several bare metal servers racked in our Data Centers. Those servers run Nginx and are designed to output hundreds of Gbps (several tens of Pb per month) to end-users. We use them to cache video content at our infrastructureâ€™s edge.\n\nThis increases efficiency of the platform 96 times out of 100, as video traffic doesnâ€™t have to flow all the way through our infrastructure, and improves user experience as it serves video faster. Also, it diminishes the cost of our Video On Demand (VOD) infrastructure as we need less servers in VOD Stack.\n\nThis in-turn increases end-users (clients of our customers) satisfaction with the service.\n\nWho needs to ingest 400GB of logs per hour anyway?\nEvery time someone watches a video, it generates traffic on our CDN, resulting in a lot of access logs. Without filtering, it averages to 400GB uncompressed logs per hour.\n\nThis is why, at first, we chose to not log 2XX or 3XX HTTP codes. We had too many of them, and we considered them not as worth it as 4XX and 5XX. The 4XX and 5XX can be especially useful for debugging a particular situation or, from a broader perspective, improving the user experience.\n\nThis was the kind of Nginx configuration we had deployed:\nmap $status $loggable {\n    ~^[23]  0;\n    default 1;\n}\naccess_log /path/to/access.log combined if=$loggable;\n\n\nGiving autonomy for all teams on logs\n\nAt the end of 2021, the finance team approached us with a challenge: how to bill our customers based on their end-users CDN usage?\nThis was in fact a need we already anticipated, we tried the nginx module Traffic_accounting, but it did not satisfy us fully. This module calculates and exposes metrics on-the-fly, which is CPU and memory intensive, especially above 50Gbps of traffic per server.\n\nWe also had another objective that wasnâ€™t addressed with the nginx module. We needed to give autonomy to QA, Video, Data, and Finance teams. We wanted to allow them to use CDN logs when they needed without having to ask for it, and ideally in a practical and unified way.\n\nThe company philosophy states that we are user obsessed and that we do not finger point. We work as a team to offer the best user experience, this is why we make all our logs available to all teams. We didnâ€™t come around to do it for the CDN as the volume of logs was too much of a constraint.\n\nTechnical Solution\n\nAt Bedrock, we like to keep things simple. We think our CDN main mission is to serve video as efficiently as possible. Our CDNâ€™s servers canâ€™t keep PetaBytes of logs on their disks. This is why we chose to output logs to Amazon S3.\n\nThe real benefit to using S3 is that you can easily plug it into Glue and Athena which allows you to request TeraBytes of data easily.\n\n\n\nSending logs to S3: Vector\n\nTo send logs from our CDN servers to Amazon S3 bucket, we had many options, but chose to test two approaches: Fluentd and Vector. Fluentd is the legacy one, and Vector the new rusty one.\n\nAfter a quick evaluation, we decided to go with Vector as it seemed more memory efficient and output more Logs Per Second under heavy load than Fluentd.\n\n\nSource: Who is the winner â€” Comparing Vector, Fluent Bit, Fluentd performance from Ajay Gupta\n\n\nWe have Nginx and Vector installed on the CDN servers. Nginx now outputs all the access logs to a file. Vector reads the file, compresses logs to GZIP format and every 10Mb sends the logs to S3. Nginx may generate at peak 600GB of logs; we only send 10GB.\n\nThose logs are then locally cleaned by Logrotate.\n\nStoring logs: S3\nWe chose to store logs on an S3 bucket. We figured it was the most scalable and time efficient. S3 buckets can grow to PetaBytes easily. It is a few terraform lines away, this is convenient as we handle all our infrastructure with Terraform.\n\nWe configured our bucket to use several lifecycle policies. One to automatically clean logs after 365 days, another to remove incomplete uploads, and another one to immediately remove files with a delete marker. Also, we configured the storage class in intelligent tiering mode to store logs according to their access frequency.\n\nThis will permit us to diminish the cost of our S3 bucket and not have an ever-increasing S3 bill.\n\nPartitioning logs on S3: Lambda stack\n\nOnce logs are stored in S3 bucket, we need to classify and sort them in order to extract valuable intel. At Bedrock, we already use a modified version of a lambda stack, that does just that. Originally designed for Cloudfront, we have been using it also for Fastly and now for our Private CDN. You can find the original version at AWS Sample Github.\n\nWe have 2 different parts in this lambda stack.\n\n\nsource: moveAccessLogs\n\nThe first part is called by S3 Event when a new file is pushed to a specific path. This lambda moves the file to a path assigned per server and per hour. This way, logs are stored for each server, each month, each day and each hour in a separate prefix.\n\n\nsource: transformPartition\n\nThen, another lambda transforms logs into Parquet format. Parquet is an open source format from the Apache Foundation. It is commonly used in big data. It takes up little space and is very effective.\n\nWe chose to use AWS glue in order to create a database of our logs. The columns of the table are based on our log format. We can then request everything we want in Athena.\n\n\n\nWe are now capable of extracting the bytes sent from a particular virtual host and sum it over a month for all CDN servers to bill our customers.\nThose logs are now available for all the teams who may need them to improve their application or to debug an issue they are facing.\n\nConclusion\nWe chose Vector to transport our private CDN logs to an S3 Bucket. Then, we chose to reuse an AWS Stack using Lambda and Glue to extract information from these logs, asynchronously. This stack is used in production for several months on other projects.\nAll the teams that needed to extract value from our CDN logs are now autonomous to do so. We are now able to bill our customers based on their CDN usage.\n"
} ,
  
  {
    "title"    : "Retour sur la confÃ©rence MiXiT 2022",
    "category" : "",
    "tags"     : " conference, agile",
    "url"      : "/2022/07/28/retour-sur-mixit-2022.html",
    "date"     : "July 28, 2022",
    "excerpt"  : "\n\nMiXiT est une confÃ©rence â€œavec des crÃªpes et du cÅ“urâ€ qui se dÃ©roule Ã  Lyon. Les sujets sont assez variÃ©s abordant autant lâ€™agilitÃ©, que la programmation, le droit ou encore lâ€™histoire de lâ€™informatique.\n\nVoici un rÃ©sumÃ© des confÃ©rences de lâ€™Ã©di...",
  "content"  : "\n\nMiXiT est une confÃ©rence â€œavec des crÃªpes et du cÅ“urâ€ qui se dÃ©roule Ã  Lyon. Les sujets sont assez variÃ©s abordant autant lâ€™agilitÃ©, que la programmation, le droit ou encore lâ€™histoire de lâ€™informatique.\n\nVoici un rÃ©sumÃ© des confÃ©rences de lâ€™Ã©dition 2022 qui nous ont le plus marquÃ©es.\n\nHow to build the alert system that France deserves?\n\nGaÃ«l Musquet nous a dâ€™abord expliquÃ© le rÃ´le de Gustave FerriÃ©, quâ€™il considÃ¨re comme le premier hacker, qui a installÃ© des mÃ¢ts de tÃ©lÃ©graphe sans fil en 1902, entre les Ã©metteurs en Martinique, pour remplacer le cÃ¢ble tÃ©lÃ©graphique, dÃ©truit lors de la catastrophe de la montagne PelÃ©e du 8 mai 1902. Cet homme avait saisi lâ€™intÃ©rÃªt dâ€™avoir un systÃ¨me de communication fiable.\n\nGaÃ«l Musquet nous explique ce quâ€™on est en droit dâ€™attendre en 2022 dâ€™un pays moderne, concernant les alertes sur les risques majeurs, qui varient selon notre emplacement (du tsunami Ã  la rupture de barrage artificiel).\n\nIl nous incite Ã  lire le DICRIM de notre ville (celui de Lyon) ainsi quâ€™Ã  nous procurer un poste de radio Ã  piles, car dans lâ€™Ã©ventualitÃ© dâ€™un moment catastrophique sans Internet et sans satellites, comment ferons-nous pour nous tenir au courant de ce quâ€™il faut faire pour rester en vie ?\n\nPage du talk sur le site de MiXiT et voir le replay\n\nMeet NULL the UNKNOWN\n\nDans cette confÃ©rence, LaÃ«tiia Avrot entame un rappel de la norme SQL, que PostgresQL implÃ©mente au plus prÃ¨s, sur la valeur de NULL en SQL. Et la valeur UNKNOWN est Ã©galement abordÃ©e. Notamment la complexitÃ© induite par le fait quâ€™un champ de type Boolean peut se retrouver avec comme valeurs possibles : True, False, UNKNOWN et NULL. Cela donne un systÃ¨me Ã  quadruple valeur. Pour un champ typÃ©.\n\nNULL est plus facile Ã  dÃ©finir par ce quâ€™il nâ€™est pas quâ€™en expliquant ce quâ€™il est.\nUne option intÃ©ressante pour mettre en Ã©vidence la valeur NULL dans PostgresQL est dâ€™en dÃ©finir nous-mÃªme une valeur affichÃ©e.\n\nEnsuite, LaÃ«titia nous propose un Quizz. Sur une base de donnÃ©es quâ€™on connaÃ®t, chaque fois la mÃªme question est posÃ©e sur â€œCombien de lignes vont Ãªtre retournÃ©es par la requÃªte SQL ?â€\n\nCâ€™est intÃ©ressant, car chaque question comporte un degrÃ© de complexitÃ© Ã©levÃ© impliquant lâ€™usage de la valeur NULL, tout en suivant la logique de la norme SQL. Cerise sur le gÃ¢teau, LaÃ«titia propose en â€œRÃ©ponse Dâ€, le nom dâ€™une scientifique cÃ©lÃ¨bre et nous en donne une courte biographie Ã  chaque question.\n\nLiens\n\n\n  Blog de lâ€™oratrice, LaÃ«titia Avrot\n  Page du talk sur le site de MiXiT et voir le replay\n\n\nParlez de vous, faites des feedbacks\n\nLe feedback est un outil communicationnel qui permet de formuler un avis sur une situation passÃ©e dans le but de gÃ©rer les situations futures.\n\nOn peut trouver plusieurs formes de feedbacks :\nle feedback est Ã  destination de la personne, pour lâ€™aider Ã  sâ€™amÃ©liorer. Elle peut dÃ©cider de le suivre ou non,\nla demande que lâ€™on fait Ã  quelquâ€™un est Ã  notre bÃ©nÃ©fice (on demande Ã  la personne de changer un comportement qui nous gÃªne) en laissant la possibilitÃ© Ã  la personne de dÃ©cider si elle veut ou non rÃ©pondre favorablement Ã  cette demande,\nlâ€™exigence qui est aussi Ã  notre bÃ©nÃ©fice, mais pour laquelle on ne laisse pas le choix (dans le cadre dâ€™une relation hiÃ©rarchique)\n\nJulie QuilliÃ© propose un modÃ¨le de feedbacks basÃ© sur la CNV (Communication Non Violente) et qui peut se rÃ©sumer de la maniÃ¨re suivante.\n\nFeedback basÃ© sur la CNV (Communication Non Violente)\n\n\n  On vÃ©rifie la disponibilitÃ© de la personne en lui demandant si elle est dâ€™accord pour quâ€™on lui fasse des feedbacks et sous quelle forme.\n  On formule le feedback :\n  DÃ©crire une Observation, les faits (= pas de jugement)\n  Exprimer le Sentiment que cette situation a engendrÃ©\n  Expliquer le Besoin qui est la source du sentiment ressenti\n  et finir par faire une Demande (= rÃ©alisable, formulÃ©e positivement, prÃ©cise)\n\n\nUn exemple :\n  Nous avions rendez-vous Ã  12h et il est 12h30 = observation, factuel.\n  Je suis trÃ¨s fÃ¢chÃ© car je mâ€™Ã©tais organisÃ© pour Ãªtre Ã  lâ€™heure = le sentiment\n  Câ€™est important pour moi de ne pas perdre de temps et de pouvoir rester libre dans mon organisation = le besoin\n  La prochaine fois que tu sais que tu seras en retard, peux-tu stp mâ€™appeler dÃ¨s que possible pour me le signaler ? De cette maniÃ¨re, je peux me rÃ©organiser facilement. = la demande\n\n\n  On vÃ©rifie ce qui a Ã©tÃ© reÃ§u par la personne. On lui propose de nous reformuler ce quâ€™elle en a retenu. Cela permet de vÃ©rifier que le message que lâ€™on voulait faire passer a bien Ã©tÃ© entendu.\n\n\n2Ã¨me possibilitÃ© pour faire un feedback : le feedback en 4 temps\n\n\n  On demande Ã  la personne ce quâ€™elle a aimÃ© dans ce quâ€™elle vient de faire\n  \n    On lui demande ensuite ce quâ€™elle aurait aimÃ© faire diffÃ©remment\n\n    On lui demande si elle veut quâ€™on lui donne notre feedback\n  \n  â€œMoi, jâ€™ai aimÃ© â€¦, parce que â€¦ â€ : on parle de ce que Ã§a nous a apportÃ© (clartÃ©, motivation, inspiration, soutien, etc.)\n  â€œEt jâ€™aurais aimÃ© â€¦  de diffÃ©rent, parce que â€¦ â€ on parle de ce que Ã§a nous apporterait (clartÃ©, motivation, inspiration, soutien, etc.)\n\n\nEt en bonus : â€œPeux-tu me dire comment tu reÃ§ois ce que je te dis ?â€\n\nPage du talk sur le site de MiXiT et voir le replay\n\nArrÃªtez lâ€™auto-sabotage et sortez de la boucle (systÃ©mique)\n\nDans cet atelier, Albane Veyron nous explique que nous avons tous des croyances sur nous-mÃªmes et sur les autres. Les croyances sont des pensÃ©es qui sont des vÃ©ritÃ©s, pour nous. Elles ont plusieurs origines : lâ€™enfance, notre cercle social et notre expÃ©rience de vie.\n\nLes croyances peuvent Ãªtre aidantes ou limitantes.\n\nLâ€™atelier commence par une premiÃ¨re phase qui consiste Ã  reconnaÃ®tre une de ses croyances limitantes :\n\n  les gÃ©nÃ©ralisations : personne, tout le monde, toujours, tout le temps, jamais, trop, je dois, il faut, pas assez\n  les barriÃ¨res infinies aka les bonnes excuses pour ne pas passer Ã  lâ€™action : jâ€™aimerais, mais â€¦ / je pourrais, mais â€¦\n  les sensations de dÃ©jÃ  vu : les blocages et les situations rÃ©currentes\n\n\nUne fois quâ€™on a repÃ©rÃ© une de ses croyances limitantes, on lâ€™Ã©crit sur une feuille et on va ensuite dÃ©composer cette croyance et rÃ©flÃ©chir Ã  :\n\n  son origine : dâ€™oÃ¹ nous vient cette croyance ? depuis combien de temps fait-elle partie de nous ? nous vient-elle de notre Ã©ducation ?\n  les bÃ©nÃ©fices : quels bÃ©nÃ©fices nous apporte cette croyance ? quâ€™est-ce quâ€™elle nous permet ?\n  les inconvÃ©nients / les freins : en quoi cette croyance nous gÃªne et quels sont les impacts sur notre vie (pro ou perso) ?\n  les contradictions : a-t-on dÃ©jÃ  fait quelque chose ou Ã©tÃ© dans une situation qui vient contredire cette croyance ?\n\n\nOn va ensuite venir agrÃ©menter notre croyance avec tous ces Ã©lÃ©ments puis, pour finir, transformer notre croyance limitante en une croyance aidante.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nComment fonctionne un gestionnaire de mots de passe\n\nLes mots de passe sont partout. Ils nous permettent dâ€™accÃ©der Ã  nos photos, nos comptes bancaires, nos documents de santÃ© et bien dâ€™autres donnÃ©es sensibles que lâ€™on ne souhaite pas voir aux mains dâ€™individus que lâ€™on ne connaÃ®t pas.\nTout le monde sait que lâ€™on doit avoir des longs mots de passe mais comment tous les retenir ? Câ€™est lÃ  que les gestionnaires de mot de passe entrent en jeu. Mais peut-on leur faire confiance ? Comment Ã§a marche au juste ? Câ€™est Ã  cette question quâ€™a souhaitÃ© rÃ©pondre Eric Daspet pendant sa confÃ©rence.\n\nLe rÃ´le dâ€™un gestionnaire de mots de passe est de permettre Ã  son utilisateur dâ€™utiliser quâ€™un seul mot de passe pour ensuite laisser lâ€™outil gÃ©nÃ©rer et mÃ©moriser tous les autres mots de passe. On a plus quâ€™Ã  retenir un seul mot de passe qui peut donc Ãªtre long et complexe. Lâ€™exercice de mÃ©moire sera alors moins compliquÃ© que si on en avait plusieurs Ã  retenir.\n\nÃ€ travers son exposÃ©, on dÃ©couvre un peu plus tous les procÃ©dÃ©s de cryptographie utilisÃ©s afin de gÃ©rer les mots de passe que lâ€™on va crÃ©er ou modifier en utilisant ces outils.\nGrÃ¢ce Ã  de nombreux schÃ©mas, il explique clairement les diffÃ©rentes Ã©tapes de chiffrements utilisÃ©es que ce soit pour la crÃ©ation du mot de passe maÃ®tre, la crÃ©ation et le changement des mots de passe, lâ€™affichage des mots de passe et mÃªme le fonctionnement du partage de mots de passe (lorsque celui-ci existe dans lâ€™outil).\n\nOn dÃ©couvre pendant cette heure que les gestionnaires de mots de passe ne cherchent pas Ã  rÃ©inventer la roue en matiÃ¨re de cryptographie mais sâ€™appuient sur des concepts dÃ©jÃ  Ã©prouvÃ©s et robustes. On apprend aussi que tout est chiffrÃ© de bout en bout et que seul celui qui dÃ©tient le mot de passe maÃ®tre (lâ€™utilisateur donc, mÃªme lâ€™outil ne le connaÃ®t pas et nâ€™en a pas besoin) peut interagir avec les mots de passe crÃ©Ã©s. Rassurant, non ? En tout cas, me voilÃ  maintenant prÃªt Ã  expliquer autour de moi pourquoi il est grand temps de passer Ã  un gestionnaire de mot de passe !\n\nPage du talk sur le site de MiXiT et voir le replay\n\nOptimiser votre revue de code avec le rebase interactif\n\nGIT est un outil bien connu des dÃ©veloppeurs de nos jours, mais dÃ¨s quâ€™on sâ€™Ã©carte des commandes traditionnelles (checkout, commit et push), on sait bien moins ce que lâ€™on peut faire dâ€™autre avec.\n\nSonia Seddiki nous explique ici comment rendre la revue de code, souvent longue et fastidieuse, plus simple et agrÃ©able pour nos collÃ¨gues avec quelques astuces quâ€™elle a partagÃ©es avec nous lors dâ€™un live coding.\nContrairement Ã  lâ€™idÃ©e que jâ€™en avais, le rebase interactif nâ€™est pas lÃ  que pour nettoyer les noms de commit sans aucun sens que jâ€™avais mis dans la prÃ©cipitation mais que câ€™est un outil bien plus puissant.\n\nElle nous a ainsi montrÃ© comment elle utilise cette commande afin dâ€™organiser et de donner une chronologie Ã  son travail rendant ainsi la revue de code plus facile. Elle a ainsi, devant nos yeux, changÃ© des fichiers de commits, rÃ©organisÃ© lâ€™ordre des commits et tout Ã§a sans altÃ©rer le code produit.\n\nÃ‰videmment, câ€™est une habitude Ã  prendre, elle-mÃªme le souligne que ce nâ€™est pas facile dâ€™exporter cette bonne pratique au sein des Ã©quipes avec qui elle travaille. Mais la dÃ©monstration mâ€™a convaincu, je vais mâ€™essayer Ã  cette pratique et qui sait, un jour jâ€™arriverai peut-Ãªtre Ã  mon tour Ã  convaincre des gens de mon Ã©quipe Ã  en faire de mÃªme.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nViolence HermÃ©neutique - Comment Ã©viter le malaise\n\nLe MiXiT est aussi un Ã©vÃ¨nement nous permettant dâ€™ouvrir notre esprit Ã  des connaissances qui sortent de notre quotidien. Cette confÃ©rence animÃ©e par Romeu Moura et Sara Dufour en fait partie. Ce talk nous fait dÃ©couvrir le concept dâ€™hermÃ©neutique, dÃ©fini en dÃ©but de prÃ©sentation comme Ã©tant â€œLa connaissance dâ€™un concept permettant lâ€™interprÃ©tationâ€. Si vous nâ€™avez rien compris Ã  cette dÃ©finition Ã  ce stade, câ€™Ã©tait Ã©galement mon cas.\n\nMalgrÃ© cette introduction confuse, petit Ã  petit, en allant de plus en plus dans le dÃ©tail, des sujets apparaissent et donnent sens Ã  ce concept. On y parle de systÃ©misme, de charge mentale, de patriarcat et autres systÃ¨mes de notre sociÃ©tÃ© dont lâ€™exercice de comprÃ©hension va plus loin que leur simple mot ou leur dÃ©finition. Lâ€™hermÃ©neutique consiste Ã  comprendre les fondements et rouages dâ€™un systÃ¨me, quâ€™on y appartienne ou non.\n\nMais notre sociÃ©tÃ©, et lâ€™humain, tend Ã  compliquer cet exercice de comprÃ©hension de concept. Câ€™est lÃ  quâ€™on arrive Ã  la notion de violence hermÃ©neutique, Ã  savoir tous les mÃ©canismes conscients et inconscients, systÃ©miques ou non, internes ou externes, qui vont venir entraver et contraindre lâ€™hermÃ©neutique. De rÃ©els freins Ã  la comprÃ©hension dâ€™un systÃ¨me. Ils peuvent prendre plusieurs formes, comme la notion de norme, le fait de nier lâ€™existence dâ€™un systÃ¨me ou de rÃ©futer un sujet du simple fait quâ€™il soit considÃ©rÃ© tabou. On y retrouve Ã©galement la dÃ©formation de mots, et le fameux â€œwokismeâ€.\n\nIl sâ€™agit dâ€™une confÃ©rence passionnante, dÃ©rangeante et Ã©clairante que je conseille Ã  tous. Le dÃ©but piÃ©tine un peu, mais le voyage en vaut la peine.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nDesigner pour le service public\n\nCela peut faire un peu peur dit comme Ã§a, mais je suis allÃ© sceptique Ã  cette confÃ©rence dâ€™Anne-Sophie Tranchet. Jâ€™Ã©tais rattachÃ© Ã  une image peu flatteuse des outils du service public, alors que ces derniers ont connu une vraie progression ces derniÃ¨res annÃ©es. Anne-Sophie fait partie du programme beta.gouv qui intervient auprÃ¨s des administrations pour les services numÃ©riques.\n\nCâ€™est armÃ© des bonnes pratiques de nos mÃ©tiers que Beta.gouv a la mission de transformer et dâ€™accompagner les services publics. On y apprendra le parcours dâ€™Anne-Sophie, ce que travailler pour le service public veut dire, ainsi que les projets et challenges qui en dÃ©coulent. Leur mÃ©thodologie centrÃ©e utilisateur leur permet de travailler en itÃ©ration, et de dÃ©livrer de la valeur, en incubation dâ€™abord, puis jusquâ€™Ã  un dÃ©veloppement national en fonction des retours sur le service.\n\nOn peut citer quelques rÃ©alisations Beta.gouv comme la plateforme dossierfacile.com, qui facilite la crÃ©ation de dossier pour une location, ou 1000 Premiers Jours, qui dÃ©livre des informations et un accompagnement sur la grossesse et les 2 premiÃ¨res annÃ©es de lâ€™enfant\n\nPage du talk sur le site de MiXiT et voir le replay\n\nNos autres confÃ©rences coup de coeur\n\n\n  Ma vie est un ticket de Romain Couturier, une confÃ©rence racontÃ© avec dessins lÃ©gÃ¨re et qui donne des idÃ©es pour lutter contre la mauvaise utilisation des outils de ticketing\n  Tout ce que lâ€™on ne vous pas dit sur lâ€™IA de AmÃ©lie Cordier, une confÃ©rence pleine dâ€™humour sur ce quâ€™est et nâ€™est pas une IA\n\n"
} ,
  
  {
    "title"    : "Comment appliquer automatiquement des modifications sur une codebase JS ğŸ¤–",
    "category" : "",
    "tags"     : " javascript, outil, cytron, frontend, react, refactor, js",
    "url"      : "/refactorer-avec-jscodeshift",
    "date"     : "July 26, 2022",
    "excerpt"  : "Dans cet article, je vais vous prÃ©senter JSCodeshift, une libraire qui va vous permettre dâ€™analyser et appliquer automatiquement des modifications sur du code Javascript ou Typescript.\n\nCas dâ€™Ã©cole ğŸ‘¨â€ğŸ“\n\nMaintenir Ã  jour les dÃ©pendances de nos proj...",
  "content"  : "Dans cet article, je vais vous prÃ©senter JSCodeshift, une libraire qui va vous permettre dâ€™analyser et appliquer automatiquement des modifications sur du code Javascript ou Typescript.\n\nCas dâ€™Ã©cole ğŸ‘¨â€ğŸ“\n\nMaintenir Ã  jour les dÃ©pendances de nos projets JS est lâ€™une des rÃ¨gles primordiales que nous nous efforÃ§ons de bien respecter pour ne pas avoir Ã  jeter nos applications tous les deux ans. ğŸ—‘\n\nCette tÃ¢che exige souvent dâ€™un dÃ©veloppeur plus de travail que de simplement changer les versions des libraires dans le package.json.\nSi une dÃ©pendance est utilisÃ©e dans diffÃ©rentes parties du code et quâ€™un breaking-change est introduit, on peut vite se retrouver avec des centaines de fichiers Ã  modifier manuellement.\n\n\n\n\nâ„¹ï¸ Exemple d&#39;un project Javascript qui ne respecte pas cette rÃ¨gle\n\n\n\nCâ€™est un problÃ¨me de ce genre que nous avons rencontrÃ© lors de la mise Ã  jour de notre librairie dâ€™internationalisation sur notre web app React en JS.\n\nAprÃ¨s mise Ã  jour, lâ€™appel Ã  lâ€™API de la librairie change de formeÂ :\n//Before\nconst t: (\n    translationKey: string,\n    // All options are passed as parameters\n    data?: object, // Data used for interpolation\n    number?: number, // Amount used for plural form\n    general?: boolean, // Use general plural form\n    renderers?: object // JSX renderers\n) =&amp;gt; string\n\n//After\nconst t: (\n    translationKey: string,\n    // Object containing all options\n    options?: {\n      data?: object, // Data used for interpolation\n      number?: number, // Amount used for plural form\n      general?: boolean, // Use general plural form\n      renderers?: object // JSX renderers\n    }\n) =&amp;gt; string\n\n\nPlus simplement, quelques exemples de transformationsÂ :\n// Before\nconst title1 = t(&#39;translationKeyExample&#39;)\nconst title2 = t(labelKey, { someData }, aNumber);\nconst title3 = t(&#39;translationKeyExample&#39;, undefined, 0);\n\n// After\nconst title1 = t(&#39;translationKeyExample&#39;); // Basic usecase with only one argument, nothing changed on this one\nconst title2 = t(labelKey, { data: { someData }, number: aNumber });\nconst title3 = t(&#39;translationKeyExample&#39;, { number: 0 });\n\n\nDans le cas le plus basique sans les arguments optionnels t(â€˜translationKeyâ€™) nous nâ€™avons rien Ã  modifier, mais dans les autres cas, il y a du changement Ã  faire. ğŸ§¹\n\nLes solutions que nous avons Ã©cartÃ©es âŒ\n\n\n  Avec un Find All, trouver toutes les utilisations de la librairie et modifier les appels problÃ©matiques Ã  la main.\n    \n      Cette solution est la plus simple, mais peut Ãªtre trÃ¨s rÃ©pÃ©titive, ce qui augmente la probabilitÃ© de faire une erreur. On aura du mal Ã  uniquement filtrer les cas spÃ©cifiques qui nous intÃ©ressent.\n    \n  \n  Utiliser des RegExp pour mieux cibler les cas spÃ©cifiques\n    \n      Cela nous a permis de faire rapidement une estimation approximative du nombre de cas quâ€™il nous faudrait modifier, mais nous avons eu du mal Ã  cibler correctement tous les appels et la modification se fait toujours Ã  la main.\n    \n  \n  CrÃ©er un fichier de dÃ©finition TypeScript pour la librairie, et laisser le Language Server Protocol ou son IDE trouver les appels problÃ©matiques\n    \n      La solution la plus rapide et la plus fiable pour la partie dÃ©tection, mais qui demande toujours de faire les modifications Ã  la main.\n    \n  \n\n\nMais il nous restait encore un Joker pour cette tÃ¢che. ğŸƒ\n\nJSCodeshift ğŸª„\n\nCette librairie permet dâ€™exposer facilement lâ€™Abstract Syntax Tree, autrement dit la reprÃ©sentation du code aprÃ¨s le parsing des fichiers.\nNous pouvons ainsi Ã©crire des scripts qui nous permettent de parcourir cet arbre, de le modifier facilement, dâ€™appliquer les modifications et de les formater.\nCes scripts sâ€™appellent des codemods.\n\nPour en savoir un peu plus sur lâ€™Abstract Syntax Tree, je vous conseille de jeter un coup dâ€™Å“il Ã  ASTExplorer qui vous permet de visualiser lâ€™AST dâ€™un fichier facilement pour en comprendre le fonctionnement.\n\nQuelques librairies ont proposÃ© des codemods lors de leurs grosses mises Ã  jour, par exemple React avec react-codemod.\n\n\n\n\nâ„¹ï¸ Capture d&#39;Ã©cran du site ASTExplorer\n\n\n\nEn application ğŸ’ª\n\nmodule.exports = function (file: FileInfo, api: API) {\n  const j = api.jscodeshift;\n\n  // If we don&#39;t find any &quot;Translate&quot; string inside our file, we can assume that it&#39;s safe to skip it\n  const regex = new RegExp(&#39;Translate[(]&#39;, &#39;i&#39;);\n  if (!regex.test(file.source)) {\n    return null;\n  }\n\n  return j(file.source)\n    .find(j.CallExpression, {\n      callee: {\n        type: &#39;Identifier&#39;,\n        name: &#39;t&#39;,\n      },\n    })\n    .filter(filterOutSimpleUsages)\n    .map(mutatePath(j))\n    .toSource();\n};\n\n\nDans la fonction principale du script, jâ€™ai utilisÃ© une expression rÃ©guliÃ¨re pour filtrer les fichiers qui ne possÃ¨dent pas la chaÃ®ne de caractÃ¨res Translate(.\nCeci permet de gagner un peu de temps sur lâ€™exÃ©cution. âŒ›ï¸\n\nEnsuite, je cherche dans le fichier une ou plusieurs variables t. Si aucune nâ€™est prÃ©sente, on peut passer au fichier suivant, sinon on continue le raffinage.\n\nOn passe dans un filtre qui va nous permettre dâ€™enlever les usages de la fonction t avec un seul argument qui ne posent pas de problÃ¨me.\n\nconst requiredPropertiesKeys = [&#39;data&#39;, &#39;number&#39;, &#39;general&#39;, &#39;renderers&#39;] as const;\n\n// Filter function to ensure that we enter the mutation function only if needed\nconst filterOutSimpleUsages = (p: ASTPath&amp;lt;CallExpression&amp;gt;) =&amp;gt; {\n  const args = p.value.arguments;\n\n  // If we only have the translation key, we don&#39;t need to refactor this usage\n  if (args.length === 1) {\n    return false;\n  }\n\n  // More than 2 arguments is an absolute sign of an old usage\n  // If second argument is not an object, we need to manually fix this case\n  if (args.length &amp;gt; 2 || args[1].type !== &#39;ObjectExpression&#39;) {\n    return true;\n  }\n\n  // If none of the above properties is found in second argument, we can say that this is an old usage\n  return requiredPropertiesKeys.every(\n    (requiredPropertyKey) =&amp;gt;\n      !(args[1] as ObjectExpression).properties.find(\n        // I needed to do some TS trickery to avoid getting warnings everywhere, sorry for that\n        (property) =&amp;gt; ((property as ObjectProperty).key as Identifier).name === requiredPropertyKey,\n      ),\n  );\n};\n\n\nFinalement, on peut passer dans la fonction de mutation, qui va nous permettre de modifier directement le code des fichiers.\n\n// Mutation function, we apply our modification to the AST\nconst mutatePath = (j: JSCodeshift) =&amp;gt; (p: ASTPath&amp;lt;CallExpression&amp;gt;) =&amp;gt; {\n  const objectProperties = requiredPropertiesKeys.reduce((acc, propertyKey, index) =&amp;gt; {\n    const argument = p.value.arguments[index + 1];\n    // If no argument or argument is a spread type, we don&#39;t take it in consideration\n    if (!argument || argument.type === &#39;SpreadElement&#39;) {\n      return acc;\n    }\n\n    // If argument is undefined, we skip it\n    if ((argument as Identifier).name &amp;amp;&amp;amp; (argument as Identifier).name === &#39;undefined&#39;) {\n      return acc;\n    }\n\n    // We create a new object property with an identifier (the object key) and put our argument inside\n    return [...acc, j.objectProperty(j.identifier(propertyKey), argument)];\n  }, [] as ObjectProperty[]);\n\n  // Finally, we keep our translation key in first position and our newly created object in second argument\n  p.value.arguments = [p.value.arguments[0], j.objectExpression(objectProperties)];\n\n  return p;\n};\n\n\nOn rÃ©cupÃ¨re les arguments dÃ©jÃ  existants, on crÃ©e un nouvel objet et on y place nos arguments !\n\nRÃ©sultats âœ¨\n\nâ± Pour Ã  peu prÃ¨s 2900 fichiers, le script a mis moins de 5,9 secondes Ã  sâ€™exÃ©cuter (Macbook Pro 13â€ 2019).\n\nJSCodeshift nous a permis de cibler trÃ¨s rapidement 99 % des cas problÃ©matiques et de les corriger automatiquement.\n\nLe pourcentage restant concerne des cas oÃ¹ il Ã©tait gÃ©nÃ©ralement difficile de cibler la fonction t (passÃ©e en props Ã  un autre composant sous un autre nom). Ces quelques cas ont pu Ãªtre corrigÃ©s rapidement Ã  la main et dÃ©tectÃ©s grÃ¢ce Ã  nos nombreux tests (heureusement quâ€™on a une rÃ¨gle de bonne pratique pour Ã§a ğŸ˜‡).\n\ntl;dr &amp;amp; conclusion ğŸƒ\n\nVous pouvez retrouver la source du codemod ici mÃªme.\n\nSi vous Ãªtes mainteneur dâ€™une librairie, il peut Ãªtre trÃ¨s intÃ©ressant de livrer des codemods en mÃªme temps que les breaking-changes pour faciliter lâ€™adoption des mises Ã  jour par exemple !\n\nAvec une prise en main relativement facile pour un rÃ©sultat trÃ¨s rapide, nous avons Ã©tÃ© trÃ¨s satisfaits de JSCodeshift et nous nâ€™hÃ©siterons pas Ã  rÃ©utiliser cette librairie dans le futur. ğŸ‘Š\n\nMerci Ã  tous pour la lecture de mon premier article et JSCodeshiftez bien. ğŸ˜˜\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #17",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/07/22/bedrock-dev-facts-17.html",
    "date"     : "July 22, 2022",
    "excerpt"  : "Lâ€™Ã©tÃ© arrive, les vacances, le repos.\nEt vu les derniers devfacts des Ã©quipes Bedrock, il semblerait quâ€™il soit temps.\n\nLes autres articles de cette sÃ©rie sont disponibles ici.\n\nCâ€™est presque fini\n\n\n  Câ€™est fait Ã  80%.\nJâ€™ai fait tout le code, mais...",
  "content"  : "Lâ€™Ã©tÃ© arrive, les vacances, le repos.\nEt vu les derniers devfacts des Ã©quipes Bedrock, il semblerait quâ€™il soit temps.\n\nLes autres articles de cette sÃ©rie sont disponibles ici.\n\nCâ€™est presque fini\n\n\n  Câ€™est fait Ã  80%.\nJâ€™ai fait tout le code, mais il ne marche pas.\n\n\nLa potion magique\n\n\n  Je nâ€™ai plus de cerveau, je vais aller au bar\n\n\nCâ€™est dommage, ils allaient le faire !\n\n\n  Notre stratÃ©gie, câ€™est dâ€™Ãªtre Ã  la bourre pour que dâ€™autres fassent le travail\n\n\nLâ€™envers du dÃ©cor du tÃ©lÃ©travail\n\n\n  Quand il y a du vent chez moi la connexion est instable\n\n\nAh !\n\n\n  Jâ€™ai tirÃ© toute mon inspiration de manager des Shadocks\n\n\nLes consÃ©quences du management Ã  la Shadock\n\n\n  Je ne vais tout de mÃªme pas faire ma sieste pendant des heures de repos\n\n\nAuto-critique du manager Shadock. On est sauvÃ© !\n\n\n  Tout le monde sâ€™en fout de ce que je fais\n\n\nQuand tout le monde le fait, mais que personne ne comprend pourquoi\n\nEn parlant du TimeSheet, de maniÃ¨re naÃ¯ve\n\n  â€œMais il y a vraiment quelquâ€™un dans la boite qui fait Ã§a correctement ?â€\n\n\nEst-ce que je peux casser Ã  moitiÃ© ?\n\n\n  Est-ce quâ€™on peut faire un semi BC-break â‰ï¸\n\n\nCâ€™est pas faux\n\n\n  Câ€™est connu et assumÃ©, Ã§a fonctionnera pas jusquâ€™au moment oÃ¹ Ã§a fonctionnera\n\n\nQuand tu as dÃ©laissÃ© la review toute la semaine\n\n\n  Allez bon vendREVIEW Ã  tous\n\n\nğŸ¤”\n\n\n  Les tests passent mais Ã§a plante\n\n\nLe dÃ©tecteur Ã  petit-dej\n\n\n  \n    â€œje cherche X â€¦â€\n    â€œva voir Ã  la cafete, il y a quelquâ€™un qui a ramenÃ© de la bouffeâ€\n    â€œcâ€™est un honeypot Ã  X Ã§a !â€\n  \n\n\nCâ€™est beau le mob programming\n\n\n\nMeurs un autre jour\n\nQuand tu veux vraiment Ãªtre sÃ»r que ton code est exÃ©cutÃ© mais que tu oublies de nettoyer lâ€™historique\n\n\nCâ€™est pas compliquÃ© lâ€™informatique\n\n\n  \n    Comment tu as fait Ã§a ?\n    Jâ€™ai appuyÃ© sur des touches de mon clavier\n  \n\n\n\n\nGoogle translate nâ€™est pas toujours ton ami\n\n\n  \n    â€œTu peux traduire lâ€™US en franÃ§ais quand tu la lis ?â€\n    â€œPas de souci, Alorsâ€¦ il faut ajouter un message de grille pain dâ€™erreur lors du â€¦\n  \n\n\nSi Ã§a passait, câ€™Ã©tait beau\n\nUne personne qui parle Ã  son Ã©cran sur lequel il y a du code :\n\n  Dis moi que Ã§a va marcher, sâ€™il te plait ğŸ™\n\n\nSur un malentendu, il a Ã©tÃ© engagÃ©\n\n\n  Lâ€™essentiel de mes connaissances, câ€™est du bluff\n\n\nEt on fini avec un instant poÃ©sie\n\n\n  Mes pâ€™tits chats, demain câ€™est dÃ©mo infra,\nPour lâ€™instant il nâ€™y a pas dâ€™inscrits,\nNâ€™hÃ©sitez pas Ã  venir prÃ©senter votre travail accompli !\nDâ€™ici Ã  ce prochain rendez-vous,\nDes bisous ğŸ˜˜\n\n"
} ,
  
  {
    "title"    : "Encrypt AWS AMIs: one way to do it wrong",
    "category" : "",
    "tags"     : " cloud, aws",
    "url"      : "/2022/07/08/encrypt-aws-amis.html",
    "date"     : "July 8, 2022",
    "excerpt"  : "At Bedrock, we build our own privately shared AMIs (Amazon Machine Images) for different parts of our stack: kubernetes platform, vod platform, etc. We build those AMIs to optimize kernel parameters,to embed some tools, and more. We have been usin...",
  "content"  : "At Bedrock, we build our own privately shared AMIs (Amazon Machine Images) for different parts of our stack: kubernetes platform, vod platform, etc. We build those AMIs to optimize kernel parameters,to embed some tools, and more. We have been using Packer for a couple of years, and everything has been working just fine.\n\nConcerned about following AWS best-practices, we recently added encryption by default to all new EBS volumes in all our accounts.\n\nWe didnâ€™t expect it, but this decision impacted our AMI creation process. We thus began to update our Packer workflow to integrate this new constraint. We were telling ourselves that more security was for the best and we didnâ€™t take enough steps back to analyze drawbacks.\n\nYou will find in this blog post multiple tips that may help you handle your AMIs encryption, but also why you shouldnâ€™t handle it our way.\n\nBuild an encrypted AMI\n\nTo build our AMI, Packer launches an EC2 in a â€œbuilderâ€ account, then a snapshot is created and copied in needed regions. To use this AMI, â€œuserâ€ accounts are listed in the AMI allowed users.\n\nWith account EBS encryption enabled, snapshots are now encrypted. The default behavior is to use the accountâ€™s default KMS Key. Our first â€œeasyâ€ problem while trying to build new AMI with Packer was the following error message:\n\nError Copying AMI (ami-xxxxxx) to region (xx-xxx-x): InvalidRequest: Snapshot snap-xxxxxxx is encrypted. Creating an unencrypted copy from an encrypted snapshot is not supported.\n\n\nTo avoid that, we enabled AMI encryption with Packer, but it resulted in another error :\n\nError modify AMI attributes: InvalidParameter: Snapshots encrypted with the AWS Managed CMK can&#39;t be shared.\n\n\nAs our AMI has to be shared to other accounts, it was impossible to encrypt our AMI with the account default KMS Key. So we created a dedicated KMS Key for Packer encryption.\n\nAnd it worked! We had our beautiful encrypted AMI, ready to be used in all our accounts.\n\n\n\nHow we build our encrypted AMIs\n\nRun an encrypted AMI\n\nThis is where it gets complex.\n\nWhen we tried to launch an EC2 instance with our newly encrypted AMI, it failed with this error code :\n\nClient.InternalError: Client error on launch\n\n\nIt means that AWS canâ€™t use this AMI because it is encrypted.\n\nFirst step was to authorize the KMS Key to be used for encryption in user (external) accounts.\n\nThere are two methods to do that, for two different needs.\n\n\n\nPolicy method\n\nTo authorize an external customer managed role (ours), we had to authorize our role in KMS Key dedicated policy to use it, then authorize KMS Key in our role policy to be used. It is some kind of symmetric reference hard to correctly maintain with IaC (Terraform). And we had to do the same for KMS Key replicas in other regions, because they have a dedicated policy.\n\n\n\nPolicy method\n\nOne important thing to know here: some KMS Key permissions arenâ€™t available for external account sharing. It means that when we try to add the permission kms:* to our role policy (for debug purposes only, we follow least privileges principles), it failed. You can find which permission is accessible in cross account use and which is not here.\n\nGrant method\n\nTo authorize an AWS managed role, like AWSServiceRoleForAutoScaling (to launch our EC2), we also needed to allow it to use our key. It is impossible to add a new policy on an AWS Managed role. So instead of using a policy method like before, we had to create a grant on that role to use our key. We tried to create that grant from the source account (where the key is created), but it didnâ€™t work. We had to create that grant from the destination account (where AWSServiceRoleForAutoScaling is), using a role in the destination account that is allowed to create a grantâ€¦ So we had to allow a role from the destination account to create a grant with Policy method, then use the previous role to allow an AWS Managed Role to use our KMS Key with Grant method. Pretty fun, right?\n\n\n\nGrant method\n\n\n\nOnce all needed roles were allowed, we tried to launch an EC2 with the allowed role attached as an instance role. It failed again, because we needed to also use the AMI KMS Key on root volume of our instance. By default, it was the account KMS Key that was used.\n\nWe attached that key on our root volume, and it worked. We also could launch our EC2 with ASG. It was all good.\n\nBut there was a big security vulnerability: instead of using one KMS key per account to encrypt our EBS volume, we were now using the same KMS key on all our accounts because of our encrypted AMI.\n\nKMS Key rotation\n\nA short word about Key rotation: it can easily be enabled to automatically rotate key materials each year. All new AMIs will be encrypted with new key material and nothing has to be changed to run encrypted AMIs.\nBut in case of a manual rotation: if a key is leaked for example, you will need to recreate a new KMS Key, its replicas, and all permissions and grants seen before.\n\nConclusion\n\nUsing privately shared encrypted AMI caused us multiple problems:\n\n  higher complexity to maintain.\n  lower security in cross-account configuration.\n\n\nFurthermore, we checked all our AMIs to see if they contain sensitive data. It isnâ€™t the case : all sensitive data is uploaded at startup by Launch Template. We had no interest in continuing to use encrypted AMI, and we would have spared so much time if we had seen that sooner.\n\nThis is why we decided to disable encryption for all new EBS volume on our builder account and stop building encrypted AMI.\n\nDoing all the previous configuration took us several weeks. We are now more aware that doing security just for the beauty of it can be really counterproductive.\n\nIf your AMIs contain sensitive data, a better way to handle encrypted AMI may be to stop creating privately shared AMIs. Instead, copy and encrypt a private AMI in each of your â€œuserâ€ accounts with a dedicated KMS Key per account. As a result, there will be a larger amount of AMI to handle (one AMI per account per region), KMS Key permissions will still be complex, but security should improved.\n\nLogo used in thumbnail\nDeath by Imogen Oh from NounProject.com\nKey by Baboon designs from NounProject.com\nGears by Aybige from NounProject.com\n"
} ,
  
  {
    "title"    : "Debugging and reviewing your Android dependencies with apktool",
    "category" : "",
    "tags"     : " android, apktool, instrumentation, debugging, productivity",
    "url"      : "/2022/06/20/android-apktool-decompiling.html",
    "date"     : "June 20, 2022",
    "excerpt"  : "If you maintain an Android application, you might be relying on performance monitoring SDKs like Firebase Performance or New Relic, to name a couple. These plugins usually have a light setup processâ€”just apply a Gradle plugin, and they provide the...",
  "content"  : "If you maintain an Android application, you might be relying on performance monitoring SDKs like Firebase Performance or New Relic, to name a couple. These plugins usually have a light setup processâ€”just apply a Gradle plugin, and they provide the ability to collect statistics about every network call and database query in your app automatically.\n\nThe usual way to achieve this is to rely on a process called instrumentation, which is supported via the Android Gradle Pluginâ€™s Transform API, or its successor, the Instrumentation API. This feature is very powerful, and potentially dangerous; in our case, a minor patch of one of these SDKs caused a production bug that left one of our core features crippled.\n\nThe visible cause of our bug, from a developerâ€™s point of view, was that the video player saw the network requests as always being extremely fast, no matter the network quality. Therefore, it assumed the device had access to a very high bandwidth, and tried loading video segments with a very high bit rate. This did not go well for users with slower network speeds.\n\nTo understand what was going on, what went wrong, how to fix it and how to take measures so that it never happens again, we had to do some investigation.\n\nDiving into the Android build process\n\nBefore we get to the topic of instrumentation, we first need to know a little about the Android app build process. Donâ€™t worry, we wonâ€™t need to dive too deep into the details.\n\nTo put it simply, during the build process, your source files (Kotlin and Java) are compiled to Dalvik bytecode, which is stored in .dex files. These files are then packaged into an APK file, which is basically just a ZIP file with all your code and resources.\n\n\nflowchart LR\n    kt[.kt files] -- kotlinc --&amp;gt; dex[.dex files] --&amp;gt; packaging[[packaging]]\n    java[.java files] -- javac --&amp;gt; dex\n    res[resource files] -- aapt --&amp;gt; resc[compiled resource files] --&amp;gt; packaging --&amp;gt; APK\n    subgraph APK\n    direction TB\n    dex1[.dex] -.- dex2[.dex] -.- dex3[.dex] -.- dex4[.dex]\n    res1[res] -.- res2[res] -.- res3[res] -.- res4[res]\n    signature -.- manifest\nend\n\n\nUnderstanding bytecode instrumentation\n\nNow, letâ€™s say you want to take an existing application with its untouched source code, and automatically inject calls to your SDK every time a network call is made, to log whether it was successful or not. How would you achieve this?\n\nThe easiest way is to plug yourself into the build, right after the code is compiled into bytecode, and modify the bytecode to your will.\n\n\nflowchart LR\n    kt[.kt files] -- kotlinc --&amp;gt; dex[.dex files] --&amp;gt; transform[[transform]] --&amp;gt; packaging[[packaging]]\n    java[.java files] -- javac --&amp;gt; dex\n    res[resource files] -- aapt --&amp;gt; resc[compiled resource files] --&amp;gt; packaging --&amp;gt; APK\n    classDef transformed fill:#ff0000\n    class transform transformed\n\n    subgraph APK\n    direction TB\n    dex1[.dex] -.- dex2[.dex] -.- dex3[.dex] -.- dex4[.dex]\n    res1[res] -.- res2[res] -.- res3[res] -.- res4[res]\n    signature -.- manifest\n    class dex1,dex2,dex3,dex4 transformed\nend\n\n\nThe Android Gradle Plugin (AGP) offers APIs to do this, so SDK vendors can just develop a Gradle plugin and ta-da! Once you apply it, your app is automatically instrumented.\n\nNote that there are other ways to achieve this without the AGP. Notably, Kotlin now uses an Intermediate Representation (IR), before it gets compiled down to a target-specific format. You can write a Kotlin IR compiler plugin to transform the IR code and add your own hooks in an Android-agnostic way, although this API is still experimental at the time of writing.\n\nReverse-engineering a built APK\n\nNow, this is great. But when you open an APK file, what do you get?\n\nLetâ€™s unzip one and look inside.\n\n.\nâ”œâ”€â”€ META-INF\nâ”œâ”€â”€ assets\nâ”œâ”€â”€ google\nâ”œâ”€â”€ okhttp3\nâ”œâ”€â”€ res\nâ”œâ”€â”€ AndroidManifest.xml\nâ”œâ”€â”€ classes.dex\nâ”œâ”€â”€ classes2.dex\nâ”œâ”€â”€ classes3.dex\nâ”œâ”€â”€ classes4.dex\nâ”œâ”€â”€ firebase-common.properties\nâ”œâ”€â”€ firebase-crashlytics.properties\nâ”œâ”€â”€ play-services-base.properties\nâ”œâ”€â”€ ...\nâ””â”€â”€ resources.arsc\n\n\nA bunch of noise, and four interesting .dex files. Thatâ€™s where the appâ€™s code is stored, but unfortunately, these files are not human-readable.\n\nTo turn them into low-level but understandable code, some tooling will be necessary. The easiest to use for this task is apktool, which is free and open-source.\n\nLetâ€™s run apktool on our APK, and see what happens:\n\n\n\n\n\n~/Downloads\nâ¯ apktool d bedrock-sample-release.apk\nI: Using Apktool 2.6.1 on bedrock-sample-release.apk\nI: Loading resource table...\nI: Decoding AndroidManifest.xml with resources...\nI: Loading resource table from file: /Users/bcandellier/Library/apktool/framework/1.apk\nI: Regular manifest package...\nI: Decoding file-resources...\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nI: Decoding values */* XMLs...\nI: Baksmaling classes.dex...\nI: Baksmaling classes2.dex...\nI: Baksmaling classes3.dex...\nI: Baksmaling classes4.dex...\nI: Copying assets and libs...\nI: Copying unknown files...\nI: Copying original files...\nI: Copying META-INF/services directory\n\n\n\nThere we go! In our case, we can ignore the warnings. apktool created a new directory with a bunch of .smali files, organized by package: one file per class, containing their Dalvik bytecode.\n\n.\nâ”œâ”€â”€ AndroidManifest.xml\nâ”œâ”€â”€ res\nâ”‚Â Â  â”œâ”€â”€ values\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ strings.xml\nâ”‚   â”‚   â””â”€â”€ ...\nâ”‚   â”œâ”€â”€ layout\nâ”‚   â”‚   â”œâ”€â”€ layout_home.xml\nâ”‚   â”‚   â””â”€â”€ ...\nâ”‚   â””â”€â”€ ...\nâ”œâ”€â”€ smali\nâ”‚Â Â  â”œâ”€â”€ com\nâ”‚Â Â   Â Â  â”œâ”€â”€ bedrockstreaming\nâ”‚Â Â   Â Â  â”‚   â”œâ”€â”€ app\nâ”‚Â Â   Â Â  â”‚   â”‚Â Â  â”œâ”€â”€ mobile\nâ”‚Â Â   Â Â  â”‚   â”‚Â Â  â”‚Â Â  â”œâ”€â”€ R$anim.smali\nâ”‚Â Â   Â Â  â”‚   â”‚Â Â  â”‚Â Â  â”œâ”€â”€ R$layout.smali\nâ”‚Â Â   Â Â  â”‚   â”‚Â Â  â”‚Â Â  â”œâ”€â”€ R$string.smali\nâ”‚Â Â   Â Â  â”‚   â”‚Â Â  â”‚Â Â  â”œâ”€â”€ R$style.smali\nâ”‚       â”‚   â”‚   â”‚   â””â”€â”€ ...\nâ”‚       â”‚   â”‚   â””â”€â”€ ...\nâ”‚       â”‚   â””â”€â”€ ...\nâ”‚Â Â   Â Â  â””â”€â”€ google\nâ”‚Â Â   Â Â      â”œâ”€â”€ android\nâ”‚Â Â   Â Â      â”‚Â Â  â”œâ”€â”€ exoplayer2\nâ”‚Â Â   Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ AbstractConcatenatedTimeline.smali\nâ”‚Â Â   Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ AudioBecomingNoisyManager.smali\nâ”‚Â Â   Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ AudioFocusManager$AudioFocusListener$$ExternalSyntheticLambda0.smali\nâ”‚Â Â   Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ AudioFocusManager$AudioFocusListener.smali\nâ”‚Â Â   Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ AudioFocusManager.smali\nâ”‚Â Â   Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ BasePlayer.smali\nâ”‚Â Â   Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ BaseRenderer.smali\nâ”‚Â Â   Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ BuildConfig.smali\nâ”‚           â”‚   â”‚   â””â”€â”€ ...\nâ”‚           â”‚   â””â”€â”€ ...\nâ”‚           â””â”€â”€ ...\nâ”œâ”€â”€ smali_classes2\nâ”‚Â Â  â”œâ”€â”€ com\nâ”‚Â Â  â”‚Â Â  â””â”€â”€ bedrockstreaming\nâ”‚Â Â  â”‚Â Â      â”œâ”€â”€ app\nâ”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ mobile\nâ”‚Â Â  â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ MobileApplication.smali\nâ”‚   â”‚       â”‚   â”‚   â””â”€â”€ ...\nâ”‚   â”‚       â”‚   â””â”€â”€ ...\nâ”‚   â”‚       â””â”€â”€ ...\nâ”‚   â””â”€â”€ ...\nâ””â”€â”€ ...\n\n\nIf you see files with mangled names and contents, make sure that you run apktool on an APK with R8 obfuscation disabled, or youâ€™ll have a hard time figuring things out.\n\nUnderstanding Dalvik bytecode\n\nNow, if you open one of these files, it will contain code that looks like the snippet below. It will look unfamiliar; thatâ€™s normal.\n\n.method private final getContent()Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    .locals 2\n    .line 119\n\n    iget-object v0, p0, Lcom/bedrockstreaming/example/HomeViewModel;-&amp;gt;state:Landroidx/lifecycle/LiveData;\n\n    invoke-virtual {v0}, Landroidx/lifecycle/LiveData;-&amp;gt;getValue()Ljava/lang/Object;\n\n    move-result-object v0\n\n    instance-of v1, v0, Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    if-eqz v1, :cond_0\n\n    check-cast v0, Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    goto :goto_0\n\n    :cond_0\n\n    const/4 v0, 0x0\n\n    :goto_0\n\n    return-object v0\n  \n.end method\n\n\nIf youâ€™ve ever worked with assembly code before, you might notice similarities in the way the code is written. Each line begins with an instruction, which can take comma-separated parameters. To work out what these instructions and their parameters mean, you will need to refer to the Dalvik bytecode documentation provided by Google.\n\nLetâ€™s take an example line from the snippet and decode it together. Looking at the table in the documentation, we can see deduce this:\n\n# We&#39;ll decode this line:\ninvoke-virtual {v0}, Landroidx/lifecycle/LiveData;-&amp;gt;getValue()Ljava/lang/Object;\n\ninvoke-virtual                                                                   # We&#39;re calling a virtual method\n               {v0},                                                             # We&#39;re calling the method on the object referenced in register v0\n                     Landroidx/lifecycle/LiveData;                               # The method we&#39;re calling is defined by androidx.lifecycle.LiveData\n                                                  -&amp;gt;getValue()                   # We&#39;re calling a method called getValue()\n                                                              Ljava/lang/Object; # This method returns an Object\n\n\nWith some determination, we can figure out what the snippet does. Here, weâ€™re defining a getContent() method that tries to cast a LiveDataâ€™s value to State.Content and returns it, or null otherwise.\n\nUsing a decompiled APK as a debugging tool\n\nInspecting suspicious code\n\nBefore doing anything else, we can already start looking at the generated code to identify patterns that could cause issues. Problem isâ€¦ there can be a lot of code to look through.\n\nBefore going this deep in the rabbit hole, we already figured our issue was, somehow, related to instrumentation: disabling it fixed this issue; downgrading to the previous release of the SDK also fixed it. This means that if we want to get a clear look at what needs to change to go from a working APK from a broken one, we could just compare an APK instrumented by the previous SDK version with an APK instrumented by the current one!\n\nOf course, we want to do this on the human-readable smali files, not the raw dex files. We can generate a full diff with the help of the diff tool:\n\ndiff -bur normal/ instrumented/\n\n\nIn our case, it also proved useful to compare an APK that has been instrumented with one that hasnâ€™t, to understand what that instrumentation is meant to achieve. Most of it was to notify the SDK of every HTTP request, along with its result.\n\nAs a simple example, the snippet below shows a class belonging to Picasso. We can see the HTTP calls it makes are being intercepted by the SDK.\n\n--- normal/smali/com/squareup/picasso/NetworkRequestHandler.smali\t2022-01-05 11:09:22.000000000 +0100\n+++ instrumented/smali/com/squareup/picasso/NetworkRequestHandler.smali\t2022-01-05 11:08:34.000000000 +0100\n@@ -128,10 +128,26 @@\n\n     .line 103\n     :cond_4\n+    instance-of v2, v1, Lokhttp3/Request$Builder;\n+\n+    if-nez v2, :cond_5\n+\n     invoke-virtual {v1}, Lokhttp3/Request$Builder;-&amp;gt;build()Lokhttp3/Request;\n\n     move-result-object v2\n\n+    goto :goto_1\n+\n+    :cond_5\n+    move-object v2, v1\n+\n+    check-cast v2, Lokhttp3/Request$Builder;\n+\n+    invoke-static {v2}, Lcom/vendor/instrumentation/okhttp3/OkHttp3Instrumentation;-&amp;gt;build(Lokhttp3/Request$Builder;)Lokhttp3/Request;\n+\n+    move-result-object v2\n+\n+    :goto_1\n     return-object v2\n .end method\n\n\nFinding the source of the issue by iteration\n\nWe havenâ€™t talked about apktoolâ€™s greatest strength yet: its ability to recompile an APK from the smali sources it has decompiled! This means we can effectively decompile an APK, make modifications to its low-level code, recompile and run it.\n\nThis proved really useful during our investigation. Since we have one directory with our APK in a bad state, and one directory with our APK in a good state, we can process by elimination to point out exactly which single class, when modified, causes our bug.\n\nIn our case, a useful workflow was to start with a suspectâ€”letâ€™s say we think instrumenting the OkHttp classes might have caused the bug.\n\n\n  Copy the OkHttp classes from the â€œbadâ€ APK, and only those, to our â€œgoodâ€ APK.\n  Recompile and run the app.\n  Does the bug occur?\n    \n      If it does, then that means it is caused by the instrumentation of at least one of the OkHttp classes. We can go through this process again, this time by selecting only a subset of OkHttpâ€™s classes, and check if the bug still occurs, etc.\n      If it doesnâ€™t, revert the OkHttp classes and try again with another suspect.\n    \n  \n\n\nThis process can be accelerated with a very simple script, to iterate faster. The recompilation step occurs incrementally, and so only takes a few seconds.\n\n#!/bin/sh\n\n# rebuild-and-run.sh\n# Rebuild, sign and install an APK from its decompiled source.\n# (c) 2022 Bedrock Streaming\n\n# Inputs:\n# DECOMPILED_APK_PATH: path to your previously decompiled APK directory\n# KEYSTORE_PATH: path to your debug keystore\n# KEYSTORE_PASSWORD: your debug keystore password\n\napktool --use-aapt2 b &quot;$DECOMPILED_APK_PATH&quot; \\\n    &amp;amp;&amp;amp; apksigner sign -ks &quot;$KEYSTORE_PATH&quot; --ks-pass &quot;pass:$KEYSTORE_PASSWORD&quot; &quot;$DECOMPILED_APK_PATH/dist/*.apk&quot; \\\n    &amp;amp;&amp;amp; adb install &quot;$DECOMPILED_APK_PATH/dist/*.apk&quot;\n\n\nHereâ€™s what it looks like in action:\n\n\n\n\n\n~/bytecode-playground\nâ¯ ./rebuild-and-run.sh\nI: Using Apktool 2.6.1\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether resources has changed...\nI: Building apk file...\nI: Copying unknown files/dir...\nI: Built apk...\nPerforming Incremental Install\nServing...\nSuccess\nInstall command complete in 445 ms\n\n\n\nIn our case, we narrowed down the issue to the instrumentation of a single class: okhttp3.internal.http.CallServerInterceptor: once it was reverted, the bug disappeared.\n\nIn fact, we narrowed it down to a very small patch with which the app runs fine:\n\n .../okhttp3/internal/http/CallServerInterceptor.smali         | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali b/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\nindex c916149f..c26eab15 100644\n--- a/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\n+++ b/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\n@@ -510,7 +510,7 @@\n \n     instance-of v8, v14, Lokhttp3/Response$Builder;\n \n-    if-nez v8, :cond_b\n+    #if-nez v8, :cond_b\n \n     invoke-virtual {v14, v15}, Lokhttp3/Response$Builder;-&amp;gt;body(Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n \n@@ -574,7 +574,7 @@\n \n     instance-of v15, v8, Lokhttp3/Response$Builder;\n \n-    if-nez v15, :cond_e\n+    #if-nez v15, :cond_e\n \n     invoke-virtual {v8, v14}, Lokhttp3/Response$Builder;-&amp;gt;body(Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n \n-- \n\n\nBasically, when the code went through this if statement, our request got wrapped by com.vendor.instrumentation.okhttp3.OkHttp3Instrumentation:\n\ninvoke-static {v8, v14}, Lcom/vendor/instrumentation/okhttp3/OkHttp3Instrumentation;-&amp;gt;body(Lokhttp3/Response$Builder;Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n\n\nAnd what does this method do, you ask? Letâ€™s take a look at the decompiled source in Android Studio, so that itâ€™s a bit easier to read:\n\npublic Builder body(ResponseBody body) {\n    try {\n        if (body != null) {\n            BufferedSource source = body.source();\n            Buffer buffer = new Buffer();\n            source.readAll(buffer);\n            return this.impl.body(ResponseBody.create(body.contentType(), buffer.size(), buffer));\n        }\n    } catch (IOException var4) {\n        log.error(&quot;IOException reading from source: &quot;, var4);\n    } catch (IllegalStateException var5) {\n        log.error(&quot;IllegalStateException reading from source: &quot;, var5);\n    }\n\n    return this.impl.body(body);\n}\n\n\nThe body is being read into memory!\n\nsource.readAll(buffer);\n\n\nWhen correlating this discovery with the source code from ExoPlayer, we could verify that, indeed, our player was expecting that the time it takes reading the response body would be the time it took to download the entire video segment. Hereâ€™s what this flow looks like in a functional app:\n\n\nsequenceDiagram\n    participant exo as OkHttpDataSource\n    participant nr as OkHttp3Instrumentation\n    participant okhttp as OkHttpClient\n    participant server as Server Endpoint\n\n    exo-&amp;gt;&amp;gt;nr: body()\n    nr-&amp;gt;&amp;gt;okhttp: body()\n    activate server\n    okhttp-&amp;gt;&amp;gt;server: \n    server-&amp;gt;&amp;gt;okhttp: \n    okhttp-&amp;gt;&amp;gt;nr: ResponseBody (length=0)\n    activate exo\n    nr-&amp;gt;&amp;gt;exo: ResponseBody (length=0)\n    server-&amp;gt;&amp;gt;exo: length=512\n    server-&amp;gt;&amp;gt;exo: length=1024\n    server-&amp;gt;&amp;gt;exo: length=1536\n    server-&amp;gt;&amp;gt;exo: length=2048\n    server-&amp;gt;&amp;gt;exo: length=2560\n    note right of exo: OkHttpDataSource controls the body reads  and can measure the time it took  to read the whole response\n    deactivate server\n    deactivate exo\n\n\nBut with this bug in the SDK, since the HTTP response has been buffered into memory by some SDK, the read was always almost-instantaneous, no matter the speed of the connection. Additionally, it messed with the overall performance since requests were no longer properly streamed by their rightful users.\n\n\nsequenceDiagram\n    participant exo as OkHttpDataSource\n    participant nr as OkHttp3Instrumentation\n    participant okhttp as OkHttpClient\n    participant server as Server Endpoint\n\n    exo-&amp;gt;&amp;gt;nr: body()\n    nr-&amp;gt;&amp;gt;okhttp: body()\n    activate server\n    okhttp-&amp;gt;&amp;gt;server: \n    server-&amp;gt;&amp;gt;okhttp: \n    activate nr\n    okhttp-&amp;gt;&amp;gt;nr: \n    server-&amp;gt;&amp;gt;nr: length=512\n    server-&amp;gt;&amp;gt;nr: length=1024\n    server-&amp;gt;&amp;gt;nr: length=1536\n    server-&amp;gt;&amp;gt;nr: length=2048\n    server-&amp;gt;&amp;gt;nr: length=2560\n    deactivate nr\n    activate exo\n    note right of exo: OkHttpDataSource is only notified  after everything is downloaded\n    nr-&amp;gt;&amp;gt;exo: ResponseBody (length=2560)\n    deactivate server\n    deactivate exo\n\n\nUsing a decompiled APK as a review tool\n\nItâ€™s no secret to developers in any software ecosystem that library updates can be a source of problems - security vulnerabilities, bugs, incompatibilities, and so on. Itâ€™s hard to vet them properly, especially in compiled form, like libraries distributed in the Java ecosystem. Things get even harder when arbitrary Gradle plugins start rewriting our own code!\n\nThe tooling needed to decompile an APK is free, fast, and easy to automate. Itâ€™s a really helpful tool to investigate obscure bugs in places your debugger wonâ€™t let you place a breakpoint, and itâ€™s also really useful to be able to see a human-readable diff between two binaries.\n\nGenerating a diff of the effects of a library upgrade can seem overkill and hard to do in practice, but at least in the case of bug-fix releases with hopefully few changes, it can be very helpful to have an actual report of what changed. Itâ€™s an accepted practice to review the code your team checks in; why not review the code of others, since it ends up in the exact same artifact?\n"
} ,
  
  {
    "title"    : "Bedrock Ã  la Kubecon 2022, 4Ã¨me partieÂ : chaos, rÃ©silience, ressenti global et conclusion gÃ©nÃ©raleâ€¦",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/16/kubecon-2022-part-4.html",
    "date"     : "June 16, 2022",
    "excerpt"  : "Pour terminer cette sÃ©rie, un ou deux sujets divers que nous nâ€™avons pas regroupÃ© dans les trois articles prÃ©cÃ©dents\n(les performances applicatives et la scalabilitÃ©, \nles performances bas niveau, le systÃ¨me et le rÃ©seau,\nla dev XP, lâ€™outillage, l...",
  "content"  : "Pour terminer cette sÃ©rie, un ou deux sujets divers que nous nâ€™avons pas regroupÃ© dans les trois articles prÃ©cÃ©dents\n(les performances applicatives et la scalabilitÃ©, \nles performances bas niveau, le systÃ¨me et le rÃ©seau,\nla dev XP, lâ€™outillage, la CI/CD et lâ€™observabilitÃ©), \npuis une conclusion globale avec ce que nous avons retenu de cette KubeCon Europe 2022.\n\n\nLa conclusion, @ KubeCon 2022Â !\n\nChaos Engineering / Chaos Testing pour une meilleur rÃ©silience aux pannes\n\nCâ€™est un des sujets sur lesquels nous avons commencÃ© Ã  travailler activement cette annÃ©eÂ : casser des choses dans nos clusters, dans notre plateforme, entre nos microservices.\nLâ€™idÃ©e sous-jacente est, bien sÃ»r, que tout va casser un jour ou lâ€™autre, donc autant provoquer du chaos nous-mÃªme, en environnement contrÃ´lÃ©. Nous identifierons ainsi des points sensibles de notre plateforme et pourrons les corriger, Ã©vitant ainsi des incidents, parfois majeurs, au mauvais moment.\n\nCe thÃ¨me du Chaos Engineering est rÃ©guliÃ¨rement abordÃ© en confÃ©rences et nous Ã©tions contents de voir que nous ne sommes pas les seuls Ã  nous interroger sur â€œcommentâ€ en mettre en place.\nNous sommes repartis avec quelques pistes dâ€™outils, comme chaos mesh ou Litmus Chaos, que nous allons peut-Ãªtre prototyper pour les comparer Ã  chaos-controller que nous avons rÃ©cemment expÃ©rimentÃ©.\n\nAu cours de la confÃ©rence, â€œCase Study: Bringing Chaos Engineering to the Cloud Native Developersâ€ (vidÃ©o) par Uma Mukkara, Litmus et Ramiro Berelleza, Okteto, nous avons pu avoir un aperÃ§u de lâ€™outil de chaos Litmus, sa force semblant rÃ©sider dans le partage des scripts de chaos au sein la communautÃ©.\nPuis, il a Ã©tÃ© dÃ©crit une approche CI des tests de chaos visant Ã  intÃ©grer certains tests de chaos dans le flux de dÃ©veloppement plutÃ´t quâ€™Ã  la fin.\n\nEnfin, toujours sur des questions de rÃ©silience en cas dâ€™interruption de service, dans sa confÃ©rence â€œBuilding for the (inevitable) Next Cloud Outageâ€ (vidÃ©o), Pavel Nikolov de Section nous a questionnÃ©s sur la maniÃ¨re dâ€™Ãªtre plus robuste Ã  une catastrophe.\nLa question nâ€™est pas de savoir si une catastrophe se produira, mais quand elle se produira. Câ€™est pourquoi il est aussi prÃ©fÃ©rable de disposer dâ€™un plan de reprise aprÃ¨s sinistre mais surtout de prÃ©voir en amont un systÃ¨me dâ€™auto-guÃ©rison permettant dâ€™Ãªtre plus rÃ©silient aux catastrophes.\nIl nous a ensuite prÃ©sentÃ© un use case spÃ©cifique au rÃ©seau, nous invitant Ã  prÃ©fÃ©rer au traditionnel â€œDNS Ã  la rescousseâ€, la mise en place de BGP (Border Gateway Protocol).\n\nQuelques sujets divers\n\nÃ€ travers quelques talks, nous avons jetÃ© des coups dâ€™Å“il sur des sujets sur lesquels nous ne travaillons pas rÃ©ellement au quotidien â€“ appelez Ã§a de la curiositÃ© intellectuelle si vous le voulez ;-)\n\n\n  Nous avons vu un ensemble de design patterns pour le dÃ©veloppement de controllers Kubernetes (vidÃ©o), approche qui devient petit Ã  petit un moyen rÃ©pandu de rÃ©pondre Ã  des problÃ©matiques, en codant directement dans Kubernetes.\n  La confÃ©rence â€œA treasure map of hacking (and defending) K8sâ€ (vidÃ©o) Ã©tait trÃ¨s sympathique, elle montrait Ã  quel point il peut Ãªtre â€œfacileâ€ de prendre le contrÃ´le dâ€™une infrastructure. Une faÃ§on de montrer que patcher est obligatoireÂ !\n  Et dans un registre hors-technique, â€œComposability is to software as compounding interest is to financeâ€ (vidÃ©o) mettait en Ã©vidence Ã  quel point, en construisant des outils, puis des projets, puis un Ã©cosystÃ¨me, les uns profitent aux autres, on construit donc plus grand et plus gros. Il suffit de voir le landscape CNCF aujourdâ€™hui par rapport Ã  4 ans en arriÃ¨re.\n\n\nConclusion, KubeCon Europe 2022\n\nNous avons commencÃ© Ã  migrer vers Le Cloud, vers AWS et Kubernetes, il y a plus de quatre ans. Notre premiÃ¨re KubeCon Ã©tait Ã  Copenhague, en 2018. Que dire, en conclusion de cette confÃ©rence annuelleÂ ? Comment conclure ces articlesÂ ?\n\nAujourdâ€™hui, les grandes idÃ©es que nous avons retenues de cette KubeCon Europe 2022, en rÃ©sumant, sont les suivantesÂ :\n\n\n  Les problÃ©matiques dâ€™auto-scaling sont bien cernÃ©es, les outils sont plutÃ´t matures. Comme beaucoup dâ€™autres entreprises, nous arrivons sur lâ€™Ã©tape suivante, qui est de dimensionner en tenant mieux compte des couts et pas uniquement des performances et/ou de la disponibilitÃ©.\n  La gestion des coÃ»ts dans Kubernetes nâ€™est toujours pas simple. Ã€ la fois pour les suivre et les rÃ©partir, mais aussi pour dÃ©cider du bon compromis entre performancesÂ / disponibilitÃ©Â / souplesseÂ / autonomie des Ã©quipesÂ / couts.\n  Service MeshÂ : nous nâ€™avons toujours pas franchi le pas et Istio, qui Ã©tait un sujet trÃ¨s Ã  la mode il y a quatre ans, nous semble dÃ©sormais presque oubliÃ©. Aujourdâ€™hui, Cilium semble Ãªtre la nouvelle approche qui sâ€™impose, et il se pourrait que nous jouions avec â€œpour voirâ€ prochainementâ€¦\n  Lâ€™observabilitÃ©, plus vraiment un problÃ¨me.\n  Le chaos engineeringÂ / chaos testingÂ : toujours une idÃ©e sÃ©duisante, mais pas encore rÃ©ellement industrialisÃ©eÂ ?\n  Lâ€™outillage autour de la CI/CD, le dÃ©ploiement progressif, le rollback (possiblement automatisÃ©) progresse, et Ã§a fait plaisirÂ !\n  Lâ€™Ã©cosystÃ¨me progresse, mÃ»rit, et on parle de sujets de plus haut niveau que quelques annÃ©es en arriÃ¨re. Par exemple, nous avons entendu plusieurs fois parler de base de donnÃ©es magiquement scalable hÃ©bergÃ©e dans Kubernetes, alors que lâ€™Ã©poque oÃ¹ nous Ã©vitions de stocker quelque Ã©tat que ce soit dans un cluster ne nous semble pas si lointaineÂ !\n\n\nEt, pour finir, quelques points dont nous nâ€™avons pas du tout ou trÃ¨s peu entendu parlerÂ :\n\n\n  Nous avons peut-Ãªtre loupÃ© des choses en crÃ©ant nos programmes, mais nous nâ€™avons vu aucun talk autour de â€œcomment nous dÃ©veloppons des applications cloud-nativeâ€. Pourtant, la problÃ©matique de lâ€™environnement de dÃ©veloppement, avec des services managÃ©s, des dÃ©ploiements vers Kubernetes et des plateformes distribuÃ©es, ne nous parait pas encore rÃ©glÃ©eÂ !\n  Lâ€™approche â€œFaaSâ€ (Function as a Service) nous paraÃ®t encore moins rÃ©pandue que quelques annÃ©es en arriÃ¨reÂ ?\n  Nous nâ€™avons pas entendu parler une seule fois de FÃ©dÃ©ration, alors que le terme revenait encore et encore il y a quatre ans. Nous avons bien fait de ne mÃªme pas essayer, on diraitÂ ;-)\n\n\n\nRejoignez-nos Ã©quipes et venez vivre les prochaines confÃ©rences avec nous lâ€™an prochain\n"
} ,
  
  {
    "title"    : "Bedrock Ã  la Kubecon 2022, 3Ã¨me partieÂ : Dev XP, outillage, CI/CD, observabilitÃ©â€¦",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/15/kubecon-2022-part-3.html",
    "date"     : "June 15, 2022",
    "excerpt"  : "Pour notre troisiÃ¨me article de cette sÃ©rie sur ce que nous avons retenu de la KubeCon Europe 2022, aprÃ¨s \nles performances applicatives et la scalabilitÃ© et \nles performances bas niveau, le systÃ¨me et le rÃ©seau, \npassons Ã  la Developper eXperienc...",
  "content"  : "Pour notre troisiÃ¨me article de cette sÃ©rie sur ce que nous avons retenu de la KubeCon Europe 2022, aprÃ¨s \nles performances applicatives et la scalabilitÃ© et \nles performances bas niveau, le systÃ¨me et le rÃ©seau, \npassons Ã  la Developper eXperience, Ã  lâ€™outillage, Ã  la CI/CD, aux rollback, Ã  lâ€™observabilitÃ© et aux incidentsÂ !\n\n\nUne nouvelle journÃ©e commence, @ KubeCon 2022Â !\n\nLa prod est tombÃ©eÂ !\n\nDans notre secteur dâ€™activitÃ©, nous avons tous subis des incidents de production et le retour dâ€™expÃ©rience, quâ€™il soit interne ou public, est important et formateur.\nEn effet, mÃªme si les incidents de production sont malheureusement inÃ©luctables dans nos mÃ©tiers, il est important de les analyser afin de mieux les comprendre et demieux sâ€™en prÃ©munir.\n\nPreuve de lâ€™importance de ces sujetsÂ : nous avons assistÃ© Ã  deux confÃ©rences trÃ¨s intÃ©ressantes sur ce thÃ¨me dans des salles pleines Ã  craquerÂ !\nToutes deux portaient sur des incidents de production majeurs suite Ã  une modification de code qui peut paraÃ®tre anodineÂ : la premiÃ¨re Ã©tait donnÃ©e par Influxdata, la seconde par Skyscanner.\nLes confÃ©rences Ã©taient particuliÃ¨rement joviales et bienveillantesÂ : les rÃ©actions du public Ã  certains slides montraient bien que ce genre de situations sentait le vÃ©cu pour certainsÂ !\n\nNous avons tous Ã  apprendre de ces cas concrets dâ€™incident, aussi, nous vous conseillons de visionner les vidÃ©os de ces confÃ©rencesÂ : skyscanner et influxdata.\nMais si nous devions les rÃ©sumerÂ : lâ€™automatisation de bout en bout demande une grande maturitÃ©, beaucoup (beaucoupÂ !) de tests et des reviews de qualitÃ©Â !\n\nEt comme il est dit dans une des slidesÂ :\n\n\n\nDebugger, en production, avec des conteneurs Ã©phÃ©mÃ¨res\n\nNous utilisons rÃ©guliÃ¨rement la commande kubectl exec pour entrer dans conteneur / pod et y lancer des commandes de dÃ©bogage â€“ parce que certains problÃ¨mes ne sont pas reproductibles ailleurs quâ€™en production, ou parce quâ€™il faut comprendre ce quâ€™il se passe avant de savoir reproduire en environnement de dÃ©veloppement.\n\nCela dit, cette approche nâ€™est pas gÃ©nialeÂ : si nous modifions des choses dans un conteneur, ces modifications persistent.\nAussi, il faut pouvoir installer des outils de dÃ©bug dans un conteneur (ce quâ€™on ne peut pas facilement faire chez Bedrock, oÃ¹ nos conteneurs ne sâ€™exÃ©cutent pas en root et ont souvent un filesystem read-only), ou les embarquer dans les images (ce qui les grossit considÃ©rablement, sans compter lâ€™augmentation du risque de failles de sÃ©curitÃ©).\n\nPour remÃ©dier Ã  cette problÃ©matique, la fonctionnalitÃ© de conteneurs Ã©phÃ©mÃ¨res (vidÃ©o) arrive en bÃªta dans Kubernetes 1.23 et Ã§a semble absolument gÃ©nialÂ !\nLâ€™outil parfait pour lancer des conteneurs temporaires Ã  lâ€™intÃ©rieur de pods existant et incroyablement puissant pour dÃ©buggerÂ !\nNous allons pouvoir rÃ©duire le nombre dâ€™outils de debug intÃ©grÃ©s Ã  nos images et parvenir Ã  dÃ©bugger plus aisÃ©ment des problÃ¨mes qui ne surviennent quâ€™en productionÂ !\n\nLes risques de lâ€™observabilitÃ© / ObservabilitÃ© piratÃ©e\n\nâ€œHow attackers use exposed Prometheus Server to Exploit Kubernetes Clustersâ€ (vidÃ©o) par David de Torres et  Miguel Hernandez, ou â€œcomment obtenir lâ€™empreinte de vos clusters k8s Ã  travers vos donnÃ©es de monitoringâ€.\n\nSysdig est venu nous remÃ©morer que le monitoring, câ€™est bien, mais que ne pas exposer ses donnÃ©es de monitoring, câ€™est mieuxÂ !\nEn effet, attention aux informations qui sont exposÃ©es Ã  lâ€™extÃ©rieur, elles pourraient Ãªtre recueillies par des attaquants externes pour acquÃ©rir des connaissances sur votre plateforme (provider cloud, version de lâ€™OS utilisÃ©â€¦) et sâ€™en servir ensuite pour sâ€™introduire dans votre infrastructure (fuite de donnÃ©es, cryptominage ou ransomware).\n\nÃ€ travers un cas dâ€™utilisation fictif, ils nous ont dÃ©montrÃ© la facilitÃ© de rÃ©cupÃ©ration de ces informations et comment elles sont utilisÃ©es pour monter une attaque.\nEnfin, ils nous ont rappelÃ© que pour se prÃ©munir de ces attaques, il suffit de suivre les recommandations de sÃ©curitÃ©Â ! CQFD.\nIl est toujours bon dâ€™avoir ces piqures de rappel et de toujours bien penser aux donnÃ©es que lâ€™on expose vers lâ€™extÃ©rieur.\n\nCI/CD, dÃ©ploiement progressif\n\nChez Bedrock, nous sommes en pleine refonte de notre chaÃ®ne de CI/CDÂ : nous basculons tous nos projets du bon vieux Jenkins â€œtemporaireâ€, que nous avions montÃ© au dÃ©but de notre migration vers Le Cloud, vers Github Actions.\nAu passage, nous nous demandons forcÃ©ment comment nous pourrions amÃ©liorer nos dÃ©ploiements et les rendre plus sÃ©curisÃ©s, tant pour la santÃ© de notre plateforme que pour la paix dâ€™esprit de nos Ã©quipes et de nos utilisateurs.\n\nLa confÃ©rence â€œAutomated progressive delivery using gitops and service meshâ€ (vidÃ©o) parlait de dÃ©ploiement progressif avec Argo CD, pour amÃ©liorer lâ€™excellence opÃ©rationnelle, rÃ©duire le MTTR, accroÃ®tre lâ€™automatisation et la fiabilitÃ© des processus de dÃ©ploiement. Bref, des idÃ©es qui nous parlentÂ !\n\nReste des fonctionnalitÃ©s, qui nous semblent primordiales avant de se lancer sur un autre outil, qui ne sont pas encore gÃ©rÃ©es, hÃ©lasÂ : mirroring de traffic, routing basÃ© sur des en-tÃªtes (typiquementÂ : pour faire du dÃ©ploiement progressif Ã  la maille â€œutilisateurâ€ et pas Ã  la maille â€œrequÃªte HTTPâ€), dÃ©tection dâ€™anomalie et rollback automatisÃ©â€¦\nUn projet Ã  suivre, donc, qui pourrait mÃ»rir dans les prochains mois.\n\nAu niveau des aspects moins sympathiquesÂ : cette approche de dÃ©ploiement progressif passe par un service mesh (envoy, ici).\nOr nous nâ€™en avons pas en place et depuis quatre ans nâ€™avons toujours pas trouvÃ© les bons arguments pour en introduire dans nos clusters, notamment Ã  cause de la complexitÃ© ajoutÃ©eâ€¦\n\nUne autre confÃ©rence (vidÃ©o) mentionnait lâ€™outil Flagger pour des dÃ©ploiements Canary.\n\nQuelques autres idÃ©es Ã  retenir\n\nNous avons aussi vu quelques autres confÃ©rences dont nous avons tirÃ© quelques idÃ©es, en plus brefÂ :\n\n\n  Kubernetes 1.23 apporte (en alpha) une nouvelle commande kubectl events, qui retourne ses rÃ©sultats dans lâ€™ordre chronologique. Ce que lâ€™actuel kubectl get events ne fait pas et Ã§a peut Ãªtre bien embÃªtant. Vue comme de la culture gÃ©nÃ©rale, la confÃ©rence â€œThe soul of a new command: adding â€˜eventsâ€™ to kubectlâ€ (vidÃ©o) racontait comment cette fonctionnalitÃ© a Ã©tÃ© implÃ©mentÃ©e et Ã©tait fort intÃ©ressante.\n  Un speaker parlait de la mise en place de Crossplane dans son entreprise (vidÃ©o). Sujet potentiellement intÃ©ressant, mais qui ne correspond pas Ã  notre approche actuelle. Nous avons toutefois retenu quelques points autour de comment il fournit des outils Ã  ses collÃ¨gues dÃ©veloppeursÂ : documentation, composition de services, management dâ€™attentes, utilisation de lâ€™Ã©cosystÃ¨meâ€¦ Des problÃ©matiques auxquelles nous nous sommes confrontÃ©s de nombreuses fois, pour encourager nos Ã©quipes Ã  adopter des Ã©volutions ou de nouveaux outilsÂ !\n  Si vous commencez Ã  mettre en place votre stack de logs, la confÃ©rence â€œShow me your labels and Iâ€™ll tell you who you areâ€ (vidÃ©o) est faite pour vous. Lâ€™idÃ©e dâ€™utiliser les labels assignÃ©s aux pods pour aller jusquâ€™Ã  filtrer lâ€™accÃ¨s aux logs via RBAC, terribleÂ ! Aussi, la crÃ©ation de flux de logs avec Logging Operator a lâ€™air fort sympathique. Si ce talk Ã©tait venu trois ans plus tÃ´t, câ€™est quelque chose que nous essayeronsÂ !\n\n\nConclusion\n\nCes confÃ©rences nous ont permis dâ€™approfondir les questions que nous nous posons actuellement alors que notre changeons de CI/CD (dÃ©ploiement progressif, rollback automatisÃ© ou nonâ€¦).\n\nPlus globalement, nous sommes contents de voir que lâ€™outillage autour de Kubernetes continue Ã  progresser et que la Developer eXperience est un sujet pris au sÃ©rieux dans notre communautÃ©.\n\n\nRejoignez-nos Ã©quipes et venez vivre les prochaines confÃ©rences avec nous lâ€™an prochain\n"
} ,
  
  {
    "title"    : "Bedrock Ã  la Kubecon 2022, 2nde partie : performances, systÃ¨me et rÃ©seau",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/14/kubecon-2022-part-2.html",
    "date"     : "June 14, 2022",
    "excerpt"  : "Pour ce second article de synthÃ¨se de la KubeCon Europe 2022, continuons sur le thÃ¨me des performances, peut-Ãªtre plus bas niveau,\net plongeons aussi dans des outils pouvant Ãªtre dÃ©ployÃ©s au cÅ“ur de nos clusters !\n\n\nCa va commencer, @ KubeCon 2022...",
  "content"  : "Pour ce second article de synthÃ¨se de la KubeCon Europe 2022, continuons sur le thÃ¨me des performances, peut-Ãªtre plus bas niveau,\net plongeons aussi dans des outils pouvant Ãªtre dÃ©ployÃ©s au cÅ“ur de nos clusters !\n\n\nCa va commencer, @ KubeCon 2022Â !\n\nLâ€™autoscaling, autrement\n\nUne des confÃ©rences : â€œAutoscaling Kubernetes Deployments: A (Mostly) Practical Guideâ€ (vidÃ©o) prÃ©sentÃ© par NewRelic prÃ©sentait le principe dâ€™autoscaling dans Kubernetes, avec les trois principales ressources associÃ©es Ã  ce concept : ClusterAutoscaler, HorizontalPodAutoscaler et VerticalPodAutoscler.\n\nCette confÃ©rence prÃ©sentait :\n\n\n  le fonctionnement du scale up/down des pods avec les pÃ©riodes de stabilisationÂ ;\n  le calcul par rapport aux indicateurs utilisÃ©sÂ ;\n  les types de mÃ©triques utilisables par les HPA et VPA.\n\n\nPas de grande dÃ©couverte technique pour nous, mais cette confÃ©rence nous a surtout permis de confirmer que, chez BedRock, \nnous sommes de plus en plus matures sur la scalabilitÃ© de nos clusters Kubernetes.\n\nLa confÃ©rence donnÃ©e par AWS (vidÃ©o) portait sur deux aspectsÂ :\n\n\n  PremiÃ¨rement, lâ€™utilisation dâ€™instances Spot (une option Ã  ne pas nÃ©gliger si vous souhaitez fortement rÃ©duire vos coÃ»ts de compute) et les bonnes pratiques Ã  mettre en place en utilisant ce type dâ€™instances EC2.\n  Le second traitait de la scalabilitÃ© des nÅ“uds avec ClusterAutoscaler mais prÃ©sentait un nouvel outil de provisionnement de nÅ“uds Kubernetes proposÃ© par AWSÂ : Karpenter.\n\n\nUne des diffÃ©rences notables par rapport Ã  cluster-autoscaler est que Karpenter ne fonctionne pas avec des AutoScalingGroup AWS \nmais provisionne directement des instances EC2.\nOutre cette fonctionnalitÃ©, Karpenter est actuellement Ã  lâ€™Ã©tude chez BedRock, notamment car il permet lâ€™utilisation de la \ndimension rÃ©gion, ce qui nâ€™est pas possible avec cluster-autoscaler et nous pose des problÃ¨mes avec nos statefullsets dans des ASG multiAZ.\n\nRÃ©seau, Bande passante et GPU\n\nAutre point abordÃ© lors de la KubeConÂ : comment intÃ©grer la bande passante comme une ressource limitante, de la mÃªme faÃ§on que le CPU et la RAM actuellement.\nNous avons pu suivre deux prÃ©sentations Ã  ce sujetÂ : â€œNetwork-aware Scheduling in Kubernetesâ€ de JosÃ© Santos, Ghent University (video et â€œBetter Bandwidth Management with eBPFâ€ de Daniel Borkmann et Christopher M. Luciano, Isovalent (video).\n\nLa premiÃ¨re session proposait un nouveau plugin (repo github) pour permettre lâ€™orchestration du dÃ©ploiement de nouveaux pods en fonction de leur charge et coÃ»t rÃ©seau, afin de rÃ©duire la latence des dÃ©ploiements.\nUne nouvelle fonctionnalitÃ© de ce plugin est par ailleurs en dÃ©veloppement et permettra dâ€™Ã©viter de dÃ©ployer sur un nÅ“ud ou la bande passante est dÃ©jÃ  saturÃ©e.\n\nLa seconde prÃ©sentation exposait comment eBPF permet de mettre en place de nouveaux pods en prenant en compte la bande passante. Le replay de la confÃ©rence est disponible ici, nous vous conseillons son visionnage.\nCette approche pourrait Ãªtre trÃ¨s intÃ©ressante pour Bedrock si nous dÃ©cidions de migrer notre plateforme VOD sur un cluster KubernetesÂ : en effet, elle nous permettrait de mieux gÃ©rer les burst rÃ©seaux et le throttling de la bande passante qui se produisent sur nos instances.\n\nCÃ´tÃ© GPU, Google, dans son exposÃ© â€œImproving GPU Utilization using Kubernetesâ€ de Maulin Patel et Pradeep Venkatachalam (video), nous a prÃ©sentÃ© deux faÃ§ons de partager des ressources GPU dans un cluster kubernetesÂ :\n\n\n  soit en partageant le temps dâ€™utilisation (timesharing, temporal multiplexing) entre conteneurs sur un mÃªme nÅ“ud,\n  soit en multi-instance GPU (MIG, spatial multiplexing) permettant de partager les ressources en parallÃ¨le entre conteneur en allouant une partie des cÅ“urs GPU et de sa mÃ©moire pour chaque conteneur.\n\n\nCette confÃ©rence sur lâ€™utilisation des GPU dans un cluster k8s nous incite Ã  rÃ©flÃ©chir aux optimisations que nous pourrions faire sur nos plateformes vidÃ©o et dataâ€¦\n\nService MeshÂ : Cilium\n\nAu cours de diverses confÃ©rences, nous avons plusieurs fois entendu le nom de â€œCiliumâ€ associÃ© au concept de Service Mesh.\nLa confÃ©rence â€œA guided tour of Cilium Service Meshâ€ (vidÃ©o) nous a permis dâ€™en apprendre plus sur ce nouveau service qui ne se base plus sur des sidecars, mais sur eBPF.\n\nUn outil peut-Ãªtre encore un peu jeune, mais clairement prometteur â€“ et trÃ¨s certainement quelque chose que nous allons Ã©tudier lors dâ€™un POC dans le courant de lâ€™annÃ©eÂ ;-)\n\nRÃ©capitulatif\n\nIl nâ€™existe toujours pas dâ€™outils magique pour passer Ã  lâ€™Ã©chelle et supporter les pics de charges.\nToutefois, les solutions prÃ©sentÃ©es au cours de cette KubeCon EU 2022 viennent rÃ©pondre Ã  des besoins qui sont apparus au fil des annÃ©es et dont peu dâ€™utilisateurs avaient mesurÃ© lâ€™impact au dÃ©but de leur pÃ©riple avec Kubernetes.\n\nAussi, eBPF continue Ã  faire parler de lui et son utilisation semble se rÃ©pandre.\nLâ€™idÃ©e dâ€™un service mesh plus lÃ©ger que Istio, par exemple, a lâ€™air fort intÃ©ressanteÂ !\n\n\nRejoignez-nos Ã©quipes et venez vivre les prochaines confÃ©rences avec nous lâ€™an prochain\n"
} ,
  
  {
    "title"    : "Bedrock Ã  la kubecon 2022, 1ere partie : performances applicatives et scalabilitÃ©",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/13/kubecon-2022-part-1.html",
    "date"     : "June 13, 2022",
    "excerpt"  : "\n\nBEDROCK Ã  la KubeCon 2022\n\nAprÃ¨s 2018 Ã  Copenhague et 2019 Ã  Barcelone, cette annÃ©e encore, nous Ã©tions trois, Coraline, Julien et Pascal, prÃ©sents Ã  la KubeCon CloudNativeCon Europe 2022, Ã  Valencia !\n\nPlus de quatre ans aprÃ¨s le dÃ©but de notre...",
  "content"  : "\n\nBEDROCK Ã  la KubeCon 2022\n\nAprÃ¨s 2018 Ã  Copenhague et 2019 Ã  Barcelone, cette annÃ©e encore, nous Ã©tions trois, Coraline, Julien et Pascal, prÃ©sents Ã  la KubeCon CloudNativeCon Europe 2022, Ã  Valencia !\n\nPlus de quatre ans aprÃ¨s le dÃ©but de notre migration vers Le Cloud (AWS + Kubernetes) racontÃ©e dans Le Plan Copenhague, nous visions Ã  dÃ©couvrir de nouvelles idÃ©es, Ã  confirmer certains de nos choix et Ã  apprendre des retours dâ€™expÃ©rience de nos pairs. AprÃ¨s tout, avec une communautÃ© aussi large (plus de 7000 participants et participantes cette annÃ©e), il serait dommage de rester seuls avec nos idÃ©es !\n\nSommaire\n\nÃ€ trois, nous avons assistÃ© Ã  une grosse quarantaine de confÃ©rences. Nous avons choisi dâ€™organiser nos notes par thÃ¨mes, en quatre articles :\n\n  Un premier, celui-ci, centrÃ© sur les performances applicatives, sur la scalabilitÃ© des applications et la gestion des coÃ»ts.\n  Le second, consacrÃ© aux performances systÃ¨me, aux services mesh, aux fonctionnalitÃ©s au niveau du cluster.\n  Le troisiÃ¨me, pour regrouper ce qui est Dev XP, outillage, CI/CD, rollback, observabilitÃ©â€¦\n  Et un dernier, pour quelques sujets divers, dont le chaos engineering et la rÃ©silience, et pour conclure sur ce que nous avons retenu de cette Ã©dition de la KubeCon publication jeudi.\n\n\nAvec une plateforme de VOD et de replay dÃ©ployÃ©e en marque blanche pour des broadcasters europÃ©en majeurs, des millions dâ€™utilisateurs actifs, des milliers de CPU consommÃ©s, des centaines dâ€™instances allumÃ©es et des dizaines de microservices, les performances sont au cÅ“ur de nos prÃ©occupations.\n\nCet article reprend nos retours sur les nombreuses confÃ©rences consacrÃ©es Ã  la scalabilitÃ© lors de cette KubeCon 2022. Cette fonctionnalitÃ© essentielle de Kubernetes est lâ€™une des raisons de notre migration sur cette plateforme. En effet, notre activitÃ© nÃ©cessite que nous adaptions la taille de nos clusters en fonction du nombre dâ€™utilisateurs connectÃ©s.\nNous avons donc assistÃ© Ã  la plupart des confÃ©rences consacrÃ©es Ã  la performance et Ã  lâ€™adaptation de celle-ci en fonction de nos besoins.\n\nLe scaling vertical\nLa confÃ©rence â€œHow Lombard Odier Deployed VPA to Increase Resource Usage Efficiencyâ€ (vidÃ©o) nous prÃ©sentait comment fonctionnent les requests et limits.\nUn sujet qui demande du temps pour Ãªtre efficace afin de ne pas Ãªtre en oversizing ou au contraire en undersizing.\n\nMais surtout, le confÃ©rencier nous a prÃ©sentÃ© son implÃ©mentation dâ€™un composant Kubernetes assez rarement utilisÃ© : Le VerticalPodAutoscaler. Le VPA Ã  fait rÃ©cemment lâ€™objet de discussions au sein de nos Ã©quipes et cette prÃ©sentation a confirmÃ© notre ressenti : cette ressource est intÃ©ressante pour des cas dâ€™usages spÃ©cifiques, notamment sur des â€œworkloadsâ€ assez consommateurs en RAM et/ou en CPU et ne pouvant pas Ãªtre dÃ©coupÃ©s en multiples pods via un HorizontalPodAutoscaler.\n\nle VPA souffre toujours dâ€™une limitation : lâ€™ajout de RAM ou CPU Ã  chaud nâ€™est pas possible et nÃ©cessite la re-crÃ©ation du pod.\n\n\n\nAmÃ©liorer la scalabilitÃ©\nUne autre confÃ©rence, donnÃ©e cette fois-ci par Intel, prÃ©sentait un projet rÃ©cent : Telemetry Aware Scheduler (vidÃ©o). Cet outil permet dâ€™amÃ©liorer les choix du scheduler de Kubernetes en sâ€™appuyant sur des mÃ©triques â€œcustomsâ€. Le projet est rÃ©cent et en ALPHA, mais Ã  surveiller dans lâ€™avenir.\n\nLors dâ€™une autre confÃ©rence intitulÃ©e â€œHow Adobe is optimizing resource usage in K8sâ€ (vidÃ©o), Carlos Sanchez a prÃ©sentÃ© un outil interne permettant dâ€™Ã©mettre des recommandations basÃ©es sur un historique de mÃ©triques, un peu comme fait VPA, mais au niveau dâ€™un namespace ou du cluster entier. Il est Ã©galement revenu sur comment ils parviennent Ã  Ã©teindre automatiquement des applications non utilisÃ©es par les clients pour rÃ©aliser des Ã©conomies consÃ©quentes.\n\nMais comment configurer les requests, limits et tout Ã§aâ€¦ sans y passer des mois ?\n\nNotre plateforme est composÃ©e de dizaines de services qui interagissent les uns avec les autres et sont soumis Ã  un trafic qui varie au quotidien, avec des pics parfois impressionnants. Le paramÃ©trage des requests et limits de chaque conteneur, ainsi que dâ€™autres ressources, comme le nombre de processus php-fpm par conteneur, est un travail de fourmi, oÃ¹ nous devons itÃ©rer quotidiennement pendant une ou deux semaines, en travaillant application par application. Et tout ce travail est Ã  refaire lorsque les applications ou leurs usages Ã©voluentâ€¦ un vrai casse-tÃªte !.\nNous ne sommes pas les seuls Ã  rencontrer ces problÃ©matiques et câ€™Ã©tait le sujet de la confÃ©rence â€œGetting the optimal service efficiency that autoscaler wonâ€™t give youâ€ (vidÃ©o), oÃ¹ une approche basÃ©e sur de lâ€™IA (ou, plutÃ´t, sur du brute-force) Ã©tait prÃ©sentÃ©e.\n\nVoici les grandes lignes de la mÃ©thodologie prÃ©sentÃ©e :\n\n  dÃ©finition dâ€™un scÃ©nario de load-test (ce qui reste difficile, il faut quâ€™il soit reprÃ©sentatif de la rÃ©alitÃ©)\n  DÃ©finition dâ€™objectifs (les temps de rÃ©ponses attendus, le pourcentage dâ€™erreursâ€¦ en fait, des SLOs que chacun devrait dÃ©jÃ  avoir pour ses services),\n  Lancer en boucle ces scenarios en retouchant request et limits (et configuration JVM) entre chaque itÃ©ration.\n\n\nSur un cas rÃ©el, aprÃ¨s la 34áµ‰ itÃ©ration (rÃ©alisÃ©es en 19 heures), environ 49% dâ€™Ã©conomies ont Ã©tÃ© rÃ©alisÃ©es. Mais surtout, cela a reprÃ©sentÃ© un jour de travail grÃ¢ce Ã  cet outillage, au lieu de deux mois Ã  la main.\n\nLe logiciel utilisÃ© ne semble pas disponible en open-source, mais lâ€™approche â€œautomatiser les itÃ©rationsâ€ en retouchant les paramÃ¨tres est trÃ¨s intÃ©ressante et nous saurions la reproduire. Elle nous permettrait de gagner beaucoup de temps, en supprimant beaucoup de tÃ¢ches fastidieuses aujourdâ€™hui. Reste Ã  continuer Ã  dÃ©finir des SLOs, puis crÃ©er de nouveaux scÃ©narios de load-testing reprÃ©sentatifs ! ;-)\n\nEt les coÃ»ts dâ€™hÃ©bergement, alors ?\n\nNous avons aussi entendu parler plusieurs fois de coÃ»ts dâ€™hÃ©bergement tout au long de cette KubeCon : comme lâ€™illustrent les travaux de la FinOps Foundation, nous sommes de plus en plus nombreux Ã  rÃ©aliser que si nous ne pensons pas Ã  lâ€™impact financier de nos infrastructures Ã©lastiques, oÃ¹ nâ€™importe quel membre des Ã©quipes peut dÃ©ployer des applications, la facture augmente vite et fort.\n\nLe talk â€œWhy Kubernetes canâ€™t get around FinOps - Cost Management best practicesâ€ (vidÃ©o) Ã©tait une bonne introduction aux principes de gestion de coÃ»ts sur Kubernetes. Rien de nouveau pour nous, sur la thÃ©orieâ€¦ mÃªme sâ€™il nous reste encore beaucoup de progrÃ¨s Ã  rÃ©aliser pour mieux maÃ®triser nos frais dâ€™hÃ©bergement !\n\nConclusion\nSur ces sujets de scalabilitÃ©, les confÃ©rences auxquelles nous avons assistÃ© confirment que bon nombre des choix que nous avons fait sont les bons, et que les problÃ©matiques qui nous font encore souffrir sont partagÃ©es par dâ€™autres membres de la communautÃ©.\n\nNous allons prochainement tenter de mettre en place VPA sur un de nos composants majeur, VictoriaMetrics, qui consomme beaucoup de ressources quelques heures par jour et pour lequel un scaling horizontal nâ€™est pas adaptÃ©.\n\nNous nâ€™en avons pas (ou peu) entendu parler pendant cette KubeCon, mais nous Ã©tudions en ce moment la solution Karpenter pour remplacer cluster-autoscaler, trÃ¨s utilisÃ© dans la communautÃ©, mais qui ne sait pas rÃ©ellement tirer profit de spÃ©cificitÃ©s liÃ©es Ã  AWS.\n\nEnfin, sur les coÃ»tsâ€¦ OK, il nâ€™y a pas que chez nous que câ€™est compliquÃ©. Et câ€™est clairement un sujet, dans Kubernetes comme au niveau dâ€™AWS, sur lequel nous avons encore du boulot devant nous pour un an ou deux. Nous avons mÃªme un poste FinOps ouvert ;-)\n\n\n"
} ,
  
  {
    "title"    : "Bedrock&#39;s backend architecture and its front API Gateway",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front",
    "url"      : "/2022/06/10/backend-bff-intro.html",
    "date"     : "June 10, 2022",
    "excerpt"  : "What is a BFF, and how does it simplify the development of frontend applications?\n\nIntroduction\n\nAhoy there o/\n\nThis article is the first in a series explaining the backend architecture we use at Bedrock.\nThis first piece is dedicated to the BFF A...",
  "content"  : "What is a BFF, and how does it simplify the development of frontend applications?\n\nIntroduction\n\nAhoy there o/\n\nThis article is the first in a series explaining the backend architecture we use at Bedrock.\nThis first piece is dedicated to the BFF API. Without further delay, letâ€™s jump into the subject!\n\nSo, whatâ€™s a BFF?\n\nFor years, we have been using a microservices pattern (1). Each with their own responsibilities.\n\nBackend and frontend development have long been decoupled.\nEvery frontend applications had to know about all microservices, call them and know what to do with their data.\n\n\n\nThis approach had three main limitations from a frontend point of view:\n\n  It forced Bedrock to duplicate logic in each application.\n  It prevented us from deprecating legacy APIs.\n  New features implied a frontend development and deployment.\n\n\nThere were other downsides, which were mainly derivatives from those listed above.\n\n  As an example, updating an icon into the menu bar required us to deploy all applications. It is not always easy or doable, and cannot be forced onto users without losing some of them.\n\n\nThe BFF tries to answer those limitations!\n\nItâ€™s a single API (2) that handles all the frontend applications queries to display contents, navigation, or even start downloads.\nIn addition, this gateway (3) gathers all business logic. This is done in order to avoid repeating the logic in each application.\nThatâ€™s what we call a Back For Front!\n\nAbstracting the microservices\n\n\n\nThe first main advantage is to abstract the backend complexity for the frontend teams.\nIn the previous model, each application had to know where each data came from, how to parse it, and what to do with it.\n\nIn the new model, we can easily deprecate an API, replace it, change how the data is stored or returned.\nTo do so, we only need the team leading the change, and the team handling the BFF to work together at their own pace.\nThey can decide, depending on the change, how to handle the migration.\nThey might decide to use a new endpoint to be switched at some point, or add a new attribute in the response, etc.\n\nAll those changes will happen without any frontend application noticing it.\n\nSimplification of the data structure\n\nAnother advantage is to simplify the data representation.\n\n\n\nBy taking all this responsibility in a single API, it now translates the data from the APIs to a single unified representation that all applications can use.\n\n\n\nThis representation is maintained by the BFF in a single openapi schema (4). It shares the same concepts between the multiple endpoints of the API.\n\nThe main usage of the BFF is to handle the navigation between the pages of the application.\nIn the pictures above and below, the central block shows the application screen. The application page is split into two parts.\n\nThe top is answered by the navigation endpoint which gives a list of groups and entries.\nEvery entry can have nested groups, and an action.\n\nThe second part is what we call the layout. Itâ€™s a representation of the page, composed of multiple blocks, each with a list of items.\nEach item has a title, a description, an image, and an action (the same type as in the entries).\n\nThis makes the BFF responsible for what to display in the page, and in which order and how to display it.\nHow to display things is described through template strings that tell how to display each block.\n\nItâ€™s important to understand that the BFF does not return HTML! It returns a JSON string that needs to be parsed and interpreted by the application.\n\nEvery application still has to care about its design system, what font to use, which iconography.\nThis means that a template Card might not be displayed exactly the same between a computer, a mobile phone or a television; even if the data are the same.\n\n\n\nThere are other usages to the BFF (5), such as handling downloads, and some others to come, but it shares the same concept by answering to the front something to display.\n\nKeeping all logic in one place\n\nThe last main gain with the BFF, is that weâ€™re able to put all the logic in one place.\nThis allows us to update and change the business rules at any time.\n\nHere are a few examples\n\n\n  When a user tries to navigate the application, if he uses a new device while he has already reached the limit of allowed devices, we can display a layout asking him to delete a device first.\n\n\nThis limit can be removed or changed at any time in all applications.\n\n\n  In France, explicit contents must be filtered out during daytime\n\n\nIf this rule changes, we will do so directly in the BFF, and no application will ever notice it.\n\nConclusion\n\nThe model known as back for front or API Gateway is nothing new and other major services already use it.\nWeâ€™ve been using this model for more than 3 years now. It has undergone some major updates (6) but this is a model weâ€™re happy with.\n\nWe plan to expand this pattern to handle even more logic inside the BFF in the coming years and keep being frontend applicationâ€™s best friend.\n\n\n\nThatâ€™s all for todayâ€™s post!\n\nIn the next part we will talk about handling the failures of the dependencies the BFF is calling, and what to do to always answer something usable by the applications.\n\nNotes\n\n\n  For more details about microservices, you can read this piece from AWS.\n  There are some other APIs called by our applications, such as the authentication service, but letâ€™s not get lost into detailsâ€¦\n  Thereâ€™s a lot of resources about API Gateway, here is one from nginx.\n  Open API is used to define the communication standards between our BFF and the clients, more explanation on the dedicated website of the organization.\n  In addition to note 1, we are currently moving to the api gateway model, and some behaviors still require the application to call dedicated microservices.\n  ( in French ğŸ‡«ğŸ‡· ) An old conference from 2020 given by Benoit VIGUIER, previous Team Lead in charge of the BFF, about API gateway and asynchronous development.\n\n\nFrom the same series\n\n\n  Whatâ€™s a BFF\n  Handling API failures in a gateway\n  Whatâ€™s an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  ğŸ‡ºğŸ‡¸ Announcing BedrockStreaming/pr-size-labeler github action\n  ğŸ‡«ğŸ‡· Retour sur lâ€™AFUP Day Lille 2022\n\n"
} ,
  
  {
    "title"    : "Migration progressive vers Redux Toolkit",
    "category" : "",
    "tags"     : " redux, lyonjs, meetup, react, javascript, conference",
    "url"      : "/2022/06/08/migration-progressive-vers-redux-toolkit.html",
    "date"     : "June 8, 2022",
    "excerpt"  : "Redux est le gestionnaire dâ€™Ã©tat global le plus populaire au sein de la communautÃ© JS.\nSes crÃ©ateurs encouragent dÃ©sormais lâ€™utilisation de Redux Toolkit (RTK). Une suite dâ€™utilitaires facilitant lâ€™usage de Redux et rÃ©duisant notamment sa verbosit...",
  "content"  : "Redux est le gestionnaire dâ€™Ã©tat global le plus populaire au sein de la communautÃ© JS.\nSes crÃ©ateurs encouragent dÃ©sormais lâ€™utilisation de Redux Toolkit (RTK). Une suite dâ€™utilitaires facilitant lâ€™usage de Redux et rÃ©duisant notamment sa verbositÃ©.\nDans cette prÃ©sentation, je vous propose un live coding pour migrer pas-Ã -pas une application React/Redux vers RTK.\n"
} ,
  
  {
    "title"    : "Comment ne pas jeter son application Frontend tous les deux ans ?",
    "category" : "",
    "tags"     : " conference, js, react, lyonjs, meetup",
    "url"      : "/2022/06/08/comment-ne-pas-jeter-votre-application.html",
    "date"     : "June 8, 2022",
    "excerpt"  : "Bonnes pratiques pour la maintenance dâ€™une application web\nRefaire son front tous les 2 ans, câ€™est devenu une pratique plutÃ´t courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la mÃªme codebase et Ã§...",
  "content"  : "Bonnes pratiques pour la maintenance dâ€™une application web\nRefaire son front tous les 2 ans, câ€™est devenu une pratique plutÃ´t courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la mÃªme codebase et Ã§a depuis plus de 7 ans! En plus, ce nâ€™est pas une petite application puisquâ€™il sâ€™agit de 6play et de salto.\nVous pourriez vous dire: â€œOh les pauvres, maintenir une application vieille de presque 10 ans Ã§a doit Ãªtre un enfer !â€\nRassurez-vous, ce nâ€™est pas le cas ! Nous avons tous travaillÃ© sur des projets bien moins vieux mais sur lesquels le dÃ©veloppement de nouvelles fonctionnalitÃ©s Ã©tait bien plus pÃ©nible.\n\nQuel est notre secret ? Câ€™est ce que vous allez dÃ©couvrir pendant ce talk !\nAutomatisation des tÃ¢ches courantes, gestion de la dette, testing et architecture seront des sujets abordÃ©s.\nCe talk propose des thÃ©matiques qui ne concernent pas que le frontend !\n\nPlus de dÃ©tails dans lâ€™article suivant.\n\n"
} ,
  
  {
    "title"    : "ğŸª Itâ€™s Cookie Jar Time ğŸª #LFT 03/06/22",
    "category" : "",
    "tags"     : " UX, lft, tech",
    "url"      : "/%F0%9F%8D%AA-its-cookie-jar-time-%F0%9F%8D%AA",
    "date"     : "June 3, 2022",
    "excerpt"  : "DÃ©couvrez â€œCookie Jarâ€, la base de donnÃ©es de connaissances utilisateurs qui centralise et documente toute lâ€™UX research produite chez Bedrock. \nPrÃ©sentÃ© par Elise Carenau.\n",
  "content"  : "DÃ©couvrez â€œCookie Jarâ€, la base de donnÃ©es de connaissances utilisateurs qui centralise et documente toute lâ€™UX research produite chez Bedrock. \nPrÃ©sentÃ© par Elise Carenau.\n"
} ,
  
  {
    "title"    : "La pression je ne la subis pas, je la fais #LFT 03/06/22",
    "category" : "",
    "tags"     : " homemade, diy, beer, brewing, lft, partage",
    "url"      : "/la-pression-je-ne-la-subis-pas-je-la-fais",
    "date"     : "June 3, 2022",
    "excerpt"  : "Dans cette prÃ©sentation, Mathieu Lopez nous prÃ©sente un retour dâ€™expÃ©rience sur le brassage de biÃ¨re.\nBrasser sa biÃ¨re, comment Ã§a marche ? Quelles sont les Ã©tapes clÃ©s ? \nAu final, câ€™est quoi une biÃ¨re ?\n\n",
  "content"  : "Dans cette prÃ©sentation, Mathieu Lopez nous prÃ©sente un retour dâ€™expÃ©rience sur le brassage de biÃ¨re.\nBrasser sa biÃ¨re, comment Ã§a marche ? Quelles sont les Ã©tapes clÃ©s ? \nAu final, câ€™est quoi une biÃ¨re ?\n\n"
} ,
  
  {
    "title"    : "Errances Ã  KiilopÃ¤Ã¤ #LFT 03/06/22",
    "category" : "",
    "tags"     : " voyage, lft, partage",
    "url"      : "/errances-a-kiilopaa",
    "date"     : "June 3, 2022",
    "excerpt"  : "Mode dâ€™emploi et retour dâ€™expÃ©rience dâ€™un trek polaire hivernal en solitaire prÃ©sentÃ© par Sylvain Guyon.\n",
  "content"  : "Mode dâ€™emploi et retour dâ€™expÃ©rience dâ€™un trek polaire hivernal en solitaire prÃ©sentÃ© par Sylvain Guyon.\n"
} ,
  
  {
    "title"    : "CrÃ©er un jeu vidÃ©o en moins dâ€™une heure sur Unity #LFT 03/06/22",
    "category" : "",
    "tags"     : " diy, livecoding, brewing, lft, tech",
    "url"      : "/creer-un-jeu-video-en-moins-dune-heure-sur-unity",
    "date"     : "June 3, 2022",
    "excerpt"  : "Julie Nginn nous prÃ©sente une introduction au moteur de jeu Unity, en livecodant la construction dâ€™un jeu video.\nPas besoin dâ€™Ãªtre dÃ©veloppeur pour pouvoir crÃ©er un jeu vidÃ©o, ce talk sâ€™adresse Ã  tout le monde ğŸ™‚\n",
  "content"  : "Julie Nginn nous prÃ©sente une introduction au moteur de jeu Unity, en livecodant la construction dâ€™un jeu video.\nPas besoin dâ€™Ãªtre dÃ©veloppeur pour pouvoir crÃ©er un jeu vidÃ©o, ce talk sâ€™adresse Ã  tout le monde ğŸ™‚\n"
} ,
  
  {
    "title"    : "Connaissez vous CacheÂ°Cache ? #LFT 03/06/22",
    "category" : "",
    "tags"     : " swift, ios, lft, tech",
    "url"      : "/connaissez-vous-cache%C2%B0cache",
    "date"     : "June 3, 2022",
    "excerpt"  : "Petite dÃ©couverte dâ€™un pattern de composition en utilisant une librairie de cache comme exemple, prÃ©sentÃ©e par notre expert Sebastien Drode.\n",
  "content"  : "Petite dÃ©couverte dâ€™un pattern de composition en utilisant une librairie de cache comme exemple, prÃ©sentÃ©e par notre expert Sebastien Drode.\n"
} ,
  
  {
    "title"    : "Comment faire un trailer vidÃ©o qui dÃ©chire avec les technos web ? #LFT 03/06/22",
    "category" : "",
    "tags"     : " video, react, js, remotion, ffmpeg, lft, tech",
    "url"      : "/comment-faire-un-trailer-video-qui-dechire-avec-les-technos-web",
    "date"     : "June 3, 2022",
    "excerpt"  : "Un jour, alors que MickaÃ«l Alves Ã©tait fraichement arrivÃ© Ã  Bedrock, il a eu le malheur de demander Ã  Antoine Caron sur quoi il bossait entre midi et deux, qui semblait fort lâ€™amuser. Quelle erreur du dev Franco-Portugais ! :scream:\nIl ne se douta...",
  "content"  : "Un jour, alors que MickaÃ«l Alves Ã©tait fraichement arrivÃ© Ã  Bedrock, il a eu le malheur de demander Ã  Antoine Caron sur quoi il bossait entre midi et deux, qui semblait fort lâ€™amuser. Quelle erreur du dev Franco-Portugais ! :scream:\nIl ne se doutait pas encore de la folie de son nouveau tech leadÂ : Â«Â Jâ€™essaie de gÃ©nÃ©rer des vidÃ©os en MP4 Ã  partir de composants React, tu veux voirÂ ?Â Â»\n"
} ,
  
  {
    "title"    : "Amateur de pression #LFT 03/06/22",
    "category" : "",
    "tags"     : " plongÃ©e, partage, lft, partage",
    "url"      : "/amateur-de-pression",
    "date"     : "June 3, 2022",
    "excerpt"  : "Ivresse des profondeurs, exploration, dÃ©passement de soi, durant ce talk, Hugo Riffiod nous partage sa passion pour la plongÃ©e sous-marine.\n",
  "content"  : "Ivresse des profondeurs, exploration, dÃ©passement de soi, durant ce talk, Hugo Riffiod nous partage sa passion pour la plongÃ©e sous-marine.\n"
} ,
  
  {
    "title"    : "Announcing BedrockStreaming/pr-size-labeler github action ğŸ‰",
    "category" : "",
    "tags"     : " oss, github, devops",
    "url"      : "/2022/05/31/github-action-pr-size-labeler.html",
    "date"     : "May 31, 2022",
    "excerpt"  : "\n\nSmaller PR for a reduced mental load\n\nFor several years at Bedrock Streaming the technical teams have used the Pull Requests code review for each project. \nBetween collective ownership, quality improvement, regression detection, knowledge sharin...",
  "content"  : "\n\nSmaller PR for a reduced mental load\n\nFor several years at Bedrock Streaming the technical teams have used the Pull Requests code review for each project. \nBetween collective ownership, quality improvement, regression detection, knowledge sharing, learning, there is no question in this article to further legitimize the immense interest to implement this practice in your teams.\n\nThis practice can however lead to some problems, each developer who proposes Pull Requests for review by his colleagues can sometimes propose monstrous diffs.\nSometimes constrained by certain project mechanics or tools, but sometimes also by the â€œWheelbarrowâ€ effect.\n\n\n  While I was there, I took the opportunity to modify this too.\n\n\nIt always starts from a good will, however, to make PR that changes several intentions. \nBy creating his wheelbarrow, the developer is adding diff to a pull request that deviates from the original intent.\n\nLimiting the number of intentions of a pull request often simplifies the proofreading of it.\n\n\n  Has anyone ever had the pleasure of reviewing a Pull Request with more than 1000 lines of changes with more than 100 modified files?\n\n\nWe also forget that making a â€œbigâ€ Pull Request can also generate a mental load on the person or persons assigned to its development. \nWe have to remember the modified files, we are more likely to generate conflicts.\n\n\n  Ok, lets make smaller PRâ€™s! We promise!\n\n\nYou canâ€™t improve anything without measuring it\n\nSaying â€œfrom now on we do smaller PR1â€ is a pious hope.\nWe have been doing application monitoring for a long time, we know that thanks to these measurements we are able to understand if the evolution is rather positive or not.\nWhy not do it on our PR sizes?\nWhy not implement monitoring on our devs?\n\nThe idea is absolutely not to measure/comparison the performance of our developers. \nIt would not be positive for the engineering manager and the dev to compare the performance of one developer against another. \nWe are all different after all!\n\nThe size of a devâ€™s PRs does not reflect his productivity at all, it just allows to evaluate the personal and collective mental load produced.\nThere are other measures we would like to follow, but letâ€™s start with the size of the PR.\n\nBe warned, the purpose of this metric is not to say â€œOh! you made an XL size PR thatâ€™s not rightâ€ ğŸ˜¡.\nIt happens from time to time, and itâ€™s not bad.\nYou should rather look at the distribution of PR sizes of a dev.\n\nLetâ€™s take the example of a dev named Bob who would have this distribution over the last month:\n\n\nHere we see that Bob is globally making large PRs (taking arbitrary t-shirt sizes), seeing this we can say: As a TechLead, how can I best accompany Bob to make smaller PRs?\n\nNext, letâ€™s look at Aliceâ€™s profile, which has a more centered distribution:\n\n\nHere, we can say that overall the majority of RPs are of moderate size (in this absolute scale), so the mental load should be lower than for Bob.\nThis remains an interpretation that will require some discussion to be sure.\n\nHow to set it up?\n\nIf you are interested in this measure and like us you use the Github Actions solution for your automation, it will be very easy for you to implement our brand new pr-size-labeler in your projects.\n\nTo do so, you can add a workflow to your Github repository:\n\nname: ğŸ· PR size labeler\n\non: \n  pull_request:\n\njobs:\n  pr-labeler:\n    runs-on: ubuntu-latest\n    name: Label the PR size\n    steps:\n      - uses: BedrockStreaming/pr-size-labeler@v1.1.0\n        with:\n          token: $\n          exclude_files: .lock # RegExp of your excluded file pattern\n\n\nThe action will then put Size/S, Size/XL tags on your PRs automatically according to the number of modified files and the number of added or deleted lines.\n\nğŸ§™â€ You can change the text of the labels used and even the thresholds for each size as you wish.\nTake a look at Github presentation page of this Github action.\n\nOnce set up, you should also notice the added labels can allow to evaluate the time needed for the review before starting it.\n\n\n  Iâ€™ve got 30 minutes to spare, Iâ€™m not going to start reviewing this PR XL.\n\n\nItâ€™s now your turn to play!\n\n  \n    \n      Alias for Pull RequestÂ &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Retour sur l&#39;AFUP Day Lille 2022",
    "category" : "",
    "tags"     : " backend, php, conference",
    "url"      : "/2022/05/30/afup-day-lille-2022.html",
    "date"     : "May 30, 2022",
    "excerpt"  : "\n\nCette annÃ©e encore, Bedrock participait Ã  lâ€™AFUP Day 2022, nous avons eu la chance de profiter de confÃ©rences de qualitÃ© et aux sujets variÃ©s.\n\nPHP 8.1 en dÃ©tail\n\nDamien Seguy nous a parlÃ© des nouveautÃ©s de PHP 8.1 mais aussi de celles de PHP 8....",
  "content"  : "\n\nCette annÃ©e encore, Bedrock participait Ã  lâ€™AFUP Day 2022, nous avons eu la chance de profiter de confÃ©rences de qualitÃ© et aux sujets variÃ©s.\n\nPHP 8.1 en dÃ©tail\n\nDamien Seguy nous a parlÃ© des nouveautÃ©s de PHP 8.1 mais aussi de celles de PHP 8.2 qui arriveront prochainement.\n\nDamien nous a parlÃ© en vrac :\n\n\n  de lâ€™ajout du format AVIF dans GD\n  lâ€™ajout des fonctions fsync et fdatasync qui permettent de synchroniser les donnÃ©es dâ€™un fichier sur lâ€™OS. Cela veut dire que le fichier sera bien Ã©crit sur le disque.\n  lâ€™arrivÃ©e de la prise en compte du format ristreto255 avec le libsodium\n  de la fonction array_is_list\n\n\nLe confÃ©rencier a insistÃ© sur le fait de ne pas hÃ©siter Ã  monter de version rÃ©guliÃ¨rement. Il est possible dâ€™utiliser les polyfills dÃ©jÃ  existants (pour la 8.1 et mÃªme la 8.2) ou dâ€™ajouter ses propres fonctions, mais de bien faire attention dâ€™utiliser function_exists pour prÃ©parer les migrations.\n\nUne des nouveautÃ©s phare de la version 8.1 est lâ€™ajout de la gestion des enums. Une Ã©numÃ©ration est un objet et non un type scalaire. Il est donc possible de lâ€™Ã©tendre (via des interfaces, â€¦). Cela veut aussi dire que nous ne pouvons pas les utiliser comme clefs de tableau par exemple. Il est par contre possible dâ€™utiliser la propriÃ©tÃ© $myEnum-&amp;gt;value. Attention, les Ã©numÃ©rations et les classes partagent le mÃªme espace de nom, nous ne pouvons donc pas avoir une enum et une classe sâ€™appelant pareil dans le mÃªme namespace.\n\nLes Fibers, solution pour rendre les programmes interruptibles, a Ã©tÃ© rapidement introduite, mais nous vous conseillons de regarder la confÃ©rence sur ce sujet de notre cher et estimÃ© collÃ¨gue Benoit Viguier.\n\nDans la derniÃ¨re version, Ã  ce jour, a Ã©tÃ© introduit le type de retour never. Une fonction ou mÃ©thode qui retourne ce type ne pourra pas faire de return (mÃªme vide) ni mÃªme un yield. Il sera, par contre, possible dâ€™appeler die, exit, throw ou suspend.\n\nLes constantes peuvent maintenant Ãªtre finales, cela bloquera la possibilitÃ© de surcharger leurs valeurs par hÃ©ritage.\n\nIl est dÃ©sormais possible de faire des propriÃ©tÃ©s readonly. Cette propriÃ©tÃ© devra forcÃ©ment Ãªtre typÃ©e, et ne pourra pas avoir de valeur par dÃ©faut. readonly ne peut pas Ãªtre utilisÃ© avec static. Si la propriÃ©tÃ© est un objet, lâ€™instance pourra Ãªtre modifiÃ©e (par exemple avec des setters), mais pas remplacÃ©e.\n\nIl est maintenant possible dâ€™instancier des valeurs par dÃ©faut. Par exemple :\n\nfunction serialize(\n    string \\$data, \n    Formatter \\$formatter = new DefaultFormatter()\n) { ... }\n\n\nCette instanciation est possible dans les arguments de fonction ou de mÃ©thode, les variables statiques ou encore les constantes globales. Par contre, ce nâ€™est pas compatible avec les constantes de classes ou les propriÃ©tÃ©s de classes (sauf si ces derniÃ¨res sont des propriÃ©tÃ©s promues).\n\nDans sa version 8.1, PHP apporte aussi les types dâ€™intersections. Un exemple prÃ©sentÃ© serait de vouloir une instance de type Traversable ET Countable. Les types scalaires ne sont pas acceptÃ©s, câ€™est uniquement avec plusieurs classes.\n\nDe lâ€™humain Ã  lâ€™ordinateur, ou dÃ©couvrir le sens dâ€™un texte avec ElasticSearch\n\nMathias ARLAUD nous a ensuite parlÃ© dâ€™ElasticSearch et de comment il est possible d&#39;attribuer un score de corrÃ©lation entre un texte donnÃ© et une multitude d&#39;autres.\n\n\nIl a dÃ©cortiquÃ© cette Ã©quation (dÃ©jÃ  simplifiÃ©e) en nous expliquant les mÃ©canismes en place pour calculer ce score.\nIl nous a parlÃ© de Term Frequency (la frÃ©quence Ã  laquelle un mot apparaÃ®t dans un document), dâ€™Inverse Document Frequency (la pertinence des mots) ainsi que de Coordination Factor (le fait de valoriser un document avec le plus haut pourcentage de mots prÃ©sents dans la requÃªte).\nAvec un exemple simple (Les DÃ©veloppeurs ğŸ¥° dÃ©velopper avec,VIM&amp;amp;excl;), il nous a montrÃ© comment les filtres de caractÃ¨res tels que html_strip, le mapping, les gÃ©nÃ©rateurs de tokens (whitespace -&amp;gt; 1 token = 1 mot) ou bien encore les filtres de tokens (phonetic, stopwords) permettent dâ€™enlever le bruit des phrases humaines pour ne rÃ©cupÃ©rer que les informations les plus pertinentes pour calculer ce score de corrÃ©lation. AprÃ¨s application de ces diffÃ©rentes Ã©tapes, son exemple se transforme en [developp] [aim] [developp] [vim], ce qui permet alors Ã  ElasticSearch dâ€™Ã©liminer des documents non pertinents et qui seraient remontÃ©s si ces filtres nâ€™avaient pas Ã©tÃ© appliquÃ©s.\nPour plus dâ€™informations, Mathias a mis Ã  disposition les slides de son\ntalk.\n\nGuide pratique d&#39;une mÃ©thodologie UX pour la conception de features\n\nJessica Martel nous a prÃ©sentÃ© une mÃ©thodologie UX pour la conception de features quâ€™elle a mise en place et suivie lors dâ€™une expÃ©rience chez Decitre et maintenant chez Unow.\nElle nous a parlÃ© de lâ€™importance de la constitution dâ€™une Ã©quipe projet regroupant tous les acteurs (PO, devs, le pÃ´le Design et les Ã©quipes mÃ©tier). Diversifier les acteurs permet d&#39;accroÃ®tre lâ€™adhÃ©sion du projet, dâ€™apporter diffÃ©rentes cultures et de cibler le besoin.\n\nS&#39;ensuivent plusieurs Ã©tapes :\n\n\n  Product concept : Ã©valuation du besoin, de la criticitÃ©\n  UX Research et audit : bench global, entretiens\n  User journey : identification des diffÃ©rentes Ã©tapes (dÃ©terminÃ©es suite au bench et entretiens), mise en place du workflow\n  Specs fonctionnelles et design : cas dâ€™usages, rÃ¨gles mÃ©tier, versions, KPI / crÃ©ation de wireframes, maquettes UI et prototypes\n\n\nCette mÃ©thodologie comprend cependant des limites ! Elle prend beaucoup de temps et est soumise au contexte, aux priorisations dâ€™autres features, au downsizerâ€¦\n\nLe PromÃ©thÃ©e moderne : embarquer PHP dans Go\n\nKÃ©vin Dunglas nous a parlÃ© de comment embarquer PHP dans du Go. AprÃ¨s avoir listÃ© les diffÃ©rentes SAPI (Module Apache, FPM, â€¦) et nous avoir expliquÃ© briÃ¨vement le langage Go et sa librairie standard net/http, KÃ©vin nous a prÃ©sentÃ© FrankenPHP et toute la rÃ©flexion et les contraintes rencontrÃ©es pour le crÃ©er. Ce projet est un nouveau serveur Web en Go qui est capable dâ€™appeler lâ€™interprÃ©teur PHP et donc de faire tourner nos applications Web. Le projet est bien avancÃ©, mais pas terminÃ©. Nâ€™hÃ©sitez pas Ã  le contacter si vous voulez essayer avec vos applications, les retours lui seront utiles. En tout cas, chez Bedrock, on va suivre Ã§a de prÃ¨s !\n\nLes subtilitÃ©s du e-commerce Ã  la franÃ§aise\n\nAprÃ¨s nous avoir prÃ©sentÃ© les diffÃ©rentes taxes franÃ§aises et chez quelques-uns de nos voisins europÃ©ens, David Buros nous a rÃ©sentÃ© les diffÃ©rents problÃ¨mes quâ€™il a rencontrÃ©s avec Sylius:\n\n\n  lâ€™affichage du prix HT et du prix TTC\n  la gestion des rÃ©ductions avec ce double affichage\n  la gestion des Ã©cotaxes\n  le paiement par mandat administratif\n\n\nComment on est passÃ© de 1 800 emails Ã  70 000 par jour chez Trustt en 1 mois avec RabbitMQ\n\nCÃ©dric Driaux nous a expliquÃ© comment ils ont mis en place RabbitMQ chez Trustt pour gÃ©rer lâ€™envoi de plus de 70 000 mails par jour, afin de remplacer une ancienne solution qui lanÃ§ait un CRON toutes les 15 minutes dans le but de faire les calculs et la distribution. Il y avait Ã©galement des appels API Ã  des outils externes, causant des ralentissements. De plus, certains reliquats de mail n&#39;Ã©taient pas envoyÃ©s.\n\nPour rÃ©pondre Ã  ce problÃ¨me, lâ€™idÃ©e Ã©tait de mettre les mails dans une file ou queue en anglais, permettant une mise en attente de lâ€™envoi des messages, puis utiliser un consumer pour traiter les messages.\n\nCÃ©dric a dÃ©cidÃ© de choisir et de mettre en place (en Ã  peine un moisâ€¯!) RabbitMQ comme solution Ã  cette problÃ©matique. Cela a permis : une baisse de charge des serveurs, notamment due Ã  la suppression des CRON, une augmentation des mails envoyÃ©s, dont ces derniers peuvent dÃ©sormais tous Ãªtre traitÃ©s dans la journÃ©e. Les erreurs sont mises de cÃ´tÃ© pour Ãªtre traitÃ©es plus tard et ne pas ralentir le processus. Enfin, il est dorÃ©navant possible pour eux dâ€™ajouter dâ€™autres mails dans la queue dans la journÃ©e.\n\nGrÃ¢ce Ã  RabbitMQ, ils ont pu fortement augmenter leurs capacitÃ©s dâ€™envoi de mails tout en soulageant les serveurs.\n\nCecil, mon gÃ©nÃ©rateur de site statique\n\nArnaud Ligny nous a prÃ©sentÃ© son projet perso de gÃ©nÃ©rateurs de sites statiques en archive phar : Cecil. Il voulait une solution rapide Ã  prendre en main, intuitive et avec une sÃ©paration entre le contenu et la mise en forme. Ce side project avait pour but de remettre ses connaissances Ã  jour en appliquant les bonnes pratiques. Lâ€™application est automatisÃ©e, le paquet .phar est automatiquement gÃ©nÃ©rÃ© par GitHub Action lors de la crÃ©ation dâ€™une release, scrutinizer qui fait des corrections, des previews sont rÃ©alisables avec netlify.\n\nEt si on Ã©tendait SQL avec du PHP\n\nAntoine BLUCHET nous a prÃ©sentÃ© plus en dÃ©tail les projets Doctrine et principalement lâ€™ORM. Cet outil est extensible, mais a quelques limites. Comment pouvons-nous faire des requÃªtes complexes avec Doctrine ? Peut-on utiliser des Common Table Expression ? La rÃ©ponse Ã  ces questions, proposÃ©e, est ESQL. Cet outil permet de construire des requÃªtes SQL complexes facilement sans se soucier des noms des tables ou des colonnes, car il permet dâ€™utiliser ces mÃ©tadonnÃ©es depuis Doctrine.\n\nPourquoi vous nâ€™attirerez et ne retiendrez pas les femmes dans vos Ã©quipes tech.\n\nMarcy Charollois fait un constat sur le monde du travail dans le numÃ©rique, dominÃ© largement par les hommes et ne laissant que trop peu de place aux femmes. Marcy commence par introduire la notion dâ€™habitus, qui dÃ©signe un systÃ¨me de prÃ©fÃ©rences, de style de vie particulier Ã  chacun, qui influence les pratiques des individus au quotidien. Ces pratiques sont intÃ©riorisÃ©es inconsciemment, car lâ€™individu sâ€™adapte et s&#39;intÃ¨gre Ã  son environnement social. Cela crÃ©e un groupe majoritaire qui devient dÃ©cisionnaire. Se met alors en place un statu quo qui va soit inclure ou exclure et qui est fortement dominÃ© par la pensÃ©e masculine. Marcy nous dÃ©voile que sur 100 % de freins ressentis par les femmes dans la tech, 30% proviennent des biais d&#39;oppression de groupe, une part donc assez consÃ©quente.\n\nLe constat est rÃ©el, les femmes dans la tech vivent mal leur condition de femmes, il faut changer ce sentiment, mais les attitudes face au changement sont variÃ©es. 15% de personnes sont rÃ©fractaires, il sera donc difficile de faire Ã©voluer les choses avec eux, 15% sont dÃ©jÃ  partantes et 70% sont neutres, potentiellement pour ce changement, mais ne savent pas comment le faire.\n\nMarcy nous donne alors des clÃ©s qui permettront dâ€™attirer les femmes dans nos Ã©quipes en mettant en avant les freins ressentis par celles-ci : une expression du genre, une lÃ©gitimitÃ© face au mÃ©tier exercÃ©, des a priori sur la provenance des profils fÃ©minins qui sont souvent reconvertis et donc potentiellement juniors :\n\n\n  fÃ©miniser les postes et en particulier sur les offres dâ€™emploi, une femme est dÃ©veloppeuse, pas dÃ©veloppeur.\n  mettre en avant les tÃ©moignages de femmes qui montent dans votre entreprise pour donner des exemples concrets de ce quâ€™elles pourraient trouver en venant chez vous\n  sâ€™intÃ©resser rÃ©ellement Ã  elles et non pas de voir en vous ce quâ€™elles mettent en avant\n\n\nLa confÃ©rence continue sur les actions Ã  mener pour garder les femmes dans nos Ã©quipes :\n\n\n  parler dâ€™Ã©gal Ã  Ã©gal pour Ã©viter la posture sachant(e)/ignorante\n  soyez clair, transparent sur les salaires, les Ã©volutions de poste\n  mettez en place des moments conviviaux plus portÃ©s sur des prÃ©fÃ©rences fÃ©minines\n  minimiser les interruptions pendant les prises de parole\n  Ã©coutez des besoins spÃ©cifiques inhÃ©rents aux femmes et accommodez-les en offrant des ressources sans juger : parentalitÃ©, menstruation, assistance psychologique, adaptation Ã  lâ€™emploi du temps\n  encouragez les femmes Ã  prendre la parole, Ã  devenir un rÃ´le modÃ¨le parce que compÃ©tente\n\n\nMarcy termine sa confÃ©rence par la prÃ©sentation de quelques chiffres sur lâ€™Ã©volution de carriÃ¨re des femmes et des enjeux psychosociaux rÃ©sultant\nde cette Ã©volution et conclut en montrant les bienfaits de lâ€™inclusion des femmes au sein des entreprises et en nous donnant quelques noms de femmes cÃ©lÃ¨bres dans ce combat.\n\nConclusion\n\nEncore une fois, lâ€™AFUP a rÃ©ussi Ã  faire un Ã©vÃ©nement chaleureux, intÃ©ressant et diversifiÃ©\n\nNous sommes ravis dâ€™avoir pu participer Ã  cette manifestation qui nous a permis de rencontrer les membres de la communautÃ© ainsi que de visiter rapidement la ville de Lille et manger des Welsh.\n\n\n"
} ,
  
  {
    "title"    : "You Build it, you run it",
    "category" : "",
    "tags"     : " conference, docker, meetup, devops",
    "url"      : "/2022/05/13/docker-meetup-you-build-it-you-run-it.html",
    "date"     : "May 13, 2022",
    "excerpt"  : "Lâ€™une des grandes Ã©tapes de lâ€™autonomie dâ€™une Ã©quipe de dÃ©veloppement dans la mÃ©thodologie DevOps est de sâ€™intÃ©resser Ã  lâ€™alerting liÃ© Ã  son infrastructure. Comment sommes-nous arrivÃ© Ã  proposer aux Ã©quipes de dÃ©veloppement de sâ€™intÃ©resser et de m...",
  "content"  : "Lâ€™une des grandes Ã©tapes de lâ€™autonomie dâ€™une Ã©quipe de dÃ©veloppement dans la mÃ©thodologie DevOps est de sâ€™intÃ©resser Ã  lâ€™alerting liÃ© Ã  son infrastructure. Comment sommes-nous arrivÃ© Ã  proposer aux Ã©quipes de dÃ©veloppement de sâ€™intÃ©resser et de maÃ®triser cet alerting ?\n"
} ,
  
  {
    "title"    : "Retour sur l&#39;Android Makers 2022",
    "category" : "",
    "tags"     : " android, mobile, conference, makers",
    "url"      : "/2022/05/09/bedrock-android-makers-22.html",
    "date"     : "May 9, 2022",
    "excerpt"  : "Que câ€™est bon de se retrouver !\n\nAprÃ¨s deux ans sans confÃ©rence en prÃ©sentiel, lâ€™Android Makers a fait son grand retour les 25 et 26 avril 2022, pour le plus grand bonheur de la communautÃ© Android. \nLâ€™Ã©quipe de dÃ©veloppeurs Android de Bedrock (don...",
  "content"  : "Que câ€™est bon de se retrouver !\n\nAprÃ¨s deux ans sans confÃ©rence en prÃ©sentiel, lâ€™Android Makers a fait son grand retour les 25 et 26 avril 2022, pour le plus grand bonheur de la communautÃ© Android. \nLâ€™Ã©quipe de dÃ©veloppeurs Android de Bedrock (dont je fais partie) a partagÃ© ce bonheur en assistant Ã  ce rendez-vous incontournable. Jetpack Compose, accessibilitÃ©, optimisation de build et autres sont autant de sujets en maturation constante : essayons dâ€™en faire le tour ensemble.\n\n\n\n\n  Que câ€™est bon de se retrouver !    \n      Penser         \n          Design System et Jetpack Compose \n          AccessibilitÃ© \n          Modularisation \n          Support de Chrome OS \n        \n      \n      DÃ©velopper         \n          Splashscreen Android 12 \n          Tester les coroutines \n        \n      \n      Optimiser         \n          Toujours plus de CI \n          Optimisation du temps de build \n        \n      \n      Extras         \n          CrÃ©ation dâ€™un UI Toolkit avec Romain Guy et Chet Haase \n        \n      \n      En conclusion \n    \n  \n\n\nPenser \n\nDesign System et Jetpack Compose \n\nPlusieurs confÃ©rences ont Ã©voquÃ© le sujet du Design et plus particuliÃ¨rement lâ€™implÃ©mentation dâ€™un Design System (DS) avec Jetpack Compose. FranÃ§ois Blavoet nous a partagÃ© lâ€™expÃ©rience dâ€™Instacart Ã  ce sujet, en dÃ©voilant quelques dÃ©tails dâ€™implÃ©mentations de leurs API mises Ã  disposition des features engineers, afin de leur faciliter lâ€™intÃ©gration des Ã©lÃ©ments du DS.\nParallÃ¨lement, il nous a aussi invitÃ© Ã  rÃ©flechir sur la nÃ©cessitÃ© dâ€™intÃ©grer Material Design dans notre implÃ©mentation du DS. En effet, si le cadre Material peut parfois sâ€™avÃ©rer utile, il est quelques fois trop contraignant et pas toujours adaptÃ© aux besoins spÃ©cifiques de nos applications.\n\nAccessibilitÃ© \n\nVoilÃ  un sujet quâ€™il est important dâ€™Ã©voquer, tant il est facile dâ€™oublier dâ€™adresser une application Ã  tous. Cette Ã©dition de lâ€™Android Makers a eu la chance dâ€™accueillir une trÃ¨s belle confÃ©rence de Fanny Demey et Gerard Paligot sur le sujet de lâ€™accessibilitÃ©. Dans une sÃ©ance de Live Coding teintÃ©e dâ€™un jeu de rÃ´le sur le thÃ¨me de lâ€™Ã©mission Câ€™est pas sorcier !, nous avons pu faire le tour de plusieurs points dâ€™attention afin dâ€™inclure au mieux nos utilisateurs porteurs de handicaps :\n\n  ne pas donner dâ€™informations inutiles via TalkBack, comme les contentDescription des icÃ´nes dÃ©coratives\n  penser Ã  la maniÃ¨re dont TalkBack va assembler les informations provenant de plusieurs vues distinctes\n  donner un retour dâ€™action sur les clics de boutons et mieux placer ces actions lorsque le mode accessibilitÃ© est activÃ©\n  et bien dâ€™autres !\n\n\nLe Live Coding a pu Ã©galement dÃ©montrer Ã  quel point Jetpack Compose considÃ¨re lâ€™accessibilitÃ© comme une fonctionnalitÃ© cruciale grÃ¢ce Ã  des APIs trÃ¨s complÃ¨tes (je pense ici Ã  Modifier.sementics par exemple).\n\nModularisation \n\nJean-Baptiste Vincey, dÃ©veloppeur chez Deezer, a partagÃ© lâ€™expÃ©rience de son Ã©quipe concernant la modularisation de leur code pour gÃ©rer le nombre grandissant dâ€™applications dans leur catalogue. BasÃ© sur la crÃ©ation de bibliothÃ¨ques internes, plusieurs stratÃ©gies ont Ã©tÃ© explorÃ©es avec leurs bons et mauvais cÃ´tÃ©s. LancÃ© dans un chantier similaire, il est important pour Bedrock de voir comment dâ€™autres acteurs du milieu ont rÃ©pondu Ã  ces questions, sans oublier que chaque entreprise a sa propre rÃ©ponse qui doit sâ€™adapter Ã  ses process, son organisation et son produit.\n\nSupport de Chrome OS \n\nFrÃ©dÃ©ric Torcheux et Pierre Issartel, lors de leur confÃ©rence sur lâ€™adaptation Chrome OS des applications Android, ont fait un constat intÃ©ressant : le nombre de Chromebook vendu a explosÃ© rÃ©cemment pour dÃ©passer le nombre de Mac vendu. Sachant quâ€™un nombre grandissant de Chromebook a accÃ¨s au Play Store, il est de plus en plus important dâ€™adapter ses applications pour cet usage.\nEn vrac : exploiter le potentiel du curseur de la souris, naviguer dans lâ€™application sans jamais quitter le clavier, supporter lâ€™environnement multi-fenÃªtrÃ© et le redimensionnement de celles-ci, autant de points dâ€™amÃ©liorations comportant piÃ¨ges Ã  Ã©viter et bonnes pratiques.\n\nDÃ©velopper \n\nSplashscreen Android 12 \n\nDeux dÃ©veloppeurs de chez Google nous ont plongÃ© dans les entrailles du WindowManager dâ€™Android, ce composant qui est chargÃ© dâ€™orchestrer les applications que nous utilisons tous les jours : charger une application, la placer Ã  lâ€™Ã©cran puis la dessiner, la dÃ©placer et gÃ©rer son cycle de vie, autant de responsabilitÃ©s pour un WindowManager complexe Ã  maitriser.\nÃ€ travers cette confÃ©rence pointue, Vadim Caen et Pablo Gamito ont rebondi sur le nouveau systÃ¨me de SplashScreen dâ€™Android 12 pour nous expliquer quel problÃ¨me il doit rÃ©soudre (essentiellement le ressenti de lenteur au lancement dâ€™une application) et comment en tirer parti. Ã€ ce titre, la documentation de Google sur la migration vers le SplashScreen dâ€™Android 12 est incontournable.\n\nLe nouveau Splashscreen pour Android 12 comporte son lot de challenges, notamment pour tenir compte des animations. Chez Bedrock, les reflexions Ã  ce sujet ont dÃ©marrÃ©, et nous comptons partager un retour dâ€™expÃ©rience sur notre propre migration !\n\nTester les coroutines \n\nMÃ¡rton Braun, aussi dÃ©veloppeur chez Google, a prÃ©sentÃ© les nouveautÃ©s de la bibliothÃ¨que kotlinx-coroutines-test en version 1.6+. Exit runBlockingTest, place au runTest qui permet, grÃ¢ce Ã  son TestCoroutineScheduler, de gÃ©rer les dÃ©lais et lâ€™ordre dâ€™execution de toutes les coroutines lancÃ©es dans un test.\nLâ€™ancienne version des API de test Ã©tant maintenant dÃ©prÃ©ciÃ©e, cette nouvelle version est encore experimentale mais vouÃ©e Ã  passer en Ã©tat stable, tant elle parait plus mature que la prÃ©cÃ©dente.\nLes Flow et StateFlow nâ€™ont pas Ã©tÃ© oubliÃ©s puisquâ€™ils ont aussi leurs spÃ©cifitÃ©s en matiÃ¨re de tests.\n\nOptimiser \n\nToujours plus de CI \n\nChez Bedrock, la CI tient une place particuliÃ¨re dans nos process de release, et il est toujours intÃ©ressant de voir comment dâ€™autres entreprises se saisissent de cet outil et amÃ©liorent leur process.\nAprÃ¨s les rappels toujours pertinent sur lâ€™importance de dÃ©lÃ©guer le maximum de tÃ¢ches rÃ©pÃ©titives Ã  nos environnement de CI, Xavier F. Gouchet, dÃ©veloppeur chez Datadog, a prÃ©sentÃ© divers outils pour y parvenir.\n\nDetekt, un plugin Gradle permet dâ€™aller encore plus loin quâ€™Android Lint en offrant lâ€™analyse statique de nâ€™importe quel code Kotlin. Son extensibilitÃ© nous est exposÃ©e via une API sur le pattern visiteur, redoutablement efficace pour parcourir le PSI (Program Structure Interface) de Kotlin. Dâ€™autres outils peuvent Ã©galement Ãªtre efficaces pour parcourir cette interface.\n\nXavier Gouchet prÃ©sente Ã©galement KSP (Kotlin Symbol Processor), le projet sponsorisÃ© par Google vouÃ© Ã  remplacer KAPT, son ancÃªtre basÃ© sur Java. CombinÃ© avec Kotlin Poet, cet outil permet dâ€™automatiser la gÃ©nÃ©ration de code Kotlin Ã  partir dâ€™un code source annotÃ© dans le projet.\n\nPour aider au dÃ©veloppement autour du PSI Kotlin, Xavier Gouchet recommande lâ€™excellent plugin PSIViewer pour IDE Jetbrains.\n\nOptimisation du temps de build \n\nZac Sweers est venu nous prÃ©senter la maniÃ¨re dont Slack, entreprise pour laquelle il travaille, optimise les builds Gradle. Les projets se complexifiant avec toujours plus de code et de modules, les temps de build ont tendance a augmenter.\n\nCette confÃ©rence a mis en lumiÃ¨re diverses optimisations pour tirer pleinement parti de Gradle et de ses nouvelles fonctionnalitÃ©s :\n\n  dÃ©sactiver les fonctionnalitÃ©s non utilisÃ©es du plugin Android\n  profiter du cache, y compris sur serveur\n  Ã©viter dâ€™utiliser buildSrc pour factoriser du code\n  Ã©crire son propre plugin de convention gradle\n  avoir un compte Gradle Entreprises pour profiter des Gradle Build Scans afin de dÃ©terminer quels sont les points de friction du projet, que ce soit pour lâ€™utilisation du cache, la parallÃ©lisation, lâ€™invalidation des builds, lâ€™optimisation des arguments de la JVMâ€¦\n  parfois mÃªme, acheter du nouveau matÃ©riel ! (Apple Silicon)\n\n\nRÃ©duire le temps de build est un enjeu constant, et participe au confort du dÃ©veloppeur au quotidien.\n\nExtras \n\nCrÃ©ation dâ€™un UI Toolkit avec Romain Guy et Chet Haase \n\nUne confÃ©rence trÃ¨s intÃ©ressante a vu Romain Guy et Chet Haase nous prÃ©senter un projet experimental dâ€™UI Toolkit maison, Apex, trÃ¨s proche de Jetpack Compose dans son API. \nCet exercice original a Ã©tÃ© un moyen de faire valoir le concept dâ€™Entity component system, un pattern se basant sur la composition pour enrichir les comportements des entitÃ©s dâ€™un systÃ¨me.\n\nLeur prÃ©sentation a mis en lumiÃ¨re la philosophie dâ€™un UI Toolkit mais a aussi et surtout soulignÃ© la quantitÃ© de travail Ã  accomplir pour passer dâ€™un projet experimental Ã  un toolkit utilisable en production. Enrichir sa boite Ã  outils avec le maximum de widgets diffÃ©rents, permettre une personnalisation maximale aux dÃ©veloppeurs, rendre le moteur de rendu multi-plateforme, autant de tÃ¢ches nÃ©cessaires pour rendre votre Toolkit vraiment utile pour la communautÃ© aujourdâ€™hui.\n\nEn conclusion \n\n\n\nCette Ã©dition de lâ€™Android Makers 2022 sâ€™est conclue sur une confÃ©rence humoristique inÃ©dite de la part de Chet Haase et Romain Guy. Il a Ã©tÃ© question de tourner en dÃ©rision la communautÃ© des dÃ©veloppeurs sur le nombre de nouveaux patterns de dÃ©veloppement qui sortent rÃ©guliÃ¨rement et les discussions acharnÃ©es (et parfois virulentes) autour de ces derniers.\nCela a Ã©tÃ© une trÃ¨s bonne maniÃ¨re de prendre du recul sur notre communautÃ© Android, et de se fÃ©liciter, tout de mÃªme, de la recherche constante dâ€™amÃ©lioration des pratiques de dÃ©veloppement au service de la qualitÃ© de nos produits et de leur accessibilitÃ©.\n"
} ,
  
  {
    "title"    : "How did we streamline the delivery of our internal conferences aka LFTs?",
    "category" : "",
    "tags"     : " lft, talks, live, stream, obs, streamyard, conference",
    "url"      : "/2022/04/29/how-we-simplify-our-lft-broadcast.html",
    "date"     : "April 29, 2022",
    "excerpt"  : "\n\nSome time ago, we shared with you an article explaining how we managed to capture and broadcast our conferences in the Bedrock auditorium. \nWe must admit, it worked great, but we wanted to make it simpler.\n\nAs a reminder, our internal conference...",
  "content"  : "\n\nSome time ago, we shared with you an article explaining how we managed to capture and broadcast our conferences in the Bedrock auditorium. \nWe must admit, it worked great, but we wanted to make it simpler.\n\nAs a reminder, our internal conferences or meetups are called LFT. \nIf you want to know more about our Last Friday Talk and the motivations that push us to do it, we invite you to read the above-mentioned article.\n\nAfter the success of this broadcasting, the (voluntary) organization team started thinking about how to make it simpler. \nIt was already a challenge in itself!\nIf you read this part of the article, you can see that we already had some ideas for improvements.\n\nImprovement areas\n\nIn the previous version, you could see that a very important quantity of material was necessary (meters of various cables, multi outlets, a camera, etcâ€¦)\nA large part of this material was kindly lent by Pascal (and we thank him for that), but we could not decently borrow it for each LFT.\n\nMoreover, this very specific equipment did not allow each Bedrock employee to manage the control room, quickly and with his or her own machine.\nFinally, as you can imagine, setting up and putting away such a large amount of equipment takes a lot of time. \nThere were no less than four of us setting up and tidying up.\n\nIn another topic, letâ€™s talk about video quality. We used to use Google Live Stream before, but the 720p broadcast, with its very low bitrate and aggressive compression, sometimes made it difficult to follow.\nText and images were often very pixelated.\n\nAlso, we wanted to, easily, handle a hybrid mode to our LFTs.\nBecause of the pandemic, telecommuting and the fact that some of Bedrockâ€™s employees are located in Paris, we need to broadcast and capture the LFT both in person and remotely.\nBy hybrid mode we also meant that during the same event, we need to host speakers from the location of their choice.\n\nWhat we did?\n\nAfter a few exchanges with other conference organizations, we decided to give Streamyard a chance.\nAfter some quick tests, we bought a license and started the new version of the LFT.\n\nWhat is Streamyard?\n\nStreamyard is a live streaming platform that runs directly in the browser.\n\nIt does not have all the customization and possibilities of OBS, but its huge advantage is its versatility.\nThis solution allows Bedrock employees to manage the LFT from any computer.\n\nHere are some sample images and overviews of the Streamyard UI:\n\n\n\n\n\nThe positive points of Streamyard:\n\n\n  Allows us to have several people in the control room at the same time\n  Streams in 1080p\n  Personalization: manage banners, chat questions, music on hold,â€¦\n  Handles multi-speakers management\n  Re-stream to Youtube / Facebook / Twitter / Linkedin / Video recording\n  A free trial mode allows one to test it before taking out their credit card.\n\n\nNew setup\n\nAs a reminder, here is the organization of our auditorium during the live broadcast of our LFT.\nThe room is large enough to accommodate all Bedrockers who wish to attend in person the presentations of their colleagues. \nThe remote speakers have their conference broadcasted on the two screens of the room.\n\n\n\n\n\n\n\nWhat has changed since we switched to Streamyard is mainly related to the way we capture the image and sound of what is displayed on the screen.\nPreviously, we were using an Elgato HD60S+ device, to capture the HDMI output from the speaker broadcast. \nHowever, we had to try several times to get it to work each time we switched speakers. Not fun at all.\n\nNow, with Streamyard, every time we switch speaker, we just need to:\n\n  share the link of the â€œStreamyard broadcastâ€\n  The speaker joins the stream (and switches off her microphone and webcam)\n  Connect the HDMI cable of the speakerâ€™s computer\n  The speaker displays her screen via the roomâ€™s projectors\n  The control room operator puts the RF microphone on the speaker(s)\n\n\nFor a remote speaker, itâ€™s even easier.\nJust pass them the Streamyard link and they can use their webcam and their own microphone.\n\nThis is what our Stream setup now looks like with the cabling:\n\n\n\n\n\nWhat did we achieve?\n\nLFT is also a group of volunteers who give their energy to offer the best event possible, to offer to all Bedrock members a space where they can share their passion, technical subjects and others.\nFrom the proposal of their subject, through rehearsals and during the broadcast, the team is there to help speakers â€“ either beginners or confirmed.\n\nThe switch to 720p and then to 1080p has been a real positive point for the quality of the live show, but also for the recording and the replay.\nMore than 200 participants during the last LFT on the day, the organizing team is delighted.\n\nThe simplification of the broadcasting setup since the switch to Streamyard has also made it easier to set up the room.\nSwitching from one speaker to another is less complex and can be done in just a few minutes.\n\nThe youtube lives allow participants to pause and rewind the broadcast.\nThis is really convenient for them.\nAlso, by going through the youtube chat, we can share questions directly on the screen and on the replay.\n\nThe LFT replays are also available on Bedrockâ€™s Youtube channel in a private way accessible to all employees.\n\n\n\nNext steps\n\nWe donâ€™t want to stop there.\nFor the next editions, we will try to do even better.\nWe are working on the matter of sharing some talks in public on our Youtube channel, so more people in our communities can learn from them.\nIn order to simplify the setup, we will try to put in place a more fixed table to avoid wiring and moving furniture.\nWe also wish to propose and train our employees to the use of this setup in order to allow us to host meetups and conferences in the best conditions.\n\nNow, itâ€™s your turn: if your company or user-group does this kind of talks, how do you manage broadcasting and recording?\n"
} ,
  
  {
    "title"    : "âš¡ï¸ Vite âš¡ï¸ the Webpack killer",
    "category" : "",
    "tags"     : " conference, js, webpack, vite, devoxx",
    "url"      : "/2022/04/21/vite-the-webpack-killer.html",
    "date"     : "April 21, 2022",
    "excerpt"  : "Toute application web a besoin dâ€™Ãªtre packagÃ©e afin dâ€™Ãªtre livrÃ©e en production. Pour rÃ©pondre Ã  cette problÃ©matique, de nombreux outils, connus sous le nom de modules bundler, sont apparus, et ces derniÃ¨res annÃ©es, câ€™est Webpack qui semble sâ€™Ãªtre...",
  "content"  : "Toute application web a besoin dâ€™Ãªtre packagÃ©e afin dâ€™Ãªtre livrÃ©e en production. Pour rÃ©pondre Ã  cette problÃ©matique, de nombreux outils, connus sous le nom de modules bundler, sont apparus, et ces derniÃ¨res annÃ©es, câ€™est Webpack qui semble sâ€™Ãªtre imposÃ© comme lâ€™outil incontournable.\n\nOn ne va pas se le cacher, si vous avez mis les mains dans une configuration webpack, câ€™est loin dâ€™Ãªtre un outil simple ni rapide.\n\nÃ‡a nâ€™a pas Ã©chappÃ© Ã  Evan You, le crÃ©ateur de Vue.JS, qui voulait rÃ©pondre Ã  ces problÃ©matiques avec une nouvelle faÃ§on de procÃ©der, avec des idÃ©es novatrices, reposant sur les derniÃ¨res fonctionnalitÃ©s des navigateurs : Vite.\n\nQuelles sont ces idÃ©es novatrices Ã  la base de Vite ? En quoi concurrence-t-il Webpack ? Câ€™est ce que nous allons voir dans ce talk et live coding!\n"
} ,
  
  {
    "title"    : "Bedrock Ã  l&#39;AWS Summit 2022",
    "category" : "",
    "tags"     : " aws, summit, cloud, sysadmin, conference, kubernetes",
    "url"      : "/2022/04/20/aws-summit-2022-notre-retour-dexperience.html",
    "date"     : "April 20, 2022",
    "excerpt"  : "\n\nRetour Ã  lâ€™AWS Summit \n\nDeux annÃ©es se sont Ã©coulÃ©es depuis le dernier AWS Summit Ã  Paris, il a fait son retour ce 12 avril !\nCet Ã©vÃ¨nement, qui a lieu au printemps dans plusieurs pays, est lâ€™occasion de rencontrer la communautÃ© AWS franÃ§aise, d...",
  "content"  : "\n\nRetour Ã  lâ€™AWS Summit \n\nDeux annÃ©es se sont Ã©coulÃ©es depuis le dernier AWS Summit Ã  Paris, il a fait son retour ce 12 avril !\nCet Ã©vÃ¨nement, qui a lieu au printemps dans plusieurs pays, est lâ€™occasion de rencontrer la communautÃ© AWS franÃ§aise, dâ€™assister Ã  de nombreuses confÃ©rences et de bÃ©nÃ©ficier de retours dâ€™expÃ©rience dâ€™autres clients.\nCâ€™Ã©tait aussi pour nous, comme en 2019, lâ€™occasion de partager les nÃ´tres !\n\nDepuis notre migration vers le Cloud, AWS et Kubernetes entre 2018 et 2021 (plus dâ€™informations dans Le Plan Copenhague), nous sommes plusieurs centaines Ã  travailler au quotidien avec AWS.\nCette annÃ©e, cinq de nos DevOps, SysOps et DÃ©veloppeurs ont eu la chance de se rendre Ã  lâ€™AWS Summit.\n\nNous partageons quelques notes que nous avons prises lors de cette journÃ©e, sur des sujets qui nous ont marquÃ©s et qui vont sans doute nous occuper une partie de lâ€™annÃ©e Ã  venir.\n\nSommaire\n\n\n  Retour Ã  lâ€™AWS Summit\n    \n      Rendez vos Ã©quipes de Data Science 10x plus productives avec SageMaker\n      Comment le cloud permet Ã  France TÃ©lÃ©visions dâ€™innover dans la diffusion de contenu live\n      DÃ©couvrez comment Treezor utilise AWS comme moteur de sa plateforme de Banking-as-a-service\n      Tracer votre chemin vers le Modern DevOps en utilisant les services AWS dâ€™apprentissage machine\n      Innover plus rapidement en choisissant le bon service de stockage dans le cloud\n      SÃ©curiser vos donnÃ©es et optimiser leurs coÃ»ts de stockage avec Amazon S3\n      Minimiser vos efforts pour dÃ©ployer et administrer vos cluster Kubernetes\n      Serverless et Ã©vÃ¨nement, les nouvelles architectures\n    \n  \n  Nous Ã©tions aussi intervenants\n    \n      Transformer le load balancing pour optimiser le cache : objectif 50 millions dâ€™utilisateurs\n      Etes-vous bien architecturÃ© ?\n      PrÃ©parez et donnez votre premier talk\n    \n  \n  Conclusion de lâ€™article\n\n\nRendez vos Ã©quipes de Data Science 10x plus productives avec SageMaker \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  Olivier Sutter - AWS Solution Architect\n  Yoann Grondin - IA Team Leader Canal+\n\n\n\n\nCette confÃ©rence prÃ©sentait le produit Amazon SageMaker, dâ€™abord dans sa globalitÃ©, puis appliquÃ© au cas des Ã©quipes de Data Scientist chez Canal+.\n\nSageMaker a Ã©tÃ© lancÃ© fin 2017 pour crÃ©er, entraÃ®ner et dÃ©ployer des modÃ¨les de machine learning. Câ€™est une solution tout-en-un, avec une interface graphique intuitive et un accent portÃ© sur lâ€™automatisation. Amazon promet Ã©galement de gagner en performance sur SageMaker via lâ€™implÃ©mentation de nombreux algorithmes dâ€™apprentissage supervisÃ©s ou non-supervisÃ©s (XGBoost, kNN, PCAâ€¦).\n\nDe maniÃ¨re gÃ©nÃ©rale, les Ã©quipes de Canal+ utilisent des solutions dâ€™apprentissage pour diffÃ©rents cas dâ€™usages :\n\n\n  Personnaliser lâ€™expÃ©rience utilisateur et proposer du contenu ciblÃ©\n  Mieux connaÃ®tre, labelliser, classifier leurs contenus vidÃ©o\n  Anticiper les besoins des abonnÃ©s ou des prospects\n\n\nIls se sont tournÃ©s vers SageMaker pour diminuer le temps passÃ© dans les Ã©tapes de preprocessing, data cleaning et dÃ©ploiement en production.\n\nChez Bedrock, nous avons aussi rencontrÃ© ces problÃ©matiques, nous avons rÃ©alisÃ© des PoC de diffÃ©rentes solutions (dont SageMaker) et nous avons retenu la plateforme Databricks.\n\nEn effet, Databricks rÃ©pond Ã  nos besoins de fine-tuning des paramÃ¨tres des clusters Spark et dâ€™intÃ©gration avec Terraform (ce qui est important pour nous car nous utilisons exclusivement de lâ€™Infra-as-Code). Nous avons Ã©galement automatisÃ© le dÃ©ploiement en production de nos modÃ¨les dâ€™apprentissage, de la mÃªme maniÃ¨re que SageMaker.\n\nCette confÃ©rence nous a confortÃ© dans lâ€™approche et lâ€™utilisation que nous avons de nos outils actuels, tout en nous confrontant Ã  dâ€™autres solutions techniques et dâ€™autres cas dâ€™usages au sein de notre industrie.\n\nRÃ©sumÃ© par Gabriel FORIEN - DevOps\n\nComment le cloud permet Ã  France TÃ©lÃ©visions dâ€™innover dans la diffusion de contenu live \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  RaphaÃ«l Goldwaser - AWS Solution Architect\n  Guillaume Postaire - Directeur de la Media Factory, France TÃ©lÃ©visions\n  Matthieu Parmentier - Responsable de lâ€™Al factory, France TÃ©lÃ©visions\n  Nicolas Pierre - Al factory Lead Tech, France TÃ©lÃ©visions\n\n\n\n\nNous avons assistÃ© Ã  une confÃ©rence prÃ©sentÃ©e par les responsables MÃ©dia et AI de France TÃ©lÃ©visions et RaphaÃ«l Goldwaser Solutions Architect chez AWS. Ils nous ont parlÃ© de lâ€™Ã©volution de leur usage du cloud dans la diffusion de vidÃ©o en direct et des diffÃ©rentes Ã©tapes de la construction dâ€™un systÃ¨me de sous-titrage automatique en direct.\n\nPour ce systÃ¨me de sous-titrage, ils utilisent les services Media &amp;amp; Entertainment fournis par AWS.\n\nLes flux vidÃ©o sont envoyÃ©s directement dans le cloud via Elemental MediaConnect pour gÃ©nÃ©rer des sous-titres automatiquement en utilisant Media-Cloud AI et Speechmatics. Une fois les fichiers de sous-titres gÃ©nÃ©rÃ©s, ils sont insÃ©rÃ©s et synchronisÃ©s sur le flux en direct.\n\nCes outils peuvent Ã©galement Ãªtre utilisÃ©s pour analyser des vidÃ©os afin de contextualiser les publicitÃ©s affichÃ©es et/ou choisir le meilleur moment pour les afficher.\n\nChez Bedrock comme chez France TÃ©lÃ©visions, nous challengeons rÃ©guliÃ¨rement les solutions MÃ©dias proposÃ©es par AWS pour amÃ©liorer nos infrastructures et apporter de nouvelles fonctionnalitÃ©s Ã  nos produits.\n\nRÃ©sumÃ© par Christian VAN DER ZWAARD - SysOps\n\nDÃ©couvrez comment Treezor utilise AWS comme moteur de sa plateforme de Banking-as-a-service \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  Armel Negret - AWS Central Sales Representative\n  Nicolas Bordes - Technical Lead and AWS Sponsor, SociÃ©tÃ© GÃ©nÃ©rale\n\n\nTreezor, une filiale du groupe SociÃ©tÃ© GÃ©nÃ©rale, fournit une plateforme complÃ¨tement APIsÃ©e qui permet aux fintechs et plus gÃ©nÃ©ralement aux acteurs de la finance dâ€™accÃ©der Ã  leurs services bancaires. Cette plateforme est hÃ©bergÃ©e sur AWS et utilise une stack de services entiÃ¨rement serverless : API Gateway, CloudWatch, Lambda, SNS et SQS entre autres. Les Lambdas sont dÃ©veloppÃ©es en PHP grÃ¢ce au framework Bref.\n\nA lâ€™instar de Treezor, Bedrock possÃ¨de Ã©galement de nombreuses Lambda dÃ©veloppÃ©es en PHP avec Bref. Ces Lambdas sont majoritairement dÃ©ployÃ©es via le framework serverless et maintenues par le pÃ´le backend. Au pÃ´le infrastructure, nous essayons dâ€™utiliser dâ€™autres langages comme Python ou Go avec lesquels nous sommes plus Ã  lâ€™aise et qui sont nativement supportÃ©s par Lambda.\n\nLâ€™approche â€œfull serverlessâ€ est intÃ©ressante car elle permet de sâ€™abstraire de la gestion de lâ€™infrastructure sous-jacente et donc de se concentrer sur des problÃ©matiques intrinsÃ¨ques au mÃ©tier.\nEn sus, les services AWS serverless apportent souvent nativement de la haute disponibilitÃ© ainsi que de lâ€™auto-scaling, deux problÃ©matiques trÃ¨s importantes pour garantir un service de qualitÃ© Ã  nos utilisateurs finaux. Câ€™est pour ces raisons que Bedrock utilise de nombreux services AWS serverless : Athena, CloudWatch, DynamoDB, Lambda, S3, SNS, SQS, Kinesis, â€¦\nLe pÃ´le infrastructure de Bedrock Ã©tant relativement â€œpetitâ€ par rapport au nombre total de dÃ©veloppeurs (23 devops/sysops pour 250 fullstack en date du 15 avril 2022), lâ€™utilisation du serverless est un rÃ©el enjeu business.\nServerless ne rÃ©pond pas Ã  tous les besoins non plus, particuliÃ¨rement sur de trÃ¨s forts pics de charge oÃ¹ nous prÃ©fÃ©rons utiliser Kubernetes.\n\nRÃ©sumÃ© par TimothÃ©e AUFORT - DevOps\n\nTracer votre chemin vers le Modern DevOps en utilisant les services AWS dâ€™apprentissage machine \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  Patrick LamplÃ© - AWS Principal Specialist SA\n\n\nLa confÃ©rence nous proposait dâ€™en apprendre un peu plus sur les nouveaux produits AWS Code Guru et DevOps Guru.\n\nCode Guru\nse dÃ©coupe en deux parties :\n\n\n  Reviewer, qui a pour ambition dâ€™accÃ©lÃ©rer la revue de code ;\n  Profiler, qui peut aider Ã  optimiser les performances dâ€™une application.\nA ce jour, ces services ne supportent que les langages Python et Java.\n\n\nDevOps Guru\npermet dâ€™identifier les comportements anormaux des applications au runtime.\nPar exemple, si une application utilise une table DynamoDB qui nâ€™est pas suffisamment provisionnÃ©e, une alerte va Ãªtre dÃ©clenchÃ©e. Cette derniÃ¨re pourrait permettre dâ€™identifier un souci de configuration avant mÃªme que lâ€™application ne soit dÃ©ployÃ©e en production.\n\nChez Bedrock, les langages utilisÃ©s Ã©tant principalement Javascript, PHP et Python, Code Guru ne sera pas une solution adÃ©quate dans toutes les situations. Nous avons donc mis en place la solution KICS qui nous permet, Ã  lâ€™aide de rÃ¨gles Open Policy Agent, dâ€™effectuer automatiquement de nombreuses validations sur le code infrastructure (Terraform, Docker, YAML, â€¦). KICS est utilisÃ© au travers de GitHub Actions pour ajouter des commentaires sur les pull requests comme Code Guru est capable de le faire.\n\nUne analyse au runtime effectuÃ©e par DevOps Guru pourrait permettre de venir complÃ©ter la liste de services AWS que nous utilisons dÃ©jÃ  et qui vÃ©rifient la configuration de notre infrastructure comme : Config, Trusted Advisor, CloudWatch,  â€¦\n\nLes outils du Modern DevOps dâ€™AWS pourraient venir en complÃ©ment dâ€™outils de qualitÃ© actuellement utilisÃ©s chez nous. Ã€ tester en complÃ©ment de KICS pendant une de nos journÃ©es R&amp;amp;D (journÃ©es organisÃ©es le dernier vendredi du mois, un mois sur deux).\n\nRÃ©sumÃ© par Valentin CHABRIER &amp;amp; MickaÃ«l VILLERS - DevOps\n\nInnover plus rapidement en choisissant le bon service de stockage dans le cloud \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  Thomas Barandon - AWS Enterprise Support Manager\n  Laurent Dirson - Directeur des Solutions Business et des Technologies, Nexity\n\n\nThomas Barandon a rappelÃ© les solutions de stockage dâ€™AWS : S3 pour du stockage objet, EBS pour le stockage bloc et EFS/FSx pour le stockage fichier.\nIl a par la suite prÃ©sentÃ© Storage Gateway, qui permet dâ€™utiliser les services de stockage AWS dans une infrastructure on-prem via un montage NFS/Samba ou iSCSI.\n\nLaurent Dirson de chez Nexity a ensuite partagÃ© la stratÃ©gie adoptÃ©e pour concevoir leur SI comme un service. Tous leurs documents sont dÃ©sormais stockÃ©s dans un bucket S3 mis Ã  disposition des agences via un montage NFS opÃ©rÃ© par lâ€™outil Storage Gateway. Une politique dâ€™Object Lock permet dâ€™utiliser le modÃ¨le WORM (write-once-read-many).\n\nChez Bedrock, nous stockons dÃ©jÃ  la grande majoritÃ© de nos donnÃ©es dans des buckets S3. Nous aimerions aussi bÃ©nÃ©ficier des avantages de ce service pour le stockage de nos mÃ©triques. Mais, comme nous utilisons VictoriaMetrics, ce mode de stockage nâ€™est pas disponible et ces donnÃ©es sont stockÃ©es dans EBS. Peut-Ãªtre que Storage Gateway nous permettrait dâ€™Ã©crire nos mÃ©triques directement sur un bucket S3 ?\n\nRÃ©sumÃ© par Coraline PETIT - SysOps\n\nSÃ©curiser vos donnÃ©es et optimiser leurs coÃ»ts de stockage avec Amazon S3 \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  Meriem Belhadj - AWS Storage Specialist Solutions Architect\n\n\nPendant cette prÃ©sentation, Meriem Belhadj est revenue sur les classes de stockage disponibles sur S3, en mettant une attention particuliÃ¨re Ã  Glacier Instant Retrieval et Ã  Intelligent-Tiering. Le second permet dâ€™appliquer une politique de stockage basÃ©e sur la frÃ©quence dâ€™accÃ¨s aux donnÃ©es au cours des 30 derniers jours.\nEn effet, pour dÃ©terminer la â€œbonneâ€ classe Ã  utiliser, il faut notamment connaÃ®tre la disponibilitÃ© des donnÃ©es. Les autres points Ã  prendre en compte sont la frÃ©quence dâ€™accÃ¨s, les performances recherchÃ©es, la taille des objets Ã  stocker et enfin la durÃ©e de rÃ©tention.\n\nNous appliquons ces pratiques chez Bedrock depuis plusieurs annÃ©es. Toutefois, il serait judicieux de mettre en place des rÃ¨gles, type AWS Config, pour sâ€™assurer que ces recommandations soient bien appliquÃ©es sur tous nos buckets S3.\n\nRÃ©sumÃ© par Coraline PETIT - SysOps\n\nMinimiser vos efforts pour dÃ©ployer et administrer vos cluster Kubernetes \n\nConfÃ©rence prÃ©sentÃ©e par :\n\n\n  Abass Safouatou - AWS Lead Solution Architect\n  SÃ©bastien Allamand - AWS Solution Architect Specialist Container\n  Patrick Chatain - CTO Contentsquare\n\n\nContentsquare, analyste de lâ€™expÃ©rience numÃ©rique, est venu nous parler de son utilisation dâ€™EKS Blueprint avec AWS CDK (Cloud Development Kit) pour la configuration et le dÃ©ploiement de leurs infrastructures Kubernetes. Cet outil leur a permis de rapidement migrer leurs infrastructures dans le Cloud.\n\nÃ€ lâ€™occasion de cette confÃ©rence, un dÃ©but de comparatif a Ã©tÃ© amorcÃ© entre les solutions de passage Ã  lâ€™Ã©chelle automatique : Cluster Autoscaler et Karpenter.\n\nCette analyse a particuliÃ¨rement attirÃ© notre attention : nous souhaitons migrer nos clusters Kubernetes, actuellement dÃ©ployÃ© par Kops vers des clusters EKS, pour gagner en maintenabilitÃ© et en rapiditÃ© de scaling. Karpenter est lâ€™une des solutions que nous Ã©tudions dans le cadre de ce projet, afin de tirer partie de cet outil qui a Ã©tÃ© dÃ©veloppÃ© par AWS et qui semble mieux tirer profit des fonctionnalitÃ©s spÃ©cifiques dâ€™AWS que Cluster Autoscaler.\n\nContentsquare a mentionnÃ© son besoin de dÃ©velopper un outil de passage Ã  lâ€™Ã©chelle basÃ© non pas sur la consommation CPU et mÃ©moire mais sur des mÃ©triques custom. Câ€™est un besoin que nous avons Ã©galement chez Bedrock, nous avons donc dÃ©veloppÃ© un outil pour y rÃ©pondre, vous trouverez plus de dÃ©tails dans lâ€™article de blog dÃ©diÃ© Ã  cet outil. Cette remarque nous a confortÃ© dans notre volontÃ© de continuer Ã  open-sourcer les outils que nous dÃ©veloppons, pour quâ€™ils bÃ©nÃ©ficient Ã  la communautÃ©.\n\nRÃ©sumÃ© par Coraline PETIT &amp;amp; Christian VAN DER ZWAARD - SysOps\n\nServerless et Ã©vÃ¨nement, les nouvelles architectures \n\nMainframe, monolithe, systÃ¨me distribuÃ©, microservices, la conception dâ€™un SI ou dâ€™un projet est en constante Ã©volution.\n\nCeci dit, depuis plusieurs annÃ©es, beaucoup dâ€™entreprises font la transition sur des architectures orientÃ©es Ã©vÃ¨nements pour limiter les couplages forts entre les microservices.\nÃ€ cela sâ€™ajoute lâ€™essor du tout Serverless : fini le temps oÃ¹ on gÃ©rait nous-mÃªmes le dimensionnement de nos serveurs.\n\nCette annÃ©e, plusieurs confÃ©rences Ã©taient consacrÃ©es Ã  ces sujets.\n\n3 designs patterns pour bien dÃ©marrer avec Serverless\n\nMatthieu Napoli, Hero AWS Serverless et crÃ©ateur de la librairie Bref, a prÃ©sentÃ© trois designs patterns pour bien dÃ©marrer avec Serverless.\n\nApplication HTTP\n\nIl est maintenant trÃ¨s simple et peu coÃ»teux de crÃ©er des applications HTTP en Serverless, en combinant diffÃ©rents services AWS :\n\n\n  Lambda function et Lambda function URL (nouvelle fonctionnalitÃ© sortie une semaine avant le Summit) ;\n  CloudFront CDN ;\n  API Gateway ;\n  S3.\n\n\nUn exemple concret :\n\n\n  CloudFront CDN dÃ©livre les assets JS/CSS/image depuis S3 ;\n  il transmet Ã©galement les retours dâ€™API Gateway ;\n  et API Gateway communique avec la/les Lambdas.\n\n\nAjoutons une base de donnÃ©es DynamoDB ou Aurora et nous voilÃ  avec une application full Serverless.\n\nFile de messages avec worker\n\nLorsquâ€™on met en place ce type de pattern, nous dÃ©ployons :\n\n\n  un projet qui pousse un message dans une file â€œproducerâ€ ;\n  une file de messages (SQS/SNS/MQ) ;\n  un second projet qui lit les messages depuis la file â€œconsumerâ€.\n\n\nDans cette architecture, le â€œconsumerâ€ se connecte Ã  la file de messages, lit les messages et se charge de toute la gestion dâ€™erreur et de retry.\n\nEn utilisant une Lambda comme â€œconsumerâ€, AWS a mis en place une intÃ©gration spÃ©cifique entre les files de messages et les Lambdas. Câ€™est maintenant la file de messages qui appelle directement la Lambda en lui donnant le message et qui gÃ¨re Ã©galement le retry : votre code applicatif est dÃ©chargÃ© dâ€™autant de responsabilitÃ©s sans valeur mÃ©tier.\n\nCommunication entre microservices\n\nQuoi de plus contraignant que de gÃ©rer la communication de plusieurs services ? Il faut gÃ©rer :\n\n\n  les erreurs : que faire si plusieurs microservices partent en timeout ou Ã©chouent dans un workflow ?\n  le couplage : lors de la crÃ©ation dâ€™un nouveau microservice, il doit Ãªtre lui aussi appelÃ© dans les chaines dâ€™appels ;\n  lâ€™authentification entre les diffÃ©rents services ;\n  et la latence : les appels de services en cascade augmentent la durÃ©e totale dâ€™exÃ©cution.\n\n\nDe ce constat, Matthieu propose une solution que nous avons dÃ©jÃ  mise en place chez Bedrock depuis plusieurs annÃ©es : communiquer avec des Ã©vÃ¨nements.\nPour cela, AWS fournit EventBridge : un service serverless de routage dâ€™Ã©vÃ¨nements sans stockage.\nAinsi, si un microservice doit en informer dâ€™autres, il lui suffit dâ€™envoyer un Ã©vÃ©nement dans EventBridge. Les autres services nâ€™auront quâ€™Ã  â€œÃ©couterâ€ lâ€™Ã©vÃ©nement.\nSNS, plus ancien, permettait la mÃªme approche, mais EventBridge propose de crÃ©er des rÃ¨gles de filtrage sur la totalitÃ© du message dâ€™un Ã©vÃ©nement.\n\nConstruire des applications serverless orientÃ©es Ã©vÃ©nements\n\nNicolas Moutschen, Solution Architect AWS et Guillaume Lannebere de chez Betclic ont fait un retour dâ€™expÃ©rience sur la mise en place de diffÃ©rents services serverless AWS orientÃ©s Ã©vÃ©nements.\n\nBetclic absorbe Ã  chaque match/course, une quantitÃ© Ã©norme de donnÃ©es (plusieurs millions dâ€™Ã©vÃ©nements) en quelques minutes.\nPar exemple, lors dâ€™un match de football, les paris sont effectuÃ©s Ã  tout instant : avant le match, Ã  la mi-temps, dans les derniÃ¨res minutesâ€¦\nLeur SI est donc soumis, frÃ©quemment, Ã  de trÃ¨s forts pics de charge pendant des laps de temps trÃ¨s courts.\n\nAfin dâ€™Ã©viter de provisionner Ã©normÃ©ment de machines pour se mettre Ã  lâ€™Ã©chelle, Betclic Ã  fait le choix du full serverless. Les applications de paris et de paiement communiquent par des messages dâ€™Ã©vÃ©nements envoyÃ©s dans le service AWS SNS : les Lambdas reÃ§oivent les messages et les traitent avec une mise Ã  lâ€™Ã©chelle quasi immÃ©diate en fonction du trafic.\n\nRÃ©sumÃ© par Fabien LALANNE - DÃ©veloppeur\n\nNous Ã©tions aussi intervenants \n\nNous aimons tout particuliÃ¨rement apprendre en lisant des articles Ã©crits par dâ€™autres membres de notre communautÃ© ou en assistant Ã  des confÃ©rences prÃ©sentÃ©es par dâ€™autres clients. Il est donc normal et important pour nous, de partager aussi notre expÃ©rience, ce que nous faisons rÃ©guliÃ¨rement, y compris sur ce blog.\n\nCette annÃ©e, nous avons eu la chance dâ€™intervenir et de partager avec notre communautÃ© lors de trois confÃ©rences. Merci Ã  AWS pour la confiance qui nous a Ã©tÃ© accordÃ©e !\n\nTransformer le load balancing pour optimiser le cache : objectif 50 millions dâ€™utilisateurs \n\nVincent Gallissot @vgallissot, Lead Cloud Architect, a expliquÃ© comment Bedrock a amÃ©liorÃ© le Load Balancing chez AWS, pour optimiser le cache de sa diffusion de vidÃ©os, avec comme objectif 50 million dâ€™utilisateurs :\n\nDÃ©marrage des confÃ©rences Ã  lâ€™#AWSSummit. Et voici un REX intÃ©ressant pour partager lâ€™une des problÃ©matiques intÃ©ressantes pic.twitter.com/kuKGZZyE2A&amp;mdash; Akram BLOUZA (@akram_Blouza) April 12, 2022\n\n\nGuillaume Marchand, Senior Solutions Architect chez AWS a dÃ©butÃ© notre talk en parlant de Load Balancing chez AWS, des diffÃ©rentes solutions et des bonnes pratiques, ainsi que des exemples dâ€™architectures possibles. Jâ€™ai ensuite expliquÃ© notre besoin de scaler des serveurs de cache et comment nous avons relevÃ© ce challenge, en dÃ©veloppant notamment Haproxy Service Discovery Orchestrator. Ce talk nâ€™a pas Ã©tÃ© enregistrÃ©, mais les slides sont disponibles sur ce lien.\n\nEtes-vous bien architecturÃ© ? \n\nPascal Martin @pascal_martin, Principal Engineer, est intervenu pour partager notre retour dâ€™expÃ©rience client pendant une confÃ©rence de prÃ©sentation du Well-Architected Framework :\n\nPour cette confÃ©rence, RÃ©mi Retureau, Partner Management SA Lead chez AWS, a commencÃ© par prÃ©senter les pratiques Well-Architected. Un ensemble de recommandations basÃ©es sur 10 ans dâ€™expertise de Solutions Architects AWS.\nJe suis ensuite intervenu pour partager un retour dâ€™expÃ©rience : comment nous utilisons Well-Architected Framework chez Bedrock pour nous aider Ã  valider lâ€™architecture de composants de notre plateforme, Ã  prioriser des Ã©volutions ou mÃªme, Ã  en identifier de nouvelles.\nEn quelques mots : nous passons une revue Well-Architected une fois par an et, si nous ne nous posons pas explicitement lâ€™ensemble des questions du Framework Ã  chaque nouveau projet, nous lâ€™intÃ©grons de plus en plus Ã  nos pratiques et habitudes.\nSi vous commencez Ã  travailler sur AWS, le Well-Architected Framework et ses recommandations, bien que peut-Ãªtre effrayantes au premier abord, sont un ensemble de bonnes pratiques qui vous aideront Ã  concevoir et Ã  construire une plateforme plus solide, plus rÃ©siliente et moins coÃ»teuse.\n\nPrÃ©parez et donnez votre premier talk \n\nPascal est aussi intervenu, cette fois en tant quâ€™AWS Hero pour guider la prÃ©paration de vos talks :\n\nPour cette seconde intervention, jâ€™ai choisi de parler dâ€™un sujet qui nâ€™est pas liÃ© Ã  AWS.\nJâ€™aime assister Ã  des confÃ©rences : je le fais depuis trÃ¨s longtemps et jâ€™apprends beaucoup ainsi.\nJe suis aussi toujours trÃ¨s content de voir dâ€™autres speakers monter sur scÃ¨ne et partager leur expÃ©rience. Je sais que beaucoup de personnes, dans notre communautÃ©, ont des connaissances et des idÃ©es gÃ©niales et jâ€™aimerais quâ€™elles les partagent plus souvent !\n\nJe sais toutefois que cet exercice est effrayant et que se lancer sur scÃ¨ne pour la premiÃ¨re fois est difficile. Jâ€™espÃ©rais donc, Ã  travers ce talk dÃ©jÃ  donnÃ© chez Bedrock lors dâ€™un Last Friday Talks (une journÃ©e de confÃ©rences internes le dernier vendredi du mois, un mois sur deux) aider de nouvelles personnes Ã  se lancer.\nLâ€™idÃ©e vous intÃ©resse mais vous nâ€™avez pas pu assister Ã  cette confÃ©rence ? Et bien, jâ€™ai aussi Ã©crit un livre pour vous accompagner : Â« PrÃ©parez et donnez votre premiÃ¨re confÃ©rence (quand ce nâ€™est pas votre mÃ©tier) Â»\nEt jâ€™ai hÃ¢te, lâ€™annÃ©e prochaine, de vous voir monter sur scÃ¨ne et partager avec notre communautÃ© !\n\nConclusion de lâ€™article \n\nAvec des milliers de participants et participantes, lâ€™AWS Summit est toujours une excellente occasion dâ€™Ã©changer et dâ€™apprendre. Nous Ã©tions Ã©galement trÃ¨s heureux de pouvoir, cette annÃ©e encore, partager notre expÃ©rience lors de trois interventions.\nCet Ã©vÃ©nement Ã©tait aussi le premier pour certains et certaines dâ€™entre nous, une trÃ¨s bonne dÃ©couverte !\n\nComme beaucoup dâ€™autres speakers et entreprises rencontrÃ©s mardi, nous recrutons : des SysOps, des DevOps, des dÃ©veloppeurs et des dÃ©veloppeuses, une ou un FinOps. Vous voulez nous aider Ã  construire et Ã  faire grandir notre plateforme ? Nous avons encore de super projets et challenges, faites-nous signe !\n"
} ,
  
  {
    "title"    : "Comment faire un trailer vidÃ©o qui dÃ©chire avec des technos web ?",
    "category" : "",
    "tags"     : " remotion, react, video, js, frontend, conference, lyonjs",
    "url"      : "/2022/04/04/comment-faire-un-trailer-qui-dechire-avec-des-technos-web.html",
    "date"     : "April 4, 2022",
    "excerpt"  : "Avec Antoine Caron on est allÃ© mettre des paillettes dans les yeux des participants du LyonJS en leur montrant comment crÃ©er des vidÃ©os avec des technos web ! âœ¨\n\nIl Ã©tait une fois â€¦ ğŸ“–\n\nUn jour, alors que jâ€™arrivais fraichement Ã  Bedrock, jâ€™ai eu l...",
  "content"  : "Avec Antoine Caron on est allÃ© mettre des paillettes dans les yeux des participants du LyonJS en leur montrant comment crÃ©er des vidÃ©os avec des technos web ! âœ¨\n\nIl Ã©tait une fois â€¦ ğŸ“–\n\nUn jour, alors que jâ€™arrivais fraichement Ã  Bedrock, jâ€™ai eu le malheur de demander Ã  Antoine Caron ce sur quoi il bossait entre midi et deux et qui semblait fort lâ€™amuser.\n\nSa rÃ©ponse : â€œJâ€™essaie de gÃ©nÃ©rer des vidÃ©os en MP4 Ã  partir de composants React, tu veux voir ?â€\n\nAprÃ¨s des heures Ã  tester chaque fonctionnalitÃ© de Remotion, il Ã©tait temps de prÃ©senter Ã§a Ã  la communautÃ© Javascript de Lyon lors du meetup nÂ°71 du Lyon JS ! ğŸ¦\n\n\n  \n\n\n\nPas le temps de regarder le replay ? â±\n\nPour vous donner une petite idÃ©e de ce que lâ€™on a fait, on vous partage un site qui gÃ©nÃ¨re dynamiquement des trailers vidÃ©o en fonction dâ€™un programme et dâ€™une couleur ! ğŸ¤¯\n\n\n  \n\n\n\nâ„¹ï¸ On vous conseille quand mÃªme de regarder le replay, mÃªme le crÃ©ateur de Remotion a aimÃ© ğŸ˜‰\n\n\n  A demo of Remotion in French at @LyonJS!Thanks for organizing this awesome talk @Slashgear_ @CruuzAzul ğŸ˜ƒhttps://t.co/xujfC7tR6e&amp;mdash; Remotion (@remotion_dev) April 3, 2022 \n\n\n\nOne more thingâ€¦ Petite surprise du chef ! ğŸ‘¨ğŸ»â€ğŸ³\n\nVoilÃ  un petit aperÃ§u dâ€™une vidÃ©o surprise que lâ€™on a fait grÃ¢ce Ã  Remotion uniquement avec des composants React ! (Si des gens sont nÃ©s avant 2000, Ã§a doit vous rappeler quelque chose ğŸ˜‰)\n\n\n  \n\n\n\nVous trouvez Ã§a incroyable et vous voulez essayer ? Nâ€™hÃ©sitez pas Ã  venir nous montrer vos vidÃ©os, ou directement sur twitter avec @Slashgear_ et @CruuzAzul ğŸ\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #16",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/03/26/bedrock-dev-facts-16.html",
    "date"     : "March 26, 2022",
    "excerpt"  : "Sur ce dÃ©but 2022 les Ã©quipes de Bedrock se sont lachÃ©es Ã§a promet pour le reste de lâ€™annÃ©e.\nLes autres articles de cette sÃ©rie sont disponibles ici.\n\nPromis !\n\n\n\nLâ€™arbre qui cache la forÃªt\n\n\n  Quand câ€™est la couleur des noms de fichiers qui te pe...",
  "content"  : "Sur ce dÃ©but 2022 les Ã©quipes de Bedrock se sont lachÃ©es Ã§a promet pour le reste de lâ€™annÃ©e.\nLes autres articles de cette sÃ©rie sont disponibles ici.\n\nPromis !\n\n\n\nLâ€™arbre qui cache la forÃªt\n\n\n  Quand câ€™est la couleur des noms de fichiers qui te permet de te repÃ©rer dans lâ€™arborescence du projetâ€¦\n\n\n\n\nLa boucle a bouclÃ©\n\nLe dÃ©bat est et sera Ã©ternel.\n\n\n  A: La meilleure extension de VSCode câ€™est dâ€™installer Webstorm.\n\n  B: Et la meilleure extension de Webstorm câ€™est dâ€™installer Phpstorm.\n\n  C: Et la meilleure extension de Phpstorm câ€™est dâ€™installer vim.\n\n  D: La meilleure extension de Vim câ€™est dâ€™installer Neovim.\n\n  E: La meilleure extension de neovim est le bloc-note.\n\n  F: Mon bloc-note câ€™est VSCode.\n\n\nQuand on va Ã  la pÃªche Ã  lâ€™exception\n\nException message (1) :\n------------------------\nAttempted to load class &quot;TruiteException&quot; from namespace &quot;Bedrock\\Stores\\Infra\\HttpClient&quot;.\nDid you forget a &quot;use&quot; statement for another namespace?\n\n\nâ€œCe nâ€™est pas un Ã©checâ€¦ Ã§a nâ€™a pas marchÃ©â€\n\n\n  Yâ€™a pas de bug, yâ€™a juste un truc qui fonctionne pas\n\n\nLes bottleneck câ€™est la vie\n\n\n  La vie est un bottleneck\n\n\nAtlas, le gÃ©ant ?\n\n\n  Qui câ€™est qui a dÃ©placÃ© la terre ?\n\n\nTranquilou bilou\n\n\n  Le pair programming nâ€™est pas du programming pÃ©pÃ¨re\n\n\nTester câ€™est â€¦\n\n\n  A: Faire des tests, câ€™est quand mÃªme moins chiant Ã  faire Ã  deux\n\n  B: Ouais, comme le sexe\n\n\nLabyrinthe\n\n\n  Pourquoi vous vous faites chier Ã  mettre les secrets dans SOPS. \nVous lâ€™auriez mis dans votre doc, personne ne les aurait trouvÃ©.\n\n\nVoilÃ  qui voilÃ  !\n\n\n  A: Je crois que la branche master elle est pas protÃ©gÃ©e.\n\n  â€¦\n\n  Ah bah heureusement que je suis lÃ  pour tester\n\n  B: Le plus Ã©tonnant câ€™est que â€˜Aâ€™ ait le droit de push non ?\n\n  A: Le plus Ã©tonnant câ€™est que jâ€™ai quelque chose Ã  push non ?\n\n  â€¦\n\n  On a rÃ©solu une faille de sÃ©curitÃ© sur le projet, grÃ¢ce Ã  mon investigation.\n\n  C: Le mec câ€™est inspecteur gadget en fait, on sait pas comment mais il finit par Ãªtre utile.\n\n\nğŸ•¯\n\n\n  The Domain is a sanctuary\n\n\nQuand il y a trop de viennoiserie chez Bedrock\n\n\n  A: Fait gaffes tu es en ordre croissant, lâ€™ordre pain au chocolat est bien meilleur\n\n\nMais oui câ€™est clair !\n\n\n  On complexifie pour faire plus simple !\n\n\nLa PRO-crastination\n\n\n  Lead: Câ€™Ã©tait qui le chargÃ© du monitoring hier ?\n\n  A: Câ€™Ã©tait moi mais jâ€™ai rien fait, mais si vous voulez je peux le refaire aujourdâ€™hui.\n\n  B: Si tu veux je peux mÃªme tâ€™aider Ã  rien refaire.\n\n\nÃ‡a sent un peu la poussiÃ¨re ici\n\n\n  A: Eh eh, lÃ  on a effectivement sur-gonflÃ© lâ€™estim â€¦ (mais le cÃ´tÃ© pas dans lâ€™inconnu lâ€™explique)\n\n  B: Ouais câ€™Ã©tait carrÃ©ment la peur de toucher Ã  un projet legacy de chez legacy, quâ€™on ne connait pas, pas dans notre scopeâ€¦\n\n  A: Mais pas de souci majeur pour un dÃ©veloppeur qui pratiquait Symfony sous Pompidou (le code date de cette Ã©poque)\n\n\nUn nouveau mot\n\n\n  Hybriditance\n\n\nDouble hit combo\n\n\n  â€œI want to identify pages where users hitâ€, le developper traduit alors â€œJe veux identifier la page oÃ¹ lâ€™utilisateur se frappeâ€\n\n\nUne histoire de dev enrhumÃ©\n\n\n\nQui sâ€™appeleriot le CPU\n\n\n  Il fait des soustractions, des additionsâ€¦ il fait des trucs de fous\n\n\nUn nouveau KPI\n\n\n  Alors mon piffomÃ¨tre du sprintâ€¦\n\n\nCâ€™est simple non ?\n\n\n  I talked to A that said that B said that C said that it is possible to parallelize steps on Jenkins.\n\n  So I asked B how to do that, saying that A said that he said that C said that it was possible to do it!\n\n"
} ,
  
  {
    "title"    : "How AWS Cloudfront is helping us deliver our Web streaming platform? - PartÂ 1",
    "category" : "",
    "tags"     : " cloudfront, aws, cdn, node.js, react, javascript, frontend",
    "url"      : "/2022/03/08/cloudfront-web-streaming-platform-part-1.html",
    "date"     : "March 8, 2022",
    "excerpt"  : "A bit of context\n\nThe web is a major platform for the distribution of our customersâ€™ content at Bedrock.\nMillions of users connect every month to watch their live, replay or the series and movies of their choice.\nThe broadcasting of sports events ...",
  "content"  : "A bit of context\n\nThe web is a major platform for the distribution of our customersâ€™ content at Bedrock.\nMillions of users connect every month to watch their live, replay or the series and movies of their choice.\nThe broadcasting of sports events such as the Euro 2020 soccer tournament represents a real technical challenge when it comes to maintaining the stability and performance of such a platform.\n\nThe web application works in SSR (Server Side Rendering) mode: we have NodeJS Express servers returning pre-rendered HTML pages.\nWe made this choice several years ago, for two reasons: SEO and to improve the first display time on slow devices.\nIn addition to the HTML pages, the web platform is also a huge collection of assets that allow the website to function: Javascript bundles, CSS, images, manifests.\n\nToday our customers have users distributed over a large part of the globe.\n\nTo meet these challenges, a CDN is a very good solution.\n\nWhat is a CDN then ?\n\nCDN for Content Delivery Network is a service delivering content to users across the internet (here our HTML pages and assets).\nWherever the user is in the world.\nTo all users, even if they are many.\n\nCloudfront is the CDN service of AWS.\nWith a large number of POPs (Point Of Presence) around the world, it helps us provide a response to each user as close as possible to their location.\nThis allows us to significantly reduce the time to first byte of our responses.\nDifferent price classes allow you to choose the global â€œareaâ€ in which your application should be available in order to achieve savings.\n\n\n\nBeing in Lyon (France), we sometimes get answers from the POP of Milan (Italia).\nIndeed, Lyon â†” Milan is almost as closer as Lyon â†” Paris.\n\nNote that it is very easy to know which Cloudfront POP answered you.\nEach POP is identified by a three letter code that corresponds to the code of the nearest international airport (here: CDG corresponds to Paris Charles de Gaulle airport).\n\nx-amz-cf-pop: CDG50-C1\n\n\nDelivering content as close to the user as possible is great, it theoretically reduces waiting time but it does not solve the problem of heavy load.\n\nThe best solution for load problems is caching.\n\n\n  You put 1 second of cache-control, and you already won!\n\n  Y. Verry, our Head of Infrastructure and Ops\n\n\nCloudfront service makes it easy to cache responses at the edge servers.\nIf we take the example of sports broadcasting, users arrive in large numbers in a very short period of time.\nCaching (telling Cloudfront to cache a web page) takes a lot of the load off our Node servers because they are not called.\n\nCaching objects in Cloudfront is also about improving response times.\nNo need to wait for our servers, the user receives the cached object directly.\nCloudfront even takes advantage of this to apply more powerful compression algorithms like Brotli on these cached objects.\nThese compressions, performed directly by the CDN, allow you to drastically reduce the size of your objects on the network.\nReducing objects size make our applications load even faster for our users.\n\nHere is our Cache hit ratio in production on 6play.fr website.\n\n\n\nCloudfront also allows us to do â€œEdge computingâ€: run code directly in Amazon edges and POPs instead of doing it in our applications.\n\nLambda at edge (on regional edges servers), Cloudfront function (function that runs on POP servers), Web Application Firewall, here are some very cool features that will allow you to do usual manipulations on your requests/responses.\n\nFinally, by using regional Pop, hundreds of end server edges do not contact your origin (your application) when the cache is invalidated or exceeded.\nYou can even activate the Origin Shield feature that allows you to further limit the load on your origins.\n\n\n\nGood per-level cache management even allowed us to completely invalidate the cache of a Cloudfront distribution a few minutes before the start of an event without generating huge traffic on our servers.\n\n\n\nAnd thatâ€™s it for this first article, in the next part (and normally the last one) you will discover how we have implemented some patterns on our sites.\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  More efficient Load Balancing and Caching at AWS, using Consistent Hashing and HAProxy\n  Scaling Bedrock video delivery to 50 million users\n\n"
} ,
  
  {
    "title"    : "Streaming recommendations at Bedrock",
    "category" : "",
    "tags"     : " recommender systems, machine learning, data, data science",
    "url"      : "/2022/02/27/streaming-recommendation.html",
    "date"     : "February 27, 2022",
    "excerpt"  : "Personalised recommendations are everywhere. No exception for the streaming world. To improve user experience, recommender systems with machine learning are uplifting.\nAt Bedrock, until recently, there was no recommendation shaped this way.\n\nBut w...",
  "content"  : "Personalised recommendations are everywhere. No exception for the streaming world. To improve user experience, recommender systems with machine learning are uplifting.\nAt Bedrock, until recently, there was no recommendation shaped this way.\n\nBut we are writing a new story.\n\nA quick win solution\n\nWe wanted to find a way to get a solution that would be quick to integrate.\n\nWe chose to use Amazon Personalize. This service aims to construct recommender systems with machine learning. The promise is to Create real-time personalized user experiences faster at scale. Perfect! It was exactly what we were looking for.\n\nRapidly, we encountered an obstacle. You canâ€™t deploy Personalize with Terraform. Yet, Terraform is the tool we use to manage our infrastructure.\n\nHow to deploy Amazon Personalize?\n\nAs Personalize is supposed to be a temporary solution in our stack, for once, we accepted not using Terraform. We developed a Python script to interact with Personalize. Apache Airflow schedules and monitors the script.\n\nPersonalize is a black box. You canâ€™t have access to explanations about the generated models. But, with Personalize, you have different ways to evaluate your recommendations.\n\nWith recommender systems, using the offline metrics to judge your model is not enough. Itâ€™s better than nothing! But to check that a recommender system works, you need to evaluate it online with real users.\n\nWe have millions of users. Releasing a recommender system to all our users is definitely not the best idea ever.\n\nHow to release a recommender system?\n\nFirst of all, check offline metrics. Theyâ€™re still a valuable hint. Then, analyse the recommendations with people from the editorialist team.\n\nNote that this kind of analysis is very subjective.\n\nFinally, deliver the functionality to a small portion of your users.\n\nWe configured an AB test that gives recommendations to 5% of users. With dashboards, we study the impacts.\n\nPerfect! We have a way to check the success of a full broadcast.\n\nBut how to be sure that the new product will support the load? We have millions of users. It means that 5% of users still represent a lot of people.\n\nHow to assess the performance of a recommender system?\n\nLaunch load tests. Today you have a myriad of tools to do that. At Bedrock, we use Artillery.\n\nDuring the load tests, we had a bug. We discovered that by default, the limit of requests per second with Personalize is 500. In our context, thatâ€™s not acceptable.\n\nWe asked Amazon to help us and they changed the option for us.\n\nThe results\n\nIf youâ€™re a big fan of reality TV shows, you will see that:\n\n\n\nIf you prefer reports, you will see that instead:\n\n\n\nWhat now?\n\nWeâ€™ve deployed an AB test for our first recommender system built with machine learning.\n\nWe donâ€™t have the results of the AB test yet. But, weâ€™ve noticed that many users interact with the recommendations.\n\nAfter different challenges, we nailed it.\n\nBut, Personalize is expensive and a black box that we canâ€™t integrate with Terraform easily (weâ€™ll have to develop something for that, at least). It doesnâ€™t suit our context. Thatâ€™s why weâ€™ve started to develop our first models.\n"
} ,
  
  {
    "title"    : "Tonight&#39;s football time, let&#39;s prescale Kubernetes to avoid a crash!",
    "category" : "",
    "tags"     : " kubernetes, scaling, high availability, aws, cloud",
    "url"      : "/2022/02/03/prescaling.html",
    "date"     : "February 3, 2022",
    "excerpt"  : "Are you experiencing peak loads on your Kubernetes-hosted platform? Rest assured, you are not alone.\nAt Bedrock, we have developed a prescaling solution. It allows us to handle sudden and abrupt, but predictable, \ntraffic spikes, like soccer games...",
  "content"  : "Are you experiencing peak loads on your Kubernetes-hosted platform? Rest assured, you are not alone.\nAt Bedrock, we have developed a prescaling solution. It allows us to handle sudden and abrupt, but predictable, \ntraffic spikes, like soccer games.\n\nKubernetes provides HorizontalPodAutoscalers to handle traffic variations. \nWeâ€™ll look at their limitations in the case of meteoric spikes in load and how prescaling helps us deal with the \nsudden arrival of several hundred thousand users.\n\nTable of Contents\n\n\n  Load and traffic vary\n  Beginning of the scaling problems\n  How does reactive scaling work in Kubernetes?\n    \n      An HorizontalPodAutoscaler\n      What scale out looks like in a real case\n      How fast is reactive scaling?\n    \n  \n  Prescalingâ€¦ What is this about?\n  Prescaling our applications\n    \n      Enabling and configuring prescaling on an HPA\n      The prescaling exporter\n      How can the HPAs prescale?\n      Prescaling works!\n    \n  \n  What about special, huge, events?\n    \n      The prescaling API\n      Letâ€™s see how an application scales during a very special event\n    \n  \n  Prescaling external services: another challenge\n\n\nLoad and traffic vary\n\nLoad and traffic have always varied over time on our platform:\n\n\nCPU per instance over time\n\nTo deal with these load variations, several tools help us to automatically adapt our Kubernetes clustersâ€™ capacity:\n\n  HorizontalPodAutoscaler \n(HPA): adds/removes Pods (= capacity) on a workload resource such as Deployment or a \nStatefulSet.\n  Cluster Autoscaler: \nadjusts the size of a Kubernetes cluster by adding/removing nodes.\n  Overprovisioning: starts â€œemptyâ€ \npods (and new â€œuselessâ€ nodes, as a consequence), so the cluster has available capacity that will be used to start \napplications pods quicker.\n\n\n\n  If you wish to know more about which tools we use and why we use them in our Kubernetes clusters, I advise you to \ncheck out another dedicated blog post named \nâ€œThree years running Kubernetes on production at Bedrockâ€.\n\n\nUnfortunately, all those tools are not sufficient to deal with heavy and sudden traffic spikes on some special \nevenings such as the final of a football game or a successful show. During this kind of events, users arrive massively, \nall at the same time. On some evenings, some of our applications see their load rise by 5 in 2 minutes, others even \nsee theirs multiplied by 10 in 2 minutes!\n\nThe predictable aspect of those arrivals is very important because it means we are able to prepare our platform \nbeforehand. Thatâ€™s why our prescaling solution was born.\n\nBeginning of the scaling problems\n\nOn an ordinary evening, when the multiple Kubernetes scaling tools were kicking in, here is basically what was \nhappening:\n\n\n\nAs the load increased, our capacity was increasing as well and we always had spare capacity. We were able to double our \ninitial capacity every 5 minutes thanks to reactive scaling when load started to rise.\n\nAfter a while and on some special evenings, we began to see this kind of behavior:\n\n\n\nSometimes, load was increasing faster than what reactive scaling could handle, that is to say about x2 in capacity \nevery 5 minutes. The consequence is that we could not serve everyone. For a while, our platform would fail for some \nusers, until autoscaling kicked in or until load stopped rising so fast.\n\nLetâ€™s see how reactive scaling works to understand how we can leverage it to prepare the platform in advance.\n\nHow does reactive scaling work in Kubernetes?\n\nAn HorizontalPodAutoscaler\n\nFor a Deployment to be autoscaled (reactively, according to varying load), the number of replicas of a Kubernetes \nDeployment is reconfigured by an HorizontalPodAutoscaler:\n\n\n\nIts manifest usually looks like this:\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\n# â€¦\nspec:\n  # â€¦\n  minReplicas: 2\n  maxReplicas: 100\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 70\n  - type: Pods\n    pods:\n      metricName: phpfpm_active_process_percentage\n      targetAverageValue: &quot;40&quot;\n  # â€¦\n\n\nHere:\n\n  minReplicas is the minimum number of Pods that will run (under normal conditions).\n  maxReplicas is the maximum number of Pods that will run.\n  metrics is a list of metrics the HPA will analyze to determine if it must scale or not. If usage gets higher \nthan the target specified for a metric, the HPA will add new Pods (= scale out). If usage gets lower than all targets \nspecified for all metrics, the HPA will remove Pods (= scale in).\n\n\nWhat scale out looks like in a real case\n\nAt Bedrock, we use three types of metrics to scale: Resource, Custom and External. Here is an example of \nan application scaling out on a Custom metric:\n\n\nProcess status (%) over time\n\nIn this graph, around 20:52:30 and 20:55:00, the percentage of active processes rises to go above \nthe target 40 we saw in the YAML example before. This triggers the scale out of the HPA of this application:\n\n\nPod status over time\n\nWe can see that around 20:53:00 (30 seconds after we first go above the target), the number of unavailable pods rises. \nThe scale-out has just started. A few moments after, the pods become available and are able to serve users.\n\nHow fast is reactive scaling?\n\nScaling in Kubernetes with HPA is not instantaneous:\n\n\n\n\n  Metrics are updated every 30 seconds (for CPU or memory) or every 60 seconds (for external and custom metrics) or so.\n  HPA analyses the metrics to know if scale-out is required every 30 seconds or more.\n  We might not have enough EC2 servers running to host the new Pods that are trying to start (we usually have between \n10% and 20% of spare capacity). If starting new EC2 servers is necessary, this takes between 2 and 4 minutes.\n  Pods (as our applications and Docker images are big) usually need between 30 and 60 seconds to start.\n\n\nThis means when a spike in load occurs, we need between 1â€™00â€™â€™ and 6â€™00â€™â€™ for new Pods to be able to handle that load. \nScaling is clearly not instantaneous.\n\nPrescalingâ€¦ What is this about?\n\nMost our applications deployed in Kubernetes use an HorizontalPodAutoscaler. As weâ€™ve seen, this autoscaling \nmechanism is reactive in nature: Pods are added when load (or another metric) gets higher than a target. \nIt can handle load that increases slowly (say, instant +20% or x2 in 5 minutes), but not huge instantaneous spikes \n(say, x10 in less than 5 minutes). It works fine for most of our usual workloads, but cannot absorb spikes we receive \nduring special events like Top Chef or when the France football team plays.\n\nThe only way we can handle a huge and sudden spike in traffic on an application is by pre-provisioning capacity. \nIn Kubernetes, this is done by running more Pods than necessary so that they are ready to handle the additional load. \nRunning additional, mostly superfluous, capacity has a costâ€¦\n\nWe know our applications receive a sudden and brutal traffic spike once a day, between 20:50 and 21:00 Paris time. \nNot taking special events into account, thatâ€™s the only time load increases violently enough for reactive autoscaling \nto be unable to handle it â€“ and, most days, it actually does quite fine. So, we could pre-provision more capacity \naround that time (only), and not pay for it the rest of the dayâ€¦\n\nPrescaling our applications\n\nEnabling and configuring prescaling on an HPA\n\nTo enable and configure prescaling on an HorizontalPodAutoscaler, we add two sets of information:\n\n  Three annotations to define:\n    \n      Time when prescaling starts.\n      Time when prescaling stops.\n      The minimum number of Pods we want between those two times.\n    \n  \n  A new metric to scale on.\n\n\nThe annotations are set in the metadata block:\n# â€¦\nmetadata:\n  # â€¦\n  annotations:\n    annotations.scaling.exporter.replica.min: &quot;25&quot;\n    annotations.scaling.exporter.time.start: &quot;19:30:00&quot;\n    annotations.scaling.exporter.time.end: &quot;23:30:00&quot;\nspec:\n  scaleTargetRef:\n    # â€¦\n    name: &quot;service-6play-images&quot;\n\n\nIn this example, we indicate we want at least 25 pods between 19:30:00 and 23:30:00. As a result, \nthe number of pods of this application will rise up to at least 25 pods during this period of time.\n\nTimes are expressed in the local timezone of the Kubernetes cluster as prescaling is linked to events on the platform. \nThose events are usually linked to events on live TV and we deploy one cluster per TV broadcaster.\n\nIn the metrics block, you need to configure a new External metric:\n# â€¦\nmetrics:\n # â€¦\n - type: External\n   external:\n     metricName: &quot;annotation_scaling_min_replica&quot;\n     metricSelector:\n       matchLabels:\n         deployment: &quot;service-6play-images&quot;\n     targetValue: &quot;10&quot;\n\n\nThe label used for deployment must be set to the value of scaleTargetRef.name (= the name of the Deployment the \nHPA reconfigures). The targetValue must always be set to 10.\n\nYou now know how we configure an HPA for prescaling. What you do not know yet is how the HPA annotations are used \nand which application exposes the metrics called annotation_scaling_min_replica. Itâ€™s now time to talk about the \nprescaling exporter.\n\nThe prescaling exporter\n\nThe prescaling exporter is a Prometheus exporter we developed (in Python). \nIt exposes metrics, used to scale Kubernetes applications on a day-to-day basis during a given time range.\n\n\n  Prometheus is one of the tools we use in our monitoring stack at Bedrock. One of its purposes, among others, is to \ncollect metrics from Pods. This article will not present our Prometheus stack in detail.\n\n\nHere is how the prescaling exporter works:\n\n\n\n\n  Every 15 seconds or so, Prometheus scrapes the prescaling exporter pod to get the metrics it exposes.\n  Scrapping triggers the generation of the metrics. Before they are exposed, the exporter first \ncalls the k8s API to list all HPAs in the cluster.\n  Then, it will:\n    \n      Filter those HPA with the required annotations (the annotations we added a bit earlier on an HPA).\n      Calculate the new annotation_scaling_min_replica metric for each HPA with the prescaling annotations.\n      The prescaling exporter can now expose the metrics.\n    \n  \n  And Prometheus can retrieve them.\n\n\nHow does the prescaling exporter calculate the metrics of the HPA subscribed to the prescaling? Well, it depends \non the content of the annotations you configured on the HPA.\n\nHere is how a Prometheus metric of the prescaling exporter looks like:\n\n\n\nIn each metric, you will find several labels. I chose to put only one here to simplify.\nThe metric can take one of three values, depending on the content of the annotations we saw earlier:\n\n  When we are not within the time range of the prescaling annotations of the HPA, it means that we do not \nneed to prescale. As a result, the metric is set to 0.\n  If we are within the time range of the prescaling annotations, it means we are in the prescaling time range. \nFrom there, two possibilities:\n    \n      If â€œCurrent number of replicasâ€ &amp;lt; â€œNumber of minimum replicas in the HPA annotationâ€, we need to add replicas so \nthe metric is set to 11.\n      If â€œCurrent number of replicasâ€ &amp;gt;= â€œNumber of minimum replicas in the HPA annotationâ€, we have enough replicas so \nthe metric is set to 10.\n    \n  \n\n\n\n  When the metric is set to 10 and we already have enough replicas running, the number of replicas will never go below \nthe minimum chosen in the prescaling annotation annotations.scaling.exporter.replica.min.\n\n\nHere is what it looks like on Grafana for the application service-6play-images during the evening:\n\n\nannotation_scaling_min_replica over time\n\nIn this example:\n\n  Until 19:30, annotation_scaling_min_replica is set to 0.\n  From 19:30 until about 19:35, annotation_scaling_min_replica is set to 11 (scale-out will happen).\n  From 19:35 until 00:00, annotation_scaling_min_replica is set to 10 (scale-out is done, we have enough Pods).\n  From 00:00 until the following evening, annotation_scaling_min_replica is set to 0 again.\n\n\nWe can guess two things from this example:\n\n  The prescaling period for this application was 19:30 until 00:00.\n  It took about 5 minutes to prescale (= add more Pods) the application.\n\n\nHow can the HPAs prescale?\n\nTo understand how the HPAs can prescale, we need to talk about \nthe prometheus adapter. It is an implementation \nof the Kubernetes metrics APIs. We use it to expose custom and external metrics for HPAs to use in order to scale:\n\n\n\n\n  Prometheus adapter collects metrics from prometheus once every minute and exposes them as External and\nCustom metrics.\n  The HPA controller manager fetches metrics provided by metrics-server and prometheus-adapter to scale out or \nscale in Deployment/Replica Set/Stateful Set resources. To do so, it uses k8s aggregated \nAPIs (metrics.k8s.io, custom.metrics.k8s.io and external.metrics.k8s.io). \nMetrics-server provides resource metrics (only CPU and memory). \nPrometheus adapter provides all non-resource metrics (external and custom).\n\n\nFor more information: HPA Kubernetes documentation.\n\nPrescaling works!\n\nAfter prescaling has been deployed into production and teams started to add annotations in their projects, \nthis is what happened:\n\n\nNumber of pods regarding their status over time\n\nAround 19:30, the number of pods for this application goes from 25 to a bit more than 55. It means its HPA was \nscaled out, based on the prescaling metric of that application. Mission accomplished!\n\nWhat about special, huge, events?\n\nSome days, during very special events, â€œnormalâ€ prescaling was not enough to handle the load that was rising \nway faster than what we usually see on our platform:\n\n\n\nAs you can see, even with prescaling doing its job before the start of the TV program (we can see capacity \nrising at the beginning of the graph), the traffic rises so quickly at 20:55 that we are still unable to scale fast \nenough to serve all users.\n\nFor these special events, we have developed an additional mechanism that allows us to set a multiplication coefficient \nto all prescaling. We use it to say â€œI want a 5x higher minimum number of Pods than whatâ€™s configured in the annotation \nweâ€™ve seen before, for all HPAs bearing this annotationâ€.\n\nTo deal with those very special events, we added another component in the prescaling stack: the prescaling API.\n\nThe prescaling API\n\nThe prescaling API is a backend application also developed in Python. It was designed to store prescaling \nsettings for future events on the platform in AWS DynamoDB. We chose DynamoDB because itâ€™s a serverless database \neasily maintainable through Terraform code. By â€œeventâ€, understand a football game or another big show such as \nTop Chef. Those settings define when and how we must enlarge the platform to sustain bigger \ntraffic spikes than on standard days (= on normal prescaling evenings).\n\n\n\nWith this API, our prescaling workflow has evolved:\n\n\n  Prometheus scrapes the prescaling exporter pod, same as before.\n  Scrapping triggers the exposition of the Prometheus metrics. Before the metrics are exposed, the exporter first \ncalls the Prescaling API server to get the current special event if there is one.\n  After calling the prescaling API, the exporter calls the k8s API (as before) to list all HPAs in the cluster.\n  Then, it:\n    \n      Filters those HPA with the required annotations.\n      Calculates the new annotation_scaling_min_replica metrics by merging information from the HPA annotations and \nthe special event from the prescaling-api server.\n    \n  \n  The prescaling exporter now exposes the metrics so that Prometheus can retrieve them.\n\n\nLetâ€™s see how an application scales during a very special event\n\nHere is how the number of pods evolves with a special prescaling event configured:\n\n\nNumber of pods by status over time\n\nFor this specific application, we had around 25 pods during the day and standard prescaling was configured at \n40 pods in the HPA annotations. On normal days, we would have had about 40 pods throughout the evening. \nOn this particular day, we had around 125 pods: a â€x3â€ multiplier was applied, thanks to the prescaling API.\n\nPrescaling external services: another challenge\n\nReactive scaling still answers most of our needs. We are still able to do â€œx2 every 5 minutesâ€ in Kubernetes. \nPrescaling is great, it can help critical applications to sustain sudden and expected traffic spikes we had \nproblems dealing with before. On top of that, prescaling for special events even allows us to deal with extreme cases.\n\nStill, the applications we prescale often depend on external services: a database, a cache, a search engineâ€¦ \nMost of these external services will not prescale as easily as with our prescaling solution. \nSome services, like AWS DynamoDB or AWS Aurora serverless, come with a reactive autoscaling solution, \nbut not all of them. And still, even those autoscaling services have limitsâ€¦\n\n\n\nVery special thanks to all my Bedrock Streaming colleagues who helped me improve this blog post.\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #15",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/12/20/bedrock-dev-facts-15.html",
    "date"     : "December 20, 2021",
    "excerpt"  : "Le pÃ¨re noÃ«l ğŸ…ğŸ» vous apporte en avance une hotte pleine de devfacts !\n\nTempora mori, tempora mundis recorda\n\n\n  Lâ€™autre jour, on allait au ski. Câ€™Ã©tait lâ€™annÃ©e derniÃ¨re\n\n\nLe pass sanitaire de la prod\n\n\n  Un test de charge câ€™est comme un test PCR, ...",
  "content"  : "Le pÃ¨re noÃ«l ğŸ…ğŸ» vous apporte en avance une hotte pleine de devfacts !\n\nTempora mori, tempora mundis recorda\n\n\n  Lâ€™autre jour, on allait au ski. Câ€™Ã©tait lâ€™annÃ©e derniÃ¨re\n\n\nLe pass sanitaire de la prod\n\n\n  Un test de charge câ€™est comme un test PCR, câ€™est valable 3h\n\n\nUn pair programming un peu trop random\n\n\n  _ â€œTu peux aller sur un site web random pour vÃ©rifier un truc ?â€\n\n  _ â€œP***hub Ã§a te va ou Ã§a te dÃ©range ?â€\n\n\nLa prÃ©cision\n\n\n  Alors, pour X, tu as un gros tas dâ€™Ã©lÃ©ments, pour Y un tas dâ€™Ã©lÃ©ments, et pour Z, un petit tas dâ€™Ã©lÃ©ments.\n\n\nOn nâ€™est pas sorti du sable\n\n\n  â€œstdout nâ€™existe plusâ€\n\n\nLâ€™effet papillon\n\n\n  â€œBon tu viens manger ?â€\n\n  â€œAttends je suis dâ€™astreinte et il y a des orages Ã  Paris qui font sauter le live en Croatieâ€\n\n\nA new hope\n\nQuand tu envisages de refacto un bout de code, que tu regardes la PR Ã  lâ€™origine de ce code et que tu trouves un commentaire que tu avais laissÃ© qui dit:\n\n\n  I really hope this will solve our problems and not introduce new ones.\n\n\nLe retour du first try\n\n\n\nUn bug ? OÃ¹ Ã§a un bug ?\n\n\n  Comme jâ€™ai pas rÃ©ussi Ã  le rÃ©soudre, on peut le mettre en terminÃ©\n\n\nAh la boulette !\n\n\n  Est-ce que vous savez si on peut abort un git rebase --abort ?\n\n\nRetour de congÃ©s compliquÃ©\n\n\n  Possible de mâ€™unlock mon compte Okta ?\n\n\nTODO\n\n\n\nUn bon conseil\n\n\n  Regardez le cÃ´tÃ© fonctionnel du succÃ¨s\n\n\nLe re-retour du first try\n\n\n\nTout est planifiÃ© depuis le dÃ©but\n\n\n  Câ€™est prÃ©vu mais Ã§a fait pas ce que je voulais.\n\n\nAh !\n\n\n  On lui a demandÃ© â€œquâ€™est-ce quâ€™on en fait ?â€œ, ils nous a rÃ©pondu â€œrienâ€\n\n\nMon nom est Paco ğŸ¦œ\n\n\n  il rÃ©pÃ¨te tout ce que tu dis â€¦ mais en faux !\n\n\nNe pas sous-estimer le miss-click\n\n\n  Tu peux pas miss-click trois fois de suite câ€™est pas possible, mÃªme en vacances tu peux pas.\n\n\nOn se la pÃ¨te un peu\n\n\n  \n    \n      Ã‡a montre bien quâ€™on est trop fort!\n    \n    \n      Ne confond pas le hasard avec une compÃ©tence!\n    \n  \n\n\nDifficulty Driven Design\n\n\n  Sommes-nous convaincus par le DDD ou des cons vaincus par le DDD ?\n\n\nLâ€™importance de se relire\n\n\n  After more reflection I said bullshit.\n\n\nVers lâ€™infini et lâ€™au-delÃ \n\n\n  Lâ€™engineering manager mâ€™a confirmÃ© que lâ€™on pourrait aller bien au delÃ , voire un peu plusâ€¦\n\n\nUn env de dev ? Pourquoi faire ?\n\n\n  \n    Tu as testÃ© en local ?\n    Bien sÃ»r que non. AH Ã§a ne fonctionne pas en local\n  \n\n\nQuand on vient annoncer une bonne nouvelle\n\n\n  Bonjour ! Comment Ã§a allait ?\n\n\nJâ€™y crois moyen\n\n\n  Pour une fois Ã§a marcherait mÃªme mieux sous Windows.\n\n\nPlus simple que simple\n\n\n  Avec les nouvelles fonctionnalitÃ©s, câ€™est compliquÃ© de faire au plus simple.\n\n\nSa vision est basÃ©e sur le mouvement\n\n\n  Gris câ€™est bien parce que câ€™est ni vert ni rouge !\n\n\nLa concurrence est rude\n\n\n  \n    (P.O. Mobile) : â€œCâ€™est mocheâ€\n    (Tech Lead Android) : â€œCâ€™est Androidâ€\n    (Dev iOs) : â€œSi on peut mÃªme plus trollerâ€\n  \n\n\nEddy malou en marque blanche\n\n\n  La customiseration\n\n\nLe crime parfait\n\n\n  Ah oui, yâ€™a des voyous ici qui mettent des espaces insÃ©cables dans les noms de fonctions.\n\n\nOn lâ€™avait pas vu venir celle-lÃ \n\n\n  Câ€™est hyper compliquÃ© de prÃ©voir des choses quâ€™on ne peut pas prÃ©voir.\n\n\nCâ€™est trop calme\n\n\n  Câ€™est lâ€™encÃ©phalogramme de lâ€™huitre ton dashboard lÃ \n\n\nThree magic words\n\nÃ€ la cafÃ©tÃ©ria:\n\n\n  \n    ğŸ‘‹ Il faut quâ€™on discute !\n    Si câ€™est pour parler boulot câ€™est mÃªme pas la peine !\n    Incident en prod\n    Ah oui jâ€™ai vu passer une notif, je regarde Ã§a de suite !\n  \n\n\nLâ€™expÃ©rience ou lâ€™age\n\nMontre un raccourci clavier dans un outil de gestion de tickets\n\n  Tâ€™inquiÃ¨te pas, jâ€™ai roulÃ© ma bosse, je gÃ¨re !\n\n\nLes geeks no-life\n\n\n  \n    tu peux bouger stp pour la lumiÃ¨re\n    pfff, jâ€™ai bougÃ© il y a pas 5mn\n  \n\n\nNe tâ€™inquiÃ¨te pas, on va trouver Ã§a dans les log\n\n\n\nFatigue du soir\n\n\n  500 câ€™est pour succÃ¨s, câ€™est Ã§a ?\n\n\nEn rouge et noir !\n\n\n  Quand câ€™est rouge câ€™est que câ€™est OK.\n\n\nDEL\n\nQuand on supprime du code et quâ€™on explique comment on lâ€™a fait dans la PR.\n\n\n  How?\n\n  Del key on the keyboard.\n\n"
} ,
  
  {
    "title"    : "Scaling Bedrock video delivery to 50 million users",
    "category" : "",
    "tags"     : " aws, cloud, sysadmin, HAProxy, video, high availability, Unified Streaming, VOD, OTT, video delivery",
    "url"      : "/2021/12/15/scaling-bedrock-video-delivery-to-50-million-users.html",
    "date"     : "December 15, 2021",
    "excerpt"  : "Hereâ€™s our journey to migrate tens of thousands of videos, accessed by millions of users, to the cloud. How we minimized our costs without losing the biggest benefit of the cloud: scaling.\n\nThe purpose of this article is to show you the evolution ...",
  "content"  : "Hereâ€™s our journey to migrate tens of thousands of videos, accessed by millions of users, to the cloud. How we minimized our costs without losing the biggest benefit of the cloud: scaling.\n\nThe purpose of this article is to show you the evolution of this cloud video delivery platform, from the first draft to the current version.\n\nTable of Contents\n\n\n  How we do streaming\n  Just In Time Packaging\n  Version 1: The quest for self\n    \n      Local cache with Nginx\n      Network Load Balancer: manages TLS and helps with scaling\n      Content Delivery Network: Keep it Simple and Stupid\n      A first conclusion: V2 needs Consistent Hashing\n    \n  \n  Version 2: Let the requests flow\n    \n      HAProxy to make Consistent Hashing\n      EC2 costs are reduced by using only Spot instances\n      Production launch on this V2\n      EC2-other: the financial abyss\n    \n  \n  Version 3: Cost Explorer Driven Development\n    \n      Be multi-AZ without inter-AZ traffic\n      Mono-AZ AutoScalingGroups\n    \n  \n  Optimizations\n    \n      Adapt HAProxy config for EC2 bandwidth throttling\n      Adjust the hash balance factor to correctly trigger scaling\n    \n  \n  Conclusion\n\n\nHow we do streaming \n\nTo stream video, we cut each video file in 6 seconds chunks. The video player loads the associated manifest, which lists these pieces and in which order it must read them. It then downloads the first video chunk, plays it, then loads the second chunk, etc.\n\n\nPictorial explanation of video streaming\n\nA video is composed of several chunks.\nFor example, a 90 minutes movie, with a duration of 6 seconds per chunk, means 90Ã—60Ã·6=900 video chunks called from the player plus another 900 audio chunks. A total of 1800 different chunks for a single video.\n\nJust In Time Packaging \n\nA client calls a manifest and chunks to play a video.\nDepending on the format (Dash, HLS, Smooth) a client supports, it will request one of three kinds of manifests+chunks.\n\nThe Unified Streaming software handles these calls. Unified Origin (which we call USP) fetches the associated video from a AWS S3 bucket. It relies on a server manifest (.ism file), stored with the video, to respond with the video format the client requested: Dash, HLS, etc.\n\nSo, we store a complete video and its server manifest on S3, and USP provides the client with a client manifest and specific chunks: this is Just In Time Packaging (JITP).\n\nAnother way is to compute all the chunks and manifests in advance and write them to S3: this is offline packaging.\nIn this case, once packaging is done, there is no need to do these calculations anymore: it lightens the architecture and avoids the availability challenges of doing real-time computing.\n\n\nComparing Just-In-Time Packaging with Offline Packaging\n\nStill, this causes a big cost problem. On AWS S3, you pay for data access (GET requests), as well as storage. The more you store, the more you pay and the more you access, the more you pay.\n\ni.e, a 90mn video, played in Dash, is cut into 900 chunks plus a single Dash manifest. The same video in HLS, itâ€™s 900 different chunks and another manifest: 1802 files written on S3. Add the Smooth Streaming format and you get 2703 files stored on S3, for a single video.\n\nOffline packaging is interesting, but incompatible with our need to manage a large number of equipments and vast catalogs: tens of thousands of program hours per customer.\n\nAnother approach, which uses the best of the two above solutions, is possible: the CMAF (Common Media Application Format) standard.\nThe player is able to chunk the video itself by adding the HTTP header Range: Bytes.\nMany devices, especially connected TVs or old Android versions, are not compatible with CMAF, which is why the rest of this article will focus on Dash/HLS in JITP.\n\nVersion 1: The quest for self \n\nWe were using USP on-prem. We decided to migrate it to the AWS cloud.\n\nThe goal of the V1 was to quickly provide a platform to our video teams to work on and certify the video players. For Bedrock Ops team, it was a first stepping stone building this platform.\n\nLetâ€™s detail the components of this V1.\n\n\nv1 of our VOD platform\n\n\n  Players send their requests to a CDN.\n  The CDN uses a network load balancer as its origin.\n  The network load balancer forwards requests using a Round Robin algorithm, to multiple AWS EC2 instances we call â€œUSP Originâ€. These EC2 instances are controlled by an AutoScalingGroup and are dynamically scaled based on their network or CPU usage.\n  EC2s retrieve files from the S3 bucket.\n\n\nLocal cache with Nginx \n\nOn EC2 instances, USP runs as a module of Apache HTTPD.\n\nWhen a player requests a specific video chunk, it sends an HTTP request to HTTPD. The USP module it embeds will:\n\n\n  load the according .ism file from S3 (the server manifest)\n  load the video metadata, stored in the first 65KB and the last 15B of a .mp4 file on S3\n  load the specific chunk from the mp4 container, according to the playerâ€™s information: bitrate, language, etc. (still on S3)\n\n\nFor each video chunk called from a player, the USP module does another call to the S3 bucket, loading the same .ism manifest and the same metadata (first 65K and latest 15B).\nTo avoid these calls and reduce S3 costs by 60%, we added Nginx on these EC2s. It goes between HTTPD and S3, to cache the manifest .ism files and metadata of .mp4 video files.\nWeâ€™re using LUA in the Nginx vhost, to cache these 65KB and 15B requests made by USP to the S3 bucket.\n\n\nDetails on the composition of a USP origin\n\nWe use Nginx for caching because we have a solid experience with it, under heavy load, on our on-prem edge servers, which each delivers up to 200Gbps of video traffic. We want to capitalize on this expertise and avoid spreading ourselves thin on multiple tools (e.g, Apache Cache module).\n\nI recommend reading the article published by unified streaming, which uses a similar method: caching via httpd directly.\n\nNetwork Load Balancer: manages TLS and helps with scaling \n\nWeâ€™re using Network Load Balancers to offload TLS. They are cheaper than Application Load Balancers and we donâ€™t need to interact with the HTTP layer: this is not the role of the load balancer, we prefer to keep a KISS principle.\n\nThe major advantage of NLBs is a single entry point (a CNAME domain name), which distributes the load over n EC2 instances. This is essential for auto-scaling: nothing to configure at the CDN level, the load balancer will distribute the load among all Ready instances, whether there are 2 or 1000.\n\nAWS managed load balancers are also interesting because certificates are auto-renewed. Another advantage is that they are distributed over all the availability zones, which was one of our prerequisites in our multi-AZ strategy.\n\nContent Delivery Network: Keep It Simple and Stupid \n\nWeâ€™re using Cloudfront CDN with a basic configuration: we respect standards and use Cache-Control header.\n\nWeâ€™re also using our on-prem Edge servers and other CDNs. Likewise, they all respect the HTTP protocol RFCs and we provide a valid Cache-Control header to be CDN agnostic.\n\nA first conclusion: V2 needs Consistent Hashing \n\nV1 of this platform allowed our video teams to work on new features and new software versions quicker, compared to on-prem. It was also new for Infra teams: we wanted to understand how to scale the platform on AWS to meet our load requirements, making the best use of managed services and auto-scaling, which we did not have on-prem.\n\nWe have identified the problem of version 1 during our tests: the cache is ineffective under heavy loads. The Round Robin algorithm (used by the NLB) is not adequate in front of cache servers because each server will try to cache all the data and will not be specialized to a part of the data. The more requests we have, the more servers we will add and the less each server will have a relevant cache.\n\n\nInefficiency of a Round Robin algorithm in front of cache servers\n\nTo use the cache as much as possible, we need an adapted load balancing method: Consistent Hashing.\n\n\nConsistent Hashing is an ideal method for caches\n\nLast two images are from our recent blog post about doing advanced load balancing at AWS.\n\nWith Consistent Hashing, we can send all requests for the same video to the same cache server. This would optimize the local Nginx cache and reduce S3 costs.\n\nVersion 2: Let the requests flow \n\nV2 will be used in production, with thousands of requests per second: we need it to handle the load, to be reliable and robust.\nAccording to our strong experience with HAProxy, we know that it is able to do Consistent Hashing with Bounded Loads, which is exactly what we need.\n\nWe started by adding HAProxy servers between the load balancer and the USP servers.\n\nHAProxy to make Consistent Hashing \n\nHAProxy is running on EC2 instances, in a dedicated AutoScalingGroup. As with the USP AutoScalingGroup, this one scales with AWS scaling policies: network bandwidth or CPU consumption. We can launch hundreds of HAProxy servers if we need to, and their scaling is independent from the number of USP servers (but they are often linked).\n\n\nv2 of our VOD platform\n\nTo send requests to USP origin, HAProxy needs to know all the healthy EC2 instances running in their AutoScalingGroup.\nWe started by using Consul, to automatically populate our HAProxy backend with these USP servers.\n\nSee the dedicated blog post to know why we preferred to develop a tool dedicated to this task, which we called HAProxy Service Discovery Orchestrator (HSDO).\n\nEC2 costs are reduced by using only Spot instances \n\nIn addition, HSDO is very responsive to movements in the AutoScalingGroup, which allowed us to replace all EC2 On Demand instances with Spot instances.\nAnd by all instances, I mean all USP servers (with cache), as well as HAProxy servers: 70% reduction in server costs.\n\nNote that replacing USP origins with Spot instances has almost no impact on the cache, as we follow the AWS best practices for Spot: use many different instance types, be multi-AZ and use the â€œCapacity Optimizedâ€ strategy. This way we observe very few reclaims, which translates to a longer cache life.\n\nProduction launch on this V2 \n\nThe production launch of this V2 has confirmed the stability and performance of the platform. We were happy to see that our objectives were met, so we started migrating all our VOD content from on-prem to the cloud, video by video, client by client.\n\n\nNginx cache hit ratio\n\nWith Consistent Hashing, the cache becomes quite efficient and we saved 62% of calls to S3.\n\nIn addition, the cache speeds up the video packaging and we have reduced the overall origin response time. Win-win.\n\nEC2-other: the financial abyss \n\nEC2-other, in this case, means network traffic between Availability Zones.\nThe private network between two data centers (AZs) at AWS is re-billed and accounted for 48% of the bill for our VOD platforms at the time.\n\n\nAWS Cost Explorer, October 2020\n\nWhen HAProxy servers sent/received traffic from USP servers, the latter might not be in the same Availability Zone and the traffic between the two was charged at full price.\n\nIt was necessary to quickly find a solution for these costs which torpedoed the project. We started the creation of version 3 as soon as we had these metrics from version 2, at the beginning of our VOD content migration.\n\nVersion 3: Cost Explorer Driven Development \n\nThe idea is to do as well for half the price.\n\nBe multi-AZ without inter-AZ traffic \n\nWe have updated HSDO so each HAProxy only sends requests to USP origins of the same AZ.\nAnd the Network Load Balancers still send traffic to the HAProxys, on multiple AZs.\n\nRemoving inter-AZ traffic was not that much work and we quickly saw the difference: 45% cost savings.\n\n\nThe costs of this platform, where v3 was deployed in mid-November 2020\n\nWe can see on the picture above that the EC2-other costs (in orange) have disappeared in December.\n\nThe work on version 3 began shortly after the start of our cloud migration.\nWe were still migrating on-prem to the cloud when we put V3 on prod. Thatâ€™s why you can see all costs have increased from October to December: weâ€™ve doubled the number of viewers during the period.\n\nMono-AZ AutoScalingGroups \n\nWe have also replaced the multi-AZ AutoScalingGroups by several mono-AZ ones. It gives us a finer scaling that corresponds to the real needs in each AZ. The randomness of the round robin and client requests means that, from time to time, one AZ receives a significantly higher load than another.\n\n\nv3 of our VOD platform\n\nSince the NLBs are using a Round Robin algorithm, each HAProxy can receive traffic for any video. Now that the HAProxy servers of an AZ only send traffic to the USP origins of the same AZ, everything that is cached exists in as many copies as we have configured of AZs.\nIt makes us all the more resilient to an AZ failure.\n\nOptimizations \n\nSince V3, we have not made any major architectural changes. However, some optimizations were necessary.\n\nAdapt HAProxy config for EC2 bandwidth throttling \n\nOn AWS, an EC2 instance has a baseline network capacity and a burst capacity (see UserGuide).\nBaseline capacity is the network bandwidth you can consume all the time.\n\nThe Burst capacity is what you may be able to consume temporarily before being throttled to the baseline capacity.\nIn the EC2 presentation, the value â€œUp toâ€ refers to the burst.\n\nLess visible in the EC2 documentation, one can find the baseline capacity for each instance type (which is public knowledge since July 2021).\n\nFor example, c5.large instances have a network bandwidth Up to 10Gbps (burst) but only 0.75Gbps baseline bandwidth.\n\nTroubles start when HAProxy sends a little more traffic to one instance than to the others: USP originâ€™s bandwidth may be throttled at some pointâ€¦ And we will observe poor performance or even service interruptions of this server.\n\n\nA server whose bandwidth is throttled (seen from CloudWatch)\n\nWe added observe layer7 to the default-server in our HAProxy backends, to remove servers returning HTTP error codes (5xx) from its load-balancing.\n\nWe also added the retry and redispatch options, which allow to retry a request sent to an unhealthy server on a healthy server. Itâ€™s not optimal for the cache, but what matters is that a clientâ€™s request is successfully answered.\n\nWe observed that with throttled bandwidth, the connection time from HAProxy to a USP server increases dramatically.\nSo weâ€™ve also reduced timeout connect to 20 milliseconds.\n\nHighlights of our HAProxy configuration:\n\ndefaults\n   timeout connect     20ms\n   retries     2\n    # We do not use &quot;all-retryable-errors&quot; because we don&#39;t want to retry on 500,\n    # which is an USP expected error code when it goes wrongly\n   retry-on     502 503 504 0rtt-rejected conn-failure empty-response response-timeout\n   option     redispatch\n   timeout server     2s\n   default-server     inter 1s fall 1 rise 10 observe layer7\n\n\nNow, if a USP origin throttles on its network bandwidth or if there is a degradation of service, HAProxy will immediately redispatch the request to another server.\n\nWe are working on adding an agent-check, so that the weight of the servers in HAProxy can be directly defined by an USP origin, if it detects that its bandwidth is throttled.\n\nAdjust the hash balance factor to correctly trigger scaling \n\nOur scaling depends on the average server utilization in an AutoScalingGroup. If a few servers are overloaded but the majority is not doing anything, we don&#39;t scale.\n\nBut all contents on our platforms are not equally popular. This affects Consistent Hashing which would result in few servers receiving way more traffic than others. Few servers would be overloaded and the majority would not do much.\n\nHere is an example in a load test:\n\n\nGraph showing few overloaded servers, using classic Consistent Hashing\n\nWe want to benefit from Consistent Hashing while being able to scale on average consumption.\nThis is what Consistent Hashing with Bounded Loads allows: to benefit from Consistent Hashing, while balancing load.\n\nThe Bounded Loads are controlled by the hash-balance-factor option in HAProxy.\nAccording to the doc:\n&amp;lt;factor&amp;gt; is the control for the maximum number of concurrent requests to\n         send to a server, expressed as a percentage of the average number\n         of concurrent requests across all of the active servers.\n\n\nWe played the same load test, once using classic Consistent Hashing, a second time using bounded loads:\n\n\nGraph showing the effects of Bounded Loads over Consistent Hashing\n\nWe did dozens of load tests before finding the best value for our use: 140.\nFor each load test, we looked at the evolution of:\n\n\n  Nginx Cache Hit Ratio\n  Number of requests to S3\n  HAProxy Backend retries and redispatches\n  HAProxy backend 5xx response codes\n  Free disk space on USP origins\n\n\nOur configuration of the Consistent Hashing with Bounded Loads remains simple:\n\nbackend usp-servers-AZ-C\n    balance hdr(X-LB)\n    hash-type consistent sdbm avalanche\n    hash-balance-factor 140\n\n\nThanks to Consistent Hashing with Bounded Loads, our cache is optimized without impacting our autoscaling.\nThere may be contents much more solicited than others, the load will be balanced and our autoscaling will be activated.\n\nConclusion \n\nWe migrated our video delivery to the cloud, moving from static servers to an end-to-end auto-scaling and multi-AZ infrastructure. We are now able to handle very high loads, which we could not do on-premise.\nWe had the opportunity to review our architecture three times, within a few weeks of each other, even though the migration had begun.\n\nThe v3 is not perfect, but it is quite well optimized, reliable and scalable.\n\nWe are thinking about V4 and saving 20% of the costs by removing the NLB. We also identified some possible improvements, adding cache on HAProxy for example, or using HAProxy Agent Check so that the weight of the servers in HAProxy is driven directly by the servers, using the Amazon metrics on network performances. Another promising performance improvement could be to use HAProxy on ARM as Graviton type instances offer significant discounts, it will be worth testing.\n\nIn parallel, we also invest time on CMAF which is for us, the long-term objective.\n\n\n\nSpecial thanks to all my colleagues at Bedrock Streaming and members of Unified-streaming, for re-re-re-and-rereading this blog post. â¤ï¸\n"
} ,
  
  {
    "title"    : "More efficient Load Balancing and Caching at AWS, using Consistent Hashing and HAProxy",
    "category" : "",
    "tags"     : " aws, cloud, sysadmin, HAProxy, video, opensource, high availability",
    "url"      : "/2021/11/18/hsdo.html",
    "date"     : "November 18, 2021",
    "excerpt"  : "AWS ALB &amp;amp; NLB currently supports Round-Robin (RR) and Least Outstanding Requests (LOR) balancing algorithms. But what happens when you try to load balance cache servers with these algorithms? How to implement an effective cache in Cloud at sca...",
  "content"  : "AWS ALB &amp;amp; NLB currently supports Round-Robin (RR) and Least Outstanding Requests (LOR) balancing algorithms. But what happens when you try to load balance cache servers with these algorithms? How to implement an effective cache in Cloud at scale?\n\nContext\nAt Bedrock we use both ALB &amp;amp; NLB for different use-cases (like in front of our Kubernetes clusters) in our platforms. For our last VOD platform, we needed to be able to load balance heavy content (videos) to our cache servers. We knew that the balancing algorithm was a key factor for our cache in Cloud at scale, and that ALB &amp;amp; NLB wonâ€™t be sufficient to achieve our goals.\n\nBalancing algorithms\n\nRound-Robin is a widely used balancing algorithms.\n\n\nRound robin or Least Outstanding Requests algorithms\n\nThe load balancer cycles through cache servers sequentially, so each cache server should receive an equal share of requests. Each cache server has to possibly store every requested object (â™ ï¸, â™¥ï¸, â™¦ï¸, and â™£ï¸ represent different objects).\n\nWith Least Outstanding Requests, load balancers send requests to the cache server with least awaiting requests. The cache server still has to store every requested object.\n\nConsistent Hashing is a very interesting balancing algorithm for caching purposes.\n\n\nConsistent Hashing algorithm\n\nThis algorithm allows to distribute the load so that all requests for the same object will always go to the same cache server. This way, each cache server has to store half of the objects.\n\nAnd more cache servers means more load balancing between the servers.\n\n\nConsistent Hashing at scale\n\nWhile at scale, Round-Robin or Least Outstanding Requests will look like this:\n\n\n\nRound Robin or Least Outstanding Requests at scale\n\nCache servers are not efficient at scale with these algorithms. There is a higher chance for the cache to miss, because all servers do not store all objects. Every time a cache expires or cache server boots, objects need to be cached again to be hit. You also need more resources, as the same object needs to be cached on all cache server disks.\n\nWith Consistent Hashing, once an object has been cached you have a greater chance for the cache to hit. And you save money by using smaller disks on cache servers and reducing network bandwidth.\n\nHAProxy implementation\n\nConsistent Hashing at AWS is not available with ALB, ELB and NLB. You need to implement it yourself.\n\nTo do this, we chose to use HAProxy.\n\nHAproxy is fast and reliable. We use it often, we know it well, and it can use consistent hashing.\n\nThis is how we architected it.\n\n\nLoad Balanced Cache Architecture\n\nHAProxy servers and Cache servers are deployed with Auto Scaling Groups (ASG). A Target Group is registering HAProxy ASG instances so NLB will load balance between them. Having separated ASG for HAProxy and Cache allows it to have dedicated automatic scaling and management.\n\nWe need to implement something for HAProxy so it could discover and register Cache ASG instances. And there is one thing important to do consistent hashing with HAProxy: a centralized and consistent configuration.\n\nCentralized configuration\n\nAll HAProxy instances need to have the same configuration with the same list of servers, or requests will be split differently depending on HAproxy instances.\n\n\nConsistent Hashing with different list of servers per HAProxy\n\nYou have a higher chance to have a miss on your cache request as a single object may be at different locations depending on the HAproxy instance.\nWith a centralized configuration, you can be sure that each HAProxy instance will request the same cache server for the same object.\n\nConsistent configuration\n\nHAProxy consistent hashing is based on backend server IDs. These IDs match the position of the server in the backend server list. For example, this HAProxy configuration:\n\nbackend cache\n   server CacheA 192.168.0.1:80 #ID = 1\n   server CacheB 192.168.0.2:80 #ID = 2\n   server CacheC 192.168.0.3:80 #ID = 3\n   server CacheD 192.168.0.4:80 #ID = 4\n\n\nwill be seen as the following:\n\n\n\nTo keep consistent hashing efficient, cache servers need to change ID rarely. HAProxy backend server list must be consistent across all HAProxy instances.\n\nIf CacheC is removed, configuration has to be like:\n\nbackend cache\n   server CacheA 192.168.0.1:80          #ID = 1\n   server CacheB 192.168.0.2:80          #ID = 2\n   server CacheC 192.168.0.3:80 disabled #ID = 3\n   server CacheD 192.168.0.4:80          #ID = 4\n\n\nCacheC backend server is now disabled until another Cache server takes its place.\n\n\n\nWith consistent configuration: â™ ï¸, â™£ï¸ and â™¥ï¸ requests are always balanced to the same cache servers, while â™¦ï¸ requests are balanced to another available cache server.\nWithout consistent configuration: all requests could be rebalanced to other cache servers. This would mean that for each cache server scale up or down, we no longer have the cached objects: we MISS the cache. This would be inefficient: we would lose the advantage of the cache.\n\nSolutions\n\nConsul\n\nAt first, we started to configure HAproxy through Consul. We already used it at BedRock, and an article gave us hope to quickly achieve what we wanted. \nConsul Service Discovery with DNS wonâ€™t provide sorted/consistent DNS records by design. We canâ€™t have a consistent configuration with it.\n\nAnother way of doing so would be to use consul-template for generating backends and registering servers into an HAProxy configuration file. With this approach, we would reload systemd to add new servers to HAProxy.\n\nBut HAProxy Runtime API is the recommended way to make frequent changes on configuration, service reloads are not considered safe.\n\nAWS EC2 Service Discovery\n\nHAProxy also released a new functionality called AWS EC2 Service Discovery in July 2021. We havenâ€™t tested it yet, but it lacks the possibility to keep a consistent list of servers between HAProxy instances, which isnâ€™t good for consistent hashing as discussed before. We opened an issue on HAProxy dedicated Github repository.\n\nAdded to the fact that we were starting to think that Consul was overkill for our needs, we start to implement our own solution.\n\nHAProxy Service Discovery Orchestrator\n\nWhat we wanted to achieve was to use maximum managed service from AWS, meet our standard of stability and resilience, and keep things simple. We choose Python with boto3 to implement our solution as it is one of our teamâ€™s favorite languages.\n\nHAProxy Service Discovery Orchestrator (or HSDO) is open-source.\n\nHSDO is composed of a server and a client.\n\nHSDO Server\n\nHSDO Server runs in standalone. It could be a Lambda, but we were more comfortable with system processes when we designed it.\nIts job is to keep track EC2 instances of one or multiple Cache ASGs and update a list accordingly.\nHSDO server provides a consistent sorted list of instances. Every time a new cache server appears in the ASG, it is added to the list at a given ID that will never change.\nThis list is stored in DynamoDB.\n\n\nDynamoDB Items View\n\nHSDO Client\n\nHSDO client is reading the DynamoDB table to get the cache servers. The client run on the same instance as HAProxy and use the runtime API to update HAProxy config.\n\n\nHAProxy Status Page\n\nBrown lines are disabled servers, while green lines are servers stored in DynamoDB as seen above.\n\n\nHSDO in Load Balanced Cache Architecture Schema\n\nWith this architecture, we achieve a centralized and consistent configuration to make consistent hashing work at scale for cache servers.\n\nConclusion\n\nWe have been using HSDO since September 2020. We are distributing VOD content for Salto and 6play streaming platforms and are able to handle at least 10.000 requests/s. This wasnâ€™t possible without a few improvements (on platform cost, timeout funnels, â€¦) and this will be presented in another post, so keep in touch. ;)\n\nSpecial thanks to all Ops team members in BedRock Streaming for re-re-re-and-rereading this blog post.\n"
} ,
  
  {
    "title"    : "Forum PHP 2021 - L&#39;Ã©dition des retrouvailles",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2021/11/02/forum-php-2021.html",
    "date"     : "November 2, 2021",
    "excerpt"  : "Cette annÃ©e encore, Bedrock participait au Forum PHP oÃ¹ Ã©tait proposÃ© une grande diversitÃ© de confÃ©rences.\nDes sujets techniques et dâ€™autres, plus gÃ©nÃ©riques, Ã©taient abordÃ©s : Symfony 6, Git, environnement, sous-reprÃ©sentation des femmes dans lâ€™i...",
  "content"  : "Cette annÃ©e encore, Bedrock participait au Forum PHP oÃ¹ Ã©tait proposÃ© une grande diversitÃ© de confÃ©rences.\nDes sujets techniques et dâ€™autres, plus gÃ©nÃ©riques, Ã©taient abordÃ©s : Symfony 6, Git, environnement, sous-reprÃ©sentation des femmes dans lâ€™informatiqueâ€¦\nAvec Sofia LESCANO, Benoit VIGUIER sur les planches et une quinzaine de participantes et participants dans le publique, lâ€™occasion de rencontrer Ã  nouveau la communautÃ© PHP en chair et en os a Ã©tÃ© saisie avec une certaine impatience.\n\nPlusieurs confÃ©rences ont retenu notre attention et auront un impact Ã  court terme sur nos projets :\n\nSuite Ã  la confÃ©rence â€œLes exceptions : le trou dans la raquette du typageâ€ de Baptiste LANGLADE, la bonne gestion des exceptions nous semble primordiale. Nos Ã©quipes sont donc en train de tester le bundle Innmind/Immutable pour mettre en place le pattern Monad (Maybe et Either) afin dâ€™amÃ©liorer la gestion dâ€™absence de donnÃ©es Ã  diffÃ©rents niveaux de nos outils.\n\nLa confÃ©rence â€œDes tests unitaires pour nos rÃ¨gles de conceptionâ€ de FrÃ©dÃ©ric BOUCHERY mettait en lumiÃ¨re lâ€™importance de documenter, expliciter et tester les rÃ¨gles de conception dâ€™un projet. La mise en place dâ€™ADR (Architectural Decision Records) et des tests unitaires associÃ©s est une bonne pratique que nous souhaitons dÃ©velopper au sein des Ã©quipes. Nous attendons avec impatience le bundle que Klaxoon devrait bientÃ´t open-sourcer: il permettra de tester facilement nos rÃ¨gles de conceptions avec PHPUnit et dâ€™automatiser une partie de la revue technique.\n\nAnne-Laure DE BOISSIEU et AmÃ©lie DEFRANCE ont rappelÃ© quelques rÃ¨gles fondamentales dâ€™accessibilitÃ© pour nos sites Internet pendant â€œAccessibilitÃ© et SEO : et si on relevait le niveau ?â€. Nous espÃ©rons dÃ©sormais amÃ©liorer lâ€™accessibilitÃ© de notre back-office. Par exemple : retravailler le contraste couleur de certains Ã©crans ou ajouter du contenu dans nos balises HTML pour faciliter la comprÃ©hension.\n\nPendant leur confÃ©rence â€œKairoi, et PHP se rÃ©concilie avec les tÃ¢ches planifiÃ©esâ€,  Emeric KASBARIAN et JÃ©rÃ©my JAMES nous ont expliquÃ© que leurs clients ont un mÃªme besoin : â€œDÃ©clencher une action automatique Ã  un moment prÃ©cis, sans aucune limite dans le tempsâ€. Par exemple : â€œsupprimer, automatiquement, un panier dâ€™achat au bout de 15 minutesâ€.\nAprÃ¨s de longues recherches, il nâ€™existe rien sur le marchÃ© pour rÃ©pondre Ã  un tel besoin. Et nous, BedRock, confirmons : nous avons le mÃªme besoin et nâ€™avons rien trouvÃ© non plus.\nKairoi est donc nÃ©. Câ€™est une application serveur Rust de planification de tÃ¢ches, avec son propre protocole (inspirÃ© de celui de Redis) qui permet de :\n\n  RÃ©cupÃ©rer des Ã©vÃ¨nements Ã  planifier\n  ConnaÃ®tre lâ€™Ã©tat dâ€™un lâ€™Ã©vÃ¨nement\n  Le dÃ©clencher au moment opportun\n    \n      soit sur un protocole AMQP\n      soit sur un shell\n    \n  \n\n\nLe Forum PHP est lâ€™occasion de parler de sujets pointus techniquement, mais aussi une occasion dâ€™Ã©change et de partage autour de sujets plus transversaux.\nNotre domaine, lâ€™IT, comme bien dâ€™autres, est sensible au sujet de lâ€™Ã©cologie. Deux axes de rÃ©flexion ont Ã©tÃ© Ã©voquÃ©s pendant deux confÃ©rences.\n\nEn ouverture, FranÃ§ois ZANIOTTO nous a partagÃ© avec entrain sa recherche de mesures fiables des dÃ©penses Ã©nergÃ©tiques. Elle lâ€™a menÃ©e Ã  dÃ©velopper  GreenFrame, un outil en cours de construction chez Marmelab, qui a pour but de cibler au plus prÃ¨s la consommation Ã©nergÃ©tique afin de tendre â€œVers la sobriÃ©tÃ© numÃ©riqueâ€.\n\nHÃ©lÃ¨ne MAITRE-MARCHOIS a sÃ» mettre en perspective le rÃ´le de chaque dÃ©veloppeuse et dÃ©veloppeur en insistant sur le fait que la responsabilitÃ© du dÃ©rÃ¨glement climatique nâ€™est pas forcÃ©ment lÃ  oÃ¹ on lâ€™attend. Avec â€œComment sauver la planÃ¨te en ne faisant rienâ€, elle entend faire prendre conscience que si, la production et consommation de contenu reprÃ©sentent des pÃ´les sur lesquels en tant que tech, nous pouvons agir. Le renouvellement du parc reste une cause prÃ©pondÃ©rante dans lâ€™impact Ã©cologique.\nLe renouvellement accÃ©lÃ©rÃ© par le foisonnement de nouvelles fonctionnalitÃ©s, trop souvent inutiles, est une obsolescence programmÃ©e.\nUtile, Accessible, Durable. VoilÃ  trois notions simples qui peuvent, pourtant, nous permettre de faire la diffÃ©rence.\n\nNicolas GREKAS, principal engineer Symfony, nous a parlÃ© de lâ€™Ã©cosystÃ¨me de ce framework Ã  travers de sa confÃ©rence â€œSymfony 6 : le choix de lâ€™innovation et de la performanceâ€. Il nous a prÃ©sentÃ© le calendrier de livraisons et de maintenance des diffÃ©rentes versions, avec la sortie dâ€™une nouvelle majeure prÃ©vue tous les deux ans. Chaque majeure voit la suppression du code dÃ©prÃ©ciÃ© dans la version prÃ©cÃ©dente. Par exemple, Symfony 6 est un Symfony 5.4 sans ses dÃ©prÃ©ciations. Les derniÃ¨res versions avant une majeure (comme la 4.4 ou 5.4) sont assurÃ©es dâ€™avoir un support Ã  long terme.\nPour les prochaines versions de Symfony, lâ€™accent est mis sur la compatibilitÃ© avec PHP8. Puisque la majoritÃ© du travail consiste Ã  remplacer les annotations @return par un typage natif, Nicolas a parlÃ© de lâ€™outil patch-type-declarations qui automatise cette tÃ¢che.\n\nPour finir cette sÃ©rie de confÃ©rences, nous avons suivi lâ€™incroyable histoire de WorkAdventure, lors de â€œWorkAdventure de la genÃ¨se Ã  aujourdâ€™hui : Retour dâ€™expÃ©rience sur 1 an dâ€™univers virtuelsâ€ prÃ©sentÃ© par David NÃ‰GRIER) : le rÃ©sultat dâ€™un hackathon fait pour pallier lâ€™ennui des confinements, qui est devenu le support dâ€™Ã©vÃ©nements majeurs lâ€™annÃ©e derniÃ¨re.\n\n\n\nLes speakers Bedrock\nLors de cette Ã©dition, deux Bedrockers ont eu lâ€™opportunitÃ© de prÃ©senter un sujet, lâ€™occasion pour nous de demander Ã  Sofia LESCANO â€œFaites confiance aux dÃ©veloppeurs.euses de votre Ã©quipe : voyez plus loin que les fonctionnalitÃ©sâ€ et Benoit VIGUIER â€œFiber : la porte ouverte sur lâ€™asynchroneâ€ comment ils ont vÃ©cu cet Ã©vÃ©nement.\n\nComment vous est venue lâ€™idÃ©e de soumettre un sujet de confÃ©rence, et comment avez-vous abordÃ© sa prÃ©paration ?\n\nSofia: Cela faisait longtemps que jâ€™avais ce sujet en tÃªte, et lâ€™idÃ©e de refaire des Ã©vÃ©nements en prÃ©sentiel mâ€™a fait me lancer. Pour moi les tech meetings Ã©taient une grande dÃ©couverte et un rituel que jâ€™apprÃ©cie vraiment et je voulais partager cela avec la communautÃ©. Pour la prÃ©paration, jâ€™ai Ã©tÃ© accompagnÃ©e par Matthieu Napoli avec le programme de mentoring de lâ€™AFUP et par mes collÃ¨gues de Bedrock.\n\nBenoit: Le PHP asynchrone est un sujet qui mâ€™occupe beaucoup Ã  Bedrock, jâ€™ai donc suivi attentivement la RFC Fiber. Lâ€™idÃ©e dâ€™en faire un sujet de confÃ©rence est venue en me rendant compte que, mÃªme au sein de nos Ã©quipes, il nâ€™Ã©tait pas Ã©vident pour tout le monde de comprendre tout ce que cet outil pouvait changer. Et puis, refaire un Ã©vÃ©nement en prÃ©sentiel me manquait vraiment ! La prÃ©paration de ce format court Ã©tait nouveau pour moi, heureusement jâ€™ai pu faire quelques rÃ©pÃ©titions Ã  Bedrock et Ã  lâ€™AFUP Lyon pour bien ajuster mon timing.\n\nMaintenant que lâ€™Ã©vÃ©nement est derriÃ¨re nous, que retenez-vous de cette expÃ©rience ?\n\nSofia: Câ€™Ã©tait une trÃ¨s belle expÃ©rience et les Ã©changes que jâ€™ai pu avoir suite Ã  ma confÃ©rence ont Ã©tÃ© trÃ¨s intÃ©ressants. Câ€™est trÃ¨s enrichissant dâ€™Ã©changer avec la communautÃ© et de voir que des pratiques similaires ont lieu ailleurs et pouvoir les enrichir dans les deux sens.\n\nBenoit: Câ€™Ã©tait un vrai plaisir de pouvoir Ã©changer avec de vraies personnes, sans Ã©crans interposÃ©s ! CÃ´tÃ© speaker, lâ€™organisation Ã©tait au top et jâ€™ai eu pleins dâ€™Ã©changes prometteurs sur le potentiel des Fibers. CÃ´tÃ© confÃ©rences, jâ€™ai vu plein de choses intÃ©ressantes, Ã§a donne toujours matiÃ¨re Ã  rÃ©flÃ©chir, que lâ€™on partage le point de vue exposÃ© ou non. Merci encore Ã  lâ€™AFUP pour avoir mis toute cette Ã©nergie au service dâ€™un si bel Ã©vÃ©nement.\n\nLe forum, particuliÃ¨rement cette Ã©dition en prÃ©sentiel, câ€™est retrouver toute une communautÃ© qui partage la mÃªme passion. Encore merci aux confÃ©renciÃ¨res et confÃ©renciers, merci aux organisatrices et organisateursâ€¦ Et Ã  lâ€™annÃ©e prochaine !\n"
} ,
  
  {
    "title"    : "Fiber: the open door to asynchronous",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2021/10/21/fiber-the-open-door-to-async.html",
    "date"     : "October 21, 2021",
    "excerpt"  : "Parmi les nouveautÃ©s apportÃ©es par Php 8.1, les Fibers tiennent une place particuliÃ¨re. Il sâ€™agit certainement dâ€™une fonctionnalitÃ© qui aura un impact majeur sur lâ€™Ã©cosystÃ¨me Php, tout en ayant un impact mineur sur le code que vous Ã©crivez tous le...",
  "content"  : "Parmi les nouveautÃ©s apportÃ©es par Php 8.1, les Fibers tiennent une place particuliÃ¨re. Il sâ€™agit certainement dâ€™une fonctionnalitÃ© qui aura un impact majeur sur lâ€™Ã©cosystÃ¨me Php, tout en ayant un impact mineur sur le code que vous Ã©crivez tous les jours. Les Fibers sont comme des gÃ©nÃ©rateurs amÃ©liorÃ©s, des fonctions interruptibles, mais qui peuvent sâ€™imbriquer de maniÃ¨re transparente avec dâ€™autres fonctions. Il est donc enfin possible de crÃ©er des fonctions similaires Ã  await et async pour rendre la programmation asynchrone moins intrusive dans notre code et permettre la compatibilitÃ© avec les frameworks existants. Voici une introduction Ã  ces nouveaux concepts, ainsi que des exemples concrets de ce que cela permettra dans lâ€™Ã©cosystÃ¨me Php.\n"
} ,
  
  {
    "title"    : "Faites confiance aux dÃ©veloppeurs.euses de votre Ã©quipe : voyez plus loin que les fonctionnalitÃ©s",
    "category" : "",
    "tags"     : " conference, afup, php",
    "url"      : "/2021/10/21/confiance-aux-devs-de-votre-team.html",
    "date"     : "October 21, 2021",
    "excerpt"  : "Deadlines, besoins produit, pression forte et fonctionnalitÃ©s Ã  livrer : nos projets ont besoin de nous ! Lâ€™amÃ©lioration du quotidien se perd dans un second plan, alors quâ€™elle a un impact majeur sur lâ€™augmentation de notre productivitÃ© et la qual...",
  "content"  : "Deadlines, besoins produit, pression forte et fonctionnalitÃ©s Ã  livrer : nos projets ont besoin de nous ! Lâ€™amÃ©lioration du quotidien se perd dans un second plan, alors quâ€™elle a un impact majeur sur lâ€™augmentation de notre productivitÃ© et la qualitÃ© et maintenabilitÃ© de notre code.\n\nConstatant que nous voulions augmenter notre confort de travail, nous avons, depuis plus dâ€™un an, mis en place des rÃ©unions techniques bi-hebdomadaires pour prendre le temps de discuter de notre plateforme et nos outils, au-delÃ  des fonctionnalitÃ©s. Chaque membre de lâ€™Ã©quipe contribue ainsi Ã  amÃ©liorer son expÃ©rience de travail et notre produit.\n\nÃ€ travers notre vÃ©cu, nos erreurs et des exemples techniques concrets, repensez vous aussi au dÃ©veloppement de votre produit.\n"
} ,
  
  {
    "title"    : "Increase performance and stability by adding an Egress Controller in a Kubernetes cluster at AWS",
    "category" : "",
    "tags"     : " php, aws, cloud, performance, sysadmin, kubernetes, HAProxy",
    "url"      : "/2021/10/18/increase-performance-and-stability-by-adding-an-egress-controller.html",
    "date"     : "October 18, 2021",
    "excerpt"  : "Introduction\n\nWe recently encountered issues with our PHP applications at scale in our Kubernetes clusters at AWS. We will explain the root cause of these issues, how we fixed them with Egress Controller, and overall improvements. We also added a ...",
  "content"  : "Introduction\n\nWe recently encountered issues with our PHP applications at scale in our Kubernetes clusters at AWS. We will explain the root cause of these issues, how we fixed them with Egress Controller, and overall improvements. We also added a detailed configuration to use HAProxy as Egress Controller.\n\nContext\n\nBedrock is using PHP for almost all of the backend API of our streaming platforms (6Play, RTLMost, Salto, â€¦). We have deployed our applications in AWS on our kops-managed Kubernetes clusters. Each of our applications is behind a CDN for caching purposes (CloudFront, Fastly). This means every time an application needs to access another API, requests go on the internet to access the latter through CDN.\n\nDuring special events with huge loads on our platforms, we started to see TCP connection errors from our applications to the outside of our VPC.\n\nErrorPortAllocation source\n\nAfter a few investigations, we saw that TCP connection errors were correlated with NAT Gateways ErrorPortAllocation.\n\n\nSome loadtesting on our platform, which you may see as no traffic, huge traffic, then no traffic again\n\nIn AWS, NAT Gateways are endpoints allowing us to go outside our VPC. They have hard limits that canâ€™t be modified:\n\n  A NAT gateway can support up to 55,000 simultaneous connections [â€¦]. If the destination IP address, the destination port, or the protocol (TCP/UDP/ICMP) changes, you can create an additional 55,000 connections. For more than 55,000 connections, there is an increased chance of connection errors due to port allocation errors. AWS Documentation\n\n\nOur applications always request the same endpoints: other APIs CDN. Destination port, IP or protocol doesnâ€™t change that much, so we start hitting max connections, resulting in ErrorPortAllocation.\n\nAt the same time, we found a very interesting blog post: Impact of using HTTP connection pooling for PHP applications at scale, which was a very good coincidence.\n\nAs you can read in Wikimediaâ€™s post, PHP applications arenâ€™t able to reuse TCP connections, as PHP processes are not sharing information from a request to another. Recreating new connections on the same endpoints is inefficient: adds latency, wastes CPU (TLS negotiation and TCP connection lifecycle) but also overconsumes TCP connections.\n\n\nPHP application calls another API on internet through the NAT gateway\n\nOutgoing requests optimization\n\nEgress Controller\n\nHAproxy is fast and reliable. We use it often and know it well. We already have it as Ingress Controller in our clusters and we know service mesh needs time to be production-ready. So we thought a service mesh might be overkill in our case and we tried to add HAProxy as Kubernetes Egress Controller in our clusters.\n\n\nOutgoing requests go through the Egress Controller, which pools and maintains TCP and TLS connections\n\nWe configured some applications to send a few outgoing requests to Egress Controller. The latter was configured to do TCP re-use and to forward to desired endpoints.\n\nEffects\n\nWith this optimization, we donâ€™t encounter ErrorPortAllocation anymore. Requests duration are reduced by 20 to 30%, and apps are consuming less CPU. Ressources were spent to instantiate a new TLS connection, which is now handled by Egress Controller.\n\n\nApplication consumes less CPU, because Egress Controller is responsible of TLS and TCP connections to the outside world, which consumes a lot of resources\n\nDetailed configuration\n\nWe generally prefer to use what already exists rather than starting from scratch, so we tried to see if HAProxy Kubernetes Ingress Controller could be used as egress.\n\nHAProxy Ingress Controller loads its frontend domains in Ingress resource, and loads backend servers in the associated Service resource. To inject an external domain as a backend server, we have to use Service ExternalName.\n\n\n\nTo use HAProxy Kubernetes Ingress Controller as an Egress Controller, we will use Ingress Kubernetes resource as Egress to define domains handled by the Controller.\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app1\nspec:\n  type: ExternalName\n  externalName: app1.example.com\n  ports:\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: app1\nannotations:\n    haproxy.org/server-ssl: &quot;true&quot;\n    haproxy.org/backend-config-snippet: |\n      # See this article for the deep reasons of both parameters: https://www.haproxy.com/fr/blog/http-keep-alive-pipelining-multiplexing-and-connection-pooling/\n      # enforce SNI with the Host string instead of the &#39;Host&#39; header, because HAProxy cannot reuse connections with a non-fixed Host SNI value.\n      default-server check-sni app1.example.com sni str(app1.example.com) resolvers mydns resolve-prefer ipv4\n      # make HAProxy reuse connections, because the default safe mode reuses connections only for the same source.ip\n      http-reuse always\n\nspec:\n  rules:\n  - host: app1.example.com\n    http:\n      paths:\n      - backend:\n          serviceName: app1\n          servicePort: 443\n\n\nWhen everything is ready, you will be able to send requests:\n\ncurl -H &quot;host: app1.example.com&quot; https://haproxy-egress.default.svc.cluster.local/health\n\n\nBy default, HAProxy resolves domain names only at bootime. But it can be configured to resolves during runtime by adding a config snippet to Egress Controller configuration:\n\nglobal-config-snippet: |\n  resolvers mydns\n    nameserver local &amp;lt;MY_DNS&amp;gt;:53\n\n\nConclusions\n\nIt seems surprising to reduce requests latency by adding a hop in a network. But it does really work, even if it has some limits.\n\nThe main problem with this approach is the fact that we are effectively creating a Single Point of Failure in our clusters if we choose to send all our egress traffic through it. Instead, we are carefully selecting applications that should use an Egress Controller to refine the configuration little by little. Some applications are tightly tied to external services and would massively gain from this and others would only be less resilient.\n"
} ,
  
  {
    "title"    : "How did we live stream our Last Friday Talks?",
    "category" : "",
    "tags"     : " lft, talks, live, stream, obs",
    "url"      : "/2021/10/14/live-streaming-lft.html",
    "date"     : "October 14, 2021",
    "excerpt"  : "At Bedrock, the last Friday of every other month1 is Last Friday Talk â€“ or LFT.\n\nThis event encourages sharing: technical topics, less technical topics, cross-team topicsâ€¦\nWe also take the opportunity to meet and talk with colleagues we donâ€™t work...",
  "content"  : "At Bedrock, the last Friday of every other month1 is Last Friday Talk â€“ or LFT.\n\nThis event encourages sharing: technical topics, less technical topics, cross-team topicsâ€¦\nWe also take the opportunity to meet and talk with colleagues we donâ€™t work with daily. And the post-LFT snack, put aside during COVID but which I hope will come back, is also meant for that!\nFinally, these LFTs are an excellent opportunity for beginner speakers to practice in front of a friendly audience. We also use them to rehearse and validate talks we will give in public later.\n\nDuring the COVID full-remote period, we moved our LFTs to Google Live Stream, where each speaker shared their screen and showed their face through the camera.\nWith the return to the office2 and partial telecommuting, a new problem arose: how can we broadcast the talks given in our auditorium to 150+ remote colleagues? In good quality, to encourage people to attend several talks in a row? While remaining enjoyable and lively on-site?\n\nThe goal: to broadcast live\n\nWe can seat up to 70 people in our amphitheater, where most of our speakers were giving their talks3. And we were aiming for 100 to 200 people remotely, for whom we wanted to broadcast live4.\n\nFor the broadcast, we worked with one of our usual tools: the organizers and speakers broadcast in a Google Meet, and the audience follows the stream via Google Live Stream5. This solution supports a large number of participants, and access is filtered through our SSO, ensuring that only our colleagues have access to the stream.\n\nWe thought we were going to do this in a bit of an ugly way, like when we were all telecommuting: each speaker joins the Google Meet, shares their screen, and speaks into their microphone, often the headset provided by the company. But, stillâ€¦ We decided to make an effort and try to provide a better experience for our colleagues attending LFT remotely!\n\nSo, how we did it, this time\n\nHow did we capture and stream slides (or even code and video!), audio, and video of each speaker?\n\nFirst step: our amphitheater\n\nLetâ€™s start from the room where we broadcast: our amphitheater.\n\n\n\nYou can see the room below. This photo is taken6 from the first row of the bleachers, on the right on the picture above:\n\n\n\nSo, the configuration of the room:\n\n\n  a speaker;\n  a lectern to put his or her PC;\n  from the HDMI output of this PC (possibly via a USB-C to HDMI adapter), we connect an HDMI cable that goes to the two projectors in the room;\n  these two projectors project (the same thing) on two screens, on the left and on the right of the speaker.\n\n\nBehind a screen, we placed a table for the capture and broadcast computer7. This way, it is not too far from the speaker8, without being visible from the audience.\n\nEquipment:\n\n\n  speakerâ€™s laptop, placed on the lectern;\n  USB-C to HDMI adapter;\n  HDMI cable;\n  two projectors (fixed to the ceiling of the room);\n  two screens (fixed to the ceiling of the room).\n\n\nThis setup allows for efficient presentation in the roomâ€¦ But we havenâ€™t started working on the live stream yet.\n\nA PC to manage the video and audio\n\nTo broadcast live, the speaker could join the Google Meet and share their screen. Thatâ€™s what we were doing during the COVIDâ€¦ But we can also do much better!\n\nLetâ€™s install a PC on the table behind the screen. It will broadcast, to the Google Meet, the video generated by OBSâ€™ virtual camera â€“ a free and open-source video recording and streaming software.\n\nA second PC, also placed on this table, is used to watch the Google Live Stream watched by all our remote colleagues. This feedback helps us validate that everything is working well, even if we suffer from a few dozen seconds of lag.\n\nEquipment:\n\n\n  a table, not too visible from the room;\n  a PC (laptop) â€“ the one used this time had two USB-A ports and one USB-C port, which affects the cables/adapters required later on;\n  its charger and, perhaps, an extension cord;\n  a good quality internet connection;\n  a second PC (laptop) to view the live stream;\n  and a headset to listen to the live stream.\n\n\nVideo capture of the speakerâ€™s slides / screen\n\nWe want to broadcast to the live stream what the speaker is projecting in the room. To do this, we place an HDMI capture box between her PC and the projector:\n\n\n\nThis capture box has an HDMI input (= the cable coming out of the speakerâ€™s PC), an HDMI output (= the cable going to the projector) and a USB-C output (= the cable going to the control / broadcast PC).\n\n\n\nOn the control / broadcast PC, the capture box connected via USB is recognized as a webcam. It is then used as a video input device in OBS.\n\nHardware:\n\n\n  additional HDMI cable;\n  capture device + passthrough: Elgato HD60 S+;\n  USB-C to USB-A cable;\n  USB-A extension cable (because the control desk is a bit far).\n\n\nThe video recording of the room\n\nIt would be even cooler if the remote audience could see the speaker! As a matter of fact, part of the message of each talk is transmitted by the speakerâ€™s gestures.\n\nSo letâ€™s position a camera in the room, at the level of the audience sitting in the stands, to give the impression the speakers are looking at the camera when they are looking at the audience.\n\n\n\nWe did not have a real camera at hand. So we used an iPhone 11 Pro Max9. It has only a Lightning portâ€¦ And, with an adapter, we can connect an HDMI cable. And the Filmic Pro10 application knows how to stream a clean HDMI11 video.\n\nTo capture the HDMI signal from the iPhone to the PC, we used a second capture box, simpler than the previous one: it has an HDMI input (= to connect the iPhone) and a USB output (= connected to the PC):\n\n\n\nFearing the iPhoneâ€™s battery wouldnâ€™t last all day while filming, we powered it via an external battery12. The Lightning-to-HDMI adapter happens to include a lightning plug for power.\n\nHardware:\n\n\n  iPhone 11 Pro Max (another high-end model less than four or five years old would have done the trick as well);\n  lightning to HDMI adapter, with Lightning input for power supply;\n  10m HDMI cable (to reach the control PC);\n  external battery;\n  HDMI capture box: Elgato Camlink 4K;\n  USB cable (between camlink and PC);\n  USB to lightning cable (between the battery and the adapter connected to the iPhone).\n\n\nThe capture box is recognized as a USB webcam. We use it as a video input device in OBS.\n\nWe first put the camera on the side of the room, to not disturb. But the speakers never looked in its direction, and it was not very nice for the remote audience, which felt less included. So we moved the camera almost in front of the speaker. This way, the remote audience feels more like the person speaking is looking in their direction.\nAlso, the camera that films the speaker also films a screen, including when the speaker wants to point to a part of what they are presenting.\n\nAudio recording of the speaker\n\nOur amphitheater is equipped with microphones and speakers, but we donâ€™t yet know how to get the sound from this audio system to inject it into a live stream.\n\nWith the resources and time we had, the best we could do for the speakers was a wireless lapel mic:\n\n\n\nWe didnâ€™t have a wireless mic, but we had something that could act as such, and we ended up with a 3.5â€ jack to plug into the microphone port of the control PC. This microphone is then used as an audio source in the Google Meet.\n\nEquipment:\n\n\n  Lapel microphone with TRRS Jack 3.5 port: RODE SmartLav+;\n  TRRS female to TRS male adapter (between the microphone and the transmitter box);\n  wireless transmitter + receiver kit (TRS 3.5 jack input on transmitter, TRS 3.5 jack output on receiver): RODE Wireless Go (1st generation);\n  TRS 3.5 male to male cable (between the receiver box and the PC).\n\n\nSome other points and nice â€œbonusesâ€\n\nOf course, we didnâ€™t stop at these main points, and lots of other little things came into play throughout the day.\n\nRemote speakers?\n\nI wrote above that I wouldnâ€™t talk about it, and I lied a bit: one of the talks that day was given by a colleague who was working from home.\nHe joined the organizersâ€™ Google Meet, shared his screen, activated his camera, and presented, exactly as many of us have done during more than a year of forced telecommuting because of COVID.\nFor those in the auditorium, one of the organizers plugged in his computer to the two screens in the room and its sound system.\n\nTransitions between sessions\n\nAt the end of each talk, we quickly took over the speakerâ€™s desk and microphone. We wanted to set up the next person, plug in their PC and position the mic so as not to interfere with them, explain how we were broadcasting and validate the setup and configuration.\nAn organizer was in charge of managing the transition, indicating at what time we would resume. But this transition, in the room, was done without a microphone â€“ and therefore, without being broadcast to the live stream.\nFor the live stream, another organizer made the same transition by joining the Google Meet of organizers and speakers. This way, our remote colleagues were not left in the dark and knew when the next talk would resume.\n\nSome useful, or even essential, utilities\n\nOf course, in a large room with few electrical outlets that are not always well placed, you need to bring extension cords and power strips.\nAlso, to avoid someone getting their feet caught in the extension cords or cables (USB, HDMI), bring a roll of duct tape to secure everything to the floor.\nAnd depending on where the lapel microphone is attached (shirt collar, shirt buttonholeâ€¦) and where its cable goes (above the shirt in front, under the shirt / under the shirt, on the shoulder and in the backâ€¦), more delicate tape13 is very handy.\n\nAt OBS level\n\nOn the control PC, in OBS, we configured several scenes to highlight the speakers or their slides. The images below are screenshots from the live stream, so in 720p maximum and much too compressedâ€¦\n\nScreen only\n\nTo display large code examples, videosâ€¦\n\n\n\nLarge screen\n\nWith speaker overlay in the bottom right corner: for slides written small, while reflecting an idea of the gestures. And we also had the same thing with the speaker inlay in another corner, for slides with important texts in the bottom right of the screen.\n\n\n\nMosaiq\n\nScreen on the left half of the screen, video of the speaker on the right half: for large written slides and to allow people at a distance to see the speaker clearly. We used this view a lot when the camera was on the side of the room â€“ and we didnâ€™t use it anymore once the camera was in front of the speaker.\n\n\n\nSpeaker only\n\nOnly the speakerâ€™s video, framed to include one of the two screens. We used this view â€œas if the person watching the stream was sitting in the standsâ€ for much of the afternoon, when the camera was placed in the middle of the audience.\n\n\n\nFor the practical aspects of managing the live stream\n\nChanging scenes in OBS can be done with the mouse (but itâ€™s not convenient) or with keyboard shortcuts (but you have to remember the shortcuts and the scenes they correspond to).\n\nBut itâ€™s much more fun with a Stream Deck: a command pad with big buttons, which you can customize! Yes, itâ€™s a bit gimmicky and not at all essential, but itâ€™s also very cool to use ;-)\n\nHardware:\n\n\n  Elgato Stream Deck;\n  since it connects to USB-A: an adapter or a USB hub.\n\n\n\n\nThe control desk, in photo\n\nA lot of unorganized cablesâ€¦ Here is what the table looked like with the control PC14:\n\n\n\nSome things to improve\n\nOf course, not everything was perfectâ€¦ Still, for less than an hour and a half of installation and testing that morning, we were quite happy with the result!\n\nFirst of all, Google Live Stream: the solution, integrated to Google Workspace, is very practical. Including the aspect of â€œlimiting access to our employeesâ€15. However, the video quality, in 720p too compressed, is not optimal :-/.\n\nOur auditorium has two cameras fixed to the ceiling. Today, they are not yet functional16, but we hope they will soon replace the iPhone â€“ and allow us to get a view of the audience for questions.\n\nAlso, having only one microphone is problematic when two speakers are speaking for a talk. It happened once during the day, and we put the lapel mic on the lectern and asked the speakers to move closer to it when it was their turn to speak. The RODE SmartLav+ being omni-directional and of good quality, it was just about rightâ€¦ Without being optimal. Also, we didnâ€™t have a microphone for the questions, so the speakers had to repeat them17.\nWe have two wireless microphones in the auditorium. Currently, they are used to amplify the sound in the room and we didnâ€™t have time in the morning when we were setting up to figure out how to capture their output and integrate it into the live stream. Maybe an improvement for next time, because they seem to be pretty good ;-)\nAlso, we didnâ€™t have the time18 to talk with our colleagues who were managing the sound of these events before the COVID: somewhere in a closet, we have a physical mixer, which could have been useful! Another area for improvement!\n\nThat said, now that weâ€™ve seen that it is possible, we want to do even better next time, in two months ;-). And, clearly, a prototype in 1h30 of setup, which worked that well all day long and which improved the experience of 150 or more remote colleagues, is a big success which will push us to do better - and we know we will be able to do so!\nAnd then, with time and successive improvements, we may spend less time each time installing and configuring!\nAlso, a more turnkey solution â€“ like Streamyard19 â€“ might make our lives easier, compared to OBSâ€¦\n\nEvery two months, we organize a LFT day at Bedrock: our Last Friday Talks.\nAfter COVID and with the partial return to the office, we wanted our remote colleagues to experience this LFT as best as they could.\nFor this iteration, we did with what we had. Itâ€™s up to us to iterate and do even better next time!\nAnd you, how do you share these kinds of events with your remote colleagues?\n\n  \n    \n      Historically, it was the afternoon of the last Friday of every month. We revised the format in 2019 to make it a full day every other month.Â &amp;#8617;\n    \n    \n      In France, some companies â€“ including ours â€“ started returning to the office, full-time or not, in June 2021, following recommendations from the government.Â &amp;#8617;\n    \n    \n      One of the talks was given by a remote speaker, I will not talk much about it in this article: he joined the Google Meet of the organizers and shared his camera and screen. An organizer then projected the Google Meet into the auditorium for the live audience.Â &amp;#8617;\n    \n    \n      We went up to 150 people watching the stream simultaneously.Â &amp;#8617;\n    \n    \n      Google Live Stream. We use this solution because Google Meet alone does not support enough people in a call.Â &amp;#8617;\n    \n    \n      Photo taken while projecting talks of the Demuxed conference, for those who wanted to see them on a big screen and togetherâ€¦Â &amp;#8617;\n    \n    \n      In the photo above, we see the legs of the table and high chairs placed around it. Yes, this space also serves as a break room.Â &amp;#8617;\n    \n    \n      And the organizers can tap them on the shoulder, to tell them to start their talk or ask them to take a break in case of technical problemsâ€¦Â &amp;#8617;\n    \n    \n      High-end smartphones have had very good quality cameras for several years. An iPhone 11 is more than enough to film and broadcast a live conference.Â &amp;#8617;\n    \n    \n      Filmic Pro: a not cheap application, but very good when it comes to filming with an iPhone.Â &amp;#8617;\n    \n    \n      Clean HDMI: the filmed video, in real time, without the decorations of the application interface.Â &amp;#8617;\n    \n    \n      External battery: because there are no electrical outlet in this part of the room and we didnâ€™t want to add more cables and extension cords.Â &amp;#8617;\n    \n    \n      I use medical tape, which is easy to cut, sticks well and comes off easily too; and is not too aggressive with clothes and skin.Â &amp;#8617;\n    \n    \n      Yes, itâ€™s a bit of a messâ€¦ And, no, you canâ€™t see everything: when we took this picture, we didnâ€™t think we would post it ^^Â &amp;#8617;\n    \n    \n      We may broadcast some talks in public in the futureâ€¦ But it was not a topic for this time and we know that some topics will remain internal no matter what.Â &amp;#8617;\n    \n    \n      We moved in recently and other more important items were configured first.Â &amp;#8617;\n    \n    \n      A person in the control room tapped the speakers on the shoulder when they forgot to repeat the questionsâ€¦Â &amp;#8617;\n    \n    \n      We should have done it a few days in advance, not the morning itselfâ€¦Â &amp;#8617;\n    \n    \n      Which weâ€™ve already used at conferences, both as organizers and speakers.Â &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Best practices for Web application maintenance",
    "category" : "",
    "tags"     : " js, react, web, frontend",
    "url"      : "/2021/09/06/web-best-practices.html",
    "date"     : "September 6, 2021",
    "excerpt"  : "\n  How not to throw away your application every two years?\n\n\nFeedback based on best practices applied to the web platform developed at Bedrock Streaming\n\nA bit of context\n\nAt Bedrock Streaming many teams develop and maintain frontend applications ...",
  "content"  : "\n  How not to throw away your application every two years?\n\n\nFeedback based on best practices applied to the web platform developed at Bedrock Streaming\n\nA bit of context\n\nAt Bedrock Streaming many teams develop and maintain frontend applications for our customers and users.\nSome of those applications are not very young.\nIn fact, the application Iâ€™m mainly working on is a website whose developments started in 2014.\nI have already mentioned it in different articles of this blog.\n\n\n\nYou might think: â€œOh poor people, maintaining an almost 10 year old application must be hell!â€\n\nDonâ€™t worry, itâ€™s not the case!\nI have worked on projects that are much less old but where the development of new features was much more painful.\n\nToday the project is technically up to date, we must be on the latest version of React while it had started on a version 0.x.x.\nIn this world of often criticized web technologies where tools and practices are constantly evolving (eg: the many articles on the Javascript Fatigue), to keep a project â€œup to dateâ€ remains a real challenge.\n\n\n\nMoreover, in the context of this project, in almost 10 years, we have had about 100 contributors.\nSome have only stayed a few months/years.\nHow can we keep the maximum knowledge on â€œHow we do things and how it works?â€ in such a moving human context?\n\n\n\nThis is what I would like to demonstrate in this post.\n\nWith the help of my colleagues, I have collected the list of good practices that still allow us to maintain this project today.\nWith Florent Dubost, we have often thought that it would be interesting to publish it.\nWe hope you will find it useful.\n\nSet rules and automate them\n\nA project that stands the test of time is first and foremost a set of knowledge that is stacked one on top of the other.\nItâ€™s like the Kapla tower you used to build as a child, trying to get as high as possible.\nA solid base on which we hope to add as much as possible before a potential fall.\n\nFrom the beginning of a project, we have to make important decisions about â€œHow do we want to do things?\nWe think for example about â€œWhat format for our files? How do we name this or that thing?â€\nWriting accurate documentation of â€œHow we do thingsâ€ might seem like a good idea.\n\nHowever, documentation is cool, but it tends to get outdated very quickly.\nOur decisions evolve, but documentation does not.\n\n\n  â€œTimes change but not READMEs.â€\n\n  Olivier Mansour (deputy CTO at Bedrock)\n\n\nAutomating the checking of each of the rules we impose on ourselves (on our codebase or our processes) is much more durable.\nTo make it simple, we avoid as much as possible to say â€œWe should do things like thatâ€, and we prefer â€œweâ€™ll code something that checks it for usâ€.\nOn top of that, on the JS side we are really well equipped with tools like Eslint that allow us to implement our own rules.\n\nSo the reflex we try to adopt is the following:\n\n\n  â€œWe should try to do it like this now!â€\n  â€œOk thatâ€™s interesting, but how can we make sure we do it like that automatically with our CI (Continuous Integration)?â€\n\n\nContinuous Integration of a project is the perfect solution to not miss anything on every Pull Request we provide.\nReviews are only easier because you donâ€™t have to worry about all the rules that are already automated.\nIn this model, the review is more for knowledge sharing than for typo copying and other non-compliance with the project conventions.\n\nIn this principle, we must therefore try to banish oral rules.\nThe time of the druids is over, if all the good practices of a project have to be transmitted orally, it will only take longer to guide new developers into your team.\n\n\n\nA project is not set in stone. These rules evolve with time.\nIt is therefore preferable to add rules that have a script that will autofix the whole codebase intelligently.\nMany Eslint rules offer this, and it is a very important selection criteria when choosing new conventions.\n\neslint --fix\n\n\nA very strict rule that will force you to modify your code manually before each push is annoying in the long run and will annoy your teams.\nWhereas a rule (even a very strict one) that can auto-fix itself at commit time will not be seen as annoying.\n\nHow to decide to add new rules ?\n\nThis question may seem thorny, take for example the case of &amp;lt;tab&amp;gt; / &amp;lt;space&amp;gt; in files.\nFor this, we try to avoid the endless debates and follow the trend and rules of the community.\nFor example, our Eslint configuration base is based on Airbnbâ€™s which seems to have some success in the JS community.\nBut if the rule we want to impose on ourselves is not available in Eslint or other tools, we sometimes prefer not to follow the rule rather than say â€œWeâ€™ll do it without a checking CIâ€.\n\nThe almost exhaustive list ğŸ¤\n\n\n\n\n  The file format is managed by Editorconfig, prettier and Eslint.\nWe have opensourced our own configuration, if it is of any use to you.\n  We use a specific commit name to generate our changelog.\nTo make sure devs follow it, a simple step in our CI checks it.\n  We donâ€™t want a dev to make our JS bundles very big in production, so we track and measure their size in the CI.\nWe use an in-house tool but we recommend to use the BuildTracker tool.\n  Test coverage is not an indicator for the team, not all lines have the same need for us to be tested.\nSome teams at Bedrock however follow this indicator which at least has the interest to give a trend.\n  Our unit tests obviously run on the CI, these must pass.\n  Our functional tests (End to end: E2E) run on Chrome Headless, they must be green.\n  The logs of our E2E tests are retrieved and parsed to avoid errors or React warnings (the parsing script is however complicated to maintain)\n  Functional tests run in a sandbox where the whole network is proxied.\nWe make sure that our tests do not depend on a non mocked API that could slow down their execution.\n  During the E2E tests we check that no image request has generated a 404.\n  We perform some accessibility checks with Axe during our E2E tests.\n  We check some rules on the CSS with Stylelint and bemlinter (we donâ€™t use BEM anymore but there is still some style managed in SCSS that we migrate little by little in StyledComponent)\n  The project is a monorepo on which we try to maintain the same dependencies versions for each package.\nWe developed a tool which automates this check: monorepo-dependencies-check\n  We check that our yarn.lock file has not been inadvertently modified or that it has been updated with respect to the modifications of the package.json.\n  Terraform is used to manage our cloud resources, we check that the file format is correct.\n\n\nTest, test, test\n\nI hope that in 2021 it is no longer necessary to explain why automatic testing of your application is essential to make it sustainable.\nIn JS, we are rather well equipped in terms of testing tools today.\nHowever, the eternal question remains:\n\n\n  â€œWhat do we want to test?â€\n\n\nGlobally if we search on the internet this question, we see that different needs make emerge very different practices and testing tools.\nIt would be very presumptuous to think that there is a good way to automatically test your application.\nThis is why it is preferable to define one or more test strategies that meet defined and limited needs.\n\nOur test strategies are based on two distinct goals:\n\n\n  To automate the verification of the functionalities proposed to the users by putting ourselves in their place.\n  To provide us with efficient solutions to specify the way we implement our technical solutions to allow us to make them evolve more easily.\n\n\nTo do this, we perform two â€œtypes of testsâ€ that I propose to present here.\n\nOur E2E tests\n\nWe call them â€œfunctional testsâ€, they are End-to-end (E2E) tests on a very efficient technical stack composed of CucumberJS, WebdriverIO with ChromeHeadless\nThis is a technical stack set up at the beginning of the project (at the time with PhantomJS for the oldest among you)\n\nThis stack allows us to automate the piloting of tests that control a browser.\nThis browser will perform actions that are as close as possible to what our real users can do while checking how the site reacts.\n\nA few years ago, this technical stack was rather complicated to set up, but today it is rather simple to do.\nThe site that hosts this blog post is itself proof of this.\nIt only took me about ten minutes to set up this stack with the WebdriverIo CLI to verify that my blog is working as expected.\n\nI recently published an article presenting the implementation of this stack.\n\nSo here is an example of an E2E test file to give you an idea:\n\nFeature: Playground\n\n  Background: Playground context\n    Given I use &quot;playground&quot; test context\n\n  Scenario: Check if playground is reachable\n    When As user &quot;toto@toto.fr&quot; I visit the &quot;playground&quot; page\n    And I click on &quot;playground trigger&quot;\n    Then I should see a &quot;visible playground&quot;\n    And I should see 4 &quot;playground tab&quot; in &quot;playground&quot;\n\n    When I click on &quot;playground trigger&quot;\n    Then I should not see a &quot;visible playground&quot;\n\n    # ...\n\n\nAnd it looks like this in local with my Chrome browser!\n\n\n\nHere is a diagram that explains how this stack works:\n\n\n\nToday, Bedrockâ€™s web application has over 800 E2E test cases running on each of our Pull Request and the master branch.\nThey assure us that we are not introducing any functional regression and thatâ€™s just great!\n\nğŸ‘ The positives\n\n\n  WebdriverIO also allows us to run these same tests on real devices on a daily basis through the paid SAAS service Browserstack.\nSo we have every day a job that makes sure that our site works correctly on a Chrome last version on Windows 10 and Safari on MacOs.\n  These tests allow us to easily document the functionality of the application using the Gherkin language.\n  They allow us to reproduce cases that are far from nominal.\nIn a TDD logic, they allow us to advance on the development without having to click for hours.\n  These tests allowed us not to break the old version of the site which is still in production for some customers while our efforts are concentrated on the new one.\n  They give us real confidence.\n  Thanks to our library superagent-mock, we can fixturer (plug, mock) all the APIs we depend on and thus even check the error cases.\nAlso, mocking the browserâ€™s XHR layer allows for a significant improvement in test execution time. ğŸš€\n  They give us access to extended uses like:\n    \n      checking accessibility rules\n      check the browser console logs (to avoid introducing errors or React Warning for example)\n      monitoring all network calls of the site through a proxy\n      and so onâ€¦\n    \n  \n\n\nğŸ‘ The complications\n\n\n  Maintaining this stack is complicated and expensive.\nSince few resources are published on this domain, we sometimes find ourselves digging for days to fix them ğŸ˜….\nSometimes we feel quite alone in having these worries.\n  It is very easy to code a so-called flaky E2E test (ie: a test that can fail randomly).\nThey make us think that something is broken.\nThey sometimes take us a long time to stabilize.\nIt is still much better to remove a test that will not give you a stable result.\n  Running all the tests takes a lot of time on our continuous integration.\nWe must regularly work on their optimization so that the feedback they provide you is as fast as possible.\nThese important times also cost money, because we have to run these tests on machines.\nFor your information, the infrastructure of the website (just the hosting of our Node servers + static files + CDN) cost much less than our continuous integration.\nThis obviously makes our Ops team smile! ğŸ˜Š\n  The new recruits in our teams have often never done this kind of testing, so there is a struggle phase of learningâ€¦\n  Some features are sometimes too complicated to test with our E2E stack (for example, payment paths that depend on third parties).\nSo we sometimes fall back on other techniques with Jest, especially with a less unitary scope.\n\n\nOur â€œunitâ€ tests\n\nTo complete our functional tests we also have a stack of tests written with Jest.\nWe call these tests unit tests because we have as a principle to try to always test our JS modules independently from the others.\n\nLetâ€™s not debate here about â€œAre these real unit tests?â€, there are enough articles on the internet about this topic.\n\nWe use these tests for different reasons that cover needs that our functional tests do not cover:\n\n\n  to help us develop our JS modules with TDD practices.\n  to document and describe how a JS module works.\n  test very/too complicated edge cases with our E2E tests.\n  facilitate the refactoring of our application by showing us the technical impacts of our modifications.\n\n\nWith these tests, we put ourselves at the level of a utility function, a Redux action, a reducer, a React component.\nWe rely mainly on the automock functionality of Jest which allows us to isolate our JS modules when we test.\n\n\n\nThe previous image represents the metaphor that allows us to explain our unit testing strategy to newcomers.\n\n\n  You have to imagine that the application is a wall made of unit bricks (our ecmascript modules), our unit tests must test one by one the bricks in total independence from the others.\nOur functional tests are there to test the cement between the bricks.\n\n\nTo summarize, we could say that our E2E tests test what our application should do, and our unit tests make sure to check how it works.\n\nToday there are more than 6000 unit tests that cover the application and allow to limit regressions.\n\nğŸ‘\n\n\n  Jest is really a great library, fast, complete, well documented.\n  Unit tests help us a lot to understand several years later how it all works.\n  We always manage to unit test our code, and it complements our E2E tests well.\n  The automock feature is really handy for breaking down tests by modules.\n\n\nğŸ‘\n\n\n  Sometimes we found ourselves limited by our E2E test stack and couldnâ€™t rely solely on unit tests.\nWe were missing something to be able to make sure that the cement between the bricks worked as we wanted it to.\nFor this, a second test stack Jest was set up called â€œintegration testâ€ where the automock is disabled.\n  The abuse of Snapshot is dangerous for your health.\nThe use of â€œSnapshot testingâ€ can save time on the implementation of your tests but can reduce the quality.\nHaving to review a 50 line object in Snapshot is neither easy nor relevant.\n  With the depreciation of EnzymeJS, we are forced to migrate to React Testing Library.\nIt is of course possible to unit test components with this new library.\nUnfortunately, this is not really the spirit and the way to do it.\nReact Testing Library pushes us not to play with shallow rendering.\n\n\nOur principles\n\nWe try to always follow the following rules when asking the question â€œShould I add tests?â€.\n\n\n  If our Pull Request introduces new user features, we need to integrate E2E test scenarios.\nUnit tests with Jest can complete / replace them accordingly.\n  If our Pull Request aims to fix a bug, it means that we are missing a test case.\nWe must therefore try to add an E2E test or, failing that, a unit test.\n\n\nIt is while writing these lines that I think that these principles could very well be automated. ğŸ¤£\n\nThe project stays, the features donâ€™t\n\n\n  â€œThe second evolution of a feature is very often its removal.â€\n\n\nAs a matter of principle, we want to make sure that every new feature in the application does not base its activation on simply being in the codebase.\nTypically, the lifecycle of a feature in a project can be as follows (in a Github Flow):\n\n\n  a person implements on a branch\n  the feature is merged on master\n  it is deployed in production\n  lives its feature life (sometimes with bugs and fixes)\n  the feature is not needed anymore\n  a person unravels the code and removes it\n  new deployment\n\n\nTo simplify some steps, we have implemented feature flipping on the project.\n\nHow does it work?\n\nIn our config there is a map key/value that lists all the features of the application associated with their activation status.\n\nconst featureFlipping = {\n  myAwesomeFeature: false,\n  anotherOne: true,\n}\n\n\nIn our code, we have implemented conditional treatments that say â€œIf this feature is activated thenâ€¦â€.\nThis can change the rendering of a component, change the implementation of a Redux action or disable a route in our react-router.\n\nBut whatâ€™s the point?\n\n\n  We can develop new evolutions progressively by hiding them behind a configuration key.\nWe deliver features in production without activating them.\n  In a test environment, we can overload this config to test features that are not yet activated in production.\n  In the case of a white label site, we can propose these features to our customers as possible options.\n  Before deleting code of a feature, we deactivate it and clean it up without risk.\n  Thanks to an in-house tool called Applaunch, this feature flipping config can be overloaded on time in a GUI without deployment.\nThis allows us to activate features without putting the code into production.\nIn the event of an incident, we can deactivate features that have been degraded.\n\n\nTo give you a more concrete example, between 2018 and 2020 we completely overhauled the applicationâ€™s interface.\nThis graphical evolution was just a featureFlipping key.\nThe graphical redesign was not a reset of the project, we still live with both versions (as long as the switchover of all our customers is not completed).\n\n\n\nA/B testing\n\nThanks to the great work of the backend and data teams, we were even able to extend the use of feature flipping by making this configuration modifiable for sub-groups of users.\n\nThis allows us to deploy new features on a smaller portion of users in order to compare our KPI.\n\nDecision making, technical or product performance improvement, experimentation, the possibilities are numerous and we exploit them more and more.\n\nThe future flipping.\n\n\n  Based on an original idea by Florent Lepretre.\n\n\nWe regularly had the need to activate features at very early hours in the future.\nFor that we had to be connected at a precise time on our computer to modify the configuration on the fly.\n\nTo avoid forgetting to do this, or doing it late, we made sure that a configuration key could be activated from a certain date.\nTo do this, we evolved our Redux selector which indicated if a feature was activated so that it could handle date formats and compare them to the current time.\n\nconst featureFlipping = {\n  myAwesomeFeature: {\n    offDate: &#39;2021-07-12 20:30:00&#39;,\n    onDate: &#39;2021-07-12 19:30:00&#39;,\n  },\n}\n\n\n\n  Many coffees â˜•ï¸ at 9am have been saved by future flipping.\n\n\nMonitor, Measure, Alert\n\nTo maintain a project as long as Bedrockâ€™s web application, testing, documentation and rigor are not enough.\nYou also need visibility on what works in production.\n\n\n  â€œHow do you know that the application you have in production right now is working as expected?â€\n\n\nWe assume that no functionality works until it is monitored.\nToday, monitoring in Bedrock on the frontend side takes the form of different tools and different stacks.\nWe rely on NewRelic, Statsd, a ELK stack and even Youbora for the video streaming part.\n\nTo give you an example, each time a user starts a browsing session we send an anonymous monitoring Hit to increment a counter in Statsd.\nWe then have to define a dashboard that displays the evolution of this number in a graph.\nIf we observe a too important variation, it can allow us to detect an incident.\n\n\n\nMonitoring also offers us solutions to understand and analyze a bug that occurred in the past.\nUnderstanding an incident, explaining it, finding its root cause are the possibilities that are open to you if you monitor your application.\nMonitoring can also allow you to better communicate with your customers about the impact of an incident and also to estimate the number of impacted users.\n\nWith the multiplication of our customers, monitoring our platforms well is not enough.\nToo much data, too many dashboards to monitor, it becomes very easy to miss something.\nSo we started to complement our metrics monitoring with automatic alerting.\nOnce we have enough confidence in the metrics, we can easily set up alerts that will warn us if there is an inconsistent value.\n\nHowever, we try to always trigger alerts only when it is actionable.\nIn other words, if an alert sounds, we have something to do.\nSounding alerts that do not require immediate human action generates noise and wastes time.\n\n\n\nLimit, monitor and update your dependencies\n\nWhat goes out of date faster than your shadow in a web project based on Javascript technologies are your dependencies.\nThe ecosystem evolves rapidly and your dependencies can quickly become unmaintained, out of fashion or completely overhauled with big breaking changes.\n\nWe therefore try as much as possible to limit our dependencies and avoid adding them unnecessarily.\nA dependency is often very easy to add but it can become a real headache to remove.\n\nThe graphic component libraries (e.g. React bootstrap, Material Design) are a good example of dependencies that we do not want to introduce.\nThey can make integration easier at first, but they often freeze the version of your component library later on.\nYou donâ€™t want to freeze the React version in your application for two form components.\n\nMonitoring is also part of our dependency management routines.\nSince the addition of reporting security flaws in an NPM package, it is possible to know if a project has a dependency that contains a known security flaw with a simple command.\nSo we have daily jobs on our projects that run the yarn audit command to force us to apply patches.\n\n\n  Dependency maintenance is greatly facilitated by our E2E test stack which sounds the alarm if the version upgrade generates a regression.\n\n\nToday, except for security flaws, we update our dependencies â€œwhen we have timeâ€, often at the end of sprint.\nWe are not satisfied with this because some dependencies can be forgotten.\nI personally use tools like yarn outdated and Dependabot on my personal projects to automate the update of my dependencies.\n\nAccepting your technical debt\n\nA project will always accumulate technical debt.\nThis is a fact.\nWhether it is voluntary or involuntary debt, a project that resists the years will inevitably accumulate debt.\nEven more so, if during all these years you keep adding features.\n\nSince 2014, our best practices, our ways of doing things have evolved well.\nSometimes we decided these changes but sometimes we underwent them (an example, the arrival of functional components with React and the Hooks api).\n\nOur project is not completely â€œstate of artâ€ and we assume it.\n\n\n\nWe try to prioritize our refactoring topics on the parts of the application on which we have the most concern, the most pain.\nWe consider that a part of the application that we donâ€™t like but on which we donâ€™t need to work (bring evolutions) doesnâ€™t deserve that we refactor it.\n\nI could name many features of our application that have not evolved functionally for several years.\nBut since we have covered these features with E2E tests since the beginning, we didnâ€™t really have to touch them.\n\nAs said above, the next evolution of a code feature is sometimes its deactivation.\nSo why spend time rewriting the whole application?\n\n\n  In any case, the code becomes â€œlegacyâ€.\n  As long as the features are tested, nothing obliges us to refactor everything permanently so that our entire codebase is state of art.\n  We focus on our pain points, we re-factor what we really need to evolve.\n\n\nTo summarize\n\nThe best practices presented here are obviously subjective and will not be perfectly/directly applicable in your contexts.\nHowever, I am convinced that they can probably help you identify what can make your project go from fun to stale.\nAt Bedrock we have other practices in place that I havenâ€™t listed here but that will be the occasion for a new article sometime.\n\nFinally, if you want me to go into more detail on some of the chapters presented here, donâ€™t hesitate to tell me, I could try to dedicate a specific article to it.\n\n"
} ,
  
  {
    "title"    : "Bonnes pratiques pour la maintenance d&#39;une application web",
    "category" : "",
    "tags"     : " js, react, web, frontend",
    "url"      : "/2021/09/01/bonnes-pratiques-web.html",
    "date"     : "September 1, 2021",
    "excerpt"  : "\n  Comment ne pas jeter son application tous les deux ans ?\n\n\nRetour dâ€™expÃ©rience basÃ© sur les bonnes pratiques appliquÃ©es Ã  la plateforme web dÃ©veloppÃ©e chez Bedrock Streaming\n\nUn peu de contexte\n\nChez Bedrock Streaming de nombreuses Ã©quipes dÃ©ve...",
  "content"  : "\n  Comment ne pas jeter son application tous les deux ans ?\n\n\nRetour dâ€™expÃ©rience basÃ© sur les bonnes pratiques appliquÃ©es Ã  la plateforme web dÃ©veloppÃ©e chez Bedrock Streaming\n\nUn peu de contexte\n\nChez Bedrock Streaming de nombreuses Ã©quipes dÃ©veloppent et maintiennent des applications frontend pour nos clients et utilisateurs.\nCertaines ne sont pas toute jeune.\nEn effet, lâ€™application sur laquelle je travaille principalement est un site web dont les dÃ©veloppements ont commencÃ© en 2014.\nJe lâ€™ai dâ€™ailleurs dÃ©jÃ  Ã©voquÃ©e dans diffÃ©rents articles de ce blog.\n\n\n\nVous pourriez vous dire: â€œOh les pauvres maintenir une application vieille de presque 10 ans Ã§a doit Ãªtre un enfer !â€\n\nRassurez-vous, ce nâ€™est pas le cas !\nJâ€™ai travaillÃ© sur des projets bien moins vieux mais sur lesquels le dÃ©veloppement de nouvelles fonctionnalitÃ©s Ã©tait bien plus pÃ©nible.\n\nAujourdâ€™hui le projet reste Ã  jour techniquement, on doit Ãªtre sur la derniÃ¨re version de React alors que celui-ci avait commencÃ© sur une version 0.x.x.\nDans ce monde des technologies web souvent dÃ©criÃ© (ex: les nombreux articles sur la Javascript Fatigue) dont les outils et les pratiques Ã©voluent constamment, conserver un projet â€œÃ  jourâ€ reste un vrai challenge.\n\n\n\nDe plus, dans le contexte de ce projet, en presque 10 ans, nous avons connu une centaine de contributeurs.\nCertains ne sont restÃ©s que quelques mois/annÃ©es.\nComment garder au maximum la connaissance sur â€œComment on fait les choses et comment Ã§a marche ?â€ dans un contexte humain si mouvant ?\n\n\n\nCâ€™est ce que je vous propose de vous prÃ©senter.\n\nAvec lâ€™aide de mes collÃ¨gues, jâ€™ai rassemblÃ© la liste des bonnes pratiques qui nous permettent encore aujourdâ€™hui de maintenir ce projet en Ã©tat.\nAvec Florent Dubost, on sâ€™est souvent dit quâ€™il serait intÃ©ressant de la publier.\nNous espÃ¨rons que cela vous sera utile.\n\nSâ€™imposer des rÃ¨gles et les automatiser\n\nUn projet qui rÃ©siste au temps câ€™est tout dâ€™abord un ensemble de connaissances quâ€™on empile les unes sur les autres.\nCâ€™est en quelque sorte la tour de Kapla que vous assembliez petit en essayant dâ€™aller le plus haut possible.\nUne base solide sur laquelle on espÃ¨re pouvoir ajouter le plus possible avant une potentielle chute.\n\nDÃ¨s le dÃ©but dâ€™un projet on est donc amenÃ© Ã  prendre des dÃ©cisions importantes sur â€œComment on souhaite faire les choses ?â€.\nOn pense par exemple Ã  â€œQuel format pour nos fichiers ? Comment on nomme telle ou telle chose ?â€\nÃ‰crire une documentation prÃ©cise de â€œComment on fait les chosesâ€ pourrait paraitre une bonne idÃ©e.\n\nCependant la documentation câ€™est cool, mais Ã§a a tendance Ã  pÃ©rimer trÃ¨s vite.\nNos dÃ©cisions Ã©voluent mais pas la documentation.\n\n\n  â€œLes temps changent mais pas les README.â€\n\n  Olivier Mansour (deputy CTO Ã  Bedrock)\n\n\nAutomatiser la vÃ©rification de chacune des rÃ¨gles quâ€™on sâ€™impose (sur notre codebase ou nos process) est bien plus pÃ©renne.\nPour faire simple, on Ã©vite dans la mesure du possible de dire â€œOn devrait faire les choses comme celaâ€, et on prÃ©fÃ¨re â€œon va coder un truc qui nous le vÃ©rifie Ã  notre placeâ€.\nEn plus de Ã§a, cotÃ© JS on est vraiment bien Ã©quipÃ© avec des outils comme Eslint qui nous permettent dâ€™implÃ©menter nos propres rÃ¨gles.\n\nLe rÃ©flexe quâ€™on essaie donc dâ€™adopter est donc le suivant:\n\n\n  â€œOn devrait essayer de faire comme cela Ã  prÃ©sent !â€\n  â€œOk câ€™est intÃ©ressant, mais comment peut-on sâ€™assurer quâ€™on fasse comme cela automatiquement avec notre CI (IntÃ©gration continue) ?â€\n\n\nLâ€™intÃ©gration continue dâ€™un projet est la solution parfaite pour ne rien louper sur chacune des Pull Request que nous proposons.\nLes reviews nâ€™en sont que plus simples car vous nâ€™avez plus Ã  vous soucier de lâ€™ensemble des rÃ¨gles qui sont dÃ©jÃ  automatisÃ©es.\nDans ce modÃ¨le, la review sert donc plus au partage de connaissance quâ€™au flicage de typo et autre non respect des conventions du projet.\n\nDans ce principe, il faut donc essayer de bannir les rÃ¨gles orales.\nLe temps des druides est terminÃ©, sâ€™il faut transmettre oralement toutes les bonnes pratiques dâ€™un projet, lâ€™accompagnement de nouveaux dÃ©veloppeurs dans votre Ã©quipe nâ€™en sera que plus long.\n\n\n\nUn projet nâ€™est pas figÃ©. Ces rÃ¨gles Ã©voluent donc avec le temps.\nOn prÃ©fÃ¨rera alors lâ€™ajout de rÃ¨gles qui possÃ¨dent un script qui autofixera toute la codebase intelligemment.\nDe nombreuses rÃ¨gles Eslint le proposent, et cela est vraiment un critÃ¨re de sÃ©lection trÃ¨s important dans nos choix de nouvelles conventions.\n\neslint --fix\n\n\nUne rÃ¨gle trÃ¨s stricte qui vous obligera Ã  modifier votre code manuellement avant chaque push est pÃ©nible Ã  la longue et Ã©nervera vos Ã©quipes.\nAlors quâ€™une rÃ¨gle (mÃªme trÃ¨s stricte) qui peut sâ€™autofixer automatiquement au moment du commit ne sera pas perÃ§ue comme gÃªnante.\n\nComment dÃ©cider dâ€™ajouter de nouvelles rÃ¨gles ?\n\nCette question peut paraitre Ã©pineuse, prenons par exemple le cas des &amp;lt;tab&amp;gt; / &amp;lt;space&amp;gt; dans les fichiers.\nPour cela, on essaie dâ€™Ã©viter les dÃ©bats sempiternels et on se plie Ã  la tendance et aux rÃ¨gles de la communautÃ©.\nPar exemple, notre base de configuration Eslint) est basÃ©e sur celle dâ€™Airbnb qui semble avoir un certain succÃ¨s dans la communautÃ© JS.\nMais si la rÃ¨gle quâ€™on souhaite sâ€™imposer nâ€™est pas disponible dans Eslint ou dâ€™autres outils, il nous arrive de prÃ©fÃ©rer ne pas suivre la rÃ¨gle plutÃ´t que de se dire â€œOn le fait sans CI qui vÃ©rifieâ€.\n\nLa liste presque exhaustive ğŸ¤\n\n\n\n\n  Le format des fichiers est suivi gÃ©rÃ© par Editorconfig, prettier et Eslint.\nNous avons opensourcÃ© notre propre configuration, si jamais celle-ci peut vous Ãªtre utile.\n  Nous utilisons un nommage de commit bien spÃ©cifique pour gÃ©nÃ©rer nos changelog.\nPour sâ€™assurer que les devs le respectent, une simple Ã©tape de notre CI le vÃ©rifie.\n  On ne souhaite pas quâ€™un dev fasse grossir Ã©normÃ©ment nos bundles JS en production, câ€™est pourquoi nous suivons et mesurons leur taille dans la CI.\nOn utilise un outil maison mais on peut vous recommander lâ€™outil BuildTracker.\n  La couverture de tests nâ€™est pas un indicateur pour lâ€™Ã©quipe, toutes les lignes nâ€™ont pas la mÃªme nÃ©cessitÃ© pour nous dâ€™Ãªtre testÃ©es.\nCertaines Ã©quipes Ã  Bedrock suivent cependant cet indicateur qui a au moins lâ€™intÃ©rÃªt de donner une tendance.\n  Nos tests unitaires tournent bien Ã©videmment sur la CI, ceux-ci doivent passer.\n  Nos tests fonctionnels (End to end: E2E) tournent sur Chrome Headless, ils doivent Ãªtre au vert.\n  Les logs de nos tests E2E sont rÃ©cupÃ©rÃ©s et parsÃ©s afin dâ€™Ã©viter lâ€™introduction dâ€™erreur ou de React warning (Le script de parsing est cependant compliquÃ© Ã  maintenir)\n  Les tests fonctionnels fonctionnent dans une sandbox oÃ¹ tout le rÃ©seau est proxyfiÃ©.\nNous surveillons que nos tests ne dÃ©pendent pas dâ€™une API non mockÃ©e qui pourrait ralentir leur exÃ©cution.\n  Durant les tests E2E nous vÃ©rifions quâ€™aucune requÃªte dâ€™image nâ€™a gÃ©nÃ©rÃ© une 404.\n  On rÃ©alise quelques vÃ©rifications dâ€™accessibilitÃ© avec Axe durant nos tests E2E.\n  On vÃ©rifie quelques rÃ¨gles sur le CSS avec Stylelint et bemlinter (on nâ€™utilise plus BEM aujourdâ€™hui mais il reste encore un peu de style gÃ©rÃ© en SCSS quâ€™on migre petit Ã  petit en StyledComponent)\n  Le projet est un monorepo sur lequel nous essayons de maintenir les mÃªmes versions de dÃ©pendances pour chaque package.\nPour cela nous avons dÃ©veloppÃ© un outil qui permet de faire cette vÃ©rification monorepo-dependencies-check\n  On vÃ©rifie que notre fichier yarn.lock nâ€™a pas Ã©tÃ© modifiÃ© par inadvertance ou bien quâ€™il a Ã©tÃ© mis Ã  jour par rapport aux modifications du package.json.\n  Terraform est utilisÃ© pour la gestion de nos ressources cloud, nous vÃ©rifions que le format des fichiers est correct.\n\n\nTester, tester, tester\n\nJâ€™espÃ¨re quâ€™en 2021 il nâ€™est plus nÃ©cessaire dâ€™expliquer pourquoi tester automatiquement son application est indispensable pour la rendre pÃ©renne.\nEn JS on est plutÃ´t bien Ã©quipÃ© en terme dâ€™outils pour tester aujourdâ€™hui.\nIl reste cependant lâ€™Ã©ternelle question:\n\n\n  â€œQuâ€™est-ce quâ€™on veut tester ?â€\n\n\nGlobalement si on recherche sur internet cette question, on voit que des besoins diffÃ©rents font Ã©merger des pratiques et des outils de testing bien diffÃ©rents.\nCe serait trÃ¨s prÃ©somptueux de penser quâ€™il y a une bonne maniÃ¨re de tester automatiquement son application.\nCâ€™est pourquoi il est prÃ©fÃ©rable de dÃ©finir une ou plusieurs stratÃ©gies de test qui rÃ©pondent Ã  des besoins dÃ©finis et limitÃ©s.\n\nNos stratÃ©gies de tests reposent sur deux volontÃ©s bien distinctes:\n\n\n  Automatiser la vÃ©rification des fonctionnalitÃ©s proposÃ©es aux utilisateurs en se mettant Ã  sa place.\n  Nous fournir des solutions efficaces pour specifier la maniÃ¨re dont nous implÃ©mentons nos solutions techniques pour nous permettre de les faire Ã©voluer plus facilement.\n\n\nPour cela, nous rÃ©alisons deux â€œtypes de testsâ€ que je propose de vous prÃ©senter ici.\n\nNos tests E2E\n\nOn les appelle â€œtests fonctionelsâ€, ce sont des tests End-to-end (E2E) sur une stack technique trÃ¨s efficace composÃ©e de CucumberJS, WebdriverIO avec ChromeHeadless\nIl sâ€™agit dâ€™une stack technique mise en place au dÃ©but du projet (Ã  lâ€™Ã©poque avec PhantomJS pour les plus anciens dâ€™entre-vous)\n\nCette stack nous permet dâ€™automatiser le pilotage de tests qui contrÃ´lent un navigateur.\nCe navigateur va rÃ©aliser des actions qui se rapprochent le plus de celles que nos vrais utilisateurs peuvent faire tout en vÃ©rifiant comment le site rÃ©agit.\n\nIl y a quelques annÃ©es, cette stack technique Ã©tait plutÃ´t compliquÃ©e Ã  mettre en place, mais aujourdâ€™hui il est plutÃ´t simple de le faire.\nLe site qui hÃ©berge cet article de blog en est lui-mÃªme la preuve.\nIl ne mâ€™a fallu quâ€™une dizaine de minutes pour mettre en place cette stack avec le WebdriverIo CLI pour vÃ©rifier que mon blog fonctionne comme prÃ©vu.\n\nJâ€™ai dâ€™ailleurs rÃ©cemment publiÃ© un article prÃ©sentant la mise en place de cette stack.\n\nVoici donc un exemple de fichier de test E2E pour vous donner une idÃ©e:\n\nFeature: Playground\n\n  Background: Playground context\n    Given I use &quot;playground&quot; test context\n\n  Scenario: Check if playground is reachable\n    When As user &quot;toto@toto.fr&quot; I visit the &quot;playground&quot; page\n    And I click on &quot;playground trigger&quot;\n    Then I should see a &quot;visible playground&quot;\n    And I should see 4 &quot;playground tab&quot; in &quot;playground&quot;\n\n    When I click on &quot;playground trigger&quot;\n    Then I should not see a &quot;visible playground&quot;\n\n    # ...\n\n\nEt Ã§a donne Ã§a en local avec mon navigateur Chrome !\n\n\n\nVoilÃ  un schÃ©ma qui explique comment cette stack fonctionne:\n\n\n\nAujourdâ€™hui, lâ€™application web de Bedrock possÃ¨de plus de 800 scÃ©narios de tests E2E qui tournent sur chacune de nos Pull Request et sur la branche master.\nIls nous assurent que nous nâ€™introduisons pas de rÃ©gression fonctionnelle et câ€™est juste gÃ©nial !\n\nğŸ‘ Les points positifs\n\n\n  WebdriverIO nous permet Ã©galement de lancer de maniÃ¨re journaliÃ¨re ces mÃªmes tests sur des vrais devices en passant par le service payant SAAS Browserstack.\nOn a donc tous les jours un job qui sâ€™assure que notre site fonctionne correctement sur un Chrome derniÃ¨re version sur Windows 10 et Safari sur MacOs.\n  Ces tests nous permettent de facilement documenter les fonctionnalitÃ©s de lâ€™application grÃ¢ce au langage Gherkin.\n  Ils nous permettent de reproduire des cas qui sont loin dâ€™Ãªtre nominaux.\nDans une logique TDD, ils permettent dâ€™avancer sur le dÃ©veloppement sans avoir Ã  cliquer pendant des heures.\n  Ces tests nous ont permis de ne pas casser lâ€™ancienne version du site qui est toujours en production pour quelques clients alors que nos efforts se concentrent sur la nouvelle.\n  Ils nous apportent une vraie confiance.\n  GrÃ¢ce notre librairie superagent-mock, nous pouvons fixturer (bouchonner, mocker) toutes les API dont on dÃ©pend et ainsi mÃªme vÃ©rifier les cas dâ€™erreurs.\nDe plus, mocker la couche XHR du navigateur permet une amÃ©lioration significative du temps dâ€™exÃ©cution des tests. ğŸš€\n  Ils nous donne accÃ¨s Ã  des usages Ã©tendus comme :\n    \n      vÃ©rification de rÃ¨gles dâ€™accessibilitÃ©\n      check les logs de la console navigateur (pour ne pas introduire dâ€™erreur ou de React Warning par exemple)\n      surveiller tous les appels rÃ©seaux du site grÃ¢ce Ã  un proxy\n      et jâ€™en passeâ€¦\n    \n  \n\n\nğŸ‘ Les complications\n\n\n  Maintenir cette stack est compliquÃ© et coÃ»teux.\nÃ‰tant donnÃ© que peu de ressources sont publiÃ©es sur ce domaine, on se retrouve parfois Ã  devoir creuser pendant plusieurs jours pour les rÃ©parer ğŸ˜….\nIl nous arrive de nous sentir parfois bien seul Ã  avoir ces soucis.\n  Il est trÃ¨s facile de coder un test E2E dit flaky (ie: un test qui peut Ã©chouer alÃ©atoirement).\nIls nous font croire que quelque chose est cassÃ©.\nIls nous prennent parfois du temps Ã  les stabiliser.\nIl reste cependant bien meilleur de supprimer un test qui ne vous donnera pas un rÃ©sultat stable.\n  Faire tourner tous les tests prend un temps important sur notre intÃ©gration continue.\nIl faut rÃ©guliÃ¨rement travailler sur leur optimisation pour que le feedback quâ€™ils vous apportent soit le plus rapide possible.\nCes temps importants coutent Ã©galement de lâ€™argent, il faut en effet bien faire tourner ces tests sur des machines.\nPour information, lâ€™infrastructure du site web (Ã  lui seul, juste lâ€™hÃ©bergement de nos servers Node + fichiers statiques + CDN) coutent bien moins cher que notre intÃ©gration continue.\nCela fait bien Ã©videmment sourire nos Ops ! ğŸ˜Š\n  Les nouvelles recrues de nos Ã©quipes nâ€™ont souvent jamais rÃ©alisÃ© ce genre de tests, il y a donc une phase de galÃ¨re dâ€™apprentissage..\n  Certaines fonctionnalitÃ©s sont parfois trop compliquÃ©es Ã  tester avec notre stack E2E (par exemple, les parcours de paiement qui dÃ©pendent de tiers).\nIl nous arrive alors de nous rabattre sur dâ€™autres techniques avec Jest notamment en ayant un scope moins unitaire.\n\n\nNos tests â€œunitairesâ€\n\nPour complÃ©ter nos tests fonctionnels nous avons Ã©galement une stack de tests Ã©crits avec Jest.\nOn qualifie ces tests dâ€™unitaires car nous avons comme principe dâ€™essayer de toujours tester nos modules JS en indÃ©pendance des autres.\n\nNe dÃ©battons pas ici sur â€œEst-ce que ce sont des vrais tests unitaires ?â€, suffisamment dâ€™articles sur internet traitent de ce sujet.\n\nOn utilise ces tests pour diffÃ©rentes raisons qui couvrent des besoins que nos tests fonctionnels ne couvrent pas:\n\n\n  nous aider Ã  dÃ©velopper nos modules JS avec des pratiques TDD.\n  documenter et dÃ©crire comment fonctionne un module JS.\n  tester des cas limites trÃ¨s/trop compliquÃ©s Ã  tester avec nos tests E2E.\n  faciliter le refactoring de notre application en nous montrant les impacts techniques de nos modifications.\n\n\nAvec ces tests, on se met au niveau dâ€™une fonction utilitaire, dâ€™une action Redux, dâ€™un reducer, dâ€™un composant React.\nOn se base essentiellement sur la fonctionnalitÃ© dâ€™automock de Jest qui nous propose dâ€™isoler nos modules JS lorsquâ€™on teste.\n\n\n\nLâ€™image prÃ©cÃ©dente reprÃ©sente la mÃ©taphore qui nous permet dâ€™expliquer notre stratÃ©gie de tests unitaires aux nouveaux arrivant.\n\n\n  â€œIl faut sâ€™imaginer que lâ€™application est un mur composÃ© de briques unitaires (nos modules ecmascript), nos tests unitaires doivent tester une Ã  une les briques en indÃ©pendance totale des autres.\nNos tests fonctionnels sont lÃ  pour tester le ciment entre les briques.â€\n\n\nPour rÃ©sumer, on pourrait dire que nos tests E2E testent ce que notre application doit faire, et nos tests unitaires sâ€™assurent eux de vÃ©rifier comment Ã§a marche.\n\nAujourdâ€™hui ce sont plus de 6000 tests unitaires qui couvrent lâ€™application et permettent de limiter les rÃ©gressions.\n\nğŸ‘\n\n\n  Jest est vraiment une librairie gÃ©niale, rapide, complÃ¨te, bien documentÃ©e.\n  Les tests unitaires nous aident beaucoup Ã  comprendre plusieurs annÃ©es aprÃ¨s comment tout cela fonctionne.\n  On arrive toujours Ã  tester unitairement notre code, et cela complÃ¨te bien nos tests E2E.\n  Lâ€™automock est vraiment pratique pour le dÃ©coupage de tests par modules.\n\n\nğŸ‘\n\n\n  Parfois, nous nous sommes trouvÃ©s limitÃ©s par notre stack de tests E2E et nous ne pouvions pas uniquement nous baser sur les tests unitaires.\nIl nous manquait quelque chose pour pouvoir sâ€™assurer que le ciment entre les briques fonctionnait comme on le souhaitait.\nPour cela, il a Ã©tÃ© mis en place une deuxiÃ¨me stack de tests Jest nommÃ© â€œtest dâ€™intÃ©grationâ€ ou lâ€™automock est dÃ©sactivÃ©.\n  Lâ€™abus de Snapshot est dangereux pour la santÃ©.\nLâ€™usage du â€œSnapshot testingâ€ peut faire gagner du temps sur lâ€™implÃ©mentation de vos tests mais peuvent en rÃ©duire la qualitÃ©.\nAvoir Ã  review un object de 50 lignes en Snapshot est ni facile, ni pertinent.\n  Avec la dÃ©prÃ©ciation dâ€™EnzymeJS, nous sommes contraints de migrer sur React Testing Library.\nIl est bien Ã©videmment possible de tester unitairement des composants avec cette nouvelle librairie.\nMalheureusement, ce nâ€™est pas vraiment lâ€™esprit et la faÃ§on de faire.\nReact Testing Library nous pousse Ã  ne pas jouer avec le shallow rendering.\n\n\nNos principes\n\nNous essayons de toujours respecter les rÃ¨gles suivantes lorsquâ€™on se pose la question â€œDois-je ajouter des tests ?â€.\n\n\n  Si notre Pull Request introduit des nouvelles fonctionnalitÃ©s utilisateurs, il faut intÃ©grer des scenarios de test E2E.\nDes tests unitaires avec Jest peuvent les complÃ©ter / remplacer en fonction.\n  Si notre Pull Request a pour but de corriger un bug, cela signifie quâ€™il nous manque un cas de test.\nOn doit donc essayer de rajouter un test E2E ou Ã  dÃ©faut un test unitaire.\n\n\nCâ€™est en Ã©crivant ces lignes que je me dis que ces principes pourraient trÃ¨s bien faire lâ€™objet dâ€™une automatisation. ğŸ¤£\n\nLe projet reste, les fonctionnalitÃ©s non\n\n\n  â€œLa seconde Ã©volution dâ€™une fonctionnalitÃ© est trÃ¨s souvent sa suppression.â€\n\n\nPar principe, nous souhaitons faire en sorte que chaque nouvelle fonctionnalitÃ© de lâ€™application ne base pas son activation sur le simple fait dâ€™Ãªtre dans la codebase.\nClassiquement, le cycle de vie dâ€™une â€œfeatureâ€ dans un projet peut Ãªtre le suivant (dans un Github Flow):\n\n\n  une personne implÃ©mente sur une branche\n  la fonctionnalitÃ© est mergÃ©e sur master\n  elle est dÃ©ployÃ©e en production\n  vis sa vie de fonctionnalitÃ© (avec parfois des bugs et des correctifs)\n  la fonctionnalitÃ© nâ€™est plus nÃ©cessaire\n  une personne dÃ©tricote le code et lâ€™enlÃ¨ve\n  nouveau dÃ©ploiement\n\n\nPour simplifier certaines Ã©tapes, il a Ã©tÃ© mis en place du feature flipping sur le projet.\n\nComment Ã§a marche ?\n\nDans notre config il y a une map clÃ©/valeur qui liste toutes les fonctionnalitÃ©s de lâ€™application associÃ©es Ã  leur statut dâ€™activation.\n\nconst featureFlipping = {\n  myAwesomeFeature: false,\n  anotherOne: true,\n}\n\n\nDans notre code, nous avons donc implÃ©mentÃ© des traitements conditionnels qui disent â€œSi cette feature est activÃ©e alorsâ€¦â€.\nCela peut changer le rendu dâ€™un composant, changer lâ€™implÃ©mentation dâ€™une action Redux ou bien dÃ©sactiver une route de notre react-router.\n\nMais Ã  quoi Ã§a sert ?\n\n\n  On peut dÃ©velopper des nouvelles Ã©volutions progressivement en les cachant derriÃ¨re une clÃ© de configuration.\nOn livre des fonctionnalitÃ©s en production sans les activer.\n  En environnement de test, on peut surcharger cette config pour tester des features qui ne sont pas encore activÃ©es en production.\n  Dans le cas dâ€™un site en marque blanche, on peut proposer ces fonctionnalitÃ©s Ã  nos clients comme des options possibles.\n  Avant de supprimer le code dâ€™une feature, on la dÃ©sactive puis on fait le mÃ©nage sans risque.\n  GrÃ¢ce Ã  un outil maison nommÃ© lâ€™Applaunch, cette config de feature flipping est surchargeable dans une interface graphique Ã  chaud sans dÃ©ploiement.\nCela nous permet dâ€™activer des fonctionnalitÃ©s sans faire de mise en production du code.\nEn cas dâ€™incident, on peut dÃ©sactiver des fonctionnalitÃ©s qui sont dÃ©gradÃ©es.\n\n\nPour vous donner un exemple plus concret, entre 2018 et 2020 nous avons complÃ¨tement refondu lâ€™interface de lâ€™application.\nCette Ã©volution graphique nâ€™Ã©tait quâ€™une clÃ© de featureFlipping.\nLa refonte graphique nâ€™a donc pas Ã©tÃ© la remise Ã  zÃ©ro du projet, on continue encore aujourdâ€™hui de vivre avec les deux versions (tant que la bascule de tous nos clients nâ€™est pas terminÃ©e).\n\n\n\nLâ€™A/B testing\n\nGrÃ¢ce au super travail des Ã©quipes backend et data, on a pu mÃªme Ã©tendre lâ€™usage du feature flipping en rendant cette configuration modifiable pour des sous groupes dâ€™utilisateurs.\n\nCela permet de dÃ©ployer des nouvelles fonctionnalitÃ©s sur une portion plus rÃ©duite des utilisateurs afin de comparer nos KPI.\n\nPrise de dÃ©cision, amÃ©lioration des performances techniques ou produit, expÃ©rimentations, les possibilitÃ©s sont nombreuses et nous les exploitons de plus en plus.\n\nLe futur flipping\n\n\n  Sur une idÃ©e originale de Florent Lepretre.\n\n\nNous avions rÃ©guliÃ¨rement le besoin dâ€™activer des feature Ã  des heures trÃ¨s trop matinales dans le futur.\nPour cela nous devions Ãªtre connectÃ© Ã  une heure prÃ©cise sur notre poste pour modifier la configuration Ã  chaud.\n\nAfin dâ€™Ã©viter dâ€™oublier de le faire, ou de le faire en retard, nous avons fait en sorte quâ€™une clÃ© de configuration puisse Ãªtre activÃ©e Ã  partir dâ€™une certaine date.\nPour cela, nous avons fait Ã©voluer notre selector redux qui indiquait si une feature Ã©tait activÃ©e pour quâ€™il puisse gÃ©rer des formats de date et les comparer Ã  lâ€™heure courante.\n\nconst featureFlipping = {\n  myAwesomeFeature: {\n    offDate: &#39;2021-07-12 20:30:00&#39;,\n    onDate: &#39;2021-07-12 19:30:00&#39;,\n  },\n}\n\n\n\n  De nombreux cafÃ©s â˜•ï¸ Ã  9h ont Ã©tÃ© sauvÃ©s grÃ¢ce au futur flipping\n\n\nMonitorer, Mesurer, Alerter\n\nPour maintenir un projet aussi longtemps que lâ€™application web de bedrock, des tests, de la documentation et de la rigueur ne suffisent pas.\nIl faut Ã©galement de la visibilitÃ© sur ce qui marche en production.\n\n\n  â€œComment sais-tu que lâ€™application que tu as en production en ce moment mÃªme fonctionne comme prÃ©vu ?â€\n\n\nOn part du principe quâ€™aucune fonctionnalitÃ© ne marche tant quâ€™elle nâ€™est pas monitorÃ©e.\nAujourdâ€™hui le monitoring Ã  Bedrock cotÃ© Frontend se matÃ©rialise par diffÃ©rents outils et diffÃ©rentes stacks.\nJe pourrais vous citer NewRelic, un Statsd, une stack ELK ou bien encore Youbora pour la vidÃ©o.\n\nPour vous donner un exemple, Ã  chaque fois quâ€™un utilisateur commence une session de navigation on envoie un Hit de monitoring anonyme pour incrÃ©menter un compteur dans Statsd.\nOn a alors plus quâ€™Ã  dÃ©finir un dashboard qui affiche dans un graphique lâ€™Ã©volution de ce nombre.\nSi on observe une variation trop importante, cela peut nous permettre de dÃ©tecter un incident.\n\n\n\nLe monitoring nous offre aussi des solutions pour comprendre et analyser un bug qui sâ€™est produit dans le passÃ©.\nComprendre un incident, lâ€™expliquer, en trouver sa root cause sont les possibilitÃ©s qui sâ€™offrent Ã  vous si vous monitorez votre application.\nLe monitoring peut Ã©galement permettre de mieux communiquer avec les clients sur les impacts dâ€™un incident et Ã©galement dâ€™estimer le nombre dâ€™utilisateurs impactÃ©s.\n\nAvec la multiplication de nos clients, bien monitorer nos plateformes nâ€™est plus suffisant.\nTrop de donnÃ©es, trop de dashboards Ã  surveiller, il devient trÃ¨s facile de louper quelque chose.\nNous avons donc commencÃ© Ã  complÃ©ter notre suivi des mesures par de lâ€™alerting automatique.\nUne fois que les mesures nous apportent suffisamment de confiance, on peut facilement mettre en place des alertes qui vont nous prÃ©venir en cas de valeur incohÃ©rente.\n\nNous essayons cependant de toujours dÃ©clencher des alertes uniquement quand celle-ci est actionnable.\nDans dâ€™autres termes, si une alerte sonne, nous avons quelque chose Ã  faire.\nFaire sonner des alertes qui ne nÃ©cessitent aucune action immÃ©diate humaine gÃ©nÃ¨rent du bruit et de la perte de temps.\n\n\n\nLimiter, surveiller et mettre Ã  jour ses dÃ©pendances\n\nCe qui pÃ©rime plus vite que votre ombre dans un projet web basÃ© sur des technologies javascript, ce sont vos dÃ©pendances.\nLâ€™Ã©cosystÃ¨me Ã©volue rapidement et vos dÃ©pendances peuvent vite se retrouver non maintenues, plus Ã  la mode ou bien complÃ¨tement refondues avec de gros breaking changes.\n\nOn essaye donc dans la mesure du possible de limiter nos dÃ©pendances et dâ€™Ã©viter dâ€™en ajouter inutilement.\nUne dÃ©pendance, câ€™est souvent trÃ¨s facile Ã  ajouter mais elle peut devenir un vrai casse-tÃªte Ã  enlever.\n\nLes librairies de composants graphiques (exemple React bootstrap, Material Design) sont un bel exemple de dÃ©pendance que nous tenons Ã  ne pas introduire.\nElles peuvent faciliter lâ€™intÃ©gration dans un premier temps mais celles-ci bloquent souvent la version de votre librairie de composant par la suite.\nVous ne voulez pas figer la version de React dans votre application pour deux composants de formulaires.\n\nLa surveillance fait aussi partie de nos routines de gestion de nos dÃ©pendances.\nDepuis lâ€™ajout du signalement de failles de sÃ©curitÃ© dans un package NPM, il est possible de savoir si un projet intÃ¨gre une dÃ©pendance qui contient une faille de sÃ©curitÃ© connue par une simple commande.\nNous avons donc des jobs journaliers sur nos projets qui lancent la commande yarn audit afin de nous forcer Ã  appliquer les correctifs.\n\n\n  La maintenance de dÃ©pendances est grandement facilitÃ© par notre stack de tests E2E qui sonnent direcement si la montÃ©e de version gÃ©nÃ¨re une regression.\n\n\nAujourdâ€™hui, hors failles de sÃ©curitÃ©, nous mettons Ã  jour nos dÃ©pendances â€œquand on a le tempsâ€, souvent en fin de sprint.\nCela ne nous satisfait pas car certaines dÃ©pendances peuvent se retrouver oubliÃ©es.\nJâ€™ai personnellement lâ€™habitude dâ€™utiliser des outils comme yarn outdated et Dependabot sur mes projets personels pour automatiser la mise Ã  jour de mes dÃ©pendances.\n\nAccepter sa dette technique\n\nUn projet accumulera toujours de la dette technique.\nCâ€™est un fait.\nQue ce soit de la dette volontaire ou involontaire, un projet qui rÃ©siste aux annÃ©es va forcÃ©ment accumuler de la dette.\nDâ€™autant plus, si pendant toutes ces annÃ©es vous continuez dâ€™ajouter des fonctionnalitÃ©s.\n\nDepuis 2014, nos bonnes pratiques, nos faÃ§ons de faire ont bien Ã©voluÃ©.\nParfois nous avons dÃ©cidÃ© ces changements mais parfois nous les avons subi (un exemple, lâ€™arrivÃ©e des composants fonctionnels avec React et lâ€™api des Hooks).\n\nNotre projet nâ€™est pas complÃ¨tement â€œstate of artâ€ et on lâ€™assume.\n\n\n\nNous essayons de prioriser nos sujets de refactoring sur les parties de lâ€™application sur lequel on a le plus de souci, le plus de peine.\nOn considÃ¨re quâ€™une partie de lâ€™application qui ne nous plaÃ®t pas mais sur laquelle on nâ€™a pas besoin de travailler (apporter des Ã©volutions) ne mÃ©rite pas quâ€™on la refactorise.\n\nJe pourrais vous citer de nombreuses fonctionnalitÃ©s de notre application qui nâ€™ont pas Ã©voluÃ© fonctionnellement depuis plusieurs annÃ©es.\nMais comme nous avons couvert ces fonctionnalitÃ©s de tests E2E depuis le dÃ©but, nous nâ€™avons pas vraiment eu Ã  y retoucher.\n\nComme dit plus haut, la prochaine Ã©volution dâ€™une feature de code est parfois sa dÃ©sactivation.\nAlors pourquoi passer son temps Ã  rÃ©-Ã©crire toute lâ€™application ?\n\n\n  Le code devient dans tous les cas du â€œlegacyâ€.\n  Tant que les fonctionnalitÃ©s sont testÃ©es, rien ne nous oblige Ã  tout refactorer en permanence pour que toute notre codebase soit state of art.\n  On se focalise sur nos pain points, on re-factorise ce quâ€™on a vraiment besoin de faire Ã©voluer.\n\n\nPour rÃ©sumer\n\nLes bonnes pratiques prÃ©sentÃ©es ici restent bien Ã©videmment subjectives et ne sâ€™appliqueront pas parfaitement/directement dans vos contextes.\nJe suis cependant convaincu quâ€™elles peuvent probablement vous aider Ã  identifier ce qui peut faire passer votre projet de fun Ã  pÃ©rimÃ©.\nÃ€ Bedrock nous avons mis en place dâ€™autres pratiques que je nâ€™ai pas listÃ©es ici mais ce sera lâ€™occasion de faire un nouvel article un jour.\n\nEnfin, si vous souhaitez que je revienne plus en dÃ©tail sur certains chapitres prÃ©sentÃ©s ici, nâ€™hÃ©sitez pas Ã  me le dire, je pourrais essayer dâ€™y dÃ©dier un article spÃ©cifique.\n\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #14",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/07/16/bedrock-dev-facts-14.html",
    "date"     : "July 16, 2021",
    "excerpt"  : "Apportez vos devfacts Ã  la plage cet Ã©tÃ©!\n\nLes phases de test\n\n  Pour une fois quâ€™on est payÃ© Ã  faire des 500\n\n\nLe panier de crabes\n\n  _ On attend le GoNoGo\n\n  _ On ne pourrait pas attendre le RustNoRust ce serait plus performant\n\n\n1â€¦ 2â€¦ 3â€¦ extinc...",
  "content"  : "Apportez vos devfacts Ã  la plage cet Ã©tÃ©!\n\nLes phases de test\n\n  Pour une fois quâ€™on est payÃ© Ã  faire des 500\n\n\nLe panier de crabes\n\n  _ On attend le GoNoGo\n\n  _ On ne pourrait pas attendre le RustNoRust ce serait plus performant\n\n\n1â€¦ 2â€¦ 3â€¦ extincteurs\n\n  Parlant Ã  un hÃ©bergeur cloud\n\n  â€œTu sais la petite boite franÃ§aise en trois lettres qui a eu chaud ce matin?â€\n\n\nÃ‰chec et mat\n\n  Weâ€™ve been hacked by our incompetence\n\n\nLe vrai du faux\n\n  Câ€™est pas le bon true\n\n\nDarladidadada\nQuand tu fais du mÃ©nage dans ton compte AWS de tests et que tu te dis qu&amp;#39;un collÃ¨gue doit avoir l&amp;#39;oreille musicale ğŸ¶ pic.twitter.com/yIINlboQBy&amp;mdash; Pascal MARTIN (@pascal_martin) March 12, 2021\n\nReview in progress\n\n\nIPV6 câ€™est pas encore hyper clair\n\n  Oui mais câ€™est une grosse IP\n\n\nLa confiance\n\n  Normalement, si tout va bien, Ã§a marche!\n\n\nAlors cette mise en production ?\n\n  Elle sâ€™est bien passÃ©e mais il a fallu revert\n\n\nEt oui mon petit\n\n  Lâ€™Internet, câ€™Ã©tait nous\n\n\nUne API câ€™est pas si mal en fait\n\n  Quand je vois lâ€™UX de la console AWS, je me demande sâ€™il nâ€™y a pas une vÃ©ritable volontÃ© Ã  nous pousser Ã  faire de lâ€™infrastructure as code\n\n\nDe la part dâ€™un dev backend Php ğŸ˜±\n\n  Jâ€™aime mettre du javascript partout\n\n\nLe retour du come-back de lâ€™automatisation manuelle\n\n  On pourra faire un rattrapage Ã  moitiÃ© manuel, mais quand mÃªme automatique\n\n\nDans les mÃ©andres des fonctionnalitÃ©s de Airtable\n\n  Bon, on va faire simple, on va Ã©crire du JS\n\n\nUn mercredi Ã  15h55\n\n  Cette phrase est trop complexe pour un lundi matin\n\n\nÃ€ bicyclette ğŸš²\n\n  Le client il sâ€™en fout que tu mettes des petites roues sur ton vÃ©lo, il veut juste que tâ€™avances sans te pÃ©ter la gueule\n\n\nEntendu dans une rÃ©union Ã  distance\n\n  Rassurez-vous câ€™est pas moi qui ronfle câ€™est mon chat\n\n\nEn parlant de choix techniques\n\n  Attention le choix pris aujourdâ€™hui fera jurisprudence sur la suite\n\n\nCâ€™est gÃ©rÃ© !\n\n  Je crois que lâ€™on gÃ¨re cette erreur, mÃªme si on ne la comprend pas\n\n\n1 + 1 = 11\n\n  30 est un nombre Ã©norme\n\n\nEntendu juste aprÃ¨s une grosse MEP\n\n  Oh! Il faut que je mette en place un mail de vacances.\n\n\nDes frites ğŸŸ\n\n  Mes critÃ¨res de succÃ¨s, câ€™est les grosses patates\n\n\nPO, pas un mÃ©tier facile\n\n  Le dev: â€œEst-ce que les PO peuvent le faire ?â€\n\n  Le PO: â€œOn ne sait rien faire nousâ€\n\n\nBougez pas\n\n  Je dÃ©ploie le deployer\n\n\nUne lÃ©gende raconte quâ€™il faut dÃ©ployer deux fois le deployer\n\nUne petite Ã©quipe\n\n  On Ã©tait 80 stagiaires sur le projet\n\n\nğŸ°\n\n  La magie du chmod -R 777. Je te dirais pas que jâ€™en ai rajoutÃ© dans mes 3 derniÃ¨res PR pour fixer de la CI\n\n\nIterm 2 secret features\nMa recherche Google:\n\n  Iterm2 big sur blinking\n\n\nGitlab issue reponse :\n\n  Just turn on Prefs &amp;gt; Advanced &amp;gt; Work around Big Sur bug where a white line flashes at the top of the screen in full screen mode.\n\n\nLes quadricolors\n\n  â€œ_ Pourquoi avoir choisi du vert pour la doc ?â€\n\n  â€œ_ Ã‰coute je suis daltonien.â€\n\n\nPourquoi douter ?\n\n  â€œ_ Je ne sais pas comment valider fonctionnellement mon dÃ©veloppement.â€\n\n  â€œ_ Ben, pareil que tu lâ€™as testÃ©.â€\n\n  â€œ_ Je ne lâ€™ai pas testÃ©.â€\n\n  â€œ_ ğŸ˜…â€\n\n\nTimber !!!!\n\n  Dans la vie comme dans git tu peux pas supprimer la branch sur laquelle tâ€™es.\n\n\nCâ€™est du propre\n\n  La refacto, câ€™est comme le mÃ©nage, il faut en faire rÃ©guliÃ¨rement.\n\n\nLes conflits de canard\n\n  Câ€™est git checkout verdun ton rebase\n\n\nReconversion\n\n  De toute maniÃ¨re avec le rÃ©chauffement climatique, je vais tout plaquer et monter un business de vente de djellaba.\n\n\nIncident sur la ligne B\n\n  Parfois les rÃ©unions câ€™est comme les transports, tu es coincÃ© dedans et Ã§a nâ€™avance pas\n\n"
} ,
  
  {
    "title"    : "No-code, ou le dÃ©veloppement dâ€™applications ouvert Ã  dâ€™autres mÃ©tiers !",
    "category" : "",
    "tags"     : " conference, nocode, lowcode, afup",
    "url"      : "/2021/06/11/no-code-developpement-applications-ouvert-autres-metiers-pascal-martin.html",
    "date"     : "June 11, 2021",
    "excerpt"  : "Construire une application sans coderÂ ? Câ€™est une idÃ©e que jâ€™entends depuis le dÃ©but de mes Ã©tudesâ€¦ Et câ€™est la promesse de no-codeÂ !\nDâ€™ailleurs, pendant que des discours dÃ©clarent que nos enfants doivent apprendre Ã  coder Ã  lâ€™Ã©cole, nous Ã©crivons...",
  "content"  : "Construire une application sans coderÂ ? Câ€™est une idÃ©e que jâ€™entends depuis le dÃ©but de mes Ã©tudesâ€¦ Et câ€™est la promesse de no-codeÂ !\nDâ€™ailleurs, pendant que des discours dÃ©clarent que nos enfants doivent apprendre Ã  coder Ã  lâ€™Ã©cole, nous Ã©crivons nous-mÃªme dÃ©jÃ  des applications no-codeÂ ! Nâ€™avez-vous pas lancÃ© Excel rÃ©cemmentÂ ?\n\nCes derniÃ¨res annÃ©es, lâ€™approche no-code a Ã©voluÃ© et devient petit Ã  petit un concept viable.\nDes entreprises, startups ou mastodontes, se lancent sur ce marchÃ© et publient des outils et solutions qui aident Ã  rivaliser avec certaines des applications que nous aurions pu dÃ©velopper nous-mÃªmeâ€¦\nJe ne parle bien sÃ»r pas (encore?) de supprimer nos mÃ©tiersâ€¦ Mais est-ce que no-code ou low-code ne permettraient pas Ã  dâ€™autres profils que les nÃ´tres dâ€™avancer plus rapidement sur leurs projetsÂ ?\n\nÃ€ travers cette introduction, vous dÃ©couvrirez un pan de lâ€™approche no-code et jâ€™espÃ¨re vous montrer que le dÃ©veloppement dâ€™applications nâ€™est plus rÃ©servÃ© quâ€™aux dÃ©veloppeursâ€¦ Et que nos langages prÃ©fÃ©rÃ©s ne sont plus la rÃ©ponse Ã  toutes les questionsÂ !\n"
} ,
  
  {
    "title"    : "The organisational challenge of building a Data team: lessons learnt",
    "category" : "Data",
    "tags"     : " Data, Data Science, Data Engineering, Agile, BigData, Organization",
    "url"      : "/data/2021/05/19/organisational-challenge-building-data-team.html",
    "date"     : "May 19, 2021",
    "excerpt"  : "Disclaimer: this is a post Iâ€™ve been meaning to share ever since the Dailymotion team published this post, so itâ€™s going way back into our history but updated to our current status. Itâ€™s also an attempt to bring a counterpart to the blog posts of ...",
  "content"  : "Disclaimer: this is a post Iâ€™ve been meaning to share ever since the Dailymotion team published this post, so itâ€™s going way back into our history but updated to our current status. Itâ€™s also an attempt to bring a counterpart to the blog posts of many web leaders that have written about what worked but rarely detail the difficulties they met and the lessons they learnt.\nHereâ€™s a very personal (therefore biased) feedback of our story and the lessons we learnt down the road.\n\nAt Bedrock weâ€™re building the best streaming platform in Europe to help our customers be local streaming champions.\nWithin Bedrock (and before that within M6 where our team was born), we embraced Data in 2016. From a top management perspective the move was straight forward once the company had understood the strategic importance of Data and the major use cases we could address: staff a team and deliver massive value (since Data was definitely the new oil back in those days). But for the teams we built (now north of 40 people), it was another story.\n\nFirst iteration: the Dream Team\n\n\n\nThe first step M6 took was to hire an amazing, PhD qualified, Business focused, Data Scientist whose mission was to build a Data Science team and explore the companyâ€™s data to find valuable use cases. A few months later, we added an experienced Product owner (me) to help set the vision, the roadmap and lead our Data initiatives. And some months later we were joined by a rock star Data/Machine learning Engineer who was to create an engineering team and the data platform we could build upon.\n\nOur collective mission was very ambitious but also very unclear. Basically, we were in charge of using Data to have a major impact on the companyâ€™s business. Nobody in the management had a very clear idea of what that meant and how to get there, that was up to us.\nEach one of us had many ideas on how to make that happen, and quite different approaches of the way to get there. The caricatural (but quite realistic) outline was:\n\n  Our lead Data Scientist wanted data, tools and manpower to explore the data, find awesome machine learning use cases that he would build with the DS team he had staffed, onboard the business, have them buy in and go to production (instantly, in his view the step to production was the tiny part).\n  Our lead Data Engineer wanted to start with small use cases, build a production infrastructure and enrich the science step by step later down the road.\n  I wanted to align with our stakeholders and management to build a shared vision, prioritize ideas and then organize the team work to iterate and increase value on a regular basis.\n\n\n\n\nThis misalignment led to an early split between the Data Scientists who did a lot of exploration + POCâ€™s and the Data Engineers who worked on an industrial production platform. The projects were of different nature and everything was pretty smooth except for some frustration on my side because I was playing little to no role in the Data Science part of things. Overall, everybody was happy, and we started to deliver valueâ€¦ until our first real business (beyond tooling) cases entered the roadmap.\n\nThe first business case where we needed to leverage both engineering and scientific skills was A/B testing. After having scanned the market solutions to find a nice solution we chose to build our own because we wanted deep integration into all our frontends (including mobile apps and set-top boxes) and backends, a lot of freedom to build KPIâ€™s upon all of our Data Lake and a few other things. \nTo build the complete toolbox, we had to create a backend serving layer that would return the correct feature flags for the A/B user groups, calculate KPIâ€™s with a large combination of filters, ensure that the calculation pipeline was run every day and display the results in a decent dashboard. From a project management perspective, it was a headache because the KPI part needed to bring statistical expertise to production, combining statistical excellence and production grade reliability. In our first iteration, a Data Scientist did a POC, but it was no way near production standards. On a separate track, the Data Engineers did a totally separate POC that delivered a decent framework, the KPIâ€™s were calculated incorrectly. Our third iteration (that took 6 months to make happen) was a 2-day session of peer programming between a Data Scientist and a Data Engineer that (at last!) delivered something real: a POC that actually calculated the right metrics in a way that could be industrialized.\n\nThe second business case was just as messed up. Our Data Science team headed off solo to build a POC of a program recommendation engine (we stream videos that belong to TV programs). Within a couple of months, they built something that seemed nice, but didnâ€™t scale when run for our millions of users. They then went into optimising their code, rewriting and optimising again without any Data Engineer help for 4 more months until the algorithm was ready for an A/B test against our long standing business rules. And the POC lost the test. After some rounds of tuning, we stopped the initiative altogether because nobody believed in any potential success.\n\nWe repeated this type of scenario a few times, building up more and more frustration within the 2 teams and the stakeholders. We tried to break it down and make the collaboration work quite a few times, but 2 points couldnâ€™t be resolved:\n\n  The Data Scientists didnâ€™t want to embrace any product/project management, agile or not (though I believe the underlying issue was more about control in a â€œusâ€ versus â€œthemâ€ mindset).\n  The Data Engineering team refused to put any Python code (the Data Scientists language of choice) into production and imposed that everything would be Java or Scala, versionned on git, unit tested and monitored.\nThis looks like very usual trolls, but they kept us stuck for more than a year.\n\n\nAt the end, our organization literally cracked up. The tensions became conflicts, some people left the team and our top management had to dive in to pacify and rebuild something that would work.\nAlthough we had delivered value in several places, we were clearly under effective.\n\nMy personal take away on our collective difficulties boils down to 2 things:\n\n  Starting off with a bunch of rock stars made collaboration impossible because each one wanted to lead in a very personal way, without much compromise.\n  Letting things slip towards a comfortable separation of 2 expertise teams with 2 very different organisations and agendaâ€™s instead of insisting on aligning them led to a form of cold war with no collaboration at all.\n\n\n\n\nSecond iteration: pluridisciplinary teams\n\nThe conclusion of the managementâ€™s deep dive was that we needed to split the historic teams in a more official and long term way.\nThe Data Scientists would form the Data Lab in which they would do research and produce POCâ€™s and whitepapers + staff their own engineers if they needed any. And we would create the Data Factory that would be focused on delivery with the Data Engineers and some new Data Scientists that would work on the teamâ€™s backlog with the Data Engineers on a daily basis.\n\n\n\nFrom the Data Lab perspective, that relieved most of the stress because they were now officially free to set a very scientific agenda for themselves. At the start it was cool, but over time (this was 3 years back), their disconnection from â€œproductionâ€ put them very far away from the real world. The internal stakeholders turned away from them because their target was to make real things. It ultimately led them to having small business impact and a poor dynamic, ultimately reducing the team down.\nFrom the Data Factory perspective, the new organisation fixed it! Over the past couple of years, we have been building the vision and roadmap within the team with no distinction between the Data Science and Data Engineering roadmap. Our work with the stakeholders feels like weâ€™re now walking on our 2 feet, the solutions we imagine for their challenges mix plain data engineering solutions with algorithms and statistics seamlessly, and weâ€™re definitely delivering more value. And of course, there is no debate on what goes to production, everything the team does will end up live! At the time Iâ€™m writing, the Data Factory has scaled from ~10 people to more than 40 members.\n\n\n\nOrganization is one thing, collaboration is another\n\nOur approach to overcome our challenges was totally around changing the organization, but I truly believe the solution was also within the evolution of our mindset.\nWhen we officially split the Data Science and Data Engineering teams, we also increased staffing bringing new people into the game. During that round of staffing, we focused very much on soft skills and mindset, considering that authentic team players would ultimately bring us more value than very strong individuals. Somehow, we moved away from building the dream team and aimed to create a real team. Our idea was that we werenâ€™t doing rocket science, we were working on a bunch of features that needed to incorporate some data science properly in our pipelines.\nOnce the newcomers joined the team, we put a lot of attention into the way people work together, the team spirit they build, the collective dynamic. That went through team building events, defining team values, helping each team member know and understand the other individuals around, creating collaboration rituals, maximizing pair programming between Data Scientists and Data Engineers, etc. And more than anything, entering a more agile test and learn approach, including within the organisation and ways to collaborate.\nI truly believe that this was the key to our success and itâ€™s my major learning today: build a team.\n\nPutting the pieces together\n\nIn the Dailymotion team post I quoted in the introduction, thereâ€™s a quite precise and documented blueprint of the organisation about who does what between Data Scientists, Data Analysts and Data Engineers. Weâ€™ve never been that documented and structured for now, weâ€™ve given each team a lot of freedom about who does what in the process depending on individual capabilities and what the team likes best.\nOne thing we did do is to sort out the language &amp;amp; tooling trolls. At first, anything that went to production was written in Scala, deployed and reviewed via our Git &amp;amp; continuous deployment pipeline, unit tested and monitored. So if a Data Scientist wanted to work on the production code it was following those rules (that has made some of our Data Scientists need to learn Scala and other things). This is now evolving, thanks to strong local collaboration and we now deliver more and more Python to production :)\n\nConclusion\n\nOver the past 4 years, our data team has scaled from 2 to more than 40 people. In the early days we underestimated the fact that putting excellent individuals together wouldnâ€™t just work. We learnt the hard way that there was specific attention to be put into the organisation and the way we build teams and successful collaboration patterns between people with different domains of expertise and backgrounds. In that, our conclusion is totally similar to the learnings of the Dailymotion team.\nNow we have fixed the mindset and collaboration part of things, we feel nothing can go wrong, even if we changed our organization pattern.\nIf you want to go further, we spoke about this and some other challenges we faced with Morgiane Meglouli in a conference, the slides (in French) are available here\n\nLast but not least, Bedrock is scaling fast to help build streaming champions around Europe. If youâ€™d like to join us, check out our open positions\n\n"
} ,
  
  {
    "title"    : "Comment nous rÃ©duisons lâ€™augmentation de nos coÃ»ts AWS",
    "category" : "",
    "tags"     : " conference, aws, costs",
    "url"      : "/2021/03/30/comment-nous-reduisons-augmentation-couts-aws-pascal-martin.html",
    "date"     : "March 30, 2021",
    "excerpt"  : "MalgrÃ© les promesses du Cloud, votre facture AWS vous fait peurÂ ? Je vous comprendsÂ !\n\nVenez dÃ©couvrir comment, chez Bedrock, nous suivons et catÃ©gorisons les coÃ»ts dâ€™infrastructure de notre plateforme de VOD et de Replay. Jâ€™enchainerai avec un re...",
  "content"  : "MalgrÃ© les promesses du Cloud, votre facture AWS vous fait peurÂ ? Je vous comprendsÂ !\n\nVenez dÃ©couvrir comment, chez Bedrock, nous suivons et catÃ©gorisons les coÃ»ts dâ€™infrastructure de notre plateforme de VOD et de Replay. Jâ€™enchainerai avec un retour dâ€™expÃ©rience basÃ© sur trois ans de rÃ©ponses Ã  la question que nous nous posons tous : Â«Â comment rÃ©duire nos coÃ»ts AWSÂ ?Â Â»\n\nDiviser par deux une facture DynamoDBÂ ? Exploiter des instances spot Ã  -70%Â ? Effectuer moins dâ€™appels dâ€™APIsÂ ? RÃ©duire les transferts inter-AZ, massifs lorsque nous manipulons des vidÃ©osÂ ? Ce ne sont que quelques-unes des pistes que nous avons envisagÃ©esâ€¦\n"
} ,
  
  {
    "title"    : "Migration de 6play vers Le Cloud, retour dâ€™expÃ©rience.",
    "category" : "",
    "tags"     : " conference, cloud, migration, cloudsud",
    "url"      : "/2021/03/11/migration-6play-vers-le-cloud-retour-experience-pascal-martin.html",
    "date"     : "March 11, 2021",
    "excerpt"  : "En 2018, nous avons entamÃ© la migration de la plateforme 6play vers Le Cloud.\nÃ€ prÃ©sent, nous pilotons notre infrastructure AWS avec Terraform, utilisons des services managÃ©s et dÃ©ployons nos applications sous Kubernetes.\n\nPendant cette confÃ©rence...",
  "content"  : "En 2018, nous avons entamÃ© la migration de la plateforme 6play vers Le Cloud.\nÃ€ prÃ©sent, nous pilotons notre infrastructure AWS avec Terraform, utilisons des services managÃ©s et dÃ©ployons nos applications sous Kubernetes.\n\nPendant cette confÃ©rence, vous dÃ©couvrirez comment nous avons rÃ©alisÃ© cette migration. Vous trouverez des rÃ©ponses aux questions que vous vous posez si vous envisagez de revoir votre hÃ©bergement.\nComment avons-nous transformÃ© notre infrastructureÂ ? Quels impacts sur nos projetsÂ ? Comment nous sommes-nous organisÃ©sÂ ? Quels choix avons-nous effectuÃ©s tout au long du processusÂ ? Quâ€™avons-nous appris, quâ€™avons-nous fait Ã©voluerÂ ? Comment nos Ã©quipes se rÃ©partissent-elles les tÃ¢chesÂ ? Avons-nous dÃ» adapter nos applications PHPÂ ? Quelles difficultÃ©s avons-nous rencontrÃ©esÂ ?\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #13",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/03/02/bedrock-dev-facts-13.html",
    "date"     : "March 2, 2021",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nDes branchements surprenants\n\n  Ah tu test ta branche en prod ? Tu peux y aller, si tu touches pas au staging pas de soucis\n\n\nAngular conventional commit\n\nTrouvÃ© dans lâ€™historique\n\n\n\nTime is relative\n\n\n ...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nDes branchements surprenants\n\n  Ah tu test ta branche en prod ? Tu peux y aller, si tu touches pas au staging pas de soucis\n\n\nAngular conventional commit\n\nTrouvÃ© dans lâ€™historique\n\n\n\nTime is relative\n\n\n  Non mais câ€™est normal, ce sont des mois de 5 minutes\n\n\nLe confinement Ã  la campagne, câ€™est parfois compliquÃ©\n\n\n  DÃ©dicace a tous ceux qui comme moi, vont passer une bonne semaine Ã  batailler avec internet ! (au moins, jâ€™ai assez de dÃ©bit pour un mp3).\n320 Kbps\n\n\nApple ğŸ’°\n\n\n  Quand ca parle dâ€™argent, Apple ne se rate pas gÃ©nÃ©ralement\n\n\nLe courage\n\n\n  Une MEP le vendredi avec la CI KO, vous Ãªtes mes hÃ©ros ğŸ˜\n\n\nNote Ã  moi-mÃªme\n\n\n  TODO: test if my code is the problem\n\n\nAucun abus\n\n\n  Bon, je crois que je vais mâ€™auto-dÃ©clarer meilleur dÃ©veloppeur de lâ€™annÃ©e!\n\n\nUne semaine qui commence bien\n\n\n  3 push master, lundi matin, 11h30. La semaine va Ãªtre bonne\n\n\nSoit gentil, pas mÃ©chant, câ€™est pas gentil dâ€™Ãªtre mÃ©chant\n\n\n  A: â€œJe comprends pas lâ€™API elle repond rienâ€\n\n\n\n  B: â€œTâ€™as essayÃ© de demander gentiment ?â€\n\n\n{\n    &quot;error&quot;: {\n        &quot;status&quot;: 400,\n        &quot;message&quot;: &quot;Invalid parameters &#39;MAIS_TU_VAS_ME_REPONDRE_UN_TRUC_COHERENT_BORDEL_DE_MERDE_!?!?!?!?!?!?!?!?!!&#39; for route &#39;xxxxxxxxxxxxx&#39;&quot;\n    }\n}\n\n\nDes rÃ©ponses magiques\n\n\n  â€œso this case is never expected, but obviously it happensâ€ Ah ğŸ˜…, nous voilÃ  rassurÃ©\n\n\nAlerte gÃ©nÃ©rale\n\n\n  Quand on crÃ©e une alerte pour pouvoir la mettre en silence juste aprÃ¨s ğŸ‘Œ\n\n\nLe consentement\n\n\n  A: â€œJâ€™ai insistÃ© et câ€™est passÃ©â€\n\n\n\n  B: â€œTu as insistÃ© mais est-ce que tu as pensÃ© au consentement de GraphQL et de postgre ?â€\n\n\nğŸ¦¥, moi paresseux ?\n\n\n  MÃªme ne rien faire câ€™est dÃ©jÃ  pas mal de taf\n\n\n? Question ?\n\n\n\nğŸ— Mnom Mnom\n\n\n  A: â€œAh mince jâ€™ai pas mis mon texte en escalopeâ€\n\n\n\n  B: â€œ?â€\n\n\n\n  A: â€œEntre quoteâ€\n\n\nLa documentation Ã©ternelle\n\n\n  Les temps changent mais pas les readme\n\n\nLes petits gÃ©nies\n\n\n  Nan mais attendez vous Ã  plein de commentaires disant que câ€™est codÃ© avec les pieds, et que Jean-Kevin, 16 ans, aurait mieux fait en 2j pendant son week-end chez sa mamie Germaine, pendant quâ€™il faisait une pause dans le dÃ©veloppement du nouveau Bitcoin.\n\n\nBlackhole sun\n\nEn parlant de lignes supprimÃ©es.\n\n\n\nTechnique de sioux\n\n\n  Nan mais rÃ©Ã©crit les status 404 en 200 et Ã§a marchera !\n\n\nOn peut revert une fois, mais pas quinze !\n\n\n\nDÃ©finition AWS\n\n\n  AZ = Ama Zones\n\n\nHeureusement pour nous\n\n\n  Les cas sans erreurs fonctionnent trÃ¨s bien\n\n\nManager === ğŸ ?\n\n\n  La formatrice : je vous donne un mot et vous me donnez le premier mot qui vous passe par la tÃªte.\n\n\n\n  La formatrice : â€œChefâ€ ?\n\n\n\n  Un participant : ChÃ¨vre\n\n\nIl nâ€™y a pas de â€œmaisâ€ !\n\n\n  Je ne veux pas remettre en cause toute lâ€™architecture MAIS â€¦\n\n\nğŸ’ Singe ?\n\n\n  Quand jâ€™entends â€œGo/NoGoâ€, mon cerveau comprend â€œBonoboâ€\n\n\nAvec du savon !\n\n\n  A: â€œIls font du SOAP!â€\n\n\n\n  A: â€œCâ€™est pour Ãªtre bien propreâ€\n\n\nLâ€™automatisation DIY\n\n\n  Câ€™est automatiquement fait Ã  la main\n\n\nğŸ¦¥ le retour\n\n\n  La paresse nâ€™est pas un dÃ©faut, câ€™est une optimisation\n\n\nAstuce de pro!\n\n\n  Si vos collÃ¨gues et votre conscience vous embÃªtent quand vous dites Â« je vais tester ce changement en prod Â», dites Ã  la place Â« je vais valider ce changement en prod Â». Vous verrez, Ã§a ira tout de suite mieux !\n\n"
} ,
  
  {
    "title"    : "Machine Learning en production",
    "category" : "",
    "tags"     : " machine learning, Lyon Data Science, conference",
    "url"      : "/2021/01/21/machine-learning-en-production.html",
    "date"     : "January 21, 2021",
    "excerpt"  : "Une fois passÃ©e la phase de prototype, comment va-t-on en production quand on fait du machine learning ?\nComment sâ€™assure-t-on que tout va bien une fois en production ?\nDÃ©ploiement, tests, monitoring, etc. Il y a beaucoup de choses Ã  penser. Sur c...",
  "content"  : "Une fois passÃ©e la phase de prototype, comment va-t-on en production quand on fait du machine learning ?\nComment sâ€™assure-t-on que tout va bien une fois en production ?\nDÃ©ploiement, tests, monitoring, etc. Il y a beaucoup de choses Ã  penser. Sur ce long sujet, je vous propose ici une petite introduction basÃ©e sur mes expÃ©riences.\n"
} ,
  
  {
    "title"    : "Three years running Kubernetes on production at Bedrock",
    "category" : "",
    "tags"     : " Infrastructure, Cloud, Kubernetes, Kops, AWS, HAProxy",
    "url"      : "/2020/12/08/Three-years-running-kubernetes-on-production-at-Bedrock.html",
    "date"     : "December 8, 2020",
    "excerpt"  : "We migrated our first application to a Kubernetes cluster at AWS in 2018 (a colleague even wrote a book about it).\nThree years later, we manage a dozen clusters, to which we have added a lot of tools and we have a much better grasp of certain subt...",
  "content"  : "We migrated our first application to a Kubernetes cluster at AWS in 2018 (a colleague even wrote a book about it).\nThree years later, we manage a dozen clusters, to which we have added a lot of tools and we have a much better grasp of certain subtleties.\nEach cluster reaches, depending on the load, hundreds of nodes and thousands of pods.\n\nTable of Contents\n\n\n  Base\n    \n      Kops and templates\n      Tools we use\n      Keep tools up to date on all clusters\n    \n  \n  Resiliency\n    \n      DNS\n      Lots of AutoScalingGroups\n      Dedicated AutoScalingGroups by app\n      QOS Guaranteed Daemonsets\n    \n  \n  Scalability\n    \n      Cluster Autoscaler\n        \n          Expander Priority\n        \n      \n      Overprovisioning\n      PriorityClass\n      Low HPA targets\n      Long downscale durations\n    \n  \n  Observability\n    \n      Metrics\n      Logs\n      Alerting\n    \n  \n  Costs\n    \n      Spot instances\n        \n          Inter accounts reclaims\n          On-demand fallback\n          Draino and node-problem-detector\n          Spot Tips\n        \n      \n      Kube-downscaler\n      HAProxy Ingress Controller\n    \n  \n\n\n\n\nBase\n\nKops and templates\n\nEKS didnâ€™t exist when we started to work on Kubernetes on AWS. So we use Kops which, by the way, works very well.\nKops creates, updates and deletes our clusters, but also associates resources on our AWS accounts: DNS zone + entries, AutoScalingGroups, SecurityGroups, etc.\nOur rolling updates and rolling upgrades are 100% handled by kops which never failed us.\n\nBecause we have several clusters, we use kops toolbox template instead of having a single YAML file per cluster. We have mutualized resources definitions, like AutoScalingGroups, DNS options or namespaces list, inside common files and use a dedicated template file per cluster, referencing mutualized configs through variables.\n\nFor example, the EC2 instance types will be defined as snippets:\n\nÂ± cat snippets/spot_4x_32Gb_machine_type.yaml:\n- c5.4xlarge\n- c5d.4xlarge\n- c5n.4xlarge\n\nAnd used inside a generic template file:\n\nÂ± cat templates/3_spot-nodes.yaml.tpl\nâ€¦\n  mixedInstancesPolicy:\n    instances:\n    { { if eq $index &quot;4x_32Gb&quot; } }\n    { { include &quot;spot_4x_32Gb_machine_type.yaml&quot; . | indent 4 } }\n    { { end } }\nâ€¦\n\nFinally, if the cluster requires an ASG with instances size 4x with 32GB RAM on Spot instances:\n\nÂ± cat vars/prod-customer.k8s.foo.bar.yaml\nâ€¦\nspot_nodes:\n  4x_32Gb:\n    az:\n      - eu-west-3a\n      - eu-west-3b\n      - eu-west-3c\n    min: 1\n    max: 100\n\nA bash script orchestrates all this. It generates manifest files, creates/updates clusters and checks everything is operating normally.\n\nAll of the above lives as files in a git repository, ensuring weâ€™re doing only Infrastructure as Code.\n\nWe never make any Infrastructure modification outside of code.\n\nTools we use\n\nWe add some tools to a raw Kubernetes cluster:\n\n\n  aws-iam-authenticator\n  cluster-autoscaler\n  cloudwatch-exporter-in-cluster\n  cni-metrics-helper\n  draino\n  elasticsearch-cerebro\n  fluentd\n  haproxy-ingress-controller\n  iam-role-for-serviceaccount\n  k8s-spot-termination-handler\n  kube-downscaler\n  logstash\n  loki\n  metrics-server\n  node-problem-detector\n  overprovisioning\n  prometheus\n  prometheus-dnsmasq-exporter\n  prometheus-pushgateway\n  statsd-exporter\n  statsd-proxy\n  victoria-metrics-cluster\n\n\nSome of those tools stand for compatibility reasons after our cloud migration, so our developers can still use our ELK stack, or a statsd format to generate metrics.\n\nWe need all these tools to have a production-ready cluster, so we can provide scaling, resilience, observability, security with controlled costs. This list isnâ€™t even exhaustive.\n\nIt evolved a lot over the last two years and will surely evolve a lot in the near future, as both Kubernetes and AWS are moving playgrounds.\n\nKeep tools up to date on all clusters\n\nWe use a Jenkins job for that.\n\nWe deploy k8s-tools the same way we deploy our apis in the cluster: with bash scripts and a helm chart, dedicated per application.\n\nÂ± tree app/loki/.cloud/       \napp/loki/.cloud/\nâ”œâ”€â”€ charts\nâ”‚   â”œâ”€â”€ Chart.yaml\nâ”‚   â”œâ”€â”€ templates/\nâ”‚   â”œâ”€â”€ values.yaml\nâ”‚   â”œâ”€â”€ values.customerX.yaml\nâ”‚   â””â”€â”€ values.customerY.yaml\nâ””â”€â”€ jenkins\n    â”œâ”€â”€ builder.sh\n    â””â”€â”€ deployer.sh\n\nA Jenkins job runs the builder.sh, then the deployer.sh script for every k8s-tool.\nbuilder.sh is run when we need to build our own Docker images.\ndeployer.sh handles the Helm Chart deployment subtleties.\nAll apps are first deployed on all our staging clusters, then on prod.\n\nConsistency is maintained over all our clusters through this Jenkins job.\n\nResiliency\n\nDNS\n\nLike everyone whoâ€™s using Kubernetes on production, at some point, we faced an outage due to DNS. It was either UDP failing because of a kernel race condition, or musl (Alpine Linuxâ€™s replacement of glibc) not correctly handling domain or search, or also the default ndots 5 dnsConfig, or even KubeDNS not handling peak loads properly.\n\nAs of today:\n\n\n  We are using a local DNS cache on each worker node, with dnsmasq,\n  We use Fully Qualified Domain Names (trailing dot on curl calls) as much as possible,\n  Weâ€™ve defined dnsConfig preferences for all our applications,\n  We use CoreDNS with autoscaling as a replacement for KubeDNS,\n  We forbid as much as possible musl/Alpine\n\n\nExample of a dns configuration in prod:\n\n  dnsConfig:\n    options:\n    - name: use-vc\n    - name: single-request\n    - name: single-request-reopen\n    - name: ndots\n      value: &quot;1&quot;\n  dnsPolicy: ClusterFirst\n\n\ndnsPolicy: ClusterFirst makes sure weâ€™re using the nodeâ€™s loopback interface, so pods will send their DNS requests to dnsmasq installed locally on each node.\nDnsmasq forwards DNS queries to CoreDNS for cluster.local. sub-domains and to the VPCâ€™s DNS server for the rest.\n\nLots of AutoScalingGroups\n\nWe had a dozen AutoScalingGroups per cluster.\nThis was both for resiliency and because we use Spot instances.\nWith Spot instance reclaims, we needed to have a lot of instance types and family types: m5.4xlarge, c5.4xlarge, m5n.8xlarge, etc.\nThis is an autoscaler recommendation to split AutoScalingGroups so that each ASG has the same amount of RAM and number of CPU cores when using mixed instances policies.\nAs a result, we had ASGs like:\n\n\n  spot_4x_32Gb\n  spot_4x_64Gb\n  spot_4x_128Gb\n\n\nLots of AutoScalingGroups doesnâ€™t work well\n\nAZ rebalancing doesnâ€™t work anymore when using more than one ASG. It becomes totally unpredictable and uncontrollable. It is even a total nightmare with a dozen ASGs.\n\nYou can see the difference of outgoing traffic between our 3 NAT Gateway over 4 hours time range :\n\nThe blue NAT gateway is used way more than the two others between 19h00 and 22h00. The green NAT gateway is used half as much as the other two during peak usage times.\nThis is because AZ-rebalacing has resulted in twice as many instances in one AZ than in the others.\n\nAlso, Kubernetesâ€™s cluster-autoscaler isnâ€™t really compatible with many AutoScalingGroups. Weâ€™ll cover how it works later in this post (Scalability/ExpanderPriority), but keep in mind that each application should run on no more than a maximum of 4 ASGs. This is due to the failover mechanism of cluster-autoscaler that doesnâ€™t detect ASGs errors like InsufficientInstanceCapacity, which considerably increases the scale-up time. We are particularly concerned because we need to scale quickly and intensely.\n\nWeâ€™ve rolled-back on the ASG number. We now have a maximum of 4 ASGs per application group (see next section: Resiliency/DedicatedAutoScalingGroups), with 2 being Spot and 2 on-demand fallbacks.\nFor this reason, we no longer respect the recommendation to split AutoScalingGroups so that each ASG has the same amount of RAM and number of CPU cores, in order to reduce ASGs number.\n\nRunning PHP, the CPU is our bottleneck, not RAM. So we made the choice to have mixed ASG with the same number of CPUs, but not the same amount of RAM.\nThis means that our ASG spot-nodes-8x is composed of m5.8xlarge as well as r5.8xlarge\n\nDedicated AutoScalingGroups by app\n\nWe started to dedicate AutoScalingGroups for some applications when Prometheus was eating all the memory of a node, ending up in OOM errors. Because Prometheus replays its WAL at startup and consumes a lot of memory doing so, adding a Limit over the memory was of no use. It was OOMKill during the WAL process, restarted, OOMKilled again, etc. . Therefore, we isolated Prometheus on nodes having a lot of memory so it could use up all of it.\n\nThen, one of our main API experienced a huge load, 60% IDLE CPU to 0% in a few seconds. Because of the brutality of such a peak, active pods started to consume all CPU available on nodes, depriving other pods. Getting rid of CPU limits is a recommendation that comes with drawbacks that we measured and chose to follow the recommendation to ensure performance. As a result, the entire cluster went down, lacking for available CPU. Airbnb shared the same experience: they removed CPU limits because of throttling, but the noisy neighbors forced them to re-introduce limits.\n\nWe tried to isolate this API on its own nodes, as such peaks can repeat in the future, because itâ€™s uncacheable and userfacing. We added Taints on dedicated nodes and Tolerations on the selected API.\n\nSince then, we had to deploy a dedicated overprovisioning on those nodes as the overprovisioning pods didnâ€™t have this Toleration. It turned out weâ€™re also able to adapt the overprovisioning specifically for this API, which wasnâ€™t the base idea, but it has proven to be very effective due to the APIâ€™s nature. We talk more about overprovisioningâ€™s conf a little later on (Scalability/Overprovisioning).\n\nNow, weâ€™re setting CPU limits, at least for all applications not using dedicated nodes and also because weâ€™ve updated our kernels to the patched version. We follow their CPU usage through Prometheus alerting, with:\n\n- labels:\n    severity: notice\n    cluster_name: &quot;{ { $externalLabels.cluster_name } }&quot;\n  annotations:\n    alertmessage: &#39;{ { $labels.namespace } }/{ { $labels.pod } }/{ { $labels.container } } : { { printf &quot;%0.0f&quot; $value } }%&#39;\n    description: Container using more CPU than expected.\n      It will soon be throttled, which has a negative impact on performances.\n    summary: &quot;{ { $externalLabels.cluster_name } } - Notice - K8S - Container using 90% CPU Limit&quot;\n  alert: Notice - K8S - Container getting close to its CPU Limit\n  expr: |\n    (\n      sum(rate(container_cpu_usage_seconds_total{job=&quot;kubelet&quot;, container!=&quot;POD&quot;, container!=&quot;&quot;}[1m])) by (container, namespace, pod)\n    / sum(kube_pod_container_resource_limits_cpu_cores{job=&quot;kube-state-metrics&quot;, container!=&quot;POD&quot;, container!=&quot;&quot;}) by (container, namespace, pod)\n    ) * 100 &amp;gt; 90\n\n\nWe donâ€™t currently have alerting on Throttling, only a Grafana graph using the metric:\n\nsum by (pod) (rate(container_cpu_cfs_throttled_seconds_total{job=&quot;kubelet&quot;, image!=&quot;&quot;,container!=&quot;POD&quot;}[1m]))\n\nAfter Prometheus, we later isolated Victoria Metrics and Grafana Loki on their own ASGs.\nWeâ€™re also isolating â€œadminâ€ tools, like CoreDNS, cluster-autoscaler, HAProxy Ingress Controller, on dedicated â€œadmin nodesâ€ group. That way, admin tools canâ€™t mess with applications pods and vice versa.\n\n\nDevelopers only deploy to Worker nodes. An applicationâ€™s pods can only be scheduled on 4 ASGs, including 2 on-demand backups.\n\nOur admin nodes are on-demand. Having an ASG of few nodes all Spot is a risk we didnâ€™t want to take regarding the criticality of those pods.\n\nQOS Guaranteed Daemonsets\n\nAll our Daemonsets have Requests and Limits set at the same value.\nWeâ€™ve found out that a lot of Daemonsets donâ€™t define those values by default.\nEnforcing QOS Guaranteed Daemonsets:\n\n\n  ensures our daemonsets request all the resources they need, which is also important for the k8s scheduler to be more effective\n  daemonsets bad behaviours can be contained through Limits, and will not mess up with pods\n  itâ€™s a good indicator of the overhead we add on each node and helps us choose our EC2 instance types better (E.g: 2x.large instances are too small)\n  itâ€™s a reminder that a server with 16 CPUs has in fact only 80% of them usable by application pods\n\n\nScalability\n\nCluster Autoscaler\n\nWe automatically scale our EC2 Instances with cluster-autoscaler.\n\n\n\nAs mentioned before, we have several AutoScalingGroups per cluster.\nWe use the service discovery feature of cluster-autoscaler to find all ASGs to work with and to control them automatically.\nThis is done in two steps:\n\n\n  We add 2 tags on ASGs that the cluster-autoscaler should manage\n\n\nk8s.io/cluster-autoscaler/enabled: &quot;true&quot;\nk8s.io/cluster-autoscaler/{ { $cluster.name } }: &quot;true&quot;\n\n\n\n  Then, inside the Chart, we add those two labels to the node-group-auto-discovery parameter:\n\n\ncommand:\n- ./cluster-autoscaler\n- --cloud-provider=aws\n- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/{ { index .Values.nodes .Values.env &quot;clusterName&quot; } }\nâ€¦\n\n\nExpander Priority\n\nWe use cluster-autoscaler with the expander: priority.\nASGs will be chosen as:\n\n\n  spot-nodes-.*\n  on-demand-.*\n\n\nCluster-autoscaler will randomly add an EC2 instance in an ASG in the first group: spot-nodes-*. If a new instance hasnâ€™t joined the cluster after the fallback timeout (--max-node-provision-time), it will try another ASG in the same group. It will try all the ASGs in this group before moving on to the next group: on-demand-*.\n\nWith a dozen ASGs, most of them being Spot, weâ€™ve already waited for 45 minutes to actually be able to successfully add an EC2 instance.\n\nLaunching an EC2 instance sometimes fails with InsufficientInstanceCapacity, especially for Spot instances. With the autoscaler recommendation to split ASGs by the same amount of CPU/RAM, there were just too many ASGs to try before falling back on-demand. Weâ€™ve reduced the cluster-autoscaler fallback timeout to 5 minutes and still are facing many scaling problems at Paris, where it seems there are not many Spot instances available.\n\n\n\nExpander priority allows us to have resilience through an automatic fall back to on-demand when there is no more Spot.\nWe have already faced, multiple times, a fallback to on-demand instances even with a dozen different instance types. InsufficientInstanceCapacity errors are not a myth. Even on-demand instances can be in InsufficientInstanceCapacity, which we hope to never face with expander priority, 10+ Spot instance types, 10+ on-demand instance types and low --max-node-provision-time.\n\nOverprovisioning\n\nWe have overprovisioning pods inside the cluster.\nThe objective is to trigger a node scale-up before a legitimate pod actually needs resources. Doing so, the pod doesnâ€™t wait minutes to be scheduled, but a few seconds. This need for speed is linked to our business and sometimes the television audience bringing us many viewers very quickly.\n\nThis works using overprovisioning pods which request resources without doing anything (docker image: k8s.gcr.io/pause). Those pods are also using a low PriorityClass (-10), lower than our apps.\n\nThis trick is the whole magic of this overprovisioning: we request space that can be reclaimed anytime and very quickly. When an app needs it, the Scheduler will free up this space by expelling overprovisioning pods (of lower priority) because the cluster doesnâ€™t have enough free space. The expelled pods then change their state to Pending with the Reason: Unschedulable because we just filled the cluster with higher priority pods from the app. Presence of Pending Pods with Unschedulable reason trigger the cluster-autoscaler to add nodes.\n\nWe follow the efficiency of this overprovisioning with these Prometheus expressions:\n\n\n  kube_deployment_status_replicas_unavailable: we know which pods are waiting to be scheduled,\n  sum(kube_node_status_condition{condition=&quot;Ready&quot;,status=&quot;false&quot;}): we know if there are UnReady nodes, like when nodes are scaling-up and new nodes donâ€™t have their daemonsets Ready.\n\n\nBecause we have some nice load peaks on our applications, we are using the ladder mode of the overprovisioning. That ensures that we always have a minimum amount of overprovisioning running in the cluster, so weâ€™re able to handle huge loads at any time. Also, we ensure that we donâ€™t waste too much resources when heavily loaded, so we donâ€™t reserve 200 nodes in a cluster of 1000 nodes for example.\n\nThe configmap looks like:\n\ndata:\n  ladder: &#39;{&quot;coresToReplicas&quot;:[[16,4],[100,10],[200,20]]}&#39;\n\n\nWe chose to have big overprovisioning pods, bigger than any other pod in the cluster, to ensure that expelling one of the overprovisioning pods is enough to schedule any Pending pod.\n\nPriorityClass\n\nWe sacrifice some applications when overprovisioning is not enough.\n\nThe overprovisioning magic is based on PriorityClass objects.\nWeâ€™re using the same logic for our other applications, using PriorityClass.\nWe have 3 of them which concern applications:\n\n\n  low: -5\n  default: 0\n  high: 5\n\n\nCritical applications are using the â€œhighâ€ PriorityClass.\nMost applications are using the â€œdefaultâ€ one, so they donâ€™t even have to explicitly use it.\nWorkers doing asynchronous tasks can be cut off for several tens of minutes without any business impact. These are the ones with the â€œlowâ€ PriorityClass we sacrifice when needed.\n\nHere is an example, during a heavy load :\n\nHundreds of unavailable pods for 10 minutes.\n\nIf we filter out â€œlowâ€ PriorityClass pods in the graph above, thereâ€™s only one application having unavailable pods:\n\nNew pods for this application stayed in the Unavailable state for 15 seconds.\n\nLow HPA targets\n\nKubernetes takes time to scale-up pods.\n\nWithout overprovisioning, weâ€™ve measured that we wait up to 4 minutes when thereâ€™s no available node where pods can be scheduled.\nThen, with overprovisioning, we mostly wait for 45seconds, between the moment the HorizontalPodAutoscaler changes the Replicas of a Deployment and for those pods to be ready and receive traffic.\n\nWe canâ€™t wait so long during our peaks, so we generally define HPA targets at 60%, 70% or 80% of Requests. That gives us more time to handle the load while new pods are being scheduled.\n\nOn the following graphs, we can see two nice peaks at 20h52 and 21h02:\n\nAbove, in green, the number of consumed CPUs for one specific application: +55% in one minute.\n\nBelow, in blue, new pods are created in response to the peak.\n\n\nThis is obviously not a good way of managing resources, as we waste them as soon as the load balances.\nThis waste effect is amplified with the load: the more pods we have, the more we waste.\n\nYou can see it in this graph that shows the number of CPU reserved but not consumed:\n\n\nWe consume more CPU during peaks and therefore, we use more efficiently the reservations that do not have time to move, because we do not yet have scale-up.\nAs soon as the new pods added in response to the peak are Ready, 40% of CPU are wasted again.\n\nWe donâ€™t have a viable solution to solve this.\n\nWeâ€™re thinking about reducing scale-up duration to 10 seconds, so we wonâ€™t need these additional resources while we launch new pods. This is a challenge as the scaling mechanism is composed of several tools (metrics-server update frequency, autoscaler controller loop frequency, pod autoscaler initial readiness delay, probe launch times, etc.) and changing only one of them can have catastrophic behavior on the cluster stability. This huge subject will need its own dedicated blogpostâ€¦\n\nLong downscale durations\n\nRecently, we have increased the HPAâ€™s downscale durations from 5 to 30 minutes.\n\nItâ€™s done through Kops spec:\n  kubeControllerManager:\n    horizontalPodAutoscalerDownscaleStabilization: 30m0s\n\n\nWhen an application fails, its traffic decreases. When front A fails, traffic on backend B decreases too. When front A comes back, both A and B will have a big peak load.\n\nThe default five minutes delay for scaling down pods is too short for us. Increasing this delay makes the return to life of front A transparent on the number of pods of the whole platform, at least for the first 30 minutes of shutdown.\n\nWeâ€™ve seen blog posts where people turn off autoscaling for those very situations.\nFailure is not an extreme case. Failure is expected. Autoscaling strategies must adapt to it.\n\nYou can see that one waits 30 minutes after an upscale, before downscaling:\n\n\nDocumentation specifies that: â€œthis duration specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed.â€\n\nWe can observe on the graph above that itâ€™s rather: â€œthis duration specifies how long the autoscaler has to wait to perform a downscale after the last upscaleâ€.\n\nObservability\n\nMetrics\n\nWe scrape application and system metrics via Prometheus.\nWeâ€™re using Victoria Metrics as long term storage. We found it really easy to deploy and it needs really few time to administer on a daily basis, unlike Prometheus.\n\nDetails:\n\n\n  Prometheus scrapes metrics of pods having:\n\n\nannotations:\n  prometheus.io/path: /metrics\n  prometheus.io/port: &quot;8080&quot;\n  prometheus.io/scrape: &quot;true&quot;\n\n\n\n  Then, inside prometheus jsonnet files, we define a remoteWrite pointing to VictoriaMetrics:\n\n\nremote_write:\n- url: https://victoria-metrics-cluster-vminsert.monitoring.svc.cluster.local.:8480/insert/001/prometheus\n  remote_timeout: 30s\n  write_relabel_configs:\n  - separator: ;\n    regex: prometheus_replica\n    replacement: $1\n    action: labeldrop\n  queue_config:\n    capacity: 50000\n    max_shards: 30\n    min_shards: 1\n    max_samples_per_send: 10000\n    batch_send_deadline: 5s\n    min_backoff: 30ms\n    max_backoff: 100ms\nâ€¦\n\n\nWe have 2 Prometheus pods per cluster, each on separate nodes.\nEach Prometheus scrapes all metrics in the cluster, for resilience.\nThey have a really low retention (few hours, because of the WAL replay issue) and are deployed on Spot instances.\n\nWe have 2 Victoria Metrics pods per cluster (cluster version), each on separate nodes, separated of Prometheus pods through a podAntiAffinity\naffinity:\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 100\n      podAffinityTerm:\n        labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - prometheus\n        topologyKey: &quot;kubernetes.io/hostname&quot;\n\n\nEach Victoria Metrics pod receives all metrics in duplicate, from the two prometheus pods.\nWe use the command-line flag dedup.minScrapeInterval: 15s to deduplicate metrics.\n\nWeâ€™re thinking about totally removing Prometheus from the mix, using only Victoria Metrics Agent to scrape metrics.\n\nLogs\n\nWe collect stderr and stdout of all our containers.\nWe use fluentd for that, as a DaemonSet, which uses the nodeâ€™s /var/log/containers directory.\nWe use Grafana Loki as an interface to filter those logs.\n\nOur developers catch most of their logs and send them directly to Elasticsearch. Fluentd and Loki are used only for uncatched errors and have little traffic.\n\nFluentd uses around 200MB of memory per node and so we look at replacing it by promtail which uses only 40MB in our case.\n\n\nWeâ€™re happy with Loki, because we have few logs to parse. Weâ€™ve tested to get our Ingress Controller access logs sent to Loki and it was a nightmare. Too many entries to parse.\n\nThereâ€™s a default limit of 1000 log entries when querying, which we raised but then Grafana became very slow. Very very slow. 3000 log entries is the best fit for us.\n\nAlerting\n\nWe mostly use alerts defined in the official prometheus-operator repo.\n\nWe also added some alerts of our own. E.g: an alert when our Ingress Controller canâ€™t connect to a pod:\n- labels:\n    severity: critical\n    cluster_name: &quot;{ { $externalLabels.cluster_name } }&quot;\n  annotations:\n    alertmessage: &#39;{ { $labels.proxy } } : { { printf &quot;%.2f&quot; $value } } requests in error per second&#39;\n    description: &#39;HAProxy pods cannot send requests to this application. Connection errors may happen when one or more pods are failing or there&#39;&#39;s no more healthy pods : Application is crashed !!&#39;\n    summary: &quot;{ { $externalLabels.cluster_name } } - Critical - K8S - HAProxy IC - Backend connection errors&quot;\n  alert: Critical -K8S - HAProxy IC - Backend connection errors\n  expr: |\n    sum(rate(haproxy_backend_connection_errors_total[1m])) by (proxy) &amp;gt; 0\n  for: 1m\n\n\nPrometheus generates alerts that it sends to 2 redundant AlertManager instances, in a separate account that centralises alerts from all our clusters.\nWe have several possibilities then:\n\n\n  Send alerts on Slack dedicated channels\n  Send alerts to PagerDuty for the on-call teams\n\n\nOur developers are managing their own alerts (Kubernetes CRD: PrometheusRule) that are following a different path regarding labels defined. They have their own alerts sent in their own channels.\n\nCosts\n\nSpot instances\n\nWeâ€™re running 100% of our application workloads on Spot instances.\n\nIt was easy at first: implement spot-termination-handler and voilÃ .\nIndeed, but that was only the first step.\n\nInter accounts reclaims\n\nWe created AWS accounts for salto.fr platform, for which we did a lot of load tests with on-demand servers.\nThatâ€™s when we reclaimed our own instances on our other accounts.\n\n\n\nYour accounts are not â€œlinkedâ€ to each other in terms of Spot reclaims. Having resources in the same region with different accounts creates a relationship itself that we had never though about.\nIn this case, launching on-demand instances on one account triggered reclaims on our other accounts in the same region.\n\nOn-demand fallback\n\nWe didnâ€™t have on-demand fallback for a year and it went well.\nThere was enough spot capacity and there was no need for fallback. Therefore, we didnâ€™t prioritize automated on-demand fallbacks.\n\nThen, all our instance types (+10) went InsufficientInstanceCapacity at the same time.\nWe could only work around with a manual ASG we have from our first days on Kubernetes at AWS, on which we could launch on-demand instances as a last-resort fallback.\n\nNow, weâ€™re using cluster-autoscaler with the expander: priority to automatically fallback on lower priority ASGs (see above Scalability/Cluster-autoscaler).\n\nIt takes us around 10mn to start a node when all our instances are InsufficientInstanceCapacity.\nThere are other mechanisms that directly detect InsufficientInstanceCapacity on an ASG, so we wouldnâ€™t have to wait 5mn before moving on to the next one. Weâ€™re thinking about implementing them, but theyâ€™re not really compatible with cluster-autoscaler right now.\n\nAs of today, we have two ASGs per application group, as Spot, and also two ASGs as on-demand automatic fallback.\n\nDraino and node-problem-detector\n\nThe problem came when downscaling : cluster-autoscaler removes the least used node, no matter if itâ€™s a Spot or an on-demand instance.\n\nWe found ourselves with a lot of on-demand nodes after load peaks and they stayed on. And they cost a lot more than Spot instances.\n\nWe were already using node-problem-detector, so we added draino, to detect if an instance is on-demand and try to remove it when it is. Draino waits for 2h after the node is launched before trying to remove it.\n\nSince then, we use on-demand only when thereâ€™s no Spot left and only for a few hours.\n\nWe can see on this graph, that we added automated on-demand fallback and we never stopped having on-demand instances, until we added draino:\n\n\nSpot Tips\n\n\n  You need to be in an â€œoldâ€ AWS region to have a large number of Spot available. I.E. consider eu-west-1 instead of eu-west-3, even if it adds latency,\n  Use the maximum number of instance types possible. A dozen is barely enough. I.E. use all instance family letters regardless of what theyâ€™re optimised for (compute, memory) as long as your workload can use it,\n  Use CapacityOptimized spot allocation strategy, to limit reclaims to the strict necessary,\n  Do not use Spot on a single AZ (this advice is not limited to spot),\n  Prepare yourself to large reclaims, dozens at a time,\n  Configure and test your on-demand fallback\n\n\nKube-downscaler\n\nOpen-source project from Zalando which allows us to scale down Kubernetes deployments after work hours: nights and week-ends.\n\nWe use it on all our staging clusters. We save 60% of EC2 instances.\n\nHAProxy Ingress Controller\n\nThe whole traffic of a cluster goes through a single ALB.\n\nWe load-balance traffic to the correct pod through HAProxy, which uses Ingress rules to update its configuration.\nWe explained the way HAProxy Ingress controller lives inside the cluster during a talk at the HAProxy Conf in 2019.\n\nReducing the number of managed load balancers at AWS isnâ€™t the only benefit of HAProxy: we have tons of metrics in a single Grafana dashboard. Requests number, errors, retries, response times, connect times, bad health checks, etc.\n\nWhatâ€™s next\n\nA lot has been done to have scalability, resilience, observability and reasoned costs over the last 3 years.\n\nUsing Kubernetes in production is not that simple.\nIt is necessary to be well equipped, to understand finely the workings of the kubernetes mechanics to find the balance that suits us. Avoid falling into the trap of adopting whatever tool everyone is talking about if you donâ€™t need it. Thereâ€™s a lot of hype around kubernetes and the cloud, it can be dangerous.\n\nThe next step will be for us to increase resilience as much as possible, while at the same time reducing costs.\nPerhaps it will be by speeding up the start-up of pods. Maybe it wonâ€™t work. Maybe we can make some modifications to cluster-autoscaler to make it more compatible with aws events (like InsufficiantInstanceCapacity). But we will certainly work around the costs.\n\n\n\nThanks to all the reviewers, for their good advice and their time â¤ï¸\n"
} ,
  
  {
    "title"    : "PHP, câ€™est vous ! Et vous pouvez contribuer !",
    "category" : "",
    "tags"     : " conference, php, open-source, afup",
    "url"      : "/2020/11/23/php-cest-vous-et-vous-pouvez-contribuer-pascal-martin.html",
    "date"     : "November 23, 2020",
    "excerpt"  : "Quand nous posons la question Â«Â qui contribue Ã  PHPÂ ?Â Â» lors des Ã©vÃ¨nements que nous organisons ou auxquels nous participons, nous nâ€™obtenons que trÃ¨s peu de rÃ©ponses. Est-ce parce que peu dâ€™entre nous savent ou aiment coder en CÂ ? Pourtant, parti...",
  "content"  : "Quand nous posons la question Â«Â qui contribue Ã  PHPÂ ?Â Â» lors des Ã©vÃ¨nements que nous organisons ou auxquels nous participons, nous nâ€™obtenons que trÃ¨s peu de rÃ©ponses. Est-ce parce que peu dâ€™entre nous savent ou aiment coder en CÂ ? Pourtant, participer et contribuer ne se limite pas Ã  des lignes de code, loin de lÃ Â !\n\nAvez-vous trouvÃ© lâ€™Ã©diteur en ligne de la documentation de PHPÂ ? Avez-vous soumis des patchs Ã  composer et symfony ou mÃªme Ã  magento et wordpress, qui sont open-source et attendent vos contributionsÂ ? Avez-vous testÃ© les versions alpha de PHPÂ 8Â ? Voyez-vous comment organiser un ApÃ©ro PHP ou un MeetupÂ ? Ou mÃªme un AFUP Day ou le Forum PHPÂ ? Savez-vous que lâ€™AFUP a besoin de vousÂ ? Que câ€™est une association qui est lÃ  pour vous aider et ce quâ€™elle peut vous apporterÂ ?\n\nAvec mille et une faÃ§ons de contribuer, venez faire un tour dâ€™horizon de modes de contribution que vous nâ€™aviez peut-Ãªtre pas encore envisagÃ©s, ou dont vous vous Ã©tiez dit quâ€™ils nâ€™Ã©taient pas pour vous. Vous verrez que, vous aussi, vous pouvez contribuer Ã  PHPÂ ;-)\n"
} ,
  
  {
    "title"    : "L&#39;open source, ce n&#39;est pas que pour le web",
    "category" : "",
    "tags"     : " conference, afup, open-source",
    "url"      : "/2020/10/23/l'open-source-ce-n-est-pas-que-pour-le-web.html",
    "date"     : "October 23, 2020",
    "excerpt"  : "Une confÃ©rence sur lâ€™open source hors des solutions informatiques uniquement, lors du forum PHP 2020 qui marquait les 20 ans de lâ€™AFUP.\n\nConnaissez-vous lâ€™open hardware ? Savez-vous ce que la NASA partage sur Github ? Vous avez certainement dÃ©jÃ  Ã©...",
  "content"  : "Une confÃ©rence sur lâ€™open source hors des solutions informatiques uniquement, lors du forum PHP 2020 qui marquait les 20 ans de lâ€™AFUP.\n\nConnaissez-vous lâ€™open hardware ? Savez-vous ce que la NASA partage sur Github ? Vous avez certainement dÃ©jÃ  Ã©coutÃ©, ou produit de la musique open source, savez-vous quâ€™il existe des mÃ©dicaments open source ? RÃ©pliquer une information, et la partager devient rapide et Ã©mancipateur, le monde se libÃ¨re un peu plus.\n\nAprÃ¨s une petite plongÃ©e dans les principes de partage de lâ€™open source, nous ferons un tour dâ€™horizon des initiatives open source dans dâ€™autres domaines que lâ€™informatique, pour en apprendre un peu plus sur la culture du libre, les rapports de force qui y conduisent, et revenir aux bases du partage.\n"
} ,
  
  {
    "title"    : "La scalabilitÃ© dâ€™une Ã©quipe / dâ€™un pÃ´le technique",
    "category" : "",
    "tags"     : " conference, forumPHP, PHP, Symfony",
    "url"      : "/2020/10/22/la-scalabilite-d-une-equipe-d-un-pole-technique.html",
    "date"     : "October 22, 2020",
    "excerpt"  : "Vous Ãªtes dans lâ€™Ã©quipe technique dâ€™une entreprise, composÃ©e de quelques dÃ©veloppeurs, dans 1 ou 2 Ã©quipes, et votre entreprise grandit, et il faut augmenter la capacitÃ© de production, et donc la taille de lâ€™Ã©quipe technique. Sauf que comme 9 femm...",
  "content"  : "Vous Ãªtes dans lâ€™Ã©quipe technique dâ€™une entreprise, composÃ©e de quelques dÃ©veloppeurs, dans 1 ou 2 Ã©quipes, et votre entreprise grandit, et il faut augmenter la capacitÃ© de production, et donc la taille de lâ€™Ã©quipe technique. Sauf que comme 9 femmes ne font pas un bÃ©bÃ© un 1 mois, 4 Ã©quipes de 6 personnes ne produisent pas automatiquement 2 fois plus que 2 Ã©quipes de 6 dÃ©veloppeurs.\n\nJe me propose de vous faire un retour dâ€™expÃ©rience sur comment nous avons abordÃ© la scalabilitÃ© du pÃ´le technique de Bedrock, pour passer de 10 Ã©quipes rÃ©parties en 3 verticaux techniques, Ã  plus de 30 Ã©quipes dans 5 verticaux techniques, en essayant de conserver une cohÃ©sion technique et fonctionnelle, et dâ€™optimiser les flux de dÃ©veloppements.\n\n"
} ,
  
  {
    "title"    : "DevOps ? Je n&#39;ai jamais voulu faire Ã§a, et pourtant â€¦",
    "category" : "",
    "tags"     : " conference, afup, php, devops",
    "url"      : "/2020/06/24/devops-je-n-ai-jamais-voulu-faire.html",
    "date"     : "June 24, 2020",
    "excerpt"  : "DÃ©veloppeuse junior : premiÃ¨re semaine. Mes collÃ¨gues mâ€™ont forcÃ©e Ã  dÃ©ployer ma premiÃ¨re feature sur 6play ! MalgrÃ© un petit frisson, tout sâ€™est bien passÃ©, grÃ¢ce aux outils et bonnes pratiques qui nous guident.\n\nCe nâ€™Ã©tait que le dÃ©but ! Depuis,...",
  "content"  : "DÃ©veloppeuse junior : premiÃ¨re semaine. Mes collÃ¨gues mâ€™ont forcÃ©e Ã  dÃ©ployer ma premiÃ¨re feature sur 6play ! MalgrÃ© un petit frisson, tout sâ€™est bien passÃ©, grÃ¢ce aux outils et bonnes pratiques qui nous guident.\n\nCe nâ€™Ã©tait que le dÃ©but ! Depuis, je gÃ¨re lâ€™infrastructure de mon projet. Je choisis mes bases de donnÃ©es, caches, mÃ©canismes de stockage, ressourcesâ€¦ En prenant en compte leur coÃ»t, les modes de backups ou les compÃ©tences dans nos Ã©quipes. Et je suis libre dâ€™expÃ©rimenter avec nâ€™importe quel service que je voudrais tester.\n\nEn un an, je suis passÃ©e de â€œsimple dÃ©veloppeuseâ€ Ã  quelquâ€™un qui a conscience de sa plateforme, qui monitore son code et est responsable de sa production. Comment ai-je vÃ©cu cette transition ? Comment ai-je grandi en tant que dÃ©veloppeuse ?\n\nVous aussi, profitez de votre nouvelle libertÃ© : devenez DevOps !\n"
} ,
  
  {
    "title"    : "6play_API-v2-Final(1).doc",
    "category" : "",
    "tags"     : " conference, php, afup, api",
    "url"      : "/2020/06/24/6play_API-v2-Final(1).html",
    "date"     : "June 24, 2020",
    "excerpt"  : "Votre API est confrontÃ©e Ã  des contraintes techniques mais elle doit surtout rÃ©pondre Ã  vos problÃ©matiques mÃ©tier qui ne cessent dâ€™Ã©voluer. Nous avons souvent vÃ©cu cette situation pour 6play (service de Replay du Groupe M6), et il nous a fallu plu...",
  "content"  : "Votre API est confrontÃ©e Ã  des contraintes techniques mais elle doit surtout rÃ©pondre Ã  vos problÃ©matiques mÃ©tier qui ne cessent dâ€™Ã©voluer. Nous avons souvent vÃ©cu cette situation pour 6play (service de Replay du Groupe M6), et il nous a fallu plusieurs gÃ©nÃ©rations dâ€™API avant dâ€™arriver Ã  une version adaptÃ©e Ã  nos besoins. Micro-services, Rest/GraphQL, Developer eXperienceâ€¦ Un rÃ©cit et des conseils pragmatiques pour concevoir et maintenir votre API.\n"
} ,
  
  {
    "title"    : "React/Redux: pitfalls and best practices",
    "category" : "",
    "tags"     : " js, react, redux, frontend",
    "url"      : "/2020/04/27/react-redux-pitfalls-and-best-pratices.html",
    "date"     : "April 27, 2020",
    "excerpt"  : "After 2 years using React with Redux for the video platform 6play, I was able to identify good practices and pitfalls to avoid at all costs.\nThe Bedrock team kept the technical stack of the project up to date to take advantage of the new features ...",
  "content"  : "After 2 years using React with Redux for the video platform 6play, I was able to identify good practices and pitfalls to avoid at all costs.\nThe Bedrock team kept the technical stack of the project up to date to take advantage of the new features of react, react-redux and redux.\n\nSo here are my tips for maintaining and using React and Redux in your application without going mad.\n\nThis article is not an introduction to React or Redux. I recommend this documentation if you want to see how to implement it in your applications.\n\nYou could also take a look at Redux offical style guide in which you could find some of those tips and others.\nNote that if you use the Redux Toolkit, some of the tips/practices presented in this article are already integrated directly into the API.\n\nAvoid having only one reducer\n\nThe reducer is the function that is in charge of building a new state at each action.\nOne might be tempted to manipulate only one reducer.\nIn the case of a small application, this is not a problem.\nFor applications expressing a complex and evolving business, it is better to opt in for the combineReducers solution.\n\nThis feature of redux allows to manipulate not one but several reducers which act respectively on the state.\n\n\n  When and how to split its application?\n\n\nWhat we recommend at Bedrock is a functional splitting of the application.\nIn my approach, we would tend to represent the business of the application more than the technical stuff implied.\nSome very good articles explain it notably through the use of DDD principles.\n\nIn Bedrock, we use a folder named modules which groups together the different folders associated with the feature of your application.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.reducer.js\n    product/\n      __tests__/\n        product.reducer.spec.js\n      components/\n      product.reducer.js\n    account/\n      __tests__/\n      account.reducer.spec.js\n      components/\n      account.reducer.js\n  store.js\n  index.js\n\n\nSo in store.js all you need to do is combine your different reducers.\n\nimport { createStore, combineReducers } from &#39;redux&#39;\nimport { user } from &#39;./modules/user/user.reducer.js&#39;\nimport { product } from &#39;./modules/user/product.reducer.js&#39;\nimport { account } from &#39;./modules/user/account.reducer.js&#39;\n\nexport const store = createStore(combineReducers({ user, product, account }))\n\n\nBy following this principle, you will:\n\n\n  keep reducers readable because they have a limited scope\n  structure and define the functionalities of your application\n  facilitate the testing\n\n\nHistorically, this segmentation has allowed us to remove complete application areas without having impacts on the entire codebase, just by deleting the module folder associated with the feature.\n\nProxy access to the state\n\nNow that your reducers have been placed in the functional module, you need to allow your components to access the state via selector.\nA selector is a function that has the state as a parameter, and retrieves its information.\nThis can also allow you to select only the props needed for the component by decoupling from the state structure.\n\nexport const getUserName = ({ user: { lastName } }) =&amp;gt; lastName\n\n\nYou can also pass parameters to a selector by wrapping it with a function.\n\nexport const getProduct = productId =&amp;gt; ({ product: { list } }) =&amp;gt;\n  list.find(product =&amp;gt; product.id === productId)\n\n\nThis will allow you to use them in your components using the useSelector hook.\n\nconst MyComponent = () =&amp;gt; {\n  const product = useSelector(getProduct(12))\n  return &amp;lt;div&amp;gt;{product.name}&amp;lt;/div&amp;gt;\n}\n\n\nIt is specified in the react-redux doc that the selector is called for each render of the component.\nIf the selector function reference does not change, a cached version of the object can be returned directly.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.reducer.js\n      user.selectors.js &amp;lt;--- This is where all module selectors are exported\n\n\nPrefix the name of your actions\n\n\n  I really advise you to define naming rules for your actions and if possible check them with an eslint rule.\n\n\nActions are in uppercase letters separated by â€˜_â€™.\nHere an example with this action: SET_USERS.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.actions.js &amp;lt;--- This is where all module action creators are exported\n      user.reducer.js\n      user.selectors.js\n\n\nAction names are prefixed by the name of the module in which it is located.\nThis gives a full name: user/SET_USERS.\nA big advantage of this naming rule is that you can easily filter the action in redux-devtools.\n\n\n\nAlways test your reducers\n\nThe reducers are the holders of your applicationâ€™s business.\nThey manipulate the state of your application.\n\nThis code is therefore sensitive.\n\nâ¡ï¸ A modification can have a lot of impact on your application.\n\nThis code is rich in business rules\n\nâ¡ï¸ You must be confident that these are correctly implemented.\n\nThe good news is that this code is relatively easy to test.\nA reducer is a single function that takes 2 parameters.\nThis function will return a new state depending on the type of action and its parameters.\n\nThis is the standard structure for testing reducers with Jest:\n\ndescribe(&#39;ReducerName&#39;, () =&amp;gt; {\n  beforeEach(() =&amp;gt; {\n    // Init a new state\n  })\n  describe(&#39;ACTION&#39;, () =&amp;gt; {\n    // Group tests by action type\n    it(&#39;should test action with some params&#39;, () =&amp;gt; {})\n    it(&#39;should test action with other params&#39;, () =&amp;gt; {})\n  })\n  describe(&#39;SECOND_ACTION&#39;, () =&amp;gt; {\n    it(&#39;should test action with some params&#39;, () =&amp;gt; {})\n  })\n})\n\n\nI also recommend that you use the deep-freeze package on your state to ensure that all actions return new references.\n\nUltimately, testing your reducers will allow you to easily refactor the internal structure of their state without the risk of introducing regressions.\n\nKeep the immutability and readability of your reducers\n\nA reducer is a function that must return a new version of the state containing its new values while keeping the same references of the objects that have not changed.\nThis allows you to take full advantage of Structural sharing and avoid exploding your memory usage.\nThe use of the spread operator is thus more than recommended.\n\nHowever, in the case where the state has a complicated and deep structure, it can be verbose to change the state without destroying the references that should not change.\n\nFor example, here we want to override the Rhone.Villeurbanne.postal value of the state while keeping the objects that donâ€™t change.\n\nconst state = {\n  Rhone: {\n    Lyon: {\n      postal: &#39;69000&#39; ,\n    },\n    Villeurbanne: {\n      postal: &#39;&#39;,\n    },\n  },\n  IsÃ¨re: {\n    Grenoble: {\n      postal: &#39;39000&#39;,\n    },\n  },\n}\n\n// When you want to change nested state value and use immutability\nconst newState = {\n  ...state,\n  Rhone: {\n    ...state.Lyon,\n    Villeurbanne: {\n      postal: &#39;69100&#39;,\n    },\n  },\n}\n\n\nTo avoid this, a member of the Bedrock team released a package that allows to set nested attribute while ensuring immutability: immutable-set\nThis package is much easier to use than tools like immutable.js because it does not use Object prototype.\n\nimport set from &#39;immutable-set&#39;\n\nconst newState = set(state, `Rhone.Villeurbanne.postal`, &#39;69100&#39;)\n\n\nDo not use the default case\n\nThe implementation of a redux reducer very often consists of a switch where each case corresponds to an action.\nA switch must always define the default case if you follow so basic eslint rules.\n\nLetâ€™s imagine the following reducer:\n\nconst initialState = {\n  value: &#39;bar&#39;,\n  index: 0,\n}\n\nfunction reducer(initialState, action) {\n  switch (action.type) {\n    case &#39;FOO&#39;:\n      return {\n        value: &#39;foo&#39;,\n      }\n    default:\n      return {\n        value: &#39;bar&#39;,\n      }\n  }\n}\n\n\nWe can naively say that this reducer manages two different actions. Itâ€™s okay.\nIf we isolate this reducer there are only two types of action that can change this state; the FOO action and any other action.\n\nHowever, if you have followed the advice to cut out your reducers, you donâ€™t have only one reducer acting on your blind.\n\nThatâ€™s where the previous reducer is a problem.\nIndeed, any other action will change this state to a default state.\nA dispatch action will pass through each of the reducers associated with this one.\nAn action at the other end of your application could affect this state without being expressed in the code.\nThis should be avoided.\n\n\n\nIf you want to modify the state with an action from another module, you can do so by adding a case on that action.\n\nfunction reducer(state = initialState, action) {\n  switch (action.type) {\n    case &#39;FOO&#39;:\n      return {\n        value: &#39;foo&#39;,\n      }\n    case &#39;otherModule/BAR&#39;:\n      return {\n        value: &#39;bar&#39;,\n      }\n    default:\n      return state\n  }\n}\n\n\nUse custom middlewares\n\nIâ€™ve often seen action behaviors being copied and pasted, from action to action.\nWhen youâ€™re a developer, â€œcopy-pasteâ€ is never the right way.\n\nThe most common example is handling HTTP calls during an action that uses redux-thunk.\n\nexport const foo = () =&amp;gt;\n  fetch(&#39;https://example.com/api/foo&#39;)\n    .then(data =&amp;gt; ({ type: &#39;FOO&#39;, data }))\n    .catch(error =&amp;gt; {\n      // Do something\n    })\n\nexport const bar = () =&amp;gt;\n  fetch(&#39;https://example.com/api/bar&#39;)\n    .then(data =&amp;gt; ({ type: &#39;BAR&#39;, data }))\n    .catch(error =&amp;gt; {\n      // Do something\n    })\n\n\nThese two actions are basically the same thing, we could very well make a factory that would do the code in common.\n\nBasically the meta action we want to represent here when it is dispatched:\n\nFetch something\n-- return action with the result\n-- in case or error, do something\n\n\nWe could very well define a middleware that would take care of this behavior.\n\nconst http = store =&amp;gt; next =&amp;gt; async action =&amp;gt; {\n  if (action.http) {\n    try {\n      action.result = await fetch(action.http)\n    } catch (error) {\n      // Do something\n    }\n  }\n  return next(action)\n}\n\n// in redux store init\nconst exampleApp = combineReducers(reducers)\nconst store = createStore(exampleApp, applyMiddleware(http))\n\n\nThus the two preceding actions could be written much more simpler:\n\nexport const foo = () =&amp;gt; ({ type: &#39;FOO&#39;, http: &#39;https://example.com/api/foo&#39; })\n\nexport const bar = () =&amp;gt; ({ type: &#39;BAR&#39;, http: &#39;https://example.com/api/bar&#39; })\n\n\nThe big advantages of using middleware in a complex application:\n\n\n  avoids code duplication\n  allows you to define common behaviors between your actions\n  standardize redux meta action types\n\n\nAvoid redux related rerender\n\nThe trick when using redux is to trigger component re-render when you connect them to the state.\nEven if rerenders are not always a problem, re-render caused by the use of redux really has to be prevented.\nJust beware of the following traps.\n\nDo not create a reference in the selector\n\nLetâ€™s imagine the next selector:\n\nconst getUserById = userId =&amp;gt; state =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId) || {}\n\n\nThe developer here wanted to ensure that its selector is null safe and always returns an object.\nThis is something we see quite often.\n\nEach time this selector will be called for a user not present in the state, it will return a new object, a new reference.\n\n\n  With useSelector, returning a new object every time will always force a re-render by default.\nDoc of react-redux\n\n\nHowever in the case of an object, as in the example above (or an array), the reference of this default value is new each time the selector is executed.\nSimilarly for the default values in destructuring, you should never do this :\n\nconst getUsers = () =&amp;gt; ({ users: [] }) =&amp;gt; users\n\n\nWhat to do then?\nWhenever possible, the default values should be stored in the reducer.\nOtherwise, the default value must be extracted into a constant so that the reference remains the same.\n\nconst defaultUser = {}\n\nconst getUserById = userId =&amp;gt; state =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId) || defaultUser\n\n\nThe same goes for the selector usage that returns a new ref at each call.\nThe use of the filter function returns a new array each time a new reference even if the filter conditions have not changed.\n\nTo continue, it is important that useSelector does not return a function.\nBasically you should never do this:\n\nconst getUserById = state =&amp;gt; userId =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId)\nconst uider = useSelector(getUserById)(userId)\n\n\nA selector should not return a view (a copy) of the state but directly what it contains.\nBy respecting this principle, your components will rerender only if an action modifies the state.\nUtilities such as reselect can be used to implement selectors with a memory system.\n\nDo not transform your data in the components\n\nSometimes the data contained in the state is not in the correct display format.\nWe would quickly tend to generate it in the component directly.\n\nconst MyComponent = () =&amp;gt; {\n  const user = useSelector(getUser)\n\n  return (\n    &amp;lt;div&amp;gt;\n      &amp;lt;h1&amp;gt;{user.name}&amp;lt;/h1&amp;gt;\n      &amp;lt;img src={`https://profil-pic.com/${user.id}`} /&amp;gt;\n    &amp;lt;/div&amp;gt;\n  )\n}\n\n\nHere, the url of the image is dynamically computed in the component, and thus at each render.\nWe prefer to modify our reducers in order to include a profileUrl attribute so that this information is directly accessible.\n\nswitch (action.type) {\n  case `user/SET_USER`:\n    return {\n      ...state,\n      user: {\n        ...action.user,\n        profileUrl: `https://profil-pic.com/${action.user.id}`,\n      },\n    }\n}\n\n\nThis information is then calculated once per action and not every time it is rendered.\n\nDonâ€™t use useReducer for your business data\n\nSince the arrival of hooks, we have many more tools provided directly by React to manage the state of our components.\nThe useReducer hook allows to set a state that can be modified through actions.\nWeâ€™re really very very close to a redux state that we can associate to a component, itâ€™s great.\n\nHowever, if you use redux in your application, it seems quite strange to have to use useReducer.\nYou already have everything you need to manipulate a complex state.\n\nMoreover, by using redux instead of the useReducer hook you can take advantage of really efficient devtools and middlewares.\n\n\n\nUseful resources\n\n\n  Use react with redux doc\n  redux flow animated by Dan Abramov\n\n  redux documentation about middlewares\n  immutable-set\n\n\nThanks to the reviewers: \n@flepretre, \n@mfrachet, \n@fdubost,\n@ncuillery,\n@renaudAmsellem\n"
} ,
  
  {
    "title"    : "How to boost the speed of your webpack build?",
    "category" : "",
    "tags"     : " js, webpack",
    "url"      : "/2020/03/05/hunting-webpack-performances.html",
    "date"     : "March 5, 2020",
    "excerpt"  : "How did I cut in half my projectâ€™s webpack build time ?\n\nWho never complained about the infinite duration of a webpack build on a project ?\nIâ€™m currently working on a big web application coded in React/Redux with server side rendering.\nThe applica...",
  "content"  : "How did I cut in half my projectâ€™s webpack build time ?\n\nWho never complained about the infinite duration of a webpack build on a project ?\nIâ€™m currently working on a big web application coded in React/Redux with server side rendering.\nThe application exists since 2015 and it has evolved a lot since then\n\n\n\nTLDR;\n\n\n  Never, ever, ever, ever work on performance improvements or optimization without monitoring!\n\n\nIf you want to optimize the duration of a job, you have to monitor precisely the duration of it and all its sub-steps.\nBy doing that, you can really focus on the most expensive task.\nThis will save you from wasting time on optimizations that will have little impact on the system as a whole.\nUse existing monitoring tools! Create them if they donâ€™t exist!\n\nWhat was the problem with webpack ?\n\nFor several weeks/months my colleagues had been complaining about the duration of our yarn build command. \nThe purpose of this command is to build the distributable package of our application in a production target with webpack.\n\nI even heard:\n\n  â€œThis command, I donâ€™t run it locally anymore, it takes too much time.â€\n  â€œMy computer starts ventilating heavily every time I run this command. Thereâ€™s nothing else I can do!â€\n\n\nDepending on the machine on which the build was launched, it took between 5 and 12 minutes.\nIt is not possible to have a build that takes so long.\nwebpack is not a slow bundler. \nIt is our use of webpack that makes it slow.\n\nFocus error, a morning lost\n\nSince this command launches a webpack build in production mode, I figured out that the culprit was webpack config itself.\nGiven that Iâ€™ve dug deep into webpack, I thought it would be interesting to focus on this performance concern.\nI have indeed open sourced a set of workshop to learn how to use webpack from scratch (https://webpack-workshop.netlify.com).\nSo at the end of January I took one day to improve the situation.\n\nI had my own idea of the task that would take the most. So I tried to improve it, spending my entire morning on it. \nI just managed to gain 17 seconds.\n\nIâ€™m not going to lie, I was very disappointed with what I achieved.\n\nThe concern in my strategy was however obvious. \nI started off with a preconceived idea â€œThis is definitely the stage that takes the longest.â€\n\nNothing was objective in my analysis.\nTo improve the performance of an application it is necessary to focus on objective facts.\n\nSuccessful afternoon\n\nWhen I came back from my lunch break, I was motivated to win more than those poor 17 seconds.\nThen I remembered the Pareto principle.\n\n\n  The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes.\nWikipedia\n\n\nThere is probably one step that takes up most of the webpack build time.\nPareto principle adapted to webpack could mean â€œ80% of the build time is caused by 20% of the configâ€\n\nLetâ€™s find the culprit ! ğŸ‰\n\nI had to determine the build time of each loader, of each plugin.\nI was very lucky, the webpack community has already proposed a plugin that allows to measure everything.\nAnd it is very easy to install. â™¥ï¸\n\nSpeed Measure Plugin\n\nHere are the results I got:\n\nSMP  â±  \nGeneral output time took 4 mins, 5.68 secs\n\n SMP  â±  Plugins\nIgnorePlugin took 57.73 secs\nTerserPlugin took 39.022 secs\nExtractCssChunksPlugin took 3.13 secs\nOptimizeCssAssetsWebpackPlugin took 1.6 secs\nManifestPlugin took 1.55 secs\nWebpackPwaManifest took 0.326 secs\nContextReplacementPlugin took 0.129 secs\nHashedModuleIdsPlugin took 0.127 secs\nGenerateSW took 0.059 secs\nDefinePlugin took 0.047 secs\nEnvironmentPlugin took 0.04 secs\nLoadablePlugin took 0.033 secs\nObject took 0.024 secs\n\n SMP  â±  Loaders\nbabel-loader, and \nrev-replace-loader took 2 mins, 11.99 secs\n  module count = 2222\nmodules with no loaders took 1 min, 57.86 secs\n  module count = 2071\nextract-css-chunks-webpack-plugin, and \ncss-loader, and \npostcss-loader, and \nsass-loader took 1 min, 43.74 secs\n  module count = 95\ncss-loader, and \npostcss-loader, and \nsass-loader took 1 min, 43.61 secs\n  module count = 95\nfile-loader, and \nrev-replace-loader took 4.86 secs\n  module count = 43\nfile-loader took 2.67 secs\n  module count = 32\nraw-loader took 0.446 secs\n  module count = 1\n@bedrock/package-json-loader took 0.005 secs\n  module count = 1\nscript-loader took 0.003 secs\n  module count = 1\n\n\nAs expected, itâ€™s not great! \nBut at least Iâ€™m starting to get who the culprits are.\nWe can see that for 2222 Javascript modules takes up 2mins but for only 95 Sass files 1min43 ğŸ¤£.\n\n\n\nDamn node-sass\n\nOnce the migration from node-sass to sass (new Sass re-implementation) and the update of sass-loader, I was shocked!\nIt took me about 10 minutes because there were few breaking changes and I gained more than 1min30 on the build time.\n\nsass-loader made big improvements on performances, you should definitely make sure you use the last version.\n\nI lost a morning on gaining 17 seconds and I spent 10 minutes to win 1min30.ğŸ¤£\n\nIgnorePlugin, TerserPlugin\n\n\n  \n    TerserPlugin is used to uglify the javascript code in order to reduce its size and readability. Itâ€™s a relatively long process, but 39 seconds is too much.\nJust by updating the version of TerserPlugin to use the one integrated in Webpack, I managed to reduce by 20 seconds the build time.\n  \n  \n    IgnorePlugin is a core plugin that was used a lot in our application to avoid loading certain scripts in order to reduce the weight of the site.\nIt was necessary, but today with Webpack we can use much better than that. Dynamic Import, ContextReplacement, there are plenty of solutions. As a general rule, we should avoid compiling files and then not using them.\n  \n\n\nRecommendations from the community\n\nTo improve the build perfs webpack provides a web page listing the actions to take to hunt what takes time.\nI strongly advise to have a look at it.\n\nhttps://webpack.js.org/guides/build-performance/\n\nFinal Result\n\n    SMP  â±  \n    General output time took 2 mins, 18.27 secs\n\n\n\n\nBased on precise and concrete measures, I was able to drastically improve the webpack build of my application.\nNo more computers suffering just to compile a bit of JS and SASS.\nI could have lost whole days on futile modifications if I had not measured precisely what penalized the build.\n\nâ„¹ï¸\n\n  Use Speed Measure Plugin to debug webpack build time\n  Track your build time evolution to detect big evolution before merge\n  Follow webpack performances recommandations\n  Look at webpack 5 new caching strategies\n  Keep your webpack config up to date\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #12",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2020/02/08/m6web-dev-facts-12.html",
    "date"     : "February 8, 2020",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nEn parlant de slack\n\n  stalker ou bosser il faut choisir\n\n\nEn parlant dâ€™ornithorynque:\n\n  Non mais lâ€™australie, câ€™est le staging du monde\n\n\nLe dresseur\n\n  Mon charisme lÃ©gendaire a encore frappÃ©â€¦ les VPN...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nEn parlant de slack\n\n  stalker ou bosser il faut choisir\n\n\nEn parlant dâ€™ornithorynque:\n\n  Non mais lâ€™australie, câ€™est le staging du monde\n\n\nLe dresseur\n\n  Mon charisme lÃ©gendaire a encore frappÃ©â€¦ les VPNs filent au pas avec moiâ€¦\n\n\nCloudix\n\n  Les gaulois qui avaient peur que le ciel/cloud leur tombe sur la tÃªte, ils Ã©taient visionnaires en fait!\n\n\nLe meilleur support\nDans un bar de Lyon\n\n  Une inconnue: â€œ je peux pas me connecter Ã  lâ€™amour est dans le prÃ© â€¦â€\n\n\n\n  Un dev dâ€™M6web: â€œcâ€™est quoi ta box ?â€\n\n\nDes choix forts\n\n  Si on fait ce choix, câ€™est comme tuer un chaton\n\n\nObjets trouvÃ©s\n\n\nManger ou dÃ©cider il faut choisir\n\n  Une rÃ©union sans nourriture devrait Ãªtre un email.\n\n\nQuand un sysadmin craque une allumette Ã  la data\n\n  Scala de toute maniÃ¨re, ce nâ€™est quâ€™un wrapper Java\n\n\nCâ€™Ã©tait prÃ©vu\n\n  â€œQuand on passera en prod le bugâ€\n\n\nChandeleur\n\n  Jâ€™ai de la pÃ¢te, jâ€™ai du sucre mais je sais pas faire des grep\n\n\nLa fatigue\n\n  DÃ©finition de fatigue =&amp;gt;  â€œAh des weekend comme Ã§a je nâ€™en veux plusâ€ â€¦ Dit-il un vendredi\n\n\nSSO POWA\n\n  Vivement quâ€™on passe slack sous le SSO, comme Ã§a les gens ne viendront plus nous dire sur slack quâ€™ils nâ€™ont pas accÃ¨s au SSO\n\n\nAquarium\n\n\nToutou\n\n  Jâ€™ai fait un fichier waf.tf dans le module waf, ca fait waf waf\n\n\nConfluence\n\n  Mais effectivement Ã  ce niveau câ€™est aussi facile dâ€™Ã©crire des .txt quâ€™on se file par clÃ© usb que dâ€™utiliser Confluence.\n\n\nQuickwin\n\n  Câ€™est un quickwin jusquâ€™Ã  ce quâ€™on fasse F5\n\n\nJavascript\n\n  Un dev PHP: â€œCâ€™est du JS mais Ã§a se comprend presqueâ€\n\n\nLa persÃ©vÃ©rance\n\n\nLes coÃ»ts\n\n  Câ€™est le type de projet oÃ¹ il y a plus de jour-homme que dâ€™utilisateurs\n\n\nJe lâ€™ai pas touchÃ©Ã©Ã©o\n\n  Un sysadmin: â€œCâ€™est quoi le foot?â€\n\n\n\n  Un autre: â€œCâ€™est le truc qui fait tomber ton infra quand on en diffuse.â€\n\n\nLa concurrence et la mauvaise foi\n\n  Du coup tous les trucs inutiles de netflix on les fait, mais tous les trucs utiles on les fait pas\n\n\nOulÃ !\n\n  Câ€™est fou ce quâ€™on peut faire aujourdâ€™hui avec des spreadsheetsâ€¦. Je crois bien que câ€™est la derniÃ¨re fois que jâ€™ai codÃ©\n\n\nUn certain K.D\n"
} ,
  
  {
    "title"    : "Migration de 6PLAY - l&#39;amour est dans le cloud",
    "category" : "",
    "tags"     : " conference, video",
    "url"      : "/2020/02/07/pascal-martin-laduckconf.html",
    "date"     : "February 7, 2020",
    "excerpt"  : "Pascal Martin a eu le plaisir dâ€™Ãªtre invitÃ© par Octo pour un REX sur la migration de notre architecture dans le cloud.\n\nNous vous invitons Ã  dÃ©couvrir sa confÃ©rence en vidÃ©o.\n\nSi vous voulez en savoir encore plus, Pascal a Ã©crit Ã©galement un livre...",
  "content"  : "Pascal Martin a eu le plaisir dâ€™Ãªtre invitÃ© par Octo pour un REX sur la migration de notre architecture dans le cloud.\n\nNous vous invitons Ã  dÃ©couvrir sa confÃ©rence en vidÃ©o.\n\nSi vous voulez en savoir encore plus, Pascal a Ã©crit Ã©galement un livre sur le sujet.\n"
} ,
  
  {
    "title"    : "ScalaIO Lyon 2019",
    "category" : "",
    "tags"     : " scalaio, scala, lyon, 2019",
    "url"      : "/2019/11/25/retour-scalaio.html",
    "date"     : "November 25, 2019",
    "excerpt"  : "Nous Ã©tions Ã  la ScalaIO 2019 organisÃ©e Ã  Lyon ! \nNous avons assistÃ© Ã  de trÃ¨s bonnes confÃ©rences. Voici quelques mots sur les interventions qui nous ont le plus marquÃ©es cette annÃ©e.\n\nA live-coding introduction to Mill: finally a build tool we ca...",
  "content"  : "Nous Ã©tions Ã  la ScalaIO 2019 organisÃ©e Ã  Lyon ! \nNous avons assistÃ© Ã  de trÃ¨s bonnes confÃ©rences. Voici quelques mots sur les interventions qui nous ont le plus marquÃ©es cette annÃ©e.\n\nA live-coding introduction to Mill: finally a build tool we can all understand!\n\nMill est outil de build qui permet de coder en scala les diffÃ©rentes Ã©tapes de notre build. La puissance de cet outil vient principalement du fait quâ€™on Ã©crit du code. On peut donc effectuer des opÃ©rations trÃ¨s complexes lors du build. Les fonctionnalitÃ©s principales sont :\n\n  La possibilitÃ© de builder avec diffÃ©rentes versions de scala une mÃªme appli\n  La possibilitÃ© de ne builder que le code qui a Ã©tÃ© modifiÃ© (grÃ¢ce Ã  lâ€™utilisation de zinc)\n  La possibilitÃ© de lancer le build automatiquement Ã  la modification dâ€™un fichier (option watch)\n\n\nContext Buddy: the tool that knows your code better than you\n\nContext Buddy est un plugin pour votre IDE (Intellij) qui permet de mieux parcourir lâ€™historique des modifications de votre code. ContextBuddy vous permet de savoir grÃ¢ce Ã  la coloration syntaxique exactement quel Ã©lÃ©ment de la ligne a Ã©tÃ© modifiÃ©. De plus, comme il se base sur les donnÃ©es du compilateur, il est capable de voir si une mÃªme classe utilise une nouvelle version de la lib voire mÃªme une nouvelle lib.\n\nRailway Oriented Programming - Une approche fonctionnelle pour la gestion dâ€™erreurs\n\nCette confÃ©rence basÃ©e sur celle de Scott Wlaschin explique trÃ¨s bien la composition de fonction et lâ€™intÃ©rÃªt dans le cas de la gestion des erreurs. En effet, le code devient plus lisible et plus facilement maintenable.\n\nMetals - your next IDE?\n\nMetals est un Language Server Protocole pour Scala, ce qui permet de lâ€™utiliser en tÃ¢che de fond pour â€œnâ€™importe quelâ€ Ã©diteur de texte ou IDE. \nConcrÃ¨tement, Metals nâ€™apporte pas aujourdâ€™hui 100% des fonctionnalitÃ©s dâ€™Idea, mais les manques sont vraiment minimes. En revanche, tout ce qui est implÃ©mentÃ© semble Ãªtre plus efficace que sur Idea.\nLes principaux avantages que jâ€™ai retenu :\n\n  Presque tout comme Idea mais beaucoup plus lÃ©ger et rapide (pour certaines tÃ¢ches)\n  Fonctionne avec Maven, SBT, Fury, Gradle, Mill\n  Gloop permet le GoToDefinition et plein de choses sympas, plus rapides que lâ€™indexation intelliJ\n  Fonctionne partiellement pour Java (juste le nÃ©cessaire)\n  Compilation incrÃ©mentale avec Zinc\n  En plein dÃ©veloppement, de super features dans les mois Ã  venir\n  Tout ce que jâ€™oublie ;-)\n\n\nRunning Amok: Igniting a Documentation Revolution\n\nLâ€™idÃ©e de Jon Pretty (@propensive) est de dÃ©corrÃ©ler la documentation du code (diffÃ©rent repo git) et de pouvoir rÃ©tro-documenter (mettre Ã  jour la doc des version antÃ©rieures).\nAmok permet de relier chaque fichier de documentation Ã  un commit, Ã  partir duquel cette documentation est valide, permettant ainsi de sortir une release du document mÃªme si la documentation nâ€™est pas entiÃ¨rement terminÃ©e (problÃ¨me rÃ©current en open-source).\n\nRefined, des Types sur mesure\n\nPermet de mettre des conditions sur un type. On peut ajouter des prÃ©dicats au type (notamment des regex sur les Strings, des range de valeurs pour les Int, etc) et ainsi rÃ©duire les valeurs possibles. Validation au compile-time quand possible, et pour le runtime des erreurs trÃ¨s explicites sont jetÃ©es.\nPour les avantages, voir T(ype)DD, principalement la sÃ©curitÃ© apportÃ©e et plus besoin de tester ce qui est inclu dans les prÃ©dicats, Refined le fait pour nous.\n\nApache Spark et le machine learning : rÃªves et rÃ©alitÃ©s\n\nA travers des exemples basÃ©s sur une population de citrouilles (câ€™Ã©tait Halloween ;-)), Nastasia Saby (@saby_nastasia) nous a fait une prÃ©sentation de Spark ML en prenant en exemple KMeans, un des algorithmes de clustering disponibles. Elle en a montrÃ© les limites et prÃ©sentÃ© KMedoids, nâ€™existant pas dans Spark ML et plus complexe mais convergeant mieux. Elle a terminÃ© sur la nÃ©cessitÃ© de mettre en balance lâ€™utilisation dâ€™outils communautaires testÃ©s et reconnus mais parfois limitÃ©s, versus dÃ©velopper ses propres librairies parfaitement adaptÃ©es Ã  ses besoins, au risque de se confronter Ã  des problÃ¨mes que dâ€™autres ont dÃ©jÃ  rÃ©glÃ©s.\n\nLes slides.\n"
} ,
  
  {
    "title"    : "Machine learning sans magie et sans s&#39;arracher les cheveux",
    "category" : "",
    "tags"     : " machine learning, blendwebmix, conference",
    "url"      : "/2019/11/14/machine-learning-sans-magie-et-sans-sarracher-les-cheveux.html",
    "date"     : "November 14, 2019",
    "excerpt"  : "Comprendre le machine learning en prenant lâ€™exemple dâ€™un barbecue.\n",
  "content"  : "Comprendre le machine learning en prenant lâ€™exemple dâ€™un barbecue.\n"
} ,
  
  {
    "title"    : "Forum PHP Paris 2019",
    "category" : "",
    "tags"     : " forumphp, php, afup, 2019",
    "url"      : "/2019/11/04/retour-forum-php-2019.html",
    "date"     : "November 4, 2019",
    "excerpt"  : "Comme tous les ans, nous Ã©tions au Forum PHP 2019 organisÃ© par lâ€™AFUP ! \nNous avons assistÃ© Ã  de trÃ¨s bonnes confÃ©rences et Ã©changÃ© avec beaucoup dâ€™entre vous. Voici quelques mots sur les interventions qui nous ont le plus marquÃ©es cette annÃ©e.\n\nâ€œ...",
  "content"  : "Comme tous les ans, nous Ã©tions au Forum PHP 2019 organisÃ© par lâ€™AFUP ! \nNous avons assistÃ© Ã  de trÃ¨s bonnes confÃ©rences et Ã©changÃ© avec beaucoup dâ€™entre vous. Voici quelques mots sur les interventions qui nous ont le plus marquÃ©es cette annÃ©e.\n\nâ€œPHP Pragmatic Developmentâ€ et â€œLâ€™architecture progressiveâ€\n\nNous sommes plusieurs Ã  avoir trouvÃ© cette Ã©dition du Forum trÃ¨s pragmatique. Les deux confÃ©rences de Frederic BOUCHERY et de Matthieu NAPOLI y sont sans doute pour quelque chose !\n\nComme Matthieu lâ€™a rappelÃ©, pas besoin dâ€™une architecture parfaite pour crÃ©er un produit qui marche. Au contraire, Ã  nous dÃ©veloppeurs de savoir choisir les bonnes solutions pour rÃ©pondre Ã  un besoin. Et faire simple peut apporter infiniment plus de valeur que mettre en place une architecture parfois trop complexe !\n\nFrederic a soulignÃ© quâ€™Ãªtre pragmatique câ€™Ã©tait savoir Ã©couter son expÃ©rience. Peut-Ãªtre mÃªme savoir dialoguer entre dÃ©veloppeurs seniors et dÃ©butants, pour que lâ€™expÃ©rience des uns limite les erreurs des autres ?\n\nPHP 8 et Just In Time Compilation\n\nLe passage de PHP 5 Ã  PHP 7 a apportÃ© des gains Ã©normes en terme de performances, et nous sommes tous impatients de voir si PHP 8 nous rÃ©servera les mÃªmes surprises.\n\nLe JIT est une bonne piste, en permettant de compiler le PHP directement en langage machine, pour se passer de lâ€™exÃ©cution sur la machine virtuelle de PHP. Benoit JACQUEMONT a trÃ¨s bien dÃ©taillÃ© lâ€™histoire du JIT dans lâ€™Ã©cosystÃ¨me PHP, son objectif et son fonctionnement. MÃªme si les tests quâ€™il a effectuÃ© ne montrent pas de gains perceptibles, le sujet Ã©tait trÃ¨s intÃ©ressant.\n\nÃ€ retenir : lâ€™optimisation du CPU pour PHP nâ€™a pas beaucoup dâ€™intÃ©rÃªt si votre application passe son temps Ã  attendre des I/O.\n\nAggressive PHP quality assurance in 2019\n\nMarco PIVETTA est trÃ¨s actif dans la communautÃ© PHP, notamment pour lâ€™ORM Doctrine. Il nous a prÃ©sentÃ© les outils quâ€™il considÃ¨re comme indispensables pour assurer la qualitÃ© et la robustesse dâ€™un projet, mais aussi lâ€™ordre dâ€™importance pour les mettre en place selon lui.\n\nSi nous Ã©tions dÃ©jÃ  convaincus par lâ€™importance de lâ€™analyse statique, nous avons Ã©tÃ© intriguÃ©s par la place quâ€™il accordait Ã  tous ces outils basÃ©s sur les annotations PHP. Par exemple, il nâ€™hÃ©site pas Ã  laisser publiques les propriÃ©tÃ©s de ses classes immutables, sans mÃ©thode get ni set, et dÃ©lÃ©guer Ã  la CI la responsabilitÃ© de vÃ©rifier que toutes les instances des classes avec lâ€™annotation @psalm-immutable ne soient jamais modifiÃ©esâ€¦ DÃ©routant, mais Ã  mÃ©diter.\n\nMercure, et PHP sâ€™enamoure enfin du temps rÃ©el\n\nPouvoir pousser, en temps rÃ©el, des informations depuis du code PHP server-side vers des centaines de milliers de clients, sans allumer des dizaines de serveurs ? Câ€™est la promesse du projet Mercure, que KÃ©vin DUNGLAS est venu nous prÃ©senter !\n\nNous avions entendu parler de ce projet sans jamais encore prendre le temps de le tester ni dâ€™y penser plus en profondeurâ€¦ AprÃ¨s cette confÃ©rence, un POC sâ€™impose ;-)\n\nTout pour se prÃ©parer Ã  PHP 7.4\n\nLa prochaine version de PHP, la 7.4, devrait Ãªtre publiÃ©e en fin dâ€™annÃ©e. Comme tous les ans, elle apportera un petit lot de nouveautÃ©s que Damien SEGUY nous a prÃ©sentÃ©es.\n\nNous avons hÃ¢te de pouvoir exploiter certaines dâ€™entre elles. En particulier, le pre-loading, qui pourrait amÃ©liorer encore notre tenue Ã  la charge lors de nos pics de trafic quotidiens !\n\nâ€œEn vracâ€\n\nLa derniÃ¨re confÃ©rence du premier jour de ce Forum, par Marie-CÃ©cile GODWIN et Thomas DI LUCCIO, visait Ã  nous ouvrir les yeux : en tant que designer, concepteurs ou dÃ©veloppeurs dâ€™applications et dâ€™outils numÃ©riques, nous devons penser au futur ; les ressources de notre planÃ¨te ne sont pas infinies.\n\nCelle du second jour Ã©tait plus lÃ©gÃ¨re : Roland LEHOUCQ nous a parlÃ© de physique, en tirant ses exemples et anecdotes de Star Wars. Quâ€™est-ce que la Force ? Quelle puissance est capable dâ€™exploiter Palpatine ? Ou combien de gigawatts extrait un sabre-laser ? Une trÃ¨s bonne clÃ´ture pour ce Forum !\n\n\n\nNous avons aussi prÃ©sentÃ© deux confÃ©rences, autour de sujets que nous pratiquons au quotidien chez M6 DistributionÂ :\n\n\n  Pascal MARTIN a donnÃ© quelques pistes pour amÃ©liorer la rÃ©silience dâ€™applications, en insistant sur le fait que nos plateformes, de plus en plus complexes, ne sont jamais opÃ©rationnelles : elle se trouvent en permanence dans un Ã©tat de service partiellement dÃ©gradÃ©.\n  Benoit VIGUIER a continuÃ© dans la lancÃ©e de sa confÃ©rence de lâ€™annÃ©e derniÃ¨re, en prÃ©sentant cette fois-ci un retour dâ€™expÃ©rience aprÃ¨s un an dâ€™utilisation de PHP asynchrone en production. Spoiler alert : PHP rÃ©pond trÃ¨s bien au besoin et les gÃ©nÃ©rateurs sont le bien ! Ã€ noter aussi son intervention aux traditionnels Lightning Talks, oÃ¹ il nous a prÃ©sentÃ© une idÃ©e un peu folle : faire des interfaces graphiques avec Php.\n\n\nLâ€™AFUP Day 2020 Lyon est dÃ©jÃ  en train de sâ€™organiser ! Nous y serons sans doute en nombre et espÃ©rons vous y rencontrer Ã  nouveau !\n\n\n"
} ,
  
  {
    "title"    : "Une application rÃ©siliente, dans un monde partiellement dÃ©gradÃ©",
    "category" : "",
    "tags"     : " conference, architecture, resilience, afup, cloud",
    "url"      : "/2019/10/25/une-application-resiliente-dans-un-monde-partiellement-degrade-pascal-martin.html",
    "date"     : "October 25, 2019",
    "excerpt"  : "Dans un monde en perpÃ©tuelle Ã©volution, pouvons-nous toujours atteindre Â«Â four-ninesÂ Â» de disponibilitÃ©Â ?\nCloud et Kubernetes. APIs et Microservicesâ€¦ Nos architectures sâ€™enrichissent et se complexifient. Au prix dâ€™une certaine fragilitÃ©Â ?\n\nNous co...",
  "content"  : "Dans un monde en perpÃ©tuelle Ã©volution, pouvons-nous toujours atteindre Â«Â four-ninesÂ Â» de disponibilitÃ©Â ?\nCloud et Kubernetes. APIs et Microservicesâ€¦ Nos architectures sâ€™enrichissent et se complexifient. Au prix dâ€™une certaine fragilitÃ©Â ?\n\nNous commencerons par dÃ©finir SLA, SLO et SLI et rappeler la signification de ces X-nines.\nNous montrerons ensuite comment, dans un contexte en permanence partiellement dÃ©gradÃ©, nos assemblages de services distribuÃ©s nuisent Ã  la fiabilitÃ© de nos plateformes.\n\nEn profitant de lâ€™expÃ©rience acquise sur 6play, nous verrons quelques pistes pour amÃ©liorer la rÃ©silience de nos applications, pour quâ€™elles rÃ©pondent Ã  nouveau aux besoins de notre public. Nous prononcerons peut-Ãªtre mÃªme le terme de Â«Â Chaos EngineeringÂ Â»Â ;-)\n"
} ,
  
  {
    "title"    : "One year of asynchronous PHP in production",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2019/10/25/one-year-of-asynchronous-code-in-production.html",
    "date"     : "October 25, 2019",
    "excerpt"  : "Oui, il est tout Ã  fait possible de faire de la programmation asynchrone en PHP et il existe des librairies matures pour le mettre en place dans vos projets. Oui, Ã§a peut amÃ©liorer considÃ©rablement la performance de vos applications, mais si câ€™Ã©ta...",
  "content"  : "Oui, il est tout Ã  fait possible de faire de la programmation asynchrone en PHP et il existe des librairies matures pour le mettre en place dans vos projets. Oui, Ã§a peut amÃ©liorer considÃ©rablement la performance de vos applications, mais si câ€™Ã©tait aussi simple tout le monde le ferait dÃ©jÃ . Cela fait plus dâ€™an que les Ã©quipes de 6play ont franchit le pas sur certains projets et les applications asynchrones tiennent toutes leurs promesses en production, mais la mise en place a soulevÃ© beaucoup de questions. Ã€ quels critÃ¨res se fier pour rendre une application asynchrone? Comment former les Ã©quipes sur ces nouveaux paradigmes? Comment adapter les outils existants et comment gÃ©rer ce nouveau type de charge sur les serveurs? Voici notre retour dâ€™expÃ©rience sur le PHP asynchrone, du dÃ©veloppement Ã  la production, en passant par la vie de tous les jours.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #11",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2019/09/02/m6web-dev-facts-11.html",
    "date"     : "September 2, 2019",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nLa persuasion\n\n  Lâ€™important, câ€™est dâ€™Ãªtre convaincu quâ€™tâ€™es convaincu\n\n\nParfois dÃ©bug câ€™est compliquer\n\n  Comment je teste lâ€™alerte kidnapping en local ?\n\n\n\n  Bah tu vas dans un parc et tu enlÃ¨ves un en...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nLa persuasion\n\n  Lâ€™important, câ€™est dâ€™Ãªtre convaincu quâ€™tâ€™es convaincu\n\n\nParfois dÃ©bug câ€™est compliquer\n\n  Comment je teste lâ€™alerte kidnapping en local ?\n\n\n\n  Bah tu vas dans un parc et tu enlÃ¨ves un enfant, dâ€™ici deux trois heures tu devrais voir lâ€™alerte.\n\n\nÃŠtre Ã  sec\n\n  Vous pourriez tirer vers nous! On a plus de flÃ¨ches de nerf ?\n\n\nSeal of approval (lu sur slack)\n\n  Moi, Mr XXXX, Lead Dev de mon Ã©tat, sain de corps et dâ€™esprit, atteste de la validation et du bien fondÃ© de cette requÃªte, mÃªme si câ€™est un vendredi\n\n\nPrÃ©cision\n\n  Si il faut que ce soit prÃ©cis, il faut prÃ©ciser\n\n\nFrench please\n\n  _ Est ce quâ€™on peut parler en franÃ§ais maintenant ici ?\n_ Yes.\n\n\nLa difficultÃ©\n\n  Il y a 2 choses de compliquÃ©es en dÃ©veloppement : nommer les choses et invalider du cache.\nFigure toi que je suis en train de nommer une PR qui invalide du cache\n\n\nLa qualitÃ© avant tout\n\nif (value instanceof Collection) {\n    return ((Collection) value).isEmpty() ? false : true;\n} else if (value instanceof String) {\n    return StringUtils.isNotBlank((String) value) ? true : false;\n}\n\n\nBoire ou coder\n\n  _ Jâ€™ai fait un bateau Ã  la ganane pour la rÃ©tro !\n_ Il y a du rhum dans la recette :rolling_on_the_floor_laughing: ?\n_ MÃªme pas !!\n\n\nLe CDD\n\n  ConfÃ©rence de presse Driven Development\n\n\nMais oui câ€™est clair\nswitch($categoryId) {\n  case &#39;16&#39;:\n    return 38;\n  case &#39;18&#39;:\n    return 40;\n  case &#39;26&#39;:\n    return 42;\n  case &#39;28&#39;:\n    return 36;\n}\n\n\nLe nommage câ€™est important\n\n$catId = &#39;turlututu&#39;;\n$programId = &#39;chapeaupointu&#39;\n\n\nLa magie nuagique\n\n\n  Le cloud est un Ã©tat dâ€™esprit, pas un endroit oÃ¹ on dÃ©ploie\n\n\nLes index commencent Ã  0\n\n\n  La reproductivitÃ© est lâ€™Ã©tape NÂ°1 du debuggage\n\n\nOn fait quoi demain ?\n\n\n  Rappelle moi dâ€™acheter un agenda stp !\n\n\nRIP\n\n// Remplissage d&#39;un tableau de donnÃ©es pour un template de job (pas Steve, il est mort)\n\n\nMme Irma\n\n  Montre moi ton diff, je te dirai qui tu es!\n\n\nLes projets Ã  succÃ¨s\n\n  Il parait quâ€™un projet legacy câ€™est un projet qui a rÃ©ussiâ€¦ bin on a certains projets qui ont vachement bien rÃ©ussi !\n\n\nLâ€™appÃ©tit des croissants\n\n  Pourriez-vous pÃ©ter la prod plus souvent ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #10",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2019/08/08/m6web-dev-facts-10.html",
    "date"     : "August 8, 2019",
    "excerpt"  : "Ã‡a faisait un trÃ¨s trÃ¨s long moment ! Voici le retour des devfacts !\n\nLâ€™erreur est humaine\n\n  Il faut mieux recevoir une erreur 500, que 500 erreurs\n\n\nLâ€™odeur du code\n\n  Le code de merde, tu sais quand câ€™est le tien, câ€™est comme les pets, pas la p...",
  "content"  : "Ã‡a faisait un trÃ¨s trÃ¨s long moment ! Voici le retour des devfacts !\n\nLâ€™erreur est humaine\n\n  Il faut mieux recevoir une erreur 500, que 500 erreurs\n\n\nLâ€™odeur du code\n\n  Le code de merde, tu sais quand câ€™est le tien, câ€™est comme les pets, pas la peine de git blame.\n\n\nRestons zen !\n\n  _ Ah mais en fait !!! Je comprends pourquoi tu viens tÃ´t le matin! Tu viens parce que câ€™est calme !!!!\n\n\n\n  _ Tais toi Mehdi !!\n\n\nPrÃ©voir lâ€™imprÃ©visible\n\n  Câ€™est la premiÃ¨re Ã©tape de lâ€™Ã©tape suivante.\n\n\nCe matin au chiffrage\n\n  Jâ€™ai pensÃ© 5 mais jâ€™ai mis 3â€¦\n\n\nSchroedinger app\n\n  _ Ã‡a marche ?\n\n\n\n  _ Je sais pas mais câ€™est en prod !\n\n\nLâ€™effet de serre\n\n  _ Il fait froid dans le bureau\n\n\n\n  _ Bah dÃ©marre 2-3 docker sur ton Mac, Ã§a devrait rÃ©soudre le problÃ¨me\n\n\nLes prioritÃ©s\n\n  _ Alleeeez, sâ€™il te plaiiiiiiiit !\n\n\n\n  _ Les incidents dâ€™abord, les dÃ©tails graphiques aprÃ¨s !\n\n\nPetite nouveautÃ©, voici quelques mÃ¨mes maison !\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "React Europe 2019",
    "category" : "",
    "tags"     : " React, JS, 6play, Conference, 2019",
    "url"      : "/2019/05/27/react-europe-2019.html",
    "date"     : "May 27, 2019",
    "excerpt"  : "The M6 Distributionâ€™s (M6 Webâ€™s new name!) front team hasnâ€™t posted for a long time. We took part as listeners of the 5th European React Conference in Paris on May 24th and 25th. Itâ€™s an opportunity to talk about what are the last moves in the Rea...",
  "content"  : "The M6 Distributionâ€™s (M6 Webâ€™s new name!) front team hasnâ€™t posted for a long time. We took part as listeners of the 5th European React Conference in Paris on May 24th and 25th. Itâ€™s an opportunity to talk about what are the last moves in the React community and at M6.\n\nAs usual, we were waiting for lot of announcements in this conference and a lot of new tools or new libraries. There has been no big declaration, no surprise. But many subjects were interesting and several talks confirmed the way we have taken over the last few years.\n\nHooks, more hooks and suspense\n\nDuring the keynote, Jared Palmer put the emphasis on hooks through an example to simplify the use of GraphQL queries. At M6 Distribution, we use hooks since the beginning of the year and many are in production. That has changed the way we write components. We already have used functionnal components before, but using hooks simplify the readability and the evolutivity of old class component. The bad point is testingâ€¦ Because we use Enzyme, testing hooks is painful for now. Until Enzyme fully supports hooks, we have implemented custom mocks.\n\nJared Palmer also showed us the interest of React Suspense to manage a main loading state in an app instead of many spinners that donâ€™t offer a good user experience. We canâ€™t use Suspense for our app because of the SSR. As it is recommended by the React team, we use Loadable Components instead. Jared  announced that a new asynchronous server renderer is in progress and could be released in 2019 supporting Suspense. Suspense will also include data fetching.\n\nOthers features or refactoring will come in the future, but there is very limited information:\n\n  React Fire\n  React Fusion (?)\n  React Native Fabric\n  React Flare\n\n\nDo the harlem shake\n\nSeveral conferences have addressed the theme of animations with React. This subject is very often complicated to solve. Finding a high-performance solution that is compatible with as many browsers as possible can be a real challenge.\n\nFor more than a year now, we have been migrating from Sass to styled-components. Despite some resentment, it should be noted that this new tool makes it very easy to create/compose our new visual components. It also helps us a lot with its management of the theme.\n\nBetween the react-spring library and the tips of Joshua Comeau, we now have many ways to integrate the new animations that designers can invent.\n\nTo summarize their comments, there is no single best way to make animations in all situations. Whether with canvas, SVG, web API, CSS, 2D sprite, you should try to do it in several ways to compare performance and rendering in order to choose the best option for your specific case.\n\nMake tests but make good tests\n\nLisa Gagarina gave us some tips to better testing the code. Even If we have been using ESLint and Prettier for a long time in our JS projects, we have chosen not to implement static typing for the moment. Indeed, we think that it is a huge step, it has a big impact on the code and it can make the onboarding of new developpers more complex. For now, we just use the React proptypes well.\n\nLisa also advices to better use Jest snapshots. They are often too many, too big, not clear, hard to review. And we have experienced it too. A best pratice can be to reduce snapshots size and inline it in the test file. The readability of a test is indeed very important and it is not recommended to refer to other files than the test file (this is valid for snapshots but also for fixtures). There are some ESLint rules to ensure this: jest/no-large-snapshots, jest/prefer-inline-snapshots. snapshot-diff can also helps by making a diff between the nominal case and the tested case of a component rendering.\n\nA difficult thing is to keep test agnostic to implementation details. We should consider the code as a black-box and only test the user interaction of the components, otherwise any refactoring of code will be painful and might discourage developers from writing tests.\n\nE2E tests are also complex to setup, write and debug but are absolutely necessary. For the back office app, unlike our front app where we use a custom stack, we choose Cypress a complete E2E framework that has saved us a lot of time.\n\nLisa concluded her very interesting lightning talk by saying that there is no such thing as a one-size-fits-all approach, the way of testing has to be adapted to the team and the project. For example on our 6play project, we have more than 3000 unit tests performed by Jest in less than 4 minutes and 450 E2E scenarios that save your life every day!\n\nIf you are interested in this subject, take a look at this article about JS testing best practices.\n\nFinally a little GraphQL in our projects\n\nOne of the main themes of the conference was also GraphQL. \nThis data exchange paradigm now seems to have its place among many users.\nFor more than a year now, we have been using GraphQL on our back office and we are already seeing a lot of benefits:\n\n\n  a front/back exchange contract materialized in a schema,\n  easy consumption thanks to queries,\n  cache management provided by Apollo (also providing a collection of great tools for GraphQL).\n\n\nKenny already talked about it in 2015 but now we use it!\n\nDevelopment worflow: the expected journey\n\nOne of the most interesting conferences in my opinion was about the need to optimize the development workflow. This is an element that is too often ignored but is very important in the life of a project. Paul Amstrong presents us here an analysis of the workflow of his team in charge of the development of the Twitter Lite application. He also presented some conclusions and solutions he implemented. In a standard development workflow there are 3 points that allow a significant margin of progress:\n\n  increased developer confidence and delivery speed,\n  automate the PR process to maximum,\n  detect errors as early as possible.\n\n\nThese words echo our own questions on the subject and these conclusions confirm our decisions. \nIt is important to have the shortest and most automated workflow possible between the developer and the user without sacrificing the developerâ€™s experience because it goes hand in hand with all the other aspects of a project.\n\nAt M6 Distribution, we use most of the tools presented, but two in particular caught our attention.\n\nThe first, React Component Benchmark, is of particular interest to us because in the past we had started to investigate the subject and used tools that are depreciated today.\n\nThe second, Build Tracker, which allows us to test the evolution of bundle size, will allow us to replace an equivalent tool developed internally while providing a more detailed analysis in order to work more accurately on these issues.\n\nHere is an example of a message posted by our in-house build tracker on a Pull Request.\n\n\n\n\n\nA11y is our new challenge\n\nSeveral very inspiring talks on accessibility seem to show that this issue finally appears to be considered by web actors. In particular Facebook, which has distinguished itself by showing its assistant to detect accessibility errors in the development of its new version. However, it is regrettable that this toolkit is not accessible to the community because it could be a great help to avoid putting people from being in a situation of handicap.\n\nIt is clear that our front web app is not yet very accessible but we are working to correct this error, especially on the new screens we have been integrating for several months.\n\nOur expectations on Yarn finally fulfilled\n\nWe were waiting for it at M6, the new yarn release named berry offers almost everything we were missing in our favorite package manager.\nIndeed, by using yarn in monorepo mode for our front projects, we were confronted with several problems that had to be overcome with in-house tools. Example here with our monorepo-dependencies-check tool which is now becoming obsolete thanks to Constraints.\n\nWe are also delighted with the functionality of Zero install. But what we expected the most was about the workspaces. We will finally have a management of the publication of these.\n\nTake a look at this repository for more information.\n\nSome projects that interest us a lot: Next.js &amp;amp; Code Sandbox\n\nEven if these two tools are not used for the development of our applications, the new features of Next.js and Code Sandbox are clearly very interesting.\n\nAs for Next.js, our front web project has its own configuration of server side rendering (Florent explains it in â€˜Last night isomorphic JS saved our life!â€™). However, NextJS is a great project that we use for many of our side projects. AMP support, client-only pages and API endpoints are clearly welcome.\n\nAs for Code Sandbox, Ives van Hoorne told us his personal story and that of his project. In addition to the great tool that is CodeSandbox, we have seen that the use of WebAssembly seems to have solved a lot of performance and implementation problems. For example, he cites the coloring of the code based on TextMate only available in C could not have been ported to the browser without going through WebAssembly.\n"
} ,
  
  {
    "title"    : "AFUP Day Lyon 2019",
    "category" : "",
    "tags"     : " php, afup, 2019",
    "url"      : "/2019/05/23/afup-day.html",
    "date"     : "May 23, 2019",
    "excerpt"  : "TechM6WEB Ã©tait trÃ¨s fier de sponsoriser le premier AFUP DAY Ã  Lyon.\n\nPour une premiÃ¨re, câ€™Ã©tait trÃ¨s rÃ©ussie. Un programme au top et variÃ©, qui mettait en avant de nombreuses problÃ©matiques techs et sociÃ©tales.\n\n\n(photo : Benjamin LÃ©vÃªque)\n\nNos c...",
  "content"  : "TechM6WEB Ã©tait trÃ¨s fier de sponsoriser le premier AFUP DAY Ã  Lyon.\n\nPour une premiÃ¨re, câ€™Ã©tait trÃ¨s rÃ©ussie. Un programme au top et variÃ©, qui mettait en avant de nombreuses problÃ©matiques techs et sociÃ©tales.\n\n\n(photo : Benjamin LÃ©vÃªque)\n\nNos confÃ©rences prÃ©fÃ©rÃ©es :\n\n  OVH.com from 1999 to 2019, car câ€™est toujours intÃ©ressant les REX de projets importants en toute humilitÃ©,\n  Lâ€™architecture progressive, bien quâ€™elle ait soulevÃ©e pas mal de trolls en interne,\n  Les merveilles mÃ©connues du SQL, car en vrai â€œyou know no SQLâ€ :)\n\n\nEt bien sur la table ronde des CTO pendant laquelle Olivier Mansour est intervenu.\n\nMerci lâ€™AFUP pour cet Ã©vÃ¨nement prÃ¨s de chez nous !\n"
} ,
  
  {
    "title"    : "Migrating production applications from on-premise to the cloud with no downtime",
    "category" : "",
    "tags"     : " Cloud, AWS, Kubernetes, Kops, HAProxy, GOReplay",
    "url"      : "/2019/03/11/Migrating-production-apps-from-on-premise-to-the-cloud-with-no-downtime.html",
    "date"     : "March 11, 2019",
    "excerpt"  : "We are migrating all our on-premise applications to AWS cloud.\nMost of them are/will be migrated to Kops-managed Kubernetes clusters, and some are/will be migrated as lambdas.\nTo secure this migration, we are using HAProxy in front of both on-prem...",
  "content"  : "We are migrating all our on-premise applications to AWS cloud.\nMost of them are/will be migrated to Kops-managed Kubernetes clusters, and some are/will be migrated as lambdas.\nTo secure this migration, we are using HAProxy in front of both on-prem and on-AWS deployments (first only sending 1% of each applicationâ€™s requests to AWS, then 5%, then 25% and so on).\nDisclaimer: This article describes a feedback from production environment. We have changed the name of applications mentioned here, but everything else is true within the limits of our knowledge.\n\nYou can find the content of this blogpost (and more) in a 25mn talk at the HAProxy conf given in Amsterdam in 2019 :\n\n\nThe first migrated application\n\nItâ€™s an API written in PHP. It has no external dependency (database, redisâ€¦), except for another API, called over HTTP.\nWe have migrated this application to the cloud like weâ€™ve done with other applications since.\n\nThe first step was to deploy this application to a kubernetes cluster and expose it over an ELB.\nThen, we wanted to send real-user requests to that app. We wanted to see how it would behave with real production traffic.\nBut we didnâ€™t want to send 100% of our users over there at once: weâ€™d first rather check everything works fine with just 1% of our users.\n\nHAProxy\n\nWeâ€™ve been using HAProxy for several years now.\nBecause of its features, like advanced backend monitoring or its enormous number of metrics, itâ€™s the perfect tool to help us on this migration.\n\nAt first, we didnâ€™t know how the app, the Horizontal Pod Autoscaler, Liveness probes etc. would react with real live production requests.\nSo, we decided to migrate only 1% of production HTTP requests to our Kubernetes cluster. The other 99% of HTTP requests would remain on premise, where the application works for sure.\n\nHereâ€™s a part of the associated HAProxy configuration:\n\nbackend application-01\n    http-response add-header X-Backend-Server %s\n    balance roundrobin\n    http-request set-header Host application-01.6play.fr\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-01.6play.fr\n    server aws-prod-Kubernetes-application-01 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-01.6play.fr ca-file ca-certificates.crt inter 1s fall 1 rise 2 resolvers m6dns observe layer7 weight 25\n    server onprem-prod-front-application-01 onprem-application-01.6play.fr:80 check resolvers m6dns weight 75\n\nSome explanations on key elements of this configuration:\n\n\n  http-response add-header: adds an HTTP header to the response, with the server chosen by HAProxy to handle the request. We added this for debugging purposes: to know who handled our request when requesting the service\n  balance roundrobin: because weâ€™re using a stateless application\n  check .. inter 1s fall 1 rise 2 in the server directive: healthchecks have 1 second of timeout, only 1 bad healthcheck is enough to mark the server as unhealthy and we need 2 good healthchecks in a row to mark it as healthy again\n  observe layer7: It will simulate a bad healthcheck for each application server error (e.g: 500, 502, 503, etc.) making this server fall in such a situation\n  weight 25: We use weights from 0 to 100 (you can go up to 256), so that corresponds to traffic percentages in our case\n\n\nWith the above configuration, as soon as there is an error, HAProxy wonâ€™t send traffic to our AWS/Kubernetes application anymore; consequently, it will have a minimum impact on endusers.\n\nTests\n\nWe first tested this with staging proxies, with a temporary domain name and a 50-50% loadbalancer, to ensure the load-balancing worked fine.\nWe tried to kill the deployment on Kubernetes to check 100% of requests came back on-prem. We tested killing random pods to see if we had some user impact. We tested to slow down the application so it would be slower than 1s to respond. We also tested to slow down only one of two pods running the application.\nIt was all OK, so we were confident to go to production.\n\nMigration steps\n\nBefore its migration, the application infrastructure looked like this:\n\n\nWe inserted HAProxy servers in that schema, so the traffic passes through them before being sent to the caches.\nThat way, HAProxy controls where traffic is sent.\nTo make those migrations as transparent as possible, we first configured HAProxy to send traffic to on-prem servers only.\nIn the same time, developers have deployed all mandatory resources (RDS, DynamoDB, elasticache, etc.) with Terraform and verified the application works fine. The application itself could have changed: either the code or kubernetes manifests.\nWhen ready, both ops and devs gave their approval to send traffic to the cloud.\n\nWe started by load-balancing 1% to the AWS ELB with HAProxy:\n\n\nWe compared everything we could:\n\n  2xx, 3xx, 4xx and 5xx percentages\n  connect and response times\n  failed healthchecks and healthchecks return codes\n  backend retries and bad responses\n\n\nWe were amazed: only 3ms in average difference between on-prem and our Kubernetes cluster in AWS cloud.\nAnd no error. Everything worked as expected. It was almost suspicious.\n\n\nThis graph shows the average connect times from HAProxy.\n\nWeâ€™re using Paris as AWS zone and our datacenters are located in Paris too, so that explains the few milliseconds to go back and forth from HAProxy (on prem) to the cloud. In fact, this one to two millisecond between our on-prem servers and AWS is one of the reasons adding HAProxy in the mix was possible.\n\n\nThis graph shows the average response times from the application.\n\nWe also had some PHP configurations to update to be ISO prod (OPCache, APCu, etc.). Why? Well, at first, we created a quickâ€™n dirty (working, but not optimized) Docker image for our application and it went straight to production, before our sysadmins could take a better look at it.\n\n\nThis graph shows the number of 2xx HTTP codes with 25% of traffic sent to AWS.\n\nOnce those PHP optimisations were fixed, we had only 2ms difference between our on-premise and our Kubernetes on AWS. Itâ€™s low enough to allow us to test this setup a bit longer without any visible user impact.\n\nDeploying more and more\n\n1% on our kubernetes cluster was great, but it was not enough to see perfs issues.\nSo we raised HAProxyâ€™s load balancing from 1%/99% to 10%/90%.\nStill not enough. We raised it to 25%/75%.\nWe checked applicationâ€™s pods CPU usage, it was really low. Too low to even trigger the Horizontal Pod Autoscaler based on cpu usage. We couldnâ€™t validate our pods Requests and Limits for that Kubernetes deployment therefore.\n\nSo far, it was enough for us to validate things we could check were working fine. The application cache was efficient, we had stable performances and no surprise on traffic peaks.\n\nOn this first application migration, we decided to stay at 25% of traffic sent to the cloud for the moment, to observe.\nWe did it because HAProxy would have saved us if something bad happened.\n\nIt did behave well: nothing happened for 26 days.\n\nSome errors encountered\n\nUnknown nodes\n\nOn the 27th day, we noticed two of our three nodes were in Unknown state. As our Replicas specified we wanted several pods, Kubernetes started new pods. But as we only had one Ready node left (3 worker nodes in the cluster, including two in Unknown state), all pods for our application were now on that single node. Not great for reliability.\nWe found that the CoreOS image we used was doing automatic updates that restarted nodes regularly. On those two nodes (and on the third one a few days later!), the restart did not go well and Kubelet wasnâ€™t starting at bootime. After investigation, it appears we changed the KOPS STATE STORE bucket. Nodes started before that change were impacted as Kubelet couldnâ€™t find its configuration. Starting new nodes and killing the old ones solved this issue.\nThat allowed us to identify two problems: we didnâ€™t restrict when and how automatic updates were started; and we didnâ€™t have enough cluster monitoring.\n\nFrom that, we knew we had to monitor:\n\n  Nodes in a state different from Ready for a certain time\n  If the number of Ready nodes is at least equal to the Auto Scaling Group minimum\n  If the number of Ready nodes is at most 90% the Auto Scaling Group maximum\n\n\nDifferences between cluster-autoscaler and ASG values\n\nWhile trying to solve the problem above, we also found that pods were living around 5mns before being destroyed and created again. After some investigation, we found that min and max EC2 instances were not configured to the same values between the AWS Auto Scaling Group and the cluster-autoscaler pod.\nSome day, we modified the Kops configuration for those worker nodes and the configuration was well applied to the ASG, but was not applied to the cluster-autoscaler. As a result, we had an ASG minimum of 4 worker nodes, but a cluster-autoscaler minimum of 3.\nAt some point in the history, the cluster-autoscaler defined that the current amount of worker nodes (four) was too high compared to the real need, so it tried to reallocate pods to free up a node. It have done that by draining pods from a node. At the same time, the cluster-autoscaler tried to change the ASGâ€™s desired value to 3 nodes. Because the minimum nodes configured for the ASG was 4, the latter denied the request. Kubernetes scheduler chose to reschedule those recently-killed pods on available nodes, starting by the one with no running pods, AKA: the one that just drained them all.\nThis last phase started again every 5 minutes, making sure that our pods did not survive longer than that.\n\nApplication latencies undetected by health checks\n\nOur application was really slow on Kubernetes/AWS (due to a network misconfiguration) but HAProxy did not disable it. We specified a 1s timeout as shown in the example above, but this is only for healthchecks. Our global server timeout is upper than 1s. Because our application calls another webservice, those calls were timeouting. HAProxy was not aware of that, because the applicationâ€™s /HealthCheck health page doesnâ€™t check external webservices and thus, were not impacted by those external webservices timeouts. This is an application choice that we can encounter on-premise too, with the exact same behavior. For that reason, we decided to change nothing for now (and weâ€™ll discuss this with the devs teams to see if thereâ€™s something we can do).\nWe donâ€™t check external webservices in our /HealthCheck page on purpose, because that page is also tested by kubernetes for livenessProbe. Kubernetes restarts a pod when it is not healthy anymore but when it comes to an external service that is failing, restarting the current pod is non-sens. Kubernetes will restart pods again and again even if the application itself canâ€™t to anything about it! The livenessProbe should test only what the pod does. The Amadeus team talked about that at the KubeCon EU 2018 while presenting Kubervisor.\n\nPedal to the metal\n\nWe were stabilized again.\nSo we raised HAProxy load-balancing to 50% on our application in the cloud.\nAfter seven days without any error, we pushed it to 75%.\nAfter another seven days, we passed the on-prem server as a backup in HAProxy, making the application in kubernetes receiving 100% traffic.\n\n\n\nWe stayed with that configuration for 2 months.\nThat gave us plenty of time to adapt pods Requests and Limits.\nThat is really important for us, because we use HorizontalPodAutoscaler resources with CPU metrics to scale most of our APIs. Here you can find slides deep diving one of our applications that autoscales in prod with kubernetes.\nWe had several events during those 2 months that helped us optimize Requests and Limits for that app. For example, we had holiday traffic, a football match and some special primetime sessions.\nWe also improved our knowledge of both Kubernetes and AWS during this time (I.e: What happens when we rolling restart worker nodes?). Finally, we have configured our Prometheus servers with effective and non-noisy alerts.\n\nAfter weeks of optimizations, we migrated this appâ€™s DNS directly on ELB without HAProxy.\nEverything works perfectly as expected since that day.\n\nNext applications to migrate\n\nWeâ€™ve done a lot of work for our first migration. Weâ€™ve capitalized that time for the next projects to finally be able to migrate them in few days.\nThe workflow stays unchanged:\n\n\n  Deploy the application into a kubernetes cluster\n  Add HAProxy servers in front of both on-prem and in-cloud instances\n  Load balance from 1% to 100% traffic to the in-cloud instance\n  Configure accurate Requests and Limits\n  Create efficient alerting\n  Point DNS to ELB\n\n\nMigrate an application path by path\n\nSome of our applications needed to be partially rewritten to be cloud native.\nOnly specific paths were affected by this rewrite.\n\nSo we decided to use HAProxy to migrate those applications, path by path.\nWe also used GOReplay to replicate production traffic for each path, to be sure we didnâ€™t messed up things before sending end-users traffic.\n\n\nThis schema shows how HAProxy were routing traffic according to specific paths.\n\nThe workflow is almost the same as above, with few changes:\n\n\n  Deploy the application into a kubernetes cluster\n  Add HAProxy servers in front of both on-prem and in-cloud instances\n  Use HAProxy map_reg to route traffic, depending of the requested URL\n  Define path routing preferences in the map file created in step 3 (see example below)\n  Configure and test each path:\n    \n      Let developers rewrite paths, I.E: /HealthCheck\n      Replicate production traffic with GOReplay, to specific paths, including /HealthCheck, from on-prem to the application in the Kubernetes cluster in AWS\n      Stabilize the application: either code optimisations or Kubernetes Requests and Limits adaptations\n      Add this newly created path /HealthCheck on the HAProxyâ€™s routing map file\n      Repeat for each new path\n    \n  \n  Create a specific HAProxy Backend section for each route to load balance traffic differently for each route\n  Increase traffic load balancing up to 100% to the cloud\n  Create efficient alerting\n  Point the DNS to the ELB\n\n\nTraffic replication with GOReplay\n\nWe use a lot GOReplay.\nNot only because itâ€™s light and easy to work with, but because we can do whatever we want with it to replicate traffic. It can rewrite headers, catch only a specific domain or a specific url. Itâ€™s the perfect tool to complete our migration workflow.\n\nHere is a script we used in the step 5.b of the workflow above:\n\n#!/bin/bash\n\nreplicate_traffic() {\n    if [[ -z $1 ]]\n    then\n        local REPLICATION_PERCENTAGE=5%\n    else\n        local REPLICATION_PERCENTAGE=$1\n    fi\n\n    if [[ -z $2 ]]\n    then\n        local TIMEOUT=45s\n    else\n        local TIMEOUT=$2\n    fi\n\n    echo &quot;Replicating traffic at ${REPLICATION_PERCENTAGE} for $TIMEOUT&quot;\n\n    ./gor -exit-after $TIMEOUT \\\n    -input-raw :8080 \\\n    -http-disallow-url /v2/critical/sensible_datas/payments/ \\\n    -http-allow-url /v2 \\\n    -input-raw-bpf-filter &quot;dst host 127.0.0.72&quot; \\\n    -output-http &quot;https://application-02.6play.fr/|${REPLICATION_PERCENTAGE}&quot; \\\n    -http-original-host \\\n    2&amp;gt;/dev/null\n}\n\n# The above allows a normal ramp-up of the traffic.\n# That means application replicas can be low and increase naturally without an insane peak\nreplicate_traffic 1% 45s\nreplicate_traffic 2% 45s\nreplicate_traffic 5% 45s\nreplicate_traffic 10% 45s\nreplicate_traffic 20% 60s\nreplicate_traffic 40% 60s\nreplicate_traffic 60% 60s\nreplicate_traffic 80% 60s\nreplicate_traffic 100% 7h\n\nWeâ€™re using this script and not directly the gor command, to do a slow ramp-up of traffic to the application in Kubernetes.\nOtherwise, since the application is not stressed before traffic is replicated, replicating 100% of traffic all of a sudden would not be representative of real user behavior. It would led to unwanted alerts that would disappear in minutes with auto-scaling, but that would have rang anyway. So we chose to avoid that noise by doing a slow ramp-up to make traffic replication more real.\n\nWe could follow the replication with HAProxy dashboard, like the following graph:\n\n\nHAProxy configuration\n\nTo achieve a path-by-path migration of an application, we used this HAProxy configuration:\n\nfrontend application-02\n    ...\n    # Defined with a &quot;map&quot; style, from file /etc/haproxy/domain2backend.map\n    # CF https://blog.haproxy.com/2015/01/26/web-application-name-to-backend-mapping-in-haproxy/\n    use_backend %[base,map_reg(/etc/haproxy/domain2backend.map,bk_default)]\n\nbackend application-02-on-prem\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns\n\nbackend application-02-on-cloud\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns\n\nbackend application-02-mixed\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns weight 75\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns inter 3s fall 1 rise 2 observe layer7 weight 25\n\nbackend application-02-mixed-critical\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns weight 99\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns inter 3s fall 1 rise 2 observe layer7 weight 1\n\nAnd hereâ€™s the associated map file:\n\n#HOST Header                                                            #HAP backend_name\napplication-02\\.6play\\.fr\\/v2\\/critical\\/(\\w{1,45})\\/payments\\/         application-02-mixed-critical\napplication-02\\.6play\\.fr\\/v2\\/customers\\/                              application-02-mixed\napplication-02\\.6play\\.fr\\/v2\\/                                         application-02-on-cloud\napplication-02\\.6play\\.fr\\/v1\\/                                         application-02-on-prem\n\n# Catch ALL for application-02.6play.fr\napplication-02\\.6play\\.fr\\/                                             application-02-on-prem\n\n\nSome examples of traffic routing made by HAProxy with the configuration above:\n\n\n  application-02.6play.fr/v2/critical/sensible_datas/payments/\n    \n      sent on the specific application-02-mixed-critical backend,\n      with 1% traffic sent to the cloud\n    \n  \n  application-02.6play.fr/v2/customers/\n    \n      sent on application-02-mixed,\n      load balanced at 25% on the cloud\n    \n  \n  application-02.6play.fr/v2/\n    \n      sent on application-02-on-cloud,\n      only on the AWS ELB: cloud only\n    \n  \n  application-02.6play.fr/v1/\n    \n      sent on application-02-on-prem,\n      only on-premise\n    \n  \n  application-02.6play.fr/admin/\n    \n      sent on-premise only,\n      this is the default\n    \n  \n\n\nWith HAProxy map files and the according backend sections, weâ€™re able to migrate path by path any application from on-premise to our kubernetes cluster in the cloud.\nWith gor on top of it, itâ€™s even easier to allow developers develop a specific path while another is being migrated, and all that, with no downtime.\n\nNext steps\n\nWeâ€™ve done most of our cloud migration with workflows explained in this blogpost.\nThanks to HAProxy, most of our applications could be migrated at the same time with no impact from on migration to another.\n\nThere are still some applications to migrate though and one of them is a tough one. This application is heavily using Cassandra database. There is no Cassandra managed in AWS, so we are completely rewriting the application to adapt it to DynamoDB and also to face upcoming business needs.\nThe challenge is to keep existing pathUrl of the application, working. In other words: the new version have to give same functionalities, keeping the same URLs, but with totally different data management under the hood.\nGOReplay is a wonderful asset to help us in this task.\n\nIf you found this useful and youâ€™d like more production return of experiences like this one, please let us know. We plan to write more in the coming weeks.\n"
} ,
  
  {
    "title"    : "7 conseils pour dÃ©marrer avec Spark",
    "category" : "",
    "tags"     : " spark, scalaio, conference",
    "url"      : "/2019/01/14/7-conseils-pour-demarrer-avec-spark.html",
    "date"     : "January 14, 2019",
    "excerpt"  : "Je suis entrÃ©e dans le monde de la data avec Spark. \nIl y a eu des moments clairement plus ou moins compliquÃ©s. \nAu dÃ©but, câ€™Ã©tait le Far West : un monde inconnu et dangereux (il ne fallait pas casser la production). \nAvec ce retour dâ€™expÃ©rience, ...",
  "content"  : "Je suis entrÃ©e dans le monde de la data avec Spark. \nIl y a eu des moments clairement plus ou moins compliquÃ©s. \nAu dÃ©but, câ€™Ã©tait le Far West : un monde inconnu et dangereux (il ne fallait pas casser la production). \nAvec ce retour dâ€™expÃ©rience, je vous propose de vous dire ce que jâ€™aurais aimÃ© quâ€™on me dise avant de me lancer. \nJe promets aussi de vous parler de ce que bien heureusement mes camarades plus expÃ©rimentÃ©s mâ€™ont aussi donnÃ© comme astuces pour mâ€™aider dans ce grand saut. \nCe sera donc une entrÃ©e en matiÃ¨re dans le monde de Spark au travers de 7 conseils qui mâ€™ont Ã©tÃ© ou mâ€™auraient Ã©tÃ© bien pratiques pour dÃ©marrer.\n"
} ,
  
  {
    "title"    : "Le Plan Copenhague : notre migration vers Le Cloud, retour dâ€™expÃ©rience",
    "category" : "",
    "tags"     : " cloud, kubernetes, aws, terraform, livre",
    "url"      : "/2018/12/20/le-plan-copenhague.html",
    "date"     : "December 20, 2018",
    "excerpt"  : "Nous avons commencÃ© Ã  migrer notre plateforme 6play vers Le Cloud il y a un an.\n\nDepuis, nous avons dÃ©couvert Kubernetes et Helm, AWS et ses services managÃ©s, Terraform, Prometheus et une multitude dâ€™autres outils. Nous avons fait des choix, rÃ©pon...",
  "content"  : "Nous avons commencÃ© Ã  migrer notre plateforme 6play vers Le Cloud il y a un an.\n\nDepuis, nous avons dÃ©couvert Kubernetes et Helm, AWS et ses services managÃ©s, Terraform, Prometheus et une multitude dâ€™autres outils. Nous avons fait des choix, rÃ©pondu Ã  de nombreuses questions, rencontrÃ© et franchi des obstacles. Nous avons mis en place des bases solides ou, parfois, pris des raccourcis pour avancer plus vite.\n\nVous aimeriez dÃ©couvrir comment nous migrons une plateforme comme 6play vers Le Cloud, comment nous exploitons Kubernetes ou les services managÃ©s dâ€™AWSâ€‰? Vous vous demandez comment nous optimiserons les coÃ»ts ? Vous migrez peut-Ãªtre vous aussi votre hÃ©bergement, et comparer votre expÃ©rience Ã  la nÃ´tre vous aiderait Ã  avancerâ€‰?\n\nPascal, DevOps qui accompagne cette migration vers Le Cloud, a commencÃ© Ã  rÃ©diger un retour dâ€™expÃ©rience autour de ce projet. Les sept premiers chapitres viennent dâ€™Ãªtre publiÃ©s : vous pouvez dÃ¨s maintenant commencer Ã  lire Â«â€‰Le Plan Copenhagueâ€‰Â» !\n\nPendant ces cent premiÃ¨res pages, vous dÃ©couvrirez notre projet, les premiÃ¨res bases que nous avons construites et la mise en place de notre environnement chez AWS et sous Kubernetes. Nous irons jusquâ€™Ã  prÃ©senter comment nous avons migrÃ© notre premiÃ¨re application vers Le Cloud en toute sÃ©curitÃ©. La rÃ©daction des chapitres suivants va sâ€™Ã©taler sur une bonne partie de 2019. Nâ€™hÃ©sitez pas Ã  vous inscrire pour Ãªtre prÃ©venu lors de leur publication ;-)\n\nBonne lecture !\n"
} ,
  
  {
    "title"    : "Forum PHP Paris 2018",
    "category" : "",
    "tags"     : " forumphp, php, afup, 2018",
    "url"      : "/2018/11/12/retour-forum-php-2018.html",
    "date"     : "November 12, 2018",
    "excerpt"  : "Comme tous les ans, nous Ã©tions au Forum PHP 2018 organisÃ© par lâ€™AFUP ! Encore une fois, nous avons pu assister Ã  plusieurs confÃ©rences et Ã©changer avec grand nombre dâ€™entre vous. Voici quelques mots sur celles qui nous ont le plus marquÃ©.\n\nâ€œBoost...",
  "content"  : "Comme tous les ans, nous Ã©tions au Forum PHP 2018 organisÃ© par lâ€™AFUP ! Encore une fois, nous avons pu assister Ã  plusieurs confÃ©rences et Ã©changer avec grand nombre dâ€™entre vous. Voici quelques mots sur celles qui nous ont le plus marquÃ©.\n\nâ€œBoostez vos applications avec HTTP/2â€\n\nNous connaissons et utilisons tous HTTP au quotidien. Mais que se cache-t-il derriÃ¨re ce protocole ?\nKÃ©vin Dunglas nous Ã  prÃ©sentÃ© les nouvelles fonctionnalitÃ©s apportÃ©es par HTTP/2.\nIl nous a dâ€™abord fait un rappel sur les concepts HTTP et sur lâ€™Ã©volution de ce protocole conÃ§u pour Ã©changer des donnÃ©es documentaires.\nAprÃ¨s plus de 20 ans en version 1, HTTP Ã©volue et passe en version 2 !\n\nHTTP/2 Ã©tait initialement propulsÃ© par Google. PremiÃ¨re bonne nouvelle, pas de changement nÃ©cessaire du cÃ´tÃ© de nos applications PHP. La version 2 introduit de nouvelles fonctionnalitÃ©s intÃ©ressantes :\n\n\n  Priorisation des requÃªtes\n  Passage au binaire (optimisation de la taille des messages)\n  Notifications push\n  Et bien dâ€™autre\n\n\nKÃ©vin nous a aussi prÃ©sentÃ© le protocole Mercure. Il permet de faire des notifications push server side vers diffÃ©rents clients. Mercure semble simplifier grandement les Ã©changes push client / serveur. Toutes les parties dialoguent au travers dâ€™un hub (rÃ´le primaire de Mercure) et se synchronisent entre elles. Une petite dÃ©mo Ã  confirmÃ© lâ€™effet â€œwhaouâ€ de cette nouvelle solution.\n\nâ€œBeyond the design patterns and principles - writing good OO codeâ€ et â€œHow I started to love what they call Design Patternsâ€\n\nCe Forum a aussi Ã©tÃ© lâ€™occasion de voir (ou revoir) quelques principes fondamentaux autour des Design Patterns, avec Matthias Noback et Samuel Roze.\n\nIls ont prÃ©sentÃ© des exemples concrets, des mises en application de certains Design Patterns incontournables, toujours dans lâ€™optique de dÃ©coupler notre logique mÃ©tier, de mieux rÃ©utiliser notre code et en amÃ©liorer la maintenabilitÃ©. Le Domain Driven Design a donc logiquement Ã©tÃ© mis Ã  lâ€™honneur, ainsi que le typage fort pour donner du sens au code et favoriser sa bonne utilisation. Pour Ã©viter la dette technique, il faut â€œacheter en avance la capacitÃ© de changerâ€.\n\n\n\nâ€œWe got rid of managementâ€\n\nMichelle Sanver nous a prÃ©sentÃ© lâ€™holacratie : un systÃ¨me dâ€™organisation basÃ© sur lâ€™intelligence collective, utilisÃ© chez Liip. Elle a dÃ©marrÃ© la confÃ©rence en expliquant les dÃ©fauts de la hiÃ©rarchie pyramidale (la photo de la slide parle dâ€™elle-mÃªme ;-)) :\n\n\n\nEn holacratie, la sociÃ©tÃ© sâ€™organise en cercles et sous cercles de responsabilitÃ© et chaque collaborateur se voit donner des rÃ´les et les moyens de lâ€™assumer. Les dÃ©cisions sont prises collectivement et le pouvoir nâ€™est pas centralisÃ© dans les mains de quelques personnes. Tout est basÃ© sur la transparence, en particulier la rÃ©munÃ©ration et les budgets. Le but principal est dâ€™assurer des conditions de travail bienveillantes et que chacun se sente en sÃ©curitÃ© pour exprimer au mieux ses talents et rÃ©duire les tensions. Notamment, toutes les rÃ©unions sont facultatives :-D\n\nLiip a dÃ©veloppÃ© un outil communautaire en SAS (www.holaspirit.com) qui simplifie lâ€™organisation en gÃ©rant les cercles, les prises de dÃ©cisions et la remontÃ©e des propositions dâ€™amÃ©liorations (ou â€œtensionsâ€). Cette outil paraÃ®t indispensable, car central dans lâ€™organisation.\n\nOn voit trÃ¨s bien comment une holacratie peut se mettre en place dans une nouvelle organisation ou de taille rÃ©duite mais la confÃ©renciÃ¨re nâ€™aborde pas trop cet aspect pour les entreprises de taille consÃ©quente.\n\nâ€œVous nâ€™avez pas besoin de Ã§aâ€\n\nLe dÃ©but de la confÃ©rence Ã©tait volontairement critique sur lâ€™utilisation des nouvelles technos puis peu Ã  peu se dirigeait sur du bon sens dans le choix des technos avec de bonnes raisons (et non pas car elles sont â€œÃ  la modeâ€).\n\nPour cela (Charles Desneuf) nous a citÃ© les avantages et inconvÃ©nients de diffÃ©rents outils (micro-services, GraphQL, SinglePageApp, Microservices, GraphQL,â€¦)\n\nLe but Ã©tait vraiment de pousser Ã  la rÃ©flexion dans ce type de choix en prenant bien en compte le contexte (utilisateurs, Ã©quipe, qualitÃ©s attendues,â€¦) et de ne pas apporter de la complexitÃ© inutilement (optimisation/abstraction prÃ©maturÃ©e, modÃ©lisation inadaptÃ©,..).\n\nUne confÃ©rence bien dirigÃ©e en concluant par :\nâ€œVous nâ€™avez (peut-Ãªtre) pas besoin de Ã§a (maintenant).â€\n\nâ€œCessons les estimationsâ€\n\nFrÃ©dÃ©ric LeguÃ©dois nous a livrÃ© une confÃ©rence proche du one man show sur les estimations et les deadlines. Il nous a rappelÃ© un point important que lâ€™on oublie parfois : Les estimations ne sont QUE des estimations, on ne peut pas les prendre comme des engagements de la part de celui qui les fait. Ses exemples humoristiques sur les diffÃ©rentes faÃ§ons dont sont faites les estimations provoquaient frÃ©quemment lâ€™hilaritÃ© de lâ€™audience qui rÃ©pondait par des applaudissements gÃ©nÃ©reux.\n\nMÃªme si le trait Ã©tait forcÃ©ment grossi pour le Â« spectacle Â» on a passÃ© un trÃ¨s bon moment et Ã§a fait rÃ©flÃ©chir, notamment sur le fait que le changement est normal.\n\nâ€œEn vracâ€\n\nQuelques autres confÃ©rences que vous pouvez visionner sur le site de lâ€™AFUP :\n\n\n  Â« Serverless et PHP Â» : Matthieu Napoli nous a montrÃ© comment dÃ©ployer des fonctions Lambda chez AWS en PHP â€“ et pourquoi.\n  Â« DÃ©veloppeurs de jeux vidÃ©o: les rois de la combine Â» : Laurent Victorino nous a complÃ¨tement enfumÃ© avec sa prÃ©sentation interactive !\n  Â« Voyage au centre du cerveau humain, ou comment manipuler les donnÃ©es binaires Â» : un retour dâ€™expÃ©rience enrichissant de Thomas Jarrand, parlant dâ€™IRM, de binaire, ou encore de voxels.\n\n\nNous avons aussi prÃ©sentÃ© trois confÃ©rences, autour de sujets que nous pratiquons au quotidien chez M6 Web :\n\n\n  Benoit Viguier a parlÃ© de programmation asynchrone avec les gÃ©nÃ©rateurs de PHP : une fonctionnalitÃ© extrÃªmement puissante, mais encore trop peu connue. Il a dâ€™ailleurs annoncÃ© la sortie de la bibliothÃ¨que Tornado. (vidÃ©o de la confÃ©rence)\n  Guillaume Bouyge nous a racontÃ© lâ€™histoire de la migration Ã  lâ€™international de la plate-forme 6play : comment, en partant dâ€™un produit dÃ©veloppÃ© pour M6, nous sommes arrivÃ©s Ã  un produit en marque-blanche vendue Ã  dâ€™autres clients dâ€™autres pays. (vidÃ©o de la confÃ©rence)\n  Et Pascal Martin a prÃ©sentÃ© Kubernetes, lâ€™outil que nous utilisons pour piloter des conteneurs Docker dans Le Cloud. Il a enchaÃ®nÃ© avec plus de dÃ©tails sur le processus que nous suivons pour migrer nos projets vers cet hÃ©bergement ; vous pourrez en apprendre plus en lisant Le Plan Copenhague. (vidÃ©o de la confÃ©rence)\n\n\nVivement lâ€™AFUP Day Lyon 2019, oÃ¹ nous aurons sans doute le plaisir de vous rencontrer Ã  nouveau ?\n"
} ,
  
  {
    "title"    : "Generators for Asynchronous Programming: User Manual",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2018/10/26/generators-for-async-programming-user-manual.html",
    "date"     : "October 26, 2018",
    "excerpt"  : "Les gÃ©nÃ©rateurs sont souvent rÃ©duits Ã  une simplification des itÃ©rateurs, mais ils sont surtout trÃ¨s pratiques et performants pour executer des traitements asynchrones. Nous aborderons le fonctionnement dâ€™un programme asynchrone, le rÃ´le des promi...",
  "content"  : "Les gÃ©nÃ©rateurs sont souvent rÃ©duits Ã  une simplification des itÃ©rateurs, mais ils sont surtout trÃ¨s pratiques et performants pour executer des traitements asynchrones. Nous aborderons le fonctionnement dâ€™un programme asynchrone, le rÃ´le des promises, et approfondirons lâ€™utilisation des gÃ©nÃ©rateurs pour simplifier lâ€™Ã©criture de notre code. Enfin nous dÃ©taillerons des cas pratiques Â« prÃªts Ã  lâ€™emploi Â» pour tout type dâ€™application, avec un retour dâ€™expÃ©rience sur ce qui a Ã©tÃ© mis en place chez M6Web.\n"
} ,
  
  {
    "title"    : "Docker en prod ? Oui, avec Kubernetes !",
    "category" : "",
    "tags"     : " conference, php, open-source, afup, docker, kubernetes",
    "url"      : "/2018/10/26/docker-en-prod-oui-avec-kubernetes-pascal-martin.html",
    "date"     : "October 26, 2018",
    "excerpt"  : "Kubernetes. Ã€ en croire certains articles, câ€™est une solution miracle. DÃ©veloppeurs, vous avez peut-Ãªtre entendu ce motÂ ?\nCâ€™est lâ€™outil qui vous permettra de dÃ©ployer du Docker en productionÂ ! Parce quâ€™autant utiliser Docker en dev câ€™est facile, a...",
  "content"  : "Kubernetes. Ã€ en croire certains articles, câ€™est une solution miracle. DÃ©veloppeurs, vous avez peut-Ãªtre entendu ce motÂ ?\nCâ€™est lâ€™outil qui vous permettra de dÃ©ployer du Docker en productionÂ ! Parce quâ€™autant utiliser Docker en dev câ€™est facile, autant en prodâ€¦\n\nMais quâ€™est-ce que KubernetesÂ ? Quelles possibilitÃ©s si intÃ©ressantes nous fournit cet orchestrateur de conteneursÂ ?\nPods, nodes, deployments, services, ou auto-scaling et health checksÂ : autant de primitives et de fonctionnalitÃ©s que vous allez dÃ©couvrir et adorer, y compris en tant que dÃ©veloppeursÂ !\n\nAprÃ¨s avoir prÃ©sentÃ© ces bases, je vous proposerai un retour dâ€™expÃ©rience sur la migration vers Kubernetes que nous sommes en train dâ€™effectuer pour 6play.fr. Comment dÃ©veloppeurs et sysadmins se rÃ©partissent-ils les tÃ¢chesÂ ? Avons-nous dÃ» adapter nos applications PHPÂ ? Quelles difficultÃ©s avons-nous rencontrÃ©es, quels compromis avons-nous acceptÃ©s et quelle route nous reste-t-il Ã  parcourirÂ ?\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, global review",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/24/kubecon-2018-copenhaguen-global-review.html",
    "date"     : "May 24, 2018",
    "excerpt"  : "After those three days at KubeCon, weâ€™ve seen and heard a lot of interesting ideas. You can read about our first day here, about our second day there, and about our third day there. If we had to do a short recap, here are the points we would list....",
  "content"  : "After those three days at KubeCon, weâ€™ve seen and heard a lot of interesting ideas. You can read about our first day here, about our second day there, and about our third day there. If we had to do a short recap, here are the points we would list.\n\nFirst of all â€œCloud nativeâ€ seems to be the buzzword of the year. Not just cloud anymore, but cloud native! What does it mean? Instead of just deploying your application to the cloud, it should fully use the cloud.\n\nThen, Kubernetes. It is a mature solution in itself. There doesnâ€™t seem to be any doubt left about that. Even if this is the case with Kubernetes, the majority of the ecosystem around it is not mature yet, and thatâ€™s a bit of a problem. We saw a lot of tools and some that were presented during talks are still WIP and some demos completely failed. If youâ€™re surfing the Kubernetes wave, please be careful with the tools you choose, and donâ€™t loose yourself adopting a fancy/non-working tool that will bring your infrastructure down. Continue to master what you do without being trapped by the hype brought by certain solutions.\n\nDeployment, CI and CD. Well, not so much. There are a few projects out there and several different approaches (kubectl apply, a bit of Jenkins around it, deployments from inside the cluster, several black boxes like CodeFreshâ€¦), but not one thing that everyone is doing/using. We are currently looking at Jenkins-X and hope weâ€™ll be able to build our CI/CD stack using it.\n\nFor monitoring, use Prometheus. Itâ€™s pretty much what everyone is using.\n\nService mesh. One of the big things this year, everybody is talking about it, Istio seems to be taking the lead. Itâ€™s still moving fast, though and not enough people are truly mastering this in a production setup.\n\nDevelopment environment. Well, â€œLOLâ€ would do it maybe. This is clearly not a priority yet. Some teams and projects (like Telepresence) have started working on this, but there is still road ahead.\n\nGitOps. Everybody is going this way. Versioning, of course. But also using Git events to pilot things.\n\nThings are beginning to move on the security side of things. Companies are starting to notice there is work to be done, startups are appearing with different services.\n\nAnd, finally, multi-clusters. We felt a few people are using multi-cluster, but itâ€™s often done by hand. That doesnâ€™t seem mature at all. Maybe a subject weâ€™ll hear more about in the future?\n\nIn any case, we had a really great time during this KubeCon in Copenhagen, we saw many interesting talks and discussed with lots of people!\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 3",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/23/kubecon-2018-copenhaguen-day-3.html",
    "date"     : "May 23, 2018",
    "excerpt"  : "Back to KubeCon 2018 in Copenhagen, for the third and last day! You can read about our first day here and about our second day there.\n\nKeynotes\n\nCloud Native ML on Kubernetes - David Aronchick, Product Manager, Cloud AI and Co-Founder of Kubeflow,...",
  "content"  : "Back to KubeCon 2018 in Copenhagen, for the third and last day! You can read about our first day here and about our second day there.\n\nKeynotes\n\nCloud Native ML on Kubernetes - David Aronchick, Product Manager, Cloud AI and Co-Founder of Kubeflow, Google &amp;amp; Vishnu Kannan, Sr. Software Engineer, Google\n\nTrying to run Machine Learning on Kubernetes? Working with Jupyter and Tensorflow? Kuebeflow could be of interest to you.\n\nRunning with Scissors - Liz Rice, Technology Evangelist, Aqua Security\n\nEven if we never fall, running with scissors could be dangerous. We wouldnâ€™t run with scissors, would we? Then, why do we keep running containers with privileges they should not need? And why do we mount more directories than needed as volumes?\n\n\n\nChaos Engineering WG Deep Dive â€“ Sylvain Hellegouarch, ChaosIQ\n\nOne of the goals of Chaos Engineering being to break stuff, the talk started by a word about mindset: one must love supporting the team, and not try to save the day herself. Also, one must nurture empathy (including for the system), be assertive but not arrogant, not blame / be snarky (many of us are not great on that point).\n\nChaos Engineering follows a continuous loop. Start with a steady state (a baseline that comes from objective observation, which means you need business metrics and to collect data), formulate an hypothesis (not necessarily business oriented), and define an experimental method (the things we vary to prove / disprove the hypothesis). Warning: donâ€™t vary too many things at once.\n\nThe talk finished with a few words about the Chaos Toolkit, which aims to be simpler than the famous Chaos Monkey.\n\nIstio - The Weather Companyâ€™s Journey - Nick Nellis &amp;amp; Fabio Oliveira, IBM\n\nSeveral istio talks werenâ€™t enough for me, I wanted more. I didnâ€™t learn much more on this new one, except those tips:\n\n  You can define route specific retries with Istio\n  They use vistio to visualize istio traffic. That tool seems based on Netflixâ€™s Vizceral. Unfortunately, I couldnâ€™t find any GH repo nor blog talking about vistio.\n  If you want to implement Istio: start small\n\n\nAre You Ready to Be Edgy? â€” Bringing Cloud-Native Applications to the Edge of the Network - Megan Oâ€™Keefe &amp;amp; Steve Louie, Cisco\n\nThere are a few common problems in a cloud computing setting. Many devices (with IoT for instance) can mean a bottleneck on the network side of things, the cloud being far away can cause latency problems. Edge computing, moving apps (or parts of apps) from a centralized cloud, could help with those problems. Think AR/VR for latency, Video for high bandwidth, facial recognition for temporary/secure data.\n\nThe talk introduced the idea of deploying Kubernetes in edge locations â€“ like in cell towers â€“ and pointing users to the closest location. Those Kubernetes clusters would only host APIs or applications with specific needs, while the main parts would remain in a centralized cloud, which means we would have to develop edge-ready applications, keeping in mind problems such as network splits, data synchronization, deployment, â€¦\n\nIntegrating Prometheus and InfluxDB - Paul Dix, InfluxData\n\nI was a lot surprised in this talk as we were like 30 in a room for 300.\nI guess people donâ€™t need to keep metrics more than 15days with the Prometheus engine, or maybe people are already using InfluxDB as a guaranteed long term storage.\nBecause we are in the second case, we are testing influxDB with Prometheus, so this talk came at the right time.\nI didnâ€™t learn much on InfluxDB + Prometheus, nor on federated queries that comes with HA.\n\nPaul then questioned why not use a single query language for all query engines? That would be more practical and maintainable. The question is still open, even if Paul proposes IFQL to rule them all. The main idea is for all query engines to coordinate and stop creating a new language on each new engine.\n\nA Hackers Guide to Kubernetes and the Cloud - Rory McCune, NCC Group PLC\n\nFirst, a word about threat models. Pretty much everyone will see random Internet hackers looking for easy preys. Some of us will be specifically targeted by attackers. Only a few will be targeted by nation states. You should think about your threat model, which may or may not be the same as your neighbourâ€™s.\n\nThen, time to think about your attack surface. Attackers will find the weakest point, which is not always where your might think. What about the cloud around your Kubernetes cluster? Github is a great way of getting accesses (many commit their credentials and/or do not remove them from history â€“ bots are crawling this!). Developersâ€™ laptop are generally full of interesting data, and are not necessarily protected enough.\n\nOn Kubernetes, external attackers will try to access the API server and etcd, the kubelets, or maybe inject malicious containers. You should turn off the insecure port, control access to kubelet and etcd, retrict the use of service tokens, restrict privileged containers, enable authentication and authorization on the API server, set pod security and network policies, and do regular upgrades. Also, donâ€™t forget about cloud permissions.\n\nCloudbursting with Kubernetes - Irfan Ur Rehman &amp;amp; Quinton Hoole, Huawei Technologies\n\nThat might not be everyoneâ€™s problem, but still interesting to hear about. If you have multiple cloud providers with different pricings, you might want to optimize your costs by using the most expensive only on load peaks. That is exactly what they did, using Kubernetes clusters federation and specific annotations. We wonâ€™t go over this approach because weâ€™ll stick with one cloud provider, but that may be interesting for some people.\n\nOperating a Global-Scale FaaS on Top of Kubernetes - Chad Arimura &amp;amp; Matt Stephenson, Oracle\n\nThis was about the Fn project. A couples of problems related to multitenancy: network isolation on Kubernetes, noisy neighbors (I/O being the bottleneck). Helm has a few limits, worked around with shell scripts.\n\nInside Kubernetes Resource Management (QoS) â€“ Mechanics and Lessons from the Field - Michael Gasch, VMware\n\nResource management goes through cgroups. With containers, we see all the CPU/RAM, but this doesnâ€™t mean weâ€™ll be able to use them all: we may have to share with other containers. Works with requests. For now, cpu and memory are stable resources, but others (hugepages, ephemeral storage, device plugins) are in beta. You should align Kubernetesâ€™ QoS with the underlying infrastructure, enable quotas in the cluster, and protect critical system pods.\n\nI have to admit I didnâ€™t take much notes during this talk, but noted the slides contain a lot of informations â€“ for more, go read them ;-)\n\nObserving and Troubleshooting your Microservices with Istio - Isaiah Snell-feikema, IBM &amp;amp; Douglas Reid, Google\n\nI promise, this is the last Istio conf I went to.\nIn case that wasnâ€™t obvious, Istio is becoming the default service mesh, like Prometheus is for metrics. I couldnâ€™t work much on it so I wanted to learn the most possible from it during this KubeCon. I can say this talk was one of the best for Istio discovery and even advanced skill. I wonâ€™t be able to summarize everything, so here are few tips I kept from it:\n\n  Envoyâ€™s /stats route gives a lot of infos of servers\n  Istio system logs gives also traffic spikes\n  Istioâ€™s access logs can be uploaded to fluentd/elk\n  Canary testing / blue-green deployments can be done via RouteRule CRD.\n\n\nIf you are considering using Istio, you must see the slides\n\nVitess and a Kubernetes Operator - Sugu Sougoumarane, YouTube\n\nI heard about Vitess for the first time during this KubeCon, even though itâ€™s an old project. Itâ€™s a middleware for MySQL, sitting between the database and our applications. It helps scale it through sharding â€“ the goal being to answer the common pain points for databases: scalability, cloud and making DBAs happy. Itâ€™s also one of the CNCF project I noted a few days ago I should take a closer look at.\n\nFinal words?\n\nThe weather was nice and our plane was only on Saturday, so we finished the day with a walk in the City.\n\n\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 2",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/22/kubecon-2018-copenhaguen-day-2.html",
    "date"     : "May 22, 2018",
    "excerpt"  : "After an interesting first day at KubeCon 2018, we are back for the second day!\n\nAutoscale your Kubernetes Workload with Prometheus - Frederic Branczyk, CoreOS\n\nThe goal of autoscaling, ultimately, is to fullfill SLO of SLA through SLI (yeah, you ...",
  "content"  : "After an interesting first day at KubeCon 2018, we are back for the second day!\n\nAutoscale your Kubernetes Workload with Prometheus - Frederic Branczyk, CoreOS\n\nThe goal of autoscaling, ultimately, is to fullfill SLO of SLA through SLI (yeah, you may have to think for a while after reading this ^^). Demand must be measured by metrics, which must themselves be collected, stored and made queryable. Autoscaling can be horizontal (increase replicas when necessary, the focus of this talk) or vertical (increase resources request/limits when necessary).\n\nAutoscaling on Kubernetes used to rely on Heapster. But Heapsterâ€™s API is unspecified, it doesnâ€™t work with Prometheus, and vendor implementations are often unmaintained. Starting with 1.8, Kubernetes provides a resource and custom metrics API. It is a specification (not an implementation), the implementations are developed and maintained by vendors, and each metric returns a single value. Kubernetesâ€™ HPA (HorizontalPodAutoscaler) uses these metrics to scale up/down.\n\nCore metrics are CPU and RAM, by pod/container/node, and there is a canonical implementation called metrics-server. The custom metrics API has the same semantics (a single value is returned) but no canonical implementation is provided (take a look at DirectXMan12/k8s-prometheus-adapter for an implementation). Each metric is related to a Kubernetes object (Pod, Service, Deployment, â€¦). Finally, there is also an external metrics API (currently in alpha stage) for things not related to a Kubernetes object (like the queue length for a queuing service provided by a Cloud provider â€“ SQS on AWS, for example).\n\nPod Anomaly Detection and Eviction using Prometheus Metrics - David Benque &amp;amp; Cedric Lamoriniere, Amadeus\n\nThis talk started with a reminder: stability is hard, especially for a distributed system. And using load-balancers doesnâ€™t help at all, on the contrary. Solutions include proximity-based load-balancing (sharding by Availability Zone, which is not something Kubernetes does natively, but for which Istio can help with its pilot-agent proxy as it is AZ-aware). Using healthchecks (liveness kills the container when it fails, readiness only removes it temporarily from service discovery when it fails) is a good idea, but you should keep probes simple (complexity = bugs) and you shouldnâ€™t check external dependencies. Also, donâ€™t forget about circuit breakers and retries. But note all this suffers from limitations, as itâ€™s only based on technical signals and depends on local decisions (pods/containers, service mesh/proxy).\n\nThis conference was about a tool called Kubervisor, which aims to detect mis-behaving Pods and remove them from the cluster (switching a label on the corresponding service). The decisions are based on metrics (using PromQL for Prometheus metrics) that can include business metrics, and not only technical ones nor data limited to a specific pod.\n\nI didnâ€™t write much about this, I was too busy watching the demo, which was really interesting. In the end, I wrote down I should (and I probably will) take a better look at this in a few months, when we have deployed a few more pieces of software to Kubernetes.\n\nChallenges to Writing Cloud Native Applications - Vallery Lancey, Checkfront\n\nBuilding and deploying an application to the cloud has advantages: automatic scaling, load-balancing, replication, infrastructure provisioning and teardown is done for us, â€¦ It also has challenges.\n\nStorage of persistent data is one of these challenges (simple replication is rarely a good idea, working with unreplicated shards is a common pattern, using multiple volatile copies is a good strategy, but the best approach is runtime data replication â€“ which requires a large setup and implies non-negligible maintenance costs). Services coupling (database + services interacting with it) and internal API calls (source of delays) are also common sources of troubles, even if the second can be mitigated with simpler API actions, endpoints for specific actions (not CRUD!), batch endpoints and caching. Testing is a huge concern, especially with microservices, and working with discrete compoments helps, as we can run one service and mock the others. Finally, local development is not a solved problem yet (in any case, you should remove the â€œbuild an imageâ€ step).\n\nGitOps for Istio - Manage Istio Config like Code - Varun Talwar, Stealth Startup &amp;amp; Alexis Richardson, WeaveWorks\n\nDuring this talk, WeaveWorks team talked about how Istio config can be managed like code through git based workflows.\nThey evoked using Terraform to describe cloud state.\nAs of gitops principles, devs shouldnâ€™t use kubectl to interact with clusters. Additionally, they should push code, not containers. GitHub events must lead deployments, not humans.\nAs part of this automation, deployments must auto-rollout when things break. They either fail or succeed cleanly.\nOne can use operator patterns to help integrating those concepts. The WeaveWorks team also talked about flux to manage environments states.\nWeave Flux brings a lot more annotations for Istio, making automated releases deployments, etc.\n\nOPA: The Cloud Native Policy Engine - Torin Sandall, Styra\n\nPolicy enforcement is a fundamental problem for an organisation, and policy decisions should be decoupled from policy enforcement. Open Policy Agent is an open-source general-purpose policy engine. It uses a high-level declarative language, can be used to implement RBAC and has integrations with Istio or Terraform. This is not my current priority, but it could be worth taking a look at OPA if you need to add policy enforcement to your application.\n\nKubernetes Multi-Cluster Operations without Federation - Rob Szumski, CoreOS\n\nA lot of people are using multiple Kubernetes clusters. For example, Zalando uses 80. It can become a mess to manage all those clusters with their specific components like secrets, controllers, configmaps, etc.\nTo solve this problem, a new cli has been created: kubefed.\nBut Rob explains that this new tool doesnâ€™t solve all the problems. I.E: Youâ€™ll have to give access to all-clusters to people and not only few clusters (that breaks isolation), the Federation API must be run by a top-root user (accessing everything), etc.\n\nCoreOs brought the concept of k8s operators.\nRob explains why that solves problems and why you should use that instead of Federation.\n\nClearly I wasnâ€™t convinced at all by this presentation. Besides, we are working in the same building and problems brought by Rob (modifications by hundred of devs/SRE split across the world) do not concern us at the moment.\n\nBuilding a Kubernetes Scheduler using Custom Metrics - Mateo Burillo, Sysdig\n\nThere are so many possibilities when scheduling pods. The scheduler first applies filters (resource requests, volumes, selectors/taints), then ranks (including the default behavior of spreading pods of the same service), then goes to applying hard constraints (taints, node selector), and soft contraints (prefer no schedule, node affinity, pod affinity, weight) and pod priority and does taint-based evictions. To understand what actually goes on and prevent complex situations, you should only add important constraints.\n\nIn some cases, you might want to build a custom scheduler, using custom metrics (when the default scheduler is not good enough for you and/or you have very specific needs). An example, based on sysdig metrics: draios/kubernetes-scheduler. And more informations in this blog-post. Remember creating a scheduler is not an easy task and many things can go wrong (think about concurrency and race conditions).\n\nIn the end, the idea of implementing a custom scheduler might be interesting, but a bit scary: messing things up could mean no pod getting scheduled, which is not a nice scenario. Iâ€™m not sure I currently see a situation in which Iâ€™d go this wayâ€¦\n\nClusters as Cattle: How to Seamlessly Migrate Apps across Kubernetes Clusters - Andy Goldstein, Heptio\n\nAs many people, Andy has a lot of clusters. To re-route traffic between clusters, he uses Envoy.\nTo maintain consistent configurations, he uses Ansible to provision everything.\nSo far, I donâ€™t really see the point of having a lot of clusters and even less of migrating a single app between clusters, but that can be interesting for people who like that trend.\n\nParty, Tivoli Gardens\n\nFor the evening, we went to an all-attendees party at Tivoli Gardens, an amusement park and pleasure garden right in the middle of Copenhagen. We walked around for a bit, before settling for a beer and a few snacks, talking with other French Kubenetes fans.\n\n\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 1",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/21/kubecon-2018-copenhaguen-day-1.html",
    "date"     : "May 21, 2018",
    "excerpt"  : "At the very beginning of May, we (Pascal and Vincent) went to KubeCon 2018. It was a three-days long event, with more than 300 conferences. 4300 people met at Bella Center, a huge conference place close to Copenhagen in Denmark. Here are some of o...",
  "content"  : "At the very beginning of May, we (Pascal and Vincent) went to KubeCon 2018. It was a three-days long event, with more than 300 conferences. 4300 people met at Bella Center, a huge conference place close to Copenhagen in Denmark. Here are some of our notes about some talks we saw.\n\nKeynotes\n\nCNCF Project Update - Liz Rice, Technology Evangelist, Aqua Security; Sugu Sougoumarane, CTO, PlanetScale Data; Colin Sullivan, Product Manager, Synadia Communications, Inc. &amp;amp; Andrew Jessup, Co-founder, Scytale Inc.\n\nDuring this keynote, I realized the CNCF is helping a lot more projects than I thought it was: itâ€™s not just Kubernetes. I will take a closer look to some of them in the future â€“ they are all listed on l.cncf.io.\n\nCERN Experiences with Multi-Cloud Federated Kubernetes - Ricardo Rocha, Staff Member, CERN &amp;amp; Clenimar Filemon, Software Engineer, Federal University of Campina Grande\n\nOK, so, sometimes, it actually is rocket-science (or pretty close to it). Itâ€™s nice seeing Kubernetes used for science and research, on a federation of around 400 clusters!\n\n\n\nWhats Up With All The Different Container Runtimes? - Ricardo Aravena, Branch Metrics\n\nOverview of the different containers runtimes, starting with OpenVZ in 2006 (still maintained, but the last 2.7 version is not as stable as the previous one, and it doesnâ€™t support Kubernetes), and LXC (Kubernetes support is low-priority WIP and LXC uses a specific image format) in 2011. Docker (initially based on LXC) arrived in 2013 and things have gone insane since then. With libcontainer and rkt in 2014, OCI in 2015 and CRI in 2016. Today, Kubernetes supports several runtimes.\n\nrkt could be interesting from a security point of view (supports TPM and VMs). With Kubernetes 1.10, the default runtime is runc. crun is the most interesting runtime for performances, but is WIP and isnâ€™t currently used much. Kata, released in 2018 has the best security (it runs containers in VMs) and is OCI compliant, but is slower and more heavyweight. Other very specific runtimes include nvidia, railcar, pouch, lmctfy, systemd-nspawnâ€¦\n\nBasically, today and for most workloads, you should go with the standard docker/containerd runtime. There is a convergence towards OCI, itâ€™s the default for Kubernetes and docker is going to adopt the CRI plugin.\n\nIntroduction to Istio Configuration - Joy Zhang, Google\n\nAn introduction to the Istio Service Mesh. All Istio components are CRDs. CRDs are becoming a standard when it comes to kubernetes customizations, here requests proxying.\n\nThis talk described Istio components, notably:\n\n\n  Mesh config - global Istio config\n  Service config - Istio operators config\n  Consumer config - overrides config model\n  Galley - Istio config per cluster\n\n\nFYI: you need as much Galleys as you have clusters + environments.\n\nContinuous Delivery Meets Custom Kubernetes Controller: A Declarative Configuration Approach to CI/CD - Suneeta Mall &amp;amp; Simon Cochrane, Nearmap\n\nKubernetes is great but, CI/CD is not really its job â€“ and CI/CD depends a lot on the company you work at and its culture. Here, they started deploying their applications with kubectl and YAML files, and even if CI usually doesnâ€™t cause much troubles, CD is not that easy. And using a managed CI (CircleCI, shippable, AWS Codepipelineâ€¦) means exposing Kubernetesâ€™s control plane on the Internet, which is not great. Also, the Gitops approach with its git is the source of truth mindset is OK, but committing version numbers again and again pollutes the history with a lot of noise â€“ when this history could be kept in the cluster itself.\n\nSo, they went with some kind of CD lite: a service running in the cluster, which monitors the registry and deploys the new version of an image when it sees it. This means the cluster doesnâ€™t need to access git and the CI chain doesnâ€™t need to access the cluster, making the configuration simpler.\n\nThe tool they developed for this is nearmap/cvmanager. It seems relatively easy to install and configure. And I like the idea of having a gap between Git/CI and CD. I may have to test this, especially to see what it can do when it comes to canary or blue/green deployments, but this is one of the things I saw at KubeCon I will discuss with my colleagues!\n\nPractical and Useful Latency Analysis using Istio and OpenCensus - Varun Talwar, Stealth Startup &amp;amp; Morgan McLean, Google\n\nOpenCensus (distributed tracing metrics system) + Istio is the combo provided for this talk to let devs debug the most of their apps.\n\nOpenCensus is a tracing tool, like the CNCFâ€™s project Opentracing. It can be implemented in many languages, starting with those we use: PHP and javascript.\n\nOpenCensus can trace RPC and http APIs calls.\n\nYou can install a dedicated dashboard that gives a lot of metrics out of the box (like 90th percentiles, etc.) and allows customizable ones.\n\nMixer (Istio tool) makes the aggregation between those metrics from what is gathered by OpenCensus.\n\nThis definitely need to be tested, but not sure it is worth migrating from OpenTracing to OpenCensus.\n\nHabitat Builder: Cloud Native Application Automation - Elliott Davis &amp;amp; Fletcher Nichol, Chef.io (Habitat.sh)\n\nI had never heard of Habitat before, so I was kind of curious what this was about. This idea of platform-independent build artifacts (with exporters for docker, Kubernetes, Helm) could be interesting for some teams, but itâ€™s not a need I currently have. The automated deployments might be interesting though, but we are already looking at other tools.\n\nKubernetes and Taxes: Lessons Learned at the Norwegian Tax Administration - A Production Case Study - Bjarte S. Karlsen, The Norwegian Tax Administration\n\nReally nice production case study from the Norwegian Tax Administration about their k8s platform. They are currently using Rancher 2.0 + OpenShift + CodeFresh on top of k8s. All their Docker images are alpine based. They develop Java applications.\n\nOne idea kept my attention is the tagging of their containers:\n\n\n  Pushing docker image tag 1.2.3 also pushes tag 1.2 and tag 1.\n  So going to v2 and rolling back to v1 effectively rolls out 2.0.0 to 1.2.3 without knowing the exact subversions.\n  You are assured that the tag of the major version always points to the latest subversion.\n\n\nItâ€™s not clear to me how to implement that. Maybe a codefresh hack. But still, I found the approach interesting.\n\nAnother lesson for those who want to migrate from on-premise to the cloud is to keep things that work on-premise and simply migrate them on cloud as it is. That will ease the migration. You can rethink all afterward if needed, but the first step is to migrate, not rebuild from scratch plus migrating.\n\nAs they use CodeFresh for CI/CD, itâ€™s easy for them to automate their pipelines. Thatâ€™s another lesson: automate everything. To automate, you need to standardize.\n\nAnother lesson is to use what is rock-solid. We all see a lot of tools and startups around the cloud nebula. A lot of them wonâ€™t last and some will, like Kubernetes. This is the tip: use what will last. Donâ€™t build your whole infrastructure on something unstable or with poor pro/community support.\n\nThe final point I kept from this really good rex, is to create a predictable infrastructure. You cannot guess what will happen. You have to know and use the right tools/annotations to make it behave the way you want, predictably and repeatedly.\n\nHow to Export Prometheus Metrics from Just About Anything - Matt Layher, DigitalOcean\n\nThis presentation was about a few good practices to follow when it comes to exposing Prometheus metrics from a Go application. Basically, you should use the Go client library, be really careful about concurrency, build reusable packages, write unit-tests, use promtool check-metrics, and read and follow the Prometheus metrics best practices.\n\nContinuously Deliver your Kubernetes Infrastructure - Mikkel Larsen, Zalando SE\n\nAnother really good rex from Zalando from their utilization of k8s in prod and lessons learned.\n\nThey talked of Stups, a Zalando toolset around AWS. That definitely needs to be tested.\n\nFrom their experience of managing a k8s cluster on AWS EC2s, they gave us few tips:\n\n\n  Always upgrade to the latest k8s version\n  Manage the smallest possible number of clusters\n  Automate all the things. The only manual step should be merging PRs. This is a base GitOps principle.\n  Define an AWS HA control plane setup behind ELBs. That can be debated but this is a good first step.\n  All cluster config files must be git versioned (another GitOps principle). An upgrade is then only a git branch merge at some point.\n\n\nSome of the points above can be achieved via a CD tool. I remember they use Jenkins for that, but not 100% sure. Alongside this CD tool, there should be a CI tool (or one tool for both).\n\nThey gave us some points on CI tests too:\n\n\n  Run e2e conformance tests for k8s config files\n  Run statefulSet tests\n  Run any additional homemade tests\n\n\nFor those who are using AWS, keep in mind the following: volumes cannot be mounted across several AZ.\n\nKeep yourself away from unavailability by always setting minAvailable.\n\nFinal tip: If you go for a self-managed k8s cluster (not EKS, GKE, etc.), check that nodes are up and running before continuing upgrade.\n\nI really enjoyed this rex that was full of good prod-ready advices.\n\nI recommend you take a look at the slides\n\nSeamless Development Environments on Kubernetes using Telepresence - Ara Pulido, Bitnami\n\nKubernetes is a great production environment, but it feels like development environment is kind of an afterthought: even for a simple application, if you want to develop (locally?), things are not easy. People are currently using two distinct ways: using docker compose to replicate the production environment (but compose doesnâ€™t do everything: rbac, job, ingressâ€¦ and having to maintain everything twice is not fun), or build/push/deploy-to-k8s and wait many seconds everytime one wants to F5 on a page, which is unberably slow (I wouldnâ€™t ask my developer colleagues to do this for even half a day!).\n\nThe solution proposed during this conference is Telepresence. It allows a developer to swap out a pod from a cluster and inject her own pod, running locally, at its place. Some sort of VPN is established between her computer and the cluster, which means the pod running locally behaves just like if it was still in the cluster (including DNS, service discovery, access to non-Kubernetes managed services and all).\n\nThere are still limitations and constraints (if two developers want to work on the same service, theyâ€™ll each need their own namespace in the cluster, as two people cannot swap out the same pod), but plans for this project are interesting and I will definitely take a closer look at it in a couple of months, when I start thinking more about our development stack!\n\nWe went to Datawireâ€™s booth and saw a nice demo. And also learnt about other tools, such as Forge and Ambassador that can duplicate production requests to a local pod. We found that this feature is ultra dope!\n\nPerformance and Scale @ Istio Service Mesh - Fawad Khaliq, VMware Inc, Laurent Demailly, Google &amp;amp; Surya V Duggirala, IBM\n\nReturn of Istio devs on projectâ€™s recent updates: closed PRs, enhancements, etc.\n\nThat was not really what I was looking for so I went to some bootcamps to say hi, especially the HAProxy bootcamp one that is always a good moment. Special thanks to Baptiste for his time and the awesome talk we had!\nFor the record: HAProxy has itâ€™s own Ingress Controller\n\nFrom Data Centers to Cloud Native - Dave Zolotusky &amp;amp; James Wen, Spotify\n\nThis last conference of the first day was about Spotifyâ€™s migration from their on-premise datacenters to the cloud. For many years, they were doing everything on-prem (including a 3000 nodes Hadoop cluster â€“ the largest in Europe, at the time), often developing their own proprietary software (like custom monitoring, proprietary messaging framework, custom Java service framework, custom container orchestrator, â€¦ Some have been open-sourced). The first step for them has been to get out of their own datacenters, moving everything to another datacenter (but still using their proprietary stuff). It took them three years and a half, trying to make this migration as seamless as possible for the development teams.\n\nNext step is to become cloud native, especially moving to Kubernetes. They did this in several steps, starting small by sending production traffic to one service deployed to one cluster for one hour (allowed them to validate DNS, logging, service discovery, metrics system, networking). Then, three services on one cluster (permissions, namespaces, quotas for each namespace, developers documentation). After that, services on a volunteer basis (clusters, scripted clusters creation, secrets, deployment tooling based on a wrapper around kubectl, CI integration =&amp;gt; a lot of learning for a lot of people). Then, two high-traffic services, including a service receiving 1.5 million requests per second (horizontal auto-scaling, network setup, confidence, reference for other projects). And, finally, self-service migration, with teams migrating when they want, following the docs, and ops not always knowing whatâ€™s running in the cluster (reliability, alerts, on-call, disaster recovery, backups, sustainable deploy). Everything going pretty much fine by now, itâ€™s time to investigate on a few odd things and specific needs, with a temporary ops team assembled to help.\n\nThe most important idea here is you donâ€™t have to do everything right from the start. For example, they waited quite a long time before setting up a sustainable deployment method, which might seem odd to many of us. But it allowed them to move forward and validate a lot of things one after the other. Thatâ€™s something I will keep in mind: if it worked for them (4000 employees, including 500 techies), it could work for many other companies!\n\nJenkins X: Easy CI/CD for Kubernetes - James Strachan, CloudBees\n\nThis might be one of the hottest project of this early 2018.\n\nWe already saw this project that was created in February, and we are using it for testing purposes. We hope to use it in production very soon.\n\nFor those of you who donâ€™t know Jenkins-X:\n\n\n  Itâ€™s piloted by jx, a command line tool (Mac/Linux)\n  It drives a Jenkins instance + Docker Registry + Nexus + Chartmuseum + Monocular\n  It allows you to manage your appâ€™s deployments via Jenkins blueoceanâ€™s pipelines with k8s endpoints\n  That means Jenkins will be able to run CI tests, Continuously Deploy your project to preview, staging, prod and so on with Skaffold/Helm to k8s\n  Jenkins will run pipelines from the Jenkinsfile in the repo to do that CI/CD part\n  In the provided pipelines given with jx import, you will use provided docker images that embed jx cli and other tools to manage the deployments of your app.\n  That allows you to promote your app between stages, build your docker image, etc. in your pipeline steps.\n  Those deployments are based on Helm Charts in the repo.\n  Jenkins-x follows GitOps strategy, that means anything useful is stored in each appâ€™s repo: it is versioned and git events will trigger pipelines.\n\n\nJenkins-X brings this CI/CD part that was missing for k8s users. Gitlab + gitlab-ci were already doing that for some years now, but nothing was that fancy for GH users.\n\nWe are very excited about Jenkins-X, as it answers a lot of problematics and brings in gitops as core concept. Weâ€™re actively adopting it and we hope to give feedback asap.\n\nKeynote: Anatomy of a Production Kubernetes Outage - Oliver Beattie, Head of Engineering, Monzo Bank\n\nThis keynote was about  major outage at Monzo. You can read more about it in the post-mortem they posted after it happened. Basically, even when you are careful, an outage can still happen: several causes combined with a very specific bug happening with specific versions in a specific case and voilÃ . Nice talk, and nice to hear a bank being so open!\n\nKeynote: Prometheus 2.0 â€“ The Next Scale of Cloud Native Monitoring - Fabian Reinartz, Software Engineer, Google\n\nPrometheus is the monitoring stack everyone seems to be using now. This keynote presented how much faster Prometheus 2.x is, compared to Prometheus 1.x. Having never used the 1.x versions, I have to admit I never suffered from it. It is still nice noting 2.x scales much better (requires less RAM/CPU and its performances donâ€™t degrade much with a huge number of metrics).\n\nWelcome reception\n\nThis first day ended with a nice buffet at Bella Center, next to the sponsor booths. As we each one went to see different talks, it allowed us to chat about what we saw and heard, even if we didnâ€™t stick around too long, after such a long day â€“ especially knowing there would be two more just after!\n\n\n"
} ,
  
  {
    "title"    : "PHP Tour Montpellier 2018",
    "category" : "",
    "tags"     : " phptour, php, afup, 2018",
    "url"      : "/2018/05/17/retour-php-tour-2018.html",
    "date"     : "May 17, 2018",
    "excerpt"  : "Cette annÃ©e encore, M6Web a sponsorisÃ© le PHP Tour, organisÃ© cette annÃ©e par lâ€™AFUP Ã  Montpellier.\nNous Ã©tions donc nombreux pour assister Ã  lâ€™ensemble des confÃ©rences. Comme dâ€™habitude avec lâ€™AFUP, les confÃ©rences Ã©taient de bonne qualitÃ©, et il ...",
  "content"  : "Cette annÃ©e encore, M6Web a sponsorisÃ© le PHP Tour, organisÃ© cette annÃ©e par lâ€™AFUP Ã  Montpellier.\nNous Ã©tions donc nombreux pour assister Ã  lâ€™ensemble des confÃ©rences. Comme dâ€™habitude avec lâ€™AFUP, les confÃ©rences Ã©taient de bonne qualitÃ©, et il y en avait pour tous : dÃ©butants comme utilisateurs avancÃ©s.\n\nPour la premiÃ¨re fois, les confÃ©rences Ã©taient donnÃ©es dans un cinÃ©ma Gaumont. Un trÃ¨s bon choix en termes de configuration : visibilitÃ©, confort, son et lumiÃ¨re !\n\nEn attendant la mise en ligne des vidÃ©os, nous remercions les confÃ©renciÃ¨res et les confÃ©renciers pour leurs prÃ©sentations. Vous trouverez ci-dessous quelques mots sur les confÃ©rences que nous avons particuliÃ¨rement apprÃ©ciÃ©es.\n\nâ€œTirer le maximum du moteur PHP7â€\n\nConfÃ©rence donnÃ©e par Nicolas Grekas.\n\nLâ€™approche de Nicolas Ã©tait trÃ¨s intÃ©ressante et nous Ã  permis de mieux comprendre le fonctionnement interne de Symfony, en lien avec les optimisations apportÃ©es par le nouveau moteur de PHP7.\n\nLes exemples citÃ©s nous ont permis de voir quâ€™avec quelques â€œtipsâ€, il est possible de â€œbypasserâ€ des Ã©tapes coÃ»teuses lors de lâ€™exÃ©cution de notre code.\n\nâ€œ100% asynchrone - 0% callback en PHPâ€\n\nUne prÃ©sentation de Joel Wurtz.\n\nCette confÃ©rence nous a permis dâ€™aborder un sujet assez peu connu dans lâ€™univers PHP : lâ€™asynchrone.\n\nDÃ¨s quâ€™un projet commence Ã  Ãªtre complexe, il est souvent possible de rÃ©aliser des tÃ¢ches en parallÃ¨le, non bloquantes, permettant dâ€™optimiser les temps de rÃ©ponse.\n\nPour rÃ©pondre Ã  ce besoin, Joel nous a prÃ©sentÃ© le concept de lâ€™asynchrone : l&#39;event loop.\nVia cette boucle, Joel nous a expliquÃ© comment les Ã©vÃ¨nements sont â€œdispatchÃ©sâ€ au travers de gÃ©nÃ©rateurs.\n\nPour aller plus loin, Joel nous a aussi parlÃ© des outils existants qui implÃ©mentent cette logique dâ€™event loop : AMP.\n\nEnfin pour terminer, pour Ãªtre 0% callback, Joel nous a prÃ©sentÃ© Fiber. Cette extension implÃ©mente la RFC Fiber actuellement en cours dâ€™homologation.\n\nNous vous recommandons de creuser ce sujet, qui selons nous, ouvre de belles perspectives dans lâ€™univers PHP !\n\nâ€œBienvenue dans la matrice !â€\n\nCette confÃ©rence Ã©tait animÃ©e par Benoit Jacquemont.\n\nAujourdâ€™hui encore, les dÃ©veloppeurs ont trop peu de connaissance sur ce quâ€™il se passe Ã  bas niveau sur nos serveurs.\n\nCette confÃ©rence, qui prÃ©sentait notamment strace (pour suivre les appels systÃ¨me) et ltrace (pour suivre les appels aux fonctions de bibliothÃ¨ques), Ã©tait donc particuliÃ¨rement rafraÃ®chissante. La dÃ©mo â€œcomment voir les requÃªtes et rÃ©ponse en HTTPS, en clairâ€, Ã©tait complÃ¨tement bluffante !\n\nâ€œSans documentation, la fonctionnalitÃ© nâ€™existe pas !â€\n\nCe talk Ã©tait proposÃ© par Sarah HaÃ¯m-Lubczanski.\n\nTout le monde, dans sa vie de dÃ©veloppeur, a Ã©tÃ© confrontÃ© au problÃ¨me suivant : Ã©crire la documentation des fonctionnalitÃ©s dÃ©veloppÃ©es. \nCâ€™est un challenge auquel nous nous sommes nous-mÃªme confrontÃ©s lorsque nous avons travaillÃ©, lâ€™annÃ©e derniÃ¨re, sur lâ€™internationalisation de notre plate-forme, puisque nous avons dÃ» documenter nos API, dÃ©sormais appelÃ©es par des collÃ¨gues basÃ©s dans dâ€™autres pays.\n\nSarah nous a montrÃ© comment faire face Ã  cette barriÃ¨re souvent perÃ§ue comme insurmontable par bon nombre dâ€™entre nous.\nElle nous a pour cela donnÃ© les clÃ©s et les bonnes pratiques pour crÃ©er, maintenir et rÃ©diger une documentation cohÃ©rente.\n\nOn retiendra aussi la prÃ©sentation des diffÃ©rents outils open source de gestion de documentation.\n\nâ€œA la dÃ©couverte du Workflowâ€\n\nConfÃ©rence animÃ©e par Gregoire Pineau.\n\nCette confÃ©rence a retenu notre attention. ParticuliÃ¨rement bien faite, elle rÃ©sume les fonctionnalitÃ©s de ce nouveau composant de Symfony, en partant dâ€™un workflow simple jusquâ€™au rÃ©seau de PÃ©tri. GrÃ©goire donnait des exemples dâ€™utilisations concrÃ¨tes.\n\nDifficile Ã  rÃ©sumer, je vous invite Ã  consulter la documentation Symfony sur ce composant.\n\nMais encore ?\n\nDe plus, trois confÃ©rences ont attirÃ© notre attention de par leur valeur pÃ©dagogique. Elles Ã©taient Ã  nos yeux particuliÃ¨rement  intÃ©ressantes pour des dÃ©butants ou des personnes ne connaissant pas encore le fonctionnement de certains processus suivis par notre communautÃ©.\n\nDans lâ€™ordre, vous trouverez :\n\n\n  IT figures par Sara Golemon, qui revient sur ce quâ€™est le FIG, organisme important qui rÃ©git aujourdâ€™hui une partie de lâ€™organisation de la communautÃ© PHP, et sur ce que sont les PSRs.\n  Nommer les choses ? Oui : avec le DNS par Julien Pauli. Cette confÃ©rence revient sur les bases du fonctionnement du DNS et son utilitÃ©.\n  Et, pour finir : Caching with PSRs par Hannes Van De Vreken. DerniÃ¨re des confÃ©rences â€œÃ  voir une foisâ€, celle-ci revient sur ce quâ€™est le cache en gÃ©nÃ©ral, pourquoi on en utilise. Puis sâ€™intÃ©resse au cache applicatif via les PSRs.\n\n\nLe dernier PHP Tour\n\nCe PHP Tour Ã©tait le dernier, puisque lâ€™AFUP proposera Ã  partir de 2019 un nouveau format pour les Ã©vÃ©nements en rÃ©gion : lâ€™AFUP Day. Nous aurons grand plaisir Ã  vous y rencontrer Ã  nouveau, Ã  Lyon cette fois-ci !\n\nEncore un grand merci Ã  lâ€™AFUP !\n\nEnfin, retrouvez toute lâ€™actualitÃ© de lâ€™Ã©vÃ©nement sur #phptour.\n"
} ,
  
  {
    "title"    : "How a fullscreen video mode ended up implementing React Native Portals?",
    "category" : "6play",
    "tags"     : " React, ReactNative, mobile",
    "url"      : "/6play/2018/04/15/how-a-fullscreen-video-mode-ended-up-implementing-react-native-portals.html",
    "date"     : "April 15, 2018",
    "excerpt"  : "This story introduces a declarative native side portal implementation module called rn-reparentable.\n\nMy teammate (Laetitia BONANNI) and I are working on a React Native module embedded in the 6Play application that aims to provide best moments of ...",
  "content"  : "This story introduces a declarative native side portal implementation module called rn-reparentable.\n\nMy teammate (Laetitia BONANNI) and I are working on a React Native module embedded in the 6Play application that aims to provide best moments of different TV shows from the M6 channel.\n\nThe module, called Refresh, is a list of videos that are playing while the user is scrolling. It also provides a â€œtheater modeâ€ which is a way to create an immersive user experience by obscuring the cards that are not focused:\n\n\n\nAs any other video application, it provides a fullscreen experience to the user by rotating the device.\n\nAt the time Iâ€™m writing this article, creating such a thing using React Native is a pain. Hereâ€™s the story why.\n\nCreating a fullscreen, the web developer way\n\nWhile being a web developer, we usually work with positioning to display something over the rest (like a popup for example).\n\nDealing with React Native and its style APIs (which really looks like the web one), we thought that it would be super-easy to simulate the exact same behaviour.\n\nThatâ€™s why our main idea was to manage the fullscreen mode by adding a style that takes the screen size and an absolute position. Thus, the video would have followed the device edges while rotating:\n\n\n\nReact Native styles are not the same as the web ones\n\nThe idea of creating an almost equivalent API as the web one is really good for the learning curve of React Native. It is a real asset when you want to create simple user interfaces.\n\nBut there is a drawback. This approach makes us want to get the exact same result as we would have on the web.\n\nIn our case, the use of absolute positioning was sadly not working. In fact, it is written in the React Native documentation:\n\nPosition\n\nposition in React Native is similar to regular CSS, but everything is set to relative by default, so absolute positioning is always just relative to the parent.\n\nIf you want to position a child using specific numbers of logical pixels relative to its parent, set the child to have absolute position.\n\nIf you want to position a child relative to something that is not its parent, just donâ€™t use styles for that. Use the component tree.\n\nSee https://github.com/facebook/yoga for more details on how position differs between React Native and CSS.\n\n\nThe fact that an element is always positioned relatively to its parent has been a problem for us since our player component is part of the list.\n\nOverflow and android are not friends\n\nAnd even if we would have found a way (we could have cheated by calculating negative values, relative to the parent, in order to stick to the edges of the device), we would have met other problems such as the fact the equivalent of overflow prop doesnâ€™t work on Android.\n\nThere are actually multiple opened issues concerning this problem. Grabbou gave a shot on this one #7229:\n\n\n\nLetâ€™s make a fullscreen, the native way\n\nHopefully, we are working with native developers, from both platforms. We have shared a lot of information and finally have found a solution.\n\nThis time, while rotating the device, we would have hidden everything around the VideoPlayer component. No more headers, no more footers, nothing except the player. Then, we would have set the player size so that it matches the device size:\n\n\n\nHereâ€™s the result we have got:\n\n\n\nWhat is happening on here?\n\nThere are multiple interesting things here, at ~200 cards down:\n\n\n  \n    Special visual effects are appearing (gray and blue background color)\n  \n  \n    The list scrolls too high and then refocuses\n  \n  \n    The video restarts\n  \n\n\nExplanations\n\nThe first thing to know is that we only keep 5 players alive (2 above, the focused one, and 2 below) and otherwise we display images. Itâ€™s important because of memory. Without this limitation, the application would have thrown some OutOfMemory errors (we met this kind of problems with Bitmap objects).\n\nThe second thing to notice is that we are always playing the video that is the most centered on the screen.\n\nThe last thing to know is that we actually have multiple rendering cycles to hide the different components around the VideoPlayer.\n\nFor now, with that information, letâ€™s imagine the following scenario:\n\n\n  \n    Scroll ~200 cards down\n  \n  \n    The most centered video is now playing\n  \n  \n    Rotate the device\n  \n  \n    It resizes all the images / VideoPlayer to match the device size\n  \n  \n    It removes ~200 headers + ~200 footers\n  \n\n\nDuring the 5th step, the list is scrolling up, because it has earned some space with the headers and footers disappearing. This creates the strange behaviour of â€œyo-yoâ€ list scrolling. Moreover, when the list is scrolling, the application finds a new â€œmost centered cardâ€, and creates the associated player. If the previous player is not part of the 5 new conserved ones, itâ€™s destroyed. Thus, the further we scroll in the list, the worse it becomes.\n\nThe combination of the 4th and 5th step creates the actual gray / blue screen in background.\n\nFor now, we have a quasi-functional solution. Itâ€™s not really user friendly but we have something close to work. The key point here is that improving the functional solution (avoid the â€œyo-yoâ€ effect) would also give a better user experience.\n\nSo, how can we avoid this â€œyo-yoâ€ behaviour ?\n\nPortal to the rescue\n\nRecently, we heard about React portals. It seems that it could have saved us from this specific situation. The idea is quite simple, we would have teleported the player from its current location to somewhere higher in the component tree, like the React Native documentation encourages us to, without triggering special state based rendering-cycles (aka: Headers + Footers removals):\n\n\n\nThe problem is that React Native doesnâ€™t support them natively: portals are part of ReactDOM, not React itself. We canâ€™t use it in our application.\n\nWeâ€™ve found and experienced some great open source alternatives on the JavaScript side such as react-gateway and we even managed to create our own one for this specific case.\n\nThe problem is that React would have created a new instance of the VideoPlayer each time we would have moved it, instead of keeping the old one. It means that we would have created 2 VideoPlayer, and lost both context.\n\nEach time we rotate the device, the video will restart from the beginning.\n\nWhat can we do with portal ? On the native side?\n\nThe portal idea is quite interesting: we need to find a way to create a portal-like behaviour with React Native, but on the native side, so that we wonâ€™t lose the VideoPlayer native context.\n\nSince we had the chance to be at the React Native Europe, we have learnt the way React Native is managing views thanks to Emile Sjolander.\n\nTo demonstrate this idea, letâ€™s take an example :\n\n\n\nThis is a simple application which provides two  components and displays some content. On the right, we can see the native tree view. The cursor shows the two native views that need to permute. The idea is to make First taking place of Second and vice versa.\n\nItâ€™s possible, using React Native, to use the module responsible of view management: UIManager (available directly from react-native module):\n\n componentDidMount() {\n    setTimeout(() =&amp;gt; {\n      // Permute child at indice 0 and 1 of parent tag 6\n      UIManager.manageChildren(6, [0], [1], [], [], []);\n    }, 3000);\n  }\n\n\nThis will end up making something like:\n\n\n\nIt seems that creating a portal-like behaviour is possible using ReactNative.\n\nThe main reason we didnâ€™t choose this solution is the fact that we didnâ€™t find a way to get the UIView native identifier from the JavaScript side (Iâ€™m not talking about nativeID or testID props, but the unique identifier of the view set on the native side).\n\nHereâ€™s a tweet from me concerning unique identifier\n\nNative implementation of â€œportalsâ€\n\nWe finally decided to implement a React Native native component called  that is able to move View children from a parent view to another one using a declarative API.\n\nUsing this approach, we gain more control over what we would like to do leveraging native side power.\n\nReparentable owns two props :\n\n\n  \n    name that represents the destination of the teleportation\n  \n  \n    target that represents the name of the target\n  \n\n\n&amp;lt;View style={styles.container}&amp;gt;\n  &amp;lt;Reparentable name=&quot;1&quot; target=&quot;&quot;&amp;gt;\n    &amp;lt;Text&amp;gt;First&amp;lt;/Text&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n\n  &amp;lt;Reparentable name=&quot;2&quot; target={this.state.shouldGo ? &quot;1&quot; : &quot;goNowhere&quot;}&amp;gt;\n    &amp;lt;Text&amp;gt;Second&amp;lt;/Text&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n&amp;lt;/View&amp;gt;\n\n\nOn this gist, &amp;lt;Reparentable name=â€2â€ â€¦/&amp;gt; will take place of &amp;lt;Reparentable name=â€1â€ â€¦/&amp;gt; when the state shouldGo will change.\n\nWhat does it mean?\n\nIn our context, it means that when the state isFullscreen is true, we are able to move the player from its current view to the higher one:\n\n&amp;lt;View style={styles.container}&amp;gt;\n  &amp;lt;Reparentable name=&quot;fullscreenView&quot; target=&quot;&quot;&amp;gt;\n    &amp;lt;FullScreenContainer /&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n\n  &amp;lt;Reparentable\n    name=&quot;videoPlayerId&quot;\n    target={this.state.isFullscreen ? &quot;fullscreenView&quot; : &quot;&quot;}\n  &amp;gt;\n    &amp;lt;VideoPlayer /&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n&amp;lt;/View&amp;gt;\n\n\nHereâ€™s the result weâ€™ve got:\n\n\n\nComparing both variants\n\n\n\nIt took us time to get this result, but we finally have something that meets our needs.\n\nLink to the library : https://github.com/mfrachet/rn-reparentable\n\nThanks for reading,\n"
} ,
  
  {
    "title"    : "The 6play platform goes international",
    "category" : "6play",
    "tags"     : " 6play, intl",
    "url"      : "/6play/2018/03/26/6play-goes-international.html",
    "date"     : "March 26, 2018",
    "excerpt"  : "Within less than a year m6web and techm6web managed to launch websites, android apps and ios apps for three RTL TV channels in Europe, all based on the 6play technology.\n\nThese deployments include:\n\n\n  broadcasted and themathic channels,\n  live st...",
  "content"  : "Within less than a year m6web and techm6web managed to launch websites, android apps and ios apps for three RTL TV channels in Europe, all based on the 6play technology.\n\nThese deployments include:\n\n\n  broadcasted and themathic channels,\n  live streaming, video catchup encoding and playout,\n  video resuming,\n  local adservers (for videos and display), DMP system and CDN,\n  contribution via our backoffice and automated through our API,\n  translations system,\n  and almost all the features of the 6play platform.\n\n\nRTL Play (Belgium)\n\n\n\nhttps://www.rtlplay.be/\n\nRTL Play (Croatia)\n\n\n\nhttps://play.rtl.hr/\n\nRTL Most (Hungary)\n\n\n\nhttps://www.rtlmost.hu/\n"
} ,
  
  {
    "title"    : "Useful (or not) M6Web OSS stuff",
    "category" : "OSS",
    "tags"     : " OSS, Open source, php, js",
    "url"      : "/oss/2018/03/20/useful-or-not-usefull-m6web-stuff.html",
    "date"     : "March 20, 2018",
    "excerpt"  : "At M6Web we do love open source and we are trying to be good open-source dev citizens! Here is a short presentation on our most interesting contributions:\n\n\n\nYou can find all our open source contributions on their dedicated page on our tech blog. ...",
  "content"  : "At M6Web we do love open source and we are trying to be good open-source dev citizens! Here is a short presentation on our most interesting contributions:\n\n\n\nYou can find all our open source contributions on their dedicated page on our tech blog. Enjoy !\n"
} ,
  
  {
    "title"    : "Atteindre les Ã©toiles avec PHP et Symfony",
    "category" : "",
    "tags"     : " conference, confoo, PHP, Symfony",
    "url"      : "/2018/03/07/atteindre-les-etoiles-avec-php-et-symfony.html",
    "date"     : "March 7, 2018",
    "excerpt"  : "Ã€ lâ€™automne 2014, M6 dÃ©cide dâ€™adapter le programme Rising Star en France, un concours de chant en direct, mais dont le jury est le public, qui vote en direct depuis son application mobile.\nA travers cette confÃ©rence, je me propose de vous prÃ©sente...",
  "content"  : "Ã€ lâ€™automne 2014, M6 dÃ©cide dâ€™adapter le programme Rising Star en France, un concours de chant en direct, mais dont le jury est le public, qui vote en direct depuis son application mobile.\nA travers cette confÃ©rence, je me propose de vous prÃ©senter lâ€™architecture mise en place pour Ãªtre capable de traiter plusieurs dizaines de millions de votes dans un dÃ©lais de quelques secondes, tout en se synchronisant avec une Ã©mission de tÃ©lÃ© en direct.\n"
} ,
  
  {
    "title"    : "Migration to Spark 2.2",
    "category" : "",
    "tags"     : " Data, Hadoop, BigData, Airflow, Hive, Spark, Java",
    "url"      : "/2017/12/13/spark-2.html",
    "date"     : "December 13, 2017",
    "excerpt"  : "To value our data in order to understand better our service and improve it, we use Spark. You can find more information in a recent article about our datalake. We recently migrated our biggest project from Spark 1.5 to Spark 2.2 and wanted to shar...",
  "content"  : "To value our data in order to understand better our service and improve it, we use Spark. You can find more information in a recent article about our datalake. We recently migrated our biggest project from Spark 1.5 to Spark 2.2 and wanted to share that story.\n\nSpark 2 has been released a year ago (July 26, 2016). Maybe we are a bit late, but better late than never.\n\nWe are working with an official version from Cloudera with Spark 1.6 as the default version.\n\nOur project runs everyday to get data from different sources and send them to different destinations.\n\nIt is built with Java and Spark 1.5, but we encountered several problems with those technologies. First of all, the Java + Spark community is smaller than the ones for Python or Scala. Secondly, the Spark 1.5 community is also smaller than the one of version 2.2.\n\nThat sometimes made information hard to find.\n\nBut most of all, we did not succeed to integrate new components that work with more recent versions.\n\nWe wanted to migrate for bugs fixes in general and in a performance purpose too.\n\nI) Workflow\n\n\n\na) Spark 1.5 to Spark 1.6\n\nFirst, we had decided to migrate to 1.6 to do a progressive migration. But we bumped into a bug with a UDF. We had difficulties fixing it, and it was resolved in 2.2.\n\nWe finally decided to migrate directly to 2.2.\n\nb) First validation with unit tests\n\nWe did the migration and ran our unit tests to see and fix the problems.\n\nc) Functional tests\n\nThen, we ran our jobs with some data sets. The idea was to check the differences with Spark 1.5.\n\nWe wanted to be sure that our unit parts were working together.\n\nd) Double run\n\nThen, we set out for a double run. It means that we had our jobs running both with Spark 1.5 and Spark 2.2 and we compared the outputs each day.\n\nWe used Airflow to deal with that. If you know Airflow, you will understand that we added a new DAG to run our project with Spark 2.2.\n\nThe idea was to see the potential differences between the two on a daily basis.\n\nAt the end, we merged our branch into master.\n\n2) Changes to migrate to 2.2\n\nThere are different changes from Spark 1.5 to 1.6 to 2.2. You will find them described in the documentation.\n\nThe idea here is to focus on the problems we met, the noticeable changes for us and how we dealt with them.\n\na) Dataset\n\nOf course, the main change is that â€œdataFrameâ€ does not exist anymore. You must replace it by â€œDataset&amp;lt;Row&amp;gt;â€.\n\nActually, â€œDataFrameâ€ and â€œDatasetâ€ were unified with Spark 2.0. In reality, for untyped API like Python, â€œDataFrameâ€ still exists. But, we work with Java.\n\n\n\nUsing â€œDataset&amp;lt;T&amp;gt;â€ is a way to apply a schema at the compilation. If there is a problem, you will get a logical exception. Before, with â€œDataFrameâ€, you could only have runtime exceptions.\n\nAs a first step, we replaced â€œDataFrameâ€ by â€œDataset&amp;lt;Row&amp;gt;â€\n\nb) SparkSession\n\nA second major difference is â€œSparkSessionâ€. It is the new entry to Spark. \nThere is no need anymore to create a â€œSparkConfâ€, a â€œSparkContextâ€ and a â€œSQLContextâ€. It is possible to get all of it just with a â€œSparkSessionâ€.\n\nBut, it is important to understand that if you just want to migrate your code in a first step to get it work with Spark 2, it is not a need to use â€œSparkSessionâ€. â€œSparkConfâ€, â€œSparkContextâ€ and â€œSQLContextâ€ still work.\n\nThat is what we decided to do.\n\nc) Iterable to Iterator\n\nThe return type â€œIterableâ€ is incompatible with â€œPairFlatMapFunctionâ€. We had to replace â€œIterable&amp;lt;&amp;gt;â€ with â€œIterator&amp;lt;&amp;gt;â€.\n\nWe replaced code like that:\n\npublic Iterable&amp;lt;String&amp;gt; call(String s) throws Exception {\n    ...\n    return list;\n}\n\n\nby something like that:\n\npublic Iterator&amp;lt;String&amp;gt; call(String s) throws Exception {\n    ...\n    return list.iterator();\n}\n\n\nd) Creating a UDF using hiveContext is not possible anymore the same way\nBefore, you could do something like that :\nhiveContext.sql(&quot;CREATE TEMPORARY FUNCTION function AS ...&quot;)\n\n\nBut now, you have to enable hive support first. You must do it with the SparkSession:\n\nSparkSession spark = SparkSession\n    .builder()\n    .appName(&quot;Java Spark Hive Example&quot;)\n    .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)\n    .enableHiveSupport()\n    .getOrCreate();\n\n\nIf â€œenableHiveSupportâ€ is not enabled, there is an error like this :\n\njava.lang.UnsupportedOperationException: Use sqlContext.udf.register(...) instead.\n\n\nWe decided not to use â€œSparkSessionâ€ in a first step and to follow the error instructions.\n\nWe replaced our direct call to Hive by a UDF registration.\n\ne) Deprecations\n\nWe noticed some deprecations like HiveContext or Accumulators for instance. But we decided not to deal with them for the moment.\n\nd) Performance\nWe have made some gains in performance. \nBefore, running our jobs lasted around three hours. Now, it lasts around two and a half hours.\n\nWe hope we will make some other gains by migrating to the Spark 2.2 philosophy (â€œSparkSessionâ€, etc).\n\nConclusion\nAs there are many backward compatibilities with Spark 2, it is not so difficult to make a first migration to make your project work. Nonetheless, it could be long to validate. It depends on your tests stategy too.\n\nOur next step now will be to integrate the new philosophy of Spark 2.2 to get the best of the new version.\n\nNastasia Saby (Zenika consultant)\n"
} ,
  
  {
    "title"    : "Forum PHP AFUP 2017",
    "category" : "",
    "tags"     : " afup, php",
    "url"      : "/2017/11/07/forum-php-2017.html",
    "date"     : "November 7, 2017",
    "excerpt"  : "M6Web Ã©tait sponsor de cette Ã©dition du Forum PHP organisÃ©e par lâ€™AFUP et une grande partie de lâ€™Ã©quipe backend avait fait le dÃ©placement. \nCe forum Ã©tait vraiment inÃ©dit de par sa taille sans prÃ©cÃ©dent : plus de 650 participants ! Il a Ã©tÃ© aussi ...",
  "content"  : "M6Web Ã©tait sponsor de cette Ã©dition du Forum PHP organisÃ©e par lâ€™AFUP et une grande partie de lâ€™Ã©quipe backend avait fait le dÃ©placement. \nCe forum Ã©tait vraiment inÃ©dit de par sa taille sans prÃ©cÃ©dent : plus de 650 participants ! Il a Ã©tÃ© aussi pour lâ€™Ã©quipe lâ€™occasion de voir des prÃ©sentations de grande qualitÃ© et trÃ¨s inspirantes. (sans compter celle de nos collÃ¨gues Fabien et Nastasia sur lâ€™AB testing).\n\n\n\nDe trÃ¨s nombreux retours exhaustifs sont disponibles sur le web et je pense que les vidÃ©os seront rapidement en ligne sur la page listant tous les talks organisÃ©s par lâ€™AFUP. On peut noter, comme Ã  chaque Forum, les tendances qui se dÃ©gagent de lâ€™ensemble des talks et suite aux discussions endiablÃ©es qui suivent les prÃ©sentations :\n\n  DDD commence Ã  Ãªtre prÃ©sent dans tous les talks type mÃ©thodo,\n  GraphQL fait parler de lui, et câ€™est tant mieux,\n  des reality checks sur les modes de ces derniÃ¨res annÃ©es (comme les micro services, la qualitÃ© au sens large), mais une maturation et un recul sur des pratiques modernes qui font plaisir Ã  voir,\n  des considÃ©rations trÃ¨s intÃ©ressantes sur la gestion du code source (refactoring, clean code, nommer les choses ;) â€¦).\n\n\nEnfin, jâ€™ai vraiment (en tant que lyonnais) apprÃ©ciÃ© la rÃ©gionalisation de lâ€™AFUP, avec une multitude dâ€™antennes locales crÃ©ees ces derniÃ¨res annÃ©es.\n\nBravo lâ€™ @afup pour la rÃ©gionalisation des badges au #ForumPHP ! pic.twitter.com/ekewUkCoKS&amp;mdash; Olivier Mansour (@omansour) 26 octobre 2017\n\n\nUn excellent cru que M6Web Ã©tait ravi de soutenir ! Et rendez-vous au PHP Tour !\n"
} ,
  
  {
    "title"    : "Genesis of M6&#39;s Datalake",
    "category" : "Data",
    "tags"     : " Data, Hadoop, BigData, Airflow, Hive, Spark, DMP",
    "url"      : "/data/2017/10/23/genesis-of-m6-datalake.html",
    "date"     : "October 23, 2017",
    "excerpt"  : "At M6 we work hard to leverage Data to help our ad sales team, our CRM teams and our product innovation process.\nOver the past 2 years, we have gone from using a market DMP to creating our second Hadoop platform. We now feel that our stack is stab...",
  "content"  : "At M6 we work hard to leverage Data to help our ad sales team, our CRM teams and our product innovation process.\nOver the past 2 years, we have gone from using a market DMP to creating our second Hadoop platform. We now feel that our stack is stable, reliable and scalable so it feels like the right time to share our experience with the community.\n\n\n\nStep 1: embracing the DMP\n\nOur first use case was to monetize data through targeted publicity.\nWe decided to start by installing a DMP (Data Management Platform) because it was a very fast solution to deliver our major needs, in particular :\n\n\n  Collect data from all our services and combine it with our user\nknowledge =&amp;gt; DMPs offer that off the shelf\n  Create segments for audience targeting =&amp;gt; The segmentation approach offered by DMPs was well adapted to the ads market\n  Activate our Data, both in house via our adservers and in the outer market =&amp;gt; DMPs generally offer simple integration with most adservers, and a very straight forward third party integration\n\n\nThe match seemed quite obvious and thereâ€™s a good reason for that: DMPs are designed for this use case above all others.\n\nWe chose Krux (now Salesforce) and deployed it over our ~30 sites and applications. Installing Krux on our network and plugging it to our video and display adservers ended up taking a few months and a decent effort. Convincing all our teams that the increase in ad revenue would make it worth the development time and the negative impact on webperf wasnâ€™t trivial, but got through thanks to our top management sponsoring. Once on the job, the deployment was quite smooth on the web and mobile apps, but validating the quality of the ingested data turned out to be an endless project.\n\nAt the end of the day, Kruxâ€™s DMP did the job. In November 2015 we launched Smart6tem, our Data platform &amp;amp; advertisement offer based on segments (announcement here, articles here here or here). This move had a very positive effect on our advertisement market, and allowed to start making Data mean something at M6.\nTo give some detail of our use of the DMP, it turned out building our own segments was very successful, but we didnâ€™t use any 3rd party interconnection because we didnâ€™t find any valuable Data to buy and didnâ€™t want to reduce the value of our own Data by sharing it out.\n\n\n\nKruxâ€™s segment builder\n\nOnce the Advertisement use case was out in the market, we moved our efforts towards leveraging the DMP for our CRM teams. The rationale was simple: targeted emails are more efficient than newsletters. We were hoping to reduce the email pressure on our users while increasing the performance both for revenue and traffic.\n\nStep 2: first round testing Hadoop\n\nHaving a DMP is both a great accomplishment and frustrating.\nItâ€™s great because you can start to combine the use of your service with the user profiles to produce segments and activate use cases to address them.\nBut for CRM, the workflows to plug segments into our emailing systems werenâ€™t native and we needed to build some custom workflows. No rocket science, but when we first received a 2 Billion line file for the user/segment map that we needed to filter and convert into another format, our developers went grumpy.\nWe also got frustrated very fast because we wanted to start to extract some unpreceded analytics insights combining our user knowledge (our major service, 6play had just switched to fully logged-in users) with usage stats or with external sources like our adserver logs. Advanced analytics was clearly not the field of Krux.\nLast but not least came some limitations (either due to the design of Krux or the pricing):\n\n\n  We could only work on 3 months of history if we wanted to keep the price reasonable (on 6play we have a lot of TV shows that run for 3 months per year, and segmenting the users who watched the show last year is important).\n  Itâ€™s based on cookies + device ids on mobile (itâ€™s the best solution for most use cases, but if your users are logged in, it introduces quite a lot of risk to make mistakes).\n  We never managed to convince our users that the amount of cookies or users inside segments was correct. Every single study we made on this point led to doubt, and our DMP support team never came up with serious answers.\n\n\nAt this point, Hadoop came in as an evidence, so we created our first cluster.\nThe process of creating this proof of concept cluster was pretty much a black box for us since we charged a partner with the job. We ended up with the following setup, all hosted by AWS :\n\n\n  2 name nodes with 16 VCPUs and 30G RAM each\n  4 data nodes adding up to 64 vcpuâ€™s and 120G RAM\n  Cloudera Enterprise with Hive, Impala, Hue, Python, R and a kinky crontab\n  Tableau Desktop + Tableau Server\n\n\nNothing crazy but that brought us into the world of Hadoop, and that was a major move. We also staffed our first Data Scientist to start to explore our Data and imagine use cases.\n\nOur first steps in Hadoop were hesitant, but within a few months, we had created our first Data Lake, our targeted CRM was live and we had produced a few dozen dashboards providing unpreceded insights throughout the company. From the business perspective, it was a success.\nFor the people who got their hands on a Data Lake for the first time the experience was ground breaking. For the first time, we could connect information from half a dozen different tools seamlessly.\nAn example: finding how many ads were seen by women from 25 to 49 years old during the NCIS TV show.\nBefore the Data Lake, this would have been impossible. The closest we could get would take the following process :\n\n\n  Extract the amount of ads viewed on NCIS from our adserver stats to a text file\n  Extract the NCIS traffic from our video consumption tracking tool (in Cassandra) to a text file\n  Extract our users Database with age and gender (in a third party tool named Gigya) to a text file\n  Load all this up into an Excel spreadsheet\n  Write a bunch of Excel formulas to produce the percentage of the traffic on NCIS thatâ€™s generated by women between 25 and 49 years old\n  Apply that percentage to the adserver stats\n\n\nAs you can see, combining information between our ecosystems involved some very manual processes and could only lead to approximations, so basically we never did them.\n\nWith our Data in Hadoop, all this turns out to be a simple SQL query in Hue (a PhpMyAdmin style interface for Hadoop):\n\nSELECT COUNT(*) FROM adserver_logs A\n   JOIN users U ON A.uid = U.uid\n   JOIN programs P on A.pid = P.id\nWHERE A.type = &#39;impression&#39;\n   AND U.age &amp;gt;= &#39;25&#39;\n   AND U.age &amp;lt;= &#39;49&#39;\n   AND U.gender = &#39;F&#39;\n   AND P.name = &#39;NCIS&#39;\n\n\nHadoop and our Data Lake, we could just jump over the barriers between tools and ecosystems within seconds. Combined with the ability to code in various languages, we could instantly start to industrialize such insights and start going further.\n\nWe convinced our top management very fast about the value of having our own Hadoop cluster, and since it was very (VERY) expensive, we decided to internalize it.\n\nStep 3: building our internal Hadoop cluster\n\nSo there we were with a quite simple roadmap: replace our v1 Hadoop cluster to reduce costs and improve performance as much as possible. We managed to divide the price by 3 while multiplying the resources by 8.\n\nThe first step on this road was to staff a tech team to design and create our platform. That ended up being very tricky and finally took us 10 months to complete.\n\nOnce the team was staffed, we got onto the job. We had 5 steps :\n\n\n  Choose the hosting platform (4 months)\n  Choose the hardware\n  Choose the software stack (2 months, done in parallel)\n  Set up the cluster (2 months)\n  Migrate all our projects to the new platform (3 months)\n  Check to be sure everything was done (1 month)\n\n\na) Hosting platform\n\nThis stage of the project was a very religious one. Many people at M6 had a very strong desire to go towards cloud and managed services, others were totally in favor of Hadoop and have full in house control over the platform. The major options were:\n\n\n  AWS\n    \n      Amazon EMR + S3\n      Amazon EC2 + S3\n    \n  \n  Google Cloud Platform\n    \n      Managed services (Dataflow, BigQuery, Compute engine, Pub/Subâ€¦) + Cloud storage\n      Dataproc + Cloud storage\n    \n  \n  On premise\n    \n      Add servers to our 6play platform at Equinix\n      Work with our hosting subsidiary, Odiso\n    \n  \n\n\nWe spent 3 months talking to the different vendors and considering options.\n\nThe first decision we took was to use Hadoop instead of managed services.\nThe AWS and Google sales teams were very convincing, but we finally declined for 2 main reasons:\n\n\n  People in our company were starting to learn how to use Hadoop, changing the stack would have forced everyone to re-learn what they were just starting to dominate. Not very efficient while building up expertise.\n  Using proprietary solutions like Big Query involves a strong locking risk. If we developped all our projets to leverage a specific platform, changing providers in a few years would involve a lot of reworking on all our code base.\n\n\nThe next step was to choose between the 3 hosting options. On a side note, we compared the price for x4 and x10 resources compared to our v1 platform.\nAt the end of the process we wrote up an evaluation grid. Here is the summary version.\n\n\n\nThe decision was there, we went for a fully on premise stack with Odiso.\nTo detail some of that evaluation, hereâ€™s a few insights on what it came down to.\n\n\n  AWS is cool, but ultra expensive. I mean itâ€™s 10 times more than our on premise option! We would have gone full AWS if the price was reasonable. The possibility to pop clusters up and down is very interesting and reduces costs, but our v1 platform was using our 4 EC2 Data Nodes at ~80% 24/7, so we could never go down to 0 servers.\n  Google feels better on the service side of things, but it involved taking chances because the commercial product is young and support + community experience seemed weak.\n  On premise was clearly much cheaper, and felt more secure for our low experience on Hadoop since weâ€™re used to managing servers and our team had managed serious Hadoop before.\n\n\nb) Hardware\n\nGoing on premise means buying physical servers and building them.\nOur goal here was to massively upgrade our current platform to scale with the companyâ€™s usage of Big Data. Since the price was very reasonable, we settled down to x8 on CPU, RAM and storage compared to our initial Hadoop cluster. Hereâ€™s the stack we bought:\n\n4 KVM servers:\n\n\n  DELL PowerEdge R630\n  OS Disks: 2x 400GB ssd, RAID 1\n  Data disks: 8 * 2To\n  RAM: 256Go (8*32G)\n  CPU: 2x12 Cores (3.0Ghz)\n\n\n15 Data Nodes:\n\n\n  DELL PowerEdge R630\n  OS: 2x 400GB ssd, RAID 1\n  Data disks: 8 * 2To\n  RAM: 384Go (24*16G)\n  CPU: 2x12 Cores (3.0Ghz)\n\n\nBuilding and racking the servers was quite straight forward, thereâ€™s nothing special about Hadoop in this process except the high quality network connectivity.\n\nc) Software stack\n\nDesigning the software stack was very straight forward.\nWe had the desire to stay as close as possible to the stack our users were getting used to, and it was pretty much a standard Cloudera stack. That suited us very well because our first priority was to avoid any regression, both for the projects (during this period, they had massively multiplied as weâ€™ll detail in the migration part below) and for the users.\nAnother early choice was to use virtual machines with Proxmox and not dive into the Kubernetes + Docker adventure. Although that was tempting and will probably be an option in future, we considered mastering the Hadoop stack was enough on our plate for the moment, we needed to reduce risk.\n\nHereâ€™s the stack we chose:\n\n\n  Puppet\n  Centos 7\n  Proxmox\n  Cloudera Hadoop 5.11 (free version)\n  Hadoop 2.6\n  Hive 1.1\n  Spark 1.6 and 2.1 (we had 1.6 before but our Data Scientists really wanted to use new features)\n  Supervisord\n  MariaDB\n  LDAP\n  Ansible\n  OpenVPN\n  Python 2.7 and 3.6 with Anaconda\n  Java 8\n  R 3.3\n  Scala\n  Airflow 1.8 (this is out of the Cloudera stack, an important and epic part of our toolkit that weâ€™ll surely talk about in more detail in a future post)\n  Sqoop\n  Hue 3 with Hive on Spark as default\n  Tableau Desktop + Tableau Online\n  Jupyter\n\n\nd) Install Hadoop and all our tools\n\nOne of the fun parts of our design process was to choose a name for our new cluster. We called it Cerebro (in reference to X-Men and the global view of Professor Xavier), and created a logo :)\n\n\n\nSetting this stack up felt very simple from my perspective, but thatâ€™s surely because our awesome team overcame the issues silently.\nOn the timeline, the biggest part of the setup was receiving the physical servers. That took about 3 months because some parts (SSD disks) were out of stock for a long time.\nWe received a first part of the Data Nodes a couple of months before the rest of the servers, so we decided to start building the cluster with temporary Name Nodes and services, and migrate them after.\n\nWe deployed Cloudera Hadoop via KVM servers (managed with Puppet) and the Cloudera Manager. Very straightforward.\nWe used Ansible to install our stack, manage all our configuration files and user access.\n\ne) Migrate our projects and Data\n\nMigration was a project in the project.\nBetween the day we decided to build our internal platform and the day we delivered, 20 months had gone by. During all that time, Big Data had been going through high pace growth inside M6. We scaled from ~1 to ~25 users, from 0 to ~200 Dashboards and ~60 projects. All of this relying on our â€œProof Of Conceptâ€ platform created with a partner.\nTo be honest, it was an utter mess in any Software Engineerâ€™s eye. Imagine: no version control, a unique user hosting all the projects and executing 6000 crontab lines each day. No job optimisation whatsoever. Moreover, most of our users had no developpement process knowledge, so they didnâ€™t see any problem with all this and werenâ€™t all in favour of any change. The context was challenging.\n\nThe first step of our migration project was to bring all this back into a â€œmigratableâ€ state. To do that, we went through the following steps :\n\n\n  Put all the code base in Git\n  Create a code deployment process\n  Split the production jobs down to a 1 user per project approach, both for code execution and data storage\n  Make all paths to data relative\n  Switch from crontab to Airflow\n  Add backups on S3\n\n\nWe reached this milestone after 4 months of a large rework of all our projects by all our teams. The collective investment in this process was a real team success.\n\nThe second step was to rebuild all the projects and databases on the new platform.\nThanks to our new backup system that copied all our Data to S3, rebuilding databases was easy. Basically it took creating a script to restore the backup in the new platform, and we could start checking integrity by querying the datasets. Rebuilding projects was a similar process, we just had to deploy each project and it was ready to test. Everything went fast and easy, proving that all the preparation moves we made were very valuable.\n\nThe third step was to double run all our projects so we could be sure everything worked on the new platform while not breaking production.\nThereâ€™s a tricky part to this because a fair amount of our projects include an output towards external servers (either other teams within M6 or 3rd parties). For this we had to add an â€œonly run onâ€ logic. That lead us to create a unified configuration and a library for exports.\nWe also had to distinguish all our code execution monitoring so we could keep an eye on what each workflow was producing, both in production and on the new platform. For this we added the platform name to all our Graphite nodes and updated all our dashboards to filter by platform.\nWith those 2 moves, most projects managed to run â€œout of the boxâ€. Some needed some refactoring, mostly for parts that had been forgotten in the first step.\n\nThe fourth step was validating that our double run was working well.\nThe theory of this validation was quite elaborate. For each table or output job we would count the number of lines in each partition produced, run checksums, dive into the details of the monitoring, and run manual tests.\nIn practice, that part cracked up quite fast because our v1 platform was being totally outscaled and therefore all our users really didnâ€™t want to look back. We checked that the backups were good with file sizes and line counts, and for the rest we relied on our monitoring to be sure that the jobs runned and produced the same output volumes. For the most critical production jobs we went into some detailed manual checking, but we took the jump very fast.\n\nThe fifth and last step was migrating all our Tableau Dashboards to Tableau online.\nWe needed all our ingestion and treatment jobs to be up and running before we could migrate our 200+ Dashboards. Once that was done, most dashboards took nothing more that being opened in Tableau Desktop and published to Tableau Online. The only exceptions were the bunch of users who had missed some tables out in step 1. Those had to run through the whole process at fast speedâ€¦ Not very pleasant for them.\n\nSo there we are, we now have our 2 feet in our second Hadoop platform. Now weâ€™re looking forwards, both on how we make this platform evolve to empower our future use cases, and to raise our innovation pace for Big Data to count much more within M6.\nBy all means stay posted, weâ€™ll update you on some of the awesome projects weâ€™ve been working on!\n\nTake away\n\n\n  Deciding to create an internal Hadoop platform took time and a few previous steps for our organisation to start to understand what Big Data was about and the way to go around it.\n  Choosing our hosting solution was hard and very conviction driven.\n  On premise hosting is cheaper than cloud solutions, but obviously less flexible.\n  No surprise for tech people and itâ€™s valid way beyond Big Data, migrating projects developed without any engineering good practices was hard and risky work.\n\n\n"
} ,
  
  {
    "title"    : "Elasticsearch: la grande migration",
    "category" : "",
    "tags"     : " Elasticsearch, Php",
    "url"      : "/2017/06/01/migration-elasticsearch.html",
    "date"     : "June 1, 2017",
    "excerpt"  : "Pour assurer la scalabilitÃ© des performances de lâ€™API 6play, les donnÃ©es suivent tout un workflow pour Ãªtre dÃ©normalisÃ©es et stockÃ©es dans Elasticsearch.\nMi-2016, nous avons identifiÃ© des dysfonctionnements majeurs sur nos serveurs, entrainant par...",
  "content"  : "Pour assurer la scalabilitÃ© des performances de lâ€™API 6play, les donnÃ©es suivent tout un workflow pour Ãªtre dÃ©normalisÃ©es et stockÃ©es dans Elasticsearch.\nMi-2016, nous avons identifiÃ© des dysfonctionnements majeurs sur nos serveurs, entrainant parfois des interruptions de service.\nSuite Ã  quelques mesures dâ€™urgences pour stabiliser lâ€™existant, nous avons entrepris de mettre Ã  jour notre version dâ€™Elasticsearch pour bÃ©nÃ©ficier des derniÃ¨res amÃ©liorations.\nNous Ã©tions alors sur la version 1.7, et souhaitions passer en version 2.0.\nAprÃ¨s plusieurs mois dâ€™efforts pour effectuer cette migration sans interruption de service ni gel technique, nous voici en versionâ€¦ 5.2!\nVoici le rÃ©cit de cette grande migration, et ce que lâ€™on a appris tout au long de ce pÃ©riple.\n\n\n\nLa thÃ©orie\n\nIl nâ€™y a pas de mÃ©thode magique pour changer de cluster sans coupure, la stratÃ©gie adoptÃ©e est assez classique:\n\n\n  dupliquer les Ã©critures sur le nouveau cluster\n  basculer les lectures sur le nouveau cluster\n  arrÃªter les Ã©critures sur lâ€™ancien cluster\n\n\nIl nâ€™est pas nÃ©cessaire dâ€™enchaÃ®ner toutes les Ã©tapes dans la mÃªme journÃ©e, cela prÃ©sente donc lâ€™avantage de pouvoir Ã©taler les dÃ©ploiements dans le temps en fonction des disponibilitÃ©s,\nainsi que de surveiller attentivement le monitoring pendant quelques jours pour vÃ©rifier que lâ€™infrastructure supporte bien les changements apportÃ©s.\n\nÃ‰critures en Y\n\nNous utilisons des workers Php pour dÃ©tecter les changements dans notre BDD,\nsuite Ã  quoi un message est publiÃ© dans une file dâ€™attente pour Ãªtre traitÃ© par un autre worker qui se chargera de synchroniser les entitÃ©s entre MySQL et Elasticsearch.\nIl Ã©tait primordial que le cluster de production ne soit pas impactÃ© par les Ã©ventuelles erreurs rencontrÃ©es sur le nouveau cluster.\nUne de nos premiÃ¨res intentions Ã©tait de publier le message de mise Ã  jour dans une deuxiÃ¨me file dâ€™attente, consommÃ©e par des workers dÃ©diÃ©s eux aussi au nouveau cluster.\n\n\nLe gros inconvÃ©nient est que cela impliquait de doubler toutes les lectures sur la BDD, toutes les requÃªtes devaient Ãªtre executÃ©es une fois par cluster,\ncela risquait donc dâ€™impacter dâ€™autres services.\nNous sommes donc partis sur une solution purement logicielle, puisque pour chaque entitÃ© mise Ã  jour ce sont les workers qui les envoient sur chaque cluster.\n\n\nIl est par contre nÃ©cessaire de gÃ©rer correctement les erreurs, que faire si une erreur intervient sur un cluster mais pas lâ€™autre?\nSi le nouveau cluster devient instable et que lâ€™on renvoie les messages systÃ©matiquement dans la file dâ€™attente, on risque dâ€™accentuer inutilement la charge en Ã©criture sur le cluster stable en production.\nNotre compromis est de dÃ©finir comme master le cluster de production (celui oÃ¹ les donnÃ©es sont lues), et seules ses erreurs provoquent la gÃ©nÃ©ration dâ€™un nouveau message.\nLes erreurs sur le cluster slave sont monitorÃ©es, mais ne gÃ©nÃ¨rent pas de nouveaux messages dans la file dâ€™attente.\nEffectivement, puisque chaque soir nous resynchronisons toutes les donnÃ©es entre MySql et Elasticsearch, on peut se permettre dâ€™avoir des donnÃ©es moins fraiches sur le cluster slave le temps dâ€™une journÃ©e.\n\nInitialement, nous pensions que ce systÃ¨me master/slave serait temporaire, mais trÃ¨s rapidement nous avons pÃ©rennisÃ© ces dÃ©veloppements, cela nous permettait de tester facilement diffÃ©rents clusters,\nou encore de vÃ©rifier que les donnÃ©es Ã©taient bien indexÃ©es de la mÃªme maniÃ¨re, faire des rollbacks en urgenceâ€¦\n\nUne derniÃ¨re difficultÃ© Ã©tait de faire cohabiter deux version diffÃ©rentes du Sdk Elasticsearch Php dans le mÃªme projet.\nIl y a effectivement une incompatibilitÃ© entre les versions 2.* et 5.*, et nous nâ€™avons pas eu dâ€™autre choix que de cloner la librairie concernÃ©e et de changer tous les namespaces pour Ã©viter les conflits de noms.\nMalgrÃ© tout, ce ne sont pas les Ã©critures dans Elasticsearch qui nous ont posÃ© le plus de problÃ¨mes.\n\nMigration des requÃªtes\n\nIl y a eu de nombreux changements apportÃ©s entre la version 1.7 et 5.0 et souvent pour le mieux.\nLes diffÃ©rentes Ã©volutions de syntaxes ont gÃ©nÃ©ralement vite Ã©tÃ© faites, car nous avions pris soin dâ€™encapsuler la construction des requÃªtes via quelques fonctions helper (une sorte de query builder).\nIl nous a donc suffit de changer ces quelques fonctions pour traduire les anciennes requÃªtes vers la nouvelle syntaxe.\n\nUne erreur classique que nous faisions en 1.7 Ã©tait de se contenter dâ€™un mapping par dÃ©faut, qui avait le mÃ©rite de fonctionner sans efforts avec nos requÃªtes et nos donnÃ©es.\nLors du passage Ã  la version 5.0, Elasticsearch a commencÃ© Ã  refuser certaines de nos requÃªtes car elles ne pouvaient pas Ãªtre performantes.\nIl fallait choisir, soit activer explicitement des options de mapping en faisant un compromis sur les performances gÃ©nÃ©rales,\nsoit affiner le mapping pour quâ€™il soit plus adaptÃ© Ã  la nature de nos requÃªtes.\nIl sâ€™agit dâ€™un bon exemple de Leaky Abstraction,\non a beau utiliser des outils pour sâ€™abstraire de la faÃ§on dont sont stockÃ©es les donnÃ©es, nous sommes toujours obligÃ©s de comprendre ce quâ€™il se passe Ã  lâ€™intÃ©rieur pour en tirer les meilleures performances.\nHeureusement pour nous notre mapping Ã©tait assez trivial Ã  changer, car nous nâ€™utilisons pas Elasticsearch pour faire de la recherche full-text\nmais seulement pour des recherches exactes sur des identifiants, des codesâ€¦ Il nous a gÃ©nÃ©ralement suffit dâ€™utiliser le nouveau type keyword\npour que nos requÃªtes puissent Ãªtre acceptÃ©es.\n\nPour sâ€™assurer du bon fonctionnement des APIs suite Ã  ces nombreux changements, nous avons investi du temps pour Ã©crire des tests fonctionnels de bout en bout,\npour vÃ©rifier que les rÃ©sultats restaient inchangÃ©s malgrÃ© le changement de version de cluster.\nBien sÃ»r, mÃªme si en local nos tests Ã©taient au vert, des erreurs pouvaient apparaÃ®tre en production.\nLe scÃ©nario Ã©tait alors simple, faire un rollback, ajouter les tests correspondants aux nouvelles erreurs detectÃ©es, les faire passer en local, puis recommencer !\n\nCe qui nous a peut-Ãªtre le plus Ã©prouvÃ© dans cette migration, câ€™est une regression introduite dans la version 5.2\nqui avait pour consÃ©quence de changer certains de nos tableaux vides en valeur null.\nIl nous a fallut quasiment repasser sur chaque requÃªte pour retransformer ces valeurs en quelque chose de cohÃ©rent. \nQuand on avait de la chance, ce bug faisait Ã©chouer nos tests, mais il est malheureusement arrivÃ© que ce soient les parseurs json des applications 6play de production qui en fassent les frais, avec diffÃ©rents plantages Ã  la clÃ©â€¦\n\nConclusions\n\nCette migration fut longue et parfois douloureuse, heureusement les rÃ©sultats sont maintenant au rendez-vous!\n\n\nDe plus, cela nous a donnÃ© lâ€™occasion dâ€™investir un peu de temps pour amÃ©liorer nos tests fonctionnels, et pour dÃ©velopper un systÃ¨me robuste pour la rÃ©plication des donnÃ©es sur plusieurs clusters Elasticsearch.\nOn peut nÃ©anmoins se poser des questions sur la stratÃ©gie trÃ¨s offensive de changement de version de la part de Elasticsearch,\nautant de releases avec autant de changements en si peu de temps, il faut Ãªtre capable de suivre!\nPendant que nous finalisions notre production sur la version 5.2, la 5.3 a eu le temps de sortir, et la 6.0 est apparue en beta.\nNous allons essayer de profiter un peu de ce nouveau cluster avant de poursuivre vers une nouvelle grande migration :)\n"
} ,
  
  {
    "title"    : "Last night isomorphic JS saved our life!",
    "category" : "",
    "tags"     : " SPA, SSR, isomorphic, javascript, node.js, high availability",
    "url"      : "/2017/05/17/spa-mode-isomorphism-js.html",
    "date"     : "May 17, 2017",
    "excerpt"  : "For more than a year and a half, we use Node.js and React together to make the best app possible for our users. These 2 technologies are complementary to write only once code executed on the server and the client side: thatâ€™s the isomorphic way! T...",
  "content"  : "For more than a year and a half, we use Node.js and React together to make the best app possible for our users. These 2 technologies are complementary to write only once code executed on the server and the client side: thatâ€™s the isomorphic way! This approach helped us to develop a reliable app with a fast first render and SEO friendly.\n\nSSR caching\n\nHere is the architecture we use for the 6play web app.\n\n\n\nYou can see that Node.js server responses are cached with Varnish. Indeed, React is not efficient with server side rendering because it just has not been designed for that. The React renderToString method blocks the event loop. Consequently the server can not process an acceptable responses rate for a service like 6play that can reach a lot of requests per second without cache. Particularly when the European Football Championship final bring together France and Portugal and is live or when the last episode of Â« Les Marseillais Â», one of the teenagers favorite programs, has just been released on the platform. So caching server responses, with a quite low caching time, is required for our application health!\n\nSPA mode\n\nIsomorphism enables search engines to parse our website without executing any line of JavaScript, only using the server side rendering. We thought that the opposite could be useful too. Imagine our Node servers are down, for various reasons. Our Varnish servers continue to deliver the application pages but only during the cache time. After, the user would get an errorâ€¦ or not!\n\nIn this case, we would switch to a Nginx server that simply delivers a blank page with the client JavaScript code. The server was responsible for the app state initialization before, the user browser has to do so now. Then it can render the page: our application becomes a simple SPA. And this is almost imperceptible for the user, the first render is just a little longer. This way secures the availability of our service.\n\n\n\nThe Varnish servers check the status of the Node ones via a specific route. When every instance is down, they route all requests to a static HTML file on the Nginx server.\n\nReally useful ?\n\nYes! It is not used until it is! A few months ago we went through a memory leak. Consequences? After some time, we saw an increase in CPU usage, then the servers fell down and SPA mode was enabled. We didnâ€™t notice the memory leak immediately because we often deploy new versions of our app and it resets the memory. When we detected the problem, it was too late to rollback because the incriminated version was probably weeks or months old.\n\nYou certainly know how difficult it is to find the code responsible for a memory leak in Node.js. It is often not a matter of hours but of days or weeks. With our SPA mode, we could debug our code with serenity. When the Node servers were down, the SPA mode took the reins. Then we simply restart the server to restore the nominal state when we were alerted (sometimes immediately, sometimes several hours after because it happened in the night). This situation went on some weeks. And we finally fixed the memory leak. No user has been affected. For us, this SPA mode is a significant safety for the high availability of our app.\n"
} ,
  
  {
    "title"    : "Symfony Live Paris 2017",
    "category" : "",
    "tags"     : " symfony, symfony live",
    "url"      : "/2017/04/03/sf-live-2017.html",
    "date"     : "April 3, 2017",
    "excerpt"  : "In March we attended Symfony Live Paris 2017, and it was very interesting.\nHere are some special feedbacks about some of our favorite talks.\n\n\nVarnish tags and invalidation\nSpeaker : JÃ©rÃ©my DerussÃ©\nDescription\nJÃ©rÃ©my presented us how to host appli...",
  "content"  : "In March we attended Symfony Live Paris 2017, and it was very interesting.\nHere are some special feedbacks about some of our favorite talks.\n\n\nVarnish tags and invalidation\nSpeaker : JÃ©rÃ©my DerussÃ©\nDescription\nJÃ©rÃ©my presented us how to host applications on an production environment on a Raspberry PI using varnish cache.\nToday, the internet traffic and the number of unique visitors increase every days.\nPeople imagines that we need strong hardware servers to be able to respond to this traffic.\n\nThe interesting part of this presentation was the way of how to better use the cache in front of our applications.\nTo be able to minimize the number of requests to the server by managing the cache duration\nand being able to invalidate it on demand when a resource expire.\n\nHow to tag and segment the cache\nBy tagging every resources by a unique id (entityName_id by example), youâ€™ll be able to invalidate later the cache linked to it.\nYou can simplify and automate this tagging by using event dispatcher and listen all entities creations / modifications. With\nthis solution you also centralize your code logic.\n\nCache tagging also helps you with caching / invalidating many-to-many relationships between content items.\n\nHow to invalidate the cache\nJÃ©rÃ©my showed us a very interesting interface, displaying the entity content, automatically refreshed when the backend needs to be called because of a cache invalidation.\n\nOnce a resource is modified and needs to be uncached, your backend needs to call the Varnish to purge the tag linked to\nresources. Once again the FOS HttpCache bundle does it very well.\n\nAdvantages / drawbacks\nCaching everything is fine, but it has limitations :\nWorks well when :\n\n  You have mostly read access\n  You have more cache hit than miss\n  Your application is able to communicate with your cache servers\n\n\nCan be difficult when :\n\n  Operations are not atomic\n  Itâ€™s complexifying / slowing backend writes\n\n\nHosting on a Raspberry PI\nJÃ©rÃ©my finally showed us an impressive demonstration of an application running on Raspberry PI.\nThe configuration :\n\n  Docker\n    \n      MySQL\n      NGINX\n      PHP7-FPM\n      Varnish\n    \n  \n\n\nThe results :\n\n  Number of calls with no caching handling : around 8 / secs\n  Number of calls with varnish cache response : around 900 / secs\n    \n      impressive !\n    \n  \n\n\nResources\nYou can manipulate cache by using the FOSHttpCacheBundle : https://github.com/FriendsOfSymfony/FOSHttpCacheBundle\n\nLink to the speaker presentation : https://www.slideshare.net/JrmyDeruss/grce-aux-tags-varnish-jai-switch-ma-prod-sur-raspberry-pi\n\nEverything a dev should know about Unicode\nSpeaker : Nicolas Grekas\nDescription\n\nNicolas Grekas made a talk about Unicode, from its origin to the latest advanced uses, and more precisely in the PHP ecosystem. He explained how utf8/16 works and the complexity of language management, especially the folding, graphs clusters, modifiers etc.\n\nPHP doesnâ€™t natively handle the unicode #RIPphp6, so it is important to understand the specificities of utf8 in order to avoid some traps, especially concerning string length calculation, comparisons and insertions in database.\n\nIn order to manage the unicode, you must use the php functions of mbstring, iconv, graph and inttl. The â€œuâ€ modifier allows the utf8 to be processed correctly for regular expressions. For MySQL, utf8_unicode_ci handles ligatures whereas utf8_general_ci does not handle them - but is therefore faster.\n\nConcerning security and avoiding typosquatting, there is a list of confusable characters for filtering: https://unicode.org/cldr/utility/confusables.jsp?a=6play&amp;amp;r=None\n\nThis presentation was very interesting and very useful. Thank you Nicolas ;-)\n\nResources\nLink to the speaker presentation : https://speakerdeck.com/nicolasgrekas/tout-ce-quun-dev-devrait-savoir-a-propos-dunicode\n\nPerformances optimisation with Php7\nSpeaker: Julien Pauli\nDescription\n\nAs usual, it was a real pleasure to listen Julien Pauli speaking about whatâ€™s under the hood of Php7.\nEverybody knows there is an actual performances gap between version 5 and 7, but this talk gave some clues to understand the technical reasons behind these major improvements.\n\nFirst, Php compiler has been totally rewritten (yes, Php is compiled in opcode and cached in opcache).\nThanks to an AST, the compiler really understands the analyzed code,\nand lot of optimizations can be done at compilation time instead of runtime.\nFor example, all constant expressions (like $a = 1024 * 2048) are now computed once for all.\nOf course, this compilation pass is now longer but it is exactly the reason why opcache\nhas been made, and why you should warm up your opcache during your scriptsâ€™ deployment.\nJulien also introduced the parameter opcache.interned-strings-buffer,\nused to configure string interning in Php.\nHe advised to increase its default value (or at least to check relevance),\nbecause even a minimal Symfony application contains a lot of huge DocBlocks (and then huge strings) so the default\nvalue could be underestimated in some cases. Ready to benchmark! :)\n\nWe had also some interesting information about packed arrays optimisations,\nand some string tips (&quot;$a - $b&quot; is better than $a . &#39; - &#39; . $b for example).\nThen he gave a very good overview of all the efforts done on internal Php structures in order to optimize memory access,\nCPU cyclesâ€¦ an impressive work!\n\nTo finish, we have now an (very) approximated date for Php8 release!! Not before 2020â€¦ be patient :)\n\nResources\n\nLink to the speaker presentation: https://www.slideshare.net/jpauli/symfony-live-2017php7performances\n\nDo not hesitate to visit Julienâ€™s blog: https://jpauli.github.io/\n\nAnd many more\n\nWe also liked :\n\n\n  Blog posts about Symfony 4 here : https://fabien.potencier.org/\n  Micro-Services Symfony at Meetic: feedback after 2 years of redesign! https://fr.slideshare.net/meeticTech/php-symfony-microservices-migration-meetictech\n  Introduction to CQRS and Event Sourcing: https://www.slideshare.net/samuelroze/introduction-to-cqrs-and-event-sourcing-74061563\n  Web security: and if we continued to break everything? https://github.com/ninsuo/slides\n\n\nYan can find most of the slides here: https://joind.in/event/symfonylive-paris-2017/schedule/list\n"
} ,
  
  {
    "title"    : "Format all the things",
    "category" : "",
    "tags"     : " prettier, javascript, react, lint, eslint, 6play",
    "url"      : "/2017/03/30/prettier.html",
    "date"     : "March 30, 2017",
    "excerpt"  : "Prettier\n\nPrettier is a brand new javascript library. Its simple goal is to reformat your code. For instance, when I write this:\n\n\nand run prettier on it, Iâ€™ll ultimately get this:\n\n\nYou can try it for yourself at prettierâ€™s website\n\nLooks awesome...",
  "content"  : "Prettier\n\nPrettier is a brand new javascript library. Its simple goal is to reformat your code. For instance, when I write this:\n\n\nand run prettier on it, Iâ€™ll ultimately get this:\n\n\nYou can try it for yourself at prettierâ€™s website\n\nLooks awesome doesnâ€™t it? You donâ€™t need to be a stylish developer anymore! But letâ€™s be honest: not all developers are enthusiastic about this new tool. There are two kinds of developers, those who like when a program helps them to format their code, and those who donâ€™t.\n\nThere are some benefits: using prettier you wonâ€™t waste your time reformatting your code because your destructuring expression overreaches your configâ€™s character cap. You will no longer have conflicts because your colleagues changed indentation on the code youâ€™re currently working on.\n\nBut it also comes with some tradeoffs. The first one is your coding style. We generally add line breaks between chained calls on an API (example here). Prettier will make this fit on one line if it can, and your code will lose some clarity. The same thing goes with the parentheses you add in complex boolean operations (example here). You will have to remember operator priorities.\n\nFor a relatively big project like ours, we prefer adding some consistency to our code in order to avoid wasting our time on solving conflicts, at the expense of losing a little bit of readability and style.\n\nHow to configure it?\n\nWe already have eslint setup in our project with some rule tweaks. So for us, Prettier had to interface smoothly with eslint. Fortunately it comes with a bunch of useful plugins.\n\n\n  eslint-config-prettier: this library disables all eslint rules that conflict with the Prettier formatting. Without it we would need to turn off those rules manually.\n  eslint-plugin-prettier: this library includes Prettier proper formatting as an eslint rule. So if our code is not well formatted, eslint will throw errors. This is the most important plugin for us, as it makes the linting fail if the code is not well formatted. Formatting is now mandatory!\n  prettier-eslint (prettier-eslint-cli): this plugin just runs prettier followed by eslint --fix command. The CLI version allows us to use it from the command line.\n\n\nWith these libraries, we can format all our code base and apply Prettier and eslint rules.\n\nMigration\n\nFirst we had to format the whole repository. This results in a pretty big PR, 670 modified files, +11700 -11113 changesâ€¦ The implication is obvious: if you choose to use Prettier on your project, it had better be set up from the start.\n\nAnyway, once this huge PR was merged, we had to rebase other PRs. You can see it coming: if your rewrite most of your code and rebase other changes on it, there is nearly no way you can avoid conflicts.\nBut in reality itâ€™s (almost) easier than it seems. Since all modifications were generated by Prettier, we can simply discard them and regenerate them after the rebase.\n\nSo, the first thing to do was to rebase on the parent of the format commit in order to resolve all conflicts that were not related to the formatting. To put it differently, we are sure that in next rebase conflicts will only be related to Prettierâ€™s changes.\n\ngit rebase 0404b07~\n# where 0404b07 is the git hash of the format commit parent\n\nAfter that, we rebased our branch on the â€œprettierâ€ commit, and we asked git to automatically keep the conflicting changes from the branch and to discard those from Prettier.\n\ngit rebase 0404b07 -s recursive -X theirs\n\nThen we just needed to re-run Prettier to reformat the rebased code. After this, branches were well formatted and could get back to their normal life-cycle.\n\nHow does it work on daily basis?\n\nFirst, adding the following scripts in the package.json file enables us to use prettier as a yarn (or npm) command.\n\n\n\nThe first line is used to format files provided as parameters and is used in a git pre-commit hook. The second line was there to format the whole codebase and should not be used anymore. This command takes around 1 minute to execute which is a little too long to be used in the development process. Itâ€™s more interesting to plug Prettier in our IDE and only format modified files.\n\nEven though we now enforce a machine-generated code style, everyone is still free to use their favorite IDE with any formatting and syntax settings they like.\nThose using atom or sublime-text can use plugins for the save action (atom plugin here with â€œESLint Integrationâ€ checkbox and sublime-text plugin here). Every saved file will be automatically formatted by Prettier. This is clearly the most comfortable solution.\n\nThose used to applying the format action in Webstorm will have to configure an external tool to do it. Here is a good article to help you setup an external tool if you are interested in this solution.\n\nFinally we wrote a pre-commit hook and added it to our documentation. It automatically runs prettier on all added files from our javascript sources. lint-stage does the same, but we donâ€™t want to force the whole team to use it. Itâ€™s clearly not necessary to run it twice, for those who have a save action which already runs Prettier.\n\nHereâ€™s an example of our pre-commit hook:\n\ngit diff --name-only HEAD | grep -E &quot;src/.*\\.js.?$&quot; | xargs yarn format\n\nIn conclusion\nPrettier is a new tool to add to your chain. Its role is to format the code for you in a very strict way. Thanks to a bunch of plugins that complement it, it also plays nicely with and applies eslint rules. Like we said, there are few sacrifices to make in terms of clarity, but it allows you to stop taking care of things that add no real value to the code you write. It also helps you to reduce meaningless conflicts and debates on â€œhow we should write thisâ€. We plan to use it on all our javascript repositories, for greater consistency and good.\n"
} ,
  
  {
    "title"    : "Nouveau socle pour une nouvelle vie",
    "category" : "",
    "tags"     : " conference, confoo, PHP, Symfony",
    "url"      : "/2017/03/10/confoo-2017-nouveau-socle-nouvelle-vie.html",
    "date"     : "March 10, 2017",
    "excerpt"  : "A travers cette confÃ©rence, je me propose de vous tracer lâ€™histoire de la migration de 6play (systÃ¨me de tÃ©lÃ©vision de rattrapage du groupe M6, premier groupe de tÃ©lÃ©vision privÃ© franÃ§ais) dâ€™une application monolithique vers un univers de micro-se...",
  "content"  : "A travers cette confÃ©rence, je me propose de vous tracer lâ€™histoire de la migration de 6play (systÃ¨me de tÃ©lÃ©vision de rattrapage du groupe M6, premier groupe de tÃ©lÃ©vision privÃ© franÃ§ais) dâ€™une application monolithique vers un univers de micro-service, des avantages en terme de maintenance, dâ€™Ã©volution, de montÃ©e en charge, mais Ã©galement des diffÃ©rents Ã©cueils rencontrÃ©s lors de ce changement de paradigme : caching, logging, complexitÃ© globale.\n"
} ,
  
  {
    "title"    : "L&#39;Ã©quipe Player de 6play.fr au Paris Video Tech",
    "category" : "",
    "tags"     : " video, ott, react, dash, hls, mse, cmaf, 6play, html5",
    "url"      : "/2017/02/20/retour-de-paris-video-tech.html",
    "date"     : "February 20, 2017",
    "excerpt"  : "\n\nPrÃ©sentation du Paris Video Tech\nMercredi 1er fÃ©vrier avait lieu la troisiÃ¨me Ã©dition du Paris Video Tech, un meetup orientÃ© autour de tous les sujets techniques de la vidÃ©o : players HTML5, formats, encodage, distribution, publicitÃ©, â€¦\n\nLâ€™Ã©quip...",
  "content"  : "\n\nPrÃ©sentation du Paris Video Tech\nMercredi 1er fÃ©vrier avait lieu la troisiÃ¨me Ã©dition du Paris Video Tech, un meetup orientÃ© autour de tous les sujets techniques de la vidÃ©o : players HTML5, formats, encodage, distribution, publicitÃ©, â€¦\n\nLâ€™Ã©quipe Player de M6 Web (FrÃ©dÃ©ric Vieudrin, Nicolas Afresne, Malik Baba AÃ¯ssa et Vincent Valot) prÃ©sentait le nouveau player HTML5 de 6play.fr : un player MSE multi-formats, dÃ©veloppÃ© entiÃ¨rement en React, le framework JS quâ€™on ne prÃ©sente plus et qui fait le succÃ¨s du nouveau 6play.fr depuis 2015.\n\nLa rencontre se dÃ©roulait dans les locaux de France TÃ©lÃ©vision Ã  Paris et proposait trois talks :\n\n\n  6play : un player MSE en React par lâ€™Ã©quipe Player de M6Web\n  CMAF DÃ©mystifiÃ© par Cyril Concolato\n  Retour dâ€™ExpÃ©rience de Roland Garros 360 par lâ€™Ã©quipe innovation de France TÃ©lÃ©vision\n\n\n\n\n6play : un player MSE en React\n\nPrÃ©sentation de 6play.fr\nDans la premiÃ¨re partie, nous avons prÃ©sentÃ© le contexte technique de 6play.fr, autour de React, ainsi que les chiffres clÃ©s du site.\n\nAprÃ¨s un rappel de lâ€™historique des players du site de Replay des chaÃ®nes du Groupe M6, nous avons prÃ©sentÃ© les enjeux de la refonte de notre prÃ©cÃ©dent player et Ã©voquÃ© nos contraintes.\n\n\n\nArchitecture du player en React / Redux\n\n\n\n\nEn octobre 2015 sortait le nouveau 6play.fr, une Single Page App dÃ©veloppÃ©e en React-Redux et Isomorphique. Le succÃ¨s de cette refonte nous a poussÃ© Ã  Ã©tudier le refonte du player 6play sur la mÃªme stack technique, historiquement en Video.js.\n\nEn complÃ©ment de lâ€™approche composant proposÃ©e par React, Redux nous a apportÃ© la solution Ã  la gestion de lâ€™Ã©tat du player dans le temps. En effet, son fonctionnement par Ã©vÃ©nements et actions Ã©tait parfaitement adaptÃ© aux Ã©vÃ©nements de la balise &amp;lt;video&amp;gt;.\n\nMedia Engines\nInspirÃ© du systÃ¨me multi-techs de Video.js, nous avons dÃ©veloppÃ© notre propre systÃ¨me de Bridge pilotant les diffÃ©rents SDK Video du marchÃ© : hls.js, dash.js, Adobe Primetime Browser TVSDK, et HTML5.\n\nTous les Bridges communiquent ainsi de la mÃªme maniÃ¨re avec notre player React au travers des MediaEvents HTML5.\n\nIntÃ©gration continue, tests, outils et mÃ©thodes de travail\n\nLes tests automatisÃ©s font partie intÃ©grante des dÃ©veloppements chez M6Web. Tester unitairement nous permet de valider nos classes et mÃ©thodes, tester fonctionnellement assure le bon fonctionnement du player sur plusieurs navigateurs et Ã©vite les rÃ©gressions.\n\nNous utilisons les Webhooks de Github pour executer nos tests, dÃ©ployer un environnement de recette dÃ©diÃ© et nous notifier du statut de la Pull Request dans Slack.\n\nLes slides de notre prÃ©sentation\n\nRevoir lâ€™Ã©vÃ©nement en replay\n\n\n\n"
} ,
  
  {
    "title"    : "Get your brownfield React Native app built on demand",
    "category" : "",
    "tags"     : " mobile, github, ci, react-native",
    "url"      : "/2017/01/31/get-brownfield-react-native-app-built-on-demand.html",
    "date"     : "January 31, 2017",
    "excerpt"  : "As you may know, at M6Web we decided to embrace React Native a few months ago.\nItâ€™s a really exciting piece of software that adds a lot of value in the mobile development ecosystem.\n\nWe already use it for a side project on a standalone app (not pu...",
  "content"  : "As you may know, at M6Web we decided to embrace React Native a few months ago.\nItâ€™s a really exciting piece of software that adds a lot of value in the mobile development ecosystem.\n\nWe already use it for a side project on a standalone app (not public yet, stay tuned!) to record table soccer games, thatâ€™s why, we (mostly @ncuillery ğŸ˜) decided to improve the upgrade process for apps made with the embedded generator. See Nicolasâ€™ blog post on it: Easier Upgrades with React Native.\n\nAs a result, we wanted to start using React Native for our most popular app: 6Play (6play iOS, 6play Android).\nSo they would become what Leland Richardson from Airbnb calls â€œbrownfieldâ€ apps.\n6play is the catchup TV platform for the French TV group M6. It offers live-streaming and full episodes for web, mobile and set-top box. Since the apps launched in 2016, there have been over 1.5 billion videos streamed. Our iOS (mostly Swift) and Android native applications, both important parts of the 6play platform, were exclusively developed externally until now.\n\nWe wanted to use React Native to develop this project in-house and to take advantage of the benefits this hybrid technology could bring into our native apps. Here are just some of the benefits we found when using React Native:\n\n\n  JavaScript development for mobile. We have a lot of awesome JavaScript developers internally who develop the 6play website using React. We love React &amp;amp; Redux and want to mutualize this piece of technology we use on most of the frontends of the 6play platform.\n  Hot fixing with CodePush. For our mobile apps, we want to accomplish the same continuous delivery process we have for the website. CodePush helps us to keep the same flexibility by allowing us to make deployments on a weekly or even daily basis.\n  Knowledge sharing. We would like to be closer to the external development of our mobile apps, which was difficult without native knowledge and without any Android or Swift internal developers. React Native allows us to be part of that, we started working closely with the native team, sharing all developments between the two teams and bringing the best of both worlds (native and web) into the same project.\n  Code Sharing. We also want to share major parts of the mobile code base between apps (Android &amp;amp; iOS). Today, the code bases for each app are completely separate and are managed by two separate teams. With React Native, we could have one common code base while being able to implement specificities for a particular platform if needed. We have also imagined some ways to share code with the 6play website.\n\n\nAs we mentioned in a previous blog post, we use Github pull requests extensively in our development process, especially for testing (automatically and manually) each new commits before merging them into the master branch.\n\nIn the past, we tried to use Appetize to preview  our apps in the browser. It was a first shot, but the functionality was quite limited: animations felt janky, some features wouldnâ€™t work (in-app purchase, video with DRM, â€¦), user identification was painful. We needed a better solution, and as a result we decided to rethink the way we develop the 6play apps.\n\nFor the second iteration of our development process, we had a few simple requirements:\n\n\n  Test each pull request in conditions as close to the reality as possible,\n  Use the same testing workflow for both iOS and Android apps.\n\n\nThis post outlines our new mobile development process for the 6play apps. Weâ€™ll walk through how we manage the environment of a brownfield React Native app, our Git repository structure, our build and release workflows, and how weâ€™ve created a CI environment that mirrors our production environment.\n\nMono Repository / Multi Repositories?\n\nThe first thing we had to do, was to decide how we wanted to organize our Git repositories.\n\nFor this, we looked into how the AirBnb team work with their brownfield app.\n\nWe soon realized we had two options here:\n\nMulti-repositories:\n\n\n  the iOS one\n  the Android one\n  and one for React Native code\n\n\nMono-repository:\n\n\n  One giant repository that has iOS, Android, and React Native folders inside.\n\n\nLetâ€™s take a look at the pros and cons of both solutions.\n\nThe Mono Repository\n\nâ”œâ”€â”€ app-6play/\nâ”‚   â”œâ”€â”€ app-android/\nâ”‚   â”œâ”€â”€ app-ios/\nâ”‚   â”œâ”€â”€ react-native-views/\n\n\nAt first glance, this solution seems like the Holy Grail:\n\n\n  (+) Everything is in the same place\n  (+) If a modification needs both native &amp;amp; React Native developments, changes can be contained in a unique pull request.\n  (-) Code, Documentation, Setup, are more difficult at the beginning (For example, how can we keep Git history of each existing repository?).\n  (-) For our workflow, we need to have everyone working the same way, with the same git workflow, and the same review process. Remember that our native team is an external team (in Belgium), the Android &amp;amp; iOS teams are two different teams (located in the same place) and the React Native one is an internal team (in France). If we succeed, weâ€™ll have synchronous development between teams, this is a really positive point, but it may be difficult to reach.\n  (-) Android, iOS and Javascript CI environments are very different (different tools, different needs), so it is really complex to setup.\n\n\nUltimately, the initial cost of setup and maintenance outweighed the benefits of a mono-repository.\n\nThe Multi Repositories\n\nâ”œâ”€â”€ app-android/\nâ”œâ”€â”€ app-ios/\nâ”œâ”€â”€ react-native-views/\n\n\n\n  (+) Each team could have its own Git workflow, branching model, review process,\n  (+) Each platform has its own CI, code conventions,\n  (-) Building the native apps including the React Native bundle is complicated,\n  (-) Three pull requests (one on each repository) are needed if the functionality includes a native bridge and React Native development.\n\n\nNeither approach was perfect. So we decided to choose the safest one, and create multiple repositories. Also, this choice doesnâ€™t forbid any change of direction toward the mono repository in the futureâ€¦ The reverse seems much more complicated.\n\nDevelopment workflow\n\nEach native developer is now forced to have the react-native-views to be able to work on the native app.\nYou need to know that the native apps need node_modules dependencies of the React Native project, because they also contain the native part of React Native, and maybe some native code for React Native 3rd party you use.\nSo, we will need to clone the native app and the React Native repository.\n\nFor Android\n\ngit clone app-android\ngit clone react-native-views\n\nSo we will have two sibling folders:\n\nâ”œâ”€â”€ app-android/\nâ”œâ”€â”€ react-native-views/\n\n\nWe decided to use symlink to have a cleaner structure (and that will make the CI configuration easy later, see Continuous Integration), so the setup for the Android project will look like this:\n\ncd app-android\nln -s ../react-native-views ./react-native-views\ncd ../react-native-views\nnpm install\n\nâ”œâ”€â”€ app-android/\nâ”‚   â”œâ”€â”€ react-native-views -&amp;gt; ../react-native-views\nâ”œâ”€â”€ react-native-views/\nâ”‚   â”œâ”€â”€ node_modules/\nâ”‚   â”œâ”€â”€ package.json\n\n\nFor iOS\n\nSimilar steps to the Android process, but it seems that Xcode has difficulty following package with a symlink â€¦ so we have to be a little smarter:\n\ngit clone app-ios\ngit clone react-native-views\n\ncd app-ios\nmkdir -p react-native-views/node_modules\ncd ../react-native-views\nln -s ../app-ios/react-native-views/node_modules ./node_modules\nnpm install\n\nWith this method, the node_modules files will be written in the symlink. So those files will be located in the source of the symlink, the app-ios/react-native-views/node_modules directory (This is pretty twisted, we had to admit).\n\nâ”œâ”€â”€ app-ios/\nâ”‚   â”œâ”€â”€ react-native-views/\nâ”‚   â”‚   â”œâ”€â”€ node_modules/\nâ”œâ”€â”€ react-native-views/\nâ”‚   â”œâ”€â”€ node_modules -&amp;gt; ../app-ios/react-native-views/node_modules\nâ”‚   â”œâ”€â”€ package.json\n\n\nReact Native\n\nNow we can choose: JavaScript developers are able to develop on any native app with the React Native packager (npm start in the react-native-views directory) and native developers can develop either with the packager started or with a pre-built React Native bundle (if their developments donâ€™t concern React Native) by switching a Scheme (iOS) or a flavour (Android).\n\nContinuous integration\n\nThe next step was to find a way to improve the mobile development workflow.\nDuring our research, we found a SAAS tool named buddybuild thatâ€™s able to build the iOS &amp;amp; Android apps on each pull request. The setup for the native apps (before the React Native integration) or the React Native side project was really straightforward. It just magically works!\n\nWith the 3 Git repositories of our brownfield apps, itâ€™s a bit more complicated than that. For this, buddybuild provides two useful hooks during the CI process. We just have to add a shell file in the repository:\n\n\n  buddybuild_postclone.sh: This is the hook that happens just after the cloning of the current repository by buddybuild\n  buddybuild_prebuild.sh: This hook is called after postclone and after buddybuild gets all dependencies (npm, Pod, Gradle â€¦), but just before the build starts\n\n\nTo allow our Product Owners to test the appâ€™s functionality, whether itâ€™s related to React Native or not, weâ€™d need:\n\n\n  An iOS build on each pull request on the iOS repository\n  An Android build on each pull request on the Android repository\n  An iOS &amp;amp; Android build on each pull request on the React Native repository\n\n\nTo meet the specific needs of our app development, we required:\n\n\n  For iOS &amp;amp; Android, we need a way to include the React Native code which lies in another repository.\n  For the React Native repository, we need a way to build the iOS &amp;amp; Android apps which lie in other repositories as well, and including the React Native code in it.\n  Our iOS and Android apps up-to-date with both the master branch of the native app, and the master branch of the React Native repository.\n  If a feature needs modifications on both the native code and the React Native code (multiple pull requests, one on each concerned repositories), we want an app synchronized with all repositories.\n\n\nSo letâ€™s dig in these 4 points.\n\nBuild the iOS &amp;amp; Android apps including the React Native bundle\n\nThe key here is to clone the React Native repository in the postclone buddybuild hook and reproduce the directory structure we have in development mode.\n\nfor iOS\n\nbuddybuild_postclone.sh:\n\ngit clone react-native-views\n\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n# Make Xcode able to access to the node dependencies\nln -s react-native-views/node_modules node_modules\n\nbuddybuild_prebuild.sh:\n\n# export React Native bundle:\nnode_modules/.bin/react-native bundle --platform ios --entry-file index.ios.js --bundle-output ../&amp;lt;appFolder&amp;gt;/main.ios.jsbundle --dev false\n\nâ”œâ”€â”€ buddybuild workspace/ (app-ios inside)\nâ”‚   â”œâ”€â”€ react-native-views/\nâ”‚   â”‚   â”œâ”€â”€ package.json\nâ”‚   â”‚   â”œâ”€â”€ node_modules/\nâ”‚   â”œâ”€â”€ package.json -&amp;gt; react-native-views/package.json\nâ”‚   â”œâ”€â”€ node_modules -&amp;gt; react-native-views/node_modules\n\n\nfor Android\n\nbuddybuild_postclone.sh:\n\ngit clone react-native-views\n\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n# When buddybuild will run `npm install`, the node dependencies will be at the right place\nln -s react-native-views/node_modules node_modules\n\nbuddybuild_prebuild.sh:\n\n# export React Native bundle:\nnode_modules/.bin/react-native bundle --platform android --entry-file index.android.js --bundle-output ../&amp;lt;appFolder&amp;gt;/main.android.jsbundle --dev false\n\nâ”œâ”€â”€ buddybuild workspace/ (app-android inside)\nâ”‚   â”œâ”€â”€ react-native-views/\nâ”‚   â”‚   â”œâ”€â”€ package.json\nâ”‚   â”‚   â”œâ”€â”€ node_modules/\nâ”‚   â”œâ”€â”€ package.json -&amp;gt; react-native-views/package.json\nâ”‚   â”œâ”€â”€ node_modules -&amp;gt; react-native-views/node_modules\n\n\nThe only thing you have to do in the buddybuild dashboard is to create the app for each platform and activate the build on pull request only (see screenshot below). Buddybuild will automatically trigger an iOS &amp;amp; Android build on each pull request for the native repositories.\n\n\n\nBuild the iOS &amp;amp; Android apps on each pull request from the React Native repository\n\nNow, weâ€™d like to easily test each react-native-views pull request on both iOS and Android apps.\n\nFor that purpose, we used the buddybuild hook again. Here is the buddybuild_postclone.sh:\n\n# Create a react-native-views folder\nmkdir react-native-views\n# Move everything in it\nmv * react-native-views\n\n# The postclone hook is ran by buddybuild for both iOS and Android builds. We distinguish the platform here, thanks to the env variable BUDDYBUILD_APP_ID (set by buddybuild)..\nif [ &quot;$BUDDYBUILD_APP_ID&quot; = &quot;&amp;lt;buddybuildAndroidAppID&amp;gt;&quot; ]; then\ngit clone app-android\ncd app-android\nelse\ngit clone app-ios\ncd app-ios\nfi\n\n# Move the native app to the root of the workspace\nmv * ..\ncd ..\n\n# Create the future node_modules location folder\nmkdir -p react-native-views/node_modules\n# Create the symbolic link for the app to be able to found the node_modules at the good place\nln -s react-native-views/node_modules node_modules\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n\nFor iOS, youâ€™ll have:\n\nâ”œâ”€â”€ buddybuild workspace/ (app-ios inside)\nâ”‚   â”œâ”€â”€ react-native-views/\nâ”‚   â”‚   â”œâ”€â”€ package.json\nâ”‚   â”‚   â”œâ”€â”€ node_modules/\nâ”‚   â”œâ”€â”€ package.json -&amp;gt; react-native-views/package.json\nâ”‚   â”œâ”€â”€ node_modules -&amp;gt; react-native-views/node_modules\n\n\nFor Android, youâ€™ll have:\n\nâ”œâ”€â”€ buddybuild workspace/ (app-android inside)\nâ”‚   â”œâ”€â”€ react-native-views/\nâ”‚   â”‚   â”œâ”€â”€ package.json\nâ”‚   â”‚   â”œâ”€â”€ node_modules/\nâ”‚   â”œâ”€â”€ package.json -&amp;gt; react-native-views/package.json\nâ”‚   â”œâ”€â”€ node_modules -&amp;gt; react-native-views/node_modules\n\n\nBy doing that, buddybuild will automatically install the npm dependencies, then launch the same prebuild hook as the native repository to build the React Native bundle.\n\nUsing buddybuild, you can create the app for each platform, and trigger new builds only when pull requests are opened, or when commits are added to existing pull requests. Buddybuild also builds both apps when React Native pull requests are opened as well.\n\nWhen master of React Native change, update the master iOS &amp;amp; Android apps\n\nBuddybuild makes it very easy to trigger a build programmatically via the API. We also use Jenkins for unit tests and lint, so we have a job triggered every time a push is made on the master branch of react-native-views. We have reused this job and append the following:\n\n# Our credentials\nACCESS_TOKEN_BB=&amp;lt;AccessToken&amp;gt;\nAPP_ID_BB_IOS=&amp;lt;buddybuildiOSAppID&amp;gt;\nAPP_ID_BB_ANDROID=&amp;lt;buddybuildAndroidAppID&amp;gt;\n\n# Build iOS\ncurl -X POST -H  &#39;Authorization: Bearer &#39;$ACCESS_TOKEN_BBâ€ -d &#39;branch=masterâ€™ &#39;https://api.buddybuild.com/v1/apps/&#39;$APP_ID_BB_IOS&#39;/build&#39;\n\n# Build Android\ncurl -X POST -H  &#39;Authorization: Bearer &#39;$ACCESS_TOKEN_BBâ€ -d &#39;branch=masterâ€™ &#39;https://api.buddybuild.com/v1/apps/&#39;$APP_ID_BB_ANDROID&#39;/build&#39;\n\nNow, you can activate the master build on the native iOS &amp;amp; Android buddybuild build, and youâ€™ll have those apps up-to-date with the master branch.\n\n\n\nCross platform feature (both native &amp;amp; React Native)\n\nAt this point, this is not enough, because if you develop a feature that needs native and React Native modifications, you will not have the corresponding app before merging everything.\n\nWe have decided here to add a rule: for a â€œcross platform featureâ€ (like a bridge for a native component for example), we have to define the same name for the branches in each repositories.\n\nA bridge for a native component (the authentication bridge as an example) would have three Git branches with the same name, and three pull requests (one on each repository).\n\nBy following this convention, we only have to checkout that branch when we clone the external repository in our postclone hooks:\n\n{\n  # Detect with the env variable BUDDYBUILD_BRANCH (given by buddybuild) the branch we are on.\n  echo &quot;Git checkout branch: $BUDDYBUILD_BRANCH&quot;\n  git checkout $BUDDYBUILD_BRANCH\n} || {\n  echo &quot;Git default branch: master&quot;\n  git checkout master # if master is the name of your default branch\n}\n\nWe do that branch name checking on the three repositories. This way, the four buddybuild projects (app-ios, app-android, react-native-views-ios, and react-native-views-android) can build native applications with modification on both sides.\n\nConclusion\n\nThanks to React Native and buddybuild, we now have a complete workflow as powerful as we have on the website. Being able to review either React Native or native code, and testing a real app before the code lands on the master branch is a big improvement for code quality and a huge step forward towards more agility.\n\nBig up to Tapptic Team, M6Web React Native team for this work, to the buddybuild support team for the help when needed.\n\nSpecial thanks to Nicolas Cuillery and Alysha for their proofreading!\n"
} ,
  
  {
    "title"    : "Une donnÃ©e presque parfaite sur 6play",
    "category" : "",
    "tags"     : " lyon, conference, elasticsearch, video",
    "url"      : "/2016/11/24/une-donnee-presque-parfaite.html",
    "date"     : "November 24, 2016",
    "excerpt"  : "Benoit Viguier, prestataire de la sociÃ©tÃ© Elao pour M6Web, a fait un retour dâ€™expÃ©rience au Forum PHP de lâ€™AFUP sur lâ€™architecture technique mise en place autour de la mise Ã   disposition des donnÃ©es nÃ©cessaires Ã  6play.\n\n\n\nLes slides sont Ã©galeme...",
  "content"  : "Benoit Viguier, prestataire de la sociÃ©tÃ© Elao pour M6Web, a fait un retour dâ€™expÃ©rience au Forum PHP de lâ€™AFUP sur lâ€™architecture technique mise en place autour de la mise Ã   disposition des donnÃ©es nÃ©cessaires Ã  6play.\n\n\n\nLes slides sont Ã©galement disponibles en PDF.\n"
} ,
  
  {
    "title"    : "EnquÃªte exclusive au coeur de la technique de 6play. Les slides.",
    "category" : "",
    "tags"     : " conference",
    "url"      : "/2016/11/03/blendwebmix-6play-conference.html",
    "date"     : "November 3, 2016",
    "excerpt"  : "Voici les slides de la confÃ©rence â€œPlus dâ€™un milliard de vidÃ©os vues par an sur 6play - EnquÃªte exclusive au coeur de la techniqueâ€ que nous avons donnÃ© le 2 novembre 2016 lors de la confÃ©rence Blend Web Mix Ã  Lyon.\n\nhttps://docs.google.com/presen...",
  "content"  : "Voici les slides de la confÃ©rence â€œPlus dâ€™un milliard de vidÃ©os vues par an sur 6play - EnquÃªte exclusive au coeur de la techniqueâ€ que nous avons donnÃ© le 2 novembre 2016 lors de la confÃ©rence Blend Web Mix Ã  Lyon.\n\nhttps://docs.google.com/presentation/d/1BZGvoiubQsIzVjH9Px22wQyYmkboXZjpiubX2UtkMA4/edit?usp=sharing\n\nNous Ã©tions ravis dâ€™Ãªtre sponsor de cet Ã©venement. Merci encore Ã  toute lâ€™organisation.\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity New York Conference 2016",
    "category" : "",
    "tags"     : " conference, velocity, webperf, devops",
    "url"      : "/2016/10/12/velocity-nyc-2016.html",
    "date"     : "October 12, 2016",
    "excerpt"  : "Nous Ã©tions cette annÃ©e Ã  New York, Ã  quelques blocs de Time Square, pour suivre lâ€™Ã©dition New Yorkaise de la Velocity Conference 2016.\nCâ€™est une confÃ©rence que nous apprÃ©cions particuliÃ¨rement et Ã  laquelle nous nous rendons quasiment chaque annÃ©...",
  "content"  : "Nous Ã©tions cette annÃ©e Ã  New York, Ã  quelques blocs de Time Square, pour suivre lâ€™Ã©dition New Yorkaise de la Velocity Conference 2016.\nCâ€™est une confÃ©rence que nous apprÃ©cions particuliÃ¨rement et Ã  laquelle nous nous rendons quasiment chaque annÃ©e, soit dans son Ã©dition europÃ©enne (Berlin, Londres, Barcelone, et Amsterdam cette annÃ©e en novembre), soit aux U.S. (prÃ©cÃ©demment Santa Clara, New York cette annÃ©e, et San JosÃ© lâ€™annÃ©e prochaine).\nCâ€™est lâ€™occasion de suivre une conf de trÃ¨s haute qualitÃ© composÃ©e de 4 ou 5 tracks en parallÃ¨le, dÃ©diÃ©e aux problÃ©matiques de performance et de scalabilitÃ©.\nOn remarque que dâ€™annÃ©e en annÃ©e la confÃ©rence sâ€™est rÃ©orientÃ©e autour du mouvement DevOps, alors quâ€™elle Ã©tait prÃ©cÃ©demment beaucoup plus centrÃ©e sur la WebPerf (desktop et mobile).\n\nLa confÃ©rence commence par lâ€™Ignite (sorte de mini confÃ©rence dans la confÃ©rence), basÃ©e sur un format court (type Lightning Talk) de 5 minutes pour une prÃ©sentation de 5 slides dÃ©filant automatiquement. On retiendra de cette premiÃ¨re partie des talks intÃ©ressants exposant les tristes chiffres de la diversitÃ© dans la tech aux US, mais aussi une confÃ©rence trÃ¨s drÃ´le de @beldhalpern de @ThePracticalDev sur des parodies des livres OReilly (voir le O RLY Cover Generator) :\n\nEnjoying the heck out of @ThePracticalDev at Ignite #velocityconf. Lololol! pic.twitter.com/ZlJWoP4cjh&amp;mdash; Bridget Kromhout (@bridgetkromhout) 20 septembre 2016\n\n\nLâ€™ignite sâ€™est fini sur le cÃ©lÃ¨bre Ignite KaraokÃ© oÃ¹ 16 volontaires se sont prÃªtÃ©s au jeu de cet exercice hilarant mais tellement difficile, consistant Ã  improviser une confÃ©rence sur le sujet de son choix sur 5 slides inconnues de lâ€™orateur et qui dÃ©filent automatiquement au bout de quelques secondes ğŸ˜ƒ. Ce quâ€™on fait aussi chez M6Web de temps en temps nommÃ© KaraokÃ© Slideshow et que Kenny avait animÃ© lors dâ€™un Forum PHP (voir la vidÃ©o).\n\n\nCrÃ©dit : Flickr\n\nNous avons ensuite suivi deux jours de confÃ©rences dont les thÃ¨mes majeurs Ã©taient :\n\n\n  Les Service Workers\n  Les microservices\n  Le monitoring\n  HTTP2\n  La sÃ©curitÃ© des apps\n  Les dÃ©tections dâ€™anomalie\n  Les ChatOps\n  Le WebMobile, AMP et les PWA\n\n\nChatOps\n\nUn des sujets assez rÃ©current, notamment dans la mouvance DevOps est lâ€™utilisation des ChatOps, sujet popularisÃ© par Github (via Hubot).\nCela consiste gÃ©nÃ©ralement en un bot ou une IA posÃ©e sur un outil de Chat type Slack, Flowdock ou Hipchat, permettant de simplifier la communication entre diffÃ©rentes Ã©quipes et les diffÃ©rents outils (ticketing, alerting, monitoring, Ã©tat dâ€™une machine, etc). Une dÃ©mo de lâ€™IA de Dynatrace Ã  reconnaissance vocale Ã  Ã©tÃ© faite, montrant comment par la voix, on pouvait recevoir dans lâ€™outil de Chat les infos sur les incidents de la veille, crÃ©er les tickets de support etc. Voir ici. Un peu gadget, mais rigolo.\n\nLâ€™un des points Ã  retenir, câ€™est que mÃªme si ces outils font partie de la Â« culture Â» DevOps, ce nâ€™est pas lâ€™ajout dâ€™un de ces outils qui fera apparaÃ®tre cette culture dans votre entreprise si vous ne lâ€™avez pas.\n\n\n  Tools will not fix a broken culture\n\n\n\nCrÃ©dit : Flickr\n\nLe WebMobile, AMP, et les PWA\n\nPlusieurs confÃ©rences avaient pour but de comparer ce que lâ€™on pouvait obtenir de nos jours via du WebMobile versus ce que lâ€™on a sur les apps natives. Le fossÃ© sâ€™est Ã©normÃ©ment rÃ©trÃ©ci et les WebApps ont dÃ©sormais accÃ¨s Ã  la plupart des fonctionnalitÃ©s prÃ©sentes cÃ´tÃ© natif :\n\n\n  Notifications\n  Ajout sur le Home Screen de lâ€™icone\n  Full Screen\n  Orientation\n  Gestion hors ligne\n  â€¦\n\n\nCe qui nous amÃ¨ne aux Progressive Web Apps : PWA\n\nPete Lepage @petele de chez Google nous a notamment prÃ©sentÃ© des projets open-source de Google pour mettre en place diffÃ©rentes politiques de cache via les Â« serviceWorkers Â» (voir https://developers.google.com/web/tools/service-worker-libraries/), ainsi que les futures api : Web Payments, Credential Management â€¦\n\n\n  Les slides.\n  Exemple de la PWA du Washington Post\n\n\nToujours sur la partie mobile, Malte Ubl (@cramforce), core dÃ©veloppeur de AMP, nous a prÃ©sentÃ© le futur de ce protocole de Google pour offrir des pages plus rapides pour la consultation de site mÃ©dia sur mobile.\n\n\n  AMP is a web component library, validator and caching layer for reliably fast web content at scale\n\n\nEn commenÃ§ant par un bilan dâ€™AMP, 3000 PR + 200 contributeurs (au bout dâ€™un an seulement !), Malte nous a expliquÃ© quâ€™un site mobile trÃ¨s optimisÃ© pouvait logiquement Ãªtre plus performant quâ€™AMP.\n\nArriveront prochainement sur AMP, le support des formulaires, des optimisations avancÃ©es dâ€™images via le Google AMP Cache, des Service Workers pour AMP pour ne jamais tÃ©lÃ©charger AMP dans le Â« chemin critique Â» du chargement de la page.\n\nUn petit focus a aussi Ã©tÃ© fait sur les PWA et AMP avec amp-install-serviceworker qui est un Service Worker permettant dâ€™installer la PWA aprÃ¨s chargement de AMP, pour faire une upgrade transparente de AMP vers une PWA (Voir une dÃ©mo ici choumx.github.io/amp-pwa)\n\n\n  AMP : Â« Start Fast, Stay Fast Â»\n\n\nNous avons aussi vu une confÃ©rence sur lâ€™optimisation de la consommation des webApps en terme de CPU / temps de rÃ©ponse, notamment via lâ€™Ã©tude des capacitÃ©s JS de chacun des devices/OS avec le benchmark JetStream Javascript.\n\nOn dÃ©couvre notamment que lâ€™iPhone 7 a des capacitÃ©s assez impressionnantes, contrairement Ã  lâ€™iPhone 5C, que le mode Â« Ã©conomie dâ€™Ã©nergie Â» ou encore une bonne insolation rendent les devices beaucoup moins performants. Dâ€™excellentes slides Ã  voir ici : hearne.me/2hot\n\nWebPerf\n\nCÃ´tÃ© WebPerf, peu de grosses nouveautÃ©s, on retiendra @nparashuram qui nous a montrÃ© comment automatiser le â€œprofilingâ€ des ChromeDevTools dans Node.js via ChromeDriver !\n\nPlus dâ€™infos ici : https://blog.nparashuram.com/2016/09/rise-of-web-workers-nationjs.html\n\nTammy Everts (@tameverts de Soasta) et Pat Meenan (@patmeenan de Google et crÃ©ateur de WebPageTest) nous ont fait un gros retour basÃ© sur toutes les mÃ©triques rÃ©coltÃ©es par Soasta mPulse (outils de Real User Monitoring en SAAS) afin de dÃ©terminer des corrÃ©lations entre les temps de chargement et dâ€™autres mÃ©triques (taux de rebond, conversion, etc.) grÃ¢ce Ã  lâ€™application de concept propre au Machine Learning sur une quantitÃ© Ã©norme de data. Toujours intÃ©ressant.\n\nSlides ici : https://conferences.oreilly.com/velocity/devops-web-performance-ny/public/schedule/detail/51082\n\n\nCrÃ©dit : Flickr\n\nCÃ´tÃ© Single Page App, le Server Side Rendering est revenu Ã  plusieurs reprises afin dâ€™avoir des SPA performantes dont le premier rendu est gÃ©nÃ©rÃ© cÃ´tÃ© serveur, ce que permet nativement React, et dÃ©sormais Ember et Angular 2. Voir notre article sur lâ€™isomorphisme.\n\nCotÃ© HTTP, on retiendra Hooman Beheshti qui nous a fait un retour dâ€™expÃ©rience sur HTTP2. AprÃ¨s une explication des nouveautÃ©s du protocole (binary, single, long-lasting TCP connection, streams encapsulation, frames, bi-directionalâ€¦), une comparaison avec HTTP 1 nous a Ã©tÃ© exposÃ©e. En conclusion, HTTP2 est complexe et la migration nâ€™est pas une simple modification de paramÃ¨tre. Bien que cette nouvelle version est lÃ©gÃ¨rement plus rapide, en particulier sur un rÃ©seau lent (&amp;lt;1Mbps), le protocole supporte trÃ¨s mal les pertes de paquets ou les fortes congestions Ã  cause de lâ€™unique connexion (TCP slow start). La recommandation est de tester sur chaque site et dâ€™optimiser ses pages selon la version dâ€™HTTP utilisÃ©e. Une piste serait HTTP2 over UDP.\n\nLes slides\n\nDevOps\n\nDe nombreuses confÃ©rences avaient pour objectif dâ€™aborder les bienfaits du DevOps et plus largement les bonnes pratiques liÃ©es au mouvement afin de gagner en qualitÃ© et fiabilitÃ©.\n\nOn retiendra notamment la confÃ©rence de Cornelia Davis (DevOps: Who does what?) explicitant les diffÃ©rents rÃ´les dans un SDLC (Software Development Life-Cycle) et leur rÃ©partition en Ã©quipe dans lâ€™organisation.\n\nLes rÃ´les dans le SDLC :\n\n\n  Architecte : Ent Archi, Biz Analyst, Portfolio Mgmt\n  SCO : Info sec\n  Infra : Srv Build, Cap Plan, Network, Ops\n  Middleware/AppDev : Middleware Eng, SW Arch, SW Dev, Client SW Dev, Svc Govern\n  Data : Data Arch, DBA\n  Biz : Prod Mgmt\n  Ent Apps : DCTM (Documentum) Eng.\n\n\nLa rÃ©partition en Ã©quipe proposÃ©e :\n\nPlatform (unique / transverse) :\n\n\n  Middleware/AppDev : Middleware Eng, Svc Govern\n  Infra : Srv Build, Cap Plan, Network, Ops\n  SCO : Info sec\n  Data : DBA\n\n\nCustomer Facing App (de 1 Ã  n Ã©quipes)\n\n\n  Middleware/AppDev : SW Arch, SW Dev, Client SW Dev\n  Data : Data Arch\n  Infra : Cap Plan, Ops\n  Biz : Prod Mgmt\n  Architecte : Biz Analyst\n\n\nEnablement (unique / transverse) :\n\n\n  Architecte : Ent Archi, Portfolio Mgmt\n\n\nDCTM - Documentum (Enterprise Content Management Platform) (unique / transverse) :\n\n\n  Infra : Ops, Cap Plan\n  Ent Apps : DCTM (Documentum) Eng.\n\n\nOn notera notamment la prÃ©sence dâ€™Ops dans les Ã©quipes Customer Facing App et inversement de Middleware Eng dans lâ€™Ã©quipe Platform.\n\nDe mÃªme, la prÃ©sence dâ€™architectes transverses (enablement) permet de garder une architecture cohÃ©rente. (Pas de slides disponibles pour cette confÃ©rence)\n\n\nCrÃ©dit : Flickr\n\nMicroservices\n\nLa confÃ©rence de @susanthesquark, axÃ©e sur les microservices, rappelait quelques bonnes pratiques :\n\n\n  Architecture sans SPOF\n  Ne pas laisser la dette technique sâ€™accumuler\n  DÃ©ploiement continu\n  Travail en Ã©quipe entre Dev / PM / SRE\n  Monitoring\n  ProcÃ©dures standard de gestion des incidents\n  Post-mortem pour apprendre de ses erreursâ€¦\n\n\nLes slides\n\nConcernant le monitoring des microservices, la confÃ©rence de Reshmi Krishna @reshmi9k sâ€™intÃ©ressait Ã  lâ€™analyse de la latence, inhÃ©rente Ã  ce type dâ€™architecture. La principale technique proposÃ©e est celle du suivi dâ€™une requÃªte de bout en bout, grÃ¢ce notamment Ã  lâ€™outil Zipkin. De mÃªme, une gestion des timeouts globale (pour chaque requÃªte pour tous les microservices) et dynamique (selon le contexte) permet de maÃ®triser les problÃ¨mes en cas de ralentissement dâ€™un service en particulier.\n\nLes slides\n\nSÃ©curitÃ©\n\nConcernant la sÃ©curitÃ©, la confÃ©rence de Kelly Lum @aloria, passait en revue le minimun vital :\n\n\n  La sÃ©curitÃ© doit Ãªtre pensÃ©e dÃ¨s la conception\n  Permettre aux utilisateurs de reporter facilement des problÃ¨mes de sÃ©curitÃ© et Ãªtre Ã  lâ€™Ã©coute des rÃ©seaux sociaux\n  Toujours remercier les utilisateurs signalant les failles\n  Avoir une Ã©quipe testant rÃ©guliÃ¨rement la sÃ©curitÃ© (Crack Team).\n  En cas de failles de sÃ©curitÃ©, aprÃ¨s correction, toujours analyser les causes et apprendre de ses erreurs.\n\n\nLes slides\n\nConclusion\n\nVous pouvez retrouver la plupart des slides ici.\net voir les vidÃ©os de certaines confÃ©rences ici.\nou ici.\n\nLes photos officielles de la conf sont ici.\n"
} ,
  
  {
    "title"    : "Use the Sensiolabs Security Checker to check potential vulnerabilities on Symfony projects",
    "category" : "",
    "tags"     : " 6tech, lyon, symfony, security, php, jenkins",
    "url"      : "/2016/07/20/sf2-security-checker.html",
    "date"     : "July 20, 2016",
    "excerpt"  : "Numerous vulnerabilities are detected every day. Thatâ€™s a good thing and a key benefit of using open source products. At m6web we donâ€™t want to be exposed to known vulnerabilities, so we use a service provided by Sensiolabs in our continuous integ...",
  "content"  : "Numerous vulnerabilities are detected every day. Thatâ€™s a good thing and a key benefit of using open source products. At m6web we donâ€™t want to be exposed to known vulnerabilities, so we use a service provided by Sensiolabs in our continuous integration tool (Jenkins) to check it.\n\nJust add those lines in your ant build file (and adapt basedir) :\n\n    &amp;lt;!-- =================================================================== --&amp;gt;\n    &amp;lt;!-- Security checker                                                    --&amp;gt;\n    &amp;lt;!-- =================================================================== --&amp;gt;\n    &amp;lt;target name=&quot;sf2-security-checker&quot;&amp;gt;\n     &amp;lt;exec executable=&quot;bash&quot; dir=&quot;${basedir}/sources/bin&quot; failonerror=&quot;true&quot;&amp;gt;\n         &amp;lt;arg value=&quot;-c&quot;/&amp;gt;\n         &amp;lt;arg value=&quot;curl -Os https://get.sensiolabs.org/security-checker.phar&quot; /&amp;gt;\n     &amp;lt;/exec&amp;gt;\n     &amp;lt;exec executable=&quot;php&quot; dir=&quot;${basedir}/sources&quot; failonerror=&quot;true&quot;&amp;gt;\n         &amp;lt;arg line=&quot;${basedir}/sources/bin/security-checker.phar security:check composer.lock&quot; /&amp;gt;\n     &amp;lt;/exec&amp;gt;\n    &amp;lt;/target&amp;gt;\n\n\nAnd automatically check your composer.lock againts vulnerabilities. Your build will fail if something wrong is detected.\n\nFor example, with the recent Guzzle one :\n\n\n\nYou can contribute to the vulnerabilities database and the checker via Github.com.\n"
} ,
  
  {
    "title"    : "Retour d&#39;expÃ©rience sur l&#39;utilisation de Cassandra sur 6play en vidÃ©o",
    "category" : "",
    "tags"     : " 6tech, lyon, conference, cassandra, video",
    "url"      : "/2016/07/04/rex-cassandra.html",
    "date"     : "July 4, 2016",
    "excerpt"  : "\n\nErratum : dans les phases de questions rÃ©ponses, jâ€™annonce une phase de test Ã  10K RPS (requÃªtes par seconde) ; il sâ€™agissait de RPM (requÃªtes par minute).\n\nLors du match Suisse vs France, diffusÃ© sur M6 pendant la coupe dâ€™Europe de football, la...",
  "content"  : "\n\nErratum : dans les phases de questions rÃ©ponses, jâ€™annonce une phase de test Ã  10K RPS (requÃªtes par seconde) ; il sâ€™agissait de RPM (requÃªtes par minute).\n\nLors du match Suisse vs France, diffusÃ© sur M6 pendant la coupe dâ€™Europe de football, la brique users est montÃ©e Ã  75K RPM (soit 1200 rps) et 84K pour Islande vs France.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Migrate smoothly your Flux isomorphic app to Redux",
    "category" : "",
    "tags"     : " react, flux, redux, fluxible, isomorphic, javascript",
    "url"      : "/2016/07/04/migrate-smoothly-flux-isomorphic-app-to-redux.html",
    "date"     : "July 4, 2016",
    "excerpt"  : "Flux, history remindersâ€¦\n\nÂ« Flux is the application architecture that Facebook uses for building client-side web applications. Â» Thatâ€™s the definition of Flux on the Facebook website. So Flux is just a pattern, not a framework, that goes well with...",
  "content"  : "Flux, history remindersâ€¦\n\nÂ« Flux is the application architecture that Facebook uses for building client-side web applications. Â» Thatâ€™s the definition of Flux on the Facebook website. So Flux is just a pattern, not a framework, that goes well with React, but not only. The model is focused on user interactions. Its main strength is the unidirectional data flow that enforces developers to be careful and ensures code consistency when application grows up.\n\nSeveral libraries propose tools to implement Flux pattern easily. If no one stood out from the crowd at the beginning, now Redux, created by Dan Abramov, is clearly the one that the community have chosen as you can see below. Most Flux based React start kits you can find are based on Redux.\n\n\n\nAt M6Web, our 6play web application is not designed with Redux, but we use Fluxible. Fluxible is another Flux library, developed by Yahoo. We chose it back in December 2014, when we started the project, because Fluxible was at the time one of the few tool designed for isomorphic applications. Moreover it was already used in production by Yahoo.\n\nWhy do we think Redux is a better choice\n\nEven though Fluxible did get the job done, we are now willing to upgrade our application to Redux. Why?\n\n\n  The popularity of Redux will certainly affect other libraries life and support in the future, maybe Fluxible will be concerned. Fluxible is only supported by a firm and not really by the community.\n  Fluxible has a powerful but complex structure based on contexts and plugins. This can be useful, however when new developers come on the project, this is not always easy to understand. We are always searching to make code simpler for maintainability and we think that Redux is a better alternative than Fluxible on this topic.\n  For a given feature, developers write less code using Redux because the design is very simple, there is no extra boilerplate, the flow is condensed as much as possible. As a consequence, unit tests are easier to write.\n  There are very useful tools about Redux that make Developer eXperience better. For instance, the Redux DevTools allows to time travel live between Flux events. The middleware concept extending capabilities of actions is also interesting.\n  We are beginning to make our development processes converge. Every new React project here starts on Redux, including the Proof Of Concept we made with React Native. Using the same libraries made code sharing easier for us.\n\n\nMigrating a big app from Fluxible to Redux is crazy, isnâ€™t it?\n\n6play is a very big web application. How to migrate to Redux in a reasonable amount of time and without risk?\n\nWe were quite sure that Redux and Fluxible could work together. The goal would be to migrate gradually to Redux without having to remove Fluxible in one giant step. First of all, because we canâ€™t mobilise enough resources to do this in a relatively short time. Secondly, we want to avoid a big deploy in production and potentially critical bugs (even though our application is well tested, there are always cases that we canâ€™t control like memory load for example).\n\nWe tried itâ€¦ And we succeeded! And this is quite simple.\n\nFirst, we define the store configuration like other Redux application.\n\n// configureStore.js\n\nimport {createStore, combineReducers, applyMiddleware, compose} from &#39;redux&#39;;\nimport thunk from &#39;redux-thunk&#39;;\nimport {canUseDOM} from &#39;fbjs/lib/ExecutionEnvironment&#39;;\n\nimport myReducer1 from &#39;./modules/myModule1/myModule1.reducer&#39;;\nimport myReducer2 from &#39;./modules/myModule2/myModule2.reducer&#39;;\n\nexport default initialState =&amp;gt; createStore(\n  combineReducers({myReducer1, myReducer2}),\n  initialState,\n  compose(\n    applyMiddleware([thunk]),\n    canUseDOM &amp;amp;&amp;amp; window.devToolsExtension ? window.devToolsExtension() : f =&amp;gt; f\n  )\n);\n\nThen we initialize Redux store in our server file. For isomorphic purposes, we have to serialize storesâ€™ state and give it to the html so that client side can take control of the application with serverâ€™s data. So here, we build data for the client by combining Redux and Fluxible states.\n\n// server.js\n\nimport {provideContext} from &#39;fluxible-addons-react&#39;;\nimport {match, RouterContext} from &#39;react-router&#39;;\nimport configureStore from &#39;./configureStore&#39;;\n\nprocessAppRequest() {\n  // ...\n\n  const fluxibleContext = FluxibleApp.createContext();\n  const reduxStore = configureStore(initialState);\n\n  match({routes: FluxibleApp.getComponent(), location: url}, (error, redirectLocation, routerState) =&amp;gt; {\n    // ...\n\n    // Original Fluxible root element\n    const rootElement = React.createElement(\n      provideContext(RouterContext, customContextTypes),\n      {...routerState, context: fluxibleContext.getComponentContext()}\n    );\n\n    // Now with Redux\n    const markup = ReactDOMServer.renderToString(\n      React.createElement(Provider, {store: reduxStore}, rootElement)\n    );\n\n    // Build state for client\n    const finalState = {\n      ...FluxibleApp.dehydrate(fluxibleContext),\n      reduxStoreState: reduxStore.getState()\n    };\n\n    // Then build the response layout with the markup and the whole state as usual\n    // ...\n  }\n}\n\nOn client side, we do the opposite operation.\n\n// client.js\n\nimport {provideContext} from &#39;fluxible-addons-react&#39;;\nimport {Router, browserHistory} from &#39;react-router&#39;;\nimport configureStore from &#39;./configureStore&#39;;\n\nconst dehydratedState = window[stateVarName];\nconst reduxStore = configureStore(dehydratedState.reduxStoreState);\n\n// Fluxible rehydrate its state\napp.rehydrate(dehydratedState, (error, fluxibleContext) =&amp;gt; {\n  // ...\n\n  // Original Fluxible root element\n  const rootElement = React.createElement(provideContext(Router, customContextTypes), {\n    history: browserHistory,\n    routes: app.getComponent(),\n    context: fluxibleContext.getComponentContext()\n  });\n\n  // Now with Redux\n  ReactDOM.render(\n    React.createElement(Provider, {store: reduxStore}, rootElement),\n    document.getElementById(rootId)\n  );\n});\n\nAnd thatâ€™s it! We can now use Redux in our component as usual, in combination with Fluxible. We can define actions and reducers for new features (instead of using Fluxible) but we can also transform progressively some Fluxible stores and actions into Redux flow, this is very easy. API requests stay in actions but data processing moves to reducers. Then data sorting and filtering logic in Fluxible stores moves to selectors.\n\nComponents connection to stores\n\nTwo files to rule them all\n\nWith Fluxible, we linked components with stores through connectToStore in the same file and exported only the connected component. But we think now it is a bad practice:\n\n\n  splitting data fetching from stores and display logic is interesting for maintainability and code understanding,\n  it is much easier to unit test the component without the connection to store, connectToStores (Fluxible) or connect (Redux) methods are parts of a 3rd-party library, and we donâ€™t need to test it.\n\n\nFrom now on, components are files named *.component.js and stores connections are in *.connector.js files in the same folder. We can link a component both with Redux and Fluxible stores.\n\n// myComponent.connector.js\n\nimport MyComponent from &#39;./myComponent.component&#39;;\n\n// Stores\nimport connectToStores from &#39;fluxible-addons-react/connectToStores&#39;;\nimport {connect} from &#39;react-redux&#39;;\nimport MyFluxibleStore from &#39;../stores/myFluxible.store&#39;;\n\n// Utils\nimport {getSomeDataFromState} from &#39;../myModule.selectors&#39;;\n\n// Redux\nexport const mapStateToProps = (state, props) =&amp;gt; {\n  return {dataFromRedux: getSomeDataFromState(state, props.myProps2)};\n};\n\n// Fluxible\nexport default connectToStores(\n  connect(mapStateToProps)(MyComponent),\n  [MyFluxibleStore],\n  (context, props) =&amp;gt; ({\n    dataFromFluxible: context.getStore(MyFluxibleStore).getSomeData(props.myProps1)\n  })\n);\n\nWe export mapStateToProps function because in a few cases it contains logic that may be interesting to unit test.\n\nStores connections order\n\nIn this example, the link to Fluxible store is higher in components tree than the Redux one as we can see below.\n\n\n\nIt means that if Redux state changes, the Fluxible wrapper component wonâ€™t be reloaded but in the reverse case, both Fluxible and Redux wrapper components will rerender. In most scenarios, it doesnâ€™t matter. Connection order of Redux and Fluxible is significant in two situations:\n\n\n  If one connection depends on data stored in the other library state, it has to be lower in components tree.\n\n\n// myComponent.connector.js\n\nimport MyComponent from &#39;./myComponent.component&#39;;\n\n// Stores\nimport connectToStores from &#39;fluxible-addons-react/connectToStores&#39;;\nimport {connect} from &#39;react-redux&#39;;\nimport MyFluxibleStore from &#39;../stores/myFluxible.store&#39;\n\n// Utils\nimport {getSomeDataFromState} from &#39;../myModule.selectors&#39;;\n\n// Fluxible wrapper depends on data from Redux state\nconst MyComponentFluxibleConnector = connectToStores(\nMyComponent,    \n  [MyFluxibleStore],\n  (context, props) =&amp;gt; ({\n    dataFromFluxible: context.getStore(MyFluxibleStore).getSomeDataFromReduxState(props.myPropsFromRedux)\n  })\n);\n\n// Redux\nexport const mapStateToProps = state =&amp;gt; {\n  return {myPropsFromRedux: getSomeDataFromState(state)};\n};\n\nexport default connect(mapStateToProps)(MyComponentFluxibleConnector);\n\n\n  If the higher connection is made on Fluxible stores (like first example of myComponent.connector.js), data passed to props must be immutable otherwise it can cause edge effects. Indeed, Redux wrapper component checks if it has to rerender when props change by comparing their references. So, if we mutate data in Fluxible store when dispatch is handled, references donâ€™t change and Redux wrapper (and sub-components) will not rerender (unless you tell the connect method that your component isnâ€™t â€œpureâ€).\n\n\nIf we watch carefully to those particular cases, we will succeed in our quest!\n\nIn a nutshell, Redux can easily work in addition to Fluxible (and certainly to other Flux libraries), most likely because of the lightness of its implementation. It is very convenient to upgrade smoothly a big application on Redux! But be aware that it is only a transitory situation, the final goal is to use only Redux. We wrote 50% less code with this upgrade, not badâ€¦ Developers are lazy, donâ€™t forget this! If you have some feedback on Redux and/or Fluxible, donâ€™t hesitate to share your experience with us :)\n\n"
} ,
  
  {
    "title"    : "Retour dâ€™expÃ©rience : rÃ©aliser des Workers en PHP - Fabien de Saint pern au PHP Tour 2016 ",
    "category" : "",
    "tags"     : " 6tech, lyon, conference, video, phptour, php, Symfony",
    "url"      : "/2016/06/23/video-phptour-worker-php.html",
    "date"     : "June 23, 2016",
    "excerpt"  : "Fabien de Saint pern - lead dev de notre team back-end 6play - Ã©tait au PHP Tour et a fait une prÃ©sentation sur la faÃ§on dont nous faisons des workers en PHP.\n\n\n\n",
  "content"  : "Fabien de Saint pern - lead dev de notre team back-end 6play - Ã©tait au PHP Tour et a fait une prÃ©sentation sur la faÃ§on dont nous faisons des workers en PHP.\n\n\n\n"
} ,
  
  {
    "title"    : "Preview your Android &amp; iOS React Native apps on your Github Pull Request",
    "category" : "",
    "tags"     : " reactnative, react, mobile, github, jenkins, fastlane, appetize",
    "url"      : "/2016/06/20/preview-android-ios-react-native-on-github-pull-request.html",
    "date"     : "June 20, 2016",
    "excerpt"  : "We are playing since a few weeks with React Native for a Proof Of Concept and wanted to have the same development workflow for mobile apps, as we have for the web.\n\nHere is the workflow we use for web development:\n\n\n  Branch  : every bugfix or fea...",
  "content"  : "We are playing since a few weeks with React Native for a Proof Of Concept and wanted to have the same development workflow for mobile apps, as we have for the web.\n\nHere is the workflow we use for web development:\n\n\n  Branch  : every bugfix or feature is developed on a new git branch,\n  Pull Request (PR)  : we make PR for each bugfix or feature to propose the modification to the Â« master Â» git branch,\n  Code Review  : other teammates have to review each PR and add :+1: when they agree with the modification,\n  Test  : a CI system (Jenkins) runs Unit and Integration tests, and Lint on each PR,\n  Preview  : an internal tool (Github Hooker) is called with a Github webhook on each PR to create a staging environment.\n\n\nWhen every step is ok, the PR is merged.\n\nThe Â« Branch step Â», Â« PR step Â» and Â« Code Review step Â» are mostly related to our CVS (Github Enterprise) and are not a problem.\nThe Â« Test step Â» is related to React Native. We already use Jest and ESLint, but we have to dig more for Integration test (Appium ?).\n\nThe Â« Preview step Â» is more interesting. It was not the simplest thing to do on our web project, but this is probably one of the most useful feature we have on our stack.\nHaving a staging environment for all open PR allows devs, PO, PM and scrum masters to play with this exact version of the code (on any browser they want), and really see if the bug is fixed, or if the feature correspond to the PO needs. It allows everyone to iterate and make feedbacks before the code lands on the master branch. Itâ€™s also a good way to be sure your app build didnâ€™t fail.\n\nSo, what we want is to have on each of our React Native PR, a link to preview iOS and Android version of our app in a web browser, refreshed after every commit on the branch.\n\nThe goal of this blog post is just to show you, that it is something doable and really useful. If you are interested in, here are some more information, that maybe can help you.\n\nThe stack\n\nConcerning the CI, we already use Jenkins, so we will continue to. Beware that for building iOS apps, a CI running on OSX is needed. In our case, we had added a Jenkins slave to our Jenkins pool. If you donâ€™t have CI system internally, you should take a look at Bitrise or CircleCi because they propose OSX CI systems.\nOur CVS is Github Enterprise, but everything is also possible with Gitlab (or any other CVS).\nWe use Fastlane.tools to automate build and credentials support. (Mostly because it was recommended by some of our iOS developers).\n\nIn order to preview iOS and Android app in a web browser, we use the amazing SAAS service Appetize.io (free for 100min/month).\n\n\n\nHow did we do ?\n\nWe had set up an OSX machine with a fresh Jenkins install, and created a job that triggers a build everytime a push is made on a PR, thanks to the â€œGithub Pull Request Builder Â» Jenkins plugin. There is also a lot of things to configure on this machine (Nodejs, Ruby, xCode â€¦), and i recommend you to do some builds (iOS and Android) manually to be sure everything is ready.\n\nFastlane is an open-source automation toolset for iOS &amp;amp; Android. It lets you write Â« lane Â» to automate a lot of things. We set up a unique Fastlane file at the root of our React Native project directory dealing with Android &amp;amp; iOS lanes.\n\nTo suit our needs, we created one lane Â« deployAppetize Â» for each platform: it performs the corresponding build, uploads it to Appetize.io via their API, and updates the Github PR Statuses during the process.\n\nIâ€™m not a Ruby programmer, so please, donâ€™t blame me, and feel free to improve the code below if you want (on this Github Gist).\nThis is neither the state of the art, nor a beautiful open source thing, we just share what we did in case it helps someone :-)\n\nBefore doing anything, youâ€™ll have to set some variables on Fastlane, so go to the Fastfile file in your fastlane folder:\n\n#3rd party lib to do some http calls\nrequire &#39;httparty&#39;\n\nfastlane_version &quot;1.95.0&quot;\ndefault_platform :ios\n\nbefore_all do\n  # put here your token and iOS scheme app\n  ENV[&quot;GITHUB_TOKEN&quot;] = &quot;----&quot;\n  ENV[&quot;APPETIZE_TOKEN&quot;] = &quot;----&quot;\n  ENV[&quot;APP_IOS_SCHEME&quot;] = &quot;----&quot;\n\n  # get the last git commit information\n  ENV[&quot;GIT_COMMIT&quot;] = last_git_commit[:commit_hash]\n\n  # Use ghprbSourceBranch env variable on CI, git_branch lane elsewhere\n  if !ENV[&quot;ghprbSourceBranch&quot;]\n    ENV[&quot;ghprbSourceBranch&quot;] = git_branch\n  end\n\nend\n\nCreate a private lane to make the POST request to your Github statuses API to avoid DRY:\n\n# Update git statuses of your commit.\nprivate_lane :githubStatusUpdate do |options|\n\n  response = HTTParty.post(\n    &quot;https://&amp;lt;yourgithubenterprisedomain.tld&amp;gt;/api/v3/repos/&amp;lt;orga&amp;gt;/&amp;lt;repos&amp;gt;/statuses/#{ENV[&quot;GIT_COMMIT&quot;]}?access_token=#{ENV[&quot;GITHUB_TOKEN&quot;]}&quot;,\n    :body =&amp;gt; {\n      :context =&amp;gt; options[:context],\n      :state =&amp;gt; options[:state],\n      :description =&amp;gt; options[:description],\n      :target_url =&amp;gt; options[:url]\n    }.to_json,\n    :headers =&amp;gt; { &#39;Content-Type&#39; =&amp;gt; &#39;application/json&#39; }\n  )\nend\n\nAppetize allows you to create different apps. We want one app per PR, and update the corresponding app when a new commit is made on a PR. For that, we keep track of the branch name by storing it in the Â« notes Â» field of the app on Appetize.io.\n\nSo, hereâ€™s a private lane to get back the public key of the corresponding app on Appetize.io, to update the good one if it already exists.\n\n# get the publicKey of the appetizeApp corresponding to your git branch\nprivate_lane :getAppetizePublicKey do |options|\n  publicKey = &quot;&quot;\n\n  response = HTTParty.get(&quot;https://#{ENV[&quot;APPETIZE_TOKEN&quot;]}@api.appetize.io/v1/apps&quot;)\n  json = JSON.parse(response.body)\n\n  # Find branch name in notes\n  json[&quot;data&quot;].each do |value|\n    if value[&quot;note&quot;] == ENV[&quot;ghprbSourceBranch&quot;] &amp;amp;&amp;amp; value[&quot;platform&quot;] == options[:platform]\n      publicKey = value[&quot;publicKey&quot;]\n    end\n  end\n\n  publicKey\nend\n\nNow, we have everything ready to do the deployAppetize lane for iOS :\n\nplatform :ios do\n\n  desc &quot;Deployment iOS lane&quot;\n\n    lane :deployAppetize do\n\n      githubStatusUpdate(\n        context: &#39;Appetize iOS&#39;,\n        state: &#39;pending&#39;,\n        url: &quot;https://appetize.io/dashboard&quot;,\n        description: &#39;iOS build in progress&#39;\n      )\n\n      Dir.chdir &quot;../ios&quot; do\n        tmp_path = &quot;/tmp/fastlane_build&quot;\n\n        #seems not possible to use gym to do the simulator release ?\n        xcodebuild_configs = {\n          configuration: &quot;Release&quot;,\n          sdk: &quot;iphonesimulator&quot;,\n          derivedDataPath: tmp_path,\n          xcargs: &quot;CONFIGURATION_BUILD_DIR=&quot; + tmp_path,\n          scheme: &quot;#{ENV[&quot;APP_IOS_SCHEME&quot;]}&quot;\n        }\n\n        Actions::XcodebuildAction.run(xcodebuild_configs)\n\n        app_path = Dir[File.join(tmp_path, &quot;**&quot;, &quot;*.app&quot;)].last\n\n        zipped_bundle = Actions::ZipAction.run(path: app_path, output_path: File.join(tmp_path, &quot;Result.zip&quot;))\n\n        Actions::AppetizeAction.run(\n          path: zipped_bundle,\n          api_token: &quot;#{ENV[&quot;APPETIZE_TOKEN&quot;]}&quot;,\n          platform: &quot;ios&quot;,\n          note: &quot;#{ENV[&quot;ghprbSourceBranch&quot;]}&quot;,\n          public_key: getAppetizePublicKey({platform: &quot;ios&quot;})\n        )\n\n        FileUtils.rm_rf(tmp_path)\n\n      end\n\n      githubStatusUpdate(\n        context: &#39;Appetize iOS&#39;,\n        state: &#39;success&#39;,\n        url: &quot;#{lane_context[SharedValues::APPETIZE_APP_URL]}&quot;,\n        description: &#39;iOS build succeed&#39;\n      )\n    end\n\n    error do |lane, exception|\n      case lane\n        when /deployAppetize/\n          githubStatusUpdate(\n            context: &#39;Appetize iOS&#39;,\n            state: &#39;failure&#39;,\n            url: &quot;https://appetize.io/dashboard&quot;,\n            description: &#39;iOS build failed&#39;\n          )\n        end\n      end\nend\n\nFor Android, itâ€™s almost the same things, except we have to do some small business logic to find the apk generated by Gradle, with this private lane :\n\n# find the path of the last apk build\nprivate_lane :getLastAPKPath do\n  apk_search_path = File.join(&#39;../android/&#39;, &#39;app&#39;, &#39;build&#39;, &#39;outputs&#39;, &#39;apk&#39;, &#39;*.apk&#39;)\n  new_apks = Dir[apk_search_path].reject { |path| path =~ /^.*-unaligned.apk$/i}\n  new_apks = new_apks.map { |path| File.expand_path(path)}\n  last_apk_path = new_apks.sort_by(&amp;amp;File.method(:mtime)).last\n\n  last_apk_path\nend\n\nAnd now you should be able to also deploy to Appetize.io on Android :\n\nplatform :android do\n\n  desc &quot;Deployment Android lane&quot;\n\n    lane :deployAppetize do\n\n      githubStatusUpdate(\n        context: &#39;Appetize Android&#39;,\n        state: &#39;pending&#39;,\n        url: &quot;https://appetize.io/dashboard&quot;,\n        description: &#39;Android build in progress&#39;\n      )\n\n      gradle(\n        task: &quot;assemble&quot;,\n        build_type: &quot;Release&quot;,\n        project_dir: &quot;android/&quot;\n      )\n\n      Actions::AppetizeAction.run(\n        path: getLastAPKPath,\n        api_token: &quot;#{ENV[&quot;APPETIZE_TOKEN&quot;]}&quot;,\n        platform: &quot;android&quot;,\n        note: &quot;#{ENV[&quot;ghprbSourceBranch&quot;]}&quot;,\n        public_key: getAppetizePublicKey({platform: &quot;android&quot;})\n      )\n\n      githubStatusUpdate(\n        context: &#39;Appetize Android&#39;,\n        state: &#39;success&#39;,\n        url: &quot;#{lane_context[SharedValues::APPETIZE_APP_URL]}&quot;,\n        description: &#39;Android build succeed&#39;\n      )\n    end\n\n    error do |lane, exception|\n      case lane\n        when /deployAppetize/\n          githubStatusUpdate(\n            context: &#39;Appetize Android&#39;,\n            state: &#39;failure&#39;,\n            url: &quot;https://appetize.io/dashboard&quot;,\n            description: &#39;Android build failed&#39;\n          )\n      end\nend\n\nItâ€™s over. You just have to add those commands to your CI to do the job :\n\nnpm install\nFastlane ios deployAppetize\nFastlane android deployAppetize\n\n\nYou have now two new checks on each PR with a link to the iOS or Android instance on Appetize.io.\n\n\n\nThe complete Fastfile on a Github Gist : FastFile\n\nConclusion\n\nAt M6web, we are glad to see the whole React Native promise taking a concrete shape: the developer experience is the same for both mobile &amp;amp; web development, even about tooling. We are continuing to play with it and weâ€™ll certainly keep posting articles here, stay tuned !\n\nP.S.: You could look at the Fabric Blog post on the device grid for Fabric but with Danger commenting on the PR instead of Github Statuses, and iOS only.\n\nP.S.2: You could also look at Reploy.io, which try to improve this workflow with extra features and a more cleaner UX than Appetize.io, but it is â€œalphaâ€ for now.\n"
} ,
  
  {
    "title"    : "M6web fera un retour d&#39;expÃ©rience sur l&#39;usage de Cassandra sur 6play le 14/06/2016",
    "category" : "",
    "tags"     : " 6tech, lyon, conference",
    "url"      : "/2016/05/25/m6web-retourdxp-cassandra.html",
    "date"     : "May 25, 2016",
    "excerpt"  : "Olivier Mansour, responsable R&amp;amp;D, sera prÃ©sent au Cassandra Days le 14 Juin Ã  Paris pour faire un retour dâ€™expÃ©rience sur lâ€™utilisation de Cassandra sur 6play.\n\n\n\nLâ€™Ã©vÃ¨nement est gratuit : https://www.eventbrite.co.uk/e/billets-datastax-day-pa...",
  "content"  : "Olivier Mansour, responsable R&amp;amp;D, sera prÃ©sent au Cassandra Days le 14 Juin Ã  Paris pour faire un retour dâ€™expÃ©rience sur lâ€™utilisation de Cassandra sur 6play.\n\n\n\nLâ€™Ã©vÃ¨nement est gratuit : https://www.eventbrite.co.uk/e/billets-datastax-day-paris-25165891860.\n\n"
} ,
  
  {
    "title"    : "ArrÃªtons de perdre du temps Ã  dÃ©buguer !",
    "category" : "",
    "tags"     : " afup, php, debug, conference",
    "url"      : "/2016/05/24/arretons-de-perdre-du-temps.html",
    "date"     : "May 24, 2016",
    "excerpt"  : "ArrÃªtons de perdre du temps Ã  dÃ©buguer ! DÃ©buguer peut se rÃ©vÃ©ler long et fastidieux. \nCâ€™est du temps perdu quâ€™on pourrait passer Ã  crÃ©er de la valeur ajoutÃ©e. \nCâ€™est dâ€™une maniÃ¨re ou dâ€™une autre une perte pour le business. \nAyant commencÃ© mon ent...",
  "content"  : "ArrÃªtons de perdre du temps Ã  dÃ©buguer ! DÃ©buguer peut se rÃ©vÃ©ler long et fastidieux. \nCâ€™est du temps perdu quâ€™on pourrait passer Ã  crÃ©er de la valeur ajoutÃ©e. \nCâ€™est dâ€™une maniÃ¨re ou dâ€™une autre une perte pour le business. \nAyant commencÃ© mon entrÃ©e dans la vie active par une TMA, jâ€™ai compris vite et de maniÃ¨re un peu brutale que Ã§a fait pourtant partie de la vie du dÃ©veloppeur qui devient parfois dÃ©bugueur. \nQuelles solutions et astuces pouvons-nous mettre en place afin dâ€™Ãªtre plus efficace dans cette tÃ¢che rÃ©barbative ?\n"
} ,
  
  {
    "title"    : "M6web sera prÃ©sent au sfpot de Lille du 16/06/16",
    "category" : "",
    "tags"     : " 6tech, lille, sfpot, conference",
    "url"      : "/2016/05/19/6tech-sfpot-lille.html",
    "date"     : "May 19, 2016",
    "excerpt"  : "Pierre Marichez, Renaud BougrÃ© et Nicolas Beze une partie de lâ€™Ã©quipe PHP de M6Web Lille, vous feront part dâ€™un retour dâ€™expÃ©rience sur lâ€™industrialisation des dÃ©veloppements.\nCa parlera jenkins, gitlab, gitlab-ci, outil de gestion de projets, api...",
  "content"  : "Pierre Marichez, Renaud BougrÃ© et Nicolas Beze une partie de lâ€™Ã©quipe PHP de M6Web Lille, vous feront part dâ€™un retour dâ€™expÃ©rience sur lâ€™industrialisation des dÃ©veloppements.\nCa parlera jenkins, gitlab, gitlab-ci, outil de gestion de projets, api, sentry, capistrano, user scripts, docker, grafana, slackâ€¦\n\nLors de ce sfpot, Kevin Dunglas prÃ©sentera le DunglasActionBundle et Alexandre SalomÃ© et Luc Vieillescazes vous feront un retour sur le sflive 2016.\n\nAlors rendez-vous tous le 16 juin 2016 Ã  partir de 19h00 au Liberchâ€™ti, 169 Boulevard de la LibertÃ© Ã  Lille (MÃ©tro RÃ©publique).\n\nPour vous inscrire, Ã§a se passe ici\n\n\n\nPour plus dâ€™informations sur cet Ã©vÃ©nement et les autres sfpot lillois, rendez-vous sur le site des Tilleuls.\n"
} ,
  
  {
    "title"    : "M6web sera prÃ©sent au PHPTour Clermont-Ferrand",
    "category" : "",
    "tags"     : " 6play, afup, phptour, conference",
    "url"      : "/2016/05/09/6tech-phptour-clermont.html",
    "date"     : "May 9, 2016",
    "excerpt"  : "Fabien de Saint Pern, un des leads devs sur la plateforme 6play, aura lâ€™occasion de prÃ©senter une confÃ©rence au PHPTour Clermont-Ferrand le 24 Mai. Il fera un retour dâ€™expÃ©rience concret sur nos pratiques autour de la rÃ©alisation de workers asynch...",
  "content"  : "Fabien de Saint Pern, un des leads devs sur la plateforme 6play, aura lâ€™occasion de prÃ©senter une confÃ©rence au PHPTour Clermont-Ferrand le 24 Mai. Il fera un retour dâ€™expÃ©rience concret sur nos pratiques autour de la rÃ©alisation de workers asynchrones en PHP (et oui !).\n\nLe PHPTour est un cycle de confÃ©rences itinÃ©rant organisÃ© par lâ€™AFUP rÃ©unissant toutes les communautÃ©s PHP, professionnelles et open-source, dÃ©diÃ© au langage et Ã  son Ã©cosystÃ¨me. Ne manquez pas cette confÃ©rence ainsi que cet Ã©vÃ¨nement qui sâ€™annonce particuliÃ¨rement riche !\n\n"
} ,
  
  {
    "title"    : "La retrospective Agile â€˜Garde Ã  vousâ€™",
    "category" : "",
    "tags"     : " agile, scrum",
    "url"      : "/2016/03/29/retro-agile-garde-a-vous.html",
    "date"     : "March 29, 2016",
    "excerpt"  : "Depuis quelques annÃ©es les Ã©quipes dâ€™M6Web se sont organisÃ©es autour des mÃ©thodes agiles. Scrum, Kanban, Lean, mÃ©thodes adaptÃ©es, nous nous efforÃ§ons de toujours garder en tÃªte lâ€™amÃ©lioration continue et le fun spirit au coeur du travail de nos Ã©q...",
  "content"  : "Depuis quelques annÃ©es les Ã©quipes dâ€™M6Web se sont organisÃ©es autour des mÃ©thodes agiles. Scrum, Kanban, Lean, mÃ©thodes adaptÃ©es, nous nous efforÃ§ons de toujours garder en tÃªte lâ€™amÃ©lioration continue et le fun spirit au coeur du travail de nos Ã©quipes.\n\nAu delÃ  des rituels â€œclassiquesâ€, lâ€™Ã©quipe des scrum master cherche de temps en temps Ã  thÃ©matiser et casser les routines en crÃ©ant des jeux autour de lâ€™agilitÃ©.\n\nNous souhaitons aujourdâ€™hui au travers de ce blog, partager avec vous ces jeux et surtout vous permettre de les reproduire. Ainsi chaque jeu sâ€™accompagnera de rÃ¨gles et dâ€™un Â« kit Â» vous permettant dâ€™imprimer le matÃ©riel nÃ©cessaire au bon dÃ©roulement.\n\nAu menu de ce premier jeu, nous avions choisi de profiter de lâ€™arrivÃ©e de lâ€™Ã©mission â€œGarde Ã  vousâ€ sur M6 afin de proposer une rÃ©trospective pas comme les autres.\n\n\n  Nom : La rÃ©tro Garde Ã  vous !\n  Type : RÃ©trospective dâ€™Ã©quipe.\n  DurÃ©e : 1 heure.\n\n\nPunchLine :\n\nVotre Ã©quipe est sÃ©lectionnÃ©e pour une retro spÃ©ciale. Un dÃ©fi difficile qui les mÃ¨nera Ã  dÃ©passer leur limite. \nÃ‰preuve physique et mentale, il y en aura pour tous. Mais surtout câ€™est en groupe quâ€™ils rÃ©ussiront les Ã©preuves. :)\n\n\n\nObjectifs :\n\n\n  Changer de la rÃ©tro classique : 2 activitÃ©s sur les 4 sont des mini-jeux.\n  CrÃ©er du team building : Le 1er jeu demande confiance et cohÃ©sion entre les membres de son Ã©quipe. Et accessoirement câ€™est trÃ¨s fun !\n  Stimuler les Ã©quipes entre-elles autour dâ€™une compÃ©tition sympa (nos 5 Ã©quipes ont fait la mÃªme rÃ©tro lors de la rotation).\n  Garder Ã  lâ€™esprit lâ€™amÃ©lioration continue au travers des 2 activitÃ©s post-it.\n\n\nPrÃ©paration / matÃ©riel :\n\n\n  Vous trouverez ici un lien vers le kit de la rÃ©tro garde Ã  vous.\n  CrÃ©er un dÃ©cor : Mettez vos Ã©quipes dans lâ€™ambiance et poussez lâ€™aspect Â« jeux de rÃ´le Â». \nExemple : filet Ã  chat, bÃ¢che de tente Quechua, palissade, affiche militaire, etc..\n  Costume de lâ€™animateur : pantalon/veste militaire (demander autour de vous), cravache, casque avec lunette de ski, chemise beige, etc..\n  Pour lâ€™Ã©tape #1 : Post-it, feutres pour Ã©crire, lâ€™affiche paperboard Â« Motivation Â».\n  Pour lâ€™Ã©tape #2 : 4 bandanas ou serviettes pour bander les yeux + gilets fluos de sÃ©curitÃ© + un chronomÃ¨tre + un parcours dans vos bureaux.\n  Pour lâ€™Ã©tape #3 : Post-it, feutres pour Ã©crire, lâ€™affiche paperboard Â« 4 thÃ¨mes dâ€™amÃ©lioration Â».\n  Pour lâ€™Ã©tape #4 : 2 Nerfs, 4 canettes vides, 4 peluches (ou autre), 4 balles en mousse, une poubelle, tapis de sol.\n  La feuille des scores.\n\n\n\n\nLe dÃ©roulement :\n\nVolontairement nous nâ€™avons donnÃ© aucun dÃ©tail Ã  nos Ã©quipes sur cette rÃ©tro. \nQuelques jours avant, nous leur avons envoyÃ© la vidÃ©o bande annonce sous forme de Â« convocation Â». \nLe jour mÃªme, ils ont eu la surprise de voir la salle dÃ©corÃ©e et leur scrum master dÃ©guisÃ©.\n\nÃ‰tape.1 - Motivation - 10 minutes :\n\nTexte possible : Â« Bonjour Ã©quipe [nom_Ã©quipe]. Je suis le sergent â€œBadassâ€, on vous a placÃ© chez moi aujourdâ€™hui pour Ã©valuer votre trouillomÃ¨tre.\n\n\n  Â« Cette rÃ©tro va se dÃ©rouler en 4 Ã©tapes. Comme je suis sympa, je ne vous dis rien. Ã§a permettra de voir votre capacitÃ© dâ€™adaptation.Â»\n\n\n\n  Â«  Sachez que nous aurons 2 Ã©preuves physiques et 2 Ã©preuves mentales. Lors des Ã©preuves physiques, nous noterons vos scores afin de  dÃ©terminer quelle est la meilleure Ã©quipe du plateau. Â»\n\n\n\n  Â«  Ãªtes vous prÃªts ? Â»\n\n\nPaperboard #1 : La motivation\n\n\n  Â« On va commencer doucement. Prenez vos post-it et vos crayons.Â»\n\n\n\n  Â«  Dites moi ce qui vous motive Ã  vous levez le matin ? pourquoi vous aimez venir bosser ? Â»\n\n\n\n  Â«  Si un aspect du boulot vous ennuie, vous cloue au lit, dites le Ã©galement.Â»\n\n\n\n  Â«  TimeBox : 2 minutes. Â»\n\n\nAu bout des 2 minutes : chacun passe au paperboard coller ses post-it et les expliquer.\n\nObjectif du scrum master :\n\n\n  rÃ©cupÃ©rer les aspects positifs de lâ€™environnement, du travail de vos Ã©quipes : ce sont des bases solides Ã  avoir en tÃªte et Ã  maintenir dans le groupe.\n  rÃ©cupÃ©rer les aspects nÃ©gatifs : Ã§a peut Ãªtre la cantine, la distance des locaux, etc.. mÃªme si certains post-it sont difficilement â€˜amÃ©liorablesâ€™ câ€™est toujours bien de lâ€™exprimer.\n\n\nÃ‰tape.2  - En avant, Marche ! - 10 minutes\n\nRÃ¨gle du jeu :\n\n\n  On se met par 2. Si vous Ãªtes un nombre impair, explication plus bas.\n  Lâ€™une des 2 personnes va avoir les yeux bandÃ©s. On lâ€™Ã©quipe dâ€™un gilet fluo de sÃ©curitÃ© afin dâ€™Ã©viter de lui rentrer dedansâ€¦ :p\n  Lâ€™autre personne devra le guider en utilisant les mots : Â« avance / recule / Ã  droite / Ã  gauche Â».\n  Il est interdit de toucher son coÃ©quipier pendant la course.\n  \n    Vous Ãªtes chronomÃ©trÃ©s. Un classement gÃ©nÃ©ral sera fait avec les autres Ã©quipes pour dÃ©terminer les plus rapides.\n  \n  Faites mettre le bandana ou la serviette. Prenez les guides et montrez leur le parcours.\n  Faites aligner les paires devant la ligne de dÃ©part.\n  3, 2, 1, Partez. Nâ€™hÃ©sitez pas Ã  les encourager ou Ã  parler fort afin de les stresser ^^.\n\n\nVersion Ã  3 : vous aurez 1 guide et 2 personnes avec les yeux bandÃ©s. Les 2 yeux bandÃ©s sont en file indienne.\n\nLa personne derriÃ¨re pose ses mains sur les Ã©paules sur la personne de devant. Bonne chance :p\n\nFeuille des scores : Notez le temps de chaque paires.\n\nÃ‰tape.3 - les 4 thÃ¨mes - 20/25 minutes\n\nRÃ¨gles :\n\n\n  Nous avons 4 thÃ¨mes affichÃ©s au paperboard.\n  Pour chaque thÃ¨me, vous pouvez Ã©crire au maximum 2 post-it positifs &amp;amp; 2 post-it nÃ©gatifs\n\n\nNous limitons le nombre de post-it pour une question de temps. Libre Ã  vous dâ€™ajuster.\n\nLes thÃ¨mes sont :\n\n\n  Communication - Dialogue dans lâ€™Ã©quipe, avec le PO, les clients finaux, utilisation des mails, etcâ€¦\n  Les outils - De dÃ©veloppement, mÃ©thode agile, communication, de dÃ©ploiement/MEP, de testing, etc..\n  RÃ©activitÃ© - Lors dâ€™une demande PO, dâ€™un incident de production, phase de cadrage avec PO, etcâ€¦\n  Leadership - PrÃ©sence de votre Lead-Dev / Responsable R&amp;amp;D, Ã©coute de vos managers, mÃ©thode dâ€™organisation dans le travail, etcâ€¦\n\n\nNous laissons 7 minutes dâ€™Ã©criture de post-it (Ã  ajuster selon vous).\nAu bout des 7 minutes : chacun passe au paperboard coller ses post-it et les expliquer.\n\nObjectif du scrum master :\n\n\n  RÃ©cupÃ©rer des axes dâ€™amÃ©lioration de lâ€™Ã©quipe.\n  Conclure sur les aspects positifs et dÃ©finir les post-it nÃ©gatifs sur lesquels on cherche Ã  agir en 1er.\n  Ã‰viter de dÃ©finir les actions Ã  mettre en oeuvre pour les post-it nÃ©gatifs. Faites le en dehors sinon Ã§a prendra trop de temps et cassera la dynamique.\n\n\nÃ‰tape.4 - Duck Hunt - 15 minutes\n\nRÃ¨gles : 3 stands sont proposÃ©s\n\n\n  Chacun choisit un stand. Les participants peuvent faire un essai rapide si câ€™est demandÃ©.\n  Nous vous laissons ajuster la distance entre les cibles et le joueur.\n  Stand 1 - Le grenadier : 4 grenades (balle en mousse), une poubelle =&amp;gt; lancer les grenades dans la poubelle. 4 essais, 1 rÃ©ussite = 1 point\n  Stand 2 - le chasseur : 4 canettes, un nerf =&amp;gt; toucher les canettes. 4 essais, 1 rÃ©ussite = 1 point\n  Stand 3 - le sniper : 4 peluches Ã©lÃ©phant PHP, un gros nerf =&amp;gt; mode allongÃ© dans les bois, 4 essais, 1 rÃ©ussite = 1 point\n\n\nCeci afin de rÃ©partir les personnes sur plusieurs stands.\n\nFeuille des scores : Notez le score de chacun.\n\n\n\nConclusion :\n\nAfficher le tableau des scores et fÃ©liciter tout le monde. \nNext step :\n\n\n  Les rÃ©sultats des Ã©quipes seront affichÃ©s le lendemain / fin de journÃ©e / autres (Ã  vous de voir)\n  Les post-it nÃ©gatifs de lâ€™Ã©tape 1 et 3 seront pris en compte par les scrum master qui travailleront avec les personnes adÃ©quates pour continuer Ã  sâ€™amÃ©liorer. Vous pouvez ajouter Ã  votre DSK (Do, Store, Keep) certaines actions.\n\n\nNâ€™hÃ©sitez pas Ã  nous envoyer vos feedbacks sur ce jeu.\n\nForce et Scrum !\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un dÃ©veloppeur player vidÃ©o JavaScript (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/2016/01/26/m6web-lyon-recherche-developpeur-player-video-web-h-f-en-cdi.html",
    "date"     : "January 26, 2016",
    "excerpt"  : "Mise jour : Le poste nâ€™est Ã  plus pourvoir. Merci\n\nAu sein de la team Tube (Ã©quipe Lecteur VidÃ©o), en charge entre autre du lecteur de 6play et des lecteurs vidÃ©os des autres portails Internet dâ€™M6 Web (Clubic.com, Deco.fr, â€¦), vous participez Ã  l...",
  "content"  : "Mise jour : Le poste nâ€™est Ã  plus pourvoir. Merci\n\nAu sein de la team Tube (Ã©quipe Lecteur VidÃ©o), en charge entre autre du lecteur de 6play et des lecteurs vidÃ©os des autres portails Internet dâ€™M6 Web (Clubic.com, Deco.fr, â€¦), vous participez Ã  la conception technique et au dÃ©veloppement de nos lecteurs vidÃ©os.\n\nVous maitrisez les problÃ©matiques et les technologies Web :\n\n\n  ECMAScript 2015 Â« ES6 Â»\n  VideoJS\n  Gulp / Grunt\n  Les outils de tests (Jasmine, qUnit, Jest, PhantomJS, BrowserStack â€¦)\n\n\nVous avez une bonne connaissance des problÃ©matiques vidÃ©os :\n\n\n  Les nombreux formats en diffusion continue (streaming) mais aussi en tÃ©lÃ©chargement progressif (Progressive Download)\n  Les contraintes dâ€™encodage vidÃ©o\n  Les problÃ©matiques autour de la sÃ©curitÃ© (le chiffrement, les DRMs du marchÃ©, â€¦)\n\n\nPar ailleurs, vous avez dÃ©jÃ  eu Ã  travailler sur lâ€™intÃ©gration de formats publicitaires.\n\nEnfin, une connaissance dâ€™ActionScript sera apprÃ©ciÃ©e.\n\nVous aurez des interactions avec les Ã©quipes Produit de Paris, ainsi quâ€™avec nos autres dÃ©veloppeurs basÃ©s Ã  Lille.\n\nLe profil recherchÃ© se caractÃ©rise par :\n\n\n  Une trÃ¨s forte sensibilitÃ© sur les sujets VidÃ©o et QualitÃ© de Service\n  Un goÃ»t prononcÃ© pour lâ€™innovation\n  Une bonne culture du web et du monde du numÃ©rique\n  Une aptitude Ã  la prise dâ€™initiatives, un grand dynamisme, une curiositÃ© et une ouverture dâ€™esprit\n  Une connaissance (idÃ©alement validÃ©e par une premiÃ¨re expÃ©rience) des MÃ©thodes Agiles (Scrum), et une culture de lâ€™amÃ©lioration continue\n\n\nPour postuler : https://www.groupem6.fr/ressources-humaines/offres-emploi/developpeur-frontend-javascript-video-h-f-258357.html\n\n"
} ,
  
  {
    "title"    : "On a testÃ© fonctionnellement notre app JS",
    "category" : "",
    "tags"     : " tests fonctionnels, javascript, phantomjs, webdriver, Cytron",
    "url"      : "/2016/01/25/tests-fonctionnels-app-js.html",
    "date"     : "January 25, 2016",
    "excerpt"  : "Lâ€™utilitÃ© des tests fonctionnels pour les applications web nâ€™est plus Ã  dÃ©montrer (comment Ã§a, vous ne testez pas encore vos apps ?). Malheureusement, tout ne peut pas Ãªtre totalement testÃ© fonctionnellement, ou de faÃ§on aisÃ©e : je pense par exemp...",
  "content"  : "Lâ€™utilitÃ© des tests fonctionnels pour les applications web nâ€™est plus Ã  dÃ©montrer (comment Ã§a, vous ne testez pas encore vos apps ?). Malheureusement, tout ne peut pas Ãªtre totalement testÃ© fonctionnellement, ou de faÃ§on aisÃ©e : je pense par exemple au player chez nous, un composant stratÃ©gique mais pauvrement testÃ© fonctionnellement de par sa nature un peu hybride (mÃ©lange de flash et de JS). Dans tous les cas, pour ce qui peut lâ€™Ãªtre, nous sommes partisans dans lâ€™Ã©quipe Cytron dâ€™user sans mesure (ou presque !) de cet outil de maniÃ¨re Ã  Ãªtre le plus zen possible au moment dâ€™appuyer sur le bouton â€œdeployâ€.\n\nQuelle stack ?\n\nNotre application est codÃ©e en JS isomorphique (ou Universal JS) grÃ¢ce Ã  React et Node.js.\n\nPour les tests fonctionnels, nous utilisons le trio Cucumber.js + WebdriverIO + PhantomJS :\n\n\n  Cucumber.js est lâ€™outil qui permet de dÃ©rouler la suite de tests Ã©crits dans la syntaxe Gherkin,\n  WebdriverIO permet dâ€™interfacer les tests traduits en JS avec un serveur Selenium (dialoguant grÃ¢ce au protocole WebDriver Wire et permettant de contrÃ´ler un browser),\n  PhantomJS est le browser dans lequel les scÃ©narios de tests seront exÃ©cutÃ©s, il embarque son propre serveur Webdriver, Ghostdriver.\n\n\nToutes nos Pull Requests lancent les tests indÃ©pendamment via Jenkins dans un environnement â€œdockerisÃ©â€, donc complÃ¨tement autonome et isolÃ©. De faÃ§on Ã  respecter ce principe jusquâ€™au bout et Ã  ne pas dÃ©pendre de donnÃ©es versatiles, nos API sont aussi mockÃ©es grÃ¢ce Ã  superagent-mock.\n\nSetup\n\nArborescence\nDans notre projet, nous avons un dossier pour les tests fonctionnels organisÃ©s comme suit :\n\nâ”œâ”€â” tests\nâ”‚ â”œâ”€â” step_definitions\nâ”‚ â”‚ â””â”€â”€ my_feature.steps.js\nâ”‚ â”œâ”€â” screenshots\nâ”‚ â”‚ â””â”€â”€ my_scenario.png\nâ”‚ â”œâ”€â” support\nâ”‚ â”‚ â”œâ”€â”€ config.json\nâ”‚ â”‚ â”œâ”€â”€ constants.json\nâ”‚ â”‚ â”œâ”€â”€ hooks.js\nâ”‚ â”‚ â””â”€â”€ world.js\nâ”‚ â””â”€â”€ my_feature.feature\n\nFeatures\nUne feature est un fichier testant une fonctionnalitÃ© de lâ€™application et regroupant plusieurs scÃ©narios de test. Il est Ã©crit en langage naturel (Gherkin) de faÃ§on Ã  Ãªtre lisible par tous.\n\n# tests/support/cookie.feature\nFeature: Scenarios about the cookie banner\n\n  Scenario: See the cookie banner and close it\n    Given My browser storage is empty\n    When I visit the &quot;homepage&quot; page\n    Then I should see the &quot;cookie banner&quot;\n\n    When I click on &quot;Accept cookie&quot;\n    Then I should not see a &quot;cookie banner&quot;\n\n    When I visit the &quot;homepage&quot; page\n    Then I should not see a &quot;cookie banner&quot;\n\nWorld\nLe fichier world.js est le point de dÃ©part pour Cucumber.js. Câ€™est ici que nous initialisons WebdriverIO et que nous mettons un place un contexte qui sera disponible pour tous les tests.\n\n// tests/support/world.js\nvar Webdriver = require(&#39;webdriverio&#39;);\nvar config = require(&#39;./config.json&#39;);\nvar assert = require(&#39;assert&#39;);\n\nvar browser = Webdriver.remote({\n  logLevel: config.logLevel || &#39;silent&#39;,\n  host: config.webdriver.host,\n  port: config.webdriver.port,\n  waitforTimeout: config.waitTimeout,\n  desiredCapabilities: {browserName: &#39;phantomjs&#39;}\n});\n\nfunction WorldConstructor() {\n  var world = {\n    browser: browser,\n\n    // Global visit method\n    visit: function (baseUrl, params) {\n      var pathUrl = url.format({\n        pathname: baseUrl,\n        query: params\n      });\n\n      return this.browser.url(pathUrl);\n    },\n\n    // Take screenshot\n    screenshot: function (filename) {\n      return browser.saveScreenshot(path.join(config.screenshot.path, filename));\n    },\n\n    assert: {\n      /**\n       * Assert if element(s) are visible\n       *\n       * @param selector    {String}   Can be query multiple DOM elements\n       * @param failMessage {String}   Fail message if no visible\n       */\n      visible: function (selector, failMessage) {\n        // ...\n      },\n    }\n\n    // ...\n  }\n\n  return world;\n}\n\nmodule.exports = WorldConstructor;\n\nHooks\nCucumber.js permet de dÃ©clencher des traitements sur certains Ã©vÃ¨nements clÃ©s lors de lâ€™exÃ©cution de la suite de tests. Nous utilisons ce systÃ¨me pour rÃ©aliser une capture dâ€™Ã©cran sur chaque scÃ©nario de test en Ã©chec qui viendra sâ€™ajouter dans le dossier screenshots.\n\n// tests/support/hook.js\nvar config = require(&#39;./config.json&#39;);\nvar sprintf = require(&#39;sprintf-js&#39;).sprintf;\n\nmodule.exports = function () {\n  this.Before(function (scenario) {\n    return this.browser.init().then(function () {\n      return this.browser.setViewportSize({\n        width: config.screenshot.width,\n        height: config.screenshot.height\n      });\n    }.bind(this));\n  });\n\n  this.After(function (scenario) {\n    if (scenario.isFailed()) {\n      return this.screenshot(sprintf(\n        &#39;%s_%d.png&#39;,\n        scenario.getName().toLowerCase().replace(&#39; &#39;, &#39;-&#39;),\n        new Date().getTime()\n      )).then(function () {\n        return this.browser.end();\n      }.bind(this));\n    } else {\n      return this.browser.end();\n    }\n  });\n};\n\nStep definitions\nCe sont les fichiers qui font le lien entre les features (Ã©crit en langage naturel) et WebdriverIO (initialisÃ© dans world.js).\n\n// tests/step_definitions/cookie.steps.js\nvar sprintf = require(&#39;sprintf-js&#39;).sprintf;\n\nmodule.exports = function () {\n  /**\n   * Visit a page\n   *\n   * @param page {String}\n   *\n   * @require config routes object\n   */\n  this.When(/^I visit the &quot;([^&quot;]*)&quot; page$/, function (page) {    \n    return this.visit(this.getRoute(page)).then(function () {\n      return this.assert.existing(&#39;#__main&#39;, &#39;React application is not loaded.&#39;);\n    }.bind(this));\n  });\n\n  /**\n   * I click on &quot;label&quot;\n   *\n   * @param label {String}   DOM selector label\n   */\n  this.When(/^I click on &quot;([^&quot;]*)&quot;$/, function (label) {\n    var selector = this.getDOMSelector(label);\n    \n    return this.action.click(selector);\n  });\n\n  /**\n   * Assert element matching the given selector is visible.\n   *\n   * @param label {String}\n   *\n   * @require config DOMSelectors object\n   */\n  this.Then(/^I should see a &quot;([^&quot;]*)&quot;$/, function (label) {\n    var selector = this.getDOMSelector(label);\n    var failMessage = sprintf(&#39;%s is not visible&#39;, label);\n    \n    return this.assert.visible(selector, failMessage);\n  });\n\n  // ...\n}\n\nDesign\nNous nâ€™avons pas mis en Å“uvre le pattern Page Object. Ce nâ€™Ã©tait pas un choix dÃ©libÃ©rÃ© mais le contexte et les enjeux du projet nous ont fait passer Ã  cÃ´tÃ©, ou ce nâ€™Ã©tait peut Ãªtre simplement pas le moment. MalgrÃ© tout, nous avons tentÃ© de rationaliser au mieux lâ€™organisation du code. Par exemple, afin de ne pas se retrouver avec des sÃ©lecteurs CSS Ã©parpillÃ©s dans plusieurs fichiers de â€œfeaturesâ€ ou de â€œstep definitionsâ€, nous avons choisi de les regrouper dans un fichier constants.json et dâ€™utiliser seulement des labels ailleurs. Nous faisons le lien entre le label et le sÃ©lecteur CSS avec la mÃ©thode getDOMSelector, visible ci-dessus et dÃ©finie dans le fichier world.js.\n\nRun\nPour lancer les tests, il faut :\n\n\n  lancer le serveur de lâ€™app en local (lâ€™URL du serveur est paramÃ©trable dans le fichier de config),\n  lancer un phantomjs en mode webdriver phantomjs --webdriver=5024 oÃ¹ 5024 est le port du serveur (Ã©galement configurable dans config.json),\n  lancer une suite de tests via Cucumberjs, au choix :\n    \n      tous les tests cucumberjs tests/,\n      une feature cucumberjs tests/cookie.feature,\n      un scÃ©nario cucumberjs tests/cookie.feature:3 oÃ¹ 3 correspond Ã  la ligne du dÃ©but du scÃ©nario ciblÃ© dans le fichier cookie.feature.\n    \n  \n\n\nParticularitÃ© de lâ€™isomorphisme\n\nDeux chemins sont possibles avec lâ€™isomorphisme. Soit lâ€™utilisateur arrive directement sur la page, auquel cas celle-ci sera gÃ©nÃ©rÃ©e sur le serveur, soit il y arrive en naviguant sur lâ€™app et câ€™est le client qui aura exÃ©cutÃ© le code. Il faut tester ces deux cas car le code concernÃ© nâ€™est pas toujours le mÃªme (la variable window par exemple nâ€™est pas accessible cÃ´tÃ© serveur).\n\nIl est bien sÃ»r impossible dâ€™Ãªtre exhaustif. Lâ€™idÃ©e est dâ€™abord de couvrir les cas les plus frÃ©quents et les plus critiques pour lâ€™application. Ensuite, il faut sâ€™astreindre Ã  ajouter un test Ã  chaque fois quâ€™un bug est dÃ©tectÃ© de faÃ§on Ã  sâ€™assurer quâ€™on ne le rencontrera plus dans le futur.\n\nPhantomJS, la stabilitÃ© en questionâ€¦\n\nBasÃ© sur Webkit, PhantomJS est le plus connu des navigateurs headless, câ€™est-Ã -dire exÃ©cutables sans interface visuelle. Dâ€™autres navigateurs lÃ©gers et crÃ©Ã©s pour les tests fonctionnels existent comme SlimerJS (basÃ© sur Gecko et pas vraiment headless) ou Zombie.js (pas de moteur de rendu). Cependant aucun nâ€™offre toutes les fonctionnalitÃ©s de PhantomJS qui se rapprochent le plus dâ€™un vrai browser. Il Ã©mule de faÃ§on transparente tout le rendu graphique avec la possibilitÃ© de rÃ©aliser des screenshots par exemple ou de tester la visibilitÃ© dâ€™un Ã©lÃ©ment du DOM (non opaque, dans le viewport, sur la couche z-index la plus hauteâ€¦).\n\nNÃ©anmoins celui-ci nâ€™intÃ¨gre pas toutes les derniÃ¨res avancÃ©es en terme de JS et de CSS. Flexbox nâ€™est par exemple pas pris en charge ce qui nous a posÃ© quelques problÃ¨mes sur les vÃ©rifications liÃ©es Ã  la visibilitÃ© des Ã©lÃ©ments. Sa version 2.0 qui date de dÃ©but 2015, malgrÃ© la bonne volontÃ© des contributeurs, nâ€™a toujours pas de build officiel sous Linux, ce qui oblige Ã  compiler les sources sur sa machine de tests ou Ã  trouver sur le net un build officieux correspondant Ã  sa distribution. Câ€™est ce que nous avons fait via M6Web/phantomjs2. Cependant, lâ€™outil est assez instable (builds officiels ou pas) et nous avons rencontrÃ© beaucoup de crashs alÃ©atoires ou reproductibles mais incomprÃ©hensibles (dus par exemple Ã  lâ€™ajout de quelques lignes de CSS anodinesâ€¦).\n\nEn local, sur sa machine, PhantomJS est encore moins stable que sur Jenkins. Il semblerait quâ€™exÃ©cution aprÃ¨s exÃ©cution, il garde des â€œchosesâ€ en cache quelque part qui, Ã  terme, produisent des crashs systÃ©matiques de lâ€™outil. Nous nâ€™avons pas rÃ©ussi Ã  Ã©tablir un scÃ©nario reproductible qui nous permette de poser une issue sur le projet. Nâ€™hÃ©sitez pas Ã  rÃ©agir en commentaire si vous vous Ãªtes trouvÃ© dans un cas similaire.\n\nPour rÃ©gler temporairement ce problÃ¨me, nous avons utilisÃ© lâ€™image docker de Gabe Rosenhouse pour le faire tourner dans un environnement indÃ©pendant mais ce nâ€™est pas faciliter la vie des dÃ©veloppeurs qui veulent juste lancer des tests sans avoir Ã  mettre en Å“uvre une usine Ã  gaz derriÃ¨re.\n\nEdit: hier, la version 2.1 de PhantomJS a (enfin) Ã©tÃ© publiÃ©e avec un build pour chaque plateforme. Plusieurs de nos soucis pourraient Ãªtre rÃ©glÃ©s avec cette nouvelle release, Ã  suivreâ€¦\n\nChrome+ChromeDriver, une alternative ?\n\nNous avons alors optÃ© pour la solution Chrome+ChromeDriver. ChromeDriver a le rÃ´le du serveur Selenium qui permet de faire communiquer WebriverIO avec Chrome. Les avantages de cette stack sont multiples. Dâ€™abord, lâ€™ensemble est beaucoup plus stable, fini les crashs impromptus. Ensuite, le debug des tests en Ã©chec est bien plus aisÃ© : on voit en effet la suite se jouer en temps rÃ©el dans son navigateur, on peut ainsi tout Ã  fait mettre un point dâ€™arrÃªt et utiliser la console de dÃ©veloppement. Enfin, on utilise la version de Chrome que lâ€™on souhaite, donc plus de problÃ¨me de CSS non supportÃ©s.\n\nAlors pourquoi se cantonner Ã  nâ€™utiliser Chrome+ChromeDriver quâ€™en local et pas en intÃ©gration continue sur Jenkins ? Chrome nâ€™est pas un browser headless et a besoin dâ€™une interface visuelle qui nâ€™est pas disponible sur Jenkins. Il existe des solutions pour simuler un affichage graphique avec Xvfb par exemple. Nous avons tentÃ© de mettre en place une telle stack sur lâ€™image docker utilisÃ©e pour crÃ©er notre environnement de test sur Jenkins en se basant sur lâ€™image de Rob Cherry. Malheureusement, aprÃ¨s y avoir consacrÃ© un peu dâ€™Ã©nergie, le rÃ©sultat nâ€™a pas Ã©tÃ© au rendez-vous car :\n\n\n  lâ€™exÃ©cution des tests dans Chrome est bien plus lente que sur PhantomJS (2 Ã  3 fois plus lent), notre intÃ©gration continue prenant dÃ©jÃ  plus de 10 minutes sur ce projet,\n  il semble difficile dâ€™obtenir ici aussi une stabilitÃ© du dispositif, les sessions Webdriver Ã©taient souvent perdues, sans que nous en trouvions la cause.\n\n\nCes raisons nous ont conduit Ã  abandonner cette piste.\n\nQuelques tips pour amÃ©liorer la stabilitÃ© de ses tests\n\nNous avons continuÃ© dâ€™espÃ©rer avoir une stack stable pour nos tests fonctionnels. Avec persÃ©vÃ©rance, nous pouvons dire quâ€™Ã  lâ€™heure actuelle grÃ¢ce Ã  ces quelques tips, nous avons une plateforme de test stable (Ã  99%) !\n\nwaitUntil\nCâ€™est la premiÃ¨re chose Ã  faire et la plus importante de notre point de vue. On ne sait jamais vraiment quand un Ã©lÃ©ment sâ€™affichera dans la page car son chargement dÃ©pend de trop de facteurs non prÃ©dictibles (la connexion, lâ€™utilisation cpu, gpu, mÃ©moire, etc.). Sur notre projet, nous avons par exemple beaucoup dâ€™animations CSS qui retardent le timing dâ€™apparition des pages et des Ã©lÃ©ments du DOM. Notre premiÃ¨re approche a Ã©tÃ© de rajouter des sleep un peu de partout dans nos tests. Chose Ã  ne pas faire. Lâ€™usage des sleep doit Ãªtre cantonnÃ© Ã  des cas trÃ¨s spÃ©cifiques. Pour tout le reste, il faut user et abuser du waitUntil de WebdriverIO, que ce soit pour des actions ou des vÃ©rifications dans la page, et en adaptant le timeout Ã  votre projet (certaines de nos animations sont assez longues).\n\nrollover\nUn autre problÃ¨me que nous avons rencontrÃ© est la bonne exÃ©cution des rollovers. En utilisant la mÃ©thode moveToObject pour pointer la souris sur un Ã©lÃ©ment, il nous arrivait que le comportement â€œhoverâ€ ne soit pas dÃ©clenchÃ©, mettant en Ã©chec la suite du test. Nous avons donc changÃ© notre maniÃ¨re dâ€™effectuer le rollover : on rÃ©pÃ¨te lâ€™action grÃ¢ce au waitUntil tant que lâ€™Ã©lement devant apparaÃ®tre au hover nâ€™est pas visible.\n\nNous nâ€™Ã©crivons plus\n\nI rollover the &quot;Header login icon&quot;\n\nmais\n\nI rollover the &quot;Header login icon&quot; to make &quot;Submenu&quot; appear\n\nrerun\nâ€œRerunâ€ est une fonctionnalitÃ© existante sur dâ€™autres frameworks de tests fonctionnels tel que Behat et crÃ©Ã©e pour les tests rÃ©calcitrants encore instables. Elle permet de stocker dans un fichier texte la liste des scÃ©narios en Ã©chec pour les relancer ensuite afin de vÃ©rifier quâ€™ils le sont rÃ©ellement. Nous avons mis en place ce process sur Jenkins, bien quâ€™il y ait quelques subtilitÃ©s qui ne facilitent pas la tÃ¢che (mais qui devraient Ãªtre bientÃ´t corrigÃ©es), et nous en sommes satisfaits.\n\nisVisible\nA nos dÃ©buts, nous avons eu quelques problÃ¨mes avec la fonction isVisible de WebdriverIO car les Ã©lÃ©ments opaques ou en dehors du viewport Ã©taient considÃ©rÃ©s comme visibles. Nous avons alors choisi dâ€™utiliser une fonction custom injectÃ©e via execute. RÃ©cemment, dans la version 3 de WebdriverIO, la fonction isVisibleWithinViewport a fait son apparition mais nous nâ€™avons pas encore tentÃ© de lâ€™utiliser dans nos tests.\n\nCet article est un retour dâ€™expÃ©rience sur notre usage des tests fonctionnels sur un projet prÃ©cis mais il est loin dâ€™exposer des vÃ©ritÃ©s absolues. Si vous avez des remarques ou nâ€™Ãªtes pas dâ€™accord avec certaines choses, nâ€™hÃ©sitez pas Ã  nous le faire savoir !\n"
} ,
  
  {
    "title"    : "L&#39;envers du dÃ©cor du nouveau 6play",
    "category" : "",
    "tags"     : " 6play, REST, Symfony, Elasticsearch, Cassandra",
    "url"      : "/2015/11/30/beta-nouveau-6play-backend.html",
    "date"     : "November 30, 2015",
    "excerpt"  : "Il y a quelques semaines, nous vous parlions ici mÃªme de la stack technique mise en place pour le nouveau front web de 6play.\n\nAujourdâ€™hui, nous vous proposons un retour sur ce qui a Ã©tÃ© mis en place cÃ´tÃ© backend pour assurer la mise Ã  disposition...",
  "content"  : "Il y a quelques semaines, nous vous parlions ici mÃªme de la stack technique mise en place pour le nouveau front web de 6play.\n\nAujourdâ€™hui, nous vous proposons un retour sur ce qui a Ã©tÃ© mis en place cÃ´tÃ© backend pour assurer la mise Ã  disposition des donnÃ©es aux diffÃ©rents frontaux 6play.\n\nTout dâ€™abord, il faut commencer par expliquer que lâ€™univers 6play ne se rÃ©sume pas que Ã  son application web. Il existe aussi une version iOS et Android, mais Ã©galement une version par Box IPTV (disons une version par FAI).\n\nPas mal de REST â€¦\n\nCâ€™est donc tout naturellement que nous sommes partis sur la mise Ã  disposition dâ€™une API REST permettant Ã  ces diffÃ©rents fronts de consommer simplement les donnÃ©es.\n\nNotre stack technique habituelle cÃ´tÃ© backend Ã©tant Symfony2, nous sommes donc partis sur ce framework, ainsi que les habituels bundles :\n\n\n  FOSRestBundle pour la gestion simple des controlleurs REST (validation des paramÃ¨tres, routing adaptÃ©, view au format JSON, gestion des retours dâ€™erreur)\n  BazingaHateoasBundle pour intÃ©grer les liens entres les diffÃ©rents endpoints directement dans les diffÃ©rentes rÃ©ponses.\n  NelmioApiDocBundle pour proposer une documentation complÃ¨te et auto-gÃ©nÃ©rÃ©e depuis le code\n\n\nPour sÃ©curiser tout Ã§a, nous utilisons toujours notre bundle DomainUserBundle permettant de sÃ©curiser et contextualiser les donnÃ©es par sous-domaine (voir notre article dÃ©diÃ© Ã  ce bundle).\n\nâ€¦ mais pas que\n\nUne fois mise en place la thÃ©orie brute, nous nous sommes heurtÃ©s Ã  la rÃ©alitÃ© des choses : face Ã  un modÃ¨le de donnÃ©es complexe, si on reste trÃ¨s strict face Ã  la philosophie RESTful, cela peux demander aux clients de rÃ©aliser un nombre consÃ©quent de requÃªtes afin dâ€™afficher une simple page.\n\nAinsi, nous avons un second applicatif, que nous nommons â€œmiddlewareâ€ qui est un hybride entre une API REST et un catalogue de donnÃ©es prÃ©formatÃ©. Dans cet applicatif, nous rÃ©alisons les agrÃ©gations qui permettent de rÃ©cupÃ©rer de maniÃ¨re unifiÃ©e les donnÃ©es liÃ©es, permettant aux frontaux de rÃ©duire leurs appels.\n\nDans ce middleware, nous essayons tout de mÃªme de respecter au maximum les verbes HTTP et le format de retour pour que les utilisateurs de ces API obtiennent des rÃ©ponses cohÃ©rentes dâ€™un service sur lâ€™autre.\n\nDes donnÃ©es Ã©lastiques\n\nPour que ce middleware puisse retourner des donnÃ©es qui sont stockÃ©es dans plusieurs tables, de maniÃ¨re rapide, tout en gÃ©rant les contraintes de donnÃ©es non publiÃ©es (notre SI contient les anciennes Ã©missions diffusÃ©es, mais Ã©galement celles Ã  diffuser), nous avons fait le choix dâ€™utiliser Elasticsearch en le remplissant avec les donnÃ©es â€œpubliablesâ€.\n\nNon seulement nous disposons dâ€™un systÃ¨me de recherche de donnÃ©es trÃ¨s performant, permettant des requÃªtes trÃ¨s puissantes et trÃ¨s rapides, dans lequel les donnÃ©es sont stockÃ©es de maniÃ¨re optimisÃ©e pour lâ€™utilisation (pas de forme normale Ã  respecter), mais nous nous permettons de nâ€™y stocker que les donnÃ©es disponibles publiquement, simplifiant donc grandement les requÃªtes sur ces donnÃ©es.\n\nWorkerize all the things\n\nPour maintenir les donnÃ©es Ã  jour dans cet index Elasticsearch, nous avons mutualisÃ© sur lâ€™expÃ©rience et le travail que nous avions rÃ©alisÃ© pour RisingStar, qui nous a apportÃ© lâ€™expÃ©rience que des daemons sont beaucoup plus efficaces que des crons. Cette technique nous apporte plusieurs avantages :\n\n\n  ScalabilitÃ© : il est facilement possible de multiplier les process qui traitent les donnÃ©es, et donc dâ€™augmenter la capacitÃ© de traitement\n  RapiditÃ© : le fait dâ€™avoir des daemons qui tournent en continue permet de traiter les demandes dÃ¨s leur arrivÃ©e, et pas lors de la minute suivante. Cela permet aussi de lisser au maximum les traitements sans crÃ©er de piles dâ€™attente inutiles.\n\n\nNous nous sommes donc appuyÃ©s sur notre DaemonBundle pour mettre en place un double systÃ¨me dâ€™indexation :\n\n\n  une fois par jour, lâ€™index est complÃ©tement reconstruit\n  un daemon tourne en continue pour dÃ©tecter les modifications en base de donnÃ©es, et envoyer des messages dans une file RabbitMQ\n  un dernier daemon est dÃ©diÃ© au traitement des messages de cette file pour mettre Ã  jour de maniÃ¨re ciblÃ©e les donnÃ©es dans Elasticsearch\n\n\nAinsi, nous assurons une fraicheur des donnÃ©es quasi-immÃ©diate et optimale.\n\nAu cours de ce travail, nous avons construit 2 nouveaux bundle : ElasticsearchBundle et AmqpBundle. Lâ€™un comme lâ€™autre sont des bundles permettant de faciliter la configuration et lâ€™utilisation des clients natifs dans Symfony2, en tant que service.\n\nEt la grosse donnÃ©e ?\n\nSi vous avez essayÃ© la nouvelle version web de 6play, vous avez certainement remarquÃ© que la personnalisation de votre compte est fortement mise en avant. Pour stocker ce fort volume de donnÃ©es, nous avons fait le choix dâ€™utiliser Cassandra, pour son approche distribuÃ©e permettant une forte scalabilitÃ©, et un ratio rapiditÃ©/redondance optimal.\n\nComme pour le reste, nous avons lÃ  aussi crÃ©Ã© un bundle Symfony2 permettant de configurer et manipuler simplement des clients Cassandra en tant que service : CassandraBundle\n\nTout le reste\n\nCÃ´tÃ© monitoring, pour respecter nos bonnes habitudes, nous utilisons toujours Statsd Ã  outrance, surtout via notre bundle StatsdBundle.\n\nCÃ´tÃ© tests, tous les tests unitaires ont Ã©tÃ© Ã©crits avec atoum.\n\nConclusion\n\nAu cours de ce projet, nous avons eu lâ€™occasion de transformer lâ€™essai de beaucoup de choses que nous avions faites pour RisingStar, de dÃ©couvrir de nouvelles technos et de mettre en place une architecture moderne et adaptÃ©e aux nouveaux challenges des fronts.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #9",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2015/11/06/m6web-dev-facts-9.html",
    "date"     : "November 6, 2015",
    "excerpt"  : "Câ€™est le digital\n\n\n  On a le doigt quelque part â€¦\n\n\nUne vague histoire de pouce\n\n\n  Sache que sans ton pouce, je ne fais rien. (et non, câ€™est pas dÃ©gueulasse)\n\n\nJe croyais que Ã§a rendait sourd plutÃ´t\n\n\n  Nâ€™empeche je me trompe toujours quand je li...",
  "content"  : "Câ€™est le digital\n\n\n  On a le doigt quelque part â€¦\n\n\nUne vague histoire de pouce\n\n\n  Sache que sans ton pouce, je ne fais rien. (et non, câ€™est pas dÃ©gueulasse)\n\n\nJe croyais que Ã§a rendait sourd plutÃ´t\n\n\n  Nâ€™empeche je me trompe toujours quand je lis pom.xml hein\n\n\nDyslexie\n\n\n  \n    X : Il parait que Marven nâ€™arrive pas Ã  hÃ©riter de la config de java par dÃ©faut Â°Â°\n    Y : marven ? connais pas\n    Z : câ€™est lâ€™Ã©quivalent Java de comrposer :)\n  \n\n\nRÃ©gime Fifa\n\n\n  Mince, jâ€™ai oubliÃ© de finir de manger â€¦\n\n\nLa faille\n\n\n  Le plus long dans cette MEP, Ã§a va Ãªtre de se connecter au Wifi\n\n\nSoif\n\n\n  \n    X : Rappel important : on a 12 litres de rosÃ© Ã  boire avant vendredi prochain (et un pack de heinekein).\n    X : si on ne sâ€™y prend pas en avance Ã§a va Ãªtre la panique\n  \n\n\nLa prod de la dev ?\n\n\n  Est ce quâ€™on a la base de donnÃ©es de dev de prod ?\n\n\nError or not error\n\n\n  \n    X : câ€™est parce que tu pars du principe que le message dâ€™erreur a un rapport avec lâ€™erreur\n    Y : euh ce qui devrait Ãªtre le cas non ?\n    X : ouais, mais bonâ€¦\n  \n\n\nCâ€™Ã©tait un message digital\n\n\n  \n    X : pour les massage dâ€™erreur\n    Y : Les massages dâ€™erreur ?\n    Z : humm les massages\n    Y : La premiÃ¨re partie me tente, mais dâ€™erreur ?\n    X : un laius digital\n    Y : oula â€¦ massage â€¦ digital\n  \n\n\nEn Famille\n\n\n  Les mecs ils regardent leur code ! On dirait que câ€™est leur photo de famille\n\n\nDeployception\n\n\n  Pour tester le deployer, il faut le deployer en test, mais pour Ã§a, il faut deployer le deployer en prod pour pouvoir deployer en test\n\n\nFormalisme\n\n\n  En tant quâ€™ordinateur ayant accÃ¨s Ã  lâ€™INTERNET MONDIAL et voulant naviguer sur le site internet de M6 se nommant 6PALY.FR mettre un titre dans la balise &amp;lt;title&amp;gt; â€¦ merci\n\n\nLes gouts et les couleurs\n\n\n  Moi, je ne fais pas confiance aux technos qui ont moins de 15 ans. Sauf si elles viennent du BrÃ©sil\n\n\nLâ€™heure est Ã  la blague\n\n\n  Câ€™est lâ€™heure dammer â€¦\n\n\nUne vÃ©ritÃ© vrai\n\n\n  Quand on dit â€œ24hâ€ gÃ©nÃ©ralement câ€™est 24h\n\n\nAuto-troll\n\n\n  Jâ€™ai crÃ©Ã© un fichier â€œechoUrlâ€ qui Ã©crit dans un fichier â€¦\n\n\nIncohÃ©rence cohÃ©rente\n\n\n  Comme Ã§a, on est cohÃ©rent dans lâ€™incohÃ©rence !\n\n"
} ,
  
  {
    "title"    : "La bÃªta du nouveau 6play est disponible",
    "category" : "",
    "tags"     : " 6play, react, isomorphic, javascript, flux",
    "url"      : "/2015/10/21/beta-nouveau-6play-react-isomorphic.html",
    "date"     : "October 21, 2015",
    "excerpt"  : "Nous vous parlions en fin dâ€™annÃ©e derniÃ¨re sur ce blog, de notre vision de la Single Page App parfaite.\n\nNous avons donc travaillÃ© depuis le dÃ©but dâ€™annÃ©e Ã  la mise en place du nouveau 6play sur cette stack technologique :\n\n\n  React(isomorphic/uni...",
  "content"  : "Nous vous parlions en fin dâ€™annÃ©e derniÃ¨re sur ce blog, de notre vision de la Single Page App parfaite.\n\nNous avons donc travaillÃ© depuis le dÃ©but dâ€™annÃ©e Ã  la mise en place du nouveau 6play sur cette stack technologique :\n\n\n  React(isomorphic/universal) avec du Node.js en backend\n  Fluxible pour la gestion de Flux client et serveur\n  webpack pour la gestion du build js cotÃ© client\n  React Router pour le routing\n  ES6 avec Babel parce que.\n\n\nAu niveau tests, et parce que nous ne concevons plus de dÃ©velopper de tels projets sans une approche qualitÃ© complÃ¨te :\n\n\n  ESLint pour le respect des conventions de codage\n  Jest pour les tests unitaires\n  Cucumber.js, WebdriverIO et PhantomJS pour les tests fonctionnels\n  superagent-mock (\\o/) pour mocker les requÃªtes HTTP des services externes\n  Jenkins pour lâ€™intÃ©gration continue\n  React Hot Loader pour amÃ©liorer la DX (Developer eXperience)\n\n\nDepuis lundi, vous pouvez dÃ©sormais tester la bÃªta de ce service vidÃ©o Ã  lâ€™adresse suivante : https://beta.6play.fr\n\n\n\nPour ceux qui veulent en savoir plus sur cette refonte (notamment front-end), une confÃ©rence sera tenue par Kenny Dits (@kenny_dee) lors de Blend Web Mix, le 29 octobre Ã  Lyon Ã  16h.\n"
} ,
  
  {
    "title"    : "Simplifiez vous la vie avec les hooks Git",
    "category" : "",
    "tags"     : " git, hooks, workflow, composer, coke",
    "url"      : "/2015/09/09/hooks-git.html",
    "date"     : "September 9, 2015",
    "excerpt"  : "La stack de base de tout projet un minimum sÃ©rieux Ã  tendance Ã  devenir de plus en plus lourde.\n\nChez M6Web, et particuliÃ¨rement dans la team Burton, nous dÃ©veloppons principalement des projets Symfony2.\nCette stack est donc composÃ©e, entre autre ...",
  "content"  : "La stack de base de tout projet un minimum sÃ©rieux Ã  tendance Ã  devenir de plus en plus lourde.\n\nChez M6Web, et particuliÃ¨rement dans la team Burton, nous dÃ©veloppons principalement des projets Symfony2.\nCette stack est donc composÃ©e, entre autre de coke et de composer.\n\nConcernant coke, lâ€™idÃ©e est de ne jamais versionner de code qui ne respecte pas les standards de dÃ©veloppements.\nInterdit les commits â€œfix standardsâ€ ou autre â€œfix cokeâ€ qui alimentent une PR avant dâ€™Ãªtre rebasÃ©s !\n\nConcernant composer, il sâ€™agit de toujours travailler sur la â€œbonne versionâ€ des dÃ©pendances. \nCertes, composer (via le composer.lock) permet dâ€™Ãªtre sÃ»r que toute personne qui lance un composer install aura la mÃªme version des dÃ©pendances.\nToutefois, il faut encore penser Ã  lancer cette commande, surtout lorsque lâ€™on rÃ©cupÃ¨re du code depuis le repository central oÃ¹ le composer.lock peut avoir Ã©voluÃ©.\n\nPour rÃ©pondre Ã  ces besoins, nous nous sommes basÃ©s sur lâ€™excellent systÃ¨me de hooks de git et nous avons dÃ©veloppÃ© 2 petits scripts.\n\ncheck-coke.sh\n\nCoke est un petit utilitaire qui facilite le lancement de PHP_CodeSniffer sur son projet.\nToutefois, ce dernier est relativement lent, surtout sur les projets composÃ©s dâ€™un grand nombre de fichiers.\nPour optimiser son exÃ©cution, le script â€œcheck-code.shâ€ se charge de lancer coke uniquement sur les fichiers en cours de modification dâ€™un point de vue Git.\nAinsi, son exÃ©cution est extrÃªment rapide, ce qui permet de lâ€™exÃ©cuter en pre-commit pour ne jamais commiter un code ne respectant pas les standards de dÃ©veloppement\n\ncheck-composer\n\nComme indiquÃ© plus haut, le soucis avec composer est quâ€™il faut penser Ã  lancer la commande composer install pour sâ€™assurer que les dÃ©pendances sont Ã  jour.\nNon seulement cette commande prend un certains temps, mais il faut surtout penser Ã  la lancer, mÃªme pour des actions qui semblent anodines, comme changer de branche.\n\nPartant de ce constat, nous avons crÃ©Ã© le script â€œcheck-composer.shâ€, qui vÃ©rifie sâ€™il y a une diffÃ©rence entre la version de dÃ©part et dâ€™arrivÃ©e du fichier composer.lock, et qui lance la commande composer install si nÃ©cessaire.\n\nNâ€™hÃ©sitez pas Ã  les essayer et nous faire part de vos retours, voir de proposer vos hooks.\nLe but de ce repository partagÃ© est de nous simplifier la vie en nous permettant de ne plus penser aux outils qui sont autour de notre code, mais de nous concentrer sur ce que nous avons Ã  faire.\n"
} ,
  
  {
    "title"    : "CR React Europe ConfÃ©rence 2015 - Day 2",
    "category" : "",
    "tags"     : " javascript, react, reactnative, video, graphql",
    "url"      : "/2015/07/10/cr-react-europe-2015-day-two.html",
    "date"     : "July 10, 2015",
    "excerpt"  : "AprÃ¨s une premiÃ¨re journÃ©e pleine de nouveautÃ©s et dâ€™annonces, voici la suite du compte rendu avec un programme encore trÃ¨s chargÃ© pour cette deuxiÃ¨me journÃ©e de la React Europe.\n\nImproving Your Workflow With Code Transformation\n\n\n(crÃ©dits : Fabie...",
  "content"  : "AprÃ¨s une premiÃ¨re journÃ©e pleine de nouveautÃ©s et dâ€™annonces, voici la suite du compte rendu avec un programme encore trÃ¨s chargÃ© pour cette deuxiÃ¨me journÃ©e de la React Europe.\n\nImproving Your Workflow With Code Transformation\n\n\n(crÃ©dits : Fabien Champigny - React Europe)\n\nNous commenÃ§ons la journÃ©e avec le crÃ©ateur du fameux Babel: Sebastian McKenzie.\nBabel est un transpiler JS permettant de transformer le code ES6/7 en code ES5.\nAprÃ¨s un petit historique sur le nom, car cet outil sâ€™appelait 6to5 avant lâ€™arrivÃ©e dâ€™ES7, pour finalement se renommer Babel :)\nLâ€™adoption par la communautÃ© a ensuite Ã©tÃ© assez massive !\n\nSÃ©bastian nous explique le fonctionnement interne de Babel avec le dÃ©coupage en 3 sections : Parser / Transformer / Generator\nIls utilisent lâ€™AST (abstract syntax tree) pour avoir une â€œdata structureâ€ du code et pouvoir faire des traitements sur cette structure.\nOn rentre trÃ¨s (trop ?) en profondeur dans les bas-fond de Babel, afin de partager les diffÃ©rentes difficultÃ©s et trucs et astuces pour les transformations que Babel rÃ©alise.\n\nLe talk se finit sur le futur de Babel, qui sera Ã  chercher du cÃ´tÃ© de :\n\n\n  Dead code elimination/minification\n  constant folding/static evaluation\n  static analysis / linting\n\n\nSteven Lusher, lâ€™un des dev Facebook travaillant sur Relay vient de mettre un blog post sur le site Babel concernant lâ€™utilisation ES6 de React\n\n\n\nThe State of Animation in React\n\n\n(crÃ©dits : Fabien Champigny - React Europe)\n\n\n\nCheng Lou fait le point sur les animations en React et nous prÃ©sente sa nouvelle approche react-motion.\n\nIl est convaincu quâ€™il faut abandonner les ReactCssTransitionGroup au profit des animations basÃ©es sur des interpolations â€œÃ -la-flashâ€.\n\nCSS Transitions\nLes Transitions CSS prÃ©sentent plusieurs dÃ©faut, elles sont difficiles Ã  controler et elles sont Ã©troitement liÃ©es au DOM. En revanche, elles sont plutÃ´t performantes, non bloquantes et rÃ©pondent Ã  la plupart des usages.\n\nDeclarative Tweens\nLes Declarative Tweens sont une solution alternative interessante qui permettent de composer une animation selon des critÃ¨res prÃ©cis (dÃ©but, durÃ©e, direction, â€¦). Cette solution prÃ©sente aussi lâ€™avantage de pouvoir crÃ©er des animations composÃ©es de plusieurs sous-animations et dâ€™interrompre leur execution sur demande.\n\nSpring\nCheng Lou a dÃ©veloppÃ© une librairie appelÃ©e &amp;lt;Spring /&amp;gt; qui parlera aux anciens Flasheurs, tout comme lui, puisquâ€™elle reprend le principe dâ€™interpolation et de courbes. Elle permet de dÃ©finir une animation sur un composant React, en prÃ©cisant sur chacune de ses propriÃ©tÃ©s des critÃ¨res simples de transformation. La librairie se charge dâ€™interpoler la structure du composant pour une animation fluide.\n\ndÃ©mo de Spring\n\n\n\nSimplifying the data layer\n\n\n(crÃ©dits : Fabien Champigny - React Europe)\n\nKevin Robinson nous prÃ©sente comment Twitter utilise React. Lâ€™approche prÃ©sentÃ©e fait la part belle au â€œfonctionnelâ€, Ã  lâ€™image de leur infrastructure backend.\n\nIl nous dÃ©taille les mÃ©canismes mis en place pour lâ€™accÃ¨s aux donnÃ©es, notamment la possibilitÃ© de gÃ©rer de maniÃ¨re dÃ©clarative les dÃ©pendances aux donnÃ©es au niveau des composants.\nAu niveau des stores, il prone lâ€™utilisation de structures immuables en stockant un â€œlogâ€ dâ€™Ã©vÃ©nement et en utilisant des â€œreducersâ€ pour en extraire lâ€™Ã©tat des donnÃ©es rÃ©el.\n\nOn retrouve dans leur approche beaucoup de concepts de la programmation fonctionnel (de la mÃªme maniÃ¨re que Redux), mais malheureusement, aucun code nâ€™est ouvert par Twitter Ã  ce sujet.\n\nOn reste un peu sur notre faim en ne pouvant pas aller jouer â€œconcrÃ¨tementâ€ avec leurs outils.\n\nGoing Mobile with React\n\n\n(crÃ©dits : Fabien Champigny - React Europe)\n\nJed Watson, crÃ©ateur du framework TouchStone JS, un framework JS (basÃ© sur React) permettant de faire des applications mobiles hybride (Ã  base de Webview via Apache Cordova), nous explique comment rÃ©aliser des applis hybride grÃ¢ce Ã  React.\n\nLe dÃ©bat ici est plutÃ´t de dÃ©montrer quâ€™on peut malgrÃ© les dires de certains et en connaissant quelques astuces, faire une appli mobile hybride qui ressemblera Ã  une appli native. Pour nous prouver cela, Jed annonce que lâ€™appli de la React Europe, dispo sur iOS et Android, et que nous avons tous utilisÃ© a Ã©tÃ© faite avec TouchStone JS !\n\n\n  If you have great developer experience, you are much more likely to get to a great UX\n\n\nJed conseille de ne pas faire de lâ€™hybride lorsquâ€™on :\n\n\n  est Facebook ou Twitter\n  a beaucoup de donnÃ©es\n  a une utilisation processeur intensive\n  a des animations complexes sur lâ€™UI\n  a des interactions complexes\n  a de la gestion mÃ©moire avancÃ©e\n\n\nLes points les plus importants pour quâ€™une application hybride fonctionne sont :\n\n\n  React\n  la gestion du Touch\n  le Layout\n  la gestion de la Nav\n\n\n\n  You should not do everything in a webview, but you can\n\n\nLa dÃ©mo prÃ©sente lâ€™ensemble des composants et des transitions disponibles. En plus de React, Touchstone.js utilise cordova, une bibliothÃ¨que dâ€™APIs permettant dâ€™accÃ©der en Javascript aux fonctions natives du mobile, comme lâ€™accÃ©lÃ©romÃ¨tre, le GPS ou lâ€™appareil photo.\n\nLe code de lâ€™application React Europe sera rendu open-source Ã  la fin de la dÃ©mo : Sketch &amp;amp; Code de lâ€™app React Europe\n\n\n\nReact Router\n\n\n(crÃ©dits : Fabien Champigny - React Europe)\n\nBelle prÃ©sentation de Michael Jackson (@mjackson), pas aussi spectaculaire quâ€™un concert de lâ€™artiste homonyme, mais assez surprenante et dÃ©lirante quand mÃªme ! Il introduit la librairie quâ€™il porte avec Ryan Florence (@ryanflorence) depuis plus dâ€™un an et qui est majoritairement utilisÃ©e par les utilisateurs de React pour mettre du routing dans leur application : React Router, â€œTheâ€ Router.\n\nMichael commence par nous prÃ©senter les bases de la librairie : la dÃ©finition des routes dans un composant React et le composant Link permettant de gÃ©nÃ©rer les liens. Il explique que ce sont des concepts simples et quâ€™ils permettent Ã  de nouveaux dÃ©veloppeurs peu expÃ©rimentÃ©s de rentrer facilement dans un projet.\n\nIl fait ensuite lâ€™analogie entre les vues et les URLs, affirmant que de bonnes URLs, bien formÃ©es, augmentent la confiance de lâ€™utilisateur envers lâ€™application. Une notion importante dans React Router est celle des transitions permettant de changer de vues (et donc dâ€™URLs) et de gÃ©rer le â€œbrowser historyâ€.\n\nMichael nous annonce une nouveautÃ© dans la prochaine version : lâ€™attribut onEnter sur la dÃ©finition de route, permettant dâ€™executer une callback avant dâ€™afficher la page (utile par exemple pour protÃ©ger une page par authentification).\n\nIl nous expose sa vision du composant comme une fonction prenant en entrÃ©e props et state et renvoyant en sortie une UI. Le router nâ€™est finalement quâ€™un composant comme un autre qui reÃ§oit en entrÃ©e lâ€™URL. Lâ€™idÃ©e que ce qui est explicite est bien meilleur que ce qui est â€œmagiqueâ€ dans une implÃ©mentation lui permet de prÃ©senter les changements de lâ€™API dans les derniÃ¨res versions du React Router avec la rÃ©cupÃ©ration des paramÃ¨tres de lâ€™URL via les props du composant (et plus via une mixin) ou la disparition du composant RouteHandler qui peut simplement Ãªtre remplacÃ© par props.children pour utiliser les â€œnested routesâ€ dans ses composants.\n\nDans les travaux en cours, on retiendra les transitions animÃ©es qui permettent Ã  Michael de faire une dÃ©mo â€œwahouâ€. Lâ€™animation est, bien entendu, rÃ©pÃ©tÃ©e en sens inverse sur lâ€™utilisation du back du navigateur. Cette fonctionnalitÃ© a dâ€™autant plus dâ€™importance que changement dâ€™URLs et animations ne sont traditionnellement pas de bons amis et posent souvent problÃ¨me.\n\nLe â€œdynamic routingâ€ est la deuxiÃ¨me dÃ©mo montrant la possibilitÃ© de contextualiser lâ€™ouverture dâ€™une URL : sur une page, un contenu peut Ãªtre ouvert dans une popup, mais en copiant et ouvrant lâ€™URL obtenue dans un autre onglet, on a une page avec le mÃªme contenu mais une prÃ©sentation diffÃ©rente (plus de popup).\n\nEnfin, le clou du spectacle sera la derniÃ¨re dÃ©mo qui Ã©chouera (un dernier commit sur le repo qui aurait provoquÃ© une erreur) qui nous aura valu une fabuleuse danse de Ryan Florence sur la scÃ¨ne (venu en renfort de Michael) ! Lâ€™idÃ©e initiale Ã©tait de prÃ©senter une fonctionnalitÃ© assez Ã©norme permettant de lazy loader les JS de sa SPA en fonction des besoins de chaque route (le â€œgradual loadingâ€) Ã©vitant de charger dÃ¨s le dÃ©part les 3Mo de son bundle webpack alors quâ€™on en utilise quâ€™une petite partie. Il faudra attendre pour voir cette fonctionnalitÃ© en actionâ€¦\n\nCreating a GraphQL Server\n\nAprÃ¨s la confÃ©rence de la veille sur GraphQL, Nick Schrock et Dan Schafer nous montrent comment rÃ©aliser un serveur GraphQL.\n\nGraphQL est une spÃ©cification dâ€™Ã©change et ne prÃ©suppose aucune technologie backend. Comme nous lâ€™avons dÃ©jÃ  vu dans la confÃ©rence prÃ©cÃ©dente, lâ€™idÃ©e est de faire de GraphQL une couche entre le client et le code backend dÃ©jÃ  existant.\n\nFacebook, en ouvrant cette spÃ©cification et lâ€™implÃ©mentation de rÃ©fÃ©rence, espÃ¨re fÃ©dÃ©rer une communautÃ© autour de cette solution. Si dâ€™autres personnes rÃ©alisent des implÃ©mentations dans diffÃ©rents langages, cela permettrait Ã  tout le monde de capitaliser sur ces techniques et de faciliter la rÃ©utilisation de code que ce soit cÃ´tÃ© client ou serveur.\n\nLa â€œstackâ€ imaginÃ©e pour GraphQL est prÃ©sentÃ©e en partant du serveur jusquâ€™au client :\n\n\n  GraphQL App Servers\n  Libs (Parse, SQL)\n  Core\n  Spec\n  Common tools (ex: graphicQL, IDE-like tool)\n  Client SDKs (Relay)\n  GraphQL Clients\n\n\nAu delÃ  de la prÃ©sentation thÃ©orique, on se pose quand mÃªme la question de la mise en oeuvre concrÃªte au dessus de code existant.\nFacebook utilise maintenant intensivement GraphQL. Par contre, ils nâ€™utilisent pas lâ€™implÃ©mentation de rÃ©fÃ©rence mais sans doute une implÃ©mentation trÃ¨s imbriquÃ©e Ã  leur backend (et donc difficile Ã  rendre open-source).\nOn manque malheureusement de retours sur des questions de mise en oeuvre comme le cache ou la gestion des droits par exemple.\nEspÃ©rons que des â€œearly-adoptersâ€ puissent nous faire des retours lÃ -dessus dans les semaines/mois Ã  venir.\n\nPour en savoir plus, un bon article sur le sujet : GraphQL overview : Getting start with GraphQL and Node.JS\n\nIsomorphic Flux\n\n\n(crÃ©dits : Fabien Champigny - React Europe)\n\nMichael Ridgway (@theridgway) aborde une notion souvent abordÃ©e ces 2 jours et sur laquelle nous avions fait un article en dÃ©cembre dernier.\n\nSelon Michael, les avantages du â€œserver renderingâ€ sont multiples :\n\n\n  le SEO\n  le support des anciens navigateurs\n  le gain de performance perÃ§u par lâ€™utilisateur\n\n\nUn des objectifs de cette dÃ©marche est de partager le maximum de code entre le serveur et le client.\n\nLa stack proposÃ©e par Michael est la suivante :\n\n\n  pour la gestion des vues, React Ã©videmment qui expose une API client et serveur\n  pour le routing, React Router (https://github.com/rackt/react-router)\n  pour le data fetching, superagent (https://github.com/visionmedia/superagent)\n  pour la logique applicative, un pattern lÃ©ger et cÃ©lÃ¨bre : Flux\n\n\nPour la mise en oeuvre de Flux cÃ´tÃ© serveur, nous avons dÃ©jÃ  vu au cours de ces 2 journÃ©es : Redux et React Nexus. Il en existe dâ€™autres comme marty.js, flummox ou alt. Michael nous propose Fluxible, la librairie dÃ©veloppÃ©e par Yahoo.\n\nFluxible crÃ©e un contexte pour chaque requÃªte cÃ´tÃ© serveur avec un dispatcher custom optimisÃ© pour cette opÃ©ration. Lâ€™Ã©tat de lâ€™application est transmis du serveur vers le client grÃ¢ce Ã  un mÃ©canisme de dÃ©shydratation/rÃ©hydratation des stores.\n\nMichael prÃ©cise que Fluxible force les dÃ©veloppeurs Ã  utiliser Flux de maniÃ¨re conforme sans transgresser les pratiques dÃ©finies par le modÃ¨le. La librairie fournit des composants de haut niveau permettant une parfaite intÃ©gration avec React. Enfin, la particularitÃ© de Fluxible est son systÃ¨me de plugins permettant de faciliter lâ€™ajout de nouvelles fonctionnalitÃ©s.\n\nMichael nous montre un exemple de chat isomorphique et la diffÃ©rence observÃ©e au chargement avec une SPA classique. Il prÃ©cise ensuite les outils de dÃ©veloppement quâ€™il utilise :\n\n\n  Babel\n  ESLint\n  webpack\n  babel-loader\n  Grunt / Gulp\n  Yeoman Generators\n\n\nMichael termine sa conf en indiquant que plusieurs applications en prod chez Yahoo utilisent la stack prÃ©sentÃ©e et Fluxible mais quâ€™il reste encore quelques amÃ©liorations Ã  apporter pour les raisons suivantes :\nLes dÃ©pendances des composants envers les donnÃ©es ne sont pas facilement connues (rendant le data fetching en amont du rendering cÃ´tÃ© serveur dÃ©licat). Relay pourrait Ãªtre une solution.\nLe rendu cÃ´tÃ© serveur de React est relativement lent (mais pourrait Ãªtre amÃ©liorÃ© dans les futures version de React).\nLe Hot Reloading (avec React Hot Loader) ne fonctionne pas avec les stores Fluxible.\n\nConclusion\n\nQue dire aprÃ¨s ces deux jours de confÃ©rence ? \nDÃ©jÃ  que la communautÃ© et lâ€™engouement autour de React ne cesse de grandir, mais aussi que ca ne chÃ´me pas cotÃ© Facebook avec Relay, GraphQL, Animated, React Native Android qui ne devraient pas tarder Ã  pointer le bout de leur nez, avec aussi la mise en place dâ€™une personne full time sur Jest ! Câ€™est rassurant sur lâ€™avenir court/moyen terme de React.\n\nLâ€™organisation Ã©tait vraiment impeccable (mise Ã  part les soucis de climatisation) avec beaucoup de trÃ¨s bonnes idÃ©es, notamment, les bureaux au fond et sur les cotÃ©s de la salle de confÃ©rence pour que les personnes avec LapTop puissent suivre confortablement.\n\nCâ€™est aussi plutÃ´t Ã©tonnant, pour une confÃ©rence en France, dâ€™avoir vu aussi peu de personnes francophones. Le public Ã©tant trÃ¨s majoritairement anglophone. On se dit que React nâ€™a pas encore complÃ¨tement pris en France.\n\nCotÃ© tendance, on voit quâ€™au niveau des librairies Flux, Redux parait clairement Ãªtre celle qui attire tous les buzz. A voir dans le temps si cela suit, mais le talent indÃ©niable de son crÃ©ateur, combinÃ© aux bonnes idÃ©es (reducers, hot reload) donne vraiment envie de sâ€™y pencher. On regrette aussi toujours le manque de sujets autour des tests.\n\nNous attendons aussi impatiemment React Native Android, pour voir si le buzz et les superbes promesses sont toujours prÃ©sentes avec deux environnements cibles, et on espÃ¨re voir sur nos stores de plus en plus dâ€™applis React Native.\n\nGraphQL + Relay parait vraiment Ãªtre la solution idÃ©ale pour rÃ©aliser simplement du data fetching cotÃ© client (React Web ou React Native), mais lâ€™absence de Relay (toujours pas open-sourcÃ©), combinÃ©e au manque de retour sur GraphQL pose encore de nombreuses questions.\n\nNous avons donc hÃ¢te dâ€™Ãªtre Ã  la prochaine React ConfÃ©rence ou React Europe pour voir la suite de lâ€™Ã©volution de React.\n\nVous pouvez retrouvez le compte rendu de la premiÃ¨re journÃ©e ici\n"
} ,
  
  {
    "title"    : "CR React Europe ConfÃ©rence 2015 - Day 1",
    "category" : "",
    "tags"     : " javascript, react, reactnative, video, graphql",
    "url"      : "/2015/07/06/cr-react-europe-2015-day-one.html",
    "date"     : "July 6, 2015",
    "excerpt"  : "AprÃ¨s la premiÃ¨re confÃ©rence officielle sur React, que nous avons dÃ©jÃ  couvert en janvier (Jour 1 et Jour 2), nous nous sommes rendus les 2 et 3 juillet Ã  Paris sous une chaleur infernale pour cette premiÃ¨re Ã©dition de la React Europe avec lâ€™envie...",
  "content"  : "AprÃ¨s la premiÃ¨re confÃ©rence officielle sur React, que nous avons dÃ©jÃ  couvert en janvier (Jour 1 et Jour 2), nous nous sommes rendus les 2 et 3 juillet Ã  Paris sous une chaleur infernale pour cette premiÃ¨re Ã©dition de la React Europe avec lâ€™envie de voir et de mesurer les Ã©volutions autour de ReactJS.\n\nKeynote\n\nAu dÃ©part, React câ€™Ã©tait simplement le V de MVC. Maintenant, on parle de â€œView Firstâ€ ou â€œ User interface Firstâ€.\n\nChristopher Chedeaux @vjeux, lâ€™un des core-dev de React, va faire un focus sur 4 axes principaux :\n\n\n  Data\n  Language\n  Packager\n  Targets\n\n\n1) Data\n\nDepuis lâ€™annonce de Flux ont fleuri beaucoup dâ€™autres implÃ©mentations du pattern, notamment :\n\n\n  Mcfly\n  Barracks\n  Reflux\n  Fluxy\n  Fluxxor\n  Redux\n\n\nDâ€™aprÃ¨s Christopher, certaines vont mourir dans les prochains mois laissant seulement la place aux implÃ©mentations les plus pertinentes (et Redux a fait un buzz sans pareil lors de ces 2 jours, voir plus bas).\n\nLâ€™immuabilitÃ© revient aussi Ã©normÃ©ment en regardant du cotÃ© de ClojureScript ou ImmutableJS.\n\nCotÃ© Data fetching, cela commence Ã  bouger pas mal avec :\n\n  Relay et GraphQL\n  Falcon &amp;amp; JSON Graph\n  Flux over the wire\n  Om Next\n\n\nIl reste encore les cotÃ©s Persistence et Temps rÃ©el qui ne sont pas traitÃ©s dans lâ€™Ã©cosystÃ¨me de React.\n\n2) Languages\n\nLe langage JS a Ã©normement Ã©voluÃ© avec CoffeeScript, jsTransform (utilisÃ© chez facebook pour la gestion du jsx, â€œinternalization pipelineâ€, â€¦)\n\n\n  â€œthink of js as a compile targetâ€\n\n\nIl y a eu Traceur et Recast, et dÃ©sormais Babel qui a tout ecrasÃ© sur son passage. Facebook convertit en ce moment tout son code Front JS Ã  Babel.\n\nOn retrouve aussi ESLint, un â€œlinterâ€ de code, et du typage de donnÃ©es avec TypeScript et Flow.\n\n3) Packager\n\nNous retrouvons Node.js, CommonJS, npm. \nDans le browser : Browserify et Webpack.\nMÃªme sâ€™il y a encore du travail Ã  faire pour avoir de bonnes performances, et ne pas attendre une compilation via les mises Ã  jour incrÃ©mentales, ou React Hot Loader sur lequel nous reviendrons.\n\n4) Targets :\n\nLes cibles de React sont dÃ©sormais multiples grÃ¢ce au Virtual DOM :\n\n\n  DOM\n  SVG\n  Canvas\n  Terminal\n\n\nUn focus est ensuite fait sur React Native, permettant de dÃ©velopper des apps natives sur iOS et Android tout en faisant du React.\n\n\n  â€œUX of a native app / DX of a web appâ€\n\n\nChristopher insiste sur le terme DX quâ€™on ne voit jamais dans des slides tech, signifiant â€œDevelopper Experienceâ€. \nIl compare aussi le dÃ©veloppement de lâ€™appli Ads de Facebook, rÃ©alisÃ© avec React Native sur iOS (7 ingÃ©nieurs pendant 5 mois), et celui qui a suivi avec React Native Android avec les mÃªmes 7 ingÃ©nieurs durant seulement 3 mois en rÃ©utilisant 87% du code !\n\nReact Native Android sera open-sourcÃ© au mois dâ€™AoÃ»t.\n\n\n  â€œLearn once : write anywhereâ€\n\n\nUn appel est fait pour stopper le â€œbashingâ€ sur les autres frameworks. Câ€™est en travaillant main dans la main entre les communautÃ©s Ember, Angular et React notamment que le web avancera.\n\n\n\n\n\nInline Styles: themes, media queries, contexts, and when itâ€™s best to use CSS\n\nStyle are not CSS\n\nMichael Chan @chantastic va nous soumettre une â€œterribleâ€ idÃ©e lors de cette conf qui va en faire crier plus dâ€™un ! â€œItâ€™s time to learn CSSâ€ est une phrase dâ€™une autre Ã©poque, Michael nâ€™hÃ©site dâ€™ailleurs pas Ã  qualifier cette idÃ©e de bullshit !\n\nCitant Jeremy Ashkenas @jashkenas, crÃ©ateur de CoffeeScript et de Backbone.js, il soumet une nouvelle vision : unifier les 3 syntaxes (CSS, HTML et JS) qui permettent de dÃ©clarer le style dâ€™une application web car contrairement Ã  ce quâ€™on pense â€œle style nâ€™est pas le CSSâ€.\n\nMichael dÃ©fend 2 autres axes importants dans React :\n\n\n  les changements de lâ€™Ã©tat de lâ€™application (pilotÃ© en JS via les â€œstatesâ€ des composants) sont des changements de lâ€™UI,\n  les composants doivent Ãªtre rÃ©utilisÃ©s comme partie entiÃ¨re et indÃ©pendante et ne doivent pas Ãªtre dÃ©tournÃ©s de leur vocation initiale, â€œje prÃ©fÃ¨re avoir 1000 composants qui font 1 choses que 100 composants qui font 2 chosesâ€.\n\n\nStyle over the time\n\nMichael reprend ensuite lâ€™histoire des CSS. A lâ€™origine, on dÃ©clarait les styles dans lâ€™attribut HTML â€œstyleâ€. Puis, on sâ€™est rendu compte de cette faÃ§on que le code Ã©tait dupliquÃ©, dâ€™oÃ¹ lâ€™introduction et la dÃ©claration des classes CSS. Le web est devenu sÃ©mantique avec lâ€™utilisation des balises &amp;lt;h1&amp;gt;,&amp;lt;p&amp;gt;, &amp;lt;b&amp;gt;, etc. sÃ©parant la prÃ©sentation dans le HTML et le CSS. Lâ€™arrivÃ©e du web 2.0 a donnÃ© au JS le moyen dâ€™intÃ©rargir avec le HTML pour diriger le comportement de lâ€™application complÃ©tant la couche prÃ©sentation HTML + CSS.\n\nNot coupled state\n\nAvec le web interactif actuel, lâ€™Ã©tat de lâ€™application est noyÃ© entre ces 3 parties constituantes. Heureusement, React permet dâ€™organiser la structure en faisant du â€œstateâ€ la partie centrale de lâ€™application et le â€œmarkupâ€, confondu avec le JS, devient lâ€™interface. NÃ©anmoins, lâ€™Ã©tat de lâ€™application est toujours couplÃ© avec la prÃ©sentation et le CSS. GrÃ¢ce Ã  lâ€™exemple dâ€™une todolist basique, Michael explique comment extraire le â€œstateâ€ des CSS (reprÃ©sentÃ© par la classe â€œis-completeâ€) pour lâ€™intÃ©grer en inline dans le render du composant React. Le CSS devient uniquement une couche gÃ©rant lâ€™apparence de lâ€™application et les composants (donc le JS) gÃ¨re intÃ©gralement leur Ã©tat.\n\nNo more CSS\n\nMichael nous montre enfin comment aller plus loin en gÃ©rant variables de style, pseudo-classes et pseudo-elements en inline dans le composant, et sans trop de difficultÃ©s. La gestion des hovers et des media queries est beaucoup plus ardue et nâ€™est clairement pas recommandÃ©. Lâ€™utilisation dâ€™une librairie comme Radium (mais il en existe dâ€™autres) permet de surmonter cet obstacle et dâ€™Ã©crire du style inline trÃ¨s clairement. On aborde quelques conseils pour gÃ©rer au mieux les couleurs et le layout. Pour voir un exemple illustrant tous les concepts abordÃ©s par Michael, vous pouvez explorer son projet React Soundplayer.\n\nPour conclure sa conf, Michael cite Sandi Metz @sandimetz, designeuse Ruby, dÃ©fendant lâ€™idÃ©e que lâ€™objectif du design est de permettre de (re-)designer plus tard son application et donc de rÃ©duire les coÃ»ts du changement. Le composant React est lâ€™interface, il se suffit Ã  lui-mÃªme.\n\nLes slides sur SpeakerDeck\n\n\n\nFlux over the Wire\n\nElie Rotenberg @elierotenberg introduit Flux, le pattern crÃ©Ã© par Facebook massivement utilisÃ© avec React pour gÃ©rer le cycle de vie des donnÃ©es Ã  lâ€™intÃ©rieur de son application. Le fondement de Flux est de pouvoir partager les Ã©tats de lâ€™application (les â€œstatesâ€) de faÃ§on simple et scalable entre lâ€™ensemble de ses composants car tous nâ€™ont pas que des rÃ©percussions locales.\n\nElie nous montre quâ€™on peut voir Flux comme un modÃ¨le symÃ©trique : les composants React sont le miroir des stores (lÃ  oÃ¹ sont stockÃ©s les states de lâ€™application) et les actions dÃ©clenchÃ©es par les composants sont le pendant des Ã©vÃ¨nements de mise Ã  jour des stores. Le pattern tourne donc autour de 4 mÃ©thodes â€œsymÃ©triquesâ€ : onUpdate/dispatch cÃ´tÃ© composant et onDisptach/update cÃ´tÃ© store. La nouveautÃ© mise en exergue par Elie est de considÃ©rer que le flux entre les composants et les stores peut Ãªtre implÃ©mentÃ© par nâ€™importe quel canal de communication : callbacks/promises par exemple mais aussi streams/EventEmitter et, plus Ã©tonnant, websockets. Ce dernier canal permettrait de partager lâ€™Ã©tat de son application entre plusieurs composants existants sur de multiples clients grÃ¢ce aux stores qui persisteraient sur un serveur node distant. Elie donne lâ€™exemple dâ€™un chat fonctionnant sur ce principe.\n\nIl prÃ©sente ensuite les librairies quâ€™il a Ã©laborÃ© autour de ses idÃ©es :\n\n\n  nexus-flux implÃ©mentant le pattern Flux de maniÃ¨re â€œclassiqueâ€, notamment autour de lâ€™EventEmitter,\n  nexus-flux-socket.io, lâ€™implÃ©mentation de Flux autour des websockets,\n  react-nexus une surcouche aux prÃ©cÃ©dentes librairies permettant dâ€™Ã©couter les stores depuis les composants React en utilisant les decorators ES7,\n  react-nexus-chat, lâ€™implÃ©mentation du chat donnÃ© en exemple.\n\n\nUne des forces de sa librairie est la facilitÃ© Ã  mettre en oeuvre lâ€™asynchronisme des actions Flux cÃ´tÃ© serveur.\n\nEnfin, on dÃ©couvre lâ€™utilisation rÃ©el de ces concepts chez Webedia :\n\n\n  Utilisation de PostgreSQL, Redis et Varnish pour la tenue en charge,\n  React Nexus est utilisÃ© pour la gestion des commentaires et le systÃ¨me utilisateur de millenium.org,\n  Une refonte complÃ¨te de jeuxvideo.com est en cours avec React Nexus,\n  Des modules React sont dÃ©jÃ  prÃ©sents sur dâ€™autres sites de Webedia.\n\n\nLes slides sur SpeakerDeck\n\n\n\nReact Native: Building Fluid User Experiences\n\nSpencer Ahrens @sahrens2012 de chez Facebook nous prÃ©sente une librairie, qui devrait Ãªtre open sourcÃ© sous peu pour gÃ©rer les animations dans React Native iOS : Animated.\n\n \nvar { Animated } = require(â€˜react-nativeâ€™) \n\nCette librairie devrait marcher directement sur React Native Android et arriver ensuite sur le web.\nLâ€™implÃ©mentation est 100% JS.\nNous avons suivi un live coding dÃ©mo sur iOS dâ€™une application sans animation au dÃ©part, consistant Ã  enrichir lâ€™expÃ©rience utilisateur en rajoutant des animations fluides via la librairie Animated.\n\nLe code des exemples et les slides, ainsi quâ€™un nouvel exemple sur lâ€™animation â€œTinderâ€\n\n\n\nExploring GraphQL + Relay: An Application Framework For React\n\n\n\nLee Byron @leeb a introduit GraphQL, une solution permettant de rÃ©soudre les problÃ©matiques dâ€™accÃ¨s aux donnÃ©es.\nLâ€™idÃ©e est de rÃ©soudre les problÃ¨mes de lâ€™approche RESTful (qui entraÃ®ne beaucoup dâ€™aller-retours avec le serveur) et lâ€™approche FQL (variante de SQL permettant de limiter les aller-retours, mais trÃ¨s compliquÃ©e Ã  maintenir).\n\nGraphQL permet au client de dÃ©finir trÃ¨s prÃ©cisÃ©ment les donnÃ©es quâ€™il souhaite obtenir via leur relations.\n\nLe principe de base est que la structure de la requÃªte permet de dÃ©finir le format de la rÃ©ponse. Ex :\n\nQuery\n{\n  user(id: 4) {\n    id,\n    name,\n    smallPic: profilePic(size: 64),\n    bigPic: profilePic(size: 1024)\n  }\n}\n\nResponse\n{\n  &quot;user&quot;: {\n    &quot;id&quot;: 4,\n    &quot;name&quot;: &quot;Mark&quot;,\n    &quot;smallPic&quot;: &quot;https://cdn.site.io/pic-4-64.jpg&quot;,\n    &quot;bigPic&quot;: &quot;https://cdn.site.io/pic-4-1024.jpg&quot;\n  }\n}\n\nLe tout donne un code trÃ¨s facile Ã  lire et Ã  raisonner. Le serveur expose un schÃ©ma des donnÃ©es disponibles, ce qui permet :\n\n\n  au client de construire sa requÃªte et de la valider\n  de gÃ©nÃ©rer du code cÃ´tÃ© client Ã  partir du schÃ©ma\n  une bonne intÃ©gration dans les IDE (autocompletion)\n  gÃ©nÃ©ration dâ€™une API Doc\n\n\nGraphQL ne sâ€™occupe pas du stockage, câ€™est uniquement la couche de requÃªtage qui peut Ãªtre implÃ©mentÃ©e avec votre code actuel.\n\nGraphQL est utilisÃ© depuis plus de 3 ans chez Facebook et sert Ã  lâ€™heure actuelle environ 260 milliards de requÃªtes par jour.\n\nLee Byron a annoncÃ© lors de sa confÃ©rence la diffusion dâ€™un â€œworking draftâ€ dâ€™une RFC GraphQL, ainsi quâ€™une implÃ©mentation de rÃ©fÃ©rence en Javascript.\n\n\n\nSuite Ã  cette prÃ©sentation de GraphQL, Joseph Savona introduit Relay, un framework proposÃ© par Facebook qui permet de gÃ©rer cÃ´tÃ© client le data-fetching via GraphQL dans les applications React.\nLe principe de Relay est que chaque composant dÃ©finit ses propres dÃ©pendances en utilisant le langage de requÃªte de GraphQL. Les donnÃ©es sont mises Ã  disposition dans le composant dans this.props par Relay.\n\nLe dÃ©veloppeur fait ses composants React naturellement, et Relay sâ€™occupe de composer les requÃªtes, permettant ainsi de fournir Ã  chaque composant les donnÃ©es prÃ©cises dont il a besoin (et pas plus), de mettre Ã  jour les composants quand les donnÃ©es changent et de maintenir un store cÃ´tÃ© client (cache) avec toutes les donnÃ©es.\n\n\n\nDonâ€™t Rewrite, React!\n\n\n\nRyan Florence @ryanflorence nous propose de profiter de la rÃ©Ã©criture de code dâ€™application historique pour introduire de nouvelles technologies et outils.\n\nLe problÃ¨me avec les rÃ©Ã©critures est que lâ€™on est gÃ©nÃ©ralement obligÃ© de le faire pour des morceaux assez important de lâ€™application (en partant du haut de lâ€™arbre fonctionnel de lâ€™application). Cela peut bloquer la correction de bug sur le code historique, empÃªcher de faire quelques Ã©volutions, obliger Ã  maintenir des branches â€œÃ  longue durÃ©e de vieâ€,â€¦\n\nAu lieu dâ€™utiliser cette approche, de haut en bas, Ryan nous propose dâ€™utiliser React en partant du bas de lâ€™arbre, câ€™est Ã  dire par une fonctionnalitÃ© unitaire trÃ¨s limitÃ©e.\n\nReact se prÃªte parfaitement Ã  ce type de travail puisque son design permet de lâ€™utiliser dans un contexte isolÃ© trÃ¨s facilement. Petit Ã  petit, on arrive Ã  remonter de plus en plus, en rÃ©Ã©crivant des fonctionnalitÃ©s de plus en plus importantes, jusquâ€™Ã  avoir rÃ©Ã©crit lâ€™application complÃ¨te.\n\n\n\nLive React: Hot Reloading with Time Travel\n\n\n\nDan Abramov @dan_abramov nous prÃ©sente son workflow React.\nIl est notamment le crÃ©ateur de React Hot Loader, et de Redux, lâ€™une des derniÃ¨res implÃ©mentations de Flux jouissant dÃ©jÃ  dâ€™une trÃ¨s grande popularitÃ©.\n\nLâ€™un des messages Ã  retenir de sa prÃ©sentation est lâ€™importance de travailler sur ses outils de dÃ©veloppement afin dâ€™avoir plus de temps Ã  passer sur ses applications.\n\nQuelques outils pour accÃ©lÃ©rer le workflow de dÃ©veloppement :\n\n\n  amok\n  figwheel\n  livereactload\n  React Hot Loader\n  webpack\n\n\nNous faisons ensuite un focus sur son workflow autour de ces principaux outils :\n\n\n  Redux\n  Redux Dev Tools\n  React Hot Loader\n  webpack\n\n\nReact Hot Loader permet de rafraÃ®chir son application instantanÃ©ment Ã  chaque modification de code, et ce, sans refresh de page, uniquement en rafraichissement les composants ayant changÃ© !\nCâ€™est trÃ¨s impressionnant en Live dÃ©mo !\n\nRajouter Ã  Ã§a le Redux Dev Tools qui permet de suivre en temps rÃ©el les actions lancÃ©es, ainsi que lâ€™Ã©tat des states, de pouvoir revenir en arriÃ¨re dans les actions â€œÃ  la gitâ€, mais aussi dâ€™avoir un error handler trÃ¨s quali en live (inspirÃ© jâ€™imagine de la gestion dâ€™erreur de React Native).\n\nLâ€™idÃ©e derriÃ¨re Redux (son implÃ©mentation du pattern Flux) est de faire un Store immuable. On peut rÃ©sumer une action Ã  une fonction prenant en entrÃ©e un Ã©tat du store et donnant en sortie un nouvel Ã©tat du Store (sans toucher au premier). En partant de ce principe, appliquer une sÃ©rie dâ€™actions revient simplement Ã  effectuer une rÃ©duction (un â€œreduceâ€). \nOn applique ici les principes dâ€™Event Sourcing.\nLâ€™immuabilitÃ© permet de stocker les diffÃ©rents Ã©tats intermÃ©diaires du store et donc de naviguer extrÃªmement facilement dans les diffÃ©rentes versions pendant le dÃ©veloppement.\n\nPlus dâ€™infos ici : The evolution of flux\n\n\n  Reducer + Flux = Redux\n\n\n\n\nBack to Text UI\n\nMikhail Davydov @azproduction a eu lâ€™idÃ©e folle de crÃ©er une interface texte pour le terminal avec les outils web : HTML, CSS, JS et donc React.\nCâ€™est complÃ©tement fou, assez impressionnant, mais on se demande quand mÃªme pourquoi ?\n\nVoir les slides\n\n\n\nLightning Talk\n\nPour finir la journÃ©e, nous avons eu le droit Ã  quelques Lightning Talk de qualitÃ© inÃ©gale, abordant lâ€™intÃ©gration de D3 avec React, de lâ€™outil Cosmos permettant de tester dans un browser ses composants React un par un, de React Native Playground , un bel outil pour tester facilement online dans un simulateur des applis ou exemple de code de React Native voir vidÃ©o du LT, et Turbine une sorte de remplacant de Relay en lâ€™attendant (voir cet article).\n\nConclusion\n\nExcellente organisation (et on ne dit pas ca seulement pour les biÃ¨res Ã  volontÃ©), un line-up du tonnerre et de belles annonces (React Native Android en AoÃ»t, GraphQL etc).\nCâ€™est dÃ©jÃ  avec plein dâ€™idÃ©es et de pistes dâ€™amÃ©liorations pour nos projets React que nous sortons de ce premier jour trÃ¨s complet.\n\nVous pouvez retrouvez le compte rendu de la deuxiÃ¨me journÃ©e ici\n"
} ,
  
  {
    "title"    : "On Ã©tait au PHPTour ! ",
    "category" : "",
    "tags"     : " conference, afup, phptour",
    "url"      : "/2015/06/04/m6web-au-phptour-luxembourg.html",
    "date"     : "June 4, 2015",
    "excerpt"  : "On Ã©tait au PHP Tour et câ€™Ã©tait bien !\n\n(y avait un gros gÃ¢teau et des biscuits en forme dâ€™elephpant)\n\nLe voyage fut un peu Ã©pique, surtout les quelques kilomÃ¨tres en plus quand le meilleur dâ€™entre nous a oubliÃ© son sac Ã  dos dans une station Ã  15...",
  "content"  : "On Ã©tait au PHP Tour et câ€™Ã©tait bien !\n\n(y avait un gros gÃ¢teau et des biscuits en forme dâ€™elephpant)\n\nLe voyage fut un peu Ã©pique, surtout les quelques kilomÃ¨tres en plus quand le meilleur dâ€™entre nous a oubliÃ© son sac Ã  dos dans une station Ã  150 km de lÃ  :)\n\n\n\nEt on nâ€™a pas pu battre la team Blablacar et Jolicode au concours de levÃ© de coudes - on est forfait les gars !\n\nPlutÃ´t quâ€™un retour exhaustif (et parce quâ€™avec les aqueducs de Mai on cherche un peu le temps), voici quelque chose de plus informel, sur notre ressenti des tendances communautaires (forcÃ©ment subjectif).\n\nRadio moquette !\n\nIl y a une bonne maturitÃ© autour des tests et du CI dans la communautÃ© PHP. On commence aussi Ã  voir de plus en plus des pratiques autour du partage de la responsabilitÃ© du provisionning entre ops et dev (avec Ansible et Vagrant notamment) mais, comme chez M6Web, câ€™est trÃ¨s balbutiant - et chacun a sa faÃ§on de faire. On voit des infras de dev qui passent dans le cloud (variabilisation des coÃ»ts, flexibilitÃ©, possibilitÃ© dâ€™expÃ©rimenter). Les services managÃ©s nâ€™ont pas la cote, on reste sur du IAAS, principalement chez AWS.\n\nDes solutions pour faire du PHP async se dessinent. Cela reste Ã  expÃ©rimenter (libevent, ReactPHP, le tradeoff vitesse, consommation CPU Ã©tant inconnu. Câ€™est Ã  creuser, car cela peut sortir Ã  moindre coÃ»t de quelques situations difficiles. Lâ€™intÃ©gration avec certaines librairies comme Guzzle est trÃ¨s intÃ©ressante.\n\nMySQL 5.7 est annoncÃ© par Oracle avec pleins de features + 2x plus rapide que 5.6 et 3x que 5.5 (query) et encore plus sur le connection time. Ils annoncent une meilleur intÃ©gration avec FusionIO et ils semblent pousser des solutions de cluster multi-master (via Fabric) alors que câ€™Ã©tait considÃ©rÃ© expÃ©rimental avant, câ€™est maintenant annoncÃ© stable.\n\nPHP7 va Ãªtre important pour le langage. Pour la performance (au moins x2 vitesse, x0.5 mÃ©moire), les nouvelles fonctionnalitÃ©s (classes anonymes, scalar type hints, stricts type hints, return type declaration, exceptions on fatals, â€¦). Presque pas de BC break, on devrait surement chez M6Web faire des tests avec la RC dÃ¨s que possible et migrer rapidement quelques services Ã  la sortie dâ€™une stable.\n\nAnother (php) brick in the wall\n\nM6Web Ã©tait reprÃ©sentÃ© par Olivier qui a fait une prÃ©sentation sur lâ€™architecture backend du second Ã©cran.\n\n\n\nNâ€™hÃ©sitez pas Ã  commenter la confÃ©rence.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #8",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2015/05/07/m6web-dev-facts-8.html",
    "date"     : "May 7, 2015",
    "excerpt"  : "Une fois nâ€™est pas coutume, cette nouvelle fournÃ©e des Dev Facts est publiÃ©e un jeudi. Mais câ€™est pour vous donner de quoi lire en ce grand week-end !\n\nEt comme Ã§a fait longtemps, voici une grosse fournÃ©e. Enjoy !\n\nIncident planifiÃ©\n\n\n  \n    X : n...",
  "content"  : "Une fois nâ€™est pas coutume, cette nouvelle fournÃ©e des Dev Facts est publiÃ©e un jeudi. Mais câ€™est pour vous donner de quoi lire en ce grand week-end !\n\nEt comme Ã§a fait longtemps, voici une grosse fournÃ©e. Enjoy !\n\nIncident planifiÃ©\n\n\n  \n    X : normal les erreurs depuis 14h ?\n    Y : câ€™est pas la maintenance ?\n    Z : oui, maintenance\n    Z : ils me prÃ©viennent au moins\n    Y : un incident planifiÃ© quoi\n  \n\n\nLa finesse.\n\n\n  Ca va bien rentrer, Ã  force quâ€™on leur en mette partout.\n\n\nUne bi-douille\n\n\n  Il vient de faire une bidouille pour bidouiller\n\n\nLe fullscreen fenÃªtrÃ©\n\n\n  Le fullscreen ne marche pas en plein Ã©cran !\n\n\nLa kalitay\n\n\n  Ils sont gardien de la qualitÃ© avec un K majuscule\n\n\nLa mÃ©thode argile\n\n\n  Nous on fait de lâ€™agile en V\n\n\nAutomagique\n\n\n  Câ€™est fait manuellement Ã  la main\n\n\nLa dure loi du travail\n\n\n  Jâ€™ai mÃªme pas rÃ©ussi Ã  refourger mon travail aux autres !\n\n\nReproduction\n\n\n  Je ne sais pas si Ã§a corrige le bug quâ€™on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nNos amis les belges\n\n\n  le script pour les belges, comme câ€™est un â€œone shotâ€, on peux dire quâ€™on va lâ€™exÃ©cuter, [accent belge]une fois[/accent belge]\n\n\nProprement sale\n\n\n  Câ€™est pas forcÃ©ment plus propre, mais câ€™est moins sale\n\n\nPas pareil, mais diffÃ©rent\n\n\n  \n    X : tu peux vÃ©rifier que câ€™est diffÃ©rent ?\n    Y : DiffÃ©rent comment ?\n    X : bah, pas pareil â€¦\n  \n\n\nÃ€ la louche !\n\n\n  Câ€™est Ã  peu prÃ¨s approximatifâ€¦\n\n\nOn a pas lâ€™habitude\n\n\n  \n    X : Pourquoi tu penses que Ã§a va prendre du temps ?\n    Y : Parce quâ€™il faut rÃ©flÃ©chir\n  \n\n\nIl faut savoir ce quâ€™on veut\n\n\n  Câ€™est pas prÃ©vu pour Ãªtre utile\n\n\nCâ€™est le Java bleu\n\n\n  Donc en fait tu regrettes Java car tu sais pas coder\n\n\nLe mythe â€¦ et la rÃ©alitÃ©\n\n\n  \n    X : Câ€™est bien connu, quand tu installes Linux, il y a 18 mannequins topless qui dÃ©filent dans le bureau\n    Y : Oui, mais des mannequins pour linuxiens, donc bof quoi\n  \n\n\nROI !\n\n\n  y a autant dâ€™utilisateurs que de jour homme pour ce projet !\n\n\nMais ca marche chez moi\n\n\n  \n    Y : navigation privÃ©e = pas de DRM = erreur 3365\n    Z : pas de DRM ? mais pourquoi ?\n    Z : mais sur les sites X ca marche, je comprends pas\n  \n\n\nPooh\n\nAu sujet dâ€™une sombre histoire dâ€™expression de besoin\n\n\n  Je ne peux pas toujours les aider Ã  faire leurs besoinsâ€¦\n\n"
} ,
  
  {
    "title"    : "Mix-IT 2015 - Jour 2",
    "category" : "",
    "tags"     : " mixit, conference, agile, technique",
    "url"      : "/2015/04/17/mixit-2015-jour-2.html",
    "date"     : "April 17, 2015",
    "excerpt"  : "Cet article est le retour du second jour du Mix-IT 2015, le vendredi 17 avril 2015.\nVous pouvez Ã©galement consulter le retour du jour 1.\n\nAller sur Mars â€¦ ou presque\nâ€” Florence Porcel\n\nFlorence est venue nous prÃ©senter son rÃªve : devenir une marso...",
  "content"  : "Cet article est le retour du second jour du Mix-IT 2015, le vendredi 17 avril 2015.\nVous pouvez Ã©galement consulter le retour du jour 1.\n\nAller sur Mars â€¦ ou presque\nâ€” Florence Porcel\n\nFlorence est venue nous prÃ©senter son rÃªve : devenir une marsonaute, Ã  savoir une personne qui va aller physiquement sur Mars.\n\nAprÃ¨s avoir pris le temps de faire le point sur tout ce qui a Ã©tÃ© fait pour permettre un jour Ã  lâ€™homme de faire le voyage pour mettre le pied sur Mars, Florence nous a expliquÃ© les diffÃ©rentes actions quâ€™elle avait entrepris pour agir sur ce cheminement :\n\n\n  Participation Ã  un projet de simulation de vie sur Mars : elle a fait partie dâ€™un groupe de personnes qui se sont isolÃ©es pendant 15 jours dans des conditions de vie semblables Ã  celles sur Mars (au milieu du dÃ©sert de lâ€™Utah, scaphandre pour sortir, nourriture lyophilisÃ©e, rationnement);\n  Participation Ã  un programme de volontaire pour le premier dÃ©part habitÃ© vers Mars;\n  â€¦\n\n\nLe but de tout Ã§a ? Suivre son rÃªve.\n\nFlorence a fini sa confÃ©rence en nous rappelant que ce nâ€™est pas notre Ã©ducation, notre formation ou notre passÃ© qui dicte ce que nous pouvons faire de notre vie, mais ce sont nos rÃªves.\nElle nous a rappelÃ© que, malgrÃ© sa formation littÃ©raire, et son mÃ©tier de comÃ©dienne, que son rÃªve dâ€™aller sur Mars un jour lâ€™a conduite Ã  faire ses actions trÃ¨s concrÃ¨tes, et que câ€™est grÃ¢ce Ã  lâ€™action un peu utopique de beaucoup de gens quâ€™on peux changer le monde.\n\nEn 2 mots, poursuivons nos rÃªves !\n\nReactJS pour les nÃ©ophytes\nâ€“ Nicolas Cuillery, Matthieu Lux et Florent Lepretre\n\nCet atelier Ã©tait un peu corporate puisque que Nicolas et Florent sont actuellement en mission chez M6Web.\n\nLors de cet atelier, qui se dÃ©composait en plusieurs TP permettant de dÃ©couvrir une Ã  une les diffÃ©rentes spÃ©cificitÃ©s de React et du pattern Flux : React et sa notion de composant, le pattern Flux et une de ses variantes, ReFlux, le systÃ¨me de routing et les tests avec Jest.\n\nBien quâ€™il leur ait manquÃ© du temps pour finir lâ€™atelier, ils Ã©taient trÃ¨s prÃ©sents pour nous aider Ã  franchir les premiers pas qui permettent dâ€™entrer dans ce nouveau monde.\n\nEn tout cas, fÃ©licitations Ã  tous les 3 pour leur implication pour ce difficile exercice quâ€™ils ont plutÃ´t bien surmontÃ©.\n\nStartups dâ€™Ã©tats\nâ€” Pierre Pezziardi\n\nPierre, qui dirige un incubateur dâ€™Ã©tat, soit un incubateur qui sÃ©lectionne et hÃ©berge des petites Ã©quipes dont le but est de faire Ã©voluer le systÃ¨me informatique public, mais Ã  la sauce dâ€™une startup : budget rÃ©duit, Ã©quipe rÃ©duite, mode â€œsurvieâ€.\n\nIl a eu lâ€™idÃ©e de ce systÃ¨me lorsquâ€™il sâ€™est posÃ© la question sur les outils qui â€œmarchentâ€ et leurs raisons. Pourquoi utilisons nous les outils de Google ou Dropbox ? Parce quâ€™ils sont simples et efficace ! Ils vont au but, et correspondent Ã  ce que lâ€™utilisateur dÃ©sire.\nEn poussant cette rÃ©flexion, il en est arrivÃ© Ã  la conclusion que tout systÃ¨me informatique est le reflet de lâ€™organisation qui le pilote : plus lâ€™organisation est tournÃ©e sur sa propre organisation, plus le produit final correspondra Ã  ce quâ€™un grand manager a demandÃ©, et pourra Ãªtre dÃ©calÃ© de ce que les utilisateurs attendent.\n\nÃ€ lâ€™inverse, quand une organisation nâ€™a pas de marge, elle va Ã  lâ€™essentiel pour que son produit convienne aux utilisateurs.\n\nVoici quelques exemples de projets qui sortent de cet incubateur dâ€™Ã©tat :\n\n\n  data.gouv.fr\n  MarchÃ©s Publics SimplifiÃ©s\n  mes-aides.gouv.fr\n\n\nCoding Dojo et Mob Programming dans les tranchÃ©es\nâ€” Bernard Notarianni\n\nCette confÃ©rence Ã©tait un retour dâ€™expÃ©rience expliquant comment une Ã©quipe de dÃ©veloppement habituÃ©e a travailler avec des releases fixes Ã  dates rÃ©guliÃ¨res a tentÃ© de mettre en place un dÃ©coupage en sprint pour amÃ©liorer sa productivitÃ©.\n\nJe dis volontairement â€œen sprintâ€ sans parler de Scrum parce quâ€™en fait, cette Ã©quipe sâ€™auto-organisait de la sorte, mais sur un cahier des charges fixe, un pÃ©rimÃ¨tre fixe pour chaque release, et pas de feedback avec le produit.\n\nAu final, la conclusion de Bernard a Ã©tÃ© sans appel : ils ont essayÃ©s, lâ€™Ã©quipe a fait son travail, a essayÃ© lâ€™amÃ©lioration continue, mais comme le client ne jouait pas le jeu, Ã§a câ€™est mal passÃ©.\n\nFabriquez votre devbox portable avec Docker\nâ€” Jean-Marc Meessen et Damien Duportal\n\nLors de cette confÃ©rence, Jean-Marc et Damien nous ont expliquÃ©s comment ils avaient rÃ©ussi Ã  utiliser Docker pour rÃ©aliser une â€œdevboxâ€ portable.\nAvant cette confÃ©rence, je pensais quâ€™ils allaient nous expliquer comment ils avaient organisÃ© leurs conteneurs pour que Ã§a soit le plus efficace, mais en fait, une â€œdevboxâ€ est plus un poste de dÃ©veloppement complet (bureau et IDE compris)\n\nLe rÃ©sultat est assez impressionnant dans le fait quâ€™ils ont rÃ©ussi Ã  virtualiser une Debian avec son UI via Docker, et quâ€™ils peuvent le faire tourner sur nâ€™importe quel poste.\n\nToutefois, je ne suis pas convaincu par cette approche. Ã€ mon sens, Docker permet Ã  tous de dÃ©velopper dans lâ€™environnement qui lui convient, tout en exÃ©cutant son code dans un environnement qui est le plus proche possible de la production.\n\nIl nâ€™en reste pas moins que je les fÃ©licite pour le rÃ©sultat quâ€™ils ont obtenu, et je suis encore plus convaincu de la puissance de Docker suite Ã  cette prÃ©sentation.\n\nReading code good\nâ€” Saron Yitbarek\n\nSaron est venue nous partager sa vision sur le moyen quâ€™elle trouve le plus efficace pour apprendre un langage, un framework, une librairie : lire du code. \nDe la mÃªme maniÃ¨re, pour progresser, lire le code des autres permet dâ€™aller au delÃ  de ce que nous pensons faire. En se confrontant au code des autres, nous apprenons sur les autres maniÃ¨res de rÃ©soudre un mÃªme problÃ¨me, sur dâ€™autres approches de code, et nous Ã©largissons notre connaissance.\n\nSuivre un tutorial, câ€™est bien, mais le soucis, câ€™est que le code est basique, sur un usage basique.\nLire un code rÃ©el, câ€™est voir un cas dâ€™utilisation rÃ©el, câ€™est voir comment le dÃ©veloppeur lâ€™a rÃ©solu, voir les techniques quâ€™il utilise, â€¦ et le moyen le plus sÃ»r de toujours dÃ©couvrir et apprendre.\n\nPour aller encore plus loin, il ne faut pas hÃ©siter Ã  discuter avec lâ€™auteur du code que lâ€™on vient de lire.\n\nLâ€™Ã©nergie de Saron et la conviction quâ€™elle met dans sa prÃ©sentation ont fait de cette confÃ©rence un vrai coup de coeur de ma part !\n\nCome to the dark side\nâ€” StÃ©phane Bortzmeyer - [PrÃ©sentation sur InfoQ]\n\nLors de cette Keynote, StÃ©phane nous a fait part de son ressenti quand Ã  lâ€™impact de lâ€™informatique dans notre vie.\n\nIl nâ€™y a encore que 20 ans, lâ€™informatique Ã©tait au service de lâ€™homme. Elle servait Ã  amÃ©liorer son quotidien Ã  faciliter son travail.\nAujourdâ€™hui, câ€™est lâ€™informatique qui dirige nos vies. Si vous Ãªtes anti Facebook, vous perdez contact avec pas mal de gens. Si vous ne voulez pas dâ€™ordinateur, il y a de plus en plus de dÃ©marches que vous ne pouvez faire.\n\nEncore plus important, nous avons dÃ©lÃ©guÃ©s de plus en plus de dÃ©cisions Ã  lâ€™informatique, sur des aspects qui impactent de plus en plus notre quotidien. Par exemple, lorsque vous faites un paiement, câ€™est un algorithme qui va dÃ©cider si la transaction est autorisÃ©e ou non, de maniÃ¨re froide et automatique, sans chercher Ã  comprendre si sa dÃ©cision peux vous laisser dans une situation compliquÃ©e.\n\nÃ€ partir de lÃ , nous, dÃ©veloppeurs, avons une grande responsabilitÃ©. Le code que nous produisons, les algorithmes que nous acceptons dâ€™implÃ©menter sont ceux qui se retrouvent dans les systÃ¨mes qui rÃ©gissent nos vies.\nIl est donc primordial que nous prenions conscience de cette responsabilitÃ© et que nous nous posions des questions sur ce que nous faisons, quitte Ã  refuser de le faire si cela va Ã  lâ€™encontre de notre Ã©thique.\n\nJâ€™ai Ã©tÃ© fortement touchÃ© par cette claque, enfin, cette confÃ©rence, car elle nous place devant nos responsabilitÃ©s, et devant notre devoir de prendre du recul sur ce que nous faisons pour ne pas Ãªtre un simple robot.\n\nConclusion\n\nComme Ã  leur habitude, les organisateurs de ce Mix-IT 2015 ont rÃ©alisÃ©s un superbe travail.\nUn grand bravo Ã  eux !\n\nRappel : cet article est dÃ©coupÃ© en 2 parties. Nâ€™oubliez pas de consulter le retour des confÃ©rences suivies lors du premier jour.\n"
} ,
  
  {
    "title"    : "Mix-IT 2015 - Jour 1",
    "category" : "",
    "tags"     : " mixit, conference, video",
    "url"      : "/2015/04/16/mixit-2015-jour-1.html",
    "date"     : "April 16, 2015",
    "excerpt"  : "Il est tout naturel que M6Web soit prÃ©sent Ã  une confÃ©rence qui mÃªle sujet technique dâ€™avant-garde et agilitÃ©, 2 sujets qui nous sont chers, surtout lorsquâ€™elle se dÃ©roule Ã  Lyon.\n\nJâ€™ai donc eu la chance de participer au Mix-IT 2015 qui se tenait ...",
  "content"  : "Il est tout naturel que M6Web soit prÃ©sent Ã  une confÃ©rence qui mÃªle sujet technique dâ€™avant-garde et agilitÃ©, 2 sujets qui nous sont chers, surtout lorsquâ€™elle se dÃ©roule Ã  Lyon.\n\nJâ€™ai donc eu la chance de participer au Mix-IT 2015 qui se tenait les 16 et 17 avril derniers au CPE Lyon.\n\nCet article est dÃ©coupÃ© en 2 parties. Dans lâ€™article que vous Ãªtes en train de lire, vous trouverez les retours des confÃ©rences que jâ€™ai suivies le premier jour, mais vous pourrez Ã©galement trouver le retour des confÃ©rences suivies lors du second jour.\n\nThe three ages of innovation\nâ€” Dan North - [Slides]\n\nDan nous a partagÃ© sa vision de lâ€™innovation Ã  travers lâ€™Ã©volution dâ€™une technologie.\n\nSelon lui, il existe donc 3 Ã¢ges dans lâ€™Ã©volution :\n\n\n  Explore (Maximize discovery)\n\n\nIl sâ€™agit de la phase initiale, celle de la dÃ©couverte, de lâ€™expÃ©rimentation.\nDans cette phase, on essaye, on se trompe, on apprend.\n\n\n  Stabilize (Minimize variance)\n\n\nIl sâ€™agit de la phase oÃ¹ on fait le tri sur tout ce quâ€™on a testÃ©, initiÃ©, et quâ€™on essaye de catÃ©goriser, stabiliser tout Ã§a, voir retirer ce qui nâ€™est pas une bonne idÃ©e.\n\nCâ€™est le moment oÃ¹ on est capable de reproduire ces crÃ©ations, et donc de les apprendre (repeatability, predictability, teachability).\n\nDans cette phase, nous apprenons Ã  rÃ©duire lâ€™incertitude autour de la maniÃ¨re de rÃ©aliser un code.\n\n\n  Commoditize (Maximize efficiency)\n\n\nCette phase est celle de lâ€™industrialisation, celle oÃ¹ on essaye de rÃ©duire les coÃ»ts pour augmenter lâ€™efficacitÃ©.\n\nEt mÃªme si ces 3 phases sont conflictuelles, lâ€™innovation existe dans chacune de ces 3 phases, et il faut savoir les respecter et sâ€™impliquer dans chacune.\n\nLe pourquoi du pourquoi de lâ€™agilitÃ©\nâ€” CÃ©dric Bodin - [Screencast de la mÃªme confÃ©rence, mais Ã  Nantes]\n\nAu cours de cette prÃ©sentation, CÃ©dric nous a poussÃ© Ã  rÃ©flÃ©chir sur la raison profonde du succÃ¨s de lâ€™agilitÃ©.\n\nNous le savons, lâ€™agilitÃ© est une solution efficace contre les plannings qui glissent, les cahiers des charges non tenables, le syndrome de la tour de cristal et autres dysfonctionnement organisationnels.\n\nDepuis 1994, le Chaos Manifesto publie un rapport indiquant le pourcentage de projets qui Ã©chouent ou rÃ©ussissent, et il y a 2 fois plus de projets qui Ã©chouent que de projets qui rÃ©ussissent. Il est courant que des projets qui sâ€™Ã©ternisent, dÃ©rapent, â€¦ soient finalement abandonnÃ©s.\n\nDe mÃªme, il est courant que le produit fini ne corresponde pas Ã  ce que lâ€™utilisateur attend, Ã  cause de la distance entre eux et les Ã©quipes qui dÃ©veloppent le produit.\n\nMais pourquoi est-ce que nous avons besoin de lâ€™agilitÃ© ? Pourquoi en sommes nous arrivÃ©s Ã  cette situation oÃ¹ tout ces syndromes apparaissent ?\n\nTout simplement parce que nous avons construit lâ€™Entreprise dâ€™aujourdâ€™hui sur la base du Taylorisme qui permet dâ€™Ãªtre le plus rentable possible en prÃ©voyant tout les cas pour rÃ©duire le hasard et donc lâ€™Ã©chec. Le socle de base de cette thÃ©orie est que rien ne change et que tout est prÃ©visible.\n\nOr, la sociÃ©tÃ© actuelle va plus vite, veut pouvoir Ãªtre trÃ¨s rÃ©active, et notre branche en particulier.\n\nÃ€ partir de lÃ , le modÃ¨le qui veux que certaines personnes pensent, prÃ©voient, organisent, pour que les techniciens nâ€™aient â€œqueâ€ Ã  exÃ©cuter est dÃ©passÃ©e. Et cela se voit ! Combien de â€œjâ€™ai fait ce que le cahier des charges demandaitâ€, combien de discussions stÃ©rile autour dâ€™un changement de pÃ©rimÃ¨tre pour recalculer des dÃ©lais ?\n\nVoici 2 exemples de phrases qui montrent le dÃ©calage entre la vision du Taylorisme dans lâ€™informatique et la rÃ©alitÃ© :\n\n\n  En informatique, la production, câ€™est la compilation. Et elle est tellement efficace quâ€™on ne la facture plus.\n\n\n\n  Nos mÃ©tiers de services ne sont pas des mÃ©tiers de production, mais des mÃ©tiers de conception.\n\n\nÃ€ partir de lÃ , il faut considÃ©rer le travail du dÃ©veloppeur comme du cÃ´tÃ© â€œpensantâ€ et pas du cÃ´tÃ© â€œexÃ©cutantâ€ : ne pas essayer de maximiser sa productivitÃ© en rÃ©duisant sa rÃ©flexion. Bien sâ€™assurer de sa comprÃ©hension du besoin au lieu de lui lister des tÃ¢ches Ã  accomplir sans rÃ©flexion.\n\nLes principes mis en avant par lâ€™agilitÃ© vont en ce sens : rapprocher lâ€™utilisateur et le dÃ©veloppeur, laisser lâ€™Ã©quipe dÃ©cider du planning, permettre lâ€™imprÃ©vu.\n\nToutefois, lâ€™agilitÃ© nâ€™est pas un dÃ©clencheur. Une sociÃ©tÃ© qui passe Ã  lâ€™agilitÃ© sans modifier son fonctionnement va Ã  lâ€™Ã©chec. Lâ€™agilitÃ© nâ€™est quâ€™un moyen de changer.\n\nNâ€™oublions pas la loi de Conway :\n\n\n  Tout logiciel reflÃ¨te lâ€™organisation qui lâ€™a crÃ©Ã©.\n\n\nSolution focus in team\nâ€” Vincent Daviet et GÃ©ry Derbier\n\nCet atelier destinÃ© plutÃ´t aux managers donnait des pistes et des exemples pour rÃ©ussir Ã  relancer de la synergie dans des Ã©quipes qui ont tendance Ã  se bloquer lors de la mise en place de lâ€™agilitÃ©.\n\nLe principal ressort de lâ€™agilitÃ© est la communication. Si cette communication est brisÃ©e, tout est en pÃ©ril, et câ€™est souvent la principale cause dâ€™Ã©chec de son adoption.\n\nNous avons rÃ©alisÃ© des mises en condition pour voir comment la communication peut Ãªtre un vÃ©ritable frein ou un formidable moteur pour avancer. En posant une mÃªme question de plusieurs maniÃ¨res, nous avons constatÃ© quâ€™il est tout aussi possible de bloquer un Ã©change qui Ã©tait intÃ©ressant, que dâ€™aller voir plus loin que les explications qui nous Ã©taient proposÃ©es.\n\nSi le TDD est mort, alors pratiquons une autopsie\nâ€” Thomas Pierrain et Bruno Boucard - [Slides]\n\nDerriÃ¨re ce titre un peu provocateur, Thomas et Bruno voulaient faire une analyse Ã  date du TDD, pour voir comment il est utilisÃ©, et pourquoi il a tendance Ã  Ãªtre dÃ©laissÃ©.\n\nComme il est un peu facile de dire que les dÃ©veloppeurs peuvent avoir du mal Ã  changer leurs habitudes, nous sommes allÃ©s voir un peu plus loin :\n\nLe dÃ©veloppeur qui est habituÃ© Ã  Ã©crire du code, voir sâ€™il marche, lâ€™Ã©diter, voir sâ€™il marche, â€¦ fonctionne selon un principe dâ€™expÃ©rimentation. Il ne sait pas trÃ¨s bien oÃ¹ il va, il essaye du code jusquâ€™Ã  ce quâ€™il fonctionne. Et une fois quâ€™il maitrise le code, il va continuer Ã  travailler de la mÃªme maniÃ¨re, mÃªme si le code fonctionne beaucoup plus rapidement quâ€™avant.\n\nMais cette maniÃ¨re de travailler reste fortement exposÃ©e Ã  2 limitations :\n\n  perte de vue de lâ€™objectif rÃ©el;\n  sur-architecture.\n\n\nAvec le TDD, le fonctionnement est de dÃ©crire ce quâ€™on veut faire dâ€™abord (se fixer un objectif) en lâ€™Ã©claircissant au maximum, au plus tÃ´t.\n\nPour Ãªtre efficace en TDD, il faut commencer par creuser son sujet et sâ€™assurer de comprendre ce quâ€™on veut, comment, avec quelles limites (les 5 pourquois). Ensuite, il faut le formuler, de prÃ©fÃ©rence Ã  haute voix pour bien sâ€™assurer de comprendre ce quâ€™on est en train de dire (mÃ©thode du canard en plastique), puis finalement lister une sÃ©rie de phrases en â€œmon code devraitâ€ indiquant le fonctionnement nominal et les cas limites.\n\nUne fois que tout Ã§a est respectÃ©, il est possible de dÃ©marrer le TDD proprement dit, Ã  savoir dâ€™Ã©crire les tests, de le voir Ã©chouer, puis de rÃ©aliser le code qui permet Ã  ces tests de fonctionner. Ainsi, il est possible de dÃ©gager son esprit du fonctionnel pour se concentrer sur le technique, jusquâ€™Ã  voir le code rÃ©ussir Ã  atteindre le but fixÃ©.\n\nRappel : cet article est dÃ©coupÃ© en 2 parties. Nâ€™oubliez pas de consulter le retour des confÃ©rences suivies lors du second jour.\n"
} ,
  
  {
    "title"    : "Introduction Ã  Immutable.Js, Relay + GraphQL et React Native",
    "category" : "",
    "tags"     : " javascript, react, reactnative, lft, video",
    "url"      : "/2015/04/01/immutablejs-relay-graphql-react-native.html",
    "date"     : "April 1, 2015",
    "excerpt"  : "Voici un petit compte rendu vidÃ©o, filmÃ© lors de notre Last Friday Talk de Mars, dâ€™un retour de veille techno suite Ã  la React ConfÃ©rence.\n\nLe retour est une introduction sur 3 des sujets qui mâ€™ont paru les plus importants lors de cette confÃ©rence...",
  "content"  : "Voici un petit compte rendu vidÃ©o, filmÃ© lors de notre Last Friday Talk de Mars, dâ€™un retour de veille techno suite Ã  la React ConfÃ©rence.\n\nLe retour est une introduction sur 3 des sujets qui mâ€™ont paru les plus importants lors de cette confÃ©rence :\n\n\n  Immutable.Js\n  Relay + GraphQL\n  React Native\n\n\nLes slides :\n\n\n\nPour plus dâ€™informations sur la React ConfÃ©rence, nos CR sont disponibles ici :\n\n\n  Compte rendu React ConfÃ©rence Jour 1\n  Compte rendu React ConfÃ©rence Jour 2\n\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Comment a-t-on bouchonnÃ© les dÃ©veloppeurs backend ?",
    "category" : "",
    "tags"     : " javascript, superagent, mock, isomorphic, cytron, open-source",
    "url"      : "/comment-a-t-on-bouchonne-les-developpeurs-backend",
    "date"     : "March 30, 2015",
    "excerpt"  : "Chez M6Web, nous travaillons actuellement sur la nouvelle version dâ€™un site web pour lequel sont dÃ©diÃ©es deux teams :\n\n\n  lâ€™Ã©quipe backend fournit lâ€™accÃ¨s aux donnÃ©es via des API sous Symfony2,\n  nous, lâ€™Ã©quipe frontend, dÃ©veloppons une applicatio...",
  "content"  : "Chez M6Web, nous travaillons actuellement sur la nouvelle version dâ€™un site web pour lequel sont dÃ©diÃ©es deux teams :\n\n\n  lâ€™Ã©quipe backend fournit lâ€™accÃ¨s aux donnÃ©es via des API sous Symfony2,\n  nous, lâ€™Ã©quipe frontend, dÃ©veloppons une application SPA isomorphe utilisant React.JS et le pattern Flux.\n\n\nDÃ©velopper le front avant les APIâ€¦\n\nNous avons dÃ©marrÃ© le projet au mÃªme moment que lâ€™Ã©quipe backend, donc sans avoir accÃ¨s aux API qui nous fournissent les donnÃ©es nÃ©cessaires au fonctionnement de lâ€™application. Nous nous sommes alors interrogÃ© sur la meilleure faÃ§on de dÃ©velopper notre front sans dÃ©pendre des API tout en impactant un minimum le code cible.\n\nLe contrat dâ€™interface\n\nLe choix technique pour notre SPA a Ã©tÃ© guidÃ© par une rÃ©flexion poussÃ©e sur les app isomorphique. Cette approche, React, Flux et tout lâ€™environnement qui tourne autour nous Ã©taient alors totalement inconnu. Nous avons eu une phase importante en amont pour poser les bases de lâ€™architecture du site, dÃ©montrer la faisabilitÃ© du projet et documenter lâ€™ensemble.\n\nCe petit dÃ©lai a permis Ã  lâ€™Ã©quipe backend dâ€™Ã©tablir des contrats dâ€™interface pour les principales routes de lâ€™API. Ã€ partir de ces informations, plus ou moins prÃ©cises, nous avons Ã©tabli des fichiers de fixtures. Lâ€™idÃ©e Ã©tait donc de retourner les donnÃ©es bouchonnÃ©es pour chaque appel Ã  une route dâ€™API non existante.\n\nSuperagent et superagent-mock\n\nPour rÃ©aliser les requÃªtes aux API, nous utilisons la librairie superagent, un client HTTP javascript facilement extensible. Il est isomorphe, câ€™est-Ã -dire quâ€™il fonctionne aussi bien sur un serveur node.js via npm que cÃ´tÃ© browser dans une application packagÃ©e via un bundler (webpack, browserify).\n\nNous avons dÃ©veloppÃ© superagent-mock, un plugin pour superagent, dont le rÃ´le est de simuler les appels HTTP lancÃ©s par superagent en retournant des donnÃ©es de fixtures en fonction de lâ€™URL appelÃ©e.\n\nEn pratique\n\nComme superagent, superagent-mock sâ€™installe via npm et peut Ãªtre utilisÃ© sur des applications serveurs ou clientes (via un bundler). Tout dâ€™abord, il faut rajouter la dÃ©pendance Ã  la librairie dans son  package.json.\n\nnpm install superagent-mock --save-dev\n\nIl faut ensuite crÃ©er le fichier de configuration. Câ€™est ici que vous allez dÃ©cider des routes Ã  bouchonner. Prenons lâ€™exemple dâ€™une route qui nâ€™existe pas et qui devra nous retourner la liste des auteurs du blog technique de M6Web : https://tech.bedrockstreaming.com/api/authors.\n\nVoici la structure du fichier de configuration Ã  mettre en place :\n\n// ./config.js file\nmodule.exports = [\n  {\n    pattern: &#39;https://tech.bedrockstreaming.com/api/authors&#39;,\n    fixtures: &#39;./authors.js&#39;,\n    callback: function (match, data) {\n      return { body : data };\n    }\n];\n\n\n  Lâ€™attribut pattern peut Ãªtre une expression rÃ©guliÃ¨re, dans le cas dâ€™une route qui contiendrait des paramÃ¨tres variables (ex : https://tech.bedrockstreaming.com/api/authors/(\\\\d+)).\n  Lâ€™attribut fixtures reprÃ©sente le lien vers le fichier de fixtures ou une callback.\n  Lâ€™attribut callback est une fonction Ã  deux arguments. match est le rÃ©sultat de la rÃ©solution de lâ€™expression rÃ©guliÃ¨re et data correspond aux donnÃ©es retournÃ©es par les fixtures. match permet dâ€™utiliser certains paramÃ¨tres de lâ€™appel (ex : lâ€™id de lâ€™auteur) pour retourner des donnÃ©es ciblÃ©es (ex : lâ€™auteur dans le fichier de fixtures correspondant Ã  cette id).\n\n\nEnsuite, il faut crÃ©er le fichier de fixtures. Câ€™est un fichier JS qui exporte une fonction retournant les donnÃ©es bouchonnÃ©es.\n\n// ./authors.js file\nmodule.exports = function () {\n  return [\n    {\n      id: 1,\n      name: &quot;John Doe&quot;,\n      description: &quot;unidentified person&quot;\n    },\n    ...\n  ];\n};\n\nPour finir, au dÃ©but du fichier JS appelÃ© par node, il suffit de patcher superagent avec le plugin superagent-mock de cette maniÃ¨re :\n\n// ./server.js file\nvar request = require(&#39;superagent&#39;);\nvar config = require(&#39;./config.js&#39;);\nrequire(&#39;superagent-mock&#39;)(request, config);\n\nCes quelques lignes permettent de surcharger certaines mÃ©thodes de superagent pour lui appliquer la configuration et simuler les requÃªtes bouchonnÃ©es. Pour comprendre plus en dÃ©tail le fonctionnement, câ€™est par ici.\n\nEt aprÃ¨s ?\n\nAvec cette astuce, vous pouvez dÃ©velopper votre front sans quâ€™aucune API en face ne soit accessible. Câ€™est trÃ¨s pratique pour travailler en local, sans accÃ¨s au net, ou pour rendre les tests fonctionnels de son application complÃ¨tement indÃ©pendants dâ€™un service tiers externe.\n\nLa partie dÃ©licate de cette approche intervient lorsque lâ€™on cÃ¢ble son application avec la vraie APIâ€¦ et que lâ€™on sâ€™aperÃ§oit que le contrat dâ€™interface nâ€™a pas Ã©tÃ© respectÃ© ! Nous avons souvent des corrections Ã  rÃ©aliser dans notre code lors de cette Ã©tape, mais les changements sont gÃ©nÃ©ralement mineurs et le gain de temps apportÃ© par lâ€™utilisation du bouchon en amont nâ€™est pas remis en cause. La partie fastidieuse reste de maintenir ses fichiers de fixtures avec lâ€™Ã©volution de lâ€™API, particuliÃ¨rement nÃ©cessaire si on sâ€™en sert dans ses tests fonctionnels.\n\nToujours plus\n\nNotre application forge elle-mÃªme lâ€™URL des images rÃ©cupÃ©rÃ©es via lâ€™API : elle nous fournit un id et nous reconstituons lâ€™URL finale grÃ¢ce Ã  un paramÃ¨tre de configuration. Ce nâ€™est pas REST compliant mais nous avons de bonnes raisons de le faire. Cette gÃ©nÃ©ration dâ€™URL utilise la librairie sprintf-js. Pour avoir une application complÃ¨tement indÃ©pendante de toute requÃªte externe, nous avons dÃ» Ã©galement bouchonner ces appels sur des images locales. Dans cette optique, nous avons dÃ©veloppÃ© sprintf-mock dont le mode de fonctionnement est Ã©trangement similaire Ã  celui de superagent-mock.\n\nLes projets superagent-mock et sprintf-mock sont open source. TrÃ¨s simple dâ€™utilisation, ils nous permettent de parallÃ©liser nos dÃ©veloppements avec lâ€™Ã©quipe backend et de rendre autonomes nos tests fonctionnels. Alors nâ€™attendez plus la finalisation de vos API pour commencer vos dÃ©veloppements front !\n\n"
} ,
  
  {
    "title"    : "How did we mock the backend developers?",
    "category" : "",
    "tags"     : " javascript, superagent, mock, isomorphic, cytron, open-source",
    "url"      : "/how-did-we-mock-the-backend-developers",
    "date"     : "March 30, 2015",
    "excerpt"  : "At M6Web we are currently working on a new version of a web site, with two separate teams:\n\n  the backend team providing data access through APIs;\n  us, the frontend team, building an isomorphic SPA application using React.JS and the flux pattern....",
  "content"  : "At M6Web we are currently working on a new version of a web site, with two separate teams:\n\n  the backend team providing data access through APIs;\n  us, the frontend team, building an isomorphic SPA application using React.JS and the flux pattern.\n\n\nDevelop the frontend before the APIs\n\nBoth teams started the project at the same time, meaning that at the beginning, we didnâ€™t have the web services needed for our application. We looked for the best way to develop it without waiting for those web services to become available.\n\nInterface\n\nOur technical choices for the SPA has been guided by a deep thinking about isomorphic applications. This approach, with React, Flux and their surrounding environment, was at the time, totally unknown. Our first important task was to build the foundations of the web site architecture, demonstrate the feasibility of the project and document everything.\n\nThis resulting delay allowed the backend team to specify the output of the API. Based on those informations, we wrote fixtures. The idea was to have data from a nonexistent web service.\n\nSuperagent and superagent-mock\n\nTo request the API we use the superagent library, an easily-extensible Javascript HTTP client. It is isomorphic, so it can be used both on server and client sides.\n\nThen we developed superagent-mock, a superagent plugin dedicated to simulate HTTP requests returning fixtures data.\n\nApplication\n\nLike superagent, superagent-mock can be installed via npm, and be used by server or client side libraries. First, you need to add the library in your package.json.\n\nnpm install superagent-mock --save-dev\n\nThen, create the configuration file, where you will define which data will be mocked. Letâ€™s take for example a nonexistent API, the authors list on our technical blog: https://tech.bedrockstreaming.com/api/authors.\n\nHere is the file structure we need:\n\n// ./config.js file\nmodule.exports = [\n  {\n    pattern: &#39;https://tech.bedrockstreaming.com/api/authors&#39;,\n    fixtures: &#39;./authors.js&#39;,\n    callback: function (match, data) {\n      return { body : data };\n    }\n];\n\n\n  The pattern attribute should be a regular expression, in case of a route containing variable parameters (ie: https://tech.bedrockstreaming.com/api/authors/(\\\\d+)).\n  The fixtures attribute represents the link to a file or a callback.\n  The callback attribute is a function with two arguments: match is the result of the regular expression and data the fixtures. match allows to use some call parameters (ie: the author id) to return relevant data (ie: the author in the fixture).\n\n\nNext, you have to create the fixture file. This is a JS file exposing a function returning the mocked data.\n\n// ./authors.js file\nmodule.exports = function () {\n  return [\n    {\n      id: 1,\n      name: &quot;John Doe&quot;,\n      description: &quot;unidentified person&quot;\n    },\n    ...\n  ];\n};\n\nFinally, at the top of the file called by node, you have to patch superagent with superagent-mock this way:\n\n// ./server.js file\nvar request = require(&#39;superagent&#39;);\nvar config = require(&#39;./config.js&#39;);\nrequire(&#39;superagent-mock&#39;)(request, config);\n\nThose few lines allow us to overload some superagent methods to apply the configuration of the mocked requests (check the source code).\n\nWhatâ€™s next\n\nWith this tip, you can develop the frontend without access to any API. Itâ€™s very useful in order to work locally on your computer, without the internet, or to make your functional tests independent of any third party.\n\nHowever it gets tricky when you connect your application with the real APIâ€¦ and you realize that the interface was not respected. We often have to fix our code at this stage, but the changes are usually minor and time saved by the mock isnâ€™t questioned. The tedious part is still to maintain fixtures with the API evolution, especially necessary if itâ€™s used with functional tests.\n\nEven more!\n\nOur app build itself the URLs of images retrieved via the API: it provides us an id and we guess the final URL through a configuration setting. This isnâ€™t REST compliant but we have good reasons to do this. The URL generation uses the library sprintf-js. To have a completely independent application of any external request, we also had to mock these calls to local images. With this in mind, we have developed sprintf-mock whose operating mode is curiously similar to that of superagent-mock.\n\nProjects superagent-mock and sprintf-mock are open source. Very easy to use, they allow us to parallelize our developments with the backend team and to make our functional tests autonomous. So donâ€™t wait API completion to start your frontend developments!\n\n"
} ,
  
  {
    "title"    : "CR React ConfÃ©rence 2015 - Day 2",
    "category" : "",
    "tags"     : " javascript, react, flux, isomorphic, conference",
    "url"      : "/2015/02/10/cr-react-conf-2015-day-two.html",
    "date"     : "February 10, 2015",
    "excerpt"  : "De retour Ã  Menlo Park pour cette deuxiÃ¨me journÃ©e de la React confÃ©rence.\n\nKeynote React Native\n\nChristopher Chedeau, @vjeux, revient sur les origines de React Native et les raisons pour lesquelles ils ont dÃ©cidÃ© de le crÃ©er.\n\nLes 3 piliers dâ€™une...",
  "content"  : "De retour Ã  Menlo Park pour cette deuxiÃ¨me journÃ©e de la React confÃ©rence.\n\nKeynote React Native\n\nChristopher Chedeau, @vjeux, revient sur les origines de React Native et les raisons pour lesquelles ils ont dÃ©cidÃ© de le crÃ©er.\n\nLes 3 piliers dâ€™une appli natives quâ€™ils ont dÃ»s traiter pour React Native sont :\n\n\n  Touch Handling : la vraie diffÃ©rence entre appli native et web\n  Native Components : tout le monde essaye de sâ€™en rapprocher mais personne nâ€™y arrive, et il y a dÃ©jÃ  beaucoup de trÃ¨s bons composants natifs\n  Style &amp;amp; Layout : le layout impacte Ã©normÃ©ment la faÃ§on dont on code, que lâ€™on soit sur le Web, iOS ou Android\n\n\nNous voyons que chaque composant natif a Ã©tÃ© recrÃ©Ã© comme un composant React : &amp;lt;View&amp;gt;, &amp;lt;Text&amp;gt; â€¦, et Christopher explique comment un composant React est transformÃ© en composant natif iOS.\n\nLa transformation du JS en natif se fait via JSCore (le moteur JS dans iOS).\n\nUne dÃ©monstration nous prouve quâ€™on peut utiliser la console Dev Tools de Chrome, pour dÃ©bugger lâ€™application et voir tout le code Â« DOM Â» React, comme si nous faisions du web classique.\n\nLa derniÃ¨re partie explique lâ€™approche des Ã©quipes de Facebook sur la maniÃ¨re de faire du CSS. Christopher avait dÃ©jÃ  fait hurler pas mal de personnes lors de sa confÃ©rence â€œfaire du CSS en JSâ€ (React CSS in JS). Il dÃ©clare le style en javascript dans une variable styles, et utilise lâ€™attribut style : &amp;lt;Text style={styles.movieYear}&amp;gt; qui inlinera le CSS.\n\nCâ€™est assez dÃ©stabilisant mais aussi ultra prometteur. Cela permet de rÃ©soudre quasiment tous les dÃ©fauts de CSS (Global Namespace, Dependencies, Dead Code Elimination, Minification, Isolation â€¦)\n\nNous parcourons ensuite les maniÃ¨res de gÃ©rer du layout nativement dans iOS, que Christophe dÃ©crit comme Â« ultra-compliquÃ© Â» ! Alors que cotÃ© web, nous avons le Box Model et Flexbox qui rÃ©solvent tous ces problÃ¨mes assez facilement.\n\nLes Ã©quipes de Facebook ont donc dÃ©cidÃ© de re-coder Flexbox et le Box Model en JS avec une approche TDD, de maniÃ¨re Ã  pouvoir utiliser la plupart des bases de Flexbox dans React Native pour faire du layout facilement sur iOS !\n\nVous pouvez retrouver le rÃ©sultat Â« Css-Layout Â» sur le Github de Facebook.\n\nLa dÃ©monstration continue sur un Â« live coding Â» montrant le Â« live reload Â» entre la modification du JS et le rafraÃ®chissement instantanÃ© du Simulator iOS.\n\nNous apprenons aussi que les modules ES6 ou Node comme Underscore, ou le SDK de Parse par exemple, fonctionneront sans problÃ¨me du moment quâ€™ils nâ€™ont pas de dÃ©pendance dans le browser ! \nCâ€™est encore une fois trÃ¨s prometteur, et si React Native vous intÃ©resse, la vidÃ©o ci-dessous est une excellente introduction.\n\nJâ€™ai, de mon cotÃ©, pu jouer quelques heures avec et câ€™est effectivement trÃ¨s sympa, intuitif et trÃ¨s rapide.\nLa version que nous avons ne contient pas encore tout ce que lâ€™on voit dans la vidÃ©o (je nâ€™ai par exemple pas trouvÃ© le Live Reload ou le Remote Debugging pour lâ€™instant), mais cela ne saurait tarder, les Ã©quipes de Facebook travaillant dâ€™arrache-pied sur le projet.\n\n\n\nThe complementarity of React and Web Components\n\nAndrew Rota, @AndrewRota, est de Boston, travaille pour Wayfair.com et explique comment utiliser des Web Components avec React, ou du React dans des Web Components.\nIl nous montre un exemple dâ€™un player html5 vidÃ©o avec un shadow dom qui contient tous les contrÃ´les du player (de simple input HTML).\n\nLa communautÃ© WebComponent a dÃ©jÃ  partagÃ© pas mal de WebComponents :\n\n\n  les x-\n  les core-\n  google-\n  paper-\n  et dâ€™autres â€¦\n\n\nPour conclure, Andrew partage les bonnes pratique pour crÃ©er un WebComponent :\n\npetit\ntrÃ¨s encapsulÃ©\naussi stateless que possible\nperformant\n\nPour en savoir plus sur les Web Components : Polymer-Project\n\n\n\nImmutable Data and React\n\nLee Byron, @leeb, enchaÃ®ne sur lâ€™immuabilitÃ© !\nConcept passionnant que nous avons entendu dans presque lâ€™intÃ©gralitÃ© des confÃ©rences.\n\n\n  Un objet immuable, en programmation orientÃ©e objet et fonctionnelle, est un objet dont lâ€™Ã©tat ne peut pas Ãªtre modifiÃ© aprÃ¨s sa crÃ©ation. Ce concept est Ã  contraster avec celui dâ€™objet variable. Source : WikipÃ©dia\n\n\nLee Byron est donc le crÃ©ateur de la librairie Immutable-JS permettant de gÃ©rer facilement des collections immuable en JS.\n\n\n  Immutable data cannot be changed once created, leading to much simpler application development, no defensive copying, and enabling advanced memoization and change detection techniques with simple logic. Persistent data presents a mutative API which does not update the data in-place, but instead always yields new updated data. Source : immutable-js\n\n\n\n  React is the V in MVC. We donâ€™t need an M. We already have arrays and objects.\n\n\nDâ€™ailleurs, on parle aussi dâ€™objets immuables cotÃ© Angular 2 : Change Detection in Angular 2\n\n\n\nBeyond the DOM: How Netflix plans to enhance your television experience\n\nLâ€™une des confÃ©rences que jâ€™attendais le plus, par Jafar Husain, @jhusain, Technical Lead chez Netflix.\nPour plusieurs raisons, dÃ©jÃ  parce que Netflix â€¦ qui en a profitÃ© pour annoncer la veille que Â« Netflix aimait React Â» mais aussi parce que Jafar est connu pour pas mal de choses (diffÃ©rents blog posts ou prÃ©sentations), ainsi quâ€™un cours interactif sur la programmation fonctionnelle en Javascript sur lequel jâ€™ai passÃ© pas mal de temps.\n\nIl nous a donc expliquÃ© les plans de Netflix pour amÃ©liorer lâ€™expÃ©rience TÃ©lÃ© sur leurs services, et comment React les a grandement aidÃ©s Ã  le faire.\n\nPour connaÃ®tre les raisons pour lesquels ils ont choisi React, je vous invite Ã  lire lâ€™article Netflix like React (Startup Speed et Server Side Rendering \\o/, Runtime Performance, Modularity).\n\nAujourdâ€™hui, Netflix dÃ©veloppe majoritairement en Javascript et ont 3 UI en JS, une pour le mobile, une pour le web et une pour les tÃ©lÃ©s.\nIls ont vu assez vite que le DOM Ã©tait trÃ¨s loin, câ€™est pourquoi ils ont crÃ©Ã© et introduit Gibbon (une sorte de Webkit maison plus rapide et adaptÃ© Ã  leur besoin sur les tÃ©lÃ©viseurs).\nIls ont donc fait Ã©voluer React (un fork au dÃ©part) pour permettre de sortir vers quelque chose dâ€™autres que du DOM afin de correspondre Ã  leur moteur Gibbon et vont donc continuer en 2015 le dÃ©ploiement de leur nouvelle UI avec React sur tous les services y compris tÃ©lÃ©s.\n\nVous pouvez retrouver le Netflix Open Source Software Center pour dÃ©couvrir le grand nombre dâ€™outils Open source de qualitÃ© quâ€™ils dÃ©livrent.\n\n\n\nScalable Data Visualization\n\nZach Nation travaille chez Dato (anciennement GraphLab) et doit traiter de trÃ¨s grandes quantitÃ©s de donnÃ©es dans ses applications.\n\nIl dÃ©montre lâ€™intÃ©rÃªt de React couplÃ©e Ã  d3.js (une librairie de visualisation exceptionnelle) pour reprÃ©senter Ã  lâ€™Ã©cran des transactions Bitcoin (en parsant un fichier de 21G ! en live).\n\n\n\nRefracting React\n\nTalk par David Nolen, @swannodette, personne trÃ¨s influente dans la communautÃ© JS (son blog). CrÃ©ateur de Om, ClojureScript, il nous explique que React doit Ãªtre vue comme une plateforme (plutÃ´t que librairie ou framework). On apprend aussi les concepts Ã  lâ€™origine dâ€™Om.\n\n\n\nFlux Panel\n\nBill Fisher (Facebook) a rassemblÃ© une partie des utilisateurs (voir des contributeurs) Ã  React pour confronter les diffÃ©rentes approches sur lâ€™utilisation de Flux, ainsi que sur la maniÃ¨re de gÃ©rer de lâ€™isomorphisme.\n\nOn parle notamment, via Michael Ridgway, @theridgway, de Fluxible, la librairie open source proposÃ©e par Yahoo (que nous utilisons), qui a annoncÃ© le jour mÃªme la crÃ©ation de sa documentation en ligne isomorphique, elle-mÃªme open source et utilisant Fluxible.\n\nSpike Brehm est aussi intervenu pour AirBnb, qui fut la premiÃ¨re sociÃ©tÃ©, je pense, Ã  parler dâ€™isomorphisme.\n\nAndres Suarez a aussi prÃ©sentÃ© la maniÃ¨re de gÃ©rer lâ€™isomorphisme chez Soundcloud. Vous pouvez avoir beaucoup plus dâ€™infos de sa part dans cette excellente vidÃ©o.\n\nSi vous Ãªtes intÃ©ressÃ©s par les diffÃ©rentes approches de Flux, vous pouvez comparez ici les implÃ©mentations : Flux Comparison\n\nQuelques questions sont aussi posÃ©es Ã  Jing Chen sur Relay.\n\nLâ€™approche Ã©tait intÃ©ressante mais le rÃ©sultat un peu dÃ©cevant car les sujets et implÃ©mentations ne sont que trop peu effleurÃ©s.\n\n\n\nCodecademyâ€™s approach to component communication\n\nBonnie Eisenman, @brindelle, travaille pour CodeAcademy.\nCodeAcademy est un service gratuit du secteur Ã©ducatif permettant dâ€™apprendre Ã  coder dans certains langages.\n\nBonnie a partagÃ© la maniÃ¨re dont son Ã©quipe a apprÃ©hendÃ© React, et notamment la communication entre composant. Ces rÃ©flexions ont eu lieu avant quâ€™ils nâ€™aient connaissance du pattern Flux.\n\nLes slides\n\n\n\nStatic typing with Flow and TypeScript\n\nJames Brantly, de chez AssureSign, commence avec une citation vu le matin (merci le JetLag) sur twitter :\n\nOn the 1st day God created the web. On the 2nd day God wrote jQuery. Then God blacked out, 3 days later awoke &amp;amp; invented React. #reactjsconf&amp;mdash; Matt Huebert (@mhuebert) 29 Janvier 2015\n\n\nIl prÃ©sente ensuite TypeScript et Flow, deux outils pour amÃ©liorer et sÃ©curiser la production de code.\n\nEn partant dâ€™une application React dâ€™exemple, il nous montre comment on intÃ©gre TypeScript et comment il lâ€™a â€œhackÃ©â€ pour quâ€™il reconnaisse le JSX, puis comment il ajoute Flow.\n\nAu final il recommande dâ€™utiliser plutÃ´t Flow, mÃªme si TypeScript est un peu plus mature, et fonctionne lui sous Windows.\n\n\n\nQA with the team\n\nLes deux journÃ©es se sont finies sur une session de Questions/RÃ©ponses avec les Ã©quipes de React chez Facebook : Tom Occhino, Ben Alpert, Lee Byron, Christopher Chedeau, Sebastian MarkbÃ¥ge, Jing Chen, et Dan Schafer.\n\n\n\nConclusion\n\nPour une premiÃ¨re confÃ©rence officielle sur React, ce fut une excellente surprise ! On retiendra clairement lâ€™annonce et les dÃ©mos de React Native, lâ€™emballement gÃ©nÃ©ral autour de React, ainsi que lâ€™approbation global de beaucoup de gros acteurs du web (Yahoo, Mozilla, Netflix, Uber, â€¦), le succÃ¨s du pattern Flux (malgrÃ© le manque de clartÃ© sur la maniÃ¨re de faire du Data Fetching), les promesses de Relay, les sujets rÃ©currents autour de lâ€™immuabilitÃ© â€¦\n\nBref, une vraie belle rÃ©ussite. Chapeau aux organisateurs (merci @vjeux ;-) ) et Ã©quipes de Facebook, ainsi quâ€™aux speakers pour cette superbe confÃ©rence.\n\nIl se passe rÃ©ellement quelque chose de grand dans la communautÃ© Front-End grÃ¢ce Ã  React. Il suffit de voir la vitesse Ã  laquelle les tickets dâ€™entrÃ©e sont partis (mÃªme chose au React Meetup parisien de DÃ©cembre 2014), de voir que tous les frameworks MVC tentent de sâ€™en inspirer (prÃ©-render, SSR, Virtual-Dom â€¦).\n\nPour finir, je voulais aussi partager le travail dâ€™une des personnes prÃ©sentes lors de cette confÃ©rence, ayant une faÃ§on trÃ¨s particuliÃ¨re de prendre des notes sur chacun des talks : https://chantastic.io/2015-reactjs-conf/\n\np.s: Retrouvez les retours sur la premiÃ¨re journÃ©e de la React confÃ©rence 2015.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "CR React ConfÃ©rence 2015 - Day 1",
    "category" : "",
    "tags"     : " javascript, react, flux, isomorphic, conference",
    "url"      : "/2015/02/04/cr-react-conf-2015-day-one.html",
    "date"     : "February 4, 2015",
    "excerpt"  : "Les 28 et 29 janvier 2015, sur le campus de Facebook (Ã  Menlo Park), avait lieu la premiÃ¨re confÃ©rence officielle sur la librairie open-source React (crÃ©Ã©e par les Ã©quipes de Facebook).\n\n2 jours de confÃ©rences riches en talks et en annonces dont v...",
  "content"  : "Les 28 et 29 janvier 2015, sur le campus de Facebook (Ã  Menlo Park), avait lieu la premiÃ¨re confÃ©rence officielle sur la librairie open-source React (crÃ©Ã©e par les Ã©quipes de Facebook).\n\n2 jours de confÃ©rences riches en talks et en annonces dont voici un petit compte-rendu, pour ceux nâ€™ayant pas eu la chance de pouvoir y assister ou de suivre les diffÃ©rents LT sur Twitter, en commenÃ§ant par la premiÃ¨re journÃ©e.\n\nLâ€™ouverture de la confÃ©rence par Tom Occhino\n\nTom Occhino, @tomocchino, a permis de rÃ©tablir la vÃ©ritÃ© sur lâ€™origine de React.\nCe sont les Ã©quipes de Facebook Ads qui sont Ã  la genÃ¨se de ce projet.\n\nA lâ€™Ã©poque, sur des applis MVC cÃ´tÃ© client, plus les applications et le nombre de dÃ©veloppeurs grandissaient, plus elles Ã©taient difficiles Ã  maintenir et devenaient lentes !\nLe Â« Two Way Data Binding Â» rendait les mises Ã  jour en cascade trop compliquÃ©es (tout lâ€™Ã©cran devait Ãªtre rafraÃ®chi) et le code devenait vraiment non prÃ©visible. Mais malgrÃ© tout, cela â€œmarchaitâ€ ! Lâ€™appli de Chat Facebook fonctionnait aussi de la mÃªme maniÃ¨re.\n\nCâ€™est ainsi que React a Ã©tÃ© crÃ©Ã© pour amÃ©liorer le rendu, lâ€™organisation et les performances de ces applications.\n\nInstagram a ensuite rejoint Facebook et les Ã©quipes ont voulu utiliser React pour refondre le site, mais React Ã©tait Ã  lâ€™Ã©poque trop couplÃ© Ã  Facebook. \nPete Hunt a donc re-factorÃ© lâ€™ensemble pour crÃ©er le React â€œopen-sourceâ€ que lâ€™on connaÃ®t aujourdâ€™hui.\n\nAprÃ¨s cette introduction sur React, Tom a expliquÃ© que lâ€™un des problÃ¨mes de React est quâ€™il nâ€™est utilisÃ© que pour le Web.\nAujourdâ€™hui, tout le monde tente de crÃ©er des composants web ressemblant aux composants natifs, mais Ã  chaque fois, le rÃ©sultat est mauvais, lâ€™environnement natif Ã©tant plus performant que celui dâ€™un browser. Un exemple donnÃ© dâ€™application native plutÃ´t sexy est celui de â€œFacebook Paperâ€.\n\nIl a ensuite rÃ©vÃ©lÃ© lâ€™annonce certainement la plus importante de ces 2 jours, lâ€™arrivÃ©e prochaine sur Github de Â« React Native Â», permettant de dÃ©velopper en Js via React des composants entiÃ¨rement natifs, avec comme exemple, lâ€™application Facebook Groups prÃ©sente sur lâ€™App Store iOs ! \nLa confÃ©rence est lancÃ©e !\n\nAvec React Native, ils ne veulent pas faire du â€œwrite once, run anywhere Â», mais du â€œlearn once, write anywhere Â» de maniÃ¨re Ã  optimiser les composants et usages suivant les devices.\n\nLe code sera fourni sur un dÃ©pot privÃ© Ã  tous les participants de la confÃ©rence lors de la Keynote de clÃ´ture !\n\nPour finir sa keynote dâ€™entrÃ©e, il a voulu lister les frameworks JS qui ont Ã©tÃ© influencÃ©s par React ces derniers mois : Tous !\n\n\n\nEbay : Tweak your page in real time, without leaving the comfort of your editor\n\nBrenton Simpsons dâ€™Ebay, @appsforartists, nous a montrÃ© comment coder en live du React de son mac, avec le rendu affichÃ© en temps rÃ©el sans reload sur un ipad.\n\nLâ€™avantage dâ€™un iPad Ã©tant sa taille qui lui permet de reprÃ©senter 3 Ã©crans dâ€™iPhone 5 sur sa largeur, soit 3 Ã©tats de son application.\nIl utilise Â« WebPack Â» et lâ€™extension pour WebPack Â« react-hot-loader Â» de Dan Abramov.\n\nEbay a aussi open-sourcÃ© un framework assez experimental (6 mois dâ€™anciennetÃ©) du nom dâ€™Ambidex pour gÃ©rer du server side rendering avec React et Flux : https://github.com/appsforartists/ambidex\n\n\n\nData fetching for React Applications at Facebook\n\nJing Chen, @jingc, et Daniel Schafer, @dlschafer, nous ont prÃ©sentÃ© Relay, une nouvelle approche au pattern Flux orientÃ© Data Fetching, permettant grÃ¢ce Ã  GraphQL de dÃ©finir au niveau de son composant les data nÃ©cessaires.\nRelay se chargeant ensuite de gÃ©nÃ©rer les bons appels HTTP grÃ¢ce Ã  GraphQL.\n\nUne approche intÃ©ressante, mais qui parait trÃ¨s couplÃ©e au fonctionnement de Facebook et soulÃ¨ve pas mal de questions : dois-je modifier toutes mes API pour supporter GraphQL ? Quid de lâ€™optimisation du cache cotÃ© API ? â€¦\n\nRelay sera open-sourcÃ© prochainement (ainsi que GraphQL jâ€™imagine ?).\n\nBeaucoup plus dâ€™infos sont disponibles ici : https://gist.github.com/wincent/598fa75e22bdfa44cf47\n\n\n\nCommunicating with channels\n\nJames Long, @jlongster, assez rÃ©putÃ© via son blog https://jlongster.com et pour son travail chez Mozilla sur les Dev Tools de Firefox, a prÃ©sentÃ© une maniÃ¨re de communiquer entre composants via des Â« channels Â» en utilisant la librairie â€œ js-scpâ€  permettant de coder Ã  la maniÃ¨re des Â« goroutine Â» de Go ou des Â« core.async Â» de Clojurescript.\n\n \n\n\n\nReact-router increases your productivity\n\nMichael Jackson, @mjackson, co-crÃ©ateur du routeur le plus populaire de React â€œreact routerâ€, est venu avec Ryan Florence (lâ€™autre co-crÃ©ateur), nous expliquer les origines du routeur, lâ€™inspiration trÃ¨s forte du router dâ€™Ember.Js, ainsi que quelques techniques avancÃ©es dâ€™utilisations (transitions, etc). Un excellent speaker et une introduction trÃ¨s drÃ´le sur les origines de React-Router.\n\n\n  â€œUrl should be part of your design processâ€\n\n\n\n\n\n\nFull Stack Flux\n\nPete Hunt, @floydophone, lâ€™une des personnes responsable des origines de React et de son Â« open-sourcage Â», ancien Lead-Dev dâ€™Instagram chez Facebook, a prÃ©sentÃ© un talk un peu particulier expliquant comment on pouvait, cotÃ© architecture serveur, reproduire le pattern Flux.\n\n\n  â€œshared mutable state is the root of all evil.â€\n\n\n\n\nMaking your app fast with high-performance components\n\nJason Bonta, de lâ€™Ã©quipe Facebook Ads, Ã  lâ€™origine de la crÃ©ation de React, a ciblÃ© sa prÃ©sentation sur les problÃ¨mes de performances que rÃ©sout React.\nCotÃ© Ads Manager, lâ€™Ã©quipe doit faire des interfaces ultra complexes, avec notamment le besoin de prÃ©senter un nombre dâ€™Ã©lÃ©ments trÃ¨s important dans un tableau.\n\nUn composant qui sera annoncÃ© comme Â« open-sourcÃ© Â» durant sa confÃ©rence : FixedDataTable\nVous pouvez aussi retrouver une Â« review Â» du composant ici : https://www.reactbook.org/blog/fixed-data-table-reactjs.html\n\nOnt Ã©tÃ© abordÃ© :\n\n\n  le ReactAddons : PureRenderMixin\n  lâ€™utilisation du shallowEqual sur le shouldComponentUpdate\n  Ainsi quâ€™une bonne pratique pour la rÃ©alisation des composants, qui est revenue plusieurs fois pendant la conf, consistant Ã  englober le composant, dans un autre composant de type container ne contenant aucune Â« props Â».\n\n\nEn rÃ©sumÃ© : \n\n\n\n\nFormat data and strings in any language with FormatJS and react-intl\n\nDerniÃ¨re confÃ©rence de la journÃ©e par Eric Ferraiuolo, @ericf, sur lâ€™internationalisation et la maniÃ¨re de la gÃ©rer dans React, grÃ¢ce Ã  react-intl (open-sourcÃ© par Yahoo).\n\nPour ceux qui douteraient encore de la complexitÃ© de gÃ©rer plusieurs langues, ainsi que les chiffres et pluralisations, et qui ont cette problÃ©matique sur un projet React, cette vidÃ©o est un must-see.\nFormat.Js a aussi Ã©tÃ© citÃ© et sâ€™apparente Ã  une collection de module Js pour lâ€™internationalisation.\n\n\n\nHype\n\nRyan Florence a fini la journÃ©e sur un showcase dâ€™exemple trÃ¨s intÃ©ressant. \nIl nous a aussi racontÃ© son histoire, et comment il est devenu dÃ©veloppeur : principalement, parce quâ€™il voulait toujours rÃ©pondre â€œouiâ€ quand on lui demandait si il pouvait faire quelque chose.\n\nBref, une excellente maniÃ¨re de finir la journÃ©e de maniÃ¨re fun avec quelques exemples trÃ¨s intÃ©ressants, notamment autour des â€œportalsâ€.\n\nVous pouvez retrouver toutes les dÃ©mos ici : https://github.com/ryanflorence/reactconf-2015-HYPE\n\nPour ceux qui douteraient encore des performances de React, je vous invite Ã  regarder les 5-6 premiÃ¨res minutes de la vidÃ©o.\n\n\n\nConclusion du premier jour\n\nBonne grosse claque sur cette premiÃ¨re journÃ©e, notamment avec lâ€™annonce de React Native. Nous avons eu le droit Ã  une organisation absolument parfaite (snack, boisson chaude et froide Ã  volontÃ©) et des speakers de trÃ¨s grand talent (ce qui nâ€™est pas toujours le cas de certaines confÃ©rences, surtout aussi ciblÃ©e que celle-lÃ ).\n\np.s: Retrouvez les retours sur la deuxiÃ¨me journÃ©e de la React confÃ©rence 2015.\n"
} ,
  
  {
    "title"    : "App Isomorphic: la Single Page App parfaite ?",
    "category" : "",
    "tags"     : " javascript, webperf, angular, react, flux, isomorphic",
    "url"      : "/2014/12/04/isomorphic-single-page-app-parfaite-react-flux.html",
    "date"     : "December 4, 2014",
    "excerpt"  : "Quâ€™est ce quâ€™une Single Page App (SPA) ?\n\n\n  Â« As rich and responsive as a desktop app but built with HTML5, CSS and Javascript Â»\n\n\nLes SPA se rÃ©pandent de plus en plus, et deviennent un choix Â« commun Â» lorsque lâ€™on veut dÃ©velopper un Front riche...",
  "content"  : "Quâ€™est ce quâ€™une Single Page App (SPA) ?\n\n\n  Â« As rich and responsive as a desktop app but built with HTML5, CSS and Javascript Â»\n\n\nLes SPA se rÃ©pandent de plus en plus, et deviennent un choix Â« commun Â» lorsque lâ€™on veut dÃ©velopper un Front riche (souvent cÃ¢blÃ© sur des API REST) que lâ€™on souhaite :\n\n\n  testable (unitairement et fonctionnellement)\n  fluide (pas de rechargement dâ€™url etc)\n  bien organisÃ©\n  maintenable et Ã©volutif\n  â€¦\n\n\nLes Frameworks type AngularJs et EmberJs tiennent le haut du panier et ont largement fait leurs preuves, mais ils continuent Ã  Ã©chouer sur deux sujets pourtant primordiaux dans beaucoup de cas :\n\n\n  La performance (dont le rendu initial)\n  Le rÃ©fÃ©rencement\n\n\nLa performance\n\nAujourdâ€™hui, quand vous chargez une SPA, voici grossiÃ¨rement ce qui se passe cotÃ© client :\n\n\n  Chargement du fichier HTML\n  Chargement des diffÃ©rents Assets (Css, image, scripts JS externe comme Angular et Jquery par exemple)\n  Ainsi que de lâ€™intÃ©gralitÃ© du code JS de votre application (sauf si vous lazyloadez)\n  Execution de tout ce petit monde, qui va devoir savoir oÃ¹ vous Ãªtes dans lâ€™application afin de gÃ©nÃ©rer le HTML correspondant Ã  lâ€™Ã©tat demandÃ©.\n\n\nAvoir ces quelques secondes Ã  attendre avant de se retrouver dans un Ã©tat fonctionnel est peut Ãªtre acceptable pour un backoffice. Mais Ã§a lâ€™est beaucoup moins pour un front riche.Et ce temps aura tendance Ã  augmenter fortement, parallÃ¨lement Ã  lâ€™enrichissement de votre application.\n\nSi lâ€™on se soucie un minimum des aspects de performances Web, câ€™est forcÃ©ment dÃ©rangeant.\nEt dâ€™un point de vue plus global, tout le monde sait aujourdâ€™hui que la performance brute nâ€™est pas le point fort de ces frameworks.\n\nLe rÃ©fÃ©rencement\n\nAutre sujet, qui peut Ãªtre trÃ¨s problÃ©matique, si le site en question sâ€™y prÃªte. Ces applications vont fournir comme Â« source HTML Â» quelque chose de ce style (pour du Angular) :\n\n&amp;lt;!doctype html&amp;gt;\n&amp;lt;html class=&quot;no-js&quot;&amp;gt;\n&amp;lt;head&amp;gt;\n    ...\n&amp;lt;/head&amp;gt;\n&amp;lt;body ng-app=&quot;myApp&quot;&amp;gt;\n    &amp;lt;ng-view&amp;gt;&amp;lt;/ng-view&amp;gt;\n    &amp;lt;script src=&quot;scripts/vendor.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&quot;scripts/main.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n\n\n  Container qui servira Ã  recevoir le HTML gÃ©nÃ©rÃ© par votre appli JS une fois exÃ©cutÃ©e.\n\n\nDe base, Google (et autres moteurs/crawler) ne verra donc rien, tout votre contenu allant Ãªtre injectÃ© via JS dans votre balise ng-view. \nExceptÃ© le fait quâ€™il parait que depuis des mois/annÃ©es, Google commence Ã  rÃ©ellement crawler du JS â€¦ Si le site est important, cette supposition ne devrait pas suffire Ã  vous convaincre, et vous avez raison.\n\nRassurez vous, Ã  ce stade, des solutions existent pour fournir spÃ©cifiquement Ã  Google une version correspondante aux snapshots HTML gÃ©nÃ©rÃ©s par vos applications.\nCes solutions sont accessibles soit en mode SAAS (payante et hÃ©bÃ©rgÃ©), soit en mode Open-Source Ã  hÃ©berger vous mÃªme. Je pense notamment Ã  Prerender.io qui fait plutÃ´t bien le job, et vous propose dâ€™indiquer aux moteurs que vous faites une application de type Â« Ajax Â» en respectant les recommandations de Google.\n\nPrerender est composÃ©e de plusieurs briques :\nUn middleware applicatif (Rails, Node, Varnish, Nginx, etc selon votre infrastructure), qui va intercepter les moteurs et les renvoyer sur votre service de Prerender \nUn service de Prerender qui est une brique Node.js qui va lancer des HeadLess Browser (PhantomJS ou SlimerJs â€¦) pour executer votre appli JS et renvoyer un snapshot HTML une fois le rendu JS terminÃ©.\n\nLa solution permet Ã  priori de faire le boulot, mais cela reste une gymnastique complexe, et beaucoup dâ€™interrogations subsistent (pertinence, maintenance, stabilitÃ©, Page Rank, pondÃ©ration vs sites classiques â€¦)\n\nLa lumiÃ¨re au fond du tunnel ?\n\nVous lâ€™avez donc compris, dans certains cas, les SPA basÃ©es sur des frameworks Js posent deux problÃ¨mes trÃ¨s gÃªnants et difficilement rÃ©solvables.\nCâ€™est lÃ  quâ€™entre en piste, une nouvelle faÃ§on de penser les SPA, grace Ã  une librairie dÃ©veloppÃ©e par Facebook : React.JS\n\nReact fait parler de lui car il commence Ã  Ãªtre utilisÃ© massivement par des trÃ¨s gros acteurs Web, Facebook bien entendu pour ses composants Chat, ou son Ã©diteur vidÃ©o, Instagram pour lâ€™intÃ©gralitÃ© du site, Yahoo Mail, Github avec lâ€™IDE Atom, Khan Academy, NyTimes, Feed.ly â€¦\n\nAu premier abord, React nâ€™est quâ€™une librairie quâ€™on pourrait comparer Ã  la partie Vue dâ€™un Framework MVC (voir aux Directives dâ€™Angular), mais il a la particularitÃ© dâ€™Ãªtre basÃ© sur un Virtual DOM.\nCe qui parait au dÃ©part simplement une bonne idÃ©e pour avoir des performances bien supÃ©rieures Ã  celle dâ€™un framework MVC basÃ© sur le DOM, et Ã©viter par exemple les Dirty checking du DOM (qui explique en partie le manque de perf dâ€™Angular), permet aussi dâ€™utiliser ces mÃªmes composants cotÃ© serveur !\n\nCâ€™est ce quâ€™on appelle lâ€™approche Â« Isomorphic Â» .\n\nUn composant React nâ€™est finalement quâ€™un module CommonJs et peut donc aussi bien Ãªtre utilisÃ© cotÃ© browser sur le client, que cotÃ© server dans du Node.Js (ou IO.js devrais-je dire maintenant ?).\nLâ€™idÃ©e de lâ€™isomorphisme est aussi dâ€™Ãªtre capable de servir le premier rendu directement par le serveur.\nExemple:\n\n\n  Vous accÃ©dez Ã  votresite.com/votrepage.html\n  Votre serveur Node, construit votre page et sert le rendu HTML gÃ©nÃ©rÃ© par votre appli au client\n  Il sert aussi votre application JS dans un Bundle (gÃ©nÃ©rÃ© via du Gulp ou Grunt par WebPack ou Browserify)\n  Le client reÃ§oit un fichier statique et lâ€™affiche (sans attendre le moindre JS)\n  Il reÃ§oit aussi le bundle Js\n  Une fois affichÃ©, React sait reprendre la main sur votre appli afin de continuer en mode SPA pour la suite de lâ€™application.\n\n\nEt lÃ , vous rÃ©pondez de maniÃ¨re parfaite aux deux points problÃ©matiques.\nGoogle nâ€™y verra que du feu, et pourra crawler votre site entiÃ¨rement comme si il nâ€™Ã©tait composÃ© que de fichiers statiques. \nLa performance du premier rendu sera quasi imbattable, car ne nÃ©cessitant aucun JS !\n\nSur le papier, câ€™est juste le rÃªve ultime de tout dÃ©veloppeur Front-end : tous les avantages dâ€™une SPA sans les inconvÃ©nients !\n\nFacebook propose aussi sur son Github, une solution pour ceux ayant dÃ©jÃ  un applicatif dans un autre language (ici PHP) : Server side rendering\n\nLa solution parfaite ?\n\nPresque.\nReact nâ€™est au final que la partie Vue de votre application, il va falloir encore organiser tout Ã§a. Câ€™est ici quâ€™entre en compte Flux, un pattern dâ€™architecture unidirectionnel proposÃ© aussi par Facebook, Ã  priori plus scalable que ne lâ€™est le pattern MVC.\n\nMais lÃ  encore, lâ€™approche de Flux est plutÃ´t prometteuse, alors quel est le problÃ¨me ?\n\n\n  Finalement câ€™est encore peu mature (dÃ©jÃ  React et Flux, mais encore plus lâ€™approche Isomorphic)\n  La montÃ©e en compÃ©tence nâ€™est pas nÃ©gligeable\n  Il nâ€™y a pas vraiment de Framework comparable Ã  date, et vous allez surement devoir rÃ©inventer la roue Ã  certains moments (Ã  suivre lâ€™arrivÃ©e imminente de React Nexus notamment)\n  La documentation est trÃ¨s faiblarde encore\n  Les ressources trÃ¨s difficiles Ã  trouver et de qualitÃ©s trÃ¨s diffÃ©rentes\n  Pas vraiment de starter-kit ou gÃ©nÃ©rateur digne de ce nom\n  Le cotÃ© Isomorphic va aussi engendrer une certaine complexitÃ© :\n    \n      Est-ce que mon client reÃ§oit bien le mÃªme Ã©tat que celui quâ€™avait mon serveur au moment du rendu initial\n      Obligation de nâ€™utiliser que des composants Isomorphic, typiquement un router qui fonctionne aussi bien cotÃ© client que serveur (React-Router ou Director), mÃªme chose pour les requÃªtes HTTP (Superagent par exemple) â€¦\n    \n  \n\n\nSi malgrÃ© ces points, vous souhaitez tester cette approche, je vous conseille de regarder du cotÃ© de Yahoo, qui aprÃ¨s avoir annoncÃ© la migration de Yahoo Mail de PHP/YUI vers React/Flux Isomorphic a aussi publiÃ© quelques packages Open-Source trÃ¨s intÃ©ressants, pouvant constituer une bonne base de dÃ©part pour un projet isomorphic :\n\n\n  Fluxible-App\n  Flux-examples\n  ou cet exemple utilisant Fluxible-app : Isomorphic-React\n\n\nSi vous souhaitez plus dâ€™infos sur React et Flux, je vous conseille ces deux articles en anglais de @andrewray:\n\n\n  React for stupid people\n  Flux for stupid people\n\n\nOu ce tuto chez nos amis de Jolicode, pour faire un Gifomatic avec React et Flux\n\nDâ€™autres solutions existent aussi conservant la mÃªme approche, mais sur la base dâ€™autres technos, notamment celle dâ€™Airbnb: RendR, permettant dâ€™utiliser du Backbone cotÃ© client et serveur.\n\nEt pour finir, si ces sujets vous passionnent tout comme nous, restez Ã  lâ€™Ã©coute ici, dâ€™autres posts pourraient arriver Ã  lâ€™avenir ;-)\n\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - troisiÃ¨me journÃ©e",
    "category" : "",
    "tags"     : " conference, velocity, webperf, devops, sysadmin",
    "url"      : "/velocity-europe-2014-day-3",
    "date"     : "December 3, 2014",
    "excerpt"  : "Velocity Barcelone, troisiÃ¨me journÃ©e\n\nLe troisiÃ¨me jour Ã©tant dÃ©diÃ© aux tutoriaux, on passe de confÃ©rence de 45min Ã  des ateliers de 1h30.\n\nExtreme Web Performance for Mobile Devices\n\nMaximiliano Firtman nous a dressÃ© un portrait vraiment exhaust...",
  "content"  : "Velocity Barcelone, troisiÃ¨me journÃ©e\n\nLe troisiÃ¨me jour Ã©tant dÃ©diÃ© aux tutoriaux, on passe de confÃ©rence de 45min Ã  des ateliers de 1h30.\n\nExtreme Web Performance for Mobile Devices\n\nMaximiliano Firtman nous a dressÃ© un portrait vraiment exhaustif du web mobile et de lâ€™Ã©tat actuel des navigateurs.\n\n\n\nEn gros câ€™est compliquÃ©. Le marchÃ© est trÃ¨s fragmentÃ©, certains constructeurs comme Samsung ajoute du bruit en diffusant massivement un navigateur modifiÃ©. Lâ€™usage des sites en webview depuis une application native nâ€™arrange pas les choses (par exemple, lâ€™application Facebook).\n\nAprÃ¨s un rappel sur lâ€™importance de la performance, lâ€™orateur a distillÃ© de nombreuses pratiques permettant de faire un web mobile plus performant.\n\nOn peut retenir :\n\n\n  Le RWD est un outil, pas une fin en soi,\n  il faut sâ€™imposer de tester sur du hardware cheap avec une connection faible,\n  ne pas oublier le temps perdu sur le rÃ©seau (600ms mandatory network overhead),\n  ne pas oublier lâ€™impact que le parsing du JS et le rendu CSS est bloquant,\n  utiliser les solutions de stockage cotÃ© client,\n  de trÃ¨s nombreux outils de simulation existent, il faut les maitriser.\n\n\nIl propose un site rÃ©capitulant toutes les informations dÃ©livrÃ©es : https://firtman.github.io/velocity/.\n\nSlides :\n\n \n  Extreme Web Performance for Mobile Devices - Velocity Barcelona 2014  from Maximiliano Firtman \n\n\n\nZero Downtime Deployment with Ansible\n\n\n\nSlides\n\nGithub Repo\n\nTutorial intÃ©ressant conduit par un dÃ©veloppeur (sur un sujet Ã  priori plus opÃ©rationnel) qui dÃ©montre bien la flexibilitÃ© et la simplicitÃ© dâ€™Ansible.\n\nAprÃ¨s avoir mener le tutorial Ã  son terme vous aurez deployÃ© deux machines avec du code Java, un load balancer NGINX, et une base de donnÃ©es PostgreSQL (utilisateur + base).\n\nA contre courant des systÃ¨mes de gestion de configurations comme SaltStack, Puppet ou Chef, Ansible est basÃ© sur le modÃ¨le push et ne nÃ©cessite aucun agent, il repose entiÃ¨rement sur SSH. Dâ€™autre part il mixe gestion de configuration et orchestration, ce que quâ€™on doit bien souvent faire via des outils tiers comme MCollective.\n\nLa simplicitÃ© de ce modÃ¨le en fait sa plus grande force. Ansible est capable de gÃ©rer dynamiquement les inventaires (de base câ€™est une liste statique contenue dans un fichier). Par exemple il est capable dâ€™interroger les APIs Amazon, Google Cloud ou RackSpace pour rÃ©cupÃ©rer la liste de vos machines, celles de votre Cluster VMWare ou nâ€™importe quel script qui sortira une liste en JSON.\n\nAlors que Chef et Puppet offrent une DSL pour dÃ©crire votre infrastructure sous forme de code, Ansible a optÃ© pour une description au format YAML. Sur lâ€™Ansible Galaxy vous retrouverez tout les modules disponibles (quelques milliers) comme Nginx, PHP etcâ€¦ DÃ©veloppÃ©s en Python, il est Ã©videmment possible de faire soit mÃªme ses modules.\n\nLe dÃ©ploiement avec zÃ©ro temps de panne peut Ãªtre implÃ©mentÃ© avec Ansible de la faÃ§on suivante:\n\n\n  rÃ©cupÃ©ration de la liste des machines\n  sortie du load balancer dâ€™une machine (Ansible est compatible GCE et AWS)\n  mise Ã  jour de la configuration (code et/ou logiciel)\n  pÃ©riode dâ€™attente: vous spÃ©cifiez si un port TCP doit Ãªtre disponible, un fichier, etcâ€¦\n  itÃ©ration sur la machine suivante\n\n\nLe nombre de machine traitÃ©es en parallÃ¨le est bien entendu configurable.\n\n\n\nJe suis Ansible depuis quelques mois dÃ©jÃ  et jâ€™ai Ã©tÃ© confortÃ© dans lâ€™idÃ©e que câ€™est un excellent produit: pas dâ€™agent, basÃ© sur une brique solide quâ€™est SSH, et dÃ©veloppÃ© en Python :) La gestion de lâ€™inventaire peut Ãªtre dÃ©licate, mais un CMDB comme Collins de Tumblr ou un taggage prÃ©cis peuvent rÃ©soudre lâ€™Ã©quation.\n\nAnsible facilite le dÃ©ploiement dâ€™infrastructure immuable, le blue/green, violet et canary deployment de par son modÃ¨le. Câ€™est un atout qui en fait Ã  mon sens le meilleur systÃ¨me de gestion de configuration aujourdâ€™hui.\n\nCependant je reste encore un peu dubitatif sur le dÃ©ploiement et le rollback de code qui ne sont pas encore Ã  la hauteur de Capistrano. Un aperÃ§u du workflow et des schÃ©mas de dÃ©veloppement auraient Ã©tÃ© aussi bienvenus.\n\n\n  Ansible Galaxy\n  Ansible Docs\n  How Twitter use Ansible\n  Ansible Tower (Payant)\n  Thoughts on deploying Symfony with Ansible\n\n\n\n\nLinux Containers from Scratch\n\n\n\nSlides\n\nQuelle est la diffÃ©rence entre le cloud, les containers et un repas gratuit ? Aucun nâ€™existe :)\n\nJoshua Hoffman (SoundCloud) est dans le top 5 de mes orateurs prÃ©fÃ©rÃ©. Jâ€™ai beaucoup apprÃ©ciÃ© ce tutorial car il fait clairement la part entre virtualisation, containers, LXC et Docker (nom qui ne sera prononcÃ© quâ€™Ã  la fin lors des questions, pas de buzzword, de hype ni de marketing, merci Joshua).\n\n\n\nLe tutorial vous amÃ©nera Ã  crÃ©er plusieurs containers portable, du plus simple ou plus complexe, avec les outils de bases du noyau. Vous apprendrez aussi Ã  vous servir des cgroups, des namespaces process, network, et mount, et serez amenÃ© Ã  utiliser des systÃ¨mes de fichiers unis, ici AUFS.\nJâ€™aurais bien aimÃ© une dÃ©mo avec le format de QEMU ou btrfs pour ce qui est des systÃ¨mes de fichiers unis au niveau bloc.\n\nCe tutorial est un must-do pour tout personne dÃ©sirant sâ€™initier aux architectures de containers. Le marketing relativement agressif de Docker ne doit pas faire oublier quâ€™il existe dâ€™autres alternatives, et que Docker est un choix de design bien particulier pas forcement adapter Ã  tous.\nEx: un container en 3 lignes:\n\n\n\nPour rappel:\n\nLXC/LXD = Ensemble dâ€™APIs et dâ€™outils dans lâ€™espace utilisateur linux exposant les capacitÃ©s dâ€™isolation du noyau (cgroups, chroot, namespaces, selinux, iptables etcâ€¦), alternative lÃ©gÃ¨re Ã  la virtualisation telle quâ€™on la connaÃ®t (avec Vmware par exemple)\n\nDocker = un des cas dâ€™usage des containers, application unique, statique, immuable, single app delivery plateform\n\nLXC\n\nDocker F.A.Q\n\n\n\nCoreOps - CoreOS for Sysadmins\n\n\n\nGithub\n\nTutorial trÃ¨s attendu par beaucoup, Kelsey Hightower (CoreOS Inc.) nous a prÃ©sentÃ© lâ€™Ã©cosystÃ¨me de CoreOs et les problÃ¨mes quâ€™il tente de rÃ©soudre. Suite Ã  la demande gÃ©nÃ©rale il nous a aussi fait une dÃ©monstration de Kubernetes, lâ€™outil de gestion de containers de Google.\n\nCoreOS est distribution Linux accompagnÃ©e dâ€™outils qui vise Ã  penser le datacentre comme une seule machine (voir Mesos/Yarn). En dâ€™autres termes, vous nâ€™avez que faire de savoir quelle application tourne sur quel serveur. Le datacentre apparaÃ®t comme une entitÃ© unique oÃ¹ lâ€™on dÃ©ploie des applications.\n\nTechniquement, CoreOS est un Linux + systemd + docker + etcd + fleet. CoreOS est basÃ© sur Chrome OS, Ã©purÃ© et lÃ©ger, il bÃ©nÃ©ficie du systÃ¨me dâ€™update en arriÃ¨re plan bien connu de Chrome. On oublie donc le gestionnaire de paquets, les outils de debug (tcpdump etc..) et tout ce qui fait un Linux en mode serveur tel quâ€™on le connaÃ®t.\n\n\n  gentoo: parfum de distribution Linux (ex: Ubuntu, Debian, Centos)\n  systemd: alternative Ã  SysV Init, le gestionnaire des dÃ©mons, le premier programme lancÃ© au dÃ©marrage (PID: 1)\n  docker: SystÃ¨me de containers lÃ©gers, ensemble dâ€™apis et librairies centrÃ©s sur le dÃ©ploiement et la gestion dâ€™application isolÃ©e du kernel.\n  etcd: base de donnÃ©es clÃ©/valeur distribuÃ©e, utilisÃ©e pour centraliser la configuration et la dÃ©couverte de service, fondÃ©e sur le protocole de consensus Raft.\n  fleet: SysV Init distribuÃ© (câ€™est la glue entre systemd et etcd), votre programme doit au minimum avoir 3 instances ? fleet sâ€™en assurera !\n\n\n\n\n\n\nLa dÃ©monstration vous amÃ¨nera Ã  lancer 1 master et plusieurs machines â€œworkersâ€ et quelques containers Docker.\n\n\n\nKubernetes est la rÃ©ponse de Google Ã  la question des gestionnaires de containers disitribuÃ©s.\n\nConstituÃ© dâ€™un certain nombre de composants quâ€™on ne dÃ©taillera pas ici, il permet de gÃ©rer des pods (un ou plusieurs containers qui doivent fonctionner localement sur le mÃªme host). Il intervient dans la rÃ©partition des applications dans le cluster, la distribution et lâ€™ordonnancement des containers Docker.\n\nLiens:\n\n\n  CoreOS Doc\n  Kubernetes\n  Introduction to Kubernetes\n\n\nâ€“\n\nResponsive and Fast: Iterating Live on a RWD Site\n\nCette confÃ©rence est globalement une redite des autres sur lâ€™optimisation cÃ´tÃ© front. Colin Bendell dâ€™Akamai nous prÃ©sente plusieurs outils comme webpagetest, mais aussi des astuces pour tester sur Device depuis chrome. Il nous rappelle quâ€™il faut faire attention aux conditions de tests avec certains facteurs comme la connexion. Il faut faire aussi attention Ã  limiter le nombre dâ€™images, de ressources (js, css â€¦). Un des gros problÃ¨mes sur un site responsive, est celui des images. Pour Ã©viter de charger des images trop importantes, il faut utiliser la balise . Cette nouvelle balise nâ€™Ã©tant pas disponible sur tous les navigateurs, il nous conseille dâ€™utiliser un composant Picturefill. En ce qui concerne les CSS, il conseille d&#39;intÃ©grer directement les css critiques dans le corps de la page et de ne charger, par la suite, que les css correspondants au device que lâ€™on utilise. Pour conclure, lâ€™utilisation dâ€™un CDN avancÃ© est hautement recommandÃ©e grÃ¢ce Ã  des options permettant de diffÃ©rencier navigateurs / devices.\n\nLiens :\n\n  Slide de la prÃ©sentation\n\n\n\n\nBuild a device lab\n\n\n  â€œQui a un placard avec pleins de devices en vrac qui nâ€™ont ni cÃ¢ble, ni batterie et dont vous ne connaissez plus le mot de passe ?â€\n\n\nJâ€™ai levÃ© la main ;) .\n\nLara Hogan et Destiny Montague nous ont expliquÃ© comment Etsy avait construit un device lab, permettant Ã  leurs collaborateurs dâ€™emprunter des appareils mobiles pour tester leurs applications, sites mobiles et newsletters.\n\nLâ€™idÃ©e est dâ€™outiller puissamment les Ã©quipes et de leur donner un accÃ¨s extrÃªmement simple Ã  un parc complet (mÃªme un chromebook pixel !) - afin dâ€™assurer un maximum de tests sur les diffÃ©rents Ã©quipements.\n\nBien sÃ»r il y a un device lab pour les Ã©quipes techniques et un autre pour le produit / marketing.\n\nLes sujets suivants ont Ã©tÃ© abordÃ©s :\n\n\n  choix des appareils\n  consommation Ã©lectrique\n  le setup des devices (Ã  lâ€™aide dâ€™un Mobile Device Management)\n  les tests\n  le rÃ©seau\n  un retour complet sur lâ€™expÃ©rience utilisateur\n\n\nUn site complet dÃ©diÃ© Ã  leur confÃ©rence est disponible : https://larahogan.me/devicelab/.\n\nUne vidÃ©o de la mÃªme confÃ©rence Ã  New York est Ã©galement en ligne :\n\n\n\n\n\nUne confÃ©rence un peu #old car dÃ©jÃ  faite, mais toujours dâ€™actualitÃ© concernant la problÃ©matique. Je suis bluffÃ© par la capacitÃ© dâ€™Etsy Ã  mettre en oeuvre des moyens et des compÃ©tences sur des sujets quâ€™ils estiment importants. Câ€™est sÃ»rement en lien avec le succÃ¨s que la sociÃ©tÃ© rencontre actuellement.\n\n\n\nConclusion\n\nUne confÃ©rence dense et intÃ©ressante, qui nous a donnÃ© lâ€™opportunitÃ© de rencontrer pleins de gens intÃ©ressants et mÃªme de visiter (un peu) Barcelone !\n\n\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - seconde journÃ©e",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2014-day-2",
    "date"     : "November 24, 2014",
    "excerpt"  : "Velocity Barcelone, seconde journÃ©e\n\nDeuxiÃ¨me jour de confÃ©rence avec un programme encore plus chargÃ© et quelques confÃ©rences allÃ©chantes repÃ©rÃ©es au prÃ©alable.\n\nMorning Keynotes\n\nUpgrading the Web: Polyfills, Components and the Future of Web Deve...",
  "content"  : "Velocity Barcelone, seconde journÃ©e\n\nDeuxiÃ¨me jour de confÃ©rence avec un programme encore plus chargÃ© et quelques confÃ©rences allÃ©chantes repÃ©rÃ©es au prÃ©alable.\n\nMorning Keynotes\n\nUpgrading the Web: Polyfills, Components and the Future of Web Development at Scale - Andrew Betts (FT Labs)\n\nLâ€™orateur fait remarquer que de nombreux systÃ¨mes existent pour packager et gÃ©rer les dÃ©pendances des applications backends, mais rien nâ€™est disponible pour les composants webs. Il nous a prÃ©sentÃ© le projet Origami qui permet de rÃ©utiliser massivement des composants HTML.\n\n\n\nSlides : the Future of Web Development at Scale\n\nTroubleshooting Using HTTP Headers - Steve Miller-Jones (Limelight Networks)\n\nSlides : Troubleshooting Using HTTP Headers\n\nUn employÃ© de Limelight nous a prÃ©sentÃ© comment lâ€™ajout de headers dans une requÃªte pouvait renvoyer des headers supplÃ©mentaires dans la rÃ©ponse HTTP. Cela peut Ãªtre utile pour dÃ©bugguer et analyser un incident.\n  Cette prÃ©sentation nous a rappelÃ©, quâ€™en interne, nos gentils ops nous permettent dÃ©jÃ  de faire ce genre chose sur nos proxy cache.\n\nMonitoring without Alerts - and Why it Makes Way More Sense than You Might Think - Alois Reitbauer (ruxit.com)\n\nAlois Reitbauer a Ã©voquÃ© la solution Ruxit dÃ©veloppÃ©e depuis plus de trois ans. Cette solution consiste Ã  installer un agent sur vos serveurs qui va automatiquement dÃ©tecter des anomalies statistiques et corrÃ©ler cette information avec dâ€™autres dÃ©viations dans le but de trouver la root cause dâ€™un incident.\n\nBeaucoup dâ€™autres solutions de ce genre existent (et la plupart Ã©taient dans le salon des sponsors). Nous nâ€™avons pas Ã©tÃ© totalement convaincu de leurs capacitÃ©s Ã  dÃ©tecter des root cause, mais elles sont toutes assez intÃ©ressantes et matures.\n\n\n\nLowering the Barrier to Programming - Pamela Fox (Khan Academy)\n\nPamela Fox nous a prÃ©sentÃ© lâ€™initiative code.org, dont le but est de promouvoir lâ€™enseignement de lâ€™informatique (bon, apparement seulement aux US).\n\nElle a Ã©galement donnÃ© quelques conseils si on veut sâ€™investir dans lâ€™enseignement de lâ€™informatique Ã  destination des plus jeunes. Par exemple crÃ©er un code club.\n\n\n\nSlides : Lowering the Barrier to Programming\n\nVelocity at GitHub - Brian Doll (GitHub)\n\nBrian a fait une prÃ©sentation trÃ¨s â€¦ minimaliste. Il est revenu rapidement sur 7 ans de dÃ©veloppement Ã  GitHub et comment ils sont venus Ã  dÃ©velopper lâ€™Enterprise Edition. Il a Ã©voquÃ© diffÃ©rents problÃ¨mes que certains de leurs clients avaient et notamment avec lâ€™utilisation de GE en environnement cloud.\n\nIl a donc annoncÃ© le lancement de GitHub Enterprise 2.0 qui fonctionne maintenant sur AWS (et un changement de la grille tarifaire) !\n\n\n\nJâ€™ai profitÃ© dâ€™un instant avec lui pour lui prÃ©senter GitHubTeamReviewer (un outil interne open-sourcÃ©). Il Ã©tait enchantÃ© de dÃ©couvrir ce qui avait Ã©tÃ© fait avec lâ€™API de Github. Il a indiquÃ© que lâ€™entreprise travaillait actuellement sur des vues permettant de pallier aux problÃ¨mes rÃ©solus par GitHubTeamReviewer.\n\nHTTP Archive and Google Cloud Dataflow - Ilya Grigorik (Google)\n\nIlya Grigorik a prÃ©sentÃ© https://bigqueri.es/, un outil permettant dâ€™interroger HTTP archive. La nouveautÃ© est que le body des requÃªtes est maintenant conservÃ© et que lâ€™on peut lâ€™analyser. Un engine Javascript a Ã©tÃ© intÃ©grÃ© au SQL de bigqueries permettant de faire des requÃªtes trÃ¨s puissantes.\n\n\n\nPour ceux qui ne voudraient pas se plonger dedans, beaucoup de recherches faites par dâ€™autres utilisateurs sont disponibles et abondamment discutÃ©es (exemple).\n\n\n\nWebpagetest-automation 2.0 - Nils Kuhn (iteratec GmbH), Uwe BeÃŸle (iteratec GmbH)\n\nWebpagetest est un outil formidable mais il est difficile Ã  automatiser. Les orateurs ont prÃ©sentÃ©s un outil pour le faire, permettant donc de rÃ©aliser une mesure continuelle de la webperf avec un parcours utilisateur complet - dÃ©monstration Ã  lâ€™appui.\n\nLeur travail est disponible sur GitHub sous licence Apache : https://github.com/IteraSpeed/OpenSpeedMonitor. Un grand merci &amp;lt;3 ! (Ã  10 minutes sur la vidÃ©o).\n\n\n\n\n\n\n\nEtsyâ€™s Journey to Building a Continuous Integration Infrastructure for Mobile Apps - Nassim Kammah (Etsy)\n\nUne parmi les trÃ¨s nombreuses confÃ©rences Etsy sur la VÃ©locity (le moment de renouveller les confÃ©renciers ?). Nassim Kammah nous a expliquÃ© comment Etsy dÃ©livrait ses applications iOS.\n\nLa livraison des applications sous iOS est au mÃªme stade que la diffusion des logiciels via CD-ROMs. Partant de ce constat un systÃ¨me de build (avec 25 mac-minis derriÃ¨re) a Ã©tÃ© mis en place Ã  chaque commit sur le master. On ne peut pas dÃ©livrer une version de lâ€™application tous les jours aux clients, mais on peut le faire pour les employÃ©s (and eat your own dog food) !\n\nIl y a Ã©galement un systÃ¨me de gamification, autour de lâ€™application livrÃ©e journaliÃ¨rement, afin de motiver tout le monde Ã  trouver des bugs.\n\n\n\nDes tests unitaires sont mis en place, ainsi que des tests fonctionnels avec AppThwack. Il est intÃ©ressant de constater quâ€™ils nâ€™attendent pas, pour les tests fonctionnels, une rÃ©ussite Ã  100% de la suite mais une tendance positive.\n\nLes Ã©quipes ont Ã©galement mis en place des testing dojos dans lesquels les ingÃ©nieurs QA encadrent des salariÃ©s dâ€™Etsy et testent Ã  fond les applications.\n\nOn peut retrouver tous les Ã©lÃ©ments de cette confÃ©rence sur le blog technique dâ€™Etsy.\n\n\n\nRecycling: Why the Web is Slowing Your Mobile App - Colin Bendell (Akamai)\n\nPourquoi recycler nos contenus pour les applications mobiles ?\n\n\n  accÃ©lÃ©rer le time to market.\n  rÃ©duire le risque\n\n\nLes APIs encouragent le recyclage.\n\n\n\nColin Blendel nous encourage Ã  utiliser les mÃªmes recettes que pour les navigateurs web et Ã  en ajouter dâ€™autres :\n\n\n  gÃ©rer le pool de connexions en groupant les appels par domaine (quitte Ã  les passer sÃ©quentiellement, par exemple, si des cookies sont utilisÃ©s),\n  surveiller les packet eaters (headers inutiles, Set-Cookies rÃ©pÃ©tÃ©s),\n  setter correctement Content-Type sur des types standard (les exemples de content-type tirÃ©s des logs dâ€™Akamai sont assez drÃ´les, comme par exemple test/binary ^^ !),\n  faire un minimum de redirections,\n  fragmenter son cache au minumum (quitte Ã  calculer des clÃ©s plus consistantes cotÃ© client),\n  ajouter du cache (Max-Age: 30s câ€™est Ã  peu prÃ¨s du temps rÃ©el et Ã§a change tout pour un CDN),\n  prÃ©fetcher les urls prÃ©sentes dans les retours dâ€™API, car on va surement en avoir besoin immÃ©diatement aprÃ¨s,\n  ne pas hÃ©siter Ã  mettre CRUD au placard et merger plusieurs appels API en un seul ; il faut trouver une balance efficace pour bien gÃ©rer la webperf.\n\n\nUne prÃ©sentation dense et vraiment intÃ©ressante !\n\nSlides : Why the Web is Slowing Your Mobile App\n\n\n\nBreaking News at 1000ms\n\nLe Guardian est un journal Anglais prÃ©sent sur le web et sur tout type de device. Ils ont rÃ©cemment fait une refonte de leur site pour passer Ã  une version Responsive avec pour challenge dâ€™afficher son contenu en moins dâ€™une seconde.\n\nLe Guardian câ€™est 110 000 utilisateurs, 7000 diffÃ©rents devices. Lâ€™ancien site avait un dÃ©but de rendu en 8 secondes pour un affichage complet en 12. Avec la nouvelle version le site sâ€™affiche en 1 seconde et le chargement complet au bout de 3. Quelles sont les principales optimisations ?\n\nPour commencer, il faut charge le contenu important pour lâ€™utilisateur en premier, Ã  savoir le menu, lâ€™article, et le widget dâ€™article populaire. Le reste du contenu sera chargÃ© dynamiquement en JS.\n\nEn ce qui concerne le css, câ€™est la mÃªme chose. Les CSS importantes (critiques) qui concernent lâ€™article et le rendu global sont intÃ©grÃ©es inline. Ainsi, nous nâ€™avons pas de blocage du rendu de la page. Le reste des css est chargÃ© via Javascript. Avec ce systÃ¨me, on gagne au moins une demi-seconde sur le dÃ©but dâ€™affichage du contenu.\nPour gagner en fluiditÃ© pour les prochains affichages, le css est stockÃ© en localStorage. On gagne ainsi des ressources pour les prochains chargements.\n\nPour les fonts ? Câ€™est la mÃªme chose, elles sont mises en cache dans le localStorage pour supprimer de nouveaux chargements.\n\nEnfin le gros morceau : les images ! Elles sont chargÃ©es de faÃ§on asynchrone en lazyloading. Cela permet de ne pas bloquer le rendu principal de la page.\n\nEn complÃ©ment, ils ont mis en place des outils, notamment pour monitorer dans Github la taille des Assets afin de vÃ©rifier quâ€™il nâ€™y a pas de grosses variations.\n\nAvec ces optimisations et un systÃ¨me de Proxy qui va gÃ©rer les donnÃ©es mises en localStorage, le site peut mÃªme Ãªtre accessible en mode offline.\n\n\n  Github du Front\n  Slide de la prÃ©sentation\n\n\n\n\nOffline-first Web Apps\n\nMatt Andrews nous prÃ©sente comment rendre une application web disponible Offline.\n\nPlusieurs contraintes peuvent nous pousser Ã  avoir besoin dâ€™une app (signet dâ€™accueil) disponible mÃªme sans connexion. Que ce soit un article dans le mÃ©tro ou une carte au milieu de nulle part sans connexion, il y a une rÃ©elle attente utilisateur.\n\nPremiÃ¨rement, il faut activer AppCache en prÃ©cisant quâ€™il faut faire un petit Hack pour quâ€™il soit vraiment utile (voir slide).\n\nEnsuite lâ€™utilisation de plusieurs outils nous permet dâ€™arriver Ã  nos fins :\n\n  Utilisation de FetchApi : Il permet de remplacer nos appels Ajax avec une fonction succÃ¨s , dâ€™erreur et les Promises pour charger le contenu, ou lire le cache en cas dâ€™absence de connexion.\n  Cache API : Il permet de choisir des Url a mettre en cache. Ainsi que de forcer le contenu de ces urls dans le code.\n  Service Worker : Il permet dâ€™intercepter les events de chargement pour ensuite appeler le systÃ¨me de Cache API.\n\n\nToutes ces optimisations nous permettent dâ€™accÃ©der au site en Offline. Mais ces optimisations nous permettent aussi dâ€™optimiser le chargement de nos pages puisquâ€™on limite le nombre dâ€™appels HTTP avec la mise en cache de certaines ressources.\n\nSlide de la prÃ©sentation\n\n\n\nLook, Ma, No Image Requests!\n\nPamela Fox nous prÃ©sente comment elle a optimisÃ© les images dâ€™un site internet.\n\nLa premiÃ¨re astuce est de compresser ces images au maximum. Il existe des outils online comme le site TinyPng qui compresse vos images et vous permet de les tÃ©lÃ©charger directement.\n\nDeuxiÃ¨me astuce, mettre les images dans les css en base 64. \nA noter quâ€™il existe des outils javascript qui effectuent la conversion dans les css Ã  lâ€™aide dâ€™un petit commentaire en bout de ligne (voir les slides de prÃ©sentation).\n\nTroisiÃ¨me solution : Les Fonts ! \nPour remplacer les petites images et surtout pour remplacer les sprites qui ne sont pas forcÃ©ment adaptÃ©s, vous pouvez utiliser des Fonts. Lâ€™avantage des fonts est quâ€™elles peuvent sâ€™adapter facilement en taille et en couleur â€¦ Des outils existent dÃ©jÃ  pour les gÃ©nÃ©rer : Font Awesome.\n\nAutre astuce, le differ de chargement des images. Pamela nous propose son outils javascript, qui va permettre de vous simplifier les chargements. Il est aussi possible de ne charger que les images prÃ©sentes Ã  lâ€™Ã©cran et de charger les suivantes lors du scroll. (lazyload).\n\nPour les vidÃ©os la mÃªme astuce est possible. Puisque les vidÃ©os sont Ã  prÃ©sent chargÃ©es dans des iFrame, leur contenu peut Ãªtre chargÃ© de faÃ§on diffÃ©rÃ©. Attention, il ne faut pas remplir le href par une url blank, sinon on perd en temps de chargement.\n\nSlide de la prÃ©sentation\n\n\n\nMicroservices - What an Ops Team Needs to Know\n\nSlides: Microservices - What an Ops Team Needs to Know\n\n\n\nLe buzzword est lÃ¢chÃ©. Le propos nâ€™Ã©tait pas ici de troller autour de la notion de micro-services, de lâ€™implÃ©mentation ou de leur utilisation, mais plutÃ´t du changement que cela implique pour les Ã©quipes dâ€™exploitation.\n\nSouvent considÃ©rÃ© comme le goulot dâ€™Ã©tranglement de la chaÃ®ne de mise en prod, lâ€™exploitâ€™ regarde les architectures de micro-services avec circonspection : en plus dâ€™avoir des dÃ©pendances entre eux, les composants sont mis Ã  jour indÃ©pendamment et rÃ©guliÃ¨rement, on peut donc vite tout casser en prod.\nPourtant en fournissant des services de bases et des outils aux Ã©quipes de dÃ©veloppement, on peut augmenter leur autonomie et la disponibilitÃ© des infras.\n\nCela passe par:\n\n\n  automatiser les VMs ou les containers en prod comme en dev\n  un systÃ¨me de mÃ©triques â€œAs a Serviceâ€ (similaire Ã  graphite / statsd)\n  un service de log central (logstash/heka/fluentd)\n  un outil de dÃ©ploiement (capistrano/deployinator)\n\n\nCes outils et services ainsi fournis vont permettre Ã  lâ€™exploitation de se concentrer sur des problÃ©matiques plus complexes. En effet les microservices ont besoin dâ€™outils de diagnostics plus poussÃ©s (on citera au passage Zipkin), dâ€™alerting et de monitoring spÃ©cialisÃ©s par exemple.\n\n\n\nQui dit droits, dit devoirs, et lÃ  je paraphraserai notre orateur Michael Brunton-Spall:\n\n\n  Give developers pagers too !\n\n\n\n  Developers should be exposed to the pain they cause\n\n\nCela sâ€™inscrit totalement dans le mouvement â€œYou build it, you run itâ€, oÃ¹ les Ã©quipes de dÃ©veloppement sont responsables de leur code depuis la conception jusquâ€™Ã  la maintenance en production.\n\n\n\nItâ€™s 3AM, Do You Know Why You Got Paged ?\n\nSlides: Itâ€™s 3AM, Do You Know Why You Got Paged ?\n\nRyan Frantz nous a rappellÃ© quelques Ã©lÃ©ments de bon sens concernant les alertes:\n\n\n  un contexte: quel hÃ´te ? serveur ? service ? lâ€™impact front / back ?\n  lâ€™historique de lâ€™alerte et de la mÃ©trique: Ã©tat de la mÃ©trique il y a 5min, 15min, 1 jour, 1 semaine, combien de fois a sonnÃ© lâ€™alerte aujourdâ€™hui ?\n  la raison dâ€™Ãªtre du check (rÃ©digÃ©e par le crÃ©ateur du check)\n  des couleurs et mise en forme permettant de trouver visuellement lâ€™information le plus rapidement possible (rappel il est 3 heure du matin, et peut Ãªtre daltonien, pensez donc bien Ã  vos codes couleurs)\n\n\n\n\n\nSeule une alerte critique doit vous faire lever Ã  3H du matin, un volume disque Ã  80% plein nâ€™est pas rÃ©ellement grave, cependant si son taux de remplissage est passÃ© de 1% par heure Ã  300% par heure, cela peut devenir problÃ©matique.\n\nRyan nous a ensuite prÃ©sentÃ© nagios-herald. Ce plugin nagios permet de multiplexer une alerte dans diffÃ©rent services (cf schÃ©ma) ci dessous.\n\n\n\nPour ma part je prÃ©fÃ¨re Sensu qui intÃ¨gre de base ce type de mÃ©canisme. On peut affecter Ã  une alerte un groupe de handlers (alerte hipchat + graphite + logstash par exemple)\n\n\n\nCustomizing Chef for Fun and Profit\n\n\n\nSlides: Customizing Chef for Fun and Profit\n\nEn suivant les Ã©tapes dâ€™application dâ€™une recette Chef, Jon Cowie a distillÃ© son savoir sur la personnalisation de Chef.\n\nIl nous a par exemple dÃ©montrÃ© quâ€™il Ã©tait trÃ¨s simple de dÃ©velopper son propre plugin ohai et ses propres handlers.\n\nJâ€™ai apprÃ©ciÃ© le passage sur la gestion des Ã©vÃ©nements Chef, en effet la sortie en ligne de commande nâ€™est quâ€™une des faÃ§ons de rÃ©cupÃ©rer les logs, les Ã©vÃ©nements sont basÃ©s sur un systÃ¨me de pub/sub, on pourrait trÃ¨s bien imaginer la publication en live stream dans un redis ou autre.\n\nPar ailleurs Jon vient de publier un livre sur le sujet:\nOâ€™Reilly - Customizing Chef\n\n\n\nMega quiz Velocity\n\nPerry Dyball et Stephen Thair avaient prÃ©parÃ© un quiz interactif avec les participants Ã  la confÃ©rence. Des questions diverses et variÃ©es dÃ©filaient sur le grand Ã©cran et une application web permettaient Ã  chacun dâ€™y rÃ©pondre. Un moment fun animÃ© par deux animateurs survoltÃ©s.\n Malheuresement il semble que lâ€™application nâ€™aient pas tenu la charge et personne nâ€™a pu votÃ© aprÃ¨s la seconde question (la prochaine fois ils devraient nous confier le projet :) ), mais un systÃ¨me de fallback a Ã©tÃ© prÃ©vu, basÃ© sur des feuilles de papier de couleur Ã  brandir bien haut pour rÃ©pondre aux questions.\n\n\n  merci le papier ! :)\n\n\n\n\nConclusion\n\nFin des confÃ©rences et direction les soirÃ©es offertes par Facebook (oÃ¹ nous avons pu discuter avec Santosh Janardhan, responsable des infrastructures de Facebook ^^ !) et Dyn.\n\nLe rÃ©sumÃ© de la premiÃ¨re journÃ©e est Ã©galement disponible.\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - premier jour",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2014-day-1",
    "date"     : "November 19, 2014",
    "excerpt"  : "Velocity Barcelone, premier jour\n\nBaptiste, FranÃ§ois et Olivier ont eu la chance de participer Ã  la VÃ©locity ConfÃ©rence Europe 2014 qui avait lieu cette annÃ©e Ã  Barcelone.\n\nVoici le compte rendu des confÃ©rences et des moments qui les ont marquÃ©s.\n...",
  "content"  : "Velocity Barcelone, premier jour\n\nBaptiste, FranÃ§ois et Olivier ont eu la chance de participer Ã  la VÃ©locity ConfÃ©rence Europe 2014 qui avait lieu cette annÃ©e Ã  Barcelone.\n\nVoici le compte rendu des confÃ©rences et des moments qui les ont marquÃ©s.\n\nMorning Keynotes\n\nLes keynotes du matin semblaient Ãªtre scÃ©narisÃ©es sur diffÃ©rents points que les organisateurs de la confÃ©rence voulaient mettre en avant.\n\nLife after human error - Steven Shorrock (EUROCONTROL)\n\nSteven Shorrock nâ€™est pas un homme de lâ€™IT, mais travaille autour de la sÃ©curitÃ© aÃ©rienne. Il se dÃ©finit comme un ergonomiste des systÃ¨mes. Il a prÃ©sentÃ© comment, autour des erreurs humaines, â€œles mots crÃ©aient le mondeâ€ et entrainaient immÃ©diatement un jugement social (â€œnÃ©gligenceâ€ est Ã©videment plus connotÃ© que â€œerreur dâ€™attentionâ€). Il peut y avoir des erreurs dans la dÃ©finition dâ€™une erreur. Qualifier une erreur demandait une dÃ©finition prÃ©cise de standards et de contextes. \nIl a Ã©galement conseillÃ© dâ€™Ã©tudier les cas de fonctionnement normaux ; ne pas faire seulement des post-mortem mais des pre et des no mortem.\n\n\n\nUne prÃ©sentation intÃ©ressante sur lâ€™incident et lâ€™erreur.\n\n\n\nMaximize the Return of Your Digital Investments - Aaron Rudger (Keynote Systems)\n\nUne prÃ©sentation sponsorisÃ©e bien faite, montrant les difficultÃ©s de communication entre deux populations (IT et biz en lâ€™occurence) et comment un outil performant et agrÃ©able peut aider Ã  combler ce gap.\nChez M6Web nous utilisons grafana, et il est vrai que cet outil pourrait largement sortir du pÃ©rimÃ¨tre de lâ€™IT.\n\nSlides : Maximize the Return of Your Digital Investments\n\n\n\nAlways Keep an Eye on Your Website Performance - PerfBar Khalid Lafi (WireFilter)\n\nUne rapide dÃ©monstration dâ€™un outil en javascript Ã  installer sur les postes de vos dÃ©veloppeurs et permettant dâ€™afficher des alertes si un site en production (ou ailleurs) dÃ©passe un certain seuil.\n\nA dÃ©couvrir : PerfBar\n\n\n\nThe Impatience Economy, Where Velocity Creates Value - Monica Pal (Aerospike Inc.)\n\nIl y a une gÃ©nÃ©ration on attendait 10 jours un Ã©change de courrier postal, aujourdâ€™hui un adolescent vÃ©rifie son tÃ©lÃ©phone toutes les 10 secondes ! Nous sommes moins attentifs, plus impatients.\nDe ce constat Monica Pal explique comment les backend web doivent sâ€™adapter et servir de plus en plus dâ€™informations contextualisÃ©es : search, sort, recommand, personalize.\n\nSlides : The Impatience Economy\n\n\n\nRecruiting for Diversity in Tech - Laine Campbell (Pythian)\n\nUn thÃ¨me rÃ©current de la velocity de cette annÃ©e. Laine explique comment lâ€™ascenseur mÃ©ritocratique est cassÃ© et que seule une dÃ©marche volontaire permettra dâ€™augmenter la diversitÃ© dans les entreprises.\n\nSlides : Recruiting for Diversity in Tech\n\n\n\nBetter Performance Through Better Design - Mark Zeman (SpeedCurve)\n\nLa derniÃ¨re keynote Ã©tait vraiment excellente. Mark Zeman, venu de Nouvelle ZÃ©lande, a expliquÃ© comment le processus crÃ©atif pouvait aider Ã  amÃ©liorer la performance. Dans ce but il a proposÃ© de redesign the design process.\n\n  se fixer certains principes/objectifs de performance dÃ¨s le dÃ©part\n  ajouter les designers dans la feature team et itÃ©rer via des prototypes\n  de partager le savoir sous forme dâ€™informations visuelles (graphique mais aussi sous forme dâ€™un bookmarklet indiquant quelle partie dâ€™un site met du temps Ã  charger)\n\n\n\n\n\n\nJe vous invite vivement Ã  regarder sa vidÃ©o :\n\n\n\n\n\nIT Janitor, How to Tidy Up - Mark Barnes (Financial Times)\n\nCe manager au Financial Times a expliquÃ© comment le journal a Ã©tÃ© touchÃ© de plein fouet par la rÃ©volution du web mobile et a dÃ» sâ€™adapter trÃ¨s rapidement.\n\n\n\nIl a expliquÃ© quelle stratÃ©gie il a adoptÃ©e pour tuer ou refaire les vieux systÃ¨mes et comment, en premier lieu, il a vendu le projet Ã  ses supÃ©rieurs.\nIl a tout dâ€™abord prÃ©sentÃ© le TCO de ce quâ€™il a appelÃ© la version â€classicâ€ de ft.com (la carotte) puis a appuyÃ© sur la peur de lâ€™incident et les problÃ¨mes de sÃ©curitÃ© (le bÃ¢ton ; le journal ayant Ã©tÃ© la cible des pirates syriens).\n\nAprÃ¨s une analyse fine du traffic il a ensuite appliquÃ© ces stratÃ©gies :\n\n  tuer directement une application inutile (il y en avait), quitte Ã  la rallumer si quelquâ€™un finalement en Ã  lâ€™usage :) (et couper un serveur Solaris avec 1833 jours dâ€™uptime !)\n  reÃ©crire lâ€™application et la redÃ©ployer sur le nouveau systÃ¨me\n  tuer une application et Ã©crire plusieurs autres (dÃ©coupage en micro services)\n\n\nSon crÃ©do Ã©tait â€try to make the right thing easier.â€ Ainsi les projets basÃ©s sur la nouvelle stack disposait out of the box de fonctionnalitÃ©s de monitoring et de log. Cela a beaucoup motivÃ© les Ã©quipes de dÃ©veloppements.\n\nAu final la purge du legacy a apportÃ© :\n\n\n  un gain de 30% de performance\n  un meilleur TTM\n  de substantiels retours sur investissement\n\n\nSlides : IT Janitor - How to Tidy Up\n\n\n\nMansplaining 101: Cisadmin Edition - Marni Cohen (Puppet Labs)\n\nLa confÃ©rence la plus geek de la journÃ©e. La confÃ©renciÃ¨re a ouvert un terminal et a tapÃ©\n\nbrew install feminism\n\n\n\n\nLa confÃ©rence Ã©tait trÃ¨s sincÃ¨re et didactique sur comment mieux intÃ©grer les femmes dans lâ€™IT.\n\nVoici les scripts et les ressources quâ€™elle a prÃ©sentÃ©s : https://gitlab.com/marni/mansplaining\n\n\n\nBuilding the FirefoxOS Homescreen - Kevin Grandon (Mozilla)\n\n\n\nSlides : Building the FirefoxOS Homescreen\n\nConfÃ©rence de prÃ©sentation de lâ€™OS pour smartphone de Firefox.\n\nLors de cette prÃ©sentation, Kevin Grandon IngÃ©nieur chez Mozilla nous a prÃ©sentÃ© le nouvel OS, et nous a initiÃ© Ã  la programmation sur ce dernier.\nCe nouvel OS est donc basÃ© sur des langages simples : HTML / CSS / Javascript.\n\nLe dÃ©veloppement est donc assez facile Ã  prendre en main, le dÃ©bugage aussi car on peux monitorer tout ce quâ€™il se passe sur le device de test via un firebug dÃ©diÃ©.\n\n\n\nDonâ€™t Kill Yourself : Mobile Web Performance Tricks that Arenâ€™t Worth it, and Somme that Are - Lyza Gardner (Cloud Four)\n\nOptimisations pour le web mobile.\n\nLyza Gardner nous a prÃ©sentÃ© sa vision de lâ€™optimisation sur web mobile. Elle nous a tout dâ€™abord fait un compte rendu sur son expÃ©rience personnelle. Liza a cherchÃ© via diffÃ©rentes analyses (speedIndexâ€¦) Ã  trouver une relation entre temps de chargement, nombres dâ€™assets etcâ€¦ Et la conclusion quâ€™elle mettait en avant, câ€™est quâ€™il nâ€™y avait pas de recette magique. \nElle a ensuite fait la parallÃ¨le entre le web lors de ces dÃ©buts qui Ã©tait limitÃ© par le dÃ©bit de nos connexions de lâ€™Ã©poque, et le web mobile tel quâ€™il est actuellement. Ainsi certaines optimisations de lâ€™Ã©poque sont adaptables, et mÃªme toujours valables, Ã  nos problÃ©matiques actuelles.\nSelon elle, il ne faut pas optimiser un site pour le mobile, mais lâ€™optimiser tout court. Elle propose de se fixer des objectifs, par exemple se fixer une limite de nombre dâ€™appel asset. Mais surtout dâ€™optimiser / limiter les images puisque 62% du trafic dâ€™un site correspond a ces derniÃ¨res.\n\n\n\nWhat are the Third-party Components Doing to Your Siteâ€™s Performance? - Andy Davies, Simon Hearne (NCC Group)\n\nSlides : Third-party components and site performance?\n\nNous utilisons tous des Â« Third-Party Â» sur nos sites, mais est-ce une bonne idÃ©e ?\n\nUn Third-Party est un script que nous chargeons depuis un autre site. Par exemple : Google Analitycs. Il existe diffÃ©rents type de Third-Party : la publicitÃ©, les analyseurs de trafic â€¦ La problÃ©matique est que nous ne pouvons pas controller ces outils. Nous nâ€™avons pas la main sur le temps de chargement, la disponibilitÃ© de lâ€™outils, et cela peut influer sur lâ€™expÃ©rience utilisateur et la qualitÃ© de nos services.\n\nPour conclure, il faut trouver le bon compromis entre ce que nous apporte le Third-Party et ce quâ€™il peut nous coÃ»ter â€¦\n\n\n\nGuide to Survive a World Wide Event - Almudena Vivanco, Mateus Bartz (TelefÃ³nica\n\nSlides : Survive a World Wide Event\n\nRetour dâ€™expÃ©rience de Movistar TV, une chaÃ®ne payante multi-support qui a diffusÃ© la coupe du monde en Espagne, au BrÃ©sil et en Argentine.\n\nCette sociÃ©tÃ© sâ€™est confrontÃ©e Ã  une problÃ©matique de traffic avec des pics de connexions importants en peu de temps. La sociÃ©tÃ© devait diffuser la coupe du monde FIFA 2014 dans plusieurs pays et sur plusieurs devices diffÃ©rents. AprÃ¨s des tests en condition rÃ©elles avant le dÃ©but de la compÃ©tition, ils se sont aperÃ§us quâ€™ils ne pouvaient pas gÃ©rer le pic de connexion qui arrivait entre 5 minutes avant le coup dâ€™envoi et 5 minutes aprÃ¨s, ainsi quâ€™Ã  la reprise du match et dÃ©but de deuxiÃ¨me mi-temps.\nIl a donc fallu tout refaire Ã  plusieurs niveaux :\n\n  CrÃ©ation dâ€™un CDN en interne\n  Refonte globale du systÃ¨me de connexion pour pouvoir supporter les pics.\n  Mise en place de monitoring via Graphite\n  Mise en place de Tests\n\n\nMise en avant de beaucoup de problÃ©matiques :\n\n  Multi plateforme\n  DÃ©ploiement sur plusieurs continents (AmÃ©rique du Sud, Europe)\n  Rassembler 11 outils de monitoring en un seul.\n\n\n\n\nIs TLS Fast yet ?\n\nSlides : Is TLS Fast yet ?\n\nTL;DR = Oui, il pourrait lâ€™Ãªtre !\n\nLe talent dâ€™Ilya pour les confÃ©rences techniques a une fois de plus fait ses preuves. \nTout en dÃ©taillant lâ€™utilitÃ© de TransportLayerSecurity (compression, vÃ©rification dâ€™erreurs, authentification, chiffrementâ€¦) Ilya nous prouve que dans le meilleur des cas, un RTT supplÃ©mentaire est nÃ©cessaire et lâ€™impact CPU trÃ¨s faible.\n\nOutre lâ€™utilisation des derniÃ¨res versions du Kernel, dâ€™OpenSSL et de votre OS serveur, la performance de TLS passe aussi par la rÃ©utilisation dâ€™Ã©lÃ©ments nÃ©gociÃ©s lors de la premiÃ¨re (et coÃ»teuse) poignÃ©e de main. Cette optimisation se fait cotÃ© serveur en conservant les â€œsessions identifiersâ€ cotÃ© serveur ou cotÃ© client avec un â€œcookieâ€ chiffrÃ©, le â€œsession ticketâ€. Il faudra bien entendu ajuster la durÃ©e de cache et/ou les timeouts (~ 1 jour).\n\nUne erreur frÃ©quemment commise consiste Ã  ne pas intÃ©grer le certificat intermÃ©diaire (peu de CA sâ€™autorise Ã  signer votre certificat avec leur CA Root) dans le certificat serveur ce qui a pour consÃ©quence de stopper le render, ouvrir une nouvelle connexion tcp et https pour rÃ©cupÃ©rer ce dernier chez lâ€™autoritÃ© de certification.\n\nLâ€™OSCP stappling permet lui dâ€™inclure directement la rÃ©ponse OCSP et ainsi Ã©viter le mÃªme problÃ¨me de blocage du rendu, connexion Ã  un tiers etcâ€¦\n\nLâ€™utilisation hasardeuse de redirection 301 peut considÃ©rablement augmenter le Time To First Byte de votre site, il est donc fortement conseillÃ© de bien analyser ses chaÃ®nes de redirections (ex: https://domain.com =&amp;gt; https://www.domain.com =&amp;gt; https://www.domain.com) et dâ€™utiliser HSTS. Ce header Ã©mis par le serveur permettra au navigateur de mettre en cache la dÃ©cision de redirection vers https.\n\nLe talk sâ€™est terminÃ© par un tableau comparatif fort intÃ©ressant des serveurs HTTP et des CDNs concernant tous ces aspects.\n\nQuelques liens supplÃ©mentaires:\n\n\n  https://www.ssllabs.com/ssltest/\n  https://www.feistyduck.com/books/bulletproof-ssl-and-tls/\n\n\n\n\nMonitoring: the math behind bad behavior\n\nSlides : the math behind bad behavior\n\nLa dÃ©tection dâ€™anomalies dans les flux continus de donnÃ©es de type Time Series nâ€™est pas une chose aisÃ©e.\n\nSe baser sur un percentile, une moyenne ou une mediane uniquement ne permet pas de capturer les phÃ©nomÃ¨nes de saisonnalitÃ© et dâ€™anomalies.\n\nThÃ©o nous a proposÃ© une mÃ©thode de dÃ©tection de ces derniÃ¨res appelÃ©e â€œlurching windowsâ€. Sur des fenÃªtres de temps glissantes, on applique la mÃ©thode CUSUM (Cumulative Sum), qui somme les donnÃ©es en affectant un poids relatif (en rÃ©alitÃ© la probabilitÃ© que cette valeur existe).\n\nA voir: https://en.wikipedia.org/wiki/CUSUM\n\n\n\nWhat ops can learn from design - Robert Treat (OmniTI)\n\nSlides : What ops can learn from design\n\nâ€œUn designer est quelquâ€™un qui designâ€.\n\nDerriÃ¨re cette lapalissade se cache en rÃ©alitÃ© plusieurs concepts importants Ã  intÃ©grer pour toutes personnes produisant un code, un service utilisÃ© par un tiers.\n\nNous sommes tous des designers. Il est donc indispensable de mettre en oeuvre 3 mÃ©canismes simples pour faciliter lâ€™utilisation de votre code/service.\n\nLe â€œFeedbackâ€: le bon code de retour lors de lâ€™Ã©chec dâ€™un script, un message intelligible et contextualisÃ© dans un log dâ€™erreur applicatif, le â€œnatural Mappingâ€: -d dans une option en ligne de commande pour indiquer â€“database, et le â€œforce functionsâ€: sous Unix, kill est par dÃ©faut non destructif, il faut forcer avec kill -9 pour tuer dÃ©finitivement un processus, tout comme on vous force Ã  fermer la porte de votre micro-onde pour le mettre en marche.\n\nConfÃ©rence intÃ©ressante qui vous fera sentir moins coupable de ne pas savoir si il fallait pousser ou tirer une porte :)\n\n\n\nStatistical Learning-based Automatic Anomaly Detection @Twitter\n\nArun Kejariwal est maintenant un habituÃ© de la Velocity, jâ€™avais particuliÃ¨rement apprÃ©ciÃ© sa prÃ©sentation lâ€™annÃ©e derniÃ¨re Ã  Londres sur la dÃ©tection dâ€™anomalies chez twitter.\n\nLâ€™ojectif est toujours le mÃªme: prÃ©dire la capacitÃ© pour ajouter du matÃ©riel en datacenter, dÃ©tecter des Ã©vÃ©nements particulier, distinguer le spam du trafic normal etcâ€¦\n\nLeur mÃ©thode est relativement identique Ã  ce qui avait Ã©tÃ© prÃ©sentÃ© lâ€™annÃ©e derniÃ¨re: sur deux semaines de donnÃ©es on applique un traitement du signal pour dÃ©composer et filtrer la saisonnalitÃ©. Il â€œsuffitâ€ ensuite dâ€™appliquer une regression ou un ESD sur les rÃ©sidus pour dÃ©tecter dâ€™Ã©ventuelles anomalies.\n\nChose Ã  savoir: Twitter va publier un package R contenant ces fonctions et algorithmes, qui seront donc utilisables par le commun des mortels !\n\nConclusion\n\nUne premiÃ¨re journÃ©e intÃ©ressante et intense, sous le soleil de Barcelone !\n\n\n\nLe rÃ©sumÃ© de la seconde journÃ©e est Ã©galement disponible.\n"
} ,
  
  {
    "title"    : "Configuration dynamique avec Symfony ExpressionLanguage",
    "category" : "",
    "tags"     : " configuration, symfony, cytron",
    "url"      : "/symfony-expression-language",
    "date"     : "November 17, 2014",
    "excerpt"  : "GrÃ¢ce Ã  notre bundle MonologExtra, nous avons la possibilitÃ© dâ€™inclure des informations statiques dans le contexte de nos logs.\nNous souhaiterions maintenant avoir aussi dâ€™autres informations plus dynamiques comme le nom de lâ€™utilisateur.\n\nPour ce...",
  "content"  : "GrÃ¢ce Ã  notre bundle MonologExtra, nous avons la possibilitÃ© dâ€™inclure des informations statiques dans le contexte de nos logs.\nNous souhaiterions maintenant avoir aussi dâ€™autres informations plus dynamiques comme le nom de lâ€™utilisateur.\n\nPour cela, nous avons donc ajoutÃ© la possibilitÃ© de configurer une expression qui sera Ã©valuÃ©e par le composant ExpressionLanguage de Symfony de cette maniÃ¨re :\n\nm6_web_monolog_extra:\n    processors:\n        userProcessor:\n            type: ContextInformation\n            config:\n                env: expr(container.getParameter(&#39;kernel.environment&#39;))\n                user: expr(container.get(&#39;security.context&#39;).getToken() ? container.get(&#39;security.context&#39;).getToken().getUser().getUsername() : &#39;anonymous&#39;)\n\nPour interprÃ©ter cette expression, nous avons injectÃ© dans notre processeur Monolog une instance de ExpressionLanguage ainsi que le container :\n\nservices:\n  m6_web_monolog_extra.expression_language:\n    class: Symfony\\Component\\ExpressionLanguage\\ExpressionLanguage\n    public: false\n  m6_web_monolog_extra.processor.contextInformation:\n    abstract: true\n    class: M6Web\\Bundle\\MonologExtraBundle\\Processor\\ContextInformationProcessor\n    arguments:\n      - @service_container\n      - @m6_web_monolog_extra.expression_language\n    calls:\n      - [ setConfiguration, []]\n\nNous utilisons une dÃ©finition de service abstraite qui sert de modÃ¨le pour les services qui sont gÃ©nÃ©rÃ©s Ã  partir de la configuration sÃ©mantique gÃ©rÃ©e par lâ€™extension du bundle :\n\n&amp;lt;?php\nforeach ($config[&#39;processors&#39;] as $name =&amp;gt; $processor) {\n    $serviceId = sprintf(&#39;%s.processor.%s&#39;, $alias, is_int($name) ? uniqid() : $name);\n\n    $definition = clone $container-&amp;gt;getDefinition(sprintf(&#39;%s.processor.%s&#39;, $alias, $processor[&#39;type&#39;]));\n    $definition-&amp;gt;setAbstract(false);\n\n    $tagOptions = [];\n    if (array_key_exists(&#39;channel&#39;, $processor)) {\n        $tagOptions[&#39;channel&#39;] = $processor[&#39;channel&#39;];\n    }\n    if (array_key_exists(&#39;handler&#39;, $processor)) {\n        $tagOptions[&#39;handler&#39;] = $processor[&#39;handler&#39;];\n    }\n    $definition-&amp;gt;addtag(&#39;monolog.processor&#39;, $tagOptions);\n\n    if (array_key_exists(&#39;config&#39;, $processor)) {\n        if ($definition-&amp;gt;hasMethodCall(&#39;setConfiguration&#39;)) {\n            $definition-&amp;gt;removeMethodCall(&#39;setConfiguration&#39;);\n            $definition-&amp;gt;addMethodCall(&#39;setConfiguration&#39;, [$processor[&#39;config&#39;]]);\n        } else {\n            throw new InvalidConfigurationException(sprintf(&#39;&quot;%s&quot; processor is not configurable.&#39;, $processor[&#39;type&#39;]));\n        }\n    }\n\n    $container-&amp;gt;setDefinition($serviceId, $definition);\n}\n\nEt lâ€™expression est finalement Ã©valuÃ©e par le processeur en utilisant le composant quand la valeur est de la forme expr(...), ceci permettant de garder une compatibilitÃ© ascendante avec les configurations statiques prÃ©cÃ©dentes.\n\n&amp;lt;?php \nprotected function evaluateValue($value)\n{\n    if (preg_match(&#39;/^expr\\((.*)\\)$/&#39;, $value, $matches)) {\n        return $this-&amp;gt;expressionLanguage-&amp;gt;evaluate($matches[1], [&#39;container&#39; =&amp;gt; $this-&amp;gt;container]);\n    }\n    return $value;\n}\n\nAvec la configuration prÃ©sentÃ©e au dÃ©but, nous rÃ©cupÃ©rons ainsi lâ€™environnement et lâ€™utilisateur connectÃ© dans le contexte de nos logs.\n\nMonologExtraBundle est disponible en open-source sur le compte GitHub de M6Web.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #7",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2014/11/13/m6web-dev-facts-7.html",
    "date"     : "November 13, 2014",
    "excerpt"  : "Ã‡a faisait un moment ! Voici le retour des devfacts !\n\nReproduction\n\n  Je ne sais pas si Ã§a corrige le bug quâ€™on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nAu moins, câ€™est efficace\n\n  Quand je fais un â€œecho $idâ€, Ã§a affich...",
  "content"  : "Ã‡a faisait un moment ! Voici le retour des devfacts !\n\nReproduction\n\n  Je ne sais pas si Ã§a corrige le bug quâ€™on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nAu moins, câ€™est efficace\n\n  Quand je fais un â€œecho $idâ€, Ã§a affiche lâ€™id\n\n\nProprement sale\n\n  Câ€™est pas forcÃ©ment plus propre, mais câ€™est moins sale\n\n\nA une vache prÃ¨s !\n\n  Câ€™est Ã  peu prÃ¨s approximatif â€¦\n\n\nIl faut savoir ce quâ€™on veut\n\n  Câ€™est pas prÃ©vu pour Ãªtre utile\n\n\nCâ€™est louche.\n\n  CommenÃ§ons par comprendre pourquoi le code fonctionne\n\n\nToi aussi fais du marketing â€¦\n\n  On pourrait leverager le ROI du big data avec de lâ€™analytics predictif.\n\n\nPour une fois â€¦.\n\n  Pour une fois câ€™est pas un bug ! câ€™est un truc qui marche.\n\n\nPooh\n\n  Au sujet dâ€™une sombre histoire dâ€™expression de besoin \nâ€œJe ne peux pas toujours les aider Ã  faire leurs besoinsâ€¦â€\n\n\nComprends moi !\n\n  â€œComprends mon incomprÃ©hension !â€\n\n\nROI !\n\n  Y a autant dâ€™utilisateurs que de jour homme pour ce projet !\n\n\n"
} ,
  
  {
    "title"    : "Retour sur le forum PHP 2014 organisÃ© par l&#39;AFUP",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2014/11/12/retour-sur-le-forumphp2014.html",
    "date"     : "November 12, 2014",
    "excerpt"  : "M6Web Ã©tait prÃ©sent en force avec 5 collaborateurs prÃ©sent Ã  lâ€™Ã©vÃ¨nement. Voici un retour des confÃ©rences qui nous ont le plus marquÃ©es.\n\nVers des applications â€œ12 factorâ€ avec Symfony et Docker\n\nCette session avait pour objectif de nous prÃ©senter...",
  "content"  : "M6Web Ã©tait prÃ©sent en force avec 5 collaborateurs prÃ©sent Ã  lâ€™Ã©vÃ¨nement. Voici un retour des confÃ©rences qui nous ont le plus marquÃ©es.\n\nVers des applications â€œ12 factorâ€ avec Symfony et Docker\n\nCette session avait pour objectif de nous prÃ©senter la mÃ©thodologie du â€œtwelve-factor appâ€, Ã  travers des exemples concrets pour PHP Ã  lâ€™aide de Symfony et Docker.\n\nâ€œThe twelve-factor appâ€ est une suite de recommandations, indÃ©pendante dâ€™un langage de programmation particulier et pouvant sâ€™appliquer Ã  toutes sortes de logiciels dÃ©veloppÃ©s en tant que service.\n\nSans revenir sur lâ€™ensemble de la prÃ©sentation, voici un retour sur les 12 facteurs :\n\n\n  Codebase : une app = un repo (ou Ã©quivalent) servant de source Ã  tous les dÃ©ploiements (dev / preprod / recette / prod  etc.). Exemples : git, mercurial etc.\n  Dependencies : dÃ©claration explicite et complÃ¨te de lâ€™arbre de dÃ©pendances, utilisÃ© uniformÃ©ment pour tous les environnements. Exemples : composer, npm etc.\n  Config : sÃ©paration stricte config/code (Resources, Backing services, Credentials, Hostname etc.). Exemples : parameters.yml pour Symfony 2 ou utilisation de variables dâ€™environnement avec Docker notamment. Utilisation de fig pour lâ€™orchestration des containers docker.\n  Backing Services : tous les services utilisÃ©s par lâ€™application sont accessibles par le rÃ©seau. Il nâ€™y a pas de distinction entre les ressources locales et distantes car toutes sont accessibles via URL et/ou Credentials. Exemples : MySQL, RabbitMQ, Postfix, Redis, S3 etc.\n  Build, release, run : sÃ©paration stricte entre\n    \n      â€œbuild stageâ€ : tÃ©lÃ©chargement dâ€™une version du code et des dÃ©pendances. Exemples : â€œdocker buildâ€\n      â€œrelease stageâ€œ : utilise le â€œbuildâ€ et le combine avec la configuration du dÃ©ploiement (une version sur un environnement). Exemple : â€œdocker pushâ€, utilisation de capistranoâ€¦\n      â€œrun stageâ€ : lancement de la â€œreleaseâ€ sur lâ€™environnement cible. Exemple â€œdocker runâ€ ou â€œfig runâ€\n    \n  \n  Processes : chaque composant de lâ€™application est â€˜sans Ã©tatâ€™ et ne doit pas partager directement des donnÃ©es. Tout doit Ãªtre partagÃ© en â€œbacking serviceâ€.\n  Port binding : les services doivent Ãªtre disponibles en mettant Ã  disposition un port dâ€™accÃ¨s, directement accessible. Cela permet une utilisation aisÃ©e en environnement de dev mais Ã©galement de rÃ©utiliser les services.\n  Concurrency : une application respectant les â€œ12 factorâ€ est facilement scalable, quel que soit son type (web, worker, etc.) car elle repose sur des composants systÃ¨mes pour son pilotage.\n  Disposability : robustesse par le lancement et lâ€™arrÃªt rapide des services, pour rendre chacune de ses services scalables.\n  Dev/Pro parity : homogÃ©nÃ©itÃ© des environnements dev/prod et gain de temps pour la prise en main dâ€™un projet. (mais le dÃ©veloppeur nâ€™aura pas une vision prÃ©cise de la configurationâ€¦ boite noire ?)\n  Logs : traitement des logs en tant que flux, utilisÃ©s par des services. Exemples : ELK, StatsD/Grafana etc.\n  Admin process : ExÃ©cuter les tÃ¢ches de maintenance sur les mÃªmes environnements/containers. Exemples : docker exec\n\n\nslides\n\nPersonnellement, jâ€™ai trouvÃ© cette confÃ©rence vraiment riche et instructive. Peut-Ãªtre un peu plus dâ€™exemples de configuration fig/docker aurait pu illustrer dâ€™avantage.\n\nLa mesure ce nâ€™est pas que pour le devops\n\nLes confÃ©renciers ont commencÃ© leur prÃ©sentation sur un rappel de ce quâ€™est le Lean Startup, hÃ©ritier de la mÃ©thode Lean mise au point par Toyota. Nous connaissions la dÃ©marche Lean mais pas du tout son approche spÃ©cifique au lancement dâ€™un produit.\n\nLe concept pourrait se rÃ©sumer Ã  : la base du lean startup est de savoir Ã©couter -ses utilisateurs- car le succÃ¨s dÃ©pend dâ€™un feedback mesurable.\n\nLe processus dâ€™application est trÃ¨s simple : un cycle construit/mesure/apprend.\n\nSâ€™en est logiquement suivi une Ã©numÃ©ration des maniÃ¨res de mettre en oeuvre le processus en sachant prendre en compte les mesures qui importent (AAA, AARRR), plutÃ´t que des â€œmesures de vanitÃ©â€ (followers, nombre de visite,..).\n\nEnfin pour appliquer ces mesures, une prÃ©sentation des outils a disposition a Ã©tÃ© faite.\n\n\n\nPHP dans les distributions RPM\n\nSlides\n\nCette session avait comme objectif de faire un Ã©tat de PHP dans les distributions RPM RHEL/Centos/Fedora.\n\nRHEL / Centos :\n\n\n  Objectif de stabilitÃ© Ã  10 ans\n  StabilitÃ© binaire et de configuration sur la durÃ©e de vie de la distribution\n  RHEL : version payante avec support (contacts avec les ingÃ©nieurs RedHat, ressources en ligne, cycles de mises Ã  jour garantis etc.).\n  Centos : mÃªme code que RHEL (juste recompilÃ©) mais uniquement un support communautaire (comme fedora, ubuntu, suseâ€¦).\n  RHEL 5 : PHP 5.1 / RHEL 6 : PHP 5.3 / RHEL 7 : PHP 5.4\n  Application des patchs de sÃ©curitÃ©s sur les versions anciennes de PHP pendant 10 ans.\n  PossibilitÃ© dâ€™utiliser des repos tiers pour choisir une version plus rÃ©cente spÃ©cifique (comme ceux de Remi Collet) - mais pas de support officiel.\n  Distributions plutÃ´t destinÃ©es Ã  des applicatifs maintenus sur le long terme.\n\n\nFedora 21+ :\n\n\n  3 sous distributions : Workstation / Server / Cloud\n  DerniÃ¨re version de PHP (PHP5.5 pour f20 et PHP5.6 pour f21)\n  IntÃ©gration continue de PHP dans les cycles de FÃ©dora. Permet dâ€™Ã©viter les rÃ©gressions.\n\n\nA venir : Software Collections (scl) permet dâ€™avoir TOUTES les versions de PHP souhaitÃ©es simultanÃ©ment sur la mÃªme installation de Fedora. Vraiment prometteur !\n\nExemple dâ€™utilisation des SCL en cli :\n\nscl enable php56 -f myscript56.php\nscl enable php56 bash\nscl enable php53 -f myscript53only.php\nscl enable php53 bash\n\nDans une config apache :\n\n&amp;lt;VirtualHost *:80&amp;gt;\n    ServerName php56scl\n    \n    # Redirect to FPM server in php56 SCL\n    &amp;lt;FilesMatch \\.php$&amp;gt;\n    SetHandler &quot;proxy:fcgi://127.0.0.1:9006&quot;\n    &amp;lt;/FilesMatch&amp;gt;\n&amp;lt;/VirtualHost&amp;gt;\n\nFrameworks: A History of Violence\n\n\n\nFrancois Zaninotto nous a offert un vrai show en se mettant dans la peau dâ€™un homme politique candidat Ã  la prÃ©sidence du parti des dÃ©veloppeurs. Avec beaucoup dâ€™humour il a fait un retour sur lâ€™Ã©volution (sa propre Ã©volution ?) du dÃ©veloppement web et son futur hypothÃ©tique, tout en distillant (son programme) de prÃ©cieux conseils pour Ãªtre un meilleur dÃ©veloppeur.\n\nSon programme :\n\n\n  Le domaine dâ€™abord : lier son dÃ©veloppement mÃ©tier Ã  un minimum de tierce partie (pas facile Ã  faire !),\n  Dites non au full-stack (Ã§a se discute !),\n  Lâ€™application plurielle : ne pas hÃ©siter Ã  mÃ©langer diffÃ©rents langages et diffÃ©rents projets dialoguant via http sur une mÃªme application,\n  Repenser le temps : passons aux 32h pour nous permettre de faire de la veille.\n\n\nA la communautÃ© PHP nous pourrions proposer une synthÃ¨se (entendu ailleurs) : â€œsoyons plus des dÃ©veloppeurs web que des dÃ©veloppeurs PHP, soyons plus des dÃ©veloppeurs que des dÃ©veloppeurs webâ€.\n\n\n\nRetour dâ€™expÃ©rience ARTE GEIE : dÃ©veloppement dâ€™API\n\nUne confÃ©rence donnÃ©e par un de nos confrÃ¨res dâ€™ARTE sur des problÃ©matiques trÃ¨s actuelles pour nous. FranÃ§ois Dume a expliquÃ© la stratÃ©gie de mise en place dâ€™une API autour de JSON API et des microservices. Lâ€™utilisation de OpenResty et du langage Lua couplÃ© Ã  un serveur oAuth en Symfony2 gÃ©rant la validation des tokens et le throttling.\n\nIl a ensuite expliquÃ© en dÃ©tail lâ€™implÃ©mentation de {json:api} dans Symfony2, en mettant en avant de nombreuses contributions open-source.\n\n\n\nUne confÃ©rence didactique et claire.\n\n\n\nVDM, DevOps malgrÃ© moi\n\nMaxime Valette nous expliquÃ© comment il a (Ã  20 ans Ã  peine) crÃ©e un business incroyable sur Internet et a surtout rÃ©ussi Ã  gÃ©rer une augmentation de 30 Ã  40K visiteurs de plus chaque jour avec pratiquement juste sa b* et son c*.\n\n- Comment on fait ? \n- Comme on peut ! \n\n\nDe vrai qualitÃ© dâ€™orateur pour Maxime et une conf trÃ¨s rafraichissante. Une dÃ©monstration de lean startup par lâ€™exemple. MÃªme si ce choix nâ€™a pas Ã©tÃ© discutÃ©, PHP Ã©tait un choix naturel pour lui Ã  lâ€™Ã©poque.\n\n\n\nAn introduction to the Laravel Framework for PHP.\n\nBien que Symfony soit trÃ¨s largement majoritaire en Europe, Laravel est trÃ¨s populaire en AmÃ©rique du Nord, câ€™est donc avec curiositÃ© que nous avons assistÃ© Ã  cette prÃ©sentation du framework faite par Dayle Rees, core developer.\n\nUne fois passÃ© la trÃ¨s longue prÃ©sentation des livres et autres activitÃ©s du confÃ©rencier, nous avons eu droit Ã  une prÃ©sentation gÃ©nÃ©rale du framework qui nous as fortement rappelÃ© Symfony1 : utilisation de singleton Ã  outrance, MagicBox (Ã©quivalent du sfContext), beaucoup de magie pour rÃ©duire la configuration (nom des contrÃ´leurs).\n\nAu final, lâ€™impression laissÃ©e est mitigÃ©e : certes, le seuil dâ€™entrÃ©e est relativement rÃ©duit, tout est simple dâ€™apparence, mais câ€™Ã©tait la mÃªme chose pour Symfony1, et lâ€™expÃ©rience nous a montrÃ© que lorsque lâ€™on essayait de sortir le framework des sentiers battus, cette simplicitÃ© devenait un vrai obstacle.\n\nAu final, Laravel est sÃ»rement une alternative intÃ©ressante pour les nostalgiques de Symfony1, puisque le projet est actif et maintenu. Mais, pour les projets que nous dÃ©veloppons, Symfony2 reste une solution tout Ã  fait adaptÃ©e.\n\nLaisse pas trainer ton log !\n\nOlivier Dolbeau nous a fait un retour sur la problÃ©matique dâ€™accÃ¨s et lâ€™interprÃ©tation des logs sur les serveurs de production.\n\n\n\nIl nous as donc prÃ©sentÃ© la solution quâ€™il utilise, Ã  savoir la stack ELK, pour ElasticSearch/LogStash/Kibana, qui permet Ã  chaque serveur dâ€™envoyer ses logs vers un serveur central, qui a pour charge de les agrÃ©ger, et de permettre leur utilisation avancÃ©e.\nFini la recherche dans des fichiers textes plats quâ€™il faut commencer par comprendre, dÃ©sormais vos applicatifs peuvent enrichir leurs logs, les envoyer sur un systÃ¨me dÃ©diÃ© Ã  la gestion des logs disposant de vraies interfaces de recherche et de consultation.\n\nNous avons Ã©tÃ© confortÃ©s dans notre idÃ©e, puisque nous mettons Ã©galement en oeuvre cette solution.\n\nTable ronde â€œEtat des lieux et avenir de PHPâ€\n\nPascal Martin a animÃ© dâ€™une main de maÃ®tre une table ronde sur lâ€™avenir de PHP. Avec Jordi Boggiano, lead developer de Composer, Pierre Joye, core dev de PHP, Julien Pauli, release manager de PHP 5.5 et co-RM de PHP 5.6.\n\nLa communautÃ© se pose beaucoup de questions sur le devenir de lâ€™engine PHP et comment va Ã©voluer le langage. \nLes dÃ©bats ont Ã©tÃ© intenses et les invitÃ©s ont pu rÃ©pondre Ã  des questions posÃ©es via Twitter. Au final peu de conclusions dÃ©finitives. On peut dÃ©duire que malgrÃ© les alternatives proposÃ©es par HHVM et HippyVM, la communautÃ© reste majoritairement sur PHP et est toujours trÃ¨s friande dâ€™Ã©volutions du langage et de sa performance. Les invitÃ©s de la table ronde ont exhortÃ©s les participants Ã  contribuer au code de PHP en nous fournissant pas mal de conseils.\n\n\n\nSlideshow KaraokÃ©\n\nUne honte ! En plus les slides nâ€™avaient aucun sens ! :) Bravo Ã  Mc Kenny pour lâ€™animation.\n\n\n\n\n\nUn grand merci Ã  lâ€™AFUP pour ce joli Ã©vÃ¨nement ! Retrouvez pas mal de ressources partagÃ©s pendant lâ€™event sur eventifier.\n"
} ,
  
  {
    "title"    : "Github Team Reviewer pour gagner la course aux Pull Requests",
    "category" : "",
    "tags"     : " outil, github, pull-requests, cytron, open-source",
    "url"      : "/github-team-reviewer-pull-requests.html",
    "date"     : "November 7, 2014",
    "excerpt"  : "Les PR, câ€™est le bien\n\nChez M6Web, nous utilisons Github Enterprise en interne pour nos projets privÃ©s et Github pour nos projets open-source. GrÃ¢ce Ã  ces outils, nous avons adoptÃ© de maniÃ¨re systÃ©matique lâ€™usage des Pull Requests pour faire relir...",
  "content"  : "Les PR, câ€™est le bien\n\nChez M6Web, nous utilisons Github Enterprise en interne pour nos projets privÃ©s et Github pour nos projets open-source. GrÃ¢ce Ã  ces outils, nous avons adoptÃ© de maniÃ¨re systÃ©matique lâ€™usage des Pull Requests pour faire relire et valider notre code par nos collaborateurs. La qualitÃ© de nos dÃ©veloppements a ainsi Ã©tÃ© grandement amÃ©liorÃ©e au fil du temps.\n\nOui, maisâ€¦\n\nNous utilisons aussi HipChat pour communiquer au quotidien et rapidemment au sein de nos Ã©quipes. Chaque crÃ©ation de Pull Request Ã©met une notification sur HipChat. Cependant, le nombre de pull requests initiÃ©es augmente avec le temps et chacun tend Ã  ignorer peu Ã  peu les notifications ou y fait moins attention. Les Pull Requests sâ€™accumulent sur certains projets et nous nâ€™avions, jusquâ€™Ã  prÃ©sent, pas vraiment de moyen pour lister par Ã©quipe toutes les PR en cours. Cela nous permettrait dâ€™avoir une vue globale et dâ€™Ãªtre plus rÃ©actifs et rigoureux.\n\nIl y a bien le nouveau Pull Requests Dashboard de Github avec ses filtres de recherche avancÃ©e qui permet de rÃ©pertorier toutes ses PR ou celles dâ€™une organisation. Cette mise Ã  jour nâ€™est pas encore entrÃ©e en application dans Github Enterprise. Mais surtout, nous avons plusieurs Ã©quipes au sein dâ€™une mÃªme organisation et nous voulons pouvoir les gÃ©rer de maniÃ¨re indÃ©pendante : cette fonctionnalitÃ© ne rÃ©soud pas notre problÃ©matique.\n\nGTR !\n\nNous avons donc dÃ©veloppÃ© Github Team Reviewer, un outil ultra simple mais efficace qui permet en un coup dâ€™Å“il de voir toutes les PR de ses Ã©quipes et leur statut, quâ€™elles soient sur un Github Entreprise interne ou sur Github. Le projet utilise AngularJS et lâ€™API fournit par Github. Lâ€™installation se fait sur nâ€™importe quel serveur web et requiert npm (via Node.js) pour builder lâ€™application grÃ¢ce Ã  Bower et Gulp.js.\n\nLâ€™application propose volontairement un nombre limitÃ© de paramÃ¨tres de configuration Ã©ditables dans le fichier config/config.json:\n\n\n  lâ€™intervalle de rafraichissement de la liste des PR,\n  la liste des Ã©quipes en dÃ©finissant pour chacune :\n    \n      son nom,\n      les utilisateurs Github concernÃ©s,\n      les organisations Github concernÃ©es,\n      lâ€™url de lâ€™API Ã  interroger (pour Github Enterprise, par dÃ©faut lâ€™url de lâ€™API public de Github est utilisÃ©e),\n      un token utilisateur (utile pour augmenter le rate limit de lâ€™API public).\n    \n  \n\n\nUne select box permet de basculer dâ€™une Ã©quipe Ã  une autre trÃ¨s facilement.\n\nGithub Team Reviewer est disponible en open-source sur le compte Github de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Providers AngularJS et configuration dynamique",
    "category" : "",
    "tags"     : " configuration, angular, cytron",
    "url"      : "/surcharger-un-provider-angular",
    "date"     : "October 9, 2014",
    "excerpt"  : "Nous avons eu besoin de surcharger un provider AngularJS â€“ AnalyticsProvider â€“ pour le rendre configurable dynamiquement en fonction dâ€™un paramÃ¨tre de la route. Le service $route nâ€™Ã©tant pas disponible dans la phase de configuration dâ€™AngularJS, i...",
  "content"  : "Nous avons eu besoin de surcharger un provider AngularJS â€“ AnalyticsProvider â€“ pour le rendre configurable dynamiquement en fonction dâ€™un paramÃ¨tre de la route. Le service $route nâ€™Ã©tant pas disponible dans la phase de configuration dâ€™AngularJS, il a fallu ruserâ€¦\n\nLe but est donc de changer la mÃ©thode $get de ce provider afin de lui ajouter notre dÃ©pendance et ainsi finir notre configuration.\n\nIl existe bien une mÃ©thode decorator() dans le service dâ€™injection de dependance dâ€™AngularJS, mais celle-ci ne permet que de dÃ©corer des services et pas leurs providers.\n\nNous allons donc mettre les mains dans lâ€™$injector pour rÃ©cupÃ©rer et modifier Ã  la volÃ©e le provider :\n\nangular.module(&#39;myModule&#39;)\n  .config(function ($injector) {\n    var AnalyticsProvider = $injector.get(&#39;AnalyticsProvider&#39;);\n    var $get              = AnalyticsProvider.$get;\n    // ...\n  });\n\nMaintenant que nous avons le $get, il faut le modifier pour ajouter notre dÃ©pendance. Et câ€™est assez simple vu quâ€™il utilise lâ€™annotation sous forme de tableau :\n\n// https://github.com/revolunet/angular-google-analytics/blob/e821407fe0436677cb42eafd5b338d767990b723/src/angular-google-analytics.js#L99\nthis.$get = [&#39;$document&#39;, &#39;$rootScope&#39;, &#39;$location&#39;, &#39;$window&#39;, function($document, $rootScope, $location, $window) {\n\nNous devons modifier ce tableau en ajoutant nos dÃ©pendances et en remplaÃ§ant la fonction :\n\n// la fonction d&#39;origine est le dernier Ã©lÃ©ment du tableau\nvar origFn = $get[$get.length - 1];\n// on la remplace par notre dÃ©pendance\n$get[$get.length - 1] = &#39;$route&#39;;\n// on ajoute notre nouvelle fonction Ã  la fin du tableau\n$get[$get.length] = function () {\n    // $route est le dernier argument\n    var $route = arguments[arguments.length - 1];\n    // on fait notre traitement\n    AnalyticsProvider.setAccount($route.current.params.partner ? &#39;partner-account&#39; : &#39;own-account&#39;);\n    // et qui rappelle la fonction originale\n    return origFn.apply(AnalyticsProvider, arguments);\n};\n\nOn peut noter lâ€™utilisation de lâ€™objet arguments qui permet de rester gÃ©nÃ©rique et de garder la compatibilitÃ© en cas de changement des dÃ©pendances du module surchargÃ©.\n\nGrÃ¢ce Ã  cette astuce, notre service Analytics est maintenant configurÃ© dynamiquement selon nos souhaits avant son utilisation.\n"
} ,
  
  {
    "title"    : "AmÃ©liorer la webperf de son application JS avec GruntJs",
    "category" : "",
    "tags"     : " webperf, angular, grunt, performance",
    "url"      : "/2014/09/30/ameliorer-la-webperf-de-son-application-js-avec-gruntjs.html",
    "date"     : "September 30, 2014",
    "excerpt"  : "Lâ€™un des principaux problÃ¨mes que nous rencontrons sur nos dÃ©veloppement chez M6Web est la tenue en charge.\nQuand elles sont liÃ©es Ã  des sites Ã  fort trafic ou Ã  une Ã©mission tÃ©lÃ© (#effetcapital), nos applications doivent Ãªtre conÃ§ues pour support...",
  "content"  : "Lâ€™un des principaux problÃ¨mes que nous rencontrons sur nos dÃ©veloppement chez M6Web est la tenue en charge.\nQuand elles sont liÃ©es Ã  des sites Ã  fort trafic ou Ã  une Ã©mission tÃ©lÃ© (#effetcapital), nos applications doivent Ãªtre conÃ§ues pour supporter des pics de charge plus ou moins importants.\n\nCâ€™est une problÃ©matique quâ€™on croit souvent liÃ©e uniquement aux backends (scripts serveurs, bases de donnÃ©es etc), en oubliant souvent que le front-end est aussi, voir tout autant concernÃ©.\n\nCâ€™est notamment le cas pour une â€œSingle Page Applicationâ€ Angular.Js que nous dÃ©veloppons en ce moment.\n\nLâ€™objectif ici est dâ€™avoir une application qui exÃ©cutera le moins de requÃªtes possible pour sâ€™afficher, et qui ensuite sera quasiment autonome en ne faisant que le minimum de requÃªtes HTTP. Ceci afin de garantir, que lorsque quelquâ€™un charge lâ€™application, lâ€™expÃ©rience est quasi parfaite, mÃªme si entre temps, le CDN ou lâ€™hÃ©bergement connaÃ®t une surcharge temporaire.\n\nLâ€™autre avantage de diminuer le nombre dâ€™appels HTTP, câ€™est aussi de limiter lâ€™impact de la latence rÃ©seau, encore plus imposante dans notre cas, car notre cible est majoritairement mobile.\n\nPour les applications â€œClient-Sideâ€, nous utilisons Grunt.Js pour automatiser toutes les tÃ¢ches de dÃ©veloppement, build, dÃ©ploiement â€¦ (Nul doute que la mÃªme chose existe avec Gulp pour les plus Hipsters dâ€™entre vous). Grunt regorge de plugins en tout genre pour automatiser Ã©normÃ©ment de choses cotÃ© WebPerf, commenÃ§ons par le plus Ã©vident et le plus simple.\n\nP.S : Je passe volontairement lâ€™installation/initialisation de Grunt ainsi que de ses plugins. Le web regorgeant de ressources lÃ  dessus.\n\nMinification HTML\n\nAfin de gagner quelques octets, nous allons minifier (suppression des espaces, retours charriot, et commentaires HTML) notre code HTML gÃ©nÃ©rÃ©.\nPour ceci, nous utilisons le plugin grunt-contrib-htmlmin.\n\noptions: {\n        collapseWhitespace: true,\n        collapseBooleanAttributes: true,\n        removeCommentsFromCDATA: true,\n        removeOptionalTags: true,\n        removeComments: true\n      }\n\nMinification CSS\n\nMÃªme chose au niveau des feuilles de styles avec grunt-contrib-cssmin.\n\nCompression des images\n\nAfin dâ€™Ã©viter dâ€™avoir des images Â« brutes Â» de taille trop importante, on utilise grunt-contrib-imagemin pour compresser au build nos diffÃ©rentes images, afin de gagner quelques ko toujours prÃ©cieux.\n\nInlining des images dâ€™interface\n\nDans notre cas, oÃ¹ nous souhaitons rÃ©duire le nombre de requÃªtes HTTP superflues, nous avons optÃ© pour lâ€™inlining des images dites dâ€™interface (boutons dâ€™actions, picto etc).\n\nNous utilisons aussi le prÃ©-compilateur CSS Less, par simplicitÃ© et pour Ã©viter le DRY CSS.\nNous avons donc un premier fichier .less qui va contenir toutes les images dâ€™interface sous cette forme :\n@facelessImg: url(&#39;images/faceless.jpgâ€™);\n\nLe plugin Grunt grunt-css-url-embed sera configurÃ© pour remplacer les urls prÃ©sentes dans ce fichier par la version data-uri (=source de lâ€™image encodÃ©e en base64).\nIl est important de se concentrer uniquement sur les images Â« dâ€™interface Â», car le poids des images sera ici augmentÃ© dâ€™environ 30% (Ã  cause du base64).\n\nDans notre CSS principale, on pourra ensuite mettre cette image en background dâ€™une classe CSS :\n\n.faceless {\n  background-image: @facelessImg;\n}\n\nEt dans notre code HTML, on pourra placer lâ€™image de la maniÃ¨re suivante :\n&amp;lt;span class=&quot;faceless&quot;&amp;gt;&amp;lt;/span&amp;gt;\n\nGrÃ¢ce Ã  cet ajout, nous Ã©conomiserons une requÃªte HTTP pour chacune des images.\n\nVersionning des assets\n\nUne autre bonne pratique est de versionner les assets en production. Cela signifie, donner un nom unique Ã  chaque fichier statique (JS, CSS, image), ne changeant pas, tant que le fichier en question nâ€™aura pas subi de modification, dans le but de pouvoir mettre un cache navigateur (Expire) et un cache CDN/Proxy Cache le plus long possible (Cache-control).\nNous passerons de /images/info.jpg Ã  /images/a21992d7.info.jpg par exemple.\n\nNous utilisons ici le plugin grunt-rev (en combinaison avec grunt-usemin), qui va dâ€™abord versionner les assets ayant changÃ©s, et ensuite, mettre Ã  jour les rÃ©fÃ©rences vers les fichiers en question dans tous vos fichiers HTML, CSS, JS.\n\nConcatÃ©nation des fichiers JS\n\nDirectement dans le code HTML, toujours avec le plugin grunt-usemin, vous allez pouvoir mettre des commentaires HTML pour dÃ©finir quels ensembles de fichiers devront Ãªtre concatÃ©nÃ©s.\nLa bonne pratique est dâ€™avoir un fichier app.js avec son code maison, un fichier vendor.js avec les librairies tierces, et potentiellement un fichier de config.js\nEtant donnÃ© que dans notre cas, 99% du poids Js est concentrÃ© dans â€œVendorâ€, nous avons dÃ©cidÃ© de concatÃ©ner lâ€™ensemble dans un seul fichier.\n\n&amp;lt;!-- build:js(.tmp) scripts/risingstar.js --&amp;gt;\n  &amp;lt;script src=&quot;bower_components/jquery/dist/jquery.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;bower_components/angular/angular.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;config.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;app.js&quot;&amp;gt;&amp;lt;/script&amp;gt; \nâ€¦.\n&amp;lt;!-- endbuild â€”&amp;gt;\n\nInlining des templates\n\nPour finir, vous aurez peut-Ãªtre remarquÃ©, si vous dÃ©veloppez des SPA avec Angular, ou un autre framework moderne, un changement de route (ou dâ€™Ã©tat) de votre application (ou lâ€™affichage dâ€™une directive) va impliquer des appels XHR pour charger les nouveaux templates Ã  afficher. La bonne pratique ici Ã©tant de dÃ©couper au maximum tous les templates dans des fichiers distincts.\nCela ne pose pas de problÃ¨me en temps normal, mais dans notre cas, cela ne respecte pas nos ambitions de dÃ©part.\n\nAngular a la particularitÃ© de permettre dâ€™utiliser la balise script pour charger des templates :\n\n&amp;lt;script type=&quot;text/ng-template&quot; id=&quot;views/info.html&quot;&amp;gt;Code HTML du template&amp;lt;/script&amp;gt;\n\nSi votre routeur ou une directive demande un template, avant de vÃ©rifier si le fichier existe, Angular vÃ©rifiera si une balise &amp;lt;script type=â€™text/ng-templateâ€™&amp;gt; a Ã©tÃ© dÃ©clarÃ©e avec lâ€™identifiant correspondant au chemin demandÃ©.\n\nGrunt via le plugin grunt-angular-inline-templates, nous permet dâ€™automatiser cette tÃ¢che au build, afin de regrouper dans le index.html du build, tous les templates dans un script avec lâ€™id correspondant au chemin du fichier html original. De cette maniÃ¨re, nous nâ€™avons plus aucun appel HTTP Ã  faire pendant toute lâ€™utilisation de lâ€™application.\nAttention toutefois, cela signifie que le poids du fichier HTML original va forcÃ©ment augmenter.\n\nConclusion\n\nComme vous avez pu le voir, nous avons grandement optimisÃ© notre application, en utilisant simplement des plugins Grunt Ã  notre disposition. Nous travaillons donc sur un espace de dÃ©veloppement respectant toutes les bonnes pratiques (dÃ©coupages des fichiers JS, CSS, HTML au maximum, code commentÃ© â€¦) et toutes les opÃ©rations dâ€™optimisation sont automatiquement effectuÃ©es au build, fait avant chaque dÃ©ploiement.\n\nAttention, cela signifie aussi que votre projet en production devient relativement diffÃ©rent de celui que vous testÃ© en dÃ©veloppement. Il devient donc important de mettre en place des tests fonctionnels sur le build de production (avec Protractor par exemple, ou mÃªme Behat), et de tester rÃ©guliÃ¨rement la bonne gÃ©nÃ©ration et le bon fonctionnement du build de prod.\n"
} ,
  
  {
    "title"    : "Tests E2E sur son application AngularJS avec Protractor",
    "category" : "",
    "tags"     : " qualite, tests, javascript, angular, protractor, cytron",
    "url"      : "/tests-e2e-application-angularjs-protractor.html",
    "date"     : "September 24, 2014",
    "excerpt"  : "Familier des tests fonctionnels avec Behat et Atoum pour des applications majoritairement PHP, nous lâ€™Ã©tions beaucoup moins avec les tests end-to-end pour des applications pures Javascript, qui plus est, sous AngularJS. Les tests end-to-end ou tes...",
  "content"  : "Familier des tests fonctionnels avec Behat et Atoum pour des applications majoritairement PHP, nous lâ€™Ã©tions beaucoup moins avec les tests end-to-end pour des applications pures Javascript, qui plus est, sous AngularJS. Les tests end-to-end ou tests e2e ne sont autres que des tests fonctionnels dans la domaine du Javascript. Lâ€™objectif de cet article est de montrer le cheminement que nous avons empruntÃ© pour mettre en place ces tests sur une de nos applications et pour gÃ©rer les difficultÃ©s qui en ont dÃ©coulÃ©.\n\nLe contexte\n\nIl sâ€™agit dâ€™une application web prÃ©sentant des Ã©crans diffÃ©rents Ã  lâ€™utilisateur en fonction des donnÃ©es contenues dans un fichier distant requÃªtÃ© Ã  intervalle rÃ©gulier court (quelques secondes). Lâ€™utilisateur est invitÃ© ou non Ã  agir avec les vues, principalement en appuyant sur des boutons, qui changent lâ€™Ã©tat interne de lâ€™application et peut, a posteriori, influer sur les Ã©crans suivants.\n\nMettre en place Protractor\n\nLa premiÃ¨re Ã©tape consiste Ã  installer Protractor, framework de tests e2e dÃ©diÃ© Ã  AngularJS et utilisant Node.js. Si vous utilisez Grunt pour gÃ©rer les tÃ¢ches de build de votre projet, il suffit dâ€™exÃ©cuter la commande :\n\nnpm install grunt-protractor-runner --save-dev\n\nPuis on crÃ©e le fichier de configuration dans le projet :\n\n/* protractor-local.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;\n};\n\nTous les tests e2e de notre application sont Ã©crits dans des fichiers javascript dont le nom est suffixÃ© par .e2e.js. Nous avons en effet fait le choix dâ€™une architecture modulaire se retrouvant dans lâ€™organisation des dossiers de notre projet : les fichiers de tests e2e se trouvent dans les mÃªmes rÃ©pertoires que les controllers auxquels ils sont rattachÃ©s 1.\n\nUn navigateur pour mes tests\n\nPour exÃ©cuter ses tests dans les conditions rÃ©elles de son application, il faut un navigateur. Nous dÃ©veloppons sur un serveur distant en SSH. Le seul navigateur utilisable est donc un browser headless, le plus connu et utilisÃ© Ã©tant PhantomJS. Cependant, combinÃ© Ã  Protractor, ce dernier est particuliÃ¨rement instable pour le moment et il nâ€™est pas recommandÃ© de lâ€™utiliser. Nous optons donc pour Chrome (via le plugin chromedriver). NÃ©cessitant une interface graphique, nous ne pourrons donc pas lancer nos tests sur le serveur de dÃ©veloppement mais nous devrons le faire en local sur nos machines.\n\n/* protractor-local.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;,\n  maxSessions: 1,\n  multiCapabilities: [\n    { browserName: &#39;chrome&#39; }\n  ]\n};\n\nOn installe les binaires nÃ©cessaires au lancement de Chrome via Protractor :\n\n./node_modules/grunt-protractor-runner/node_modules/.bin/webdriver-manager update\n\nPuis on ajoute les tÃ¢ches Grunt :\n\n/* Gruntfile.js */\ngrunt.initConfig({\n  connect: {\n    dist: {\n      options: {\n        port: 9000,\n        hostname: &#39;localhost&#39;,\n        base: &#39;dist&#39;\n      }\n    }\n  },\n  protractor: {\n    local: {\n      options: {\n        configFile: &quot;protractor-local.conf.js&quot;\n      }\n    }\n  }\n});\n\ngrunt.registerTask(&#39;test&#39;, [\n  &#39;build&#39;,\n  &#39;connect:dist&#39;,\n  &#39;protractor:local&#39;\n]);\n\nIntÃ©gration continue\n\nLâ€™ensemble de nos projets joue automatiquement leurs tests sur un serveur Jenkins commun qui ne dispose pas de navigateurs graphiques. Nous aurions pu mettre en place au sein de notre infrastructure un serveur Selenium pour rÃ©pondre Ã  cette problÃ¨matique. Mais les contraintes du projet ne nous autorisaient pas Ã  y consacrer le temps nÃ©cessaire. Nous avons donc optÃ© pour une solution tiers plus rapide Ã  mettre en Å“uvre : SauceLabs, plateforme de tests hÃ©bergÃ©e dans le â€œcloudâ€.\n\nUne fois enregistrÃ© sur le site, on crÃ©e un nouveau fichier de configuration Protractor :\n\n/* protractor-saucelabs.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;,\n  allScriptsTimeout: 30000,\n  jasmineNodeOpts: {\n    defaultTimeoutInterval: 60000\n  },\n  maxSessions: 1,\n  sauceUser: &#39;mySauceUser&#39;,\n  sauceKey: &#39;mySauceKey&#39;,\n  multiCapabilities: [\n    {\n      browserName: &#39;chrome&#39;,\n      platform: &#39;Linux&#39;\n    },\n    {\n      browserName: &#39;firefox&#39;,\n      platform: &#39;Linux&#39;\n    },\n    {\n      browserName: &#39;safari&#39;,\n      platform: &#39;OS X 10.9&#39;\n    },\n    {\n      browserName: &#39;chrome&#39;,\n      platform: &#39;Windows 8.1&#39;\n    }\n  ]\n};\n\nNotons que lâ€™on peut lancer ses tests sur autant de couples OS/navigateurs que lâ€™on souhaite en remplissant le tableau multiCapabilities. Le fichier de configuartion Grunt doit Ãªtre adaptÃ© pour lancer SauceConnect, lâ€™interface entre SauceLabs et lâ€™application, avant le dÃ©marrage des tests :\n\n/* Gruntfile.js */\ngrunt.initConfig({\n  connect: {\n    dist: {\n      options: {\n        port: 9000,\n        hostname: &#39;localhost&#39;,\n        base: &#39;dist&#39;\n      }\n    }\n  },\n  protractor: {\n    local: {\n      options: {\n        configFile: &#39;protractor-local.conf.js&#39;\n      }\n    },\n    saucelabs: {\n      options: {\n        configFile: &#39;protractor-saucelabs.conf.js&#39;\n      }\n    }\n  },\n  run: {\n    installsc: {\n      options: {\n        wait: true\n      },\n      cmd: &#39;bash&#39;,\n      args: [\n        &#39;-c&#39;,\n        &#39;test -d sc-4.2-linux || (wget https://saucelabs.com/downloads/sc-4.2-linux.tar.gz &amp;amp;&amp;amp; tar xvf sc-4.2-linux.tar.gz)&#39;\n      ]\n    },\n    sauceconnect: {\n      options: {\n        wait: false,\n        quiet: true,\n        ready: /Sauce Connect is up/\n      },\n      cmd: &#39;./sc-4.2-linux/bin/sc&#39;,\n      args: [\n        &#39;-u&#39;,\n        &#39;mySauceUser&#39;,\n        &#39;-k&#39;,\n        &#39;mySauceKey&#39;\n      ]\n    }\n  }\n});\n  \n\ngrunt.registerTask(&#39;test-e2e&#39;, function (target) {\n  var tasks = [\n    &#39;build&#39;,\n    &#39;connect:dist&#39;\n  ];\n\n  if (target === &#39;local&#39;) {\n    tasks.push(&#39;protractor:local&#39;);\n  } else {\n    tasks.push(&#39;run:installsc&#39;);\n    tasks.push(&#39;run:sauceconnect&#39;);\n    tasks.push(&#39;protractor:saucelabs&#39;);\n    tasks.push(&#39;stop:sauceconnect&#39;);\n  }\n\n  grunt.task.run(tasks);\n});\n\nAvec cette configuration, nous lanÃ§ons les tests en local sur notre machine avec la commande grunt test-e2e:local ou Ã  distance sur SauceLabs avec grunt test-e2e.\n\nNotre premier test\n\nLe premier test que nous avons Ã©crit pour valider lâ€™architecture est plutÃ´t basique :\n\ndescribe(&#39;Controller: MainCtrl&#39;, function () {\n  it(&#39;should work&#39;, function () {\n    browser.get(browser.baseUrl);\n    expect(true).toBe(true);\n  })\n});\n\nOn remarque que lâ€™Ã©criture dâ€™un test e2e utilise, comme les tests unitaires, la syntaxe du framework Jasmine : un bloc describe regroupe une suite de tests dÃ©finis dans des blocs it. Les variables de configuration dÃ©finies dans les fichiers de configuration Protractor sont utilisables via la variable globale browser, variable qui nous permettra dâ€™entretenir le lien entre nos tests et le code exÃ©cutÃ© dans le navigateur. Pour mieux apprÃ©hender les Ã©tapes du processus et les erreurs qui se produisent, il est en effet trÃ¨s important de bien comprendre la sÃ©paration entre le code Javascript exÃ©cutÃ© dans Node.js via Protractor, qui correspond au dÃ©roulement des tests, et le code Javascript de notre application qui lui est exÃ©cutÃ© dans le browser et avec lequel on ne peut interagir depuis les tests que par certaines fonctions du framework (element, executeScript, addMockModule, etc.)2. Ce sont deux univers dâ€™exÃ©cution bien distincts.\n\nDÃ©bugger avec Protractor\n\nLorsque vous lancerez les tests en local, vous remarquerez que Chrome est rÃ©ellement exÃ©cutÃ© mais vous ne verrez pas grand chose car lâ€™affichage est bien trop rapide. Il est possible de mettre des points dâ€™arrÃªt dans ses tests pour y voir plus clair et pour, par exemple, consulter la console Javascript du navigateur. Pour cela, il faut utiliser la fonction browser.debugger() comme point dâ€™arrÃªt et ajouter lâ€™option debug dans la configuration Grunt :\n\n/* Gruntfile.js */\nprotractor: {\n  local: {\n    options: {\n      configFile: &#39;protractor-local.conf.js&#39;,\n      debug: true\n    }\n  }\n}\n\nPour passer dâ€™un point dâ€™arrÃªt Ã  lâ€™autre, on saisit c comme continue. Notez que cela ne fonctionnera pas si vous avez plus dâ€™un navigateur dans le tableau multiCapabilities de votre configuration.\n\nOn peut Ã©galement ajouter lâ€™option --debug Ã  la commande grunt test-e2e:local pour afficher lâ€™ensemble des requÃªtes lancÃ©es par lâ€™application.\n\nMocker sa config\n\nComme souvent dans les projets AngularJS, nous utilisons un module pour dÃ©finir nos variables de configuration :\n\nangular.module(&quot;config&quot;, [])\n  .constant(&quot;config&quot;, {\n    &#39;ma_variable&#39;: &#39;une_valeur&#39;\n  });\n\nDans les tests e2e, on veut tout tester, en particulier les comportements qui diffÃ¨rent en fonction des valeurs de configuration. Comment faire puisque ce module est chargÃ© une fois pour toute au lancement de lâ€™application ? Protractor introduit la fonction addMockModule qui permet de bouchonner Ã  la volÃ©e un module Angular.\n\nit(&#39;comportement avec une autre valeur&#39;, function () {\n  browser.addMockModule(&#39;config&#39;, function () {\n  \tangular.module(&#39;config&#39;, []).constant(&#39;config&#39;, {\n    \t\t&#39;ma_variable&#39;: &#39;une_autre_valeur&#39;\n  \t});\n  });\n\n  // mon test\n  \n  browser.removeMockModule(&#39;config&#39;);\n});\n\nMocker le service $http\n\nDans notre application, un fichier externe est requÃªtÃ© rÃ©guliÃ¨rement via le service Angular $http. AngularJS fournit dÃ©jÃ  un mock complet de ce service nommÃ© $httpBackend. Pour y avoir accÃ¨s, il faut ajouter la dÃ©pendance angular-mocks en devDependencies dans son fichier bower.json et inclure le fichier bower_components/angular-mocks/angular-mocks.js dans lâ€™application en dÃ©veloppement. $httpBackend permet de dÃ©finir quels appels HTTP doivent Ãªtre interceptÃ©s et quelles rÃ©ponses doivent Ãªtre renvoyÃ©es.\n\nLa difficultÃ© dans notre cas rÃ©side dans le fait de pouvoir simuler le changement dâ€™Ã©tat du fichier distant dans un mÃªme test pour pouvoir vÃ©rifier les changements de vue qui en dÃ©coulent. Il est possible de le faire directement via $httpBackend moyennant quelques acrobaties, mais la librairie HttpBackend simplifie grandement son utilisation pour ce type de tests 3.\n\nvar HttpBackend = require(&#39;httpbackend&#39;);  \nvar backend;\n\ndescribe(&#39;Test workflow&#39;, function() {  \n  beforeEach(function() {\n    backend = new HttpBackend(browser);\n  });\n\n  afterEach(function() {\n    backend.clear();\n  });\n\n  it(&#39;should display result when status is changed to RESULT&#39;, function(done) {\n    backend.whenJSONP(/status.json/).respond({status: &#39;initial&#39;});\n\n    browser.get(&#39;/&#39;);\n\n    var result = element(by.binding(&#39;result&#39;));\n    expect(result.getText()).toEqual(&#39;no result&#39;);\n    \n    backend.whenJSONP(/status.json/).respond({status: &#39;result&#39;, percentage: 70});\n    \n    browser.wait(function () {\n      return browser.getLocationAbsUrl().then(function (currentUrl) {\n        return currentUrl === &#39;https://localhost:9000/#/result&#39;;\n      });\n    }, 5000).then(function () {\n      expect(result.getText()).toEqual(&#39;70 %&#39;);\n      done();\n    });\n  });\n});\n\nOui, maisâ€¦\nProtractor nous a Ã©tÃ© indispensable pour implÃ©menter les tests fonctionnels sur notre application car son intÃ©gration avec AngularJS offre des possibilitÃ©s que les autres frameworks de tests fonctionnels nâ€™ont pas. On pense principalement Ã  la synchronisation qui est mise en Å“uvre entre les tests et lâ€™initialisation dâ€™Angular dans la page (â€œwait for angularâ€). Cependant, avec le recul que lâ€™on peut avoir sur notre projet :\n\n\n  il faut lâ€™avouer, Protractor nâ€™est pas aussi simple Ã  mettre en place que Behat par exemple,\n  le debuggage est assez pÃ©nible car les messages dâ€™erreur sont souvent peu verbeux et, câ€™est lâ€™inconvÃ©nÃ©nient de tester du javascript avec du javascript, on ne sait pas toujours oÃ¹ se situe lâ€™erreur (dans les tests ou dans le code applicatif ?),\n  Protractor est parfois instable avec les webdrivers utilisÃ©s, ce qui nous oblige Ã  relancer les tests manuellement,\n  nos tests dans SauceLabs sont (trÃ¨s) lents, ce qui nous a contraint Ã  la longue Ã  rÃ©duire le nombre de navigateurs testÃ©s (amÃ©liorant par la mÃªme occasion la stabilitÃ© des tests).\n\n\n\n  \n    \n      Scalable code organization in AngularJSÂ &amp;#8617;\n    \n    \n      Protractor APIÂ &amp;#8617;\n    \n    \n      Angular e2e tests, Mock your backend.Â &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "ContrÃ´lez facilement votre cohÃ©rence de code sur votre projet Symfony2 avec coke",
    "category" : "",
    "tags"     : " code sniffing, coke, Symfony2",
    "url"      : "/2014/08/05/verifier-la-coherence-du-code-d-un-projet-symfony2-avec-coke.html",
    "date"     : "August 5, 2014",
    "excerpt"  : "Pour quâ€™un projet persiste dans le temps, il est important que le style de codage soit le mÃªme. Et quand vous vous reposez sur des outils, autant faire en sorte que le style de codage retenu soit proche, si ce nâ€™est le mÃªme, que les briques que vo...",
  "content"  : "Pour quâ€™un projet persiste dans le temps, il est important que le style de codage soit le mÃªme. Et quand vous vous reposez sur des outils, autant faire en sorte que le style de codage retenu soit proche, si ce nâ€™est le mÃªme, que les briques que vous utilisez. Et dans le cas oÃ¹ vous utilisez un framework, câ€™est dâ€™autant plus important.\n\nAvec Symfony2, câ€™est dâ€™autant plus facile que lâ€™architecture des bundles est trÃ¨s marquÃ©e, et quâ€™un coding guide est publiÃ©.\n\nÃ‡a, câ€™est pour la thÃ©orie, mais en pratique, si ce nâ€™est pas super simple, automatique, une somme de toutes petites erreurs apparaissent et le sentiment dâ€™abandon sâ€™installe rapidement.\n\nCoke\n\nIl y a un peu plus dâ€™un an, chez M6Web, nous avons dÃ©veloppÃ© coke pour configurer simplement lâ€™exÃ©cution de PHP_CodeSniffer.\n\nDepuis quelques mois, il est possible dâ€™installer coke via Composer :\n\n{\n  &quot;require&quot;: {\n    &quot;m6web/coke&quot;: &quot;~1.2&quot;\n  }\n}\n\nLâ€™avantage de passer par Composer, câ€™est que coke va lui-mÃªme installer PHP_CodeSniffer en tant que dÃ©pendance Composer (dans le dossier vendor), permettant de ne pas avoir Ã  suivre la fastidieuse procÃ©dure dâ€™installation via PEAR.\n\nInstaller un coding standard via Composer\n\nLorsque nous voulons utiliser un coding standard qui nâ€™est pas inclus par dÃ©faut avec PHP_CodeSniffer, il est possible de lâ€™installer en utilisant Composer\n\nSymfony2-coding-standard\n\nChez M6Web, nous maintenons le standard Symfony2-coding-standard qui permet de valider que le code dâ€™un projet respecte les coding standard de Symfony2.\n\nPour rendre Ã  CÃ©sar ce qui appartient Ã  CÃ©sar, nous avons rÃ©cupÃ©rÃ© la base du standard telle que crÃ©Ã© par opensky.\n\nSi nous avons dÃ©cidÃ© de le forker, câ€™est que la structure ne correspondait pas Ã  ce qui est nÃ©cessaire pour une installation de ce standard via Composer\n\nProcÃ©dure complÃ¨te, pas-Ã -pas\n\nCrÃ©er le fichier composer.json suivant :\n\n{\n  &quot;require-dev&quot;: {\n    &quot;m6web/coke&quot;                       : &quot;~1.2&quot;,\n    &quot;m6web/symfony2-coding-standard&quot;   : &quot;~1.1&quot;,\n  }\n}\n\nInstaller les dÃ©pendances Composer :\n\ncomposer install\n\nCrÃ©er le fichier .coke suivant :\n\n# Standard used by PHP CodeSniffer (required)\nstandard=vendor/m6web/symfony2-coding-standard/Symfony2\n\nIl est dÃ©sormais possible dâ€™appeler la commande suivante pour valider le style de codage de votre projet\n\n./vendor/bin/coke\n\nConclusion\n\nAvec cette technique, il est trÃ¨s simple de valider le style de codage dâ€™un projet. Du coup, plus dâ€™excuse pour ne pas le faire ;)\n\nBonus\n\nLâ€™idÃ©al, pour ne jamais commiter un code ne respectant pas les conventions de codage, est dâ€™utiliser les hooks de commit pour que cette vÃ©rification soit faite automatiquement.\n\nLa maniÃ¨re la plus simple de le faire est dâ€™ajouter la ligne ./vendor/bin/coke dans le fichier .git/hooks/pre-commit, mais cette mÃ©thode a le dÃ©faut de vÃ©rifier tout le projet, et pas uniquement le code modifiÃ© et Ã  commiter.\n\nPour aller plus loin, vous pouvez vous inspirer du script suivant qui ne lance coke que sur les fichiers dans le â€œstagingâ€ de Git (les fichiers Ã  commiter).\n\n"
} ,
  
  {
    "title"    : "M6Web Ã©tait prÃ©sent au PHPTour Lyon 2014",
    "category" : "",
    "tags"     : " afup, phptour, conference, video",
    "url"      : "/2014/06/25/m6web-etait-au-phptour-lyon-2014.html",
    "date"     : "June 25, 2014",
    "excerpt"  : "Le Lundi 23 et Mardi 24 juin a eu lieu lâ€™Ã©vÃ©nement PHP de lâ€™annÃ©e Ã  Lyon : le PHPTour Lyon.\nÃ€ cette occasion, les Ã©quipes dâ€™M6Web ont prÃ©sentÃ© un talk, dont voici les slides et vidÃ©os :\n\n#Nouveau socle pour une nouvelle vie, chez M6Web (par Kenny ...",
  "content"  : "Le Lundi 23 et Mardi 24 juin a eu lieu lâ€™Ã©vÃ©nement PHP de lâ€™annÃ©e Ã  Lyon : le PHPTour Lyon.\nÃ€ cette occasion, les Ã©quipes dâ€™M6Web ont prÃ©sentÃ© un talk, dont voici les slides et vidÃ©os :\n\n#Nouveau socle pour une nouvelle vie, chez M6Web (par Kenny Dits)\n\nLa seconde confÃ©rence de @techM6Web a Ã©tÃ© tenue par Kenny Dits (@kenny_dee) : â€œNouveau socle pour une nouvelle vie, chez M6Webâ€.\n\n\n\n\n\nVoir les commentaires sur Joind.in\n\nNous avons aussi retrouvÃ© une bonne partie des dÃ©veloppeurs de lâ€™Ã©quipe (anciens ou actuels) qui ont jouÃ© le jeu de la borne photo Pixiway mise Ã  disposition :\n\n\n\n#Conclusion\n\nLâ€™Afup a encore rÃ©alisÃ© un boulot considÃ©rable cette annÃ©e, pour accoucher sans aucun doute, du meilleur PHP Tour jamais fait.\nBravo Ã  toute la team pour lâ€™organisation sans faille, et aux autres speakers pour la qualitÃ© de leurs talks.\n"
} ,
  
  {
    "title"    : "M6Web sera prÃ©sent au PHPTour Lyon 2014",
    "category" : "",
    "tags"     : " afup, phptour",
    "url"      : "/2014/05/15/m6web-sera-present-au-phptour-lyon-2014.html",
    "date"     : "May 15, 2014",
    "excerpt"  : "M6Web sera bien reprÃ©sentÃ© au PHPTour 2014 organisÃ© par lâ€™AFUP et est trÃ¨s heureux de soutenir lâ€™Ã©vÃ¨nement en Ã©tant sponsor Argent.\n\n\n\nVenez nombreux augmenter votre pilositÃ© faciale, tel un vrai sysadmin.\n\nFaites le plein dâ€™anecdotes croustillant...",
  "content"  : "M6Web sera bien reprÃ©sentÃ© au PHPTour 2014 organisÃ© par lâ€™AFUP et est trÃ¨s heureux de soutenir lâ€™Ã©vÃ¨nement en Ã©tant sponsor Argent.\n\n\n\nVenez nombreux augmenter votre pilositÃ© faciale, tel un vrai sysadmin.\n\nFaites le plein dâ€™anecdotes croustillantes et dÃ©couvrez lâ€™histoire de M6Web Lyon avec Kenny Dits.\n"
} ,
  
  {
    "title"    : "Babitch, the story behind our table soccer web application",
    "category" : "",
    "tags"     : " opensource, babyfoot, angularjs, d3js, symfony",
    "url"      : "/2014/04/23/babitch-the-story-behind-our-table-soccer-web-application.html",
    "date"     : "April 23, 2014",
    "excerpt"  : "At M6Web, we love playing foosball!\nWe have one old (incredibly strong) soccer table in our Â« fun room Â», and at lunch time, a part of us enjoy playing it.\n\nThe soccer table in enterprise is awesome for a lot of things:\n\n\n  Team building between e...",
  "content"  : "At M6Web, we love playing foosball!\nWe have one old (incredibly strong) soccer table in our Â« fun room Â», and at lunch time, a part of us enjoy playing it.\n\nThe soccer table in enterprise is awesome for a lot of things:\n\n\n  Team building between each players,\n  Donâ€™t think about work (almost) when we are playing,\n  Fun! a lot of!\n  Attract good people â€¦\n\n\nOur rules are simple : Doubles (4 players) only, and the first team at ten win.\n\nSo, as programmers, we tend to have software ideas for each questions in our life ! and the questions we had with foosball were:\n\n\n  Who is the best player?\n  How many goals did I score ?\n  Who won the most games?\n  â€¦\n\n\nSo, we begin to talk several months ago about a foosball app, allowing us to record each games, and each goals, to compute lot of stats about our games.\nEveryone had good ideas about it, but someone had many more than us. Even more that it ruined all motivation of the other folks wanting to do work on this webapp, because the first steps to begin the app with all the features we had in mind was too big for all of us â€¦\n\nSo before having started the project, it was over !\n\nFew months later, an undercover part of the team began the development of a more simple and stupid foosball application : it just allowed to select 4 players, and to register matches by telling who scored and what kind of goal it was (normal or own goal).\nThis was ugly as possible, but it worked ! And it was an awesome start for improving it and giving back all the motivation developpers had lost before !\n\nBabitch was born :)\n\nArchitecture\n\nAt the beggining, there was only one project, with the server API, and the client part.\nThis was bad. It was a good way to start fast, but a bad way to allow each project to evolve on its own side.\nSo we decided to divide the Babitch Project into two parts, Babitch, the server API, and BabitchClient, a client to consume Babitch Api data.\n\nBabitch, the API\n\nThe Babitch API is a simple PHP/MySQL project, based on Symfony2, Doctrine, FosRestBundle, and NelmioApiDocBundle.\nThe documentation generated by the NelmioApiDocBundle is available at /api/doc and allows to view each Route of the API, and to send requests on them with the Sandbox menu.\nThe API is functionnaly tested by Behat.\nA Vagrant file is available if you want to try it easily. (More information in the Readme.md)\n\nBabitch, the Client\n\nThe Babitch Client, is the â€œofficialâ€ client for the API.\nThe client side doesnâ€™t require any webserver, itâ€™s just an Angular.js app, doing REST queries to the Babitch API thanks to Restangular.\n\nWe used Yeoman to bootstrap the project because it helps in many ways:\n\n\n  adds grunt configuration and support for serving, building and testing the project,\n  have generators for controllers, service, etc â€¦\n\n\nFor development on this client, we are heavily using Grunt, Karma for Unit Testing, and the new Protactor for E2E testing.\n\nThe client is divided into four major parts:\n\nNew Game\n\n\n\nThis is the main feature, it allows to begin a new game, choose 4 players, and assign each goals to the right players.\nThe game is saved only when the last goal is made.\nEach player is represented by his Gravatar for a nicer UI :)\n\nLive\n\n\n\nThe table soccer is not at the same floor than we are, so we are using our monitoring screen to show at lunch time the live state of the table score !\n\nWith a screen on this feature, we could see:\n\n\n  if a game is played right now,\n  whoâ€™s playing,\n  the live score,\n  for each goal, in live, who scored, and on which side :)\n\n\nThe live part use a Faye server, which you can host freely on Heroku (more information on Readme.md). You configure a channel name, and all actions done on the new game view are forwarded to the Faye server, forwarded back to the client listening on the Live view. It just rocks !\n\nStats\n\n\n\nAll of this would be useless if you donâ€™t have any way to compare your â€¦ stats to others competitors, right ?\nSo stats section is here for that purpose.\nIt shows you :\n\n\n  the last played match,\n  a sortable table by each stat of each player,\n  data visualization on each type of stats,\n  an individual card by player,\n  a sortable table by each stat of each team.\n\n\nAnd for each player and team, you have access to a lot of stats:\n\n\n  Elo Ranking (According to the Bonzini Usa Player Ranking/Rating System),\n  Percentage of goals per ball played,\n  Percentage of victory/loose per game,\n  Number of games played,\n  Team Goalaverage,\n  â€¦\n\n\nThere was some long debate about how stats have to be computed : on the server side ? (not really the goal of a REST Api â€¦), or on the client side ?\nAfter some successfull tries, we decided to compute stats on the client side, in an Angular.Js Service.\nThe service loads the last 300 games, and computes team and player stats fastly.\nWe also use the awesome D3.Js framework for data visualization.\n\nAdmin\n\nProbably the first screen you will need, for a simple way to add, modify, or delete players.\n\nConclusion\n\nWorking on our free (or lunch) time on a side-project like this is awesome!\nIt allows us to use several technologies or tools we donâ€™t use often, to improve our knowledge on tons of other things, to view the project on the product owner side and to mix teams who donâ€™t work a lot together.\n\nOne other thing interesting to remember about that project: Keep things as simple and small as possible (according to KISS principles) ! And only when your simple project is done, iterate by adding more and more features.\n\nTry and contribute?\n\nSo, if like us, you love foosball and play at work, give it a try, and give us feedback if you use it :)\n\nAlso, itâ€™s open-source, so youâ€™re welcome to contribute on BabitchClient and BabitchService, by posting/reading/commenting issue and PR.\n\nThanks ! :)\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un Lead Developpeur / Architecte web (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/2014/04/11/m6web-lyon-recherche-un-lead-developpeur-architecte-web-h-f-en-cdi.html",
    "date"     : "April 11, 2014",
    "excerpt"  : "\n\nM6Web Lyon recrute, en CDI, un Lead DÃ©veloppeur LAMP, avec une trÃ¨s forte expertise sur les technologies PHP 5.5, MySQL, Symfony2, GIT, et capable dâ€™encadrer une petite Ã©quipe de dÃ©veloppement.\n\nNous recherchons quelquâ€™un de trÃ¨s passionnÃ©, enth...",
  "content"  : "\n\nM6Web Lyon recrute, en CDI, un Lead DÃ©veloppeur LAMP, avec une trÃ¨s forte expertise sur les technologies PHP 5.5, MySQL, Symfony2, GIT, et capable dâ€™encadrer une petite Ã©quipe de dÃ©veloppement.\n\nNous recherchons quelquâ€™un de trÃ¨s passionnÃ©, enthousiaste, et mordu de veille technologique : un missionnaire de lâ€™open source, un intÃ©griste de la qualitÃ© de code, des tests unitaires et fonctionnels, et un architecte de projets aguerri avec une premiÃ¨re approche en mÃ©thodologie de dÃ©veloppement agile, et une expÃ©rience de management de dÃ©veloppeurs.\n\nSi, en plus, vous Ãªtes un malade de lâ€™optimisation back-end et front-end, que des technologies comme Node.js vous Ã©moustillent, que, malgrÃ© la qualitÃ© de MySQL, vous envisagez dans certains cas des solutions NoSQL alternatives (Mongo, Redisâ€¦), votre profil nous intÃ©resse !\n\nVenez apporter vos compÃ©tences aux Ã©quipes techniques de M6Web en travaillant sur des sites Ã  trÃ¨s forte charge (6Play, m6.fr, clubic.com, jeuxvideo.fr â€¦), et partagez-les grÃ¢ce Ã  des confÃ©rences internes ou externes et des articles sur notre blog.\n\nSi vous avez les qualitÃ©s requises et lâ€™envie de nous rejoindre, allez sur le lien ci-dessous et faites nous part de votre CV, de votre compte github, et dâ€™une lettre attrayante pour nous motiver Ã  vous rencontrer.\n\nSi vous souhaitez postuler ou avoir plus dâ€™infos : https://www.groupem6.fr/ressources-humaines/offres-emploi/lead-developpeur-architecte-web-h-f-229879.html\n"
} ,
  
  {
    "title"    : "ConfÃ©rence au Symfony Live 2014 : Symfony Ã  la tÃ©lÃ©",
    "category" : "",
    "tags"     : " symfony, conference",
    "url"      : "/2014/04/10/SfLive2014-symfony-a-la-tele.html",
    "date"     : "April 10, 2014",
    "excerpt"  : "InvitÃ© par SensioLabs au Symfony Live 2014, jâ€™ai pu prÃ©senter le travail des Ã©quipes de M6Web, et de nos partenaires, autour de Symfony 2.\n\nVoici les slides de la confÃ©rence :\n\n \n\nLâ€™enregistrement audio (avec les slides) est disponible ici :\n\n\n\nJe...",
  "content"  : "InvitÃ© par SensioLabs au Symfony Live 2014, jâ€™ai pu prÃ©senter le travail des Ã©quipes de M6Web, et de nos partenaires, autour de Symfony 2.\n\nVoici les slides de la confÃ©rence :\n\n \n\nLâ€™enregistrement audio (avec les slides) est disponible ici :\n\n\n\nJe tiens Ã  remercier toutes les personnes avec qui jâ€™ai pu Ã©changer autour des thÃ©matiques de la confÃ©rence. Jâ€™ai eu beaucoup de plaisir Ã  mâ€™apercevoir que de nombreux confÃ©renciers citaient le travail de M6Web pendant leur talk !\n\nRendez vous au phptour Ã  Lyon pour les prochaines confÃ©rences techniques M6Web.\n"
} ,
  
  {
    "title"    : "Utilisation du StatsdBundle avec le composant Console",
    "category" : "",
    "tags"     : " statsd, php, symfony, console, monitoring, cytron",
    "url"      : "/2014/03/04/utilisation-du-statsdbundle-avec-la-console.html",
    "date"     : "March 4, 2014",
    "excerpt"  : "Le StatsdBundle\n\nChez M6Web, nous utilisons StatsD et nous avons crÃ©Ã© un bundle pour cela.\nCe bundle permet dâ€™ajouter facilement des incrÃ©ments et des timings dans StatsD sur des Ã©vÃ©nements Symfony2.\n\nDe la Request Ã  la console\n\nOr pour des raison...",
  "content"  : "Le StatsdBundle\n\nChez M6Web, nous utilisons StatsD et nous avons crÃ©Ã© un bundle pour cela.\nCe bundle permet dâ€™ajouter facilement des incrÃ©ments et des timings dans StatsD sur des Ã©vÃ©nements Symfony2.\n\nDe la Request Ã  la console\n\nOr pour des raisons de performances, lors des Ã©vÃ©nements Symfony, les incrÃ©ments et timings sont seulement stockÃ©s dans une variable et ne sont envoyÃ©s rÃ©ellement Ã  StatsD que pendant le kernel.terminate qui se dÃ©roule aprÃ¨s lâ€™envoi de la rÃ©ponse HTTP au client.\nCeci pose un problÃ¨me pour les Ã©vÃ©nements lancÃ©s depuis une commande Symfony puisque en console, il nâ€™y pas de Request et donc pas de kernel.terminate.\nNous avons envisagÃ© dâ€™utiliser lâ€™Ã©vÃ©nement console.terminate pour palier Ã  cela, mais cela pose deux problÃ¨mes :\n\n\n  pour une commande qui est censÃ©e tourner indÃ©finiment (par exemple un consumer), on ne veut pas attendre la fin de la commande pour envoyer les donnÃ©es,\n  dans le cas dâ€™une exception pendant la commande, lâ€™Ã©vÃ©nement console.terminate est lancÃ© avant console.exception.\n\n\nLa premiÃ¨re solution Ã©tait donc dâ€™appeler manuellement $container-&amp;gt;get(&#39;m6_statsd&#39;)-&amp;gt;send() dans la commande ou dans un ConsoleExceptionListener mais cela nous fait perdre le principal intÃ©rÃªt du StatsdBundle Ã  savoir le dÃ©couplage entre la commande et le client StatsD.\n\nLa seconde solution a donc Ã©tÃ© de modifier le StatsdBundle et dâ€™ajouter une configuration au niveau de lâ€™Ã©vÃ©nement pour forcer lâ€™envoi instantanÃ© des donnÃ©es.\n\nAinsi, avec la configuration suivante :\n\nclients:\n    event:\n        console.exception:\n            increment:      mysite.command.&amp;lt;command.name&amp;gt;.exception\n            immediate_send: true\n        m6kernel.exception:\n            increment: mysite.errors.&amp;lt;status_code&amp;gt;\n\nLâ€™incrÃ©ment mysite.command.&amp;lt;command.name&amp;gt;.exception sera envoyÃ© en temps rÃ©el, alors que les autres comme mysite.errors.&amp;lt;status_code&amp;gt; continueront Ã  Ãªtre envoyÃ©s pendant kernel.terminate.\n"
} ,
  
  {
    "title"    : "Refonte de notre systÃ¨me de vote",
    "category" : "",
    "tags"     : " api, symfony, redis, monitoring, qualite, cytron",
    "url"      : "/2014/02/18/refonte-de-notre-systeme-de-vote.html",
    "date"     : "February 18, 2014",
    "excerpt"  : "Notre systÃ¨me de vote est utilisÃ© dâ€™une part pour gÃ©rer lâ€™ensemble des questions et des rÃ©ponses associÃ©es utilisÃ©es dans nos quizz et dâ€™autre part pour rÃ©colter le nombre de votes des internautes lors des jeux concours.\n\nActuellement, le trafic g...",
  "content"  : "Notre systÃ¨me de vote est utilisÃ© dâ€™une part pour gÃ©rer lâ€™ensemble des questions et des rÃ©ponses associÃ©es utilisÃ©es dans nos quizz et dâ€™autre part pour rÃ©colter le nombre de votes des internautes lors des jeux concours.\n\nActuellement, le trafic gÃ©nÃ©rÃ© par cette fonctionnalitÃ© varie entre quelques votes par minute la nuit Ã  quelques dizaines de votes par seconde lors des premiÃ¨res parties de soirÃ©e.\n\nHistorique\n\nComme souvent, les besoins ont rÃ©guliÃ¨rement Ã©voluÃ© depuis la mise en place initiale du systÃ¨me en 2009, faisant parfois prendre des chemins tortueux Ã  lâ€™implÃ©mentation technique. Au fil des demandes, notre systÃ¨me a par exemple dÃ» stocker ses donnÃ©es dans nos forums pour une fonctionnalitÃ© qui a ensuite Ã©tÃ© rapidement abandonnÃ©e.\n\nLâ€™annÃ©e 2012 a vu lâ€™arrivÃ©e du second Ã©cran : Ã  lâ€™aide de lâ€™application gratuite adÃ©quate, les pÃ©riphÃ©riques mobiles peuvent dÃ©sormais se synchroniser avec lâ€™Ã©mission en cours de visionnage, en direct ou en diffÃ©rÃ©, sur la TV ou sur le web (la synchronisation se fait par la bande son). Cette synchronisation nous permet de pusher instantanÃ©ment sur les pÃ©riphÃ©riques mobiles du contenu adaptÃ© Ã  ce que le tÃ©lÃ©spectateur regarde : le dÃ©tail de la recette que le cuisinier prÃ©pare dans Top Chef ou un sondage concernant la derniÃ¨re trouvaille linguistique dâ€™un châ€™tit face Ã  sa châ€™tite.\n\nLe second Ã©cran sâ€™annonÃ§ait alors comme une source importante de trafic supplÃ©mentaire pour notre systÃ¨me de vote. Effectivement, en plus du trafic historique gÃ©nÃ©rÃ© par les sites web, nous allions aussi recevoir tous les votes provenant des pÃ©riphÃ©riques mobiles.\n\nCe nouveau trafic a une saisonnalitÃ© trÃ¨s marquÃ©e : il est principalement prÃ©sent en dÃ©but de soirÃ©e et reste trÃ¨s dÃ©pendant du programme diffusÃ© et de la contribution apportÃ©e.\n\nProblÃ©matique\n\nLa principale problÃ©matique venait de lâ€™architecture des bases de donnÃ©es MySQL. Ã‰tant fortement couplÃ©es sur lâ€™ensemble de la plateforme, la moindre dÃ©faillance de lâ€™une dâ€™elles, due Ã  une surcharge sur un sondage, risquait de pÃ©naliser les internautes de tous nos autres sites (un sondage du second Ã©cran pouvait donc impacter lâ€™expÃ©rience utilisateur de Clubic).\n\nLe code Ã©tait aussi fortement couplÃ© entre nos diffÃ©rentes applications : lâ€™action PHP dâ€™un vote Ã©tait exÃ©cutÃ©e sur la mÃªme plateforme que notre BO permettant Ã  tous nos web services de fonctionner ainsi quâ€™aux contributeurs dâ€™ajouter du contenu. Une surchage sur les votes aurait donc pu entrainer des perturbations sur le fonctionnement global du site m6.fr et de ses web services, donc de beaucoup de produits par extension.\n\nPour rÃ©sumer, lâ€™imbrication du code et des bases de donnÃ©es dans lâ€™usine logiciel ne permettait pas de calibrer le systÃ¨me de vote pour quâ€™il puisse recevoir la charge attendue par le second Ã©cran.\n\nCâ€™est donc dÃ©but 2013 que Kenny Dits mâ€™a contactÃ© pour que nous trouvions une solution permettant de dÃ©coupler le systÃ¨me de vote tout en faisant Ã©voluer son architecture interne afin quâ€™il puisse facilement sâ€™adapter Ã  la charge inconstante du second Ã©cran.\n\nSolution\n\nNous avons alors conÃ§u un nouveau service dÃ©diÃ© uniquement Ã  la gestion des questions, rÃ©ponses et votes des utilisateurs. Ce nouveau service Polls est autonome, ce qui nous permet de le dÃ©coupler complÃ¨tement de notre usine logicielle avec laquelle il communique via une API REST.\n\nConcernant le stockage des donnÃ©es, nous avons simplement choisi un moteur trÃ¨s performant qui supporterait la charge sur une seule machine bien calibrÃ©e. Cela nous Ã©vitait alors les problÃ©matiques complexes de clustering. Mais nous devions tout de mÃªme stocker quelques informations relationnelles : il fallait donc avoir accÃ¨s Ã  quelques primitives nous permettant dâ€™Ã©muler les relations minimum entre nos donnÃ©es. Redis sâ€™est donc imposÃ© comme la solution adÃ©quate. Cela reste malgrÃ© tout une solution thÃ©oriquement insatisfaisante, car non rÃ©ellement scalable. Mais en pratique, les trÃ¨s bonnes performances de Redis permettent de rÃ©pondre Ã  (bien plus que) nos attentes.\n\nLe code se trouve, pour sa part, complÃ¨tement isolÃ© sur son propre serveur.\nComme ce service est complÃ¨tement stateless et que notre base de donnÃ©e est centralisÃ©e et suffisamment performante, nous pouvons donc facilement ajouter ou supprimer des serveurs web selon la charge attendue : on peut dire quâ€™en pratique le service Polls est scalable horizontalement.\n\nLorsque lâ€™architecture mise en place permet de rÃ©partir la charge sur un nombre variable de machines, le contrat est rempli : ce nâ€™est plus quâ€™une question dâ€™argent pour supporter nâ€™importe quelle charge. Et comme tout le monde le sait : lâ€™argent nâ€™est pas un problÃ¨me, câ€™est une solution.\n\nDÃ©veloppement\n\nLe service Polls a Ã©tÃ© dÃ©veloppÃ© en PHP avec Symfony et le FOSRestBundle. Nous avons dâ€™abord suivi certaines rÃ©fÃ©rences, puis nous avons ensuite dÃ©veloppÃ© un micro ORM maison pour faire persister nos donnÃ©es dans Redis et enfin nous avons monitorÃ© tous ce que lâ€™on pouvait Ã  lâ€™aide de notre bundle dÃ©diÃ©.\n\nUne attention toute particuliÃ¨re a Ã©tÃ© portÃ©e Ã  la qualitÃ© avec des tests unitaires couvrant un maximum de code et des tests fonctionnels couvrant la plupart des cas dâ€™utilisation des clients. Les nombreuses mises en production journaliÃ¨res pendant la phase dâ€™optimisation ont ainsi Ã©tÃ© grandement facilitÃ©es, notamment grÃ¢ce Ã  la sÃ©rÃ©nitÃ© apportÃ©e par lâ€™intÃ©gration continue.\n\nMise en production\n\nLâ€™intÃ©gration de ce nouveau service Polls a cependant Ã©tÃ© bien plus longue que son dÃ©veloppement. Nous lâ€™avons dâ€™abord mis en production en doublon de lâ€™ancien systÃ¨me : toutes les Ã©critures Ã©taient faites sur les deux systÃ¨mes, mais lâ€™ancien Ã©tait encore la rÃ©fÃ©rence lors de la lecture des rÃ©sultats par les clients.\n\nPuis aprÃ¨s deux semaines, lorsque nous avons validÃ© lâ€™exacte corrÃ©lation entre les deux courbes du nombre de votes par minute Ã  lâ€™aide de Graphite, nous avons alors changÃ© les clients pour quâ€™ils viennent lire les rÃ©sultats sur le service Polls.\n\nEncore deux semaines plus tard, lorsque tout Ã©tait validÃ© et que nous avions dÃ©veloppÃ© et exÃ©cutÃ© un script dâ€™import de lâ€™historique, nous avons dÃ©branchÃ© lâ€™ancien systÃ¨me.\n\nLâ€™intÃ©gration a donc Ã©tÃ© au moins trois fois plus longue, et donc couteuse, que le dÃ©veloppement du service en lui-mÃªme.\n\nOptimisation\n\nLa premiÃ¨re optimisation est simplement conceptuelle : nous avons concentrÃ© la criticitÃ© sur une seule route, celle qui est utilisÃ©e par chaque client pour voter. Il est ainsi plus simple de mesurer et donc dâ€™amÃ©liorer les performances du service Polls. Cette route est critique parce quâ€™elle est utilisÃ©e par tous les clients, quâ€™elle ne peut pas Ãªtre cachÃ©e et quâ€™il faut Ã©crire des donnÃ©es en base lors de chaque appel.\n\nIl existait plusieurs pistes dâ€™optimisation connues (systÃ¨me de queue, node.js, etc.) mais dans une optique KISS, nous avons dâ€™abord optÃ© pour lâ€™utilisation des technologies en place pour ensuite interprÃ©ter les rÃ©sultats rÃ©cupÃ©rÃ©s lors des tests de charge et sâ€™adapter si besoin.\n\nDans un premier temps, nous avons lÃ©gÃ¨rement ajustÃ© notre modÃ¨le de donnÃ©es pour limiter le nombre dâ€™action Ã  rÃ©aliser sur la base de donnÃ©es : nous avons seulement deux instructions Redis de complexitÃ© constante O(1) Ã  rÃ©aliser pour chaque vote. Puis nous avons utilisÃ© les transactions pour grouper ces deux instructions et Ã©viter la latence dâ€™une connexion supplÃ©mentaire vers notre serveur Redis.\n\nNous avons enfin supprimÃ© la vÃ©rification de deux contraintes dâ€™intÃ©gritÃ© sans importance. Le code retour en cas dâ€™erreur est juste un peu moins cohÃ©rent (400 au lieu de 422) mais cela nâ€™impacte ni lâ€™intÃ©gritÃ© des votes ni la sÃ©curitÃ© du service.\n\n\n\n\n\nAfin de savoir si nous nâ€™avions pas complÃ¨tement pris une mauvaise direction dans notre utilisation de Symfony, nous avons alors fait appel Ã  Alexandre SalomÃ©, consultant SensioLabs, pour auditer notre code.\n\nLors de cette journÃ©e, durant laquelle nous avons beaucoup appris, nous avons simplement dÃ©sactivÃ© tous les bundles que nous nâ€™utilisions pas rÃ©ellement en production : principalement Twig. Cela a occasionnÃ© une lÃ©gÃ¨re modification de notre code car le FOSRestBundle nÃ©cessite Twig pour afficher les erreurs mÃªme lorsque celles-ci sont en JSON.\n\nUne fois cette modification apportÃ©e, nous avons gagnÃ© les ultimes millisecondes nous permettant de passer sous la barre symbolique des 10ms de temps de rÃ©ponse sur notre route critique.\n\n\n\nVous remarquerez que nous avons dâ€™abord dÃ©ployÃ© le systÃ¨me en production avant de chercher Ã  lâ€™optimiser : nous pouvions ainsi mesurer en temps rÃ©el lâ€™impact de nos dÃ©veloppements sur une multitude dâ€™indicateurs dont le temps de rÃ©ponse.\n\nMise en pratique\n\nLe service Polls a facilement tenu la charge pour la premiÃ¨re Ã©mission mettant en avant le second Ã©cran : un Ã©pisode de HawaÃ¯ 5-0 durant lequel les internautes pouvaient choisir le coupable avec un sondage (sur leur tÃ©lÃ©phone, tablette ou PC).\n\n\n\nPlus prÃ©cisÃ©ment, nous sommes montÃ©s Ã  150 requÃªtes par secondes (ce qui est Ã©videmment bien moins que nos tests de charge), mais nous savons que nous pourrons maintenant nous adapter trÃ¨s simplement Ã  une charge beaucoup plus forte en ajoutant des serveurs web. Notamment lors dâ€™Ã©missions faisant grandement appel au second Ã©cran.\n\nDans le pire des cas, si le service Polls devient indisponible, aucune autre partie de notre infrastructure ne sera compromise.\n\nLeÃ§ons\n\nAu cours du dÃ©veloppement, de la mise en production et de la maintenance de ce service, jâ€™ai appris plusieurs choses que jâ€™essaierai de ne pas oublier trop vite :\n\n\n  Yes we can! Il est possible de combler petit Ã  petit la dette technique, mais uniquement si câ€™est la volontÃ© des dÃ©cideurs,\n  la sÃ©rÃ©nitÃ© apportÃ©e par les tests automatisÃ©s est sans Ã©gale pour le confort de dÃ©veloppement,\n  Redis est trÃ¨s performant.\n\n\nHa ? Attendez ! On me dit dans lâ€™oreillette que certains doutaient encore quâ€™il Ã©tait possible de faire du code performant avec un framework full stack comme Symfony.\n\nPas moi :-)\n"
} ,
  
  {
    "title"    : "How we use StatsD",
    "category" : "",
    "tags"     : " statsd, graphite, php, nodejs, monitoring",
    "url"      : "/2014/01/28/how-we-use-statsd.html",
    "date"     : "January 28, 2014",
    "excerpt"  : "What we want\n\nAs developers, we (M6Web) want to keep our eyes open on what is going on in production.\n\nOur local CMO (chief monitoring officier ;) ) did a nice presentation about this (in french).\n\nAs someone very wise (Theo Schlossnagle) said: â€œI...",
  "content"  : "What we want\n\nAs developers, we (M6Web) want to keep our eyes open on what is going on in production.\n\nOur local CMO (chief monitoring officier ;) ) did a nice presentation about this (in french).\n\nAs someone very wise (Theo Schlossnagle) said: â€œItâ€™s not in production unless itâ€™s monitoredâ€. Another cool mantra is: â€œI am wondering what to monitor ? everything dude !â€. Finally â€œif you can not measure it, you can not improve it.â€ (Lord Kelvin).\n\nWe ship new apps very often, so we have to industrialise this practice.\n\nWhat is it?\n\nStatsD is a Node.Js daemon allowing you to send metrics (increment values and timers) over UDP. The fire and forget feature of UDP is great for reducing risks of introducing latency or crashes in your application.\n\nStatsD is open sourced by etsy. In our configuration, we use several StatsD deamons and aggregate metrics on Graphite - one point per minute. Many servers allows us to scale, because we donâ€™t sample the data at all.\n\nOn client side, we use a simple consistent hashing algorithm to dispatch metrics overs StatsD nodes on the same server.\n\nCollecting metrics\n\nFrom raw PHP\n\nWeâ€™ve created a simple PHP lib to dispatch metrics over UDP. Check it out on Github or Packagist.\n\nThe usage is pretty straightforward :\n\n&amp;lt;?php\n// client creation\n$client = new Statsd\\Client(\n                    array(\n                        &#39;serv1&#39; =&amp;gt; array(&#39;address&#39; =&amp;gt; &#39;udp://200.22.143.12&#39;),\n                        &#39;serv2&#39; =&amp;gt; array(&#39;port&#39; =&amp;gt; 8125, &#39;address&#39; =&amp;gt; &#39;udp://200.22.143.12&#39;)\n                    )\n                );\n// usage\n$client-&amp;gt;increment(&#39;a.graphite.node&#39;);\n$client-&amp;gt;timing(&#39;another.graphite.node&#39;, (float) $timing);\n\nFrom Symfony2\n\nAs basic Symfony2 fanboys, weâ€™ve built a bundle on top of the StatsD component.\nIt provides these features:\n\n\n  manage multiple Symfony services with different configurations\n  bind any event to increment nodes and collect timers\n\n\nDuring Symfony 2 execution, metrics are collected and sent only at the kernel shutdown. A nice feature is that you can easily collect basic metrics based on events without touching your code.\n\nFor example, in conjunction with the M6Web\\HttpKernelBundle, just dropping this in config.yml is enough:\n\nm6_statsd:\n    clients:\n        default:\n            servers: [&#39;all&#39;]\n            events:\n              m6.terminate:\n                increment:     request.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;\n                timing:        request.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;\n                custom_timing: { node: memory.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;, method: getMemory }\n              m6kernel.exception:\n                increment: errors.&amp;lt;status_code&amp;gt;.yourapp\n\n\n\nOffering this to the tech team means that I am now pretty sure that almost all new PHP apps pop with those metrics out of the box.\n\nPlease checkout the bundle documentation on github.\n\nFrom anywhere else\n\nFrom Flex, mobile app or JS applications weâ€™ve developed a simple Node.js app, translating an HTTP call to a StatsD UDP one. Like the PHP implementation, this application shards the metrics over multiple servers.\n\nPlease consider sending metrics asynchronously and add a timeout to this HTTP call.\n\nLiving with metrics\n\nAbout 120K metrics are collected on our platform. Thatâ€™s a lot.\n\nGraphite dashboards are quite rustic. But surprisingly lots of non-techs people use this tool: SEO experts, advertising managers, contributors, â€¦\n\n\n\n\n\nFor now we keep using Graphite. We try to keep our dashboards organised and well named.\n\nFor alerting purpose, a tool based on Graphite JSON output has been developed. It sends emails when it reaches some user defined conditions. Honestly, it does the job, but frankly we are still looking for something else, more flexible with more notification systems than emails.\n\nIf you use such a tool, and youâ€™re happy with it, please let us know in the comments.\n\nFound a typo or bad english langage, just propose a pull request.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #6",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2014/01/20/m6web-dev-facts-6.html",
    "date"     : "January 20, 2014",
    "excerpt"  : "Parce que nous en avons encore une quantitÃ© incroyable en stock, voici une nouvelle sÃ©lection des meilleures phrases entendues dans nos bureaux !\n\nTo code or not to code\n\n\n  \n    Je veux faire du code qui marche, et qui sert Ã  quelque chose !\n    ...",
  "content"  : "Parce que nous en avons encore une quantitÃ© incroyable en stock, voici une nouvelle sÃ©lection des meilleures phrases entendues dans nos bureaux !\n\nTo code or not to code\n\n\n  \n    Je veux faire du code qui marche, et qui sert Ã  quelque chose !\n    Câ€™est soit lâ€™un, soit lâ€™autre, mec â€¦\n  \n\n\nImage trompeuse\n\nUn dÃ©v teste une appli sur ipad\n\n\n  \n    Ah mais lâ€™image est toute moche\n    Câ€™est ton reflet que tu vois\n  \n\n\nUne histoire de chocolat\n\n\n  \n    Qui a testÃ© la Kindle HDX ?\n    Moi jâ€™ai pas trop aimÃ©\n    Pourquoi ?\n    Je prÃ©fÃ©re le Kindle Surprise\n  \n\n\ncâ€™est automatique je te dis\n\n\n  Mais je te dis que câ€™est automatique â€¦ Ã  90%\n\n\nUne Ã©vidence Ã©vidente\n\n\n  Câ€™est pas toujours simple parce que câ€™est compliquÃ©\n\n\nTout tout tout\n\n\n  Ca couvre la quasi totalitÃ© de tout\n\n\nVers lâ€™infini et au delÃ \n\n\n  Il faut que le forum soit cachÃ© intÃ©gralement autant que possible\n\n\nFichier introuvable\n\n\n  Je tâ€™envoie par mail lâ€™url du fichier\n\n\nPar mail :\n\n\n  file://C:/Users/**/Desktop/Sanstitre-1.html\n\n\nUnknown Notice\n\n\n  Les notices câ€™est tabous, on en viendra tous Ã  bout\n\n\nLa minute de 30 secondes - le retour\n\n\n  Tu as pris six mois dâ€™annÃ©e sabatique\n\n"
} ,
  
  {
    "title"    : "Vagrant &amp; Cie, du DÃ©v Ã  la Prod avec Julien Bianchi",
    "category" : "",
    "tags"     : " lft, vagrant, video",
    "url"      : "/2014/01/18/vagrant-julien-bianchi.html",
    "date"     : "January 18, 2014",
    "excerpt"  : "Un grand merci Ã  Julien Bianchi qui, Ã  notre demande, est venu nous parler un peu de vagrant lors dâ€™un de nos fameux Last Friday Talk.\n\nSes slides : https://speakerdeck.com/jubianchi/vagrant-and-cie-du-dev-a-la-prod.\n",
  "content"  : "Un grand merci Ã  Julien Bianchi qui, Ã  notre demande, est venu nous parler un peu de vagrant lors dâ€™un de nos fameux Last Friday Talk.\n\nSes slides : https://speakerdeck.com/jubianchi/vagrant-and-cie-du-dev-a-la-prod.\n"
} ,
  
  {
    "title"    : "API Ã  consommer avec modÃ©ration",
    "category" : "",
    "tags"     : " outil, api, symfony, doctrine, cytron, open-source",
    "url"      : "/2014/01/08/api-a-consommer-avec-moderation.html",
    "date"     : "January 8, 2014",
    "excerpt"  : "AprÃ¨s avoir travaillÃ© pendant plusieurs mois sur la crÃ©ation et les tests de nos API avec Symfony, le moment de leur publication est enfin arrivÃ© !\n\nOr, les clients de nos API sont multiples : il peut sâ€™agir dâ€™applications mobiles, de sites web ma...",
  "content"  : "AprÃ¨s avoir travaillÃ© pendant plusieurs mois sur la crÃ©ation et les tests de nos API avec Symfony, le moment de leur publication est enfin arrivÃ© !\n\nOr, les clients de nos API sont multiples : il peut sâ€™agir dâ€™applications mobiles, de sites web mais aussi dâ€™un back office interne. Chacun de ces clients peut nÃ©cessiter des â€œvuesâ€ diffÃ©rentes de lâ€™API.\n\nEffectivement, alors que le BO devra pouvoir accÃ©der Ã  la totalitÃ© des ressources disponibles, lâ€™application mobile ne devra avoir accÃ¨s quâ€™aux ressources publiÃ©es. De la mÃªme maniÃ¨re, la gestion du cache ainsi que la disponibilitÃ© des routes doit pouvoir sâ€™adapter facilement aux clients qui consomment lâ€™API.\n\nNous avons optÃ© pour lâ€™utilisation dâ€™un sous-domaine par client afin de lâ€™identifier et ainsi de lui appliquer des configurations particuliÃ¨res. Ex :\n\n\n  https://bo.api.monservice.fr pour le BO,\n  https://mobile.api.monservice.fr pour lâ€™application mobile.\n\n\n####Â Authentification\n\nNous utilisons le composant sÃ©curitÃ© de Symfony, qui permet de crÃ©er un utilisateur authentifiÃ© Ã  la volÃ©e et de charger la configuration spÃ©cifique Ã  celui-ci.\n\nNous avons tout dâ€™abord besoin de crÃ©er une classe User implÃ©mentant Symfony\\Component\\Security\\Core\\User\\UserInterface, et contenant les informations de configuration spÃ©cifique.\n\nLes diffÃ©rents Users sont ensuite crÃ©Ã©s Ã  lâ€™aide dâ€™un fournisseur dâ€™utilisateurs implÃ©mentant Symfony\\Component\\Security\\Core\\User\\UserProviderInterface.\nDans notre cas, chaque utilisateur possÃ¨de son propre fichier de configuration yml. Le fournisseur dâ€™utilisateur vÃ©rifie donc que lâ€™utilisateur demandÃ© possÃ¨de un fichier de configuration et instancie un objet User avec cette configuration. Ce UserProvider est dÃ©fini comme service dans notre bundle et configurÃ© dans security.yml.\n\nIl faut ensuite crÃ©er notre propre fournisseur dâ€™authentification pour avoir une authentification par nom de domaine. Pour cela nous avons suivi et adaptÃ© le cookbook de Symfony. Cette authentification sâ€™articule autour de 2 classes : un FirewallListener et un AuthenticationProvider. Pour que notre FirewallListener puisse facilement rÃ©cupÃ©rer le client associÃ©, nous avons ajoutÃ© un paramÃ¨tre au routing Symfony :\n\nhost: {client}.api.monservice.fr\n\nLe FirewallListener utilise donc ce paramÃ¨tre du routing comme nom dâ€™utilisateur et le transmet Ã  notre AuthenticationProvider. Celui-ci rÃ©cupÃ¨re le User grÃ¢ce au UserProvider et profite de cette phase pour vÃ©rifier que lâ€™adresse IP du client est bien autorisÃ©e dans sa configuration grÃ¢ce au FirewallBundle.\n\nEffectivement, nous avons ajoutÃ© un filtrage initial (mais optionnel) sur les IPs pour chaque client, dans le fichiers app/config/users/{username}.yml :\n\nfirewall:\n    user_access:\n        default_state: false\n        lists:\n            m6_prod: true\n            m6_preprod: true\n            m6_dev: true\n            m6_lan: true\n            m6_local: true\n            m6_public: true\n\nPour plus de prÃ©cisions, voir la documentation du FirewallBundle.\n\nAutorisation\n\nPour gÃ©rer les autorisations dâ€™accÃ¨s des utilisateurs aux diffÃ©rentes routes, nous avons crÃ©Ã© un EventListener qui Ã©coute kernel.request et qui dÃ©cide de laisser passer la requÃªte ou non en fonction de la configuration de lâ€™utilisateur.\n\nallow:\n    default: true\n    methods:\n        delete: false\n    resources:\n        exam: false\n    routes:\n        get_articles: false\n\nDans cet exemple, lâ€™utilisateur a accÃ¨s par dÃ©faut Ã  toutes les routes sauf les mÃ©thodes DELETE, les routes concernant les exams et la route spÃ©cifique get_articles.\n\n####Â DurÃ©e de cache\n\nLes temps de cache sont diffÃ©rents en fonction de lâ€™utilisation des donnÃ©es. Les donnÃ©es du backoffice ne seront pas cachÃ©es, tandis que les donnÃ©es de lâ€™application mobile auront un temps de cache de 300s.\nNous avons lÃ -aussi crÃ©Ã© un EventListener qui Ã©coute cette fois kernel.response et qui modifie les headers de cache de la rÃ©ponse en fonction de la configuration utilisateur qui peut contenir une durÃ©e par dÃ©faut de cache et des durÃ©es de cache par route.\n\nFiltrage automatique avec Doctrine\n\nNous pouvons offrir une â€œvueâ€ diffÃ©rente de nos donnÃ©es Ã  chaque client en dÃ©finissant des critÃ¨res de filtrage pour Doctrine (ex: date de publication, ressource activÃ©e, etc.) dans les fichiers de configuration des clients :\n\nentities:\n    article:\n        active: true\n        publication: false\n\nAfin de ne pas modifier le comportement par dÃ©faut de Doctrine, nous avons ajoutÃ© une mÃ©thode findWithContext Ã  nos repositories qui reprend les mÃªmes paramÃ¨tres que la mÃ©thode findBy en injectant le SecurityContext. Cette mÃ©thode permet donc de rÃ©cupÃ©rer des entitÃ©s filtrÃ©es en fonction des paramÃ¨tres dâ€™un client :\n\n&amp;lt;?php\n$article = $this\n    -&amp;gt;get(&#39;m6_contents.article.manager&#39;)\n    -&amp;gt;getRepository()\n    -&amp;gt;findWithContext($this-&amp;gt;container-&amp;gt;get(&#39;security.context&#39;), [&#39;id&#39; =&amp;gt; $id]);\n\n####Â Personnalisation avancÃ©e\n\nGrÃ¢ce Ã  lâ€™utilisation du Bundle Security de Symfony, toute la configuration spÃ©cifique Ã  un sous-domaine est stockÃ©e dans lâ€™utilisateur courant. Et dans Symfony, lâ€™utilisateur courant est facilement rÃ©cupÃ©rable Ã  partir du service security_context. Il est ainsi possible de personnaliser nâ€™importe quelle brique de lâ€™application en y injectant la dÃ©pendance sur ce service.\n\nDomainUserBundle\n\nAfin dâ€™implÃ©menter facilement ce fonctionnement sur nos API, nous avons dÃ©veloppÃ© un bundle dÃ©diÃ©. Il peut donc aussi vous permettre de gÃ©rer lâ€™authentification et la configuration de vos API par nom de domaine.\n\nDomainUserBundle est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Qui a bouchonnÃ© mon Redis ?",
    "category" : "",
    "tags"     : " qualite, outil, redis, cytron, open-source",
    "url"      : "/redismock-qui-a-bouchonne-mon-redis",
    "date"     : "December 11, 2013",
    "excerpt"  : "Les tests fonctionnels tiennent un rÃ´le majeur dans la rÃ©ussite et la pÃ©rennitÃ© dâ€™un projet web, dâ€™autant plus sâ€™il est dÃ©ployÃ© continuellement. Nous nous Ã©tions donc dÃ©jÃ  intÃ©ressÃ©s Ã  cette problÃ©matique dans le cas dâ€™un service proposant une API...",
  "content"  : "Les tests fonctionnels tiennent un rÃ´le majeur dans la rÃ©ussite et la pÃ©rennitÃ© dâ€™un projet web, dâ€™autant plus sâ€™il est dÃ©ployÃ© continuellement. Nous nous Ã©tions donc dÃ©jÃ  intÃ©ressÃ©s Ã  cette problÃ©matique dans le cas dâ€™un service proposant une API REST et utilisant MySQL et Doctrine. Mais nous dÃ©veloppons aussi des services du mÃªme type utilisant dâ€™autres systÃ¨mes de stockage de donnÃ©es comme Redis.\n\nAfin de tester fonctionnellement ces services, nous avons dâ€™abord eu lâ€™idÃ©e dâ€™installer une instance Redis sur nos serveurs de tests. Mais nous allions inÃ©luctablement retomber sur les mÃªmes obstacles quâ€™avec MySQL :\n\n\n  il nâ€™est pas toujours possible de monter une instance Redis dÃ©diÃ©e aux tests,\n  mais surtout une telle architecture nâ€™est pas viable dans un systÃ¨me de tests concurrentiels.\n\n\nLa librairie RedisMock\n\nNous nous sommes alors penchÃ©s sur la possibilitÃ© de bouchonner Redis, chose qui parait au premier abord plus aisÃ©e que de bouchonner Doctrine : Redis propose une API simple et bien documentÃ© (mÃªme si abondante). Nous pensions trouver une librairie PHP dÃ©jÃ  existante mais nos recherches sont restÃ©es vaines.\n\nNous avons donc crÃ©Ã© la librairie RedisMock qui reprend simplement les commandes de lâ€™API de Redis et simule leur comportement grÃ¢ce aux fonctions natives de PHP. Ã‰videmment, toutes les commandes Redis nâ€™ont pas encore Ã©tÃ© implÃ©mentÃ©es, seules celles qui sont utilisÃ©es dans nos tests sont prÃ©sentes. Vous pouvez nous proposer lâ€™implÃ©mentation de nouvelles fonctions Redis, selon vos besoins, via des Pull Requests sur le projet.\n\nToutes les commandes exposÃ©es par le mock sont testÃ©es unitairement via atoum en reprenant pour chaque cas les spÃ©cifications Ã©noncÃ©es dans la documentation Redis.\n\nUtiliser RedisMock dans vos tests sur Symfony\n\nTout dâ€™abord, il faut rajouter la dÃ©pendance Ã  la librairie dans le composer.json et mettre Ã  jour les vendors :\n\n\n\nLâ€™utilisation du mock reste trÃ¨s simple dans un projet Symfony. Chez M6Web, nous utilisons notre propre composant Redis, lui mÃªme basÃ© sur Predis. Afin que le mock puisse complÃ¨tement se faire passer pour la librairie Redis lors de lâ€™execution des tests, nous avons implÃ©mentÃ© une factory qui crÃ©e Ã  la volÃ©e un adapteur hÃ©ritant de la classe Ã  bouchonner. La mÃ©thode getAdpaterClass permet de rÃ©cupÃ©rer le nom de la classe Ã  instancier.\n\n\n\nPour simplifier la crÃ©ation de lâ€™adapteur et son injection dans lâ€™application via le fichier config_test.yml, on peut utiliser la mÃ©thode getAdapter qui instancie directement lâ€™objet sans paramÃ¨tre. Il nous suffit alors de modifier la dÃ©finition du service Redis dans lâ€™environnement de test.\n\n\n\nEt voilÃ , le tour est jouÃ© ! Les tests utilisent maintenant le mock Ã  la place du vÃ©ritable Redis. Attention cependant, si votre librairie utilise des fonctionnalitÃ©s non implÃ©mentÃ©es dans RedisMock, vous pourriez faire face Ã  des comportements alÃ©atoires indÃ©sirables.\n\nRedisMock  est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Composer installation without github.com (nor packagist) dependency - like a boss !",
    "category" : "",
    "tags"     : " satis, composer, aws, s3, github, packagist, cloud",
    "url"      : "/composer-installation-without-github",
    "date"     : "December 2, 2013",
    "excerpt"  : "\n\nFirst a thought about github, composer, packagist : we like / adore / thanks the contributors, for those great services and all the open source people dropping great software on it.\n\nThat said, picture yourself operating an online PHP service, g...",
  "content"  : "\n\nFirst a thought about github, composer, packagist : we like / adore / thanks the contributors, for those great services and all the open source people dropping great software on it.\n\nThat said, picture yourself operating an online PHP service, generating hundreds euros per hour (cool isnâ€™t it ?).\n\nIf you use Symfony2 and other public packages, like us, youâ€™re probably deploying your application using composer.\n\n\n\nSuddenly the service is dealing with more and more and more traffic (maybe someone talk about on national tv â€¦ or Justin Bieber tweet something â€¦ maybe :) ). No problem, says the system administrator (yes our sysadmins are cool), lets pop more virtual machines and deploy more instance of the service !\n\nAnd then :\n\n\n\nboum ! =&amp;gt;composer install command canâ€™t download distant packages on api.github.com (website is down, or the network connection or whatever).\n\nGood luck explaining to your boss that you rely on free hosting service to deploy your business critical website !\n\nThis is our situation. So here how we deal with that.\n\nPrinciples.\n\n\n\nWe chose to use Satis - a great tool provided by the Composer team. The main idea is, regulary download packages and their informations on our local servers. We (at M6Web) deployed services on our local infrastructure and on S3 servers in Amazon Web Services.\n\nHow to ? For your local network.\n\nWe set 2 different satis instance. One for our private packages, and another for all the dependencies we use (basically around Symfony2). The first one (satis-private) will build every 5 minutes, the second (satis-public) every half hour.\n\nfor example :\n\n\n  satis-private.yourcompany.com\n  satis-public.youcompany.com\n\n\nSatis for private package configuration (data/satis.json) :\n\n{\n    &quot;name&quot;: â€œsatis-private&quot;,\n    &quot;homepage&quot;: &quot;https://satis-private.yourcompany.com&quot;,\n    &quot;archive&quot;: {\n        &quot;directory&quot;: &quot;dist&quot;,\n        &quot;absolute-directory&quot; : &quot;/srv/data/satis-private/dist&quot;,\n        &quot;format&quot;: &quot;zip&quot;,\n        &quot;skip-dev&quot;: true\n    },\n    &quot;repositories&quot;: [\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/great-bundle&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/great-component&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/awesomelib&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/raoul&quot; },\nâ€¦\n    ],\n    &quot;require-all&quot;: true\n}\n\n\nSatis for public package configuration (data/satis.json) :\n\n{\n    &quot;name&quot;: â€œsatis-public&quot;,\n    &quot;homepage&quot;: &quot;https://satis-public.yourcompany.com&quot;,\n    &quot;archive&quot;: {\n        &quot;directory&quot;: &quot;dist&quot;,\n        &quot;format&quot;: &quot;zip&quot;,\n        &quot;skip-dev&quot;: false,\n        &quot;absolute-directory&quot; : &quot;/srv/data/satis-public/dist&quot;\n    },\n    &quot;repositories&quot;: [\n        { &quot;type&quot;: &quot;composer&quot;, &quot;url&quot;: &quot;https://packagist.org&quot; }\n    ],\n    &quot;require&quot;: {\n\n        &quot;m6web/firewall-bundle&quot; : &quot;*&quot;,\n        &quot;m6web/statsd-bundle&quot;   : &quot;*&quot;,\n\n        &quot;doctrine/orm&quot;               : &quot;~2.3&quot;,\n        &quot;doctrine/common&quot;            : &quot;~2.4&quot;,\n        &quot;doctrine/dbal&quot;              : &quot;~2.3&quot;,\n        &quot;doctrine/doctrine-bundle&quot;   : &quot;~1.2&quot;,\n\n        &quot;naderman/composer-aws&quot;      : &quot;~0.2.3&quot;,\nâ€¦\n    },\n    &quot;require-dependencies&quot;: true\n}\n\n\nOn the crontab, add this command for each satis instance :\n\nphp -d memory_limit=xx bin/satis build data/satis.json web\n\n\n(increasing memory_limit was mandatory for us with satis-public).\n\nPlease note the require-dependencies directive. It tell satis to digg on sub-dependencies on the required packages. And yes, it can take a while. You will probably hit the Github API rate limit. To increase it, add a Github key on your composer configuration file on the satis servers.\n\n$ cat .composer/config.json\n{\n    &quot;config&quot;: {\n        &quot;github-oauth&quot;: {\n            &quot;github.com&quot;: â€œxxxxxx&quot;\n        }\n    }\n\n}\n\n\nIn your projects, edit the composer.json and replace the repositories entry by\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://satis-private.yourcompany.com&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://satis-public.yourcompany.com&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nRemove your composer.lock and vendors then run composer update on the project.\n&quot;packagist&quot;: false&quot; mean : â€œdo not search missing packages on packagist.comâ€. If a package is missing during install, you have to add it in satis-public configuration file then try again.\n\nthatâ€™s it :)\n\nHow to ? For AWS.\n\nSync our 2 satis servers with an S3 bucket.\n\n\n\nOn satis servers, use s3cmd to keep in sync the S3 bucket. Letâ€™s say : yourcloud-satis.\n\nAdd some commands after the build script of satis :\n\nphp -d memory_limit=xx bin/satis build data/satis.json web\ncd web\nsed &#39;s#https://satis-private\\.yourcompany\\.com#s3://yourcloud-satis/satis-private#&#39; packages.json &amp;gt; packages-cloud.json\ns3cmd put index.html s3://yourcloud-satis/satis-private/index.html\ns3cmd put packages-cloud.json s3://yourcloud-satis/satis-private/packages.json\ncd /srv/data/satis-private/\ns3cmd sync ./dist s3://6cloud-satis/satis-private/\n\n\n(do the same for satis-public).\n\nupdate your projects\n\nIn your projects, edit the composer.json and replace the repositories entry by\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-private/&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-public/&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nEnable the AWS plugin in EC2 servers\n\nAdd our repositories in ~./composer/composer.json file of the user used to deploy your code.\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-private/&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-public/&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nYou have to install the S3 plugin for composer on your EC2 instance.\n\n$ composer global require &quot;naderman/composer-aws:~0.2.5&quot;\n\n\nIf you donâ€™t use IAM roles, add the following composer config on your EC2 servers (~/.composer/config.json) :\n\n{\n    &quot;config&quot;: {\n        &quot;amazon-aws&quot;: {\n            &quot;key&quot;:    &quot;KEYYYYYY&quot;,\n            &quot;secret&quot;: &quot;seeeeecret&quot;\n        }\n    }\n}\n\n\ncomposer install --prefer-dist command will now download all the packages files from S3 !\n\nThanks to Pierre and Jeremy for their help.\n\nFound a typo or bad english langage, just propose a pull request.\n"
} ,
  
  {
    "title"    : "JenkinsLight, mettez en lumiÃ¨re vos jobs Jenkins",
    "category" : "",
    "tags"     : " outil, jenkins, ci, cytron, open-source",
    "url"      : "/jenkinslight-mettez-en-lumiere-vos-jobs-jenkins",
    "date"     : "November 20, 2013",
    "excerpt"  : "Lâ€™idÃ©e de JenkinsLight a germÃ© lorsque nous nous sommes fait taper sur les doigts pour la troisiÃ¨me fois (Ã  juste titre) parce que lâ€™on avait dÃ©sactivÃ© la publicitÃ© sur nos sites de chaÃ®ne lors dâ€™une mise en production. Or la publicitÃ© est un poin...",
  "content"  : "Lâ€™idÃ©e de JenkinsLight a germÃ© lorsque nous nous sommes fait taper sur les doigts pour la troisiÃ¨me fois (Ã  juste titre) parce que lâ€™on avait dÃ©sactivÃ© la publicitÃ© sur nos sites de chaÃ®ne lors dâ€™une mise en production. Or la publicitÃ© est un point critique car directement reliÃ©e au chiffre dâ€™affaires. Le pire est que nous testions dÃ©jÃ  le bon fonctionnement de la publicitÃ© en intÃ©gration continue sur nos serveurs de preprod, avant la mise en production. Mais une configuration lÃ©gÃ¨rement diffÃ©rente sur les serveurs de prod rendait le nouveau code instable. Cette situation rend donc impossible la dÃ©tection de certaines anomalies avant la mise en productionâ€¦\n\nDâ€™oÃ¹ notre besoin dâ€™avoir un tableau de bord nous permettant de vÃ©rifier chaque instant la disponibilitÃ© des fonctionnalitÃ©s nÃ©vralgiques de nos sites en production afin de rÃ©agir au plus vite en cas de problÃ¨mes. Et ce, avant mÃªme que lâ€™anomalie ne nous soit remontÃ©e par les autres secteurs. Nous avions dÃ©jÃ  nos tests dans Jenkins que nous avons alors fait pointer vers la prod. Il nous manquait donc juste une sorte de â€œPanic Boardâ€ sur un Ã©cran placÃ© au sein de nos bureaux nous remontant rapidement le moindre problÃ¨me sur nos sites en production.\n\nNous avons crÃ©Ã© JenkinsLight qui permet dâ€™afficher distinctement le statut des jobs dâ€™une vue Jenkins en quasi temps rÃ©el. Le projet utilise AngularJS et lâ€™API de Jenkins pour rÃ©cupÃ©rer les informations nÃ©cessaires. Lâ€™installation se fait sur nâ€™importe quel serveur web et requiert uniquement Bower pour installer les composants. Afin de permettre Ã  lâ€™API dâ€™Ãªtre appelÃ©e en crossdomain (CORS), il est Ã©galement nÃ©cessaire dâ€™installer un plugin spÃ©cifique sur votre serveur Jenkins.\n\nLâ€™application propose quelques variables de configuration Ã©ditables dans le fichier â€œapp/scripts/config.jsâ€ permettant de spÃ©cifier :\n\n\n  lâ€™url du serveur Jenkins,\n  lâ€™identification au serveur (si nÃ©cessaire),\n  la vue Jenkins par dÃ©fault,\n  les types de jobs affichÃ©s,\n  une regexp pour exclure certains jobs,\n  le nombre maximum de jobs par ligne sur lâ€™Ã©cran,\n  lâ€™intervalle de rafraÃ®chissement (en millisecondes),\n  une image de fond quand il nâ€™y a aucun job Ã  afficher.\n\n\nJenkinsLight est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 3",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-3",
    "date"     : "November 19, 2013",
    "excerpt"  : "\n\nDerniÃ¨re journÃ©e de cette Velocity Europe, avec en plus du track Performance et Ops, lâ€™ouverture dâ€™un track Culture.\n\nPour rappel, si vous les avez ratÃ©s, les CR des journÃ©es prÃ©cÃ©dentes sont Ã  retrouver ici :\n\n\n  CR Velocity Europe 2013 - Day 1...",
  "content"  : "\n\nDerniÃ¨re journÃ©e de cette Velocity Europe, avec en plus du track Performance et Ops, lâ€™ouverture dâ€™un track Culture.\n\nPour rappel, si vous les avez ratÃ©s, les CR des journÃ©es prÃ©cÃ©dentes sont Ã  retrouver ici :\n\n\n  CR Velocity Europe 2013 - Day 1\n  CR Velocity Europe 2013 - Day 2\n\n\nPour ce â€œDay 3â€, nous nous retrouvons tous dans la grande salle avec la vidÃ©o â€œSlow Motion Water Balloon Fightâ€ en guise dâ€™introduction :\n\n\n\nExtreme Image Optimisation: WebP &amp;amp; JPEG XR\n\nIdo Safruti (Akamai) @safruti\n\n\n\nA ce jour, dâ€™aprÃ¨s HTTP Archive, 62% du poids des pages Web correspond aux images sur desktop, 65% sur mobile.\n\nNous utilisons toujours des technos vieilles de plus de 15 ans ! Jpg, Png, Gif â€¦\n\n\n  â€œDeploying new image formats on the web is HARD (but doable)â€ Ilya Grigorik\n\n\nLa confÃ©rence traite de deux formats bien plus rÃ©cents :\n\n\n  WebP (2011)\n  JXR : Jpeg eXtended Range (2009)\n\n\nqui supportent le lossless et lossy, ainsi que la transparence (en lossless et lossy aussi)\n\nOn retrouve un tableau trÃ¨s intÃ©ressant sur une comparaison taille entre les diffÃ©rents formats sur un mÃªme niveau de qualitÃ© :\n\n\n\nAttention, certaines fois (quelques %), lâ€™image peut Ãªtre plus grosse quâ€™en jpg. Si vous partez du Jpg pour la compression, comparez les tailles et affichez le Jpg sâ€™il est plus petit.\n\nLe support de ces formats reste toutefois minime :\n\n\n  WebP : Chrome &amp;gt;= 23, Opera &amp;gt;= 12, â€¦\n  Jpeg XR : IE =&amp;gt; 10, â€¦\n\n\nJXR, gÃ¨re notamment le progressive, ce que ne gÃ¨re pas encore date WebP. Plus dâ€™infos sur les â€œprogressives Jpegâ€ sur le blog de Patrick Meenan (CrÃ©ateur de WebPageTest).\n\nUne petite anecdote intÃ©ressante sur WebP aussi. Facebook avait mis en place WebP mais est revenu en arriÃ¨re car les utilisateurs rÃ¢laient ! Quand ils enregistraient ou partageait une photo de Chrome (donc en WebP), les utilisateurs IE notamment, ne pouvaient pas la consulter â€¦\n\nIdo fera un article sur lâ€™incontournable calendrier de lâ€™avant sur le sujet : Performance Calendar en dÃ©cembre 2013 !\n\n\n\nSLOWING DOWN TO GO FASTER: Responsive Web Design And The Problem Of Agility vs Robustness\n\nTom Maslen (BBC News) @tmaslen\n\n\n\nGros retour dâ€™expÃ©rience des Ã©quipes de BBC News sur leur approche du â€œResponsive Web Designâ€, et sur la maniÃ¨re dont cela a impactÃ© leurs workflows, ainsi que leur culture.\n\nLe RWD prend du temps, beaucoup plus de temps (3x), sur le Design, le dÃ©veloppement, le test.\n\nTom parcourt les optimisations â€œclassiquesâ€, ainsi que la maniÃ¨re dont ils enrichissent lâ€™expÃ©rience : Ils dÃ©livrent une â€œCore Experienceâ€ Ã  tous, et une â€œEnhanced Experienceâ€ aux navigateurs qui le supportent, utilisent Grunt pour certaines automatisations (pour fournir les bonnes images Ã  la bonne taille https://github.com/BBC-News/Imager.js/, versionner les assets https://github.com/kswedberg/grunt-version â€¦).\n\nVous pouvez aussi dÃ©couvrir Wraith, leur outil de comparaison de screeshot Responsive.\n\nBref, une excellente confÃ©rence avec un trÃ¨s bon speaker (trÃ¨s drÃ´le sur la fin).\n\n\n  â€œDonâ€™t do whoopsies on other people thingsâ€ Tom Maslen\n\n\n\n\nAn Introduction to Code Club\n\nJohn Wards (White October)\n\n\n\nCode Club est un projet leadÃ© par des bÃ©nÃ©voles pour crÃ©er des clubs de coding dans les Ã©coles, pour des enfants entre 9 et 11 ans. Plus de 1400 clubs ont dÃ©jÃ©tÃ© crÃ©Ã©s en Angleterre !\n\nLes enfants utilisent le projet Scratch, et qui permet via un langage de programmation assez simple, de programmer des jeux.\n\nLeurs vidÃ©os de prÃ©sentation sont de plus assez fun, notamment celle ci, qui nous Ã  Ã©tÃ© montrÃ©e, Ã  6mn50 dans la vidÃ©o ci dessous :\n\n\n\nLightning Demo: Automating WebPagetest with wpt-script\n\nJonathan Klein (Etsy) @jonathanklein\n\n\n\nAfin dâ€™automatiser la prise de mesure synthÃ©tique Ã  lâ€™aide de WebPageTest (notamment si vous avez installÃ© une instance privÃ©e), les gars dâ€™Etsy ont dÃ©veloppÃ© un wrapper Php Ã  lâ€™Api de Webpagetest. Le wrapper permet aussi de pousser les rÃ©sultats dans un Graphite ou un Splunk.\n\nLâ€™outil est dispo sur Github : Wpt-Script\n\n\n\nLightning Demo: Introducing a New RUM Resource From SOASTA\n\nBuddy Brewer (SOASTA)\n\n\n\nSOASTA, sociÃ©tÃ© connue notamment pour avoir rachetÃ© LogNormal (outil de R.U.M. lâ€™annÃ©e derniÃ¨re), propose aujourdâ€™hui un outil de R.U.M. nommÃ© : mPulse.\n\nIls ont publiÃ© de nombreuses statistiques sur leur site, sur les performances, suivant le navigateur, la localitÃ© etc : https://www.soasta.com/summary/\n\n\n\nLightning Demo: Automating The Removal Of Unused CSS\n\nAddy Osmani (Google Chrome) @addyosmani\n\n\n\nLâ€™un des petits problÃ¨mes rÃ©currents du dÃ©veloppement web est situÃ© dans nos fichiers Css. A force dâ€™ajout de fonctionnalitÃ©s ou de framework (notamment les fameux frameworks Css, Bootstrap &amp;amp; co), on finit par obtenir des fichiers CSS gigantesques, dans lesquels il devient trÃ¨s compliquÃ© de savoir ce qui est utilisÃ© ou pas sur votre site.\n\nAddy prÃ©sente des solutions quâ€™on peut retrouver :\n\n\n  pour un nettoyage mono page, dans la DevTools de Chrome (Onglet Audit puis Run puis â€œRemove Unused Css Rulesâ€)\n  pour un nettoyage dâ€™un site complet, via des outils autour de Grunt, notamment Grunt Uncss https://github.com/addyosmani/grunt-uncss fait par Addy en personne, basÃ© sur le module Uncss de Giakki\n\n\n\n\nLearning from the Worst of WebPagetest\n\nRick Viscomi (Google)\n\n\n\nRick travaille pour YouTube chez Google, comme WebDÃ©veloppeur Front-end orientÃ© performance.\n\nSa passion, se moquer des mauvais rÃ©sultats sur les historiques du WPT public :-)\n\nCâ€™est dâ€™ailleurs pour lui, lâ€™une des bonnes sources pour dÃ©couvrir les â€œanti-patternsâ€ de la perf, et les choses Ã  ne pas faire.\n\nIl prÃ©sente une Pull Request en cours sur WebPageTest avec le Multi Variate Testing, permettant de tester tout un site, sur plusieurs localitÃ©s. Plus dâ€™infos sur lâ€™article de son blog sur le sujet : https://jrvis.com/blog/wpt-mvt/\n\nAre todayâ€™s good practices â€¦ tomorrows performance anti-patterns\n\nAndy Davies @andydaviesâ€‹\n\n\n\nAvec lâ€™arrivÃ©e dâ€™HTTP 2.0, on se demande, si les optimisations WebPerf que nous rÃ©alisons aujourdâ€™hui ne seront pas gÃªnantes demain : Les dataURI, le JS inline, le domain sharding, les sprites â€¦\n\nLes rÃ©ponses ne sont pas aussi simples, et nous â€œdÃ©veloppeursâ€ nous devons de nous poser les questions afin dâ€™avoir les bonnes rÃ©ponses avant lâ€™arrivÃ©e dâ€™HTTP 2.0. Andy Ã  le mÃ©rite de lancer le dÃ©bat, via des protocoles de test pour chacun des cas. en comparant HTTP 1.0 et SPDY.\n\n\n\nProvisioning the Future - Building and Managing High Performance Compute Clusters in the Cloud\n\nMarc Cohen, Mandy Waite (Google)\n\nMarc et Mandy nous ont prÃ©sentÃ© le Google Cloud, alternative AWS. BasÃ© sur du KVM hautement modifiÃ© par les Ã©quipes de Google, on retrouve grossiÃ¨rement des services identiques (stockage Ã©lastique persistent, SDN pour le rÃ©seau, load-balancing, des profils de machines highmem ou highcpu). Toutefois on notera lâ€™absence dâ€™un marketplace pour les images des VMs et seules Debian et Centos sont disponibles. Ã‰norme avantage par rapport AWS: la facturation la minute au bout de 15min ! Mandy nous a fait la dÃ©mo du lancement de 1000 Vms en 2min15. Google fournit une api complÃ¨te et un outil en ligne de commande pour piloter absolument tout: gcutil.\n\nSecurity Monitoring (With Open Source Penetration Testing Tools)\n\nGareth Rushgrove (Government Digital Service)\n\nCombien dâ€™entre nous testent la sÃ©curitÃ© de leur applicatif en continu ? Elle devrait pourtant faire partie de lâ€™assurance qualitÃ© du dÃ©veloppement dâ€™un logiciel. Gareth propose donc dâ€™ajouter des tests de sÃ©curitÃ© via Jenkins et des tests unitaires dans notre pipeline de dÃ©veloppement. Parmis la liste dâ€™outils (rkhunter, naxsi, logstash, fail2ban, auditd, la distrib BackTrack, clamav, Arachni) certains sont aisÃ©ment intÃ©grables au workflow. A tester le trÃ¨s bon OWASP ZAP (et https://www.dvwa.co.uk/ pour se faire la main !).\n\nLes slides :\n\n\n\nBeyond Pretty Chartsâ€¦. Analytics for the cloud infrastructure\n\nToufic Boubez (Metafor Software) @tboubez\n\nToufic travaille depuis 20 ans dans la gestion des donnÃ©es des datacenters et la dÃ©tection dâ€™anomalies. Comme lors de la prÃ©sentation de Twitter il explique quâ€™on ne peut pas appliquer ces donnÃ©es temporelles des mÃ©thodes statistiques classiques (holt winter forecast, rÃ©gression linÃ©aire, smooth splines), car elles sont non stationnaires (violant ainsi le principe dâ€™homogÃ©nÃ©itÃ©) et la plupart du temps elles ne sont pas distribuÃ©s normalement (principal prÃ©-requis). Il nous a donc prÃ©sentÃ© le test de Kolmogorov-Smirnov couplÃ© aux techniques de bootstraping qui permet dâ€™avoir des prÃ©dictions assez fiables. Comme les modÃ¨les ARIMA, il fait partie de la famille des mÃ©thodes statistiques non paramÃ©triques, qui ne prÃ©supposent pas de la distribution des donnÃ©es)\n\nLes slides :\n\n\n\nAutomated Multi-Platform Golden Image Creation, Unlocking New Potential\n\nMitchell Hashimoto (HashiCorp) @mitchellh\n\nAvoir un environnement stable, clonable depuis la dev vers la prod est le rÃªve conjoint des dÃ©veloppeurs et sysadmins. Lâ€™utilisation des images pour le dÃ©ploiement de machines et de code peut Ãªtre fastidieux, au moindre changement de version il faut instancier lâ€™image, faire la modification, recrÃ©er lâ€™image etcâ€¦Ce qui nâ€™est pas forcement adaptÃ© au cloud computing et la virtualisation. A lâ€™inverse nâ€™utiliser que des logiciels de gestion de la configuration (Cfengine, Puppet, Chef) ne certifie pas quâ€™un serveur vierge aura le mÃªme comportement quâ€™un serveur sur lequel on aura passÃ© 10.000 modifications. Il mâ€™arrive rÃ©guliÃ¨rement dâ€™avoir des erreurs de run puppet cause dâ€™une dÃ©pendance non satisfaite (packages) ou de problÃ¨mes rÃ©seau, (le plus souvent on souffre de la lenteur dâ€™application dâ€™un profil puppet).\n\nDe plus passÃ© du VMWare du AWS ou Vagrant demande dâ€™avoir autant dâ€™images que de plateforme ! Ce saint graal du â€œserveur immuableâ€ et agnostique de la plateforme est possible en utilisant une mÃ©thode intermÃ©diaire : Packer permet de crÃ©er des images (Aws, Virtualbox, VMWare) partir dâ€™une source et avec lâ€™aide dâ€™un chef/puppet/cfengine. Le workflow proposÃ© est le suivant: commit dans le repository =&amp;gt; build avec Packer =&amp;gt; CI (Jenkins) =&amp;gt; Image ready !\n\nCela permet de garder la flexibilitÃ© dâ€™un Puppet avec lâ€™idempotence des images et leur facilitÃ©, rapiditÃ© de dÃ©ploiement.\n\nLâ€™orchestration peut Ãªtre rÃ©alisÃ©e avec Serf, il implÃ©mente un protocole Gossip (tout le monde se parle, mais pas en mÃªme temps). Câ€™est un agent installÃ© sur le serveur qui gÃ¨re des messages et des handlers (scripts personnalisÃ©s dans le langage de votre choix). Dans lâ€™exemple de dÃ©ploiement de multiples load balancer, lâ€™image va permettre dâ€™avoir un systÃ¨me fonctionnel rapidement, les utilisateurs, les logiciels et câ€™est Serf qui rÃ©cupÃ©rera la configuration appliquer au load balancer.\n\nOn peut aller plus loin en dÃ©ployant les code avec Docker, ce qui ajoute une couche dâ€™abstraction supplÃ©mentaire extrÃªmement puissante.\n\nDOM to Pixels: Accelerate Your Rendering Performance\n\nPaul Lewis (Google - Team Chrome) @aerotwist\n\nPaul Lewis explique quelques principes mis en oeuvre lors du rendu graphique dans Chrome tel que la gestion des calques qui permet dâ€™utiliser plus intensÃ©ment la puissance du GPU mais dont la multiplication peut sâ€™avÃ©rer contre productive : la gestion de trop nombreux calques par le CPU contrebalance la performance du rendu par le GPU (Ã©videmment, sinon cela serait trop simple).\n\nPour bien apprÃ©hender cette prÃ©sentation, il mâ€™a semblÃ© nÃ©cessaire dâ€™avoir un petit background dans la programmation graphique : savoir, par exemple, pourquoi une ombre ou un flou sont couteux pour le rendu (cause des calculs entre les diffÃ©rentes zones de mÃ©moire contenant les informations graphiques superposer).\n\nPaul prÃ©sente ensuite en dÃ©tail lâ€™outil de debug du rendu dans les WebTools : comment enregistrer en temps rÃ©el les diffÃ©rentes frames affichÃ©es par Chrome et visualiser les diffÃ©rents temps de calculs.\n\nCette prÃ©sentation, bien que trÃ¨s intÃ©ressante par son contenu fut aussi mise en valeur par lâ€™interprÃ©tation de Paul Lewis : toujours prÃ©cise mais simple, sÃ©rieuse et fun la fois.\n Ce fÃ»t pour moi, la meilleure prÃ©sentation (show!) de la VÃ©locity.\n\nEt nâ€™oubliez pas :\n\n\n  â€œTools, not rulesâ€ Paul Lewis\n\n\nConclusion :\n\nEpuisÃ© par trois jours de confÃ©rence dâ€™une densitÃ© incroyable, La Velocity a encore aisÃ©ment tenu toute ses promesses.\n\nNous espÃ©rons que ces comptes-rendus vous auront Ã©tÃ© utile, autant que les prÃ©sentations nous lâ€™ont Ã©tÃ©.\n\nNâ€™hÃ©sitez pas Ã  commenter lâ€™un des CR pour donner votre avis, sur le CR, ou sur certains points couvert par les Talks.\n\nMerci.\n\nVous pouvez retrouver :\n\n\n  quelques vidÃ©os de la confÃ©rence sur Youtube\n  les slides sur le site dâ€™Oreilly\n  et les photos ici sur Flickr : https://www.flickr.com/photos/oreillyconf/sets/72157637657689424/\n\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 2",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-2",
    "date"     : "November 16, 2013",
    "excerpt"  : "\n\nDe retour Ã  lâ€™hÃ´tel Hilton de Londres, afin de commencer cette deuxiÃ¨me journÃ©e qui sâ€™annonce trÃ¨s chargÃ©e : jusquâ€™4 tracks en parallÃ¨le. Performance, Mobile, Ops, et Sponsors.\n\nMaking Government digital services fast\n\nPaul Downey @psd\n\n\n\nPaul e...",
  "content"  : "\n\nDe retour Ã  lâ€™hÃ´tel Hilton de Londres, afin de commencer cette deuxiÃ¨me journÃ©e qui sâ€™annonce trÃ¨s chargÃ©e : jusquâ€™4 tracks en parallÃ¨le. Performance, Mobile, Ops, et Sponsors.\n\nMaking Government digital services fast\n\nPaul Downey @psd\n\n\n\nPaul est un â€œTechnical Architectâ€ pour le gouvernement anglais. Il nous explique comment ils gÃ¨rent et priorisent les problÃ©matiques de performances pour offrir des services internet centrÃ©s sur les besoins des utilisateurs avant ceux du gouvernement.\n\nAvec notamment une rÃ©duction drastique du nombre de pages, ce qui leur a permis dâ€™obtenir plus de visites au final !\n\nLe tout est entiÃ¨rement documentÃ© en ligne, en accÃ¨s public, et regorge dâ€™informations intÃ©ressante que vous pouvez retrouver sur le Gov.Uk Service Manual.\n\n\n\nStand down your smartphone testing army\n\nMitun zavery (keynote)\n\n\n\nMitun travaille chez Keynote, et fait une dÃ©monstration de deux de leurs outils :\n\nKeynote DA Free (DA = Device Anywhere)\n\nLâ€™outil trÃ¨s intÃ©ressant propose un grand nombre dâ€™appareils mobiles quâ€™on peut acquÃ©rir pendant 10 minutes, afin de lancer des tests. Le gros intÃ©rÃªt est quâ€™on parle ici de vrais appareils, pas de simulateurs.\n\nLe service est disponible sur : https://dafree.keynote.com aprÃ¨s vous Ãªtres inscrit gratuitement sur cette url https://www.keynotedeviceanywhere.com/da-free-register.html\n\n(Le service ne fonctionne pas sur Chrome mac pour ma part)\n\nCâ€™est plutÃ´t impressionnant techniquement, on lance les applications que lâ€™on souhaite, rentre du texte, change lâ€™orientation â€¦ ! A mÃ©moriser.\n\nMITE, le deuxiÃ¨me outil qui Ã  lâ€™air trÃ¨s complet permet dâ€™aller beaucoup plus loin, mais avec des simulateurs cette fois : https://mite.keynote.com/download.php\n\nDommage que lâ€™on oublie les Macs dans lâ€™histoire.\n\n\n\nTesting all the way to production\n\nSam adams (lmax exchange) @LMAX\n\n\n\nPour ceux qui ne seraient pas encore convaincu de lâ€™intÃ©rÃªt des tests automatisÃ©s, Lmax exchange (site sur lâ€™univers boursier gÃ¨rant des sommes dâ€™argent assez phÃ©nomÃ©nales) prÃ©sente le workflow de dÃ©veloppement basÃ© sur les tests pour dÃ©ployer du code le plus souvent possible en Ã©vitant au maximum les rÃ©gressions.\n\n\n\nMost of the time we measure the performance of others\n\nklaus enzenhofer (compuware) @kenzenhofer\n\n\n\nCourte prÃ©sentation de Compuware qui Ã©dite des solutions de monitoring, et aussi Dynatrace Ajax avec une Ã©tude de cas assez simple sur la dÃ©tection dâ€™anomalies sur un site (142 domaines de 3rd party chargÃ©s !).\n\nLe blog de Compuware regorge dâ€™article en tout genre sur la Webperf.\n\n\n\nMaking performance personal at Ft labs\n\nAndrew Betts @triblondon\n\n\n\nLes Ã©quipes du Financial Times sont trÃ¨s actifs dans le domaine de la webperf, avec notamment lâ€™outil FastClick qui permet dâ€™enlever le delay du touch sur mobile (entre 100 et 300 ms!). Ils developpent aussi une webapp html5 trÃ¨s riche, et expliquent comment rendre la problÃ©matique de performance importante aux yeux du â€œproduitâ€. On apprend pas mal dâ€™astuces pour mesurer la performance, gÃ©rer le appcache, Ã©viter les sprites etc â€¦\n\nVous pouvez retrouvez les slides ici : https://triblondon.github.io/talk-makingperfpersonal/#/\n\n\n\nLightning demo : Global web page performance\n\nJames Smith (Devopsguys) @thedevmgr\n\n\n\nJames est venu prÃ©senter lors dâ€™une lightning dÃ©mo : worldwidepagetest.com\n\nUn outil permettant de tester partout dans le monde les performances de son site, basÃ© sur Webpagetest, et les locations et browsers disponible.\n\nEn plus de lâ€™Ã©chec total de la dÃ©mo (bug/plantage â€¦ â€œworst demo everâ€ dâ€™aprÃ¨s le speaker lui mÃªme), lâ€™outil qui parait intÃ©ressant sur le papier me semble une fausse bonne idÃ©e et le risque de saturer les instances de WPT mondial Ã  cause de ce type dâ€™outil me parait bien plus gÃªnant que les avantages quâ€™il apporte.\n\nLightning demo: HTTP Archive, BigQuery, and you!\n\nIlya Grigorik @igrigorik\n\n\n\nLâ€™impressionnante quantitÃ© de donnÃ©es agrÃ©gÃ©es par HTTP Archive est maintenant disponible dans Google BigQuery : toutes les donnÃ©es statistiques sur les requÃªtes et rÃ©ponses HTTP de plusieurs centaines de milliers de site diffÃ©rents sont donc simplement requÃªtables et disponibles la vitesse de la lumiÃ¨re (câ€™est une expression Ã  la mode en ce moment par ici).\n\nUn article dâ€™Ilya explique la marche suivre pour utiliser les donnÃ©es de HTTP Archive stockÃ©es sur BigQueries : https://www.igvita.com/2013/06/20/http-archive-bigquery-web-performance-answers/\n\nNous pouvons ainsi aisÃ©ment effectuer quelques comparaisons avec nos â€œconcurrentsâ€ et nÃ©anmoins amis prÃ©sents la confÃ©rence :-) : https://denisroussel.fr/httparchive-bigquery-french-test.html\n\nIlya prÃ©sente aussi le site communautaire BigQueri.es (Powered by Discourse), permettant de partager les rÃ©ponses des questions statistiques sur les bases prÃ©sentes dans BigQuery (notamment celle de HTTP Archive)\n\n\n\nGimme More! Enabling User Growth in a Performant and Efficient Fashion\n\nArun Kejariwal (Twitter) @arun_kejariwal, Winston Lee (Twitter Inc.) @winstl\n\nLa planification de la capacitÃ© (â€œcapacity planningâ€) chez Twitter passe par lâ€™utilisation de modÃ¨les statistiques et la prÃ©diction sur des donnÃ©es temporelles. Câ€™est absolument nÃ©cessaire pour le dimensionnement des plateformes techniques.\n\nLâ€™utilisation dâ€™une simple rÃ©gression linÃ©aire capture la tendance globale, mais ne prends pas en compte la saisonnalitÃ© ni les pics de trafic (positifs ou nÃ©gatifs). Un modÃ¨le â€œsmooth splinesâ€ correctement paramÃ©trÃ©, de part son design, ne le permet pas non plus. Idem pour le Holt Winters (que vous pouvez tester avec graphite). Ils utilisent donc le modÃ¨le ARIMA (Autoregressive integrated moving average), qui permet dâ€™effectuer des prÃ©dictions partir de donnÃ©es temporelles non stationaires (câ€™est dire que la moyenne et la variance change = pics de trafic). Le nettoyage des donnÃ©es et la vÃ©rification du modÃ¨le reprÃ©sente la majoritÃ© du travail. Les donnÃ©es journaliÃ¨res permettent de prÃ©dire jusquâ€™90 jours, et les donnÃ©es la minute un trimestre. Les prÃ©dictions sur 1 mois des mÃ©triques systÃ¨mes (cpu, ram) sont considÃ©rÃ©es comme fiables alors que les mÃ©triques business (nombre dâ€™utilisateurs, nombres de photos ou vidÃ©os stockÃ©es) le sont pour 3 ou 4 mois. Pour les Ã©vÃ¨nements exceptionnels (superbowl, nouvelle annÃ©e) ces prÃ©dictions ne sont pas assez fiables, ils se basent donc simplement sur les annÃ©es prÃ©cÃ©dentes.\n\nWhen dynamic becomes static: the next step in web caching techniques\n\nWim Godden (Cu.Be Solutions)\n\nLe monsieur effectue dâ€™abord un rÃ©capitulatif des pratiques de cache dans le web depuis son commencement : sans cache, avec du cache applicatif, avec du reverse proxy cache et enfin avec beaucoup trop de systÃ¨mes de cache qui rende lâ€™architecture trÃ¨s complexe (tiens tiens).\n\nPuis apparaissent les ESI, câ€™est en gros du reverse proxy cache par bloc (en rÃ©alitÃ©, ils sont parmi nous depuis bien longtemps). Mais une limitation conceptuelle Ã©vidente borne leur utilisation : les sites sont trÃ¨s souvent personnalisÃ©s en fonction de lâ€™utilisateur (affichage du nom de lâ€™utilisateur connectÃ© par exemple). Et du coup, les blocs personnalisÃ©s, mÃªme simples, ne peuvent bÃ©nÃ©ficier du reverse proxy cache. Ce qui dÃ©fie un peu le concept.\n\nPour pallier Ã  ce problÃ¨me, Wim et son Ã©quipe ont dÃ©veloppÃ© un langage spÃ©cifique dans Nginx (qui est aussi un reverse proxy en plus dâ€™Ãªtre un serveur http) permettant au serveur web de gÃ©rer des variables directement dans le reverse proxy afin que celui-ci les stock dans son propre memcache et puisse y accÃ©der pour retourner la page au client sans faire un appel supplÃ©mentaire au serveur web : SCL.\n\nAlors oui. Câ€™est pas forcÃ©ment lâ€™idÃ©al de commencer poser des variables dans le reverse proxy. Mais nâ€™ayons crainte, ce nâ€™est pas pour tout de suite : la release publique ne devrait pas arriver avant mi-2014 :-)\n\nLes slides\n\nDeveloper-Friendly Web Performance Testing in Continuous Integration\n\nMichael Klepikov (Google, Inc)\n\nIntÃ©grer les mesures/tests de rÃ©gressions de performance dans nos outils dâ€™intÃ©grations continues et une tÃ¢che trÃ¨s compliquÃ©. Michael prÃ©sente une approche assez maligne consistant utiliser les tests fonctionnels dÃ©jen place, pour rÃ©colter les mesures des outils de R.U.M. dÃ©jÃ  prÃ©sent sur le site (soit parce que les mesures sont prÃ©sentes dans lâ€™url dâ€™appel de lâ€™outil de R.U.M.), soit en rÃ©cupÃ©rant les valeurs dans les DevTools de Chrome.\n\nLâ€™outil TSviewDB permet dâ€™avoir une interface qui agrÃ¨ge plusieurs time-series sur une seule time-series (plus dâ€™infos dans le Readme du projet).\n\nPas mal dâ€™informations Ã  creuser dans les slides, comme lâ€™envoi de donnÃ©e directe WebPageTest pour utiliser lâ€™UI sur le rÃ©sultat, ou la faÃ§on de rÃ©cupÃ©rer les infos de la DevTools de Chrome en vidÃ©o\n\nLes slides :\n\n\n\nIntegrating multiple CDN providers at Etsy\n\nMarcus Barczak (Etsy), Laurie Denness (Etsy)\n\nPour des raisons de haute disponibilitÃ©, de rÃ©silience et de balance des coÃ»ts, Etsy a mis en place depuis 2012 un systÃ¨me qui leur permet dâ€™utiliser de multiples CDN (parmi eux Akamai, Fastly, et EdgeCast). Leur critÃ¨res dâ€™Ã©valuations sont le hit ratio et la dÃ©charge de trafic des serveurs origine, le reporting, le pilotage via des APIs, la personalisation et lâ€™accÃ¨s aux logs HTTP. Ils ont pour cela dÃ» faire du mÃ©nage dans leur codes et dans leurs headers (cache-control, expires, etag, last-modified)\n\nIls ont commencÃ© par les images (1% puis 100% du trafic), et se sont servis des CDNs pour effectuer leur tests A/B. Lâ€™equilibrage de charge entre les CDNs se fait manuellement via une interface web ou via un outil en ligne de commande quâ€™ils ont mis disposition de la communautÃ© (cdncontrol sur leur github). Lâ€™inconvÃ©nient de cette solution est la multiplication des requetes DNS (puisquâ€™ils utilisent des CNAME type img1.etsystatic.com =&amp;gt; global-ssl.fastly.net par exemple), la non atomicitÃ© et le delais des modifications DNS qui engendrent une long tail importante et le debug plus complexe. Les requetes depuis les CDNs vers les origines sont trackÃ©s via un header HTTP et sont monitorÃ©s dans un graphite surveillÃ© par un nagios selon un seuil dÃ©terminÃ©.\n\n\n  â€œIf you can do it at the origin, do it !â€.\n\n\n\n\nWhat is the Velocity of an Unladen Swallow? A quest for the Holy Grail.\n\nPerry Dyball (Seatwave Ltd) @perrydyball\n\nRetour dâ€™XP trÃ¨s utile, plein dâ€™honnÃªtetÃ© et dâ€™humilitÃ©, de SeatWave (site permettant dâ€™acheter des tickets concerts/spectacle etc), sur lâ€™effet douloureux de la premiÃ¨re pub tÃ©lÃ© quâ€™ils ont achetÃ© pour promouvoir leur service. On dÃ©couvre la faÃ§on dont ils ont su optimiser leur application pour supporter les publicitÃ©s suivantes, grÃ¢ce un systÃ¨me de queuing avec un dÃ©compte en cas de fortes charges, ainsi que les impacts sur une multitude de mÃ©triques et le cotÃ© financier.\n\nLe phÃ©nomÃ¨ne est presque un running gag chez nous (ou mÃªme sur Twitter), quand votre (ou mÃªme lâ€™un de nos â€¦) site est prÃ©sentÃ© dans une pub ou Capital, et que votre application et/ou serveur ne supporte pas la charge.\n\nBref, encore un bel exemple de culture dâ€™entreprise, qui dÃ©montre que la performance nâ€™est pas un projet ou une feature â€œone shotâ€, mais une culture et une mentalitÃ© constante.\n\n\n  â€œPerformance itâ€™s not just for today, itâ€™s for every dayâ€ Peter Dyball\n\n\n\n\nGetting 100B metrics to disk\n\nJonathan Thurman (New Relic)\n\nNew relic a prÃ©sentÃ© lâ€™architecture MySQL qui stocke leur 196 milliards de mÃ©triques journaliÃ¨res. Elle est basÃ©e sur des shards MySQL, propulsÃ©e par de puissants serveurs (12 actuellement) Ã©quipÃ©s de SSD Intel et de shelf de disques Dell. Les shards sont fait (via leur API shardGuard) par numÃ©ro de client, et les tables MySQL sont construites sur le modÃ¨le numÃ©roClient_year_julianDay_metricResolution. Il y a environ 200.000 tables par databases. Les mÃ©triques sont rÃ©gulierement (toutes les heures), purgÃ©es et aggrÃ©gÃ©es, en utilisant le innodb_lazy_drop_table de percona 5.5 (et surtout par delete from ou drop table).\n\nLe code initialement en Ruby est passÃ© Java/Jetty. Les inserts se font sÃ©quentiellement en batch de 5000 et sont buffurisÃ©s en RAM (ils doivent se faire en moins dâ€™une minute, rÃ©solution minimale du produit). Ils prÃ©voient dâ€™utilisÃ© de multiples instances MySQL par serveurs et de gonfler leur capacitÃ© hardware (SSD 800G, 96G RAM)\n\n\n\nHigh Velocity Migration\n\nJoshua Hoffman (SoundCloud) @oshu\n\nJoshua nous a contÃ© lâ€™histoire dâ€™une startup (fictive mais pas vraiment car câ€™est celle de Tumblr), qui a commencÃ© en 2006 entre deux amis souhaitant partager des images de parties dâ€™Ã©checs et comment en 2012 elle a dÃ», en 6H de maintenance, basculer 1200 serveurs et les donnÃ©es de 50M dâ€™utilisateurs. Il nous a dÃ©taillÃ© lâ€™Ã©volution annÃ©e par annÃ©e de lâ€™infrastructure et du nombre de devs/sysadmins. Les ingrÃ©dients pour gÃ©rer une croissance comme celle ci sont selon lui : le provisionning automatique (ipxe, kickstart), la gestion de la configuration (puppet/chef/ansible), le monitoring et lâ€™alerting et les outils de dÃ©ploiement de code. Il prÃ©cisÃ© quâ€™il faut accepter lâ€™imperfection de ses outils, ne pas chercher rÃ©inventer la roue mais plutÃ´t utiliser lâ€™open source, ne pas hÃ©siter tuer les projets Zombies (ceux qui durent depuis trop longtemps et qui nâ€™ont pas les fonctionnalitÃ©s attendues !) et surtout respecter le principe KISS (keep it simple and stupid).\n\nLa migration en 2012 de leur plateforme gÃ©rÃ©e par une sociÃ©tÃ© tierce vers leur propre infrastructure a commencÃ© 120 jours plus tÃ´t avec lâ€™installation des serveurs, machines, systÃ¨mes de dÃ©ploiements, lâ€™acquisition de leur numÃ©ro dâ€™AS, et lâ€™utilisation en front dâ€™un proxy pour plus tard pouvoir rediriger le trafic de faÃ§on transparente vers la nouvelle infrastructure. Le jour J la fenetre de maintenance du site de 6H Ã©tÃ© suffisante pour synchroniser les donnÃ©es utilisateurs entre les deux infras, tester et mettre en production.\n\n\n\nCode is Evil\n\nDan Rathbone (British Sky Broadcasting)\n\nFace aux problÃ¨mes de performance du site https://www.skybet.com/, qui doit probablement attirer une quantitÃ© impressionnante de parieurs tout en devant afficher des donnÃ©es trÃ¨s fraÃ®ches (cela fait partie du business model), Dan a remis plat toute la logique de dÃ©veloppement du site.\n\nLorsque la performance passe seule au premier plan, il est ainsi possible de renverser le paradigme du dÃ©veloppement dans son ensemble : alors quâ€™en gÃ©nÃ©ral, les donnÃ©es sont stockÃ©es structurÃ©es puis extraites pour peupler du code mÃ©tier puis affichÃ©es par des templates Ã©laborÃ©s, dans ce cas particulier, les donnÃ©es sont directement stockÃ©es de maniÃ¨res dÃ©normalisÃ©es, directement prÃªtes Ãªtre affichÃ©es par des templates simplistes. Le code mÃ©tier est en amont et sert prÃ©-calculÃ© les donnÃ©es qui sont stockÃ©es en base.\n\nIl est ainsi possible de minimiser drastiquement la quantitÃ© de code critique. Et cela ouvre beaucoup de portes : peu de code = peu de maintenance, aucun framework nÃ©cessaire, aucun cache nÃ©cessaire, etc.\n\nCâ€™Ã©tait une prÃ©sentation assez polÃ©mique mais particuliÃ¨rement intÃ©ressante (ce qui nâ€™Ã©tait pas lâ€™avis de lâ€™audience, semblerait-il) et rafraÃ®chissante car elle permet de sortir des cas standards du monde du web. Entre nous, tous ces principes Ã©taient dÃ©jen vogue dans le dÃ©veloppement des jeux vidÃ©o dans les annÃ©es 90 : nous devions constamment contourner la limitation du matÃ©riel (les optimisations Ã©taient au cycle processeur prÃ¨s).\n\nBreaking 1000ms Mobile Barrier\n\nIlya Grigorik (Google)\n\n\n\nComment arriver afficher sa page web sans dÃ©passer la barriÃ¨re de 1000 ms ! Un dur challenge dont les Ã©preuves sont dÃ©taillÃ©s par Ilya.\n\n\n\nDes problÃ¨matiques de latence sur le â€œTouchâ€ mobile, sur les communications 3G/4G, du fonctionnement TCP, du critical rendering path au niveau CSS et JS, mod_page_speed et ngx_page_speed, ainsi que des Ã©volutions venir sur Page Speed Insights, câ€™est un panel ultra complet de la WebPerf qui Ã©tÃ© couvert sur cette heure ultra dense, mais oh combien indispensable. Câ€™est donc, comment souvent avec Ilya Grigorik, un must read absolu pour ceux que la Performance Front-End et Mobile, ainsi que la latence, passionne.\n\nLes slides sont ici https://docs.google.com/presentation/d/1wAxB5DPN-rcelwbGO6lCOus_S1rP24LMqA8m1eXEDRo/present#slide=id.p19\n\nLive Sketching !\n\nAvant de conclure, petit hommage Natalia Talkowska , qui, sur chaque confÃ©rence, rÃ©alisait un live sketching dâ€™une qualitÃ© incroyable\n\n@Natalka_Design #livesketching is back with @allspaw @souders and @courtneynash opening up #velocityconf, let&amp;#39;s go! pic.twitter.com/FYBQIVk8tr&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @psd at #velocityconf as first #keynote! pic.twitter.com/9zAMXJZNWW&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @keynotesystems at #velocityconf pic.twitter.com/S8XYaNFKzU&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @LMAX at #velocityconf pic.twitter.com/UYLYDTSuPO&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @kenzenhofer at #velocityconf pic.twitter.com/l2ndwj8V1G&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nMust follow: @NatiTal: #livesketching @psd at #velocityconf as first #keynote! pic.twitter.com/W8xTTjC581 #Awesomeness&amp;mdash; Mike Hendrickson (@mikehatora) November 14, 2013\n\n\n#livesketching @triblondon at #velocityconf pic.twitter.com/VTF2gZFEsH&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @thedevmgr at #velocityconf pic.twitter.com/iaaZgRxVwN&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nNot a bad likeness! â€œ@NatiTal: #livesketching @triblondon at #velocityconf pic.twitter.com/Pq4NiAPEW7â€&amp;mdash; Andrew Betts (@triblondon) November 14, 2013\n\n\n#livesketching @igrigorik at #velocityconf pic.twitter.com/5rBPelS9an&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @edgecast at #velocityconf last #keynote pic.twitter.com/rZ5Pa1MvhQ&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nConclusion :\n\nCâ€™est complÃ¨tement lessivÃ© que nous sortons de cette journÃ©e, avec une quantitÃ© dâ€™idÃ©es / projets tester incroyable.\n\nVous pouvez retrouver le compte rendu de la premiÃ¨re journÃ©e ainsi que de la derniÃ¨re sur notre Blog.\n\nNâ€™hÃ©sites pas donner vos retours (positifs ou nÃ©gatifs en commentaire). Merci :-)\n\nÂ© des photos : Flickr officiel Oâ€™Reilly\n\nCR rÃ©digÃ© par Baptiste, Denis Roussel et Kenny Dits\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 1",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-1.html",
    "date"     : "November 14, 2013",
    "excerpt"  : "Introduction :\n\n\n\nNous voici de retour Ã  Londres pour la troisiÃ¨me Ã©dition de la VÃ©locity Europe, qui se dÃ©roule, pour la deuxiÃ¨me fois Ã  Londres (la prÃ©cÃ©dente Ã©tait Ã  Berlin).\n\nPour rappel, la VÃ©locity est la confÃ©rence autour de la performance ...",
  "content"  : "Introduction :\n\n\n\nNous voici de retour Ã  Londres pour la troisiÃ¨me Ã©dition de la VÃ©locity Europe, qui se dÃ©roule, pour la deuxiÃ¨me fois Ã  Londres (la prÃ©cÃ©dente Ã©tait Ã  Berlin).\n\nPour rappel, la VÃ©locity est la confÃ©rence autour de la performance web. Quâ€™elle soit Front-End, Back-End, DÃ©v ou Ops. Câ€™est lâ€™Ã©vÃ©nement de lâ€™annÃ©e Ã  ne pas manquer en Europe, ou aux US (ou Chine) pour les plus chanceux\n\nCette premiÃ¨re journÃ©e (ayant eu lieu le 13 novembre 2013) est axÃ©e sur le signe des â€œTutorialsâ€. De looongues confÃ©rences de 90 minutes dont voici le compte rendu Ã©crit Ã  6 mains.\n\nLa confÃ©rence â€œclassiqueâ€ commence le 14 et se dÃ©roulera sur deux journÃ©es.\n\nGone in 60 frames per second\n\nAddy Osmani (Google Chrome) @addyosmani\n\n\n\nAddy est une figure incontournable du web. CrÃ©ateur de TodoMVC, Lead dÃ©v de Yeoman et travail dans la Google Chrome Team sur les outils Ã  destination des dÃ©veloppeurs autour du navigateur.\n\nAprÃ¨s la gÃ©nÃ©ration du code html par les serveurs et le transfert de ce code par les rÃ©seaux, le rendu graphique de la page par le navigateur est le dernier Ã©vÃ¨nement significatif du chargement de la page lors de la consultation dâ€™un site par un client.\n\nVoici donc un rÃ©sumÃ© des bonnes pratiques permettant dâ€™obtenir un meilleur framerate (nombre de rafraÃ®chissement de la page par seconde) et ainsi une meilleure fluiditÃ© lors de la navigation :\n\n\n  disposer des images Ã  la bonne taille pour Ã©viter les redimenssionnements Ã  la volÃ©e,\n  limiter les handlers sur lâ€™Ã©vÃ©nement onScroll(),\n  limiter tous les Ã©lÃ©ments â€˜fixedâ€™ car cela force le navigateur Ã  recalculer constamment la zone affichÃ©e (ou utiliser lâ€™astuce translateZ(0)),\n  \n    limiter les directives CSS qui nÃ©cessites un calcul supplÃ©mentaire (lorsque tout est dÃ©jÃ  affichÃ©) :\n  \n  les ombres,\n  les flous,\n  et les dÃ©gradÃ©s : (Bootstrap a supprimÃ© tous les dÃ©gradÃ©s sur ses boutons : +100% de rapiditÃ© lâ€™affichage).\n\n\nEnsuite, il reste quelques conseils plus gÃ©nÃ©raux :\n\n\n  Il faut se souvenir que les performances des tÃ©lÃ©phones ne sont pas celles des PC,\n  un framerate de 60 fps est parfait (câ€™est dÃ» au matÃ©riel), mais un framerate de 30 fps peut aussi Ãªtre suffisant pour peu quâ€™il soit constant,\n\n\nEnfin, comme souvent, tous les outils pour comprendre et amÃ©liorer le rendu graphique de ses pages web sont disponible dans tous les navigateurs. Dans Chrome, il suffit dâ€™aller dans la section â€œFramesâ€ de lâ€™onglet â€œTimelineâ€ des DevTools.\n\nLes slides sont disponible ici : https://speakerdeck.com/addyosmani/velocityconf-rendering-performance-case-studies\n\nLa prÃ©sentation de la confÃ©rence par lâ€™auteur lui-mÃªme : https://addyosmani.com/blog/making-a-site-jank-free/\n\n\n\n\n\nVidÃ©o de la mÃªme confÃ©rence (donnÃ©e Ã  la Smashing Conf 2013)\n\nBring the noise : Making effective use of a quarter million metrics\n\nJon Cowie (Etsy) @jonlives\n\n\n\nJon est â€œOps Engineerâ€ chez Etsy (Dont le VP, John Allspaw, co-organise avec Steve Souders, la VÃ©locity).\n\nQuelques donnÃ©es sur Etsy :\n\n\n  Ils font du dÃ©ploiement continu\n  1.5 milliards de pages vues\n  250 contributeurs (tout le monde dÃ©ploie du code, mÃªme les chiens)\n  ils utilisent Deployinator pour dÃ©ployer leur code avec un unique â€œboutonâ€, et schemanator pour les migrations SQL\n  60 dÃ©ploiements par jour / 8 commit par deploiement\n  Â¼ millions de mÃ©triques !\n\n\n\n  â€œWe optimize for quick recovery by anticipating problems instead of fearing human errorâ€ John Cowie\n\n\n\n  â€œCanâ€™t Fix what you donâ€™t measureâ€ W. Edwards Deming\n\n\nLeurs outils pour le monitoring :\n\n\n  \n    Not homemade :\n  \n  Ganglia\n  Graphite\n  Nagios\n  \n    Homemade :\n  \n  StatsD : Simple Daemon for easy stats integration\n  Supergrep : Real time log streamer\n  Skyline : A real time anomaly detection system\n  Oculus : A metric correlation component\n\n\n\n  â€œNot All things that break throw errorsâ€ Oscar Wilde\n\n\n\n  â€œIf it moves, graph it ! If it doesnâ€™t move, graph it anywayâ€ Jon Cowie\n\n\nLa prÃ©sentation sâ€™axe ensuite plus particuliÃ¨rement sur la stack â€œKaleâ€, qui englobe deux outils que lâ€™on va dÃ©tailler : Skyline et Oculus. Voir lâ€™article sur le blog technique de Etsy https://codeascraft.com/2013/06/11/introducing-kale/\n\nLâ€™objectif de Skyline, est de dÃ©tecter les comportements anormaux (gros pics par exemple), avec pour principal challenge, la rÃ©cupÃ©ration des donnÃ©es (via le â€œrelay agentâ€ de Graphite, ils envoient en continue les donnÃ©es dans Redis via redis.append() ), le stockage de 250 000 mÃ©triques (dans Redis) au format MessagePack. Oculus quand lui permet de corrÃ©ler les mÃ©triques, en utilisant les donnÃ©es brutes de lâ€™api de Graphite, car il est bien plus efficace de comparer des chiffres, que des images â€¦\n\nIl nâ€™y a pas un mais huit algorithmes de dÃ©tections dâ€™anomalies qui sont utilisÃ©s dans un vote Ã  majoritÃ©, dÃ©terminant ainsi si lâ€™anomalie est avÃ©rÃ©e (parmi ceux ci, OLS, Grubbâ€™s test, lâ€™histogramme bining etcâ€¦). La dÃ©tection se fait sur une fenÃªtre dâ€™une heure et une seconde de 24 heures. Skyline souffre encore de quelques faiblesses: lâ€™absence de prise en compte de la saisonnalitÃ©, les pics qui peuvent en cacher dâ€™autres plus faibles, le postulat pas toujours vrai que les donnÃ©es sont normalement distribuÃ©es et les corrÃ©lations nÃ©gatives.\n\nIls comparent donc la distance euclidienne (slide 99), en gÃ©rant aussi le dÃ©calage temporel (dynamic time warping / DTW) (voir slide 100).\n\nLa partie la plus intÃ©ressante est la simplification dâ€™une mÃ©trique temporelle, en la normalisant sur une courbe Ã©chelle rÃ©duite (de 0 Ã  25), et en la transformant en une chaine textuelle comportant cinq valeurs :\n\n\n  sharpdecrement\n  decrement\n  flat\n  increment\n  sharpincrement\n\n\nEt ceci en fonction de la valeur en cours par rapport Ã  la valeur prÃ©cedente.\n\nIls poussent toutes ces mÃ©triques normalisÃ©es dans Elastic Search dans un champ non tokenisÃ© en rÃ©alisant des recherches de phrases afin de corrÃ©ler les mÃ©triques ayant le mÃªme pattern et en scorant via un plugin codÃ© par leurs soins (incluant une version â€œrapideâ€ du DTW).\n\nUne fois les mÃ©triques corrÃ©lÃ©es affichÃ©es, il est possible de sauvegarder un snapshot de ces derniÃ¨res et dâ€™inclure des commentaires dans une â€œcollectionâ€. Cela permet notamment de construire une base de donnÃ©es de connaissance sur les incidents ou les comportements anormaux mais explicables.\n\nSkyline est visible par tous dans leur bureaux, sur lâ€™un des 6 Ã©crans de dashboards, devant lesquels on peut notamment lire le nombre de requÃªtes HTTP par seconde, le top 10 des pages, les temps de gÃ©nÃ©rations et dâ€™affichage etcâ€¦\n\nLes slides sont disponible ici : https://www.slideshare.net/jonlives/bring-the-noise\n\n\n\nResponsive images Technique and Beyond\n\nYoav Weiss (WL Square) @yoavweiss\n\n\n\nYoav est un spÃ©cialiste de la WebPerf et travaille sur les problÃ©matiques des images liÃ©es au Responsive Web Design. Il est aussi Technical Lead au RICG (Responsive images community Group)\n\nLe principal problÃ¨me des images responsive, câ€™est de charger lâ€™image correctement dimensionnÃ©e par rapport Ã  une page, de maniÃ¨re efficace.\n\n72% des sites RWD servent les mÃªmes ressources entre les rÃ©solutions petites et grandes â€¦\n\nOn peut Ã©conomiser 72% en taille dâ€™image en compressant correctement (voir https://timkadlec.com/2013/06/why-we-need-responsive-images/).\n\nYoav a dÃ©veloppÃ© un outil utilisant PhantomJs, permettant de mesurer la diffÃ©rence entre les images chargÃ©es, et celle qui seraient correctement dimensionnÃ©es : Sizer Soze\n\nOn aborde ensuite les deux cas principaux gÃªnant :\n\n\n  Servir une dimension diffÃ©rentes de lâ€™image Ã  diffÃ©rents support. (et les Retina uniquement aux devices le supportant)\n  et le â€œArt directionâ€, avoir une image qui correspond au layout\n\n\nAinsi que lâ€™intÃ©rÃªt du Pre-loader, souvent peu connu. Beaucoup plus dâ€™infos sur cet article dâ€™Andy Davies (https://andydavies.me/blog/2013/10/22/how-the-browser-pre-loader-makes-pages-load-faster/)\n\nYoav parcours ensuite toutes les techniques des images responsive avec avantages/inconvÃ©nients et exemple pour chacune, que vous pouvez retrouvez dÃ¨s la slide 57 de la prÃ©sentation ci aprÃ¨s : https://yoavweiss.github.io/velocity-eu-13-presentation/#/\n\nLâ€™Ã©tude et les retours sont extrÃªmement complet, et immanquable, si vous travaillez ou allez travaillez sur le sujet. Il aborde aussi une approche en cours dâ€™Ã©tude, qui verra peut Ãªtre le jour prochainement (Responsive Image Container).\n\nPerformance Analysis of JVM components for non-specialists\n\nBen Evans (JClarity) @kittylyst\n\n\n\nLa performance et la complexitÃ© des applications qui fonctionnent sur la JVM ont suivi lâ€™Ã©volution de la loi de Moore. MalgrÃ© que nous ayons gagnÃ© de la puissance et des transistors, notre code sâ€™est complexifiÃ© dâ€™annÃ©e en annÃ©e et dâ€™autant plus avec le boom dâ€™Internet.\n\nLe tuning de la JVM est indispensable pour avoir une application performante et doit se faire de faÃ§on rigoureuse et scientifique, il faut comprendre, mesurer, tester, vÃ©rifier et rÃ©pÃ©ter ce processus jusquâ€™ce que lâ€™on considÃ¨re la performance comme bonne.\n\nBen a ensuite dÃ©taillÃ© lâ€™anatomie de la JVM, les spÃ©cificitÃ©s du langage Java, les â€œmid 90â€™s decisions designâ€ qui ont Ã©tÃ© faites, comment est gÃ©rÃ© lâ€™allocation mÃ©moire, la heap, et le fonctionnement du garbage collector (mark and sweep, stop the world). La durÃ©e du â€œstop the worldâ€ est ridicule comparÃ© aux temps de latence rÃ©seau, ceux engendrÃ©s par lâ€™hyperviseur etcâ€¦\n\nIl a prÃ©sentÃ© quelques optimisations indispensables selon lui, et a insistÃ© sur le fait que lâ€™optimisation prÃ©maturÃ©e pouvait Ãªtre la source de bien des soucis cotÃ© code.\n\nTuning Network Performance to Eleven\n\nIlya Grigorik (Google) @igrigorik\n\n\n\nAKA comment condenser un livre dans un tutorial dâ€™1H30. Exercice encore plus difficile lorsquâ€™il faut rÃ©sumer le rÃ©sumÃ© dâ€™un livre aussi dense et complet. Ilya en tant que spÃ©cialiste de la webperf a examinÃ© les mÃ©canismes de la latence et de la bande passante, le fonctionnement du protocole TCP, la gestion de congestion, les problÃ¨mes structurels de HTTP 1.0 et HTTP 1.1, lâ€™impact de TLS (le chiffrement) sur les performances. Il a donnÃ© ses recommandations pour optimiser TCP et bien utilisÃ© TLS.\n\nâ€œbandwidth + latence =~ performanceâ€\n\n\n  â€œVideo streaming is bandwidth limited, web browsing is latency limitedâ€ Ilya Grigorik\n\n\nIl a ensuite expliquÃ© comment fonctionne le rÃ©seau radio 2G/3G/4G et les contraintes que ces architectures exercent sur les temps de chargement et la durÃ©e de vie des batteries pour les appareils mobiles.\n\nLe tutorial sâ€™est achevÃ© sur les dÃ©fauts de HTTP 1.1 et les nouveautÃ©s (nombreuses et sexys) dâ€™HTTP 2.0. Ce fut extrÃªmement plaisant dâ€™assister Ã  cette prÃ©sentation, tant Ilya est pointu techniquement, prÃ©cis et didactique dans ses dÃ©monstrations. Le livre est un MUST-READ !\n\nIl est dâ€™ailleurs disponible gratuitement ici : https://chimera.labs.oreilly.com/books/1230000000545\n\nLes slides sont disponible ici\n\nBe Mean to your code with Gauntlt and the Rugged Way\n\nJames wickett (Mentor Graphics) @wickett\n\n\n\nCette prÃ©sentation fut le seul et unique vrai â€œWorkshopâ€ du jour, dans le sens oÃ¹ une machine virtuelle (monter avec Vagrant) Ã©tait fournie pour rÃ©aliser lâ€™atelier au fur et mesure de la prÃ©sentation sur sa machine.\n\nGauntlt est un framework autour de la sÃ©curitÃ©, qui fournie des hooks pour de nombreux outils dâ€™attaques (Xss, Sql injection etc â€¦).\n\nAprÃ¨s une introduction un peu longue autour de la place de la â€œsÃ©curitÃ©â€ aux seins de nos services.\n\nLâ€™approche de Gauntlt est basÃ©e sur le â€œRugged Manifestoâ€\n\nGauntlt permet donc dâ€™automatiser au sein de son systÃ¨me dâ€™intÃ©gration continue, des tests autour de la sÃ©curitÃ© de son applicatif et de son infra, basÃ©s sur Cucumber, utilisant le langage Gherkin (que certains connaissent peut Ãªtre mieux dans le monde php via Behat), et interfaÃ§ant des outils tels que :\n\n\n  Garmr\n  Nmap\n  Arachni\n  Sqlmap\n  â€¦\n\n\nSi vous voulez tester lâ€™outil, qui Ã  lâ€™air trÃ¨s prometteur, vous pouvez suivre ce tutoriel : https://bit.ly/gauntlt-demo-instructions qui vous fourni la Virtual Box, les consignes dâ€™installations, et les exemples ayant Ã©tÃ© rÃ©alisÃ©s pendant la confÃ©rence, ainsi quâ€™une application de test en Ruby Railsgoat pour servir de cible Ã  vos tests.\n\nLes slides sont disponible ici\n\n\n\nHands-on Web Performance Optimization Workshop\n\nAndy Davies (Asteno) @andydavies , Tobias Baldauf (Freelancer) @tbaldauf\n\n\n\nDerniÃ¨re session de la journÃ©e, avec Andy et Tobias, sur un workshop axÃ© Performance Web.\n\nOn commence par une prÃ©sentation gÃ©nÃ©ral dâ€™un outil quâ€™on ne devrait plus prÃ©senter : WebPageTest, lâ€™outil principal pour les problÃ©matiques de performances front-end.\n\nAndy aborde ensuite quelques autres outils :\n\n\n  PhantomJs (un headless browser)\n  Simple Website Speed Test\n  et surtout Phantomas, un module PhantomJs pour collecter les mÃ©triques de Webperf.\n  le wrapper Node.Js pour WebPageTest de Marcel Duran\n  SiteSpeed.io pour monitorer toutes les pages de son site, basÃ© notamment sur Yslow\n  HttpArchive, lâ€™excellent service de Steve Souders qui tracke le web avec une multitude de stats intÃ©ressante, que vous pouvez dâ€™ailleurs installer pour une instance privÃ©e afin de tracker vos sites : https://bbinto.wordpress.com/2013/03/25/setup-your-own-http-archive-to-track-and-query-your-site-trends/ \\o/\n\n\nLa suite de la confÃ©rence consister a analyser en live certains sites dont quelques uns assez hilarant au niveau performance :\n\n\n  Dailymail.co.uk avec ces +de 800 requÃªtes HTTP et 7 mo !\n  Wildbit.com qui consomme un CPU Ã©norme cause de lâ€™animation sur le logo quâ€™on ne voit quasiment pas :)\n\n\nLes slides :\n\n\n\nConclusion :\n\nBonne premiÃ¨re journÃ©e avec ce format â€œTutorialsâ€ un peu trop touffu (90 minutes par confÃ©rence â€¦). DÃ©jÃ  des tonnes dâ€™idÃ©es qui ressortent, on a hÃ¢te de voir la suite.\n\nRetrouvez les autres CR :\n\n\n  Compte rendu du jour 2 \n  Compte rendu du jour 3 \n\n\nÂ© des photos : Flickr officiel Oâ€™Reilly\n\nCR rÃ©digÃ© par Baptiste, Denis Roussel et Kenny Dits\n"
} ,
  
  {
    "title"    : "Tester fonctionnellement une API REST",
    "category" : "",
    "tags"     : " qualite, symfony, atoum, tests fonctionnels",
    "url"      : "/2013/10/tester-fonctionnellement-une-api-rest-symfony-doctrine-atoum",
    "date"     : "October 14, 2013",
    "excerpt"  : "Un des enjeux des tests fonctionnels est de pouvoir Ãªtre jouÃ©s dans un environnement complÃ¨tement indÃ©pendant, dissociÃ© de lâ€™environnement de production, afin de ne pas Ãªtre tributaires de donnÃ©es versatiles qui pourraient impacter leur rÃ©sultat. ...",
  "content"  : "Un des enjeux des tests fonctionnels est de pouvoir Ãªtre jouÃ©s dans un environnement complÃ¨tement indÃ©pendant, dissociÃ© de lâ€™environnement de production, afin de ne pas Ãªtre tributaires de donnÃ©es versatiles qui pourraient impacter leur rÃ©sultat. Il faut, cependant, que cet environnement soit techniquement similaire Ã  celui de production pour que les tests aient une rÃ©elle validitÃ© fonctionnelle.\n\nAvec la Team Cytron, nous sommes tombÃ©s face Ã  cette problÃ©matique lorsque nous avons voulu tester fonctionnellement un service agnostique de contenu mettant Ã  disposition une API REST et utilisant Symfony2, MySQL, Doctrine et atoum.\n\nMonter un serveur de donnÃ©es dÃ©diÃ© aux tests\n\nDans le cas dâ€™une application utilisant MySQL, on pense alors monter un serveur applicatif de test reliÃ© Ã  une base de donnÃ©es de test. Plusieurs problÃ¨mes peuvent alors dÃ©couler dâ€™un tel systÃ¨me :\n\n\n  il faut Ãªtre en mesure de pouvoir mettre en Å“uvre un serveur MySQL dÃ©diÃ© uniquement aux tests,\n  mais surtout cette architecture nâ€™est pas exploitable pour exÃ©cuter des tests de maniÃ¨re concurrentielle (ce qui pose problÃ¨me pour lâ€™intÃ©gration continue). En effet, des collisions apparaÃ®traient en base de donnÃ©es et le rÃ©sultat des tests ne seraient plus exploitables.\n\n\nMocker Doctrine\n\nNotre seconde rÃ©action a Ã©tÃ© de vouloir mocker Doctrine pour devenir indÃ©pendant de MySQL. Lourde tÃ¢che.\n\nTant bien que mal, nous sommes arrivÃ©s Ã  un rÃ©sultat plutÃ´t satisfaisant car notre API rÃ©alise des opÃ©rations simples : ajout, modification, suppression et consultation avec un filtrage Ã©lÃ©mentaire.\n\nLa premiÃ¨re chose Ã  faire est de sâ€™assurer que notre serveur de test nâ€™accÃ¨de pas aux donnÃ©es de production dans MySQL en changeant la configuration Doctrine dans le fichier config_test.yml.\n\n\n\nEnsuite, nous avons crÃ©Ã© une classe abstraite dont hÃ©ritent toutes nos classes de test, et qui permet dâ€™initialiser le mock de Doctrine.\n\nAutant vous dire que le dÃ©veloppement de cette classe a Ã©tÃ© fastidieux car incrÃ©mental : chaque nouveau besoin de manipulation de donnÃ©es dans nos tests, il a fallu modifier le mock pour prendre en compte des mÃ©thodes ou des fonctionnalitÃ©s de mÃ©thodes qui nâ€™avaient pas Ã©tÃ© encore mockÃ©es (comme le filtrage par critÃ¨res dans la fonction findBy).\n\nLes possibilitÃ©s de ce mock reste limitÃ©es. Nous sommes, par exemple, tombÃ©s sur le cas oÃ¹ deux managers de donnÃ©es en relation (des recettes et leurs ingrÃ©dients) dÃ©pendaient dâ€™un mÃªme EntityManager Doctrine : tel que nous lâ€™avons dÃ©veloppÃ©, le mock ne sait pas gÃ©rer cette situation et engendre des erreurs Ã  lâ€™exÃ©cution. Il aurait fallu refactoriser le code pour parvenir Ã  nos fins et passer encore plus de temps sur ce projetâ€¦ et nous nâ€™en avions pas beaucoup !\n\nAutre problÃ¨me : nous utilisons des fonctionnalitÃ©s de la librairie Gedmo/DoctrineExtensions pour la gestion automatique des dates de crÃ©ation et de modification. Ã‰videmment, elles ne sont pas opÃ©rationnelles avec notre mock et nous aurions encore dÃ» dÃ©velopper pour faire passer nos tests.\n\nUtiliser les transactions de Doctrine\n\nIl a donc fallu nous rendre lâ€™Ã©vidence : cette solution ne correspondait pas Ã  nos besoins ! Nous avons alors Ã©mis lâ€™hypothÃ¨se dâ€™une alternative qui nous permettrait peut-Ãªtre de nous passer dâ€™une config spÃ©cifique MySQL pour nos tests : lâ€™utilisation des transactions via Doctrine.\n\nAu dÃ©but de chaque test, nous aurions ouvert une transaction mais qui nâ€™aurait jamais Ã©tÃ© commitÃ©e par la suite, Ã©vitant toute interaction avec la base de donnÃ©es de production. Mais avec cette solution, dangereuse Ã  mettre en place et Ã  maintenir, nous aurions couru le risque de modifier des donnÃ©es de production.\n\nRemplacer MySQL par un autre SGBD uniquement pour les tests\n\nFinalement, nous sommes partis sur une autre piste, celle qui fait actuellement tourner nos tests fonctionnels sur ce projet. Nous utilisons SQLite dans notre environnement de test la place de MySQL. Ce SGBD est trÃ¨s lÃ©ger et simple mettre en Å“uvre : pas besoin dâ€™une installation sur un serveur dÃ©diÃ©, il suffit simplement dâ€™activer une extension de PHP. SQLite se base sur des fichiers physiques pour gÃ©rer le stockage des donnÃ©es. Ainsi, chaque build de test peut avoir ses propres fichiers de BDD dans son rÃ©pertoire Ã©vitant toute collision dans le cas de tests concurrentiels.\n\nNous avons donc configurÃ© Doctrine, pour quâ€™il utilise SQLite lors de son exÃ©cution en environnement de test en modifiant le config_test.yml\n\n\n\nComme pour le mock de Doctrine, nous avons mis en place une classe abstraite qui permet de gÃ©rer la rÃ©initialisation de la base pour chaque test.\n\nNous pouvons donc maintenant tester unitairement et fonctionnellement notre API REST dÃ©veloppÃ©e en PHP lâ€™aide de Symfony2 et Doctrine. Et nous ne nous en privons pas : notre API est couverte par bientÃ´t 5.000 assertions.\n\nGÃ©nÃ©ration des donnÃ©es de test\n\nAprÃ¨s avoir trouvÃ© une solution pour lâ€™accÃ¨s Ã  la structure de donnÃ©es en environnement de test, nous nous sommes penchÃ©s sur la question du contenu de ces donnÃ©es de tests. Pas longtemps.\n\nNotre service REST permettant des opÃ©rations CRUD, nous partons pour chaque test dâ€™un contenu vide que nous remplissons Ã  lâ€™aide de notre propre service. Cela permet de tester beaucoup plus de cas dâ€™utilisation. Mais surtout cela permet aussi de tester des cas plus rÃ©els, plus proches de son utilisation par nos clients.\n"
} ,
  
  {
    "title"    : "Distribuez votre vidÃ©o partout avec 3 euros en poche et devenez millionaire. Ou presque.",
    "category" : "",
    "tags"     : " lft, video",
    "url"      : "/2013/10/distribuez-votre-video-partout-avec-3-euros-en-poche-et-devenez-millionaire-ou-presque.html",
    "date"     : "October 9, 2013",
    "excerpt"  : "â€œComment gagner des millions, sans sortir de chez vous, en robe de chambre, en distribuant des vidÃ©os de chats sur les internets, grÃ¢ce Ã  ffmeg, h264, dash, tous pleins de buzz word, justin bieberâ€ (Merci ! Toute lâ€™Ã©quipe SEO).\n\nUne prÃ©sentation d...",
  "content"  : "â€œComment gagner des millions, sans sortir de chez vous, en robe de chambre, en distribuant des vidÃ©os de chats sur les internets, grÃ¢ce Ã  ffmeg, h264, dash, tous pleins de buzz word, justin bieberâ€ (Merci ! Toute lâ€™Ã©quipe SEO).\n\nUne prÃ©sentation de Ludovic Bostral, notre ex valeureux responsable R&amp;amp;D en charge - jusquâ€™il y a peu de temps - de la fabrication de toutes nos vidÃ©os et du SI associÃ©.\n\nSi vous vous posez des questions ce sujet, nâ€™hÃ©sitez pas Ã  venir lui faire un petit coucou virtuel, ou sur Nantes. Ca marche aussi pour discuter zombie ou nanar. Ou mieux, un nanar avec des zombies !\n\nRetrouvez Ludovic sur son site : https://digibos.com.\n\n"
} ,
  
  {
    "title"    : "Le NoSQL, Focus sur MongoDB par CÃ©dric Derue (Altran)",
    "category" : "",
    "tags"     : " lft, nosql, mongodb, video",
    "url"      : "/le-nosql-focus-sur-mongodb-par-cedric-derue-altran",
    "date"     : "October 8, 2013",
    "excerpt"  : "Porte-Ã©tandard des bases de donnÃ©es NoSQL de type document, MongoDB nous a Ã©tÃ© prÃ©sentÃ© cet Ã©tÃ© par CÃ©dric Derue (@cderue) , de la sociÃ©tÃ© Altran, lors de nos confÃ©rences internes.\n\nDans cette prÃ©sentation dâ€™environ une heure, il aborde un tour dâ€™...",
  "content"  : "Porte-Ã©tandard des bases de donnÃ©es NoSQL de type document, MongoDB nous a Ã©tÃ© prÃ©sentÃ© cet Ã©tÃ© par CÃ©dric Derue (@cderue) , de la sociÃ©tÃ© Altran, lors de nos confÃ©rences internes.\n\nDans cette prÃ©sentation dâ€™environ une heure, il aborde un tour dâ€™horizon des diffÃ©rentes catÃ©gories de bases de donnÃ©es NoSQL, pour sâ€™attacher ensuite sur un focus assez complet de MongoDB, agrÃ©mentÃ© de quelques dÃ©monstrations.\n\nMerci Ã  Altran et CÃ©dric pour le partage de cette prÃ©sentation.\n\nVous pouvez aussi retrouver dâ€™autres sessions de nos Last Friday Talk :\n\n\n  Introduction Drupal par Claire Roubey (Clever Age)\n  Redis on Fire\n  La POO Canada Dry\n\n"
} ,
  
  {
    "title"    : "Vigo, le flÃ©au des Carpates",
    "category" : "",
    "tags"     : " outil, qualite, javascript, tests fonctionnels",
    "url"      : "/vigo-le-fleau-des-carpates-la-tristesse-de-moldavie",
    "date"     : "August 13, 2013",
    "excerpt"  : "CasperJS permet dâ€™Ã©crire des scripts javascript qui vont automatiser des tests fonctionnels de pages web. Il exÃ©cute ces tests dans une instance de PhantomJS qui est un navigateur scriptable et sans interface graphique (â€œHeadlessâ€ dit-on dans le m...",
  "content"  : "CasperJS permet dâ€™Ã©crire des scripts javascript qui vont automatiser des tests fonctionnels de pages web. Il exÃ©cute ces tests dans une instance de PhantomJS qui est un navigateur scriptable et sans interface graphique (â€œHeadlessâ€ dit-on dans le milieu).\n\nAfin de mieux structurer nos tests, de faciliter leur Ã©criture et de pouvoir les lancer avec une commande unique, nous avons crÃ©Ã© VigoJS, une surcouche pour CasperJS.\n\nFonctionnalitÃ©s\n\nToutes les fonctionnalitÃ©s de base de CasperJS sont accessibles. Nous y avons simplement ajoutÃ© un mÃ©canisme de configuration contenant plusieurs paramÃ¨tres de base dont lâ€™URL de test par dÃ©faut, lâ€™authentification HTTP Ã©ventuelle ou encore la taille de lâ€™Ã©cran virtuel. Il est Ã©galement possible de spÃ©cifier des environnements (dev, preprod, prodâ€¦) pour diffÃ©rencier les comportements de certains tests. Ainsi, en fonction de lâ€™environnement demandÃ© dans la ligne de commande, les tests peuvent Ãªtre jouÃ©s sur des URL diffÃ©rentes avec la bonne authentification HTTP.\n\nQuelques fonctions utilitaires sont aussi disponibles pour rÃ©aliser rapidement certaines vÃ©rifications rÃ©currentes et ainsi faciliter le dÃ©veloppement des tests. On peut, par exemple, rechercher aisÃ©ment la prÃ©sence dâ€™erreurs ou warnings PHP dans une page. Il est aussi possible de faire un retry lorsquâ€™un test a Ã©chouÃ© afin dâ€™Ãªtre certain que ce nâ€™est pas une erreur du type â€œMySql server has gone awayâ€ qui peut se produire de temps en temps sur les serveurs de tests. Par ailleurs, quand un test Ã©choue, VigoJS exporte une capture dâ€™Ã©cran qui sâ€™avÃ¨re trÃ¨s pratique pour comprendre ce quâ€™il sâ€™est passÃ© !\n\nTous les paramÃ¨tres ajoutÃ©s Ã  la ligne de commande et dans la configuration sont injectÃ©s et accessibles dans la classe de test. On garde, de cette maniÃ¨re, une certaine flexibilitÃ©. Cela peut permettre, par exemple, de dÃ©couper les tests avec de la pagination :\n\n\n\n\n\nAffichage dans le terminal\n\nNous avons aussi amÃ©liorÃ© lâ€™affichage des rÃ©sultats des tests. Il est ainsi possible de prÃ©ciser pour chaque test : un titre et une description personnalisÃ©s afin de rendre les comptes-rendus plus comprÃ©hensible pour les utilisateurs. De mÃªme des commentaires utilisateurs peuvent Ãªtre ajoutÃ©s plus simplement dans le dÃ©roulement des tests.\n\n\n\nIntÃ©gration continue\n\nCasperJS gÃ©nÃ¨re nativement des rapports xUnit. VigoJS intÃ¨gre donc cette fonctionnalitÃ© pour Ãªtre utilisÃ© sur une plateforme dâ€™intÃ©gration continue comme Jenkins. Il est aussi possible de modifier le paramÃ¨tre classPath dans le fichier xUnit pour amÃ©liorer la lisibilitÃ© des rÃ©sultats :\n\n\n\nLe chemin dans lequel est gÃ©nÃ©rÃ© le rapport est configurable par lâ€™option â€“buildPath (ou dans la configuration) :\n\n\n\nIl suffit ensuite de configurer le job Jenkins pour quâ€™il rÃ©cupÃ¨re le rapport de test dans ce dossier. Sans oublier de faire un job pour tester les Pull Requests de votre projet.\n\nVigoJS est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Introduction Ã  la qualitÃ© logicielle avec Node.js",
    "category" : "",
    "tags"     : " nodejs, javascript, qualite",
    "url"      : "/introduction-qualite-logicielle-avec-node-js",
    "date"     : "August 12, 2013",
    "excerpt"  : "\n\n(Source : https://www.flickr.com/photos/dieselbug2007/414348333/)\n\nChez M6Web, nous avons rÃ©cemment rÃ©ecrit un de nos projets Node.js.\nLe monde Node.js Ã©volue trÃ¨s rapidement et a fait dâ€™Ã©norme progrÃ¨s dans le domaine de la qualitÃ© logicielle.\nN...",
  "content"  : "\n\n(Source : https://www.flickr.com/photos/dieselbug2007/414348333/)\n\nChez M6Web, nous avons rÃ©cemment rÃ©ecrit un de nos projets Node.js.\nLe monde Node.js Ã©volue trÃ¨s rapidement et a fait dâ€™Ã©norme progrÃ¨s dans le domaine de la qualitÃ© logicielle.\nNous avons donc decidÃ© de monter en qualitÃ© sur nos projets Node.js en utilisant les derniers outils proposÃ©s par la communautÃ©.\n\nPour cela, nous mesurons maintenant diffÃ©rentes mÃ©triques sur nos projets Node:\n\n\n  la qualitÃ© du code (checkstyle)\n  des tests unitaires et fonctionnels\n\n\net tout ceci est lancÃ© par notre serveur dâ€™intÃ©gration continue: Jenkins.\n\nTests unitaires\n\nPour tout ce qui est â€œtestsâ€, nous avons choisi le trÃ¨s bon duo :\n\n\n  Mocha\n  Chai\n\n\n\n\nMocha câ€™est un â€œtest-runnerâ€ javascript qui fonctionne aussi bien sur Node que dans un navigateur web. Plus simplement mocha est lâ€™outil qui va contenir nos tests: il va exÃ©cuter les tests et afficher les rÃ©sultats.\n\n\n\nChai est une librairie dâ€™assertion assez complÃ¨te, permettant plusieurs syntaxe :\n\n\n  assert.equal(foo, â€˜raoulâ€™);\n  foo.should.equal(â€˜raoulâ€™);\n  expect(foo).to.equal(â€˜barâ€™);\n\n\nCes deux outils fonctionnent aussi bien pour tester vos javascripts Node que front.\n\nCe duo permet une Ã©criture de test simple et trÃ¨s lisible, dont voici un exemple :\n\n\n\nTests fonctionnels\n\nPour les tests fonctionnels, nous avons choisi dâ€™utiliser Supertest, un package Node.js qui permet de simplifier lâ€™Ã©criture de requÃªte HTTP (une surcouche au package http disponible dans Node.js).\n\nCi-dessous, un exemple de tests fonctionnels :\n\n\n\nCheckstyle\n\nEn javascript, on peut aussi Ã©crire du code propre et respecter des conventions de codage.\n Afin de vÃ©rifier que notre code respecte les standards en vigueur, nous utilisons JsHint.\n\nIntÃ©gration continue\n\nToutes ces mÃ©triques sont rÃ©coltÃ©es grÃ¢ce Ã  Jenkins-CI Ã  lâ€™aide du fichier Ant suivant :\n\n\n\n\n\nLe rÃ©sultat de lâ€™intÃ©gration continue dans jenkins.\n\nConclusion\n\nNode.js propose des outils trÃ¨s performants pour la qualitÃ© logicielle, et Ã©crire des tests avec le duo â€œMocha + Chaiâ€ devient vite quelque chose de simple. Et mÃªme les dÃ©veloppeurs les plus rÃ©fractaires aux tests devraient apprÃ©cier.\n\nNâ€™hÃ©sitez pas Ã  commenter cet article et Ã  indiquez la solution que vous utilisez pour vos projets Node.\n\n"
} ,
  
  {
    "title"    : "Introduction Ã  Drupal par Claire Roubey (Clever Age)",
    "category" : "",
    "tags"     : " lft, drupal, video",
    "url"      : "/introduction-%C3%A0-drupal-par-claire-roubey-clever-age",
    "date"     : "July 19, 2013",
    "excerpt"  : "Drupal, le CMS trÃ¨s trÃ¨s connu mais que nous on connait pas ! A notre demande Clever Age, par lâ€™intermÃ©diaire de Claire Roubey, est venue nous prÃ©senter cet outil lors dâ€™un de nos fameux Last Friday Talk.\n\nMalheureusement, la vidÃ©o est coupÃ©e Ã  en...",
  "content"  : "Drupal, le CMS trÃ¨s trÃ¨s connu mais que nous on connait pas ! A notre demande Clever Age, par lâ€™intermÃ©diaire de Claire Roubey, est venue nous prÃ©senter cet outil lors dâ€™un de nos fameux Last Friday Talk.\n\nMalheureusement, la vidÃ©o est coupÃ©e Ã  environ la moitiÃ© de sa durÃ©e (fort dommage car les questions Ã©taient trÃ¨s intÃ©ressantes). Les slides sont toutefois disponibles : https://fr.slideshare.net/claire_/drupal-m6-web310513.\n\nUn Ã©norme merci Clever Age et Claire !\n"
} ,
  
  {
    "title"    : "LÃ¢che moi la branch !",
    "category" : "",
    "tags"     : " qualite, jenkins, github",
    "url"      : "/lache-moi-la-branch",
    "date"     : "July 15, 2013",
    "excerpt"  : "Test continu des Pull Requests\n\nMaintenant que nous utilisons GitHub Enterprise chez M6Web, nous avons la joie de pouvoir utiliser les Pull Requests de faÃ§on abusive. Mais leur puissance nâ€™est maximale que lorsquâ€™elles peuvent Ãªtre testÃ©es individ...",
  "content"  : "Test continu des Pull Requests\n\nMaintenant que nous utilisons GitHub Enterprise chez M6Web, nous avons la joie de pouvoir utiliser les Pull Requests de faÃ§on abusive. Mais leur puissance nâ€™est maximale que lorsquâ€™elles peuvent Ãªtre testÃ©es individuellement avant dâ€™Ãªtre mergÃ©es sur le master.\n\n\n\nPour ce faire, nous avons utilisÃ© le plugin GitHub Pull Request Builder de Jenkins, qui aprÃ¨s une configuration assez simple, nous a permis de crÃ©er un job qui lance automatiquement un build lorsquâ€™une Pull Request est modifiÃ©e. Ce build se positionne sur la branch pointÃ©e par la Pull Request et exÃ©cute les tests.\n\n\n\nIl est donc nÃ©cessaire de crÃ©er un job dÃ©diÃ© au test des Pull Requests pour chaque projet dont nous souhaitons voir les Pull Request automatiquement testÃ©es. Ã‡a peut paraÃ®tre Ã©vident, mais lorsquâ€™on a plus de 200 repositories, câ€™est tout de suite moins trivial.\n\nConfiguration du plugin\n\nLe fonctionnement par dÃ©faut du plugin GitHub Pull Request Builder est assez restrictif. Il nÃ©cessite quâ€™un contributeur ajoute un commentaire sur la Pull Request en demandant un test puis quâ€™un admin (parmi une liste Ã  configurer) rÃ©ponde avec un deuxiÃ¨me commentaire acceptant de lancer les tests (le tout avec des phrases types configurables). Câ€™est uniquement ensuite que Jenkins lancera un build.\n\nOr dans notre contexte dâ€™entreprise, nous souhaitons que lâ€™automatisation soit totale, comme dans Travis : chaque modification dâ€™une Pull Request lance lâ€™ensemble des tests. Pour arriver ce fonctionnement, il suffit de cocher â€œBuild every pull request automatically without asking (Dangerous!)â€ dans la section â€œAvancÃ©eâ€ des options de lancement de build par â€œGithub pull requests builderâ€.\n\nTest continu du master\n\nNous essayons tant que possible de suivre le workflow de dÃ©ploiement de GitHub : on dÃ©veloppe une fonctionnalitÃ© par branch, on fait une Pull Request sur le master et on ne merge que lorsque tout le monde est dâ€™accord et que les tests sont passÃ©s. Cela nous permet de garder le master toujours dÃ©ployable.\n\nNous avons donc, pour chaque projet, un second job qui lance lâ€™ensemble des tests lors de chaque modification du master. Cela nâ€™arrive normalement que lors du merge des nouvelles fonctionnalitÃ©s contenues dans les Pull Requests, qui ont dÃ©jÃ  Ã©tÃ© individuellement testÃ©es. Nous sommes donc sereins sur lâ€™intÃ©gration croisÃ©e de toutes les nouvelles fonctionnalitÃ©s sur le master.\n\nDÃ©ploiement\n\nAvant de dÃ©ployer Ã  lâ€™aide de Capistrano, nous vÃ©rifions que les tests passent (rÃ©sultat de lâ€™intÃ©gration continue + lancement manuel des tests). Le manque dâ€™automatisation concernant ces mises en production fait apparaitre une faille assez large. Pour la rÃ©sorber, nous pourrions par exemple accepter le dÃ©ploiement dâ€™un service, uniquement si ses tests sont passÃ©s et si aucun autre nâ€™est en cours ou en attente. MÃªme si cela ajoute une dÃ©pendance aux serveurs dâ€™intÃ©gration continue, cela sÃ©curise les dÃ©ploiements.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #5",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-5",
    "date"     : "July 12, 2013",
    "excerpt"  : "On est dredi, et on a encore pas mal de phrases chocs de nos dÃ©veloppeurs, entendues dans nos locaux partager.\n\nEn voici une nouvelle sÃ©lection avec les Dev Facts #5\n\nFaille de sÃ©cu ?\n\n\n  Câ€™est un peu complexe, câ€™est une back-office front !\n\n\nLa b...",
  "content"  : "On est dredi, et on a encore pas mal de phrases chocs de nos dÃ©veloppeurs, entendues dans nos locaux partager.\n\nEn voici une nouvelle sÃ©lection avec les Dev Facts #5\n\nFaille de sÃ©cu ?\n\n\n  Câ€™est un peu complexe, câ€™est une back-office front !\n\n\nLa boucle est bouclÃ©e\n\n\n  Il finit lÃ  oÃ¹ il sâ€™est arrÃªtÃ©\n\n\nEnvoi impossible\n\n\n  Mais tu nâ€™Ã©tais pas en piÃ¨ce jointe !\n\n\nCâ€™est lâ€™histoire dâ€™une fille\n\n\n  Câ€™est un peu comme â€œUn gars, une filleâ€ mais sans le gars\n\n\nLa minute de 30 secondes\n\n\n  Jâ€™ai pris une annÃ©e sabatique de 6 mois.\n\n\nLâ€™annÃ©e des deux mains\n\n\n  Ca dÃ©pend si câ€™est une annÃ©e ambidextre !\n\n\nLes dents de la neige\n\n\n  \n    Vous avez entendu, une championne de snowboard est morte.\n    Elle sâ€™est faite manger par un requin ?\n  \n\n\nLe vrai du faux\n\n\n  Ils ont trouvÃ© une faille infaillible\n\n\nLe roi de la combine\n\n\n  \n    En France, un truc comme ca, tâ€™en as pour 280 euros !\n    Et tu lâ€™as eu a combien ?\n    280 euros.\n  \n\n\nA bientÃ´t pour un prochain Ã©pisode\n\n"
} ,
  
  {
    "title"    : "Benchmarking WebSockets avec NodeJs",
    "category" : "",
    "tags"     : " nodejs, websockets, benchmark, open-source",
    "url"      : "/benchmarking-websockets-avec-nodejs",
    "date"     : "July 5, 2013",
    "excerpt"  : "Nous avons rÃ©cemment eu Ã  repenser une application Node.js de timeline temps rÃ©el, basÃ©e sur les WebSockets afin de tenir une charge plus Ã©levÃ©e.\n\nLâ€™application timeline\n\nFonctionnellement, lâ€™application timeline est relativement simple: elle cons...",
  "content"  : "Nous avons rÃ©cemment eu Ã  repenser une application Node.js de timeline temps rÃ©el, basÃ©e sur les WebSockets afin de tenir une charge plus Ã©levÃ©e.\n\nLâ€™application timeline\n\nFonctionnellement, lâ€™application timeline est relativement simple: elle consiste Ã  afficher un flux de message publiÃ©s par des contributeurs en temps rÃ©el pour les internautes prÃ©sent sur la page. Pour cela lâ€™application se base sur socket.io pour la partie websocket, et supporte Ã  peu prÃ¨s 15 000 connexions simultanÃ©es.\n\nAfin dâ€™augmenter la capacitÃ© de lâ€™application, nous avons dÃ©cidÃ© de la rendre scalable horizontalement. Câ€™est dire, rÃ©partir la charge sur un nombre X de serveurs communiquant entre eux, par exemple, par le biais de Redis.\n\n\n\nPour cela socket.io propose un store redis qui permet aux diffÃ©rentes instances de communiquer entre elles. Malheureusement les performances de ce store sont plutÃ´t dÃ©sastreuses car le store que propose socket.io est beaucoup trop verbeux et Ã©crit absolument tous les Ã©vÃ¨nements que reÃ§oit un serveur sur un seul channel redis. Lâ€™application devenait inutilisable autour de 8 000 connexions. Il Ã©tait donc inenvisageable de lâ€™utiliser en production.\n\nNous avons donc dÃ©cidÃ© rapidement de passer une autre solution que socket.io. AprÃ¨s pas mal de recherche nous avons fait notre choix sur Faye, une implÃ©mentation du protocole de Bayeux, bien documentÃ© et proposant aussi dâ€™utiliser redis comme â€œstoreâ€. AprÃ¨s test, cette solution sâ€™est rÃ©vÃ©lÃ©e bien plus performante que socket.io.\n\nTests de charge\n\nUne des problÃ©matiques rapidement rencontrÃ©e sur ce projet a Ã©tÃ© de tester la charge de notre application: comment simuler 15 000 connexions simultanÃ©es ?\n\nEn faisant le tour des solutions de benchmark de websocket (thor, â€¦) ,nous nâ€™avons pas trouvÃ© la solution qui nous permettait de faire les tests que nous souhaitions. Siege, ab ne le propose pas encore,Gatling, Jmeter, Tsung ont des plugins web-socket mais lâ€™utilisation et le reporting ne sont pas des plus clair.\n\nLa solution ?\n\nWebsocket-bench\n\nNous avons donc dÃ©cidÃ© de dÃ©velopper notre propre outil de benchmark de websocket (Socket.io ou Faye), au nom trÃ¨s original : websocket-bench.\n\nCet outil se base sur les clients Node que proposent Faye et Socket.io. Il peut Ãªtre facilement Ã©tendu Ã  lâ€™aide de â€œgeneratorâ€ (module Node), afin de rajouter la logique de votre application. Par exemple dans le cas de notre application, en se connectant, un client doit envoyer un message au serveur pour valider la connexion.\n\nCi dessous un exemple de gÃ©nÃ©rateur quâ€™on a pu utiliser lors de nos tests de charge.\n\n\n\nCet outil, lancÃ© sur des instances Amazon, nous a permis dâ€™exÃ©cuter nos tests de charge.\n\nUn exemple : la commande ci dessous va lancer 25 000 connexions, Ã  raison de 1000 connexions par seconde en utilisant le generateur â€œgenerator.jsâ€ :\n\n\n\n\n\nNombre de clients connectÃ©s sur Graphite\n\nAu delÃ  de 25 000 connexions, lâ€™instance Amazon (large) qui lanÃ§ait les tests ne tenait plus. Une solution pour tester un nombre plus Ã©levÃ©s de connexions serait dâ€™utiliser plusieurs machine de tests, peut Ãªtre Ã  lâ€™aide de bees with machin guns et ainsi dâ€™utiliser plusieurs instances pour lancer les tirs de charge.\n\nBonnes pratique de test de charge\n\nLors de votre test de charge (et pour la prod), nâ€™oubliez pas dâ€™augmenter le nombre maximal de descripteurs de fichiers cotÃ© client ET cotÃ© injecteur (ulimit -n 256000 par exemple dans la conf de supervisor, et dans le terminal avant de lancer le benchmark).\n\nSurveillez votre conntrack (si firewall iptables), augmentez votre plage locale de port, et si vous Ãªtes amenÃ©s Ã  tester plus de 25K connexions, utilisez plusieurs machines et/ou plusieurs IP sources diffÃ©rentes.\n\nComment contribuer au projet ?\n\nNâ€™hÃ©sitez pas Ã  remonter dâ€™Ã©ventuels bug via les issues ou Ã  contribuer au projet lâ€™aide de pull request github (https://github.com/BedrockStreaming/websocket-bench)\n\n"
} ,
  
  {
    "title"    : "Performances web et &quot;Disaster case&quot; sur applications mobile native",
    "category" : "",
    "tags"     : " webperf, mobile",
    "url"      : "/performances-web-disaster-case-applications-mobile-native",
    "date"     : "July 2, 2013",
    "excerpt"  : "La performance Web (ou grossiÃ¨rement temps de chargement) est devenue aujourdâ€™hui une problÃ©matique majeure dans tout dÃ©veloppement Web.\n\nLes outils pour mesurer / comprendre sont plutÃ´t reconnus dÃ©sormais et arrivent a une certaine maturitÃ©. Il y...",
  "content"  : "La performance Web (ou grossiÃ¨rement temps de chargement) est devenue aujourdâ€™hui une problÃ©matique majeure dans tout dÃ©veloppement Web.\n\nLes outils pour mesurer / comprendre sont plutÃ´t reconnus dÃ©sormais et arrivent a une certaine maturitÃ©. Il y a toutefois encore un crÃ©neau plutÃ´t peu documentÃ© (Ã  mon goÃ»t) dans le domaine, celui permettant de mesurer les temps de chargement dans des applications mobiles natives (Android / iOs â€¦)\n\nVoici un retour des mÃ©thodes que nous utilisons pour mesurer les performances (notamment de chargement) de nos applications natives et gÃ©nÃ©rer des Waterfall Charts, mais aussi sur la mise en place de tests â€œdisaster caseâ€ en cas dâ€™indisponibilitÃ© de services utilisÃ©s par lâ€™application.\n\nPour les besoins de ce tutoriel, nous allons prendre comme configuration, un Mac, avec une application native sur un iPhone 4 (reliÃ© au mÃªme rÃ©seau Wi-Fi que le Mac), ainsi que la version dâ€™essai du logiciel CharlesProxy installÃ©. (Mais la configuration et procÃ©dure est la mÃªme sur un autre OS, ou un autre mobile, et fonctionne aussi pour tester des webapps ou sites mobiles)\n\nCharlesProxy\n\nNous allons donc utiliser le logiciel payant CharlesProxy, qui est un proxy HTTP ou Reverse Proxy permettant de capturer le traffic HTTP de son ordinateur. Il existe une version dâ€™essai sans limite de 30 jours. Il y a peut Ãªtre des alternatives libres, mais Charles Ã©tant plutÃ´t une rÃ©fÃ©rence, câ€™est lâ€™outil que nous utilisons.\n\nCommencez donc par aller sur le site et installez CharlesProxy.\n\nUne fois installÃ©, lancez le, il devrait automatiquement commencer Ã  capturer le trafic rÃ©seau.\n\nConnexion Wi-Fi et rÃ©cupÃ©ration IP\n\nLa deuxiÃ¨me Ã©tape consiste Ã  connecter votre Ordinateur et votre TÃ©lÃ©phone sur le mÃªme rÃ©seau Wi-Fi.\n\nRÃ©cupÃ©rons ensuite notre adresse Ip via les â€œPrÃ©fÃ©rences SystÃ¨meâ€, section â€œInternet et sans filâ€ et icone â€œRÃ©seauâ€ sur votre configuration Wi-Fi :\n\n\n\nConfiguration du proxy sur son iPhone\n\nPassons ensuite sur le tÃ©lÃ©phone, dans vos prÃ©fÃ©rences Wi-Fi.\n\nCliquez ensuite sur la flÃ¨che bleu Ã  droite du nom de la connexion sur le paramÃ©trage Wi-Fi de notre iPhone, et descendre tout en bas du paramÃ©trage pour configurer manuellement notre proxy HTTP :\n\nConfigurez le proxy de cette connexion pour passer par le Proxy Charles, avec lâ€™adresse IP rÃ©cupÃ©rÃ©e plus haut, et le port par dÃ©faut de Charles 8888.\n\nUne fois la connexion lancÃ©e avec le Proxy activÃ© et Charles bien lancÃ© sur votre Mac, une popup dâ€™activation devrait apparaitre :\n\n\n\nAller ensuite sur un site mobile via Safari pour vÃ©rifier que le trafic est bien capturÃ© par votre Proxy.\n\nA ce stade, tout est prÃªt pour commencer les mesures.\n\nPrise de mesure avec Charles\n\nPour prendre une mesure avec Charles, allez dans le menu â€œProxyâ€, dÃ©cochez le â€œMAC OS X Proxyâ€ afin de ne pas parasiter vos mesures, et nettoyez lâ€™Ã©cran de Charles pour commencer une â€œsessionâ€ propre.\n\n\n\nVous nâ€™avez ensuite plus quâ€™a lancer une application pour mesurer la liste des requÃªtes HTTP nÃ©cessaire Ã  son dÃ©marrage.\n\nDans la partie Structure, un clic sur un domaine vous donnera plus dâ€™infos (nombre de requÃªte, et dÃ©tails de chacune) â€¦\n\nSÃ©lectionnez toutes les requÃªtes, puis cliquez sur â€œChartâ€ sur la droite, pour obtenir un premier Waterfall (made in Charles)\n\n\n\nGÃ©nÃ©ration de Waterfall (plus complet)\n\nToujours sous Charles, avec toutes les structures sÃ©lectionnÃ©es, Fichier / Export puis selectionner le format Http Archive (.har)\n\nNous allons ensuite utiliser lâ€™outil harviewer, pour visualiser le waterfall sous une forme plus complÃ¨te que dans Charles.\n\nRendez vous ici (avec Firefox, plutÃ´t que Chrome dont le rendu est buggÃ© sur cet outil) : https://www.softwareishard.com/har/viewer/\n\nDÃ©cochez la case â€œValidate data before processing?â€ pour Ãªtre moins embÃªtÃ© par des problÃ¨mes de compatibilitÃ© surement liÃ©s Ã  lâ€™export de Charles.\n\n\n\nEnsuite, faites un Drag &amp;amp; Drop de votre fichier .har dans le textarea de HarViewer pour obtenir votre waterfall, trÃ¨s proche de lâ€™onglet RÃ©seau de Firebug ou Network de la console de Chrome.\n\nVous retrouvez donc pour chaque requÃªte tous les Ã©lements classique, avec dÃ©tail des rÃ©ponses, code HTTP de retour, taille etc, et le tout sur une timeline trÃ¨s prÃ©cise.\n\n\n\nThrottling\n\nPour le moment, nous avons donc testÃ© notre application sur notre connexion Wi-Fi, cas plutÃ´t idÃ©al. Mais comment simuler une connexion 3g par exemple, peut Ãªtre plus proche de la rÃ©alitÃ© des utilisateurs de lâ€™applications ?\n\nPour cela, il vous suffit dâ€™aller dans Charles, puis le menu â€œProxyâ€ et â€œThrottle Settingsâ€.\n\nLa latence par dÃ©faut configurÃ©e est un peu Ã©levÃ©e (600ms), mais vous pouvez la modifier et affiner vos tests pour se rapprocher de conditions plus rÃ©elles.\n\nEnsuite, toujours dans le menu â€œProxyâ€, activÃ© lâ€™option â€œThrottleâ€ et vous pourrez tester sur une connexion diffÃ©rente.\n\n\n\nDisaster Case ?\n\nComment savoir comment se comporte votre application si vos Webservices sont injoignables ? ou si lâ€™un des services tiers que vous utilisez est down ? Comment trouver les SPOF (Single Point Of Failure) de vos apps ?\n\nToujours dans Charles, Allez dans â€œToolsâ€, puis â€œMap Remoteâ€.\n\nIci, vous allez pouvoir rediriger les domaines de vos choix, vers un domaine de type Blackhole.\n\nCâ€™est Ã  dire que le domaine choisi rÃ©agira comme si votre serveur web Ã©tait dans un Ã©tat de mort cÃ©rÃ©brale ! Pas celui oÃ¹ il rejette la connexion immÃ©diatement (trop facile), celui oÃ¹ il vÃ©gÃ¨te sans arriver Ã  acquitter la rÃ©ponse (le fameux â€œen attente de https:// â€¦.â€)\n\nPour ce besoin, nous allons utiliser le Blackhole fourni par Patrick Meenan pour lâ€™outil de mesure de performance web : WebPageTest : https://blackhole.webpagetest.org\n\n\n\nVous pouvez ensuite jouer avec les domaines, et regarder comment se comporte votre application dans le cas oÃ¹ lâ€™un dâ€™entre eux est inaccessible.\n\nSur notre iPhone 4 de test, on remarque dâ€™ailleurs un timeout sur les requÃªtes de 75 secondes ! Imaginez le cas, oÃ¹ le dÃ©veloppement et lâ€™appel Ã  ce service est synchrone ? 75 secondes de loading dans votre application avant de passer aux requÃªtes suivantes â€¦\n\n\n\nVoilÃ , vous avez dÃ©sormais une solution vous permettant de gÃ©nÃ©rer des Waterfall Charts pour vos apps natives, et de tester des conditions de mauvaises connexions, ou dâ€™indisponibilitÃ© de service.\n\nSi vous avez dâ€™autres mÃ©thodes, plus simples ou plus complÃ¨tes, ou tout autre remarque sur cette article, nâ€™hÃ©sitez pas Ã  le faire dans les commentaires ci-dessous.\n\nMerci.\n\nP.s: pour complÃ©ment, nâ€™hÃ©sitez pas Ã  creuser le blogpost de Steve Souders sur les waterfall mobile, qui utilise une mÃ©thode trÃ¨s diffÃ©rente avec tcpdump et pcapperf https://www.stevesouders.com/blog/2013/03/26/mobile-waterfalls/\n\n"
} ,
  
  {
    "title"    : "Coke, pour bien sniffer son code",
    "category" : "",
    "tags"     : " outil, qualite, php, open-source",
    "url"      : "/coke-pour-bien-sniffer-son-code",
    "date"     : "June 27, 2013",
    "excerpt"  : "Afin dâ€™uniformiser nos dÃ©veloppements, nous avons dÃ©cidÃ© de suivre des conventions de code. Les projets deviennent ainsi plus homogÃ¨nes et la revue de code, comme la maintenance, sâ€™en trouvent simplifiÃ©es. Comme la majoritÃ© de nos services sont en...",
  "content"  : "Afin dâ€™uniformiser nos dÃ©veloppements, nous avons dÃ©cidÃ© de suivre des conventions de code. Les projets deviennent ainsi plus homogÃ¨nes et la revue de code, comme la maintenance, sâ€™en trouvent simplifiÃ©es. Comme la majoritÃ© de nos services sont en PHP, nous utilisons PHP CodeSniffer.\n\nLe manque\n\nCependant, lâ€™Ã©ventail des frameworks utilisÃ©s en interne (Symfony, ZF, homemade) ne nous permet pas dâ€™employer une seule et mÃªme convention. De plus, lâ€™organisation des projets est assez hÃ©tÃ©rogÃ¨ne (ex: les rÃ©pertoires de test ne se nomment pas tous de la mÃªme maniÃ¨re). Nous avions donc besoin de pouvoir configurer spÃ©cifiquement PHP CodeSniffer pour chacun de nos projets.\n\nLe deal\n\nA la maniÃ¨re de Travis, nous avons optÃ© pour la mÃ©thode dite â€œdu fichier .truc posÃ© Ã  la racine de chaque projetâ€ (tm). Nous avons donc dÃ©veloppÃ© Coke, un script de sniff, qui lance PHP CodeSniffer avec la configuration contenu dans le fichier â€œ.cokeâ€ la racine du projet :\n\n\n\nAinsi, lorsque le fichier est paramÃ©trÃ© et que le script coke est correctement installÃ© sur le systÃ¨me, il suffit dâ€™exÃ©cuter la commande â€œcokeâ€ depuis la racine du projet sniffer.\n\nLe fix\n\nDans lâ€™optique dâ€™automatiser le plus possible nos processus, nous avons insÃ©rÃ© la vÃ©rification des coding styles Ã  lâ€™aide de Coke, dans un hook git de pre-commit.\n\nCoke est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Encodage - packaging - DRM - tout sur la vidÃ©o",
    "category" : "",
    "tags"     : " video, codec, drm, lft",
    "url"      : "/encodage-packaging-drm-tout-sur-la-vid%C3%A9o",
    "date"     : "June 26, 2013",
    "excerpt"  : "\n\nUne nouvelle vidÃ©o de lâ€™annÃ©e derniÃ¨re provenant dâ€™un Last Friday Talk.\n\nSouvent le monde de vidÃ©o est source dâ€™imprÃ©cision, cette vidÃ©o met Ã  plat lâ€™ensemble des termes qui sont utilisÃ©s dans le domaine :\n\n\n  encodage, transcodage\n  packaging (...",
  "content"  : "\n\nUne nouvelle vidÃ©o de lâ€™annÃ©e derniÃ¨re provenant dâ€™un Last Friday Talk.\n\nSouvent le monde de vidÃ©o est source dâ€™imprÃ©cision, cette vidÃ©o met Ã  plat lâ€™ensemble des termes qui sont utilisÃ©s dans le domaine :\n\n\n  encodage, transcodage\n  packaging (transformation du conteneur vidÃ©o)\n  DRM\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #4",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-4",
    "date"     : "June 14, 2013",
    "excerpt"  : "Parceque câ€™est dredi et que ca nous fait toujours rire de partager les phrases chocs entendues dans nos bureaux, voici les Dev Facts #4.\n\nCaptain Obvious\n\n\n  \n    â€œVous avez oubliez quelque chose ?â€\n    â€œLe problÃ¨me, câ€™est que quand tâ€™oublie, tâ€™y ...",
  "content"  : "Parceque câ€™est dredi et que ca nous fait toujours rire de partager les phrases chocs entendues dans nos bureaux, voici les Dev Facts #4.\n\nCaptain Obvious\n\n\n  \n    â€œVous avez oubliez quelque chose ?â€\n    â€œLe problÃ¨me, câ€™est que quand tâ€™oublie, tâ€™y penses pasâ€\n  \n\n\nLa minute de 30 secondes\n\n\n  Si je gagne cette somme, je me prends 6 mois dâ€™annÃ©e sabatique\n\n\n$i++\n\n\n  Avec un ticket resto, tu peux manger 2, avec toute ta famille\n\n\n???\n\n\n  On tâ€™a pas sonnÃ© les oreillettes\n\n\nla mÃ©moire\n\n\n  La musique est mÃ©morable, mais je mâ€™en souviens plus\n\n\nAnonymous Proxy Land\n\n\n  Pour poster en anonyme, il faut Ãªtre logguÃ©.\n\n\nJÃ©sus multipliait les pains\n\n\n  On va dÃ©dupliquer les clics par quatre\n\n\nAbsent coupable!\n\n\n  â€œDÃ¨s que je ne suis pas lÃ , jâ€™ai toujours tord â€¦ :(â€œ\n â€œBein oui! Les innocents ont toujours tordâ€\n\n\nTransgiving\n\n\n  1 mec sur 3 qui regardent des pornos sur Internet sont des femmes\n\n\nFatal error never die\n\n\n  PHP : Fatal error: date() [function.date]: Timezone database is corrupt - this should never happen! in ///**/error.php on line 105\n\n\n"
} ,
  
  {
    "title"    : "Firewall applicatif PHP et bundle Symfony",
    "category" : "",
    "tags"     : " outil, php, symfony, open-source",
    "url"      : "/firewall-applicatif-php-et-bundle-symfony",
    "date"     : "May 30, 2013",
    "excerpt"  : "Nous publions aujourdâ€™hui notre firewall applicatif sur notre compte GitHub. Il se compose :\n\n\n  dâ€™un composant PHP (5.4+) gÃ©rant les IPs (V4 et V6), plages, wildcards, white/black lists, etc.\n  dâ€™un bundle Symfony permettant dâ€™utiliser le composa...",
  "content"  : "Nous publions aujourdâ€™hui notre firewall applicatif sur notre compte GitHub. Il se compose :\n\n\n  dâ€™un composant PHP (5.4+) gÃ©rant les IPs (V4 et V6), plages, wildcards, white/black lists, etc.\n  dâ€™un bundle Symfony permettant dâ€™utiliser le composant Firewall dans les controllers Ã  lâ€™aide des annotations et de retourner une rÃ©ponse HTTP personnalisÃ©e.\n\n\nIls utilisent tous les deux Composer et sont disponibles sur Packagist.\n\nQuâ€™est ce quâ€™un Firewall applicatif ?\n\nUn Firewall applicatif permet de restreindre lâ€™accÃ¨s de certaines IPs Ã  certaines parties dâ€™une application. Vous pouvez par exemple dÃ©finir la liste des IPs autorisÃ©es dans la section dâ€™administration ou au contraire celles que vous souhaitez bloquer dans un forum.\n\nPourquoi cette implÃ©mentation ?\n\nNous souhaitions Ã©viter de redÃ©finir lâ€™ensemble des IPs chaque point de restriction. Nous avons donc cherchÃ© centraliser la configuration. Le FirewallBundle permet de mettre en place des listes hiÃ©rarchisÃ©es ainsi que des configurations prÃ©dÃ©finies que nous pouvons rÃ©utiliser et adapter chaque besoin.\n\nComment contribuer ?\n\nSi notre firewall applicatif rÃ©pond certaines de vos problÃ©matiques, mais que vous souhaitez le voir Ã©voluer, nâ€™hÃ©sitez pas participer son dÃ©veloppement :\n\n\n  forkez les projets sur GitHub,\n  faites une branche par fonctionnalitÃ©,\n  proposez-nous vos Ã©volutions et optimisations via les Pull Requests.\n\n\nVous pouvez Ã©galement nous remonter les problÃ¨mes rencontrÃ©s lors de son utilisation dans les issues du composant ou les issues du bundle.\n\nEnfin, nâ€™hÃ©sitez pas utiliser les commentaires de cet article pour nous faire part de vos rÃ©actions.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #3",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-3",
    "date"     : "May 24, 2013",
    "excerpt"  : "Episode 3 des devfacts ! Parce quâ€™on ne sâ€™en lasse pas.\n\nFort Boyaux\n\n\n  Cassandra tÃªte de tigre !\n\n\nla dev mÃªme en prod\n\n\n  Câ€™est un environnement de dev mÃªme en prod !\n\n\nPCF\n\n\n  \n    Jâ€™adore le prÃ©nom de Staline !\n    Sylvester ?\n  \n\n\nglory and ...",
  "content"  : "Episode 3 des devfacts ! Parce quâ€™on ne sâ€™en lasse pas.\n\nFort Boyaux\n\n\n  Cassandra tÃªte de tigre !\n\n\nla dev mÃªme en prod\n\n\n  Câ€™est un environnement de dev mÃªme en prod !\n\n\nPCF\n\n\n  \n    Jâ€™adore le prÃ©nom de Staline !\n    Sylvester ?\n  \n\n\nglory and consequences\n\n\n  OH PUTAIN jâ€™ai Ã©tÃ© RT par un participant de la belle &amp;amp; ses princes ! : jour de gloire :\n\n\nlâ€™optimiste\n\n\n  Câ€™est moins pire que rien\n\n\nle blagueur\n\n\n  Tâ€™as trois poussins sur une table ; comment tu fais pour en avoir plus que deux ? â€¦ Tâ€™en pousses un.\n\n\nI can haz root access !\n\n\n  \n    Tu peux me donner les accÃ¨s MySQL ?\n    Ils sont en root.\n    Et ils arrivent quand ?\n  \n\n\nle jimmy cliff\n\n\n  I can see clearly now, the regexâ€™s gone.\n\n\nla honte\n\n\n  \n    jâ€™ai un peu honte de ce que je fais lÃ  â€¦\n    quoi tu fais du javascript ?\n  \n\n\nlâ€™aveuglement\n\n\n  Tu peux Ãªtre valide w3c, lâ€™aveugle, il verra toujours rien !\n\n\nle choix dans la date\n\n\n  Date de sortie (jj/dd/yyyy)\n\n"
} ,
  
  {
    "title"    : "Redis on fire !",
    "category" : "",
    "tags"     : " redis, nosql, lft, video",
    "url"      : "/redis-on-fire",
    "date"     : "May 22, 2013",
    "excerpt"  : "On continue la diffusion de quelques LFT triÃ©s sur le volet.\n\nCette fois ci câ€™est Kenny Dits qui sâ€™y colle avec une prÃ©sentation de Redis et des cas dâ€™utilisation de cette technologie.\n",
  "content"  : "On continue la diffusion de quelques LFT triÃ©s sur le volet.\n\nCette fois ci câ€™est Kenny Dits qui sâ€™y colle avec une prÃ©sentation de Redis et des cas dâ€™utilisation de cette technologie.\n"
} ,
  
  {
    "title"    : "CR ConfÃ©rence Agora Cms du 15 mai 2013",
    "category" : "",
    "tags"     : " conference, cms",
    "url"      : "/cr-conference-agora-cms-du-15-mai-2013",
    "date"     : "May 20, 2013",
    "excerpt"  : "\n\nLe 15 mai 2013 avait lieu Ã  Paris, la premiÃ¨re Ã©dition de lâ€™AgoraCMS, ConfÃ©rence axÃ©e sur les CMS et la gestion de contenu Web.\n\nCette confÃ©rence est organisÃ©e par des acteurs importants du milieu, de chez Microsoft, Epitech, Oxalide, Cap Gemini...",
  "content"  : "\n\nLe 15 mai 2013 avait lieu Ã  Paris, la premiÃ¨re Ã©dition de lâ€™AgoraCMS, ConfÃ©rence axÃ©e sur les CMS et la gestion de contenu Web.\n\nCette confÃ©rence est organisÃ©e par des acteurs importants du milieu, de chez Microsoft, Epitech, Oxalide, Cap Gemini â€¦\n\nAu rendez-vous, des sujets sur Drupal, Wordpress, le Responsive, les RÃ©seaux Sociaux dâ€™entreprise, et des retours dâ€™expÃ©rience de diffÃ©rents acteurs franÃ§ais sur leurs utilisations de CMS public, â€œhome madeâ€ ou propriÃ©taire.\n\nLes CMS - Ã©cosystÃ¨me - Ã©tat des lieux et tendance, par Marine Soroko (CoreTechs) et FrÃ©dÃ©ric Bon (Clever Age)\n\nPremiÃ¨re confÃ©rence, et bonne introduction en la matiÃ¨re avec une prÃ©sentation de la typologie des CMS.\n\nOn nous met en garde sur les â€œÃ©diteurs de contenuâ€ notamment, ou un CMS ne doit pas permettre de gÃ©nÃ©rer du contenu dans les pages. Un CMS de nos jours, doit permettre de gÃ©rer un rÃ©fÃ©rentiel de contenu, que nos pages doivent pouvoir requÃªter, sinon nous devenons vite confrontÃ©s a des problÃ¨mes de rÃ©-usabilitÃ©.\n\nLes diffÃ©rents acteurs du marchÃ© sont prÃ©sentÃ©s via les fameux â€œQuadrant magicâ€ quâ€™on nous annonce finalement trÃ¨s loin de la rÃ©alitÃ©.\n\nOn finit sur une projection du futur des CMS qui devront rÃ©pondre aux problÃ©matiques suivantes :\n\n\n  multi-canal (support tablette / site tiers etc)\n  multi-source\n  personnalisation\n  intÃ©gration e-commerce\n  Analyses et statistique\n  interactions et e-services\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nVotre CMS intelligent grÃ¢ce lâ€™analyse des logs, par JÃ©rome Renard (Belogik)\n\nJÃ©rome Renard, ancien dÃ©veloppeur EzPublish, a montÃ© ces derniers mois une start-up proposant du Log As a Service, avec une solution du nom de Belogik, un peu comparable un Loggly.\n\nLa confÃ©rence prÃ©sente lâ€™intÃ©rÃªt dâ€™analyser les logs (ici sortant dâ€™un CMS, mais transposable tout site/service web) :\n\n\n  Incident de production\n  Service de la preuve\n  SEO\n  Performances\n  Gestion applicative\n  SÃ©curitÃ©\n  DÃ©veloppement\n\n\nMais aussi la difficultÃ© Ã  traiter des logs de formats diffÃ©rents, pas forcÃ©ment disposition, quand vous ne maitrisez pas ou peu lâ€™hÃ©bergement.\n\nPour la recherche dans ses logs, on a des solutions comme SolR ou les diffÃ©rents produits basÃ©s sur Lucene comme ElasticSearch (je rajouterais aussi le couple LogStash + Kibana utilisant aussi ElasticSearch)\n\nBref, une excellente prÃ©sentation, dans la veine de celles que nous avions pu prÃ©senter chez m6web au niveau du Monitoring, un sujet trÃ¨s complÃ©mentaire avec le Logging.\n\nPour plus dâ€™informations, belogik.com, ou sur leur compte twitter : @belogikCom.\n\nSinon vous pouvez consulter les slides sur le lien ci-dessous.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nVotre CMS intelligent grÃ¢ce lâ€™analyse des logs\n\nhttps://jrenard.info/talks/agoracms2013/\n\nTendances du design et nouveaux usages, par Patrick Maruejouls (Think Think)\n\nTrÃ¨s bonne prÃ©sentation sur lâ€™importance du design dans le premier sens du terme : un outil pouvant et devant permettre de servir les intÃ©rÃªts stratÃ©giques dâ€™une entreprise. Il en dÃ©coule quâ€™il serait bon de placer le design en amont des dÃ©cisions et non en aval comme câ€™est souvent le cas.\n\nDe ses nombreuses expÃ©riences, nous pouvons retenir quâ€™une des plus importantes tendances pour les annÃ©es venir est lâ€™adaptation du design lâ€™utilisateur. Ainsi, une grande enseigne de prÃªt-Ã -porter masculin a insÃ©rÃ© une puce RFID dans lâ€™Ã©tiquette de ses vÃªtements pour que lâ€™ambiance des cabines dâ€™essayage sâ€™adapte aux vÃªtements essayÃ©s (ex: une petite musique des Ã®les se dÃ©clenche lorsque le client essaye une chemise hawaÃ¯enne).\n\nDe mÃªme, cette tendance dâ€™adaptation du design lâ€™utilisateur pourra se voir magnifiÃ©e par la TV connectÃ©e et le second Ã©cran.\n\n\n\n\n\nLe second Ã©cran chez M6Web\n\nLes meilleurs thÃ¨mes et modules Drupal, par Dorian Marchan (Kernel 42) et Romain Jarraud (Trained People)\n\nPetite introduction Drupal, prÃ©sentÃ© comme un CMS, mais surtout un CMF (Content Management Framework), avec la prÃ©sentation de quelques modules trÃ¨s intÃ©ressants pour des dÃ©veloppeurs ou pour rÃ©aliser son â€œUsine Ã  Siteâ€.\n\nOn retiendra GCC dont une dÃ©mo trÃ¨s intÃ©ressante sera faite, Drupal Commerce, Ã©tant une distribution de Drupal avec un assemblage de modules et personnalisation pour orienter son drupal vers le e-commerce, mais aussi la prÃ©sentation dâ€™un des thÃ¨mes les plus Ã©voluÃ©s de Drupal : Omega, thÃ¨me ultra complet, Responsive avec un back-office assez puissant.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nLes RÃ©seaux sociaux dâ€™entreprise, par Edouard Ly (Oxalide) et Marine Soroko (Core Techs)\n\nTerme la mode depuis ces derniÃ¨res annÃ©es, les RSE commencent envahir les entreprises.\n PrÃ©sentation (sans dÃ©mo ou screenshot malheureusement), des diffÃ©rents acteurs du marchÃ© (trÃ¨s peu dâ€™acteurs open source dâ€™ailleurs :( ) :\n\n\n  Jive (leader du marchÃ©)\n  BlueKiwi\n  Telligent\n  BuddyPress : extension Wordpress\n  Yammer\n  Elgg\n  Chatter\n  Sharepoint\n  Liferay\n  Drupal commons\n  â€¦\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nResponsive design : un site mobile en moins dâ€™une heure, par Raphael Goetter (AlsaCreations)\n\nRaphael Gotter ( @goetter), le crÃ©ateur dâ€™AlsaCrÃ©ations, est complÃ¨tement incontournable pour tout ce qui touche lâ€™intÃ©gration HTML, ou le RWD (Responsive Web Design) dans la communautÃ© francophone, nous explique les approches Ã  avoir pour rÃ©aliser un site RWD.\n Notamment que le Mobile First, paraÃ®t toujours Ãªtre la bonne approche, ainsi quâ€™un rappel sur le fait que le RWD doit Ãªtre pensÃ© et prÃ©vu en amont.\n\nRaphael nous dÃ©montre aussi que son titre, et la rÃ©alisation dâ€™un site RWD en moins dâ€™une heure est infaisable. En prenant lâ€™exemple du site dâ€™Agora CMS, et en nous prÃ©sentant lâ€™approche pour transformer sa HP en Responsive.\n Plus de 15 jours de boulot au final, et des slides trÃ¨s intÃ©ressantes dÃ©couvrir ci-dessous, remplies dâ€™astuces, de checklists, et autres notions Ã  bien comprendre avant de sâ€™intÃ©resser et de se lancer dans le RWD :\n\n\n  Comprendre les surfaces dâ€™affichage\n  ConnaÃ®tre les Media Queries css 3\n  le Box-sizing\n  Halte aux dÃ©bordements\n\n\nJe vous invite Ã  tester sur un mobile Mobitest.me pour bien comprendre la diffÃ©rence entre viewport, largeur en pixel rÃ©elle, â€¦\n\nPrÃ©sentation de la propriÃ©tÃ© hyphens aussi pour gÃ©rer les dÃ©bordements de texte, coupler avec la propriÃ©tÃ© word-wrap, le framework / css de base Knacss.com, les tailles de typo en â€œremâ€.\n\nBref, confÃ©rence trÃ¨s riche, drÃ´le et a creuser impÃ©rativement pour tout ceux qui travaillent de prÃ¨s ou de loin sur ces problÃ©matiques.\n\nCompte rendu par Raphael lui mÃªme : https://blog.goetter.fr/post/50567713227/conference-un-site-responsive-en-une-heure avec le rÃ©sultat voir ici : https://kiwi.gg/rg/agora/\n\nEt les slides ci-dessous :\n\n\n\n20 minutes: gÃ©rer le multi-canal, apps, web, devices, par Arnaud Limbourg (20 minutes)\n\nArnaud ( @arnaudlimbourg) a rapidement expliquÃ© la stratÃ©gie mise en place par 20minutes pour permettre le multi-support : centralisation des donnÃ©es dans un rÃ©fÃ©rentiel accessible via une API permettant aux diffÃ©rents devices de se fournir en donnÃ©es, chacun leur maniÃ¨re.\n\nIl a ensuite donnÃ© quelques conseils afin de fournir une bonne API, en prÃ©cisant quâ€™il Ã©tait trÃ¨s difficile dâ€™en rÃ©aliser une bonne :\n\n\n  architecture REST en HTTP,\n  faibles temps de rÃ©ponse,\n  bon monitoring,\n  facilitÃ© dâ€™apprentissage et dâ€™utilisation,\n  concepts simples,\n  documentation.\n\n\nEnfin, Arnaud explique quâ€™il nâ€™y a pas de solution miracle entre lâ€™internalisation ou lâ€™externalisation des dÃ©veloppements (lâ€™app Android a Ã©tÃ© dÃ©veloppÃ©e en interne alors que le dÃ©veloppement de lâ€™app iOS a Ã©tÃ© externalisÃ©e).\n De la mÃªme maniÃ¨re, le choix entre le HTML5 et le code natif dÃ©pends des besoins et des ressources.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nRetour dâ€™expÃ©rience : France TÃ©lÃ©visions, par LÃ©o Poiroux (France TÃ©lÃ©visions)\n\nNos confrÃ¨res de chez France TÃ©lÃ©visions nous prÃ©sentent leur retour dâ€™expÃ©rience sur leurs usines site, avec au micro, LÃ©o ( @Leo_Px).\n\nAu dÃ©part, sur Spip, il nous explique pourquoi ils ont migrÃ© (douloureusement au dÃ©but) sur Drupal, et quâ€™est ce que cela leur a apportÃ©.\n\nUne confÃ©rence trÃ¨s transparente, drÃ´le, agrÃ©able et intÃ©ressante.\n\nLâ€™avenir de leur cÃ´tÃ© tend vers une transformation de lâ€™usine Ã  site, vers une â€œusine Ã  interfaceâ€ notamment pour gÃ©rer les multi-Ã©crans, et une ouverture de leurs api/services vers de lâ€™openApi, OpenData.\n\nFT câ€™est une trÃ¨s grosse DT (une centaine de personnes), une Ã©quipe dâ€™expert transverse surnommÃ©e SWAT (composÃ©e dâ€™Expert frontend JS/WebPerf, Expert Archi/Varnish, Expert Drupal, et de deux Coach Agile).\n\nDâ€™autres excellentes idÃ©es comme leurs Dojo et Safari.\n Les Dojo sont des sessions dâ€™une heure oÃ¹ les dÃ©veloppeurs se relaient toutes les 5 minutes sur un mÃªme code, pour avancer un dÃ©veloppement..\n Les safaris sont une sorte de â€œVis ma vieâ€ avec une journÃ©e en immersion dans une autre Ã©quipe de dÃ©veloppement par exemple ou dans une Ã©quipe de journaliste utilisant lâ€™un des sites quâ€™ils ont dÃ©veloppÃ©s.\n\n\n  Pourquoi Drupal ? \n â€œLa maison blanche utilise Drupalâ€\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nTable Ronde. Quel modÃ¨le choisir : crÃ©er mon propre CMS ? utiliser un CMS existant ? OpenSource / propriÃ©taire ?\n\nLa journÃ©e se termine sur une table ronde composÃ©e de diffÃ©rents responsables techniques :\n\n\n  Olivier Grange Labat â€“ @ogrange DT @ Le Monde interactif : Olivier est en charge de la technique de Lemonde.fr. Il a fait le choix de dÃ©velopper son propre outil de gestion de contenu\n  Damien Cirotteau â€“ @cirotix : Damien est CTO chez Rue89 qui utilise et est spÃ©cialisÃ© en Drupal.\n  Olivier Fouqueau â€“ DSI de la mairie dâ€™Aulnay-sous-bois : Olivier est en charge des SystÃ¨mes dâ€™informations et de lâ€™innovation la Mairie dâ€™Aulnay-sous-bois. Il fait le choix du CMS Ametys.\n  Galdric Pons â€“ @hebiflux Chef de projet digital @ BNP Paribas : Galdric est Chef de projet digital au pÃ´le innovation de BNP Paribas ou il a mis en place une usine site avec WordPress.\n\n\nChacun des participants au cours dâ€™une interview animÃ©e par Cyril Pierre de Geyer ( @cyrilpdg) , va expliquer son choix de CMS, les avantages et les inconvÃ©nients et donner de prÃ©cieux conseils aux personnes dans la mÃªme situation.\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nConclusion\n\nUne journÃ©e haute en couleur, avec de trÃ¨s bonnes confÃ©rences et des retours dâ€™XP toujours aussi intÃ©ressants.\n On salue une organisation impeccable et le tout pour un prix trÃ¨s accessible (20â‚¬).\n\nCe CR ne rend-compte finalement que dâ€™une mince partie des confÃ©rences (4 confÃ©rences en parallÃ¨le pour cause), mais vous pouvez retrouver dâ€™autres slides sur le site officiel de lâ€™Ã©venement.\n\nNâ€™hÃ©sitez pas commenter ce CR si vous avez des remarques ;-)\n\nP.s: Merci @nsilberman pour les photos que vous pouvez retrouver en intÃ©gralitÃ© ici : https://photos.silberman.fr/Other/AgoraCMS/\n"
} ,
  
  {
    "title"    : "La POO Canada Dry",
    "category" : "",
    "tags"     : " php, poo, lft, video",
    "url"      : "/la-poo-canada-dry",
    "date"     : "May 6, 2013",
    "excerpt"  : "Nous vous avions parlÃ©, il yâ€™a quelques mois, de nos confÃ©rences interne, les Last Friday Talk.\n\nVoici une premiÃ¨re vidÃ©o de lâ€™une de ces sessions sur â€œla POO Canada Dryâ€.\n\nLa POO (Programmation OrientÃ© Objet) ne consiste pas Ã  mettre du code dans...",
  "content"  : "Nous vous avions parlÃ©, il yâ€™a quelques mois, de nos confÃ©rences interne, les Last Friday Talk.\n\nVoici une premiÃ¨re vidÃ©o de lâ€™une de ces sessions sur â€œla POO Canada Dryâ€.\n\nLa POO (Programmation OrientÃ© Objet) ne consiste pas Ã  mettre du code dans des classes, elle fait appel Ã  des concepts vitaux pour le dÃ©veloppeur moderne. Olivier nous prÃ©sente quelques mauvais exemples tirÃ©s de code legacy et quelques bonnes pratiques pour faire de la POO (mais en fait câ€™est surtout du troll).\n\nEnjoy\n"
} ,
  
  {
    "title"    : "CR Real Time ConfÃ©rence Europe 2013 - Day 2",
    "category" : "",
    "tags"     : " conference, nodejs, realtime",
    "url"      : "/cr-real-time-conference-europe-2013-day-2",
    "date"     : "April 26, 2013",
    "excerpt"  : "\n\nCrÃ©dit : https://www.flickr.com/photos/andyet-photos/8679275805/in/set-72157633306379029\n\nAprÃ¨s la premiÃ¨re journÃ©e, on continue avec la deuxiÃ¨me journÃ©e de confÃ©rence. Toujours sur le format de 20 minutes pour prÃ©senter le sujet.\n\nWOOT, Arial B...",
  "content"  : "\n\nCrÃ©dit : https://www.flickr.com/photos/andyet-photos/8679275805/in/set-72157633306379029\n\nAprÃ¨s la premiÃ¨re journÃ©e, on continue avec la deuxiÃ¨me journÃ©e de confÃ©rence. Toujours sur le format de 20 minutes pour prÃ©senter le sujet.\n\nWOOT, Arial Balkan\n\n\n\nCrÃ©dit : https://twitter.com/OriPekelman/status/326600103475425281/photo/1\n\n@aral a travaillÃ© sur une solution dâ€™Ã©dition partagÃ©e de contenu, une solution sans transformation opÃ©rationnelle (OT sur Wikipedia) : WOOT qui signifie Without Operational Transformation.\n\nLe concept exposÃ© est de ne pas faire de suppression des caractÃ¨res dâ€™une chaÃ®ne, mais plutÃ´t de travailler sur la notion de visible / invisible et de position du caractÃ¨re au sein de la chaÃ®ne. Lâ€™objectif Ã©tant de garder la convergence et de prÃ©server les intentions des utilisateurs.\n\nIl nous indique diffÃ©rentes ressources pour approfondir le sujet :\n\n\n  une vidÃ©o de Google Tech Talk :Issues and Experiences in Designing Real-time Collaborative Editing Systems\n  une librairie JS : ShareJS\n  pour aller plus loin dans la gestion du travail collaboratif, lâ€™Inria rend disponible diffÃ©rents travaux de recherches sur le sujet\n\n\nConvention-Driven JSON, Steve Klabnik\n\n\n\nCrÃ©dit : https://twitter.com/OriPekelman/status/326607298556489728/photo/1\n\n@steveklabnik avait dÃ©jÃ  parlÃ© la veille sur un autre sujet. Aujourdâ€™hui, il nous expose la problÃ©matique de passer des objets (quelque soit le langage) JSON. Bien souvent, on utilise JSON pour la communication entre diffÃ©rents services (lâ€™un en PHP et lâ€™autre en Python par ex, ou deux services en PHP).\n\nPour un site web, on arrive souvent au rÃ©sultat suivant :\n\nObjet -&amp;gt; Template -&amp;gt; HTML\n\nPour Ã©viter certains problÃ¨mes lors des Ã©changes de donnÃ©es, il propose par exemple en ruby dâ€™utiliser active_model_serializers ce qui permet dâ€™obtenir le rÃ©sultat suivant :\n\nObjects -&amp;gt; Serializer -&amp;gt; HTML\n\nEn rÃ©sumÃ©, il recommande de passer par un outil de serialisation des donnÃ©es afin de ne pas perdre la structure de lâ€™objet et donc dâ€™avoir une plus grande rÃ©activitÃ© entre le client et le serveur.\n\nRealtime vs Real world, Tyler Mac Mullen\n\n@tbmcmullen travaille pour Fastly. Sa sociÃ©tÃ© propose des solutions dâ€™optimisation au sein des infrastructures de type CDN.\n\n\n\nCrÃ©dit : https://twitter.com/OriPekelman/status/326613272969228288/photo/1\n\nIl commence sa prÃ©sentation en dÃ©finissant les deux termes :\n\n\n  Realtime = rÃ©duire la latence\n  Realworld = notion dâ€™infrastructure\n\n\nIl indique Ã©galement lâ€™impossibilitÃ© de construire des infrastructures en temps-rÃ©el. Seul les CDNs ont la possibilitÃ© de sâ€™approcher du temps-rÃ©el. La notion de purge est Ã©galement essentielle.\n\nTyler prÃ©sente ensuite 3 possibilitÃ©s de purge :\n\n\n  Utilisation de Rsyslog : soit via TCP (problÃ¨me : lenteur), soit via UDP (problÃ¨me : pas de retour dâ€™erreur). Un noeud notifie tous les autres.\n  Love triangle : pas de serveur central mais notion de peer-to-peer. Chaque noeud interagit avec 2/3 autres noeuds. ProblÃ¨me : il nâ€™y a pas dâ€™Ã©tat global ni de possibilitÃ© de scalabilitÃ© avec ce type dâ€™infrastructure\n  Hybride : on met en place des switchs au niveau des noeuds. Les switchs interagissent entre eux, puis redistribuent lâ€™information au niveau de ces serveurs.\n\n\nLa sociÃ©tÃ© a dÃ©jÃ  fait dâ€™autres sessions lors dâ€™autres confÃ©rences tel que Velocity qui pourrait fortement intÃ©ressÃ© les adminsys ;-)\n\nDiscoRank : optimizing discoverability on SoundCloud, par AmÃ©lie Anglade\n\n\n\nCrÃ©dit : https://www.flickr.com/photos/andyet-photos/8680450402/in/set-72157633306379029\n\n\n\n@utstikkar est une franÃ§aise qui travaille pour Soundcloud en tant que MIR Software Engineer.\n\nElle nous a expliquÃ© lâ€™Ã©volution effectuÃ©e au sein de leur moteur de recherche : Discorank. Ce systÃ¨me peut Ãªtre assimilÃ© au PageRank de Google.\n\nIls utilisent pour cela : MySQL puis HDFS et enfin tout est re-manipulÃ© dans ElasticSearch\n\nBuddyCloud - Rethinking Social, par Simon Tennant\n\n\n\nCrÃ©dit : https://twitter.com/OriPekelman/status/326635400175161344/photo/1\n\nSimon Tennant est CEO de la sociÃ©tÃ© BuddyCloud. Il a tentÃ© de faire passer les informations suivantes :\n\n\n  fÃ©dÃ©rer ou mourir\n  travailler sur les protocoles non sur les APIs\n  pour construire du social dans un produit, il recommande de se baser sur lâ€™open source, les standards et protocoles ouverts\n\n\nLâ€™objectif de sa sociÃ©tÃ© est de permettre aux personnes de construire un rÃ©seau social fÃ©dÃ©rÃ© et bien entendu temps-rÃ©el.\n\nNâ€™hÃ©sitez pas Ã  fouiller dans leur source sur Github) qui fourmille de fonctionnalitÃ©s.\n\nRealtime at Microsoft, Pierre Couzy\n\nPierre Couzy travaille depuis plus de 10 ans chez Microsoft. Il nous prÃ©sente un projet rÃ©alisÃ© par des dÃ©veloppeurs US : SerialR (Github). Le projet utilise les Websockets.\n\n\n\nCrÃ©dit : https://www.flickr.com/photos/andyet-photos/8679336549/in/set-72157633306379029\n\nLe projet possÃ¨de deux dÃ©pendances principales :\n\n\n  Json.net cÃ´tÃ© serveur\n  Jquery cÃ´tÃ© client\n\n\nIl est noter que la nÃ©gociation exacte entre le client et le serveur dÃ©pend du navigateur utilisÃ©.\n\nA noter que cette prÃ©sentation est lâ€™une des rares qui nâ€™a pas Ã©tÃ© rÃ©alisÃ©e avec un MacBook ;-)\n\nLearning from Past Mistakes, a new node http layer, par Tim Caswell\n\n@creationix Ã©tait un des anciens core dev de NodeJS. Il nous expose les diffÃ©rents points cause desquels il a quittÃ© le projet.\n\n\n\nCrÃ©dit : https://twitter.com/OriPekelman/status/326670519371980800/photo/1\n\nUn des autres problÃ¨mes avec NodeJS est que le changement est difficile :\n\n\n  utilisÃ© en production par diffÃ©rentes sociÃ©tÃ©s\n  difficultÃ© Ã  modifier les APIs\n\n\nIl a donc dÃ©veloppÃ© Luvit basÃ© sur la technologie Lua (lÃ©gÃ¨re, rapide et permettant les co-routines).\n Cette nouvelle couche HTTP donne la possibilitÃ© :\n\n\n  de suspendre et de reprendre la fibre actuelle\n  lorsque lâ€™on a des fibres on peut faire dâ€™autres choses\n  Ã©crire sur les objets stream avec .write(item)\n  lire sur les objets stream avec .read()\n  de terminer un stream avec false item\n\n\nVous pouvez retrouver lâ€™ensemble des slides sur Github\n\nHTTP Proxy, par Nuno Job\n\n\n\nCrÃ©dit : https://www.flickr.com/photos/andyet-photos/8680421388/in/set-72157633306379029\n\n@dscape nous propose un bon article sur le load balancing avec nodejs.\n Pour le speaker nodejs câ€™est : net protocols &amp;amp;&amp;amp; libuv &amp;amp;&amp;amp; v8 &amp;amp;&amp;amp; npm\n\nVous pourrez retrouver lâ€™ensemble des slides sur Github\n\nLearning How To Let Go, par Kyle Drake\n\n@kyledrake introduit dâ€™autres solutions en remplacement de JSON : basÃ©s sur des donnÃ©es en binaire.\n\n\n\nCrÃ©dit : https://twitter.com/OriPekelman/status/326684373090963456/photo/1\n\nCependant, tout le monde nâ€™utilise pas correctement les Ã©changes binaires. Le speaker nous fait un trÃ¨s bon rÃ©sumÃ© de la situation pour effectuer des pushs sur la plateforme dâ€™Apple (Apple Push Notification Service) et des types de retours effectuÃ©s par Apple. Ceci rÃ©sume assez bien la situation.\n\n\n\nCrÃ©dit : https://twitter.com/noel_olivier/status/326686457257406464\n\nJS ne propose pas dâ€™API â€œnativeâ€ mais un projet permet de traiter du binaire : binaryjs (du binaire via websockets).\n\nKyle effectue diffÃ©rents benchs sur la taille des contenus envoyÃ©s : lâ€™un en JSON, lâ€™autre en Binary JSON et le dernier via MessagePack. Bien entendu, ce sont les contenus en binaire qui sont les plus lÃ©gers, mais il reste voir lâ€™impact du tÃ©lÃ©chargement du JS associÃ© et du traitement cÃ´tÃ© client.\n\nUn sujet donc Ã  Ã©tudier qui rappelle AMF pour Ã©changer des informations au format binaire entre PHP et Flash.\n\nSecuring socket applications, par James Coglan\n\nDans un premier temps, @jcoglan nous indique que la sÃ©curitÃ© câ€™est difficile et que cela concerne :\n\n\n  lâ€™authentification\n  la vie privÃ©e\n  les XSS\n  les CSRF\n\n\nPour rÃ©pondre aux diffÃ©rentes problÃ©matiques, il nous prÃ©sente Faye un systÃ¨me simple de message pub/sub pour le web. Ses slides sont disponibles.\n\nReal-time design, par Jan-Christoph Borchardt\n\n\n\nCrÃ©dit : https://twitter.com/OriPekelman/status/326711206209527809/photo/1\n\n@jancborchardt nâ€™est pas un dÃ©veloppeur, mais il nous rappelle quelques concepts importants :\n\n\n  Plus lâ€™utilisateur sâ€™ennuie, plus la confusion augmente\n  Sous 0,1 ms, lâ€™utilisateur considÃ¨re cela comme du temps rÃ©el\n  Attention aux transitions\n  Ne pas tuer la fluiditÃ©\n  â€œInterruptificationâ€ (exemple flagrant sur lâ€™image ci-dessous)\n  Pas de notifications pendant lâ€™utilisation (ex batterie faible 20% sur un mobile)\n\n\n\n\nEn rÃ©sumÃ©, lâ€™interface et le design sont importants Ã©galement pour sâ€™approcher dâ€™une expÃ©rience utilisateur temps-rÃ©el.\n\nFin de la seconde journÃ©e\n\nPour les plus courageux, le livestream est Ã©galement disponible.\n\n\n\nCet Ã©vÃ©nement Ã©tait trÃ¨s intÃ©ressant :\n\n\n  par son avance de phase, la majoritÃ© des prÃ©sentations correspondaient des rÃ©sultats de R&amp;amp;D, voire dâ€™innovation.\n  par lâ€™ambiance\n  par le networking que lâ€™on pouvait y faire\n\n\nOn peut en revanche peut Ãªtre un peu regretter le nombre de franÃ§ais (la fois cÃ´tÃ© speaker et Ã©galement cÃ´tÃ© public).\n\nPour terminer, un grand merci Julien Genestoux qui a organisÃ© lâ€™Ã©vÃ©nement.\n\nRendez-vous pour la prochaine Ã©dition.\n"
} ,
  
  {
    "title"    : "CR Real Time ConfÃ©rence Europe 2013 - Day 1",
    "category" : "",
    "tags"     : " conference, nodejs, zeromq, rabbitmq, realtime",
    "url"      : "/cr-real-time-conference-europe-2013-day-1",
    "date"     : "April 25, 2013",
    "excerpt"  : "Les 22 et 23 Avril 2013, ont eu lieu, la Real Time ConfÃ©rence en version EuropÃ©enne.\n\nPour cette premiÃ¨re Ã©dition, les festivitÃ©s se dÃ©roulaient Ã  Lyon, Ã  la Plateforme, une pÃ©niche posÃ©e sur les quais du RhÃ´ne trÃ¨s sympathique.\n\nPassÃ© lâ€™accueil â€œ...",
  "content"  : "Les 22 et 23 Avril 2013, ont eu lieu, la Real Time ConfÃ©rence en version EuropÃ©enne.\n\nPour cette premiÃ¨re Ã©dition, les festivitÃ©s se dÃ©roulaient Ã  Lyon, Ã  la Plateforme, une pÃ©niche posÃ©e sur les quais du RhÃ´ne trÃ¨s sympathique.\n\nPassÃ© lâ€™accueil â€œla Titanicâ€ avec lâ€™orchestre dans le hall dâ€™entrÃ©e, nous descendons au sous-sol pour commencer suivre la premiÃ¨re journÃ©e de confÃ©rence, qui sâ€™annonce dÃ©jÃ  trÃ¨s chargÃ©e.\n\n\n\nLa vue de la salle de confÃ©rence (CrÃ©dit : https://twitter.com/frescosecco/status/326302218515017729/photo/1 )\n\nWebSuckets, par Arnout Kazemier\n\nPremiÃ¨re confÃ©rence autour des Websockets et des bugs ou difficultÃ©s dâ€™implÃ©mentation que lâ€™on peut rencontrer.\n\nOn parle notamment de Firefox qui en prend pour son grade : Si lâ€™on appuie sur ESC aprÃ¨s que la page soit chargÃ©e, toutes les connexions sont fermÃ©es â€¦ Firefox ne peut pas se connecter non plus sur une Websocket non sÃ©curisÃ©e en HTTPS.\n\nCotÃ© Safari Mobile, Ã©crire dans une Websocket fermÃ©e plante votre tÃ©lÃ©phone, et cela arrive quand on revient sur un onglet qui utilisait des Websocket, ou lorsque lâ€™on rÃ©ouvre un safari prÃ©cÃ©demment rÃ©duit.\n\nBref, en gros, ca dÃ©motive un petit peu sur lâ€™utilisation des Websockets !\n\nArnout ( @3rdEden ) dÃ©conseille aussi lâ€™utilisation des Websocket sur mobile, et indique de ne les utiliser que quand câ€™est vraiment nÃ©cessaire sur desktop.\n\nQuelques prÃ©sentations dâ€™outillages :\n\n\n  HA Proxy\n  HTTP-Proxy\n  Nginx-devel\n\n\nVous pouvez retrouver une battle sur les perfs de ces proxys ici : github.com/observing/balancerbattle\n\nOn aborde aussi les problÃ©matiques de tirs de charge sur les Websocket avec :\n\n\n  wsbench\n  websocketbenchmark\n\n\nLes deux Ã©tant, dâ€™aprÃ¨s Arnout, incomplets ou dÃ©passÃ©s â€¦\n\nIl a donc dÃ©veloppÃ© son propre outil : Thor, â€œsmasher of Websocketsâ€ Ã  tester de toute urgence : https://github.com/observing/thor\n\nLes frameworks mentionnÃ©s pour en simplifier lâ€™implÃ©mentation :\n\n\n  Faye\n  Signalr\n  xsockets\n  sockjs\n  socket.io\n\n\nAttention aussi aux Ã©lÃ©ments perturbateurs : firewall, extensions de browsers, antivirus, ou proxy qui peuvent bloquer les ports utilisÃ©s par les Websockets.\n\nBref, une premiÃ¨re entrÃ©e en matiÃ¨re trÃ¨s complÃ¨te et intÃ©ressante qui couvre vraiment toute la partie moins glamour des Websockets.\n\nJe vous invite aussi Ã  consulter son blog si le sujet vous intÃ©resse : https://blog.3rd-eden.com/\n\n\n\n(CrÃ©dit : https://twitter.com/hintjens/status/326243158109347841/photo/1 )\n\n\n\nSocketStream 0.4, par Owen Barnes\n\nVoici lâ€™un des frameworks pour lâ€™implÃ©mentation des Websockets, oÃ¹ son crÃ©ateur ( @socketstream ) nous partagÃ© ses idÃ©es de la conception et de lâ€™utilisation dâ€™un framework : dÃ©couplage, simplicitÃ©, modularitÃ© etc.\n\nLe framework est â€œTransport Agnosticsâ€ et peut donc utiliser sockJs, Engine.io, ou Websockets native juste en changeant une simple ligne.\n\nLe FW est basÃ© sur Prism, un module de serveur realtime, lui aussi open-sourcÃ© sur github.com/socketstream/prism.\n\nLa 0.4 prÃ©sentÃ©e est en cours de finalisation, et sera disponible prochainement en version finale sur le github https://github.com/socketstream/\n\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8672407195/in/set-72157633306379029/ )\n\nXSockets, par Magnus Thor\n\nMagnus ( @dathor ), nous prÃ©sente son framework Xsockets pour lâ€™utilisation des Websockets avec une dÃ©mo â€œlive codingâ€ peut-Ãªtre intÃ©ressante, mais tentÃ©e â€œonlineâ€ et avec une connexion bien foireuse (comme dans toutes les confÃ©rences techniques, non ?) â€¦\n\nBref, un peu douloureux Ã  regarder, mais la dÃ©mo avait lâ€™air dâ€™avoir du potentiel : une application web utilisant WebRPC pour partager en mode Peer To Peer la Webcam de lâ€™utilisateur.\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8673638532/in/set-72157633306379029/ )\n\nZeroMQ as scriptable sockets, par Lourens NaudÃ© (Bear Metal)\n\nLourens ( @methodmissing) est lâ€™un des â€œco-maintainerâ€ de ZeroMq.\n\nIl nous prÃ©sente ZeroMq comme une solution de messagerie instantanÃ©e pour les apps. Ca nâ€™est pas un serveur, ni un broker, mais une librairie sur la communication et gestion de la concurrence.\n\nOn parcourt ensuite les diffÃ©rents types de sockets supportÃ©s :\n\n\n  Req / Rep\n  Pub / Sub\n  Push / Pull\n\n\nVoir la prÃ©sentation ci dessous :\n\n\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8672695927/in/set-72157633306379029/ )\n\nWebRTC, par Sam Dutton (Google)\n\nSam Dutton, Developper Advocate chez Google ( @sw12 ), quâ€™on Ã  dÃ©ja vu/entendu par le passÃ© Ã  la VÃ©locity Conf (voir prÃ©cÃ©dent CR) nous parle de WebRPC.\n\nOn parcourt les diffÃ©rentes API disponible, le support des navigateurs (Chrome, Firefox Nightly et IE Chrome Frame â€¦).\n\nOn dÃ©couvre ensuite de nombreuses dÃ©mos trÃ¨s sympa :\n\n\n  Ascii CamÃ©ra\n  GetUserMedia\n  Webcam Toy\n  Magic Xylophone\n  Screen Capture (nÃ©cessite Chrome Canary)\n  â€¦\n\n\nPour dÃ©bugger plus facilement, utilisez le chrome://webrtc-internals\n\nLibs, apps et frameworks pour XML RPC :\n\n\n  easyRTC : full stack\n  conversat.io built with SimpleWebRTC\n  PeerJS : API abstraction\n  webRTC.io\n  Sharefest\n\n\nPlus dâ€™infos/codes ou dÃ©mos sur les slides : https://samdutton.net/realtime2013/\n\n\n  â€œWebRTC and HTML5 could enable the same transformation for real-time communications that the original browser did for information.â€ Phil Edholm / Nojitter\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8673907134/in/set-72157633306379029 )\n\nEnemy of the state : An introduction to functional reactive programming with Bacon.Js, par Phiilip Roberts (Float)\n\n\n\nLâ€™une des prÃ©sentations les plus intÃ©ressantes de la journÃ©e par Philip Roberts ( @philip_roberts ), CTO et co-founder de Float avec lâ€™introduction Bacon.Js et la â€œFunctional Reactive Programmingâ€ en Javascript. Une faÃ§on diffÃ©rente de coder pour Ã©viter les â€œcallback hellâ€ notamment.\n\nLe projet rÃ©pond aussi Ã  une problÃ©matique trÃ¨s courante des dÃ©v JS, avec lâ€™exemple du â€œCheck Username Availibilityâ€ qui lance une requÃªte Ajax chaque KeyPress et dont lâ€™ordre nâ€™est pas maitrisÃ©. (partir de la slide 38)\n\nBacon.Js est dispo sur Github : https://github.com/raimohanska/bacon.js\n\nP.s: la visualisation des streams sur ses slides Ã©tait trÃ¨s sympa : https://latentflip.com/bacon-examples/\n\nPlus dâ€™infos sur les slides : https://latentflip.com/bacon-talk-realtimeconfeu/\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8673684632/in/set-72157633306379029 )\n\nQuick Wins with Redis for your website, par Cathering Jung\n\nCatherine Jung ( @bufferine ) travaille sur des services de paris en ligne. Elle explique les problÃ©matiques de temps rÃ©el quâ€™elle doit affronter, et comment Redis lui permet de mieux supporter la charge.\n\nAu final, on parle un peu plus de Scala que de Redis, mais tout retour dâ€™expÃ©rience est toujours bon prendre.\n\nRetrouvez les slides ici :\n\nhttps://docs.google.com/file/d/0By6ZH5wplIR-MzgyOEJCMEkyWmc/edit?usp=sharing \n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8673926016/in/set-72157633306379029/ )\n\nRealtime and Go : Leaving the frameworks behind, par Paddy Foran\n\nPaddy ( @paddyforan ) nous prÃ©sente le language Go.\n\nA la question : â€œWhat is Go ?â€ la rÃ©ponse est :\n\n\n  A better C, from the guys that didnâ€™t bring you C++\n\n\nhttps://goonaboat.com/\n\nBref, Go câ€™est :\n\n\n  Compiled\n  Static typed\n  Fast\n  Elegant\n  Concurrent\n\n\nLes slides sont disponibles ici : https://goonaboat.com/ et le code de la prÃ©sentation : https://github.com/paddyforan/goonaboat\n\nPlus dâ€™infos sur le langage ici : https://golang.org/ avec un â€œTourâ€ qui parait trÃ¨s bien fait : https://tour.golang.org/#1\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8673939104/in/set-72157633306379029 )\n\nCloud Messaging with Node.Js and RabbitMQ, par Alvaro Videla\n\nAlvaro ( @oldsound ) est le co-auteur de â€œRabbit Mq In actionâ€.\n\nIl a prÃ©sentÃ© lâ€™intÃ©rÃªt dâ€™utiliser un rabbitMQ dans un projet qui est un fork dâ€™Instagram, mais Real Time : CloudStagram, sur une stack â€œCloud Foundyâ€, Rabbit MQ, Redis, MongoDB et SockJS\n\nNotamment le concept de tout gÃ©rer via Ã©vÃ©nement (slide 54 ci-dessous).\n\nBref, pas mal de bonnes idÃ©es Ã  retenir et pas mal de projets intÃ©ressants sur son github : https://github.com/videlalvaro , comme le RabbitMqSimulator pour prÃ©senter clairement le fonctionnement des RabbitMQ\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8673976982/in/set-72157633306379029/ )\n\n\n\nOffline first!, par Jan Lehnardt\n\nJan ( @janl ) bosse sur CouchDb. Apache CouchDB est une base de donnÃ©es de type document basÃ©e sur le format JSON et utilisant Javascript (notamment pour les MapReduce).\n\nIl commence sa prÃ©sentation par un â€œYou are all doing it wrong !â€. En rÃ©expliquant que le rÃ©seau est toujours rapide, mais que câ€™est la latence qui est problÃ©matique. (Voir lâ€™excellent article de 2010 de @edasfr sur le sujet toujours aussi pertinent ).\n\nIl faut aujourdâ€™hui travailler Offline First ! (un peu lâ€™Ã©quivalent dâ€™un Mobile First cotÃ© apps), et prend pas mal dâ€™exemples de bonne ou mauvaise implÃ©mentation (de la gestion hors connexion, du passage dans un tunnel, en se moquant de la mauvaise couverture franÃ§aise dans le TGV).\n\nOn aborde ensuite les :\n\n\n  CouchDB\n  PouchDB : Javascript database that syncs!\n  TouchDB : CouchDB-compatible embeddable database engine for mobile &amp;amp; desktop apps\n\n\net la prÃ©sentation du framework Hoodie : https://hood.ie/, basÃ© sur le Offline par dÃ©faut.\n\n\n  â€œThink of CouchDB as Git for your application dataâ€ Jan Lehnardt\n\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8672884965/in/set-72157633306379029/ )\n\nBuilding Realtime HTML5 apps for Android and Firefox OS, par Anthony Ricaud\n\nPrÃ©sentation par Anthony Ricaud ( @rik24d ) des fonctionnalitÃ©s HTML5 implÃ©mentÃ©es par les Ã©quipes de Mozilla, notamment pour connaitre lâ€™Ã©tat de la batterie, lâ€™orientation, la gestion des apps supportant la sÃ©lection de photos par exempleâ€¦\n\nChaque site peut Ãªtre une apps, Ã  condition de mettre les lignes nÃ©cessaires dans un fichier manifest. Beaucoup de dÃ©bats aussi autour des systÃ¨mes fermÃ©s de MarketPlace.\n\nPlus dâ€™infos dans les slides ci dessous :\n\n\n\n(CrÃ©dit : https://www.flickr.com/photos/andyet-photos/8672894401/in/set-72157633306379029 )\n\n\n\nNodeCopter + Hackathon\n\nJe laisse la parole Ã  Olivier Mansour pour la prÃ©sentation du NodeCopter :\n\nRomain Huet ( @romainhuet ) nous a prÃ©sentÃ© et fait une petite dÃ©monstration du pilotage dâ€™un AR Drone avec NodeJs.\n\nIssu du projet nodecopter https://nodecopter.com/, un ensemble de librairies Node.js est disponible et rend le pilotage du drone complÃ¨tement accessible. Mouvements en vol, stream de la camÃ©ra, Romain nous a fait une dÃ©monstration fun et captivante de lâ€™engin.\n\nEt faire voler un drone dans un bateau â€¦ on avait jamais vu Ã§a !\n\nFin de la 1Ã¨re journÃ©e :\n\nUne premiÃ¨re journÃ©e trÃ¨s sympathique, bourrÃ©e dâ€™idÃ©es et dâ€™outils en tout genre. Lâ€™organisation est vraiment au poil, et on repart en voulant refaire le monde techniquement :-)\n\nLe compte rendu de la deuxiÃ¨me journÃ©e est ici : https://tech.bedrockstreaming.com/cr-real-time-conference-europe-2013-day-2\n\nP.s : Merci &amp;amp;Yet pour la plupart des photos prÃ©sentes ici : https://www.flickr.com/photos/andyet-photos/sets/72157633306379029/\n\nPour les plus motivÃ©s, la confÃ©rence Ã©tÃ© enregistrÃ©e en vidÃ©o :\n\n\nVidÃ©o de la premiÃ¨re journÃ©e de la RealTime Conf Europe\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #2",
    "category" : "",
    "tags"     : " humour, devfacts",
    "url"      : "/m6web-dev-facts-2",
    "date"     : "April 9, 2013",
    "excerpt"  : "On continue la sÃ©rie des DÃ©v Facts, phrases oh combien cultes prononcÃ©es par nos chers dÃ©veloppeurs lors dâ€™oublis cÃ©rÃ©braux :-)\n\nPour ceux qui avaient ratÃ© la premiÃ¨re sÃ©rie, câ€™est ici : https://tech.bedrockstreaming.com/m6web-dev-facts-1\n\nEnjoy\n\n...",
  "content"  : "On continue la sÃ©rie des DÃ©v Facts, phrases oh combien cultes prononcÃ©es par nos chers dÃ©veloppeurs lors dâ€™oublis cÃ©rÃ©braux :-)\n\nPour ceux qui avaient ratÃ© la premiÃ¨re sÃ©rie, câ€™est ici : https://tech.bedrockstreaming.com/m6web-dev-facts-1\n\nEnjoy\n\nCaptain Obvious\n\n\n  Chef de Projet : â€œVous avez oubliÃ© quelque chose ?â€\n\n  DÃ©veloppeur : â€œLe problÃ©me câ€™est que quand tâ€™oublie, tâ€™y pense pasâ€\n\n\nPaye ta culture\n\n\n  Câ€™est une citation de la bible â€¦ et de Civilisation IV\n\n\nSeigneur non !\n\n\n  \n    Avant jâ€™encodais les vidÃ©os pour lâ€™Ã©mission â€œLe jour du Seigneurâ€\n    Câ€™est quoi ? une Ã©mission porno ?\n    Ca dÃ©pend comment tâ€™Ã©cris â€œseigneurâ€\n  \n\n\nLes congÃ©s du fantastique\n\n\n  Le 4, je suis en RPG â€¦ euh RTT\n\n\nDouble compÃ©tence â€¦\n\n\n  Je viens de recevoir le CV dâ€™un gars, il a fait une formation â€œMaÃ®trise en patisserieâ€ â€¦ il doit maÃ®triser les cookies non ? :)\n\n\nIncomparable pour ne rien comparer\n\n\n  \n    Ah bon ? Câ€™est les TCL qui font le plus souvent grÃ¨ve en France ? Plus que la SNCF ?\n    Oui, Ã  titre de comparaison, je crois que câ€™est incomparable â€¦\n  \n\n\nRetour vers le futur\n\n\n  Donc vous savez que je ne suis pas lÃ  la semaine derniÃ¨re\n\n\nTrouvÃ© !\n\n\n  Je viens de trouver une dÃ©couverte !\n\n\nLa preuve par dix\n\n\n  On a doublÃ© la bande passante par dix\n\n\nLe flegme illustrÃ©\n\n\n  De toute facon, yâ€™a pas de consÃ©quence : au pire, on meurt.\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #1",
    "category" : "",
    "tags"     : " humour, devfacts",
    "url"      : "/m6web-dev-facts-1",
    "date"     : "March 21, 2013",
    "excerpt"  : "Depuis de nombreuses annÃ©es, toutes les â€œphrases chocsâ€ dites par les Ã©quipes techniques de M6Web sont archivÃ©es, loggÃ©es, historisÃ©es. Pas moyen de sortir une Ã¢nerie sans quâ€™elle soit gravÃ©e dans le marbre.\n\nDu coup, nous avons dÃ©cidÃ© de profiter...",
  "content"  : "Depuis de nombreuses annÃ©es, toutes les â€œphrases chocsâ€ dites par les Ã©quipes techniques de M6Web sont archivÃ©es, loggÃ©es, historisÃ©es. Pas moyen de sortir une Ã¢nerie sans quâ€™elle soit gravÃ©e dans le marbre.\n\nDu coup, nous avons dÃ©cidÃ© de profiter de cette pÃ©riode trÃ¨s calme sur le blog pour vous faire partager une petite sÃ©lection en vrac de quelques Dev Facts entendus dans les locaux dâ€™M6Web Lyon :\n\ninvalider le cache, nommer les choses â€¦\n\n\n  \n    On purge le cache infini tous les jours.\n    Non câ€™est le cache 7 jours quâ€™on purge tous les jours !\n  \n\n\nJâ€™ajoute -2\n\n\n  Jâ€™ai rÃ©duit de fois 10\n\n\nwork - not - flow\n\n\n  On a des workflows, on a aussi des worknotflows\n\n\nLa bonne affaire\n\n\n  Pour le mÃªme prix, tâ€™as bien moins cher ailleurs !\n\n\nEt pour arrÃªter ? tu cliques sur dÃ©marrer !\n\n\n  La vidÃ©o a Ã©tÃ© mise en erreur avec succÃ¨s.\n\n\nBig or what ?\n\n\n  Il y a une librairie Rennes, elle est Ã©norme! Tu y rentres, câ€™est tout petitâ€¦ !\n\n\nNiveau CE2\n\n\n  je fais un mail de rÃ©ponse avec une explication niveau CE2\n\n\nBref\n\n\n  \n    \n      Jâ€™ai demandÃ© Pierre, qui mâ€™a dit quâ€™il ne savait pas mais que si jâ€™avais lâ€™info, il la voulait bien. Du coup, jâ€™ai demandÃ© Kenny, qui mâ€™a dit de demander a Antonyâ€¦ qui ne savait pas et mâ€™a dit de demander Pierre\n    \n    \n      Bref, jâ€™ai posÃ© une question un admin\n    \n  \n\n\nCDP Junior\n\n\n  Raa mais chui un cdp junior moi je sais rien faire de mes 10 doigts :(\n\n\nLa suite dans un prochain Ã©pisode ;-)\n\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un Lead Developpeur / Architecte web (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/m6web-lyon-recherche-un-lead-developpeur-architecte-web-h-f-en-cdi",
    "date"     : "February 5, 2013",
    "excerpt"  : "\n\nMise jour : Le poste nâ€™est Ã  plus pourvoir. Merci\n\nM6Web Lyon recrute, en CDI, un Lead DÃ©veloppeur LAMP, avec une trÃ¨s forte expertise sur les technologies PHP 5.4, MySQL, Symfony2, GIT, et capable dâ€™encadrer une petite Ã©quipe de dÃ©veloppement.\n...",
  "content"  : "\n\nMise jour : Le poste nâ€™est Ã  plus pourvoir. Merci\n\nM6Web Lyon recrute, en CDI, un Lead DÃ©veloppeur LAMP, avec une trÃ¨s forte expertise sur les technologies PHP 5.4, MySQL, Symfony2, GIT, et capable dâ€™encadrer une petite Ã©quipe de dÃ©veloppement.\n\nNous recherchons quelquâ€™un de trÃ¨s passionnÃ©, enthousiaste, et mordu de veille technologique : un missionnaire de lâ€™open source, un intÃ©griste de la qualitÃ© de code, des tests unitaires et fonctionnels, et un architecte de projets aguerri avec une premiÃ¨re approche en mÃ©thodologie de dÃ©veloppement agile, et une expÃ©rience de management de dÃ©veloppeurs.\n\nSi, en plus, vous Ãªtes un malade de lâ€™optimisation back-end et front-end, que des technologies comme Node.js vous Ã©moustillent, que, malgrÃ© la qualitÃ© de MySQL, vous envisagez dans certains cas des solutions NoSQL alternatives (Mongo, Redisâ€¦), votre profil nous intÃ©resse !\n\nVenez apporter vos compÃ©tences aux Ã©quipes techniques de M6Web en travaillant sur des sites trÃ¨s forte charge (m6.fr, clubic.com, jeuxvideo.fr â€¦), et partagez-les grÃ¢ce des confÃ©rences internes ou externes et des articles sur notre blog.\n\nSi vous avez les qualitÃ©s requises et lâ€™envie de nous rejoindre, allez sur le lien ci-dessous et faites nous part de votre CV, de votre compte github, et dâ€™une lettre attrayante pour nous motiver vous rencontrer.\n\nSi vous souhaitez postuler ou avoir plus dâ€™infos : https://www.groupem6.fr/ressources-humaines/offres-emploi/lead-developpeur-architecte-web-h-f-229879.html\n\n"
} ,
  
  {
    "title"    : "Organiser des confÃ©rences technique en interne",
    "category" : "",
    "tags"     : " conference, culture, lft",
    "url"      : "/organiser-des-conferences-technique-en-interne",
    "date"     : "December 5, 2012",
    "excerpt"  : "\n\nLes â€œLast Friday Talkâ€ : Le concept\n\nDepuis 10 mois dÃ©sormais, chez M6Web, nous organisons chaque â€œdernier vendrediâ€ du mois, une manifestation que nous avons nommÃ©e â€œLast Friday Talkâ€.\n\nLe concept : 2 heures de 13h30 15h30, oÃ¹ 4 sessions â€œtype ...",
  "content"  : "\n\nLes â€œLast Friday Talkâ€ : Le concept\n\nDepuis 10 mois dÃ©sormais, chez M6Web, nous organisons chaque â€œdernier vendrediâ€ du mois, une manifestation que nous avons nommÃ©e â€œLast Friday Talkâ€.\n\nLe concept : 2 heures de 13h30 15h30, oÃ¹ 4 sessions â€œtype confÃ©renceâ€ de 25 minutes, suivies de 5 minutes de questions, sont prÃ©sentÃ©es par des personnes de la Direction Technique. La participation (orateur ou public) est bien entendue facultative.\n\nLâ€™idÃ©e, est que chacun a quelque chose dire dans le web de nos jours, quelque chose prÃ©senter/partager aux autres. Soit un retour dâ€™experience sur une techno, un outil, une mÃ©thodologie, soit mÃªme une prÃ©sentation de ses dÃ©veloppements ou projets passÃ©s.\n\nDes exemples ?\n\nQuelques exemples de prÃ©sentations qui ont Ã©tÃ© faites :\n\n\n  ZÃ©roMq, la bibliothÃ¨que rÃ©seau\n  PrÃ©sentation du langage Python\n  VIM pour les nuls\n  Les mÃ©thodes agiles\n  Doctrine 2\n  La sÃ©curitÃ© SQL\n  Le dÃ©ploiement chez Facebook\n  La WebPerf avancÃ©e\n  PrÃ©sentation de CoffeeScript\n  â€¦\n\n\nLâ€™interÃªt ?\n\nLes apports pour vos Ã©quipes sont nombreux :\n\n\n  Toute la direction technique participe et partage une partie de la veille technologique de chacun.\n  Les confÃ©renciers amÃ©liorent leur communication de â€œgroupeâ€.\n  Ils deviennent souvent le rÃ©fÃ©rent sur le sujet dans lâ€™entreprise.\n  Câ€™est du Team Building, et un rendez-vous mensuel avec vos Ã©quipes.\n  Et cela donne des idÃ©es Ã  tous les autres dÃ©veloppeurs pour de futurs projets (perso ou dâ€™entreprise bien sÃ»r), et attise leur curiositÃ©.\n\n\nNous avons donc tous les mois entre 5 et 10 prÃ©sentations proposÃ©es pour nâ€™en choisir que 4, et une trentaine de participants par session au niveau du public, et nous filmons toutes les confÃ©rences, et les archivons sur un site interne.\n\nLes â€œKaraokÃ© Slideshowâ€ pour plus de fun !\n\nPour combler les mois les plus creux, nous avons aussi tentÃ© une session â€œKaraokÃ© Slideshowâ€ qui fut hilarante (dans lâ€™esprit des â€œIgnite KaraokÃ©â€ pour ceux qui connaissent) ! \n Le concept : Entre 5 et 10 volontaires font une prÃ©sentation tour tour, sur une sÃ©rie de 5 slides quâ€™ils nâ€™ont jamais vus, dont chaque slide dÃ©file toutes les 15 secondes. Le thÃ¨me est libre et est improvisÃ© en fonction des slides !\n\nCâ€™est un bel exercice dâ€™improvisation et le fun est garanti :-)\n\nConclusion\n\nAlors si votre entreprise ou votre univers le permet, nous vous conseillons vraiment de tenter lâ€™aventure. Câ€™est trÃ¨s formateur, instructif, et intÃ©ressant pour tout le monde, et cela apporte une vraie culture de la veille dans votre environnement ;-)\n\n\n\nOlivier Mansour nous prÃ©sente la Programmation OrientÃ© Objet â€œCanada Dryâ€ !\n\n"
} ,
  
  {
    "title"    : "M6Web au banquet de la cuisine du web",
    "category" : "",
    "tags"     : " conference, lcdw",
    "url"      : "/m6web-au-banquet-de-la-cuisine-du-web",
    "date"     : "November 23, 2012",
    "excerpt"  : "Une partie de lâ€™Ã©quipe de M6Web Ã©tait prÃ©sente au banquet de la cuisine du web avec la fine fleur du web Lyonnais !\n\n\n\n",
  "content"  : "Une partie de lâ€™Ã©quipe de M6Web Ã©tait prÃ©sente au banquet de la cuisine du web avec la fine fleur du web Lyonnais !\n\n\n\n"
} ,
  
  {
    "title"    : "CR Velocity ConfÃ©rence Europe 2012 : Day 3",
    "category" : "",
    "tags"     : " conference, velocity, webperf, mobile, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-3",
    "date"     : "October 9, 2012",
    "excerpt"  : "\n\nTroisiÃ¨me et derniÃ¨re journÃ©e la VÃ©locity Europe.\n\nOn arrive dÃ©jÃ  fatiguÃ© et gavÃ© dâ€™informations et idÃ©es en tout genre, mais on a hÃ¢te de dÃ©marrer cette journÃ©e ! :-)\n\n[Mobile] The Performance of Web Vs Apps, par Ben Galbraith et Dion Almaer (W...",
  "content"  : "\n\nTroisiÃ¨me et derniÃ¨re journÃ©e la VÃ©locity Europe.\n\nOn arrive dÃ©jÃ  fatiguÃ© et gavÃ© dâ€™informations et idÃ©es en tout genre, mais on a hÃ¢te de dÃ©marrer cette journÃ©e ! :-)\n\n[Mobile] The Performance of Web Vs Apps, par Ben Galbraith et Dion Almaer (Walmart.com)\n\nBen (@bgalbs) et Dion (@dalmaer) nous reprennent dans les grandes lignes, la confÃ©rence faite la VÃ©locity Us (voir le CR + la vidÃ©o de ce talk : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-3-devops-webperf )\n\nLâ€™idÃ©e est de comparer les experiences possibles sur WebApp et Apps Native, avec toujours cette comparaison trÃ¨s drÃ´le entre le mode de distribution des apps natives ce que cela donnerait si les show tv devraient Ãªtre distribuÃ©s de la mÃªme maniÃ¨re en prenant lâ€™exemple de la sÃ©rie Friends : Hilarant !\n\nVoir la vidÃ©o ci-dessous vers 17min 30 :\n\n\n\nBen Galbraith et Dion Almaer (Source : https://royal.pingdom.com/2012/10/05/report-from-velocity-europe-day-3/ )\n\n\n\n[WebPerf] Lightning DÃ©mos, par Steve Souders et Patrick Meenan (Google)\n\nPatrick nous a montrÃ© les derniers ajouts intÃ©ressants de WebPageTest.org, avec notamment le â€œBlock Ads Featureâ€, lâ€™onglet â€œSPOFâ€ dans les paramÃ¨tres avancÃ©s pour tester si nos scripts tiers sont des SPOF sur nos sites (jâ€™y reviendrai) â€¦ Lâ€™outil sâ€™enrichit progressivement et reste toujours LA rÃ©fÃ©rence ultime du domaine !\n\n\n\nSteve Souders de son cÃ´tÃ©, est revenu sur un cas Ã©tudiÃ© la veille, Ã  savoir lâ€™implÃ©mentation dâ€™un LazyLoader sur un caroussel, afin de dÃ©terminÃ© via Browserscope.js si cela repoussait le OnLoad event, et câ€™est le cas !\n\nPetite parenthÃ¨se sur les caroussels, je vous invite lire cet article : Donâ€™t Use Automatic Image Sliders or Carousels, Ignore the Fad\n\n\n\n[WebPerf] Do All Users Benefit Equally from Web Performance Optimizations? , par Arnaud Becart (ip-label)\n\nTalk sponsorisÃ© assez intÃ©ressant, qui Ã©tudie les donnÃ©es rÃ©coltÃ©es par Ip-label afin de voir si tout le monde profite de la WebPerf de la mÃªme maniÃ¨re. La rÃ©ponse est Ã©vidente, mais câ€™est intÃ©ressant de rappeler quâ€™il faut toujours comparer tests synthÃ©tiques au rÃ©el, et quâ€™en fonction du navigateur, du terminal, de la puissance de votre machine, â€¦, des optimisations WebPerf auront un impact diffÃ©rent, du nÃ©gatif au trÃ¨s positif.\n\n\n\n[DevOps] From DevOps to Operation Science, par Christopher Brown (Opscode)\n\nSon twitter : @skeptomai\n\nTalk orientÃ© â€œCultureâ€ intÃ©ressant par lâ€™un des crÃ©ateurs de EC2. Mon moment dâ€™absence de la journÃ©e :)\n\n\n\n[WebPerf] Performance and Metrics on lonelyplanet.com, par Mark Jennings et Dave Nolan (Lonely Planet)\n\nRetour dâ€™experience des gars de Lonelyplanet (sorte de Routard) trÃ¨s enrichissant. Notamment sur la faÃ§on de communiquer Ã  des Ã©quipes non techniques, les diffÃ©rentes expÃ©rimentations rÃ©alisÃ©es, et le changement de culture opÃ©rÃ©, lâ€™utilisation de Graphite avec notamment les Holt-Winters â€¦\n\n\n  â€œBeing Right isnâ€™t Always Enoughâ€ !\n\n\n\n  â€œGive your metrics a public presenceâ€\n\n\nLes slides : https://fr.slideshare.net/mbjenn/performance-and-metrics-at-lonely-planet-14589911\n\n\n\nMark Jennings et Dave Nolan (Source : https://twitter.com/smcinnes/status/253805752312033280/photo/1 )\n\n\n\n[WebPerf] Third-Party Scripts and You, par Patrick Meenan (Google)\n\nPatrick Meenan nous parle ici de SPOF (Single Point Of Failure ou Point Individuel De Defaillance en franÃ§ais â€¦) et des 3rd party scripts.\n\nLâ€™idÃ©e est de montrer comment suivant lâ€™intÃ©gration Javascript de scripts tiers, vous pouvez rendre lâ€™affichage de votre site dÃ©pendant du bon fonctionnement des serveurs du script tiers.\n\nLes navigateurs mettent en gÃ©nÃ©ral 20 secondes (45 sous mac et linux) avant de rejeter une connexion sur un script tiers en rade. Vous pouvez voir des vidÃ©os de lâ€™effet que Ã§a peut avoir sur vos pages dans les slides.\n\nAfin de les dÃ©tecter, il existe lâ€™extension SPOF-O-Matic :\n\nhttps://chrome.google.com/webstore/detail/spof-o-matic/plikhggfbplemddobondkeogomgoodeg\n\nEn surfant, vous saurez rapidement si un SPOF est prÃ©sent sur votre site ou non et combien de contenu il bloque, et pourrez gÃ©nÃ©rer un WebPageTest comparatif en simulant le plantage du script en question (Redirection sur domaine blackhole.webpagetest.org)\n\nPour rÃ©gler ces problÃ¨mes, plusieurs solutions : Script Ã  charger dynamiquement via Js de maniÃ¨re asynchrone, script avec async et defer, ou au pire, script avant le /body.\n\nLes slides : https://www.slideshare.net/patrickmeenan/velocity-eu-2012-third-party-scripts-and-you\n\n\n\n[Ops] How Draw Something Absorbed 50 Million New Users, in 50 Days, with Zero Downtime, par J Chris Anderson (Couchbase)\n\nNous arrivons Ã  lâ€™entrÃ©e de la salle oÃ¹ a lieu cette prÃ©sentation, barrÃ© par des commerciaux CouchBase, nous empÃ©chant de rentrer sans prendre le prospectus CouchBase et sans se faire scanner son QRcode prÃ©sent sur nos badges â€¦ Ã§a commence mal â€¦\n\nAu bout de 2 minutes de talk par J Chris Anderson ( @jchris ) , co-fondateur de Couchbase, le malaise est confirmÃ© : on ne parlera pas ici de Draw Something, mais de Couchbase 2.0 uniquement, le nom Draw Something nâ€™Ã©tant lÃ  que pour appÃ¢ter du client potentiel, et Ã§a marche, la salle est comble â€¦\n\nDifficile du coup dâ€™Ãªtre concentrÃ© dans cette approche plus que douteuse â€¦ les questions au final seront aussi assez violentes sur le sujet : â€œpourquoi appÃ¢ter les gens avec Draw Something, si Ã§a nâ€™est que pour parler de CouchBase ?â€ La rÃ©ponse est Ã©vasive â€¦ Nous nâ€™avons pas eu le droit dâ€™en parler â€¦\n\nBref, le produit Couchbase a tout de meme lâ€™air trÃ¨s intÃ©ressant, et plutÃ´t costaud, avec de trÃ¨s chouettes Dashboard de monitoring temps rÃ©els built-in.\n\nJâ€™en sors quand mÃªme avec lâ€™impression trÃ¨s dÃ©sagrÃ©able de mâ€™Ãªtre fait piÃ©ger â€¦\n\nLes slides (non dispo) ressemblait fortement Ã  cette autre prÃ©sentation de J Chris : https://speakerdeck.com/u/jchris/p/nosql-landscape-speed-scale-and-json\n\n\n\n[WebPerf] WebPagetest - Beyond the Basics, par Aaron Peters (Turbobytes), Andy Davies (Asteno)\n\nPas mal de confÃ©rences parlaient de WebPageTest, mais celle-ci promettait dâ€™aller en profondeur. Le crÃ©ateur nâ€™a jamais cachÃ© son manque de talent pour les interfaces, et WPT regorge de richesses en tout genre cachÃ©es dans les mÃ©andres de ses pages :-)\n\nEnormement dâ€™informations et tips sont donc prÃ©sents dans les slides de cette confÃ©rence.\n\nPour rappel : instruction dâ€™Andy pour monter une instance privÃ©e de WPT : https://andydavies.me/blog/2012/09/18/how-to-create-an-all-in-one-webpagetest-private-instance/\n\nLes slides : https://www.slideshare.net/AndyDavies/web-page-test-beyond-the-basics\n\n\n\nAndy Davies et Aaron Peters (Source : https://royal.pingdom.com/2012/10/05/report-from-velocity-europe-day-3/ )\n\n\n\n[DevOps] What HTTP/2.0 Will* Do For You, par Mark Nottingham (Akamai)\n\nLâ€™une des confÃ©rences les plus intÃ©ressantes de la VÃ©locity pour ma part avec notamment lâ€™annonce que HTTP/2.0 sera basÃ© sur SPDY dÃ©jâ€¦\n\nMark Nottingham ( @mnot ), Chair of the IETF HTTPbis Working Group, excellent confÃ©rencier, nous explique donc ce que sera HTTP/2.0 :\n\n\n  Aucun changement la sÃ©mantique HTTP\n  BasÃ© sur Speedy\n  Multiplexing (voir slide 22)\n  Header Compression, technique trÃ¨s intÃ©ressante, pour Ã©viter de rÃ©-envoyer les memes headers pour chaque requÃªte HTTP\n\n\nPas mal de ressources sont disponibles sur son site : https://www.mnot.net/\n\nLes slides sont un modÃ¨le du genre, simples et efficaces : https://www.slideshare.net/mnot/what-http20-will-do-for-you\n\n\n\n[DevOps] Web &amp;amp; Native Cross-Platform Multiplayer, par Ashraf Samy Hegab (Orange)\n\nComment dÃ©velopper une expÃ©rience de Gaming multi-plateforme : Web, Android, Iphone ? Câ€™est la question Ã  laquelle Ashraf a essayÃ© de rÃ©pondre, sur cette derniÃ¨re confÃ©rence avec un humour et une Ã©nergie trÃ¨s communicative.\n\nPas mal de bonnes ideÃ©s applicables au web traditionnel, sur une stack NodeJs/Mongo/Socket.io, pour faire la majoritÃ© du travail et communiquer avec les parties natives dâ€™app Android et Ios.\n\nNous avons aussi fait une dÃ©mo live sur le jeu Phone Wars (Disponible sur Appstore et Google Play) dâ€™une Ã©xperience Gaming Multi-plateforme.\n\nTrÃ¨s rafraÃ®chissant pour finir ces 3 journÃ©es marathons !\n\nConclusion :\n\nÃ‡a yâ€™est, la VÃ©locity Europe est finie pour cette annÃ©e. Les bouchÃ©es doubles ont Ã©tÃ© mises par rapport Ã  la VÃ©locity Berlin de lâ€™annÃ©e derniÃ¨re, et cette confÃ©rence reste vraiment la conf incontournable pour tous ceux que la Webperf, les Devops, et les sites fort traffic intÃ©ressent !\n\nOn regrette simplement que seule la grande salle ait Ã©tÃ© filmÃ©e, que la chasse aux slides soit toujours aussi tordue (trop peu renseignÃ© sur le site de la VÃ©locity). Le reste est juste parfait !\n\nA la prochaine, et merci pour vos retours.\n\nP.s: Merci aussi aux Ã©quipes de Pingdom pour leurs Twitt Live et les chouettes photos prises ( https://royal.pingdom.com/ )\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-2\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-1\n\n"
} ,
  
  {
    "title"    : "CR Velocity ConfÃ©rence Europe 2012 : Day 2",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-2",
    "date"     : "October 8, 2012",
    "excerpt"  : "\n\nDeuxiÃ¨me journÃ©e, avec le dÃ©but â€œofficielâ€ de cette confÃ©rence, oÃ¹ lâ€™on nous donne rendez vous dans lâ€™immense Kingâ€™s Room.\n\nPour bien dÃ©marrer, on commence avec en quelque sorte lâ€™hymne de la VÃ©locity : Speed &amp;amp; Velocity !\n\n\n\n[DevOps] Move Fa...",
  "content"  : "\n\nDeuxiÃ¨me journÃ©e, avec le dÃ©but â€œofficielâ€ de cette confÃ©rence, oÃ¹ lâ€™on nous donne rendez vous dans lâ€™immense Kingâ€™s Room.\n\nPour bien dÃ©marrer, on commence avec en quelque sorte lâ€™hymne de la VÃ©locity : Speed &amp;amp; Velocity !\n\n\n\n[DevOps] Move Fast and Ship Things, par Girish Patangay (Facebook)\n\nPremiÃ¨re prÃ©sentation de la journÃ©e par lâ€™un des Manager chez Facebook, (maintenant dans les bureaux londoniens), sur la capacitÃ© de Facebook, et leur volontÃ©, dâ€™ Ã©voluer et de dÃ©ployer rapidement.\n\nIl nous raconte les dÃ©buts de Facebook, avec peu de serveurs, des changements infÃ©rieurs 5Mb, Rsync pour pusher en prod etc â€¦ puis la migration vers HipHop.\n\nDÃ©sormais chaque changement nÃ©cessite de recompiler un gros binaire de 1.2Go, et dâ€™y envoyer sur plus de 10 000 serveurs, et ce plusieurs fois par jour !\n\nAvec Bittorent, ils envoient 500Mb en moins dâ€™une minute sur les 10 000 serveurs.\n\nOn a eu le droit une prÃ©sentation de GateKeeper, outil interne, permettant de faire du feature flipping gÃ©olocalisÃ©. La timeline a par exemple plus dâ€™une centaine de GateKepper.\n\nAujourdâ€™hui, Facebook cherche trouver le moyen de scaler de plus de 1000 dÃ©veloppeurs Ã  10000, et dâ€™Ã©voluer sur ce systÃ¨me de â€œMove Fastâ€ dans le mobile natif.\n\nPour ceux que Ã§a intÃ©resse, en plus de la vidÃ©o Ã§i dessous, Quora est une mine dâ€™or dâ€™infos sur FB : https://www.quora.com/Girish-Patangay\n\n\n\n(Source https://royal.pingdom.com/2012/10/03/report-from-velocity-europe-day-2/ )\n\n\n\n[WebPerf] Keynote KITE and MITE, par Robert Castley (Keynote)\n\nVient ensuite la confÃ©rence de Keynote (SponsorisÃ©e), qui nous prÃ©sente deux outils intÃ©ressant :\n\n\n  KITE pour Keynote Internet Testing Environnement : https://kite.keynote.com/\n  MITE pour Mobile Internet Testing Environnement : https://mite.keynote.com/\n\n\nMITE est dâ€™ailleurs utilisÃ© par le site de Google Howtogomo.com\n\nBref, si vous Ãªtes sur Windows (â€¦), jetez y un oeil. Plus dâ€™infos en vidÃ©o :\n\n\n\n[WebPerf] Lightning DÃ©mos\n\nDÃ©mo 1 : Chrome Dev Tools :\n\nPremiÃ¨re dÃ©mo de Iliya Grigorik ( @igrigorik) sur les capacitÃ©s avancÃ©es de la chrome Dev Toolbar.\n\nOn peut par exemple faire un clic droit sur lâ€™onglet Network pour rÃ©cupÃ©rer le har (voir le format HTTP Archive) en json, et utiliser dâ€™autres outils avec ce har, notamment Yslow dont je parlais dans le compte rendu suivant, qui permet dâ€™ajouter les rÃ©gressions possible WebPerf dans votre CI Jenkins.\n\nOn parle aussi du chrome://tracing , du dÃ©bugger mobile, que la devtools est une WebApp avec une url propre et scriptable, du Chrome Benchmarking (extension) â€¦\n\nVoir la prÃ©sentation suivante pour plus dâ€™infos, bien plus complÃ©te : https://www.igvita.com/slides/2012/devtools-tips-and-tricks/\n\n\n\nDÃ©mo 2 : Box Anemometer :\n\nGavin Towey ( @gtowey), DBA MySql chez Box.com nous prÃ©sente une interface pour visualiser et traiter correctement le slow log de Mysql. BasÃ© sur Php 5.3, les outils Percona et Bootstrap pour lâ€™interface, lâ€™outil est un vrai bonheur pour tous ceux qui font un peu dâ€™optimisation MySql au quotidien, dÃ©veloppeur, sysadmin ou dba. Nous lâ€™avions dÃ©jÃ  dÃ©couvert aux DevOpsDays Mountain View cet Ã©tÃ©, et lâ€™utilisons massivement depuis.\n\nLe projet est sur Github : https://github.com/box/Anemometer\n\nVoir aussi le projet Rain Gauge dans la lignÃ©e de Anemometer, toujours par Gavin Towey : https://github.com/box/RainGauge\n\nVoici la dÃ©mo en vidÃ©o :\n\n\n\n[WebPerf] Emerging Markets / Growth Markets, par Jeff Kim (CDnetworks)\n\nJeff Kim, Chief Operating Officer chez CDnetworks nous a partagÃ© quelques donnÃ©es et chiffres intÃ©ressants sur les marchÃ©s Ã©mergeants comme lâ€™Inde, IndonÃ©sie, les Philippines, le BrÃ©sil etc\n\nOn apprend que Chrome a une part de marchÃ© de 62% au BrÃ©sil, 51% en Inde, et OpÃ©ra de 26% en Russie. Que lâ€™e-commerce au final nâ€™a pas vraiment encore dÃ©marrÃ© en Inde, BrÃ©sil et Russieâ€¦\n\nAprÃ¨s des Ã©tudes dâ€™Eye Tracking View, on remarque aussi entre la population chinoise et amÃ©ricaine, que sur une page de rÃ©sultats de recherche, les amÃ©ricains ne regardent que le coin haut gauche de la page pour se contenter des premiers rÃ©sultats, alors que les chinois consultent vraiment toute la hauteur de la page, pour regarder tous les rÃ©sultats.\n\n\n\n[WebPerf] Why page speed isnâ€™t enough, par Tim Morrow (Betfair)\n\nSon twitter : @timmorrow\n\nAncien de ShopZilla, et dÃ©ja prÃ©sent Berlin lâ€™annÃ©e derniÃ¨re, Tim a partagÃ© son retour dâ€™expÃ©rience sur la refonte de BetFair (trÃ¨s gros site de pari en ligne). Les gens se plaignaient dâ€™une mauvaise expÃ©rience (alors que les pages refondues Ã©taient plus rapides), mais nÃ©cessitaient Ã  priori beaucoup plus de temps pour parvenir au pari final que dans lâ€™ancienne version. En gros, le temps de chargement de vos pages nâ€™est pas suffisant, il faut aussi regarder les scÃ©narios fonctionnels de vos sites. Ils sont passÃ©s sur une navigation typÃ©e Ajax pour ne pas rafraÃ®chir dans certains cas lâ€™intÃ©gralitÃ© de la page.\n\nVoir les slides et la vidÃ©o : https://fr.slideshare.net/timmorrow/why-page-speed-isnt-enough-tim-morrow-velocity-europe-2012\n\n\n\n\n[WebPerf] W3C Status on Web Performance, par Alois Reitbauer (Compuware, Dynatrace)\n\nBeau rÃ©capitulatif du status du â€œW3C performance working groupâ€ sur la performance Web, avec des rappels sur la NavTiming et les diffÃ©rents standards, quelques Ã©changes autour de la NavTimingV2, dâ€™une Resource time Measurement etc â€¦\n\nSon twitter : @compuware et @AloisReitbauer\n\nPlus dâ€™infos dans la vidÃ©o ci dessous :\n\n\n\n[WebPerf] 3.5s Dash for attention and other stuff we found about RUM, par Philipp Tellis (Log Normal)\n\nSon twitter : @bluesmoon\n\nLe transcript et dÃ©tail complet de la prÃ©sentation est prÃ©sent sur un blogPost trÃ¨s complet de Philip Tellis, crÃ©ateur de Log Normal (rachetÃ© par SOASTA) :\n\nhttps://www.lognormal.com/blog/2012/10/03/the-3.5s-dash-for-attention/\n\nBeaucoup de chiffres et dâ€™infos intÃ©ressantes autour de la WebPerf, avec notamment cette mÃ©trique basÃ©e sur un Bounce Rate &amp;gt;= 50%\n\nLes slides sont disponibles : https://fr.slideshare.net/bluesmoon/the-35s-dash-for-user-attention-and-other-things-we-found-in-rum\n\n\n\n[Mobile] Escaping the uncanny valley, par Andrew Betts (FT Labs)\n\nSon twitter : @triblondon\n\nConfÃ©rence trÃ¨s intÃ©ressante par Andrew, le directeur dâ€™FT Labs (Financial Times), qui a parcouru lâ€™ensemble des travaux que ces Ã©quipes ont effectuÃ©s autour des capacitÃ©s mobile pour faire une webapp HTML5 intÃ©grÃ©e dans une appli native avec la meilleure experience possible.\n\nAu menu, rappel sur la lourdeur de parcours du DOM en JS, les incohÃ©rences du SetTimeout (notamment sur IOs), les problÃ©matiques de lâ€™AppCache ou localStorage. Le fait dâ€™utiliser au maximum lâ€™accÃ©lÃ©ration matÃ©rielle sur les CSS, lâ€™optimisation des paint, les spinners et loading bar, le progressive rendering â€¦\n\nRappel de la latence sur les â€œclicsâ€ tactiles avec le projet Fastclick, qui enlÃ¨ve les 300ms de dÃ©lai sur le clic mobile : https://github.com/ftlabs/fastclick/\n\n[Mobile] Make your mobile web apps fly, par Sam Dutton (Google)\n\nSon twitter : @sw12\n\nSam Dutton, Developers Advocate (?) chez Google, nous fait un parcours assez complet des bases dâ€™optimisations WebPerf pour le mobile. Un peu de redondance versus dâ€™autres confs vues plus tÃ´t, mais le sujet est bien maÃ®trisÃ© et bien couvert.\n\nQuelques petits outils intÃ©ressants, notamment le Multires pour la gestion des images Retina : https://fhtr.org/multires/\n\nA noter aussi une phrase que jâ€™ai beaucoup aimÃ© sur lâ€™ergnonomie mobile, et la position des boutons de contrÃ´le :\n\n\n  â€œControls should be beneath content: think calculatorâ€\n\n\nLes slides : https://www.samdutton.com/velocity2012/ \n\n[DevOps] Scaling Instagram, par Mike Krieger (Instagram)\n\nSon twitter : @mikeyk\n\nPetit debrief orientÃ© Ops de la success story dâ€™Instragram. Le rythme de la prÃ©sentation Ã©tait vraiment pushy, donc plutÃ´t dur suivre pour nous autre francophones â€¦\n\nEn plus dâ€™avoir lâ€™impression de voir un demi milliard bouger sur scÃ¨ne, nous avons quand mÃªme appris certaines choses sur la Stack Instagram : EC2, Python Django, Postgres, Gearman, RabbitMQ â€¦\n\nLa prÃ©sentation nâ€™Ã©tant pas nouvelle, est disponible ici : https://fr.slideshare.net/iammutex/scaling-instagram\n\n\n\nMike Krieger (Source : https://royal.pingdom.com/2012/10/03/report-from-velocity-europe-day-2/ )\n\n\n\n[WebPerf] Bringing HTML5 into Nativelandia: A Tale of Caution, par Jackson Gabbard (Facebook)\n\nLâ€™une de confÃ©rences que jâ€™attendais beaucoup, de Jackson Gabbard, Mobile Engineer chez Facebook, qui nous explique le passage du HTML5 au native pour les applications mobile (IOs pour le moment?).\n\nPassÃ© les stats toujours aussi hallucinantes, il explique tout ce qui nâ€™Ã©tait pas convainquant sur lâ€™ancienne webApp HTML5, et que ce qui a vraiment Ã©chouÃ©, est le marriage entre la WebView et le natif. Que lâ€™expÃ©rience â€œnativeâ€ est bien plus concluante pour lâ€™utilisateur par rapport ce quâ€™ils souhaitent obtenir niveau fluiditÃ©, performances, efficacitÃ©, utilisation rÃ©seau etc â€¦\n\nIls ont du coup redeveloppÃ© pas mal de leurs outils pour sâ€™adapter a ce mode de fonctionnement (un GateKeeper plus light) etc â€¦\n\nConfÃ©rence vraiment passionnante, avec un gars plutÃ´t trÃ¨s transparent sur Facebook et les raisons qui ont poussÃ©es ce changement.\n\nConclusion :\n\nDu lourd encore une fois, avec Ã©normement de sujets trÃ¨s intÃ©ressants, mÃªme si lâ€™on commence Ã  tourner un peu en rond autour de la WebPerf Mobile.\n\nA noter aussi quâ€™un grand nombre de livre Oreilly ont Ã©tÃ© donnÃ© et dÃ©dicacÃ© par Steve Souders et John Allspaw ;-)\n\nLes salles sont dÃ©jÃ  bien plus sympa, et lâ€™organisation toujours au top ! Vivement demain.\n\nPour finir, voici quelques vidÃ©os diffusÃ©es pendant les breaks la VÃ©locity :\n\n\nHot Wheels World Record: Double Loop Dare at the 2012 X Games Los Angeles\n\n\nJeb Corliss â€œ Grinding The Crackâ€\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-3\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-1\n\n"
} ,
  
  {
    "title"    : "CR Velocity ConfÃ©rence Europe 2012 : Day 1",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-1",
    "date"     : "October 4, 2012",
    "excerpt"  : "Nous voici de retour pour une VÃ©locity Conference, le paradis de la WebPerf et des Devops !\n\nAprÃ¨s lâ€™excellente moisson de la VÃ©locity US de Santa Clara, dont voici nos 3 CR :\n\n\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-da...",
  "content"  : "Nous voici de retour pour une VÃ©locity Conference, le paradis de la WebPerf et des Devops !\n\nAprÃ¨s lâ€™excellente moisson de la VÃ©locity US de Santa Clara, dont voici nos 3 CR :\n\n\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-2-devops-webperf\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-3-devops-webperf\n\n\nEt aprÃ¨s la session de lâ€™annÃ©e derniÃ¨re qui avait lieu Ã  Berlin, nous nous retrouvons cette fois dans la capitale anglaise Londres, au Hilton hÃ´tel.\n\nVoici le compte rendu des confÃ©rences de la premiÃ¨re journÃ©e, journÃ©e un peu Ã  part placÃ©e sous le signe des â€œTutorialsâ€ (2 octobre 2012)\n\n[OPS] Monitoring and Observability in complex architecture, par Theo Schlossnagle (OmniTI)\n\nPremiÃ¨re confÃ©rence de la journÃ©e, avec Theo, habituÃ© des Velocity, et plutÃ´t expert dans les domaines â€œinfraâ€ et â€œmonitoringâ€. CrÃ©ateur de Omniti, MessageSystems et Circonus.\n\nSon twitter : @postwait\n\nTheo nous explique comment monitorer et observer des archi complexes avec une prÃ©sentation trÃ¨s bas niveau.\n\nLes outils de collectes de statistiques quâ€™il cite :\n\n\n  Metrics.js : https://github.com/mikejihbe/metrics\n  Resmon : https://labs.omniti.com/labs/resmon\n  Folsom : https://github.com/boundary/folsom\n  Metrics : https://metrics.codahale.com/\n  Metrics-net : https://github.com/danielcrenna/metrics-net\n  StatsD : https://github.com/etsy/statsd\n\n\nEt pour le stockage :\n\n\n  Reconnoiter : https://labs.omniti.com/labs/reconnoiter\n  Graphite : https://graphite.wikidot.com/\n  OpenTSDB : https://opentsdb.net/\n  Circonus : https://circonus.com/\n  Librato : https://metrics.librato.com/\n\n\nOnt suivi ensuite des dÃ©mos tcpdump assez poussÃ©es plutÃ´t intÃ©ressante :\n Par exemple, pour voir les nouvelles connexions entrantes (rÃ©cupÃ©ration des packets SYN) : tcpdump -nnq -tttt -s384 â€˜tcpport 80 and(tcp[13] &amp;amp; (2|16) == 2)â€™\n\n\n  dâ€™exemples sont disponibles dans les slides, avec aussi dâ€™autres exemples live sur â€œstraceâ€ et â€œdtraceâ€.\n\n\nBref, ca commence trÃ¨s (trop?) fort, surtout pour nous, pas forcÃ©ment de nature trÃ¨s â€œOPSâ€ !\n\nLes slides sont disponible ici et plutÃ´t parlantes pour ceux qui voudraient creuser le sujet : https://www.slideshare.net/postwait/monitoring-and-observability\n\n\n\n\n  â€œYou cannot correct what you cannot measureâ€ Theo Schlossnagle\n\n\n\n\nTheo Schlossnagle (Source : https://img.ly/o0Ht )\n\n[DevOps] Escalading Scenario : a deep dive into outage falls, par John Allspaw (Etsy)\n\nOn prend toujours autant de plaisir Ã©couter John Allspaw, VP dâ€™Esty et Co-organisateur de la VÃ©locity avec Steve Souders, nous parler dâ€™incident, et de la meilleure maniÃ¨re de les gÃ©rer.\n\nSon Twitter : @allspaw\n\nBeaucoup de parallÃ¨les sont faits avec des incidents dans lâ€™aviation, lâ€™industrie, voir lâ€™armÃ©e, avec des rÃ©fÃ©rences aussi au PostMortem dâ€™appolo 13 â€¦ Un joli condensÃ© de bouquins â€œMust Readâ€ dâ€™aprÃ¨s Allspaw comme : Normal Accidents ou encore Naturalistic Decision Making, nous sont prÃ©sentÃ©s.\n\nDifficile dâ€™en faire un rÃ©sumÃ©, tellement la confÃ©rence Ã©tait bourrÃ©e dâ€™informations, graphiques et anecdotes en tout genre ! MalgrÃ© tout, dans les â€œchosesâ€ retenir, en vrac :\n\n\n  Attention au contexte de vos graphiques : un graph qui parait anormal sur une heure, peut sâ€™avÃ©rer normal sur une Ã©chelle de temps dâ€™une journÃ©e par exemple\n  Des bons conseils sur la rÃ©solution dâ€™incident en Ã©quipe, notamment au niveau de la communication avec des exemples de conversation pendant des incidents chez Etsy trop ambigus. Il est important de confirmer les rÃ©ponses, corriger la communication des autres, afin dâ€™Ã©viter tout soucis de comprÃ©hension\n  Ne pas hÃ©siter Ã  demander des â€œpre-mortemâ€. Dire Ã  lâ€™auteur du projet par exemple, que cela va planter dans plusieurs mois, et lui demander dâ€™essayer de trouver la ou les raisons qui pourraient amener le projet au plantage.\n  â€¦\n\n\nJe vous invite encore consulter les slides pour plus dâ€™informations et de rÃ©fÃ©rences : https://fr.slideshare.net/jallspaw/velocity-eu-2012-escalating-scenarios-outage-handling-pitfalls\n\n\n\n\n\nJohn Allspaw (Source : https://twitter.com/lozzd/status/253074489540239360 )\n\n[WebPerf] Running a WebPerf Dashboard in 90 minutes, par Jeroen Tjepkema\n\nSon twitter : @jeroentjepkema\n\nLâ€™objectif de cette confÃ©rence Ã©tait de proposer en 90 minutes, les Ã©tapes nÃ©cessaire pour monter un dashboard orientÃ© WebPerf.\n\nOn re-parcourt du coup un peu tout le classique de la performance web, en prÃ©sentant dÃ©jÃ  quelques exemples de Dashboard (rien de trÃ¨s sexy â€¦, hormis peut-Ãªtre celui de Nrc.nl orientÃ© â€œaudience Ã©ditorialeâ€ plutot intÃ©ressant), la pertinence de certains types de â€œgraphsâ€ comme les â€œheatmapâ€ et aussi en comparant les diffÃ©rentes solutions pour mesurer la performance web, avec avantages et inconvÃ©nients :\n\n\n  Synthetic Monitoring (Gomez, Keynote, IpLabel, Pingdom etc) (slide 104-105)\n  Real User Monitoring (LogNormal dont lâ€™acquisition par Soasta a aussi Ã©tÃ© annoncÃ© ce jour, Boomerang.js, Torbit, Google Analytics â€¦) (slide 121-122)\n  Real User Benchmarking (WebPageTest) (slide 134)\n\n\nQuelques idÃ©es sympa de design sont dissÃ©minÃ©es tout au long de cette longue prÃ©sentation, on regrette simplement de survoler toujours un peu tous les concepts, mais malgrÃ©s tout, cela reste lâ€™une des rares tentatives de faire un dashboard WebPerf accessible Ã  des â€œnon-techniciensâ€. Chapeau pour cela.\n\nUne dÃ©mo du dashboard est testable ici : https://app.measureworks.nl/secured/dashboard (Login : demo@measureworks.nl , password: performance )\n\nLes slides sont disponibles ici : https://www.slideshare.net/MeasureWorks/measureworks-velocity-conference-europe-2012-a-web-performance-dashboard-final\n\n\n\n\n\nJeroen Tjepkema (Source : https://twitter.com/pingdom/status/253135289327951872 )\n\n[WebPerf] Deep Dive into Performance analysis, par Steve Souders et Patrick Meenan (Google)\n\nLeurs Twitter : @souders et @patmeenan\n\nDernier â€œTutorialsâ€ du jour avec Steve Souders (Chief performance Officer chez Google), et Patrick Meeman (aussi chez Google dÃ©sormais, crÃ©ateur de Webpagetest).\n\nHistoire de sâ€™adapter au public Anglais, les deux compÃ¨res ont dÃ©cidÃ©s de sâ€™attaquer aux sites des Ã©quipes de Premier League du Foot Anglais :-)\n\nComme imaginÃ©, ca nâ€™est pas â€œfolichonâ€, et on Ã©tudiera en profondeur en live les sites de Chelsea et Tottenham, qui chacun enchaine un nombre dâ€™aberrations plus grandes les une que les autres !\n\nLe tableur utilisÃ© pendant la prÃ©sentation avec les liens vers les tests WebPageTest : goo.gl/YfbRn\n\nQuelques exemples sur le site de Chelsea : https://www.chelseafc.com/\n\nLes tests WPT annoncent un Load Time 21 secondes, 203 requÃªtes HTTP et 3mo4 tÃ©lÃ©chargÃ©s !\n\nLe Waterfall que vous pouvez voir ici, est on ne peut plus parlant, avec une mention spÃ©ciale pour la liste des images prÃ©sentes sur la HP : https://velocity.webpagetest.org/pageimages.php?test=120925_0_13&amp;amp;run=2&amp;amp;cached=0\n\nJâ€™imagine que le problÃ¨me va vous sauter aux yeux, entre les fonds Ã©normissimes et images non comprÃ©ssÃ©es, les images de chacun des joueurs de lâ€™Ã©quipe (oui, le pauvre carroussel en bas de page â€¦), et le nombre incroyable de logo et ou picto en tout genre, on voit vite comment amÃ©liorer la page :-)\n\nLe carroussel du haut donne aussi un effet assez comique sur le â€œFilmStrip Viewâ€ oÃ¹ lâ€™ont voit vers les 10 secondes, un dÃ©but dâ€™image se charger, pour sâ€™effacer car le carroussel passe dÃ©ja au panel suivant â€¦ Merci au passage au jCaroussel qui charge bÃªtement toutes les images â€¦\n\nOn remarque aussi un nombre consÃ©quent de JS qui retarde grandement le Start Render. Optez autant que possible pour les positionner juste avant le /body, ou les charger en async/defer ou via un chargement asynchrone en Js.\n\nPas mal de petites astuces sont partagÃ©es par Patrick et Steve, notamment sur lâ€™utilisation de la courbe de Bande Passante, pour voir les parties pouvant Ãªtre optimisÃ©es (celle ou la bande passante nâ€™est pas utilisÃ©e fond par exemple), on remarque aussi quelques ajouts rÃ©cents comme lâ€™affichage des Ã©venements Paint sur la FilmStrip View (Screenshot encadrÃ© de Orange), ou encore la possibilitÃ© via clic droit dans la Chrome Dev Tools de vider le cache et les cookies rapidement etc â€¦\n\nNous avons aussi eu la confirmation que Google prenait le Onload Time comme rÃ©fÃ©rence pour ses algorithmes de ranking.\n\nBref, superbe application de tous les concepts WebPerf avec des cas concrets dâ€™Ã©tude et une confÃ©rence trÃ¨s interactive avec suffisament dâ€™astuces pour combler aussi les habituÃ©s de WPT.\n\n\n\nPatrick Meenan &amp;amp; Steve Souders (Source : https://twitter.com/simonox/status/253146271156670464 )\n\n[DevOps] Ignite Talk\n\nPour finir cette journÃ©e, rendez vous dans lâ€™immense salle (dans laquelle aura lieu la majoritÃ© des confÃ©rences suivantes), pour un Ignite Talk combinÃ© entre la VÃ©locity ConfÃ©rence et Strata ConfÃ©rence qui ont lieu au mÃªme moment dans lâ€™Hilton hÃ´tel Londres.\n\n\n\nSalle Kingâ€™s Room (Source : https://twitter.com/cmsj/status/253139957093367808/photo/1 )\n\nNous suivrons une sÃ©rie de 7 ou 8 Ignite Talk dont le concept est de prÃ©senter un sujet sur 20 slides dÃ©filant automatiquement toutes les 15 secondes. Câ€™est assez dÃ©calÃ©, fun, et lâ€™exercice parait trÃ¨s â€œsportâ€ â€¦ :)\n\nPas mal de sujets tournaient autour de lâ€™Open Data ou Big Data, les DataViz â€¦\n\nVoici par exemple un talk sympa sur les â€œDataviz as interfaceâ€ par @makoto_inoue : https://fr.slideshare.net/inouemak/data-viz-asinterfacemakotoinoue\n\n\n\n\n\nMakoto Inoue, de lâ€™Ã©nergie, de la danse, et de la biÃ¨re ! (Source: https://royal.pingdom.com/2012/10/02/velocity-europe-1/ )\n\n[WebPerf] Step by Step Mobile Optimization, par Guy Podjamy (Akamai)\n\nConfÃ©rence auquelle je nâ€™ai pas pu assistÃ© : https://fr.slideshare.net/guypod/step-by-step-mobile-optimization\n\n\n\nConclusion :\n\nBonne premiÃ¨re journÃ©e avec dÃ©jpas mal de choses retenir et appliquer quotidiennement !\n\nOn regrette le fait dâ€™avoir Ã©tÃ© dans des petites salles (sÃ»rement Ã  cause de la derniÃ¨re journÃ©e de la Strata ConfÃ©rence), et du coup dâ€™avoir alternÃ© le manque de place, avec la chaleur des salles â€¦ Et je nâ€™ai pas lâ€™impression que les sessions du jour Ã©taient filmÃ©es malheuresement !\n\nCâ€™est en tout cas un trÃ¨s bon avant-gout de ce qui nous attend demain ;-)\n\nEnjoy !\n\nP.s : Nâ€™hÃ©sitez pas nous faire des retours sur ce CR ! :)\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-2\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-3\n\n"
} ,
  
  {
    "title"    : "Tentative d&#39;explication des Fast-Forward sous Git",
    "category" : "",
    "tags"     : " git",
    "url"      : "/tentative-d-explication-des-fast-forward-sous-git",
    "date"     : "July 13, 2012",
    "excerpt"  : "\n\nTous les projets M6Web sont passÃ©s rÃ©cemment sous le systÃ¨me de gestion de contenu Git.\n\nGit, câ€™est super cool ! On peut faire facilement des branches, les â€œmergerâ€ les unes aux autres et â€œswitcherâ€ dâ€™une branche une autre. Pratique donc (dans l...",
  "content"  : "\n\nTous les projets M6Web sont passÃ©s rÃ©cemment sous le systÃ¨me de gestion de contenu Git.\n\nGit, câ€™est super cool ! On peut faire facilement des branches, les â€œmergerâ€ les unes aux autres et â€œswitcherâ€ dâ€™une branche une autre. Pratique donc (dans lâ€™idÃ©e) !\n\nIl a Ã©tÃ© finalement assez facile de se faire au vocabulaire et au fonctionnement de git. Je ne dis pas que je ne fais pas non plus mes commit sur la bonne branche chaque fois, mais on arrive tout de mÃªme assez facilement Ã  sâ€™en sortir (â€œgit reset â€“helpâ€ si vous Ãªtes dans ce cas) !\n\nLe trÃ¨s intÃ©ressant article â€œA successful Git branching modelâ€ a mis en avant la gestion des fast-forward, cela dit lâ€™utilitÃ© mâ€™est restÃ© assez floue et ne couvrait pas lâ€™ensemble de mes questions.\n\nJâ€™ai donc fouillÃ© la documentation de Git afin de dÃ©broussailler les â€œfast-forwardâ€.\n\nQue fait git lors dâ€™un merge\n\nGit â€œmergeâ€ deux branches lorsque :\n\n\n  La commande â€œmergeâ€ est utilisÃ©e, par exemple git merge feature-myfeature,\n  la commande pull est utilisÃ©e, exemple git pull origin master\n\n\nGit â€œfast-forwardâ€\n\nMettons que Bob fasse une modification sur une branche, il crÃ©e un commit Y.\n\nIl fait une autre modification quâ€™il commit, il crÃ©e alors un commit Z.\n\nAutomatiquement git â€œfast-forwardâ€, câ€™est dire quâ€™il fait pointer la branche, qui pointait sur le commit Y, vers le commit Z. Sur les graphiques de git log, les deux commit sont liÃ©s par un trait continu.\n\n- Y - Z\n\nTant que Bob continue faire des modifications + commit sans toucher au fast-forward, git va automatiquement â€œfast-forwarderâ€. On aura donc un enchaÃ®nement de commit qui sont liÃ©s par un trait continu.\n\n- Y - Z - AA - AB - AC - ...\n\nGit ne â€œfast-forwardâ€ pas\n\nNous sommes maintenant sur une branche qui en est la rÃ©vision X.\n\nAlice travaille sur son projet et crÃ©e la rÃ©vision A.\n\nCela dit, Bob travaille aussi sur le projet et crÃ©e la rÃ©vision B.\n\nAlice pousse ses modifications :\n\nLe commit A a pour parent le commit X, qui est le dernier commit connu par la branche, Git peut donc â€œfast-forwarderâ€.\n\n- X - A\n\nBob pousse ses modifications :\n\nLe commit de bob ne connaÃ®t pas de commit A dans son historique (son commit parent est le commit X).\n\n\n- X - A\n   \\\n    B\n\nSi git â€œfast-forwardaitâ€ ici, il ferait pointer la branche sur le commit B, et perdrait le commit A. Comme on ne souhaite pas perdre les modifications dâ€™Alice, il va donc passer en mode â€œno fast-forwardâ€ automatiquement.\n\nGit va donc rÃ©cupÃ©rer les modifications de A et les mÃ©langer (merge) aux modifications de B en crÃ©ant un commit C.\n\nLe commit C a pour parent les commit** B** et** A, le pointeur de dernier commit peut donc Ãªtre placÃ© sur **C sans risque de perte dâ€™historique.\n\n- X - A\n   \\   \\\n    B - C\n\nOption* â€“no-ff*\n\nLâ€™auteur de lâ€™article citÃ© prÃ©cÃ©demment conseille dâ€™utiliser lâ€™option â€“no-ff sur les merge.\n\nCette option force git a crÃ©er un commit de â€œmergeâ€ qui aura pour parents notre commit de modification, et le dernier commit connu sur la branche, mÃªme si il nâ€™y a pas eu de modification sur cette derniÃ¨re.\n\nCela permet de revenir facilement Ã  la version antÃ©rieure de la branche, sans avoir Ã  fouiller dans les nombreux commits ayant pu amener un bug : on revient Ã  la version initiale avant de passer plus de temps pour corriger le bug.\n\n- M1 -- -- -- -- M2\n    \\            /\n     B1 - B2 - B3\n\nDans lâ€™exemple ci-dessus, on peut facilement revenir au commit M1, et exclure ainsi toute la branche B. Si on avait â€œfast-forwardÃ©â€, il nous aurait fallut retrouver le commit M1 en regardant tous les commit prÃ©cÃ©dent.\n\nMode auto contre â€“no-ff\n\nForcer le *â€“no-ff *ne sera finalement utile que lorsque vous dÃ©veloppez une fonctionnalitÃ© pour laquelle vous allez beaucoup â€œcommiterâ€, sans que personne dâ€™autre ne commit entre-temps.\n\nA vous de lâ€™utiliser de maniÃ¨re intelligente !\n\nSources\n\n\n  nvie.com : A successful Git Branching Model\n  git push â€“help\n\n\n"
} ,
  
  {
    "title"    : "Retrouvez l&#39;intervention du CTO de M6 Web, Martin Boronski, Ã  la table ronde du Forum PHP 2012",
    "category" : "",
    "tags"     : " forumphp, afup, video, php",
    "url"      : "/retrouvez-l-intervention-du-cto-de-m6-web-martin-boronski-a-la-table-ronde-du-forum-php-2012",
    "date"     : "July 11, 2012",
    "excerpt"  : "\n\n",
  "content"  : "\n\n"
} ,
  
  {
    "title"    : "IntÃ©gration continue avec Jenkins et Atoum",
    "category" : "",
    "tags"     : " php, atoum, jenkins, ci",
    "url"      : "/integration-continue-avec-jenkins-et-atoum",
    "date"     : "July 11, 2012",
    "excerpt"  : "\n\nChez M6 Web nous tentons de crÃ©er une approche open-source intra entreprise. Lâ€™objectif est que certains composants gÃ©nÃ©riques adaptÃ©s notre mÃ©tier puissent Ãªtre crÃ©es et diffusÃ©s largement parmis les dizaines de projets gÃ©rÃ©s chaque annÃ©e. Un p...",
  "content"  : "\n\nChez M6 Web nous tentons de crÃ©er une approche open-source intra entreprise. Lâ€™objectif est que certains composants gÃ©nÃ©riques adaptÃ©s notre mÃ©tier puissent Ãªtre crÃ©es et diffusÃ©s largement parmis les dizaines de projets gÃ©rÃ©s chaque annÃ©e. Un prochain post traitera de cette problÃ©matique.\n\nDans cette optique, il faut nous assurer de la qualitÃ© et la non rÃ©grÃ©ssion de ces composants. Pour cela nous avons mis en place Jenkins afin dâ€™assurer lâ€™intÃ©gration continue de nos tests unitaires. Voici un exemple dâ€™intÃ©gration avec Atoum (ce nâ€™est pas forcement la meilleur mÃ©thode, nâ€™hÃ©sitez pas Ã  la commenter).\n\nStructure du composant :\n\n\n  ./src contient les classes du composants au format PSR-0\n  ./tests contient les TU Atoum\n  ./build-tools/jenkins contient les fichiers de configuration pour Atoum et Ant\n  ./vendor contient les dÃ©pendances externes du projet (gÃ©rÃ©es avec Composer)\n\n\nVoici le composer.json utilisÃ©.\n\n\n\nVoici le fichier de configuration de Atoum : build-tools/jenkins/atoum.ci.php et celui de jenkins build-tools/jenkins/build.xml\n\n\n\n(cette configuration inclut lâ€™ensemble des outils dâ€™analyse statique que lâ€™on utilise)\n\nEnfin, voici la configuration faire sur Jenkins (en image).\n\n\n\n\n\n\n\nVia cette conf on obtient le rÃ©sultat des tests (naturellement) ainsi que la couverture de code des tests avec la coloration des lignes couvertes ou non couvertes dans les classes testÃ©es.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 3 (DevOps/WebPerf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops, mobile",
    "url"      : "/cr-velocity-conference-2012-day-3-devops-webperf",
    "date"     : "June 28, 2012",
    "excerpt"  : "DerniÃ¨re journÃ©e de cette monstrueuse confÃ©rence quâ€™est la VÃ©locity ConfÃ©rence.\n\nOn commence dans la joie et la bonne humeur avec la â€œSeven Databases Songâ€ :D\n\n\n\n[Mobile WebPerf] The Performance of Web vs. Apps, par Ben Galbraith (Walmart.com) &amp;am...",
  "content"  : "DerniÃ¨re journÃ©e de cette monstrueuse confÃ©rence quâ€™est la VÃ©locity ConfÃ©rence.\n\nOn commence dans la joie et la bonne humeur avec la â€œSeven Databases Songâ€ :D\n\n\n\n[Mobile WebPerf] The Performance of Web vs. Apps, par Ben Galbraith (Walmart.com) &amp;amp; Dion Almaer (Walmart.com)\n\n\n\nPetit sujet assez trollesque sur les WebApps vs Apps. ConfÃ©rence hyper Ã©nÃ©rgÃ©tique et trÃ¨s drÃ´le ! Notamment le passage Ã  12mn dans la vidÃ©o, oÃ¹ lâ€™on compare le mode de distribution des apps natives Ã  ce que cela donnerait si les show tv devraient Ãªtre distribuÃ©s de la mÃªme maniÃ¨re en prenant lâ€™exemple de la sÃ©rie Friends : Hilarant !\n\nLâ€™idÃ©e intÃ©ressante sur la fin du talk, concerne le rendu de lâ€™application, qui grÃ¢ce Node.js (dispo dÃ©sormais en v0.8.0 enfin) peut Ãªtre aussi bien fait cotÃ© client que serveur suivant le client qui demande. A creuser.\n\n\n\n[WebPerf] Akamai Internet Insights, Stephen Ludin (Akamai)\n\n\n\nPetit talk de Stephen Ludin â€œChief Architect for Akamaiâ€™s Site Acceleration and Security groupâ€.\n\nAprÃ¨s une prÃ©sentation assez hallucinante en quelques chiffres du traffic et des donnÃ©es qui passent chez Akamai :\n\nToutes les 60 secondes =&amp;gt; 1 milliard 3 de logs, + de 6200 heures de vidÃ©os streamÃ©s â€¦\n\nIl a aussi partagÃ© une initiative louable et trÃ¨s intÃ©ressante sur un projet de partage des donnÃ©es rÃ©coltÃ©es chez Akamai : https://www.akamai.com/io\n\nOn y observe quelques statistiques (relativement peu date) sur les browsers notamment. On voit dâ€™ailleurs quelque chose dâ€™assez fun sur les IE8 : chaque weekend, on apercoit une baisse de prÃ©sence sur IE8 (qui se retrouve sur dâ€™autres navigateurs plus rÃ©cent) â€¦ Bref, on voit encore que câ€™est le monde de lâ€™entreprise qui ralenti la propagation des navigateurs rÃ©cents !\n\nSource : \nhttps://www.akamai.com/html/io/io_dataset.html#stat=browser_ver&amp;amp;top=5&amp;amp;type=line&amp;amp;start=20120601&amp;amp;end=20120626&amp;amp;net=n\n\nEt slides ici : \nhttps://assets.en.oreilly.com/1/event/79/Akamai%20Internet%20Insights%20%20Presentation.pptx\n\nLightning Demos, par Marcel Duran (Twitter Inc.), Nat Duca (Google), Lindsey Simon (Twist)\n\n\n\nEnsuite, viennent trois sessions de Lightning Talk : 5 minutes pour prÃ©senter un sujet.\n\nOn commence par Marcel Duran, crÃ©ateur de Yslow, cÃ©lÃ¨bre extension WebPerf de Firebug Ã  lâ€™origine, qui fait son petit bonhomme de chemin depuis :\n\n\n  Disponible dans quasiment tous les browsers\n  Ruleset personnalisable (cf C3PO voir plus bas)\n  Une version en ligne de commande (en Node.Js) pour extraire les donnÃ©es YSlow partir dâ€™un HAR : https://github.com/marcelduran/yslow/wiki/Command-Line-%28HAR%29\n  Un serveur Node.js que vous pouvez tester ici nÃ©cessitant aussi un HAR : https://yslow.nodester.com/\n  et le meilleur pour la fin, une version pour Phantom.Js (Projet trÃ¨s impressionnant dâ€™Headless Browser) qui vous permet de simplement mentionner lâ€™url et dâ€™avoir le rÃ©sultÃ¢t en sortie ! Avec en plus la possibilitÃ© via le format TAP (Test Any Protocol), dâ€™intÃ©grer les rÃ©sultats dans votre IntÃ©gration Continue pour Ã©viter les rÃ©gressions. Juste ultime, tout est expliquÃ© sur ce Github : https://github.com/marcelduran/yslow/wiki/PhantomJS\n\n\nJâ€™ai hÃ¢te dâ€™implÃ©menter tout ca chez M6Web :) Une vidÃ©o Ã  voir donc absolument :\n\n\n\n\n\nOn continue dans le lourd, avec Nat Duca qui travaille sur le dÃ©veloppement du navigateur Chrome et qui nous dÃ©montrer une feature trÃ¨s bas niveau mais au combien intÃ©ressante : le chrome://tracing/\n\nCette fonctionnalitÃ© va vous permettre de profiler les actions du navigateur au plus bas niveau possible. Encore un excellent nouvel ajout au niveau du panel dâ€™outillage du browser Chrome destination des dÃ©veloppeurs. Voir vidÃ©o ci dessous :\n\n\n\n\n\nEt pour finir cette jolie session de Lightning Talk, Lindsey Simon, nous prÃ©sentÃ© Browserscope : https://www.browserscope.org/\n\nOutil dont la puissance et lâ€™interÃªt pour tout dÃ©veloppeurs Front-end Desktop ou Mobile nâ€™est plus dÃ©montrer.\n\nSi vous ne connaissez pas, passez 5 minutes de votre temps sur cette vidÃ©o :\n\n\n\n[WebPerf] Browsers, par Luz Caballero (Opera Software), Tony Gentilcore (Google), Taras Glek (Mozilla Corporation)\n\nPetite dÃ©ception sur cette classique des VÃ©locity, oÃ¹ les talks ce sont plutÃ´t concentrÃ© sur les nouveautÃ©s des browsers mobile de Google et OpÃ©ra Mini, et oÃ¹ le gars de Mozilla nâ€™a pas jouer le jeu et prÃ©fÃ©rÃ© parler de la lenteur du SetTimeout Javascript ainsi que de lâ€™api LocalStorage â€¦\n\nSlide Mozilla : \nhttps://people.mozilla.com/~tglek/velocity2012/#/step-1\n\nSlide OpÃ©ra mini avec notes : \nhttps://www.slideshare.net/gerbille/speed-in-the-opera-mobile-browsers-13476236\n\nConcernant Google, la confÃ©rence par Tony Gentilcore (crÃ©ateur de FasterFox pour ceux qui ce souviennent) Ã©tait plus intÃ©ressante, dÃ©ja par lâ€™annonce suivante :\n\n\n  Chrome for Android will be the default browser starting with Jelly Bean\n\n\nTony Gentilcore\n\nIl a aussi parlÃ© du fonctionnement de WebKit, du Compositor Thread, ainsi que du Chrome Remote Debugging\n\nPour info, Google a peu de temps aprÃ¨s annoncÃ© la prÃ©sence de Chrome sur iOs !\n\n[DevOps] Simple log analysis and trending, par Mike Brittain (Etsy)\n\n\n\nOn retrouve Mike sur un sujet un peu diffÃ©rent : Comment analyser des logs Apache pour en sortir des graphites. Quelques astuces sur la fonction PHP apache_note() sont mentionnÃ©es, sur le traitement des logs avec les commandes linux â€œawkâ€ et â€œsedâ€, et lâ€™utilisation assez Ã©tonnante de Gnuplot pour grapher : https://www.gnuplot.info/ !\n\nLes slides sont dispos ici : https://www.mikebrittain.com/blog/2012/06/22/velocity-2012/ , et les codes dâ€™exemples sur Github : https://github.com/mikebrittain/presents\n\nEncore pas mal dâ€™idÃ©es piocher ! (Ca commence faire beaucoup dâ€™idÃ©es â€¦)\n\n\n\n[WebPerf] Social Button BFFs, par Stoyan Stefanov (Facebook)\n\nStoyan nâ€™est plus prÃ©senter dans lâ€™industrie des performances web. Il est dÃ©sormais chez Facebook, travailler sur les performances des plugins, dont le â€œLikeâ€ ! Suivez le sur Twitter, câ€™est bourrÃ© de superbes infos @stoyanstefanov ainsi que son blog : https://www.phpied.com/ !\n\nLâ€™idÃ©e du talk est de faire en sorte que les boutons sociaux (et widgets tiers) en gÃ©nÃ©ral, deviennent vos BFF ! (Best Friend Forever :D) : https://www.phpied.com/social-button-bffs/\n\nIl explique de quel maniÃ¨re doit-on intÃ©grer ces widgets sur nos sites, et vous permet de le vÃ©rifier par lâ€™extension Chrome quâ€™il a dÃ©veloppÃ© 3PO#Fail (3PO = 3rd Party Optimization) ou via une extension de RuleSet pour YSlow.\n\nLes slides ici : https://www.slideshare.net/stoyan/social-button-bffs\n\n\n  â€œFriends donâ€™t let friends do document.writeâ€ Stoyan Stefanov\n\n\n[WebPerf] 5 Essential Tools for UI Performance, par Nicole Sullivan (Stubbornella)\n\nEncore un excellent talk pour ce dernier jour avec Nicolas Sullivan, Experte et consultante dans lâ€™optimisation CSS, sur le fonctionnement trÃ¨s prÃ©cis de la gestion des CSS par vos navigateurs et toutes les optimisations rÃ©centes quâ€™ils y ont apportÃ©es, ainsi quâ€™une dÃ©mo (qui fait toujours son petit effet dans une salle Geek) de Tilt sur Firefox\n\nLes slides ne sont malheuresement pas encore en ligne, mais cela ne devrait tarder sur son Slideshare.\n\nVous pouvez retrouvez lâ€™idÃ©e du talk sur lâ€™interview ci dessous rÃ©alisÃ©e elle aussi lors de la VÃ©locity.\n\n\n\nConclusion\n\nVoil, câ€™est terminÃ© pour ce compte rendu en 3 actes de ce que jâ€™ai vÃ©cu et retenu cette VÃ©locity ConfÃ©rence 2012. FranÃ§ois prendra le relais pour prÃ©senter sa vision dâ€™autres talks, mais orientÃ©s Ops (Sysadmin).\n\nJâ€™espÃ¨re que ces comptes rendu auront servi Ã  partager quelques outils, liens ou best practices qui vous donnerons des tonnes dâ€™idÃ©es de nouvelles choses Ã  faire cotÃ© Web dans votre sociÃ©tÃ©. De mon cotÃ©, comme la VÃ©locity Berlin lâ€™annÃ©e derniÃ¨re, jâ€™ai appris beaucoup et apprÃ©ciÃ© une grande partie des confÃ©rences. Cette conf reste pour moi (et nous chez M6Web) la plus importante au monde sur les aspects de Performance.\n\nPour finir, je vous remercie pour vos retours (et je vous invite Ã  continuer mâ€™en faire un maximum) et lectures. Vous pouvez en connaitre dâ€™avantage sur les autres talks avec quelques vidÃ©os gratuite disponible sur \nhttps://www.youtube.com/playlist?list=PL80D9129677893FD8, Ainsi que les slides qui continuent dâ€™arriver sur\nhttps://velocityconf.com/velocity2012/public/schedule/proceedings\n\nEt pour ceux que ca intÃ©resse, sachez que Oreilly mettra disposition un pack complet des vidÃ©os pour gÃ©nÃ©ralement un tarif autour des 400$, et quâ€™une VÃ©locity Europe aura lieu Londres les 3 et 4 octobre 2012.\n\nMerci tous !\n\n\n  CR VÃ©locity Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n  CR VÃ©locity Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-2-devops-webperf\n\n\n(CrÃ©dit photo : https://www.flickr.com/photos/oreillyconf/sets/72157630300659948/)\n\n\nLe Job Board assez hallucinant !\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 2 (DevOps/WebPerf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops, mobile",
    "url"      : "/cr-velocity-conference-2012-day-2-devops-webperf",
    "date"     : "June 28, 2012",
    "excerpt"  : "Compte rendu des tracks DevOps/WebPerf de cette deuxiÃ¨me journÃ©e de cette VÃ©locity ConfÃ©rence Ã  Santa Clara (Californie) qui marque lâ€™ouverture â€œofficielleâ€ de la confÃ©rence, la veille Ã©tant considÃ©rÃ©e comme des confÃ©rences bonus orientÃ©es Tutoria...",
  "content"  : "Compte rendu des tracks DevOps/WebPerf de cette deuxiÃ¨me journÃ©e de cette VÃ©locity ConfÃ©rence Ã  Santa Clara (Californie) qui marque lâ€™ouverture â€œofficielleâ€ de la confÃ©rence, la veille Ã©tant considÃ©rÃ©e comme des confÃ©rences bonus orientÃ©es Tutoriaux.\n\nLa matinÃ©e offrait un track unique dans une salle gigantesque.\n\n\n\nVidÃ©o dâ€™intro Ã  la VÃ©locity !\n\n\n\nLâ€™ouverture officielle est donc prÃ©sentÃ©e par Steve Souders (Google) et John Allspaw (Etsy), toujours dans un show lâ€™amÃ©ricaine, et mÃªme dÃ©guisÃ©s. \n\n\nSâ€™enchaine ensuite un condensÃ© de session plutÃ´t courte par des acteurs trÃ¨s prestigieux du web.\n\n(CrÃ©dit photo : https://instagr.am/p/MV5xAAJLSt/ )\n\n[DevOps] Building for a billion Users, par Jay Parikh (Facebook)\n\nPremiÃ¨re confÃ©rence du matin, avec une prÃ©sentation du â€œVP of Infrastructure Engineering at Facebookâ€.\n\nOn suis avec attention, une prÃ©sentation trÃ¨s dense de lâ€™infrastructure de Facebook, avec quelques chiffres hors normes.\n\nLa philosophie Facebook est prÃ©sentÃ©e en 4 points :\n\n\n  Focus on Impact\n  Move Fast\n  Be Bold\n  Be Open\n\n\nAvec une explication sur les fameux Bootcamp cher Facebook, formation obligatoire auquelle tout le monde participe en rentrant chez Facebook.\n\nUne prÃ©sentation trÃ¨s brÃ¨ve des outils internes utilisÃ©s et dÃ©veloppÃ©s par Facebook : Perflab, GateKeeper (sorte de Feature Flipping), Claspin, Tasks, SevManager â€¦\n\nUne explication sur les procÃ©dures de dÃ©ploiement chez Facebook et leur gestion du cache, sur leur volontÃ© de constamment tout refaire, pour toujours Ãªtre meilleur.\n\n\nEt pour finir, une anecdote assez drÃ´le sur un incident ayant eu lieu chez Facebook, oÃ¹ toutes les fonctionnalitÃ©s â€œnon terminÃ©esâ€ se sont un jour retrouvÃ©es en production.\n\nBref, une confÃ©rence intÃ©ressante, mais trÃ¨s dense, dont je vous conseille de regarder la vidÃ©o Ã§i dessous.\n\n\n  â€œFix More, Whine less.â€ Jay Parikh\n\n\n\n\n[DevOps] Investigating Anomalies, par John Rauser (Amazon)\n\nBelle surprise de la journÃ©e, avec cette confÃ©rence sur la gestion dâ€™incident, qui raconte lâ€™histoire de lâ€™Ã©pidÃ©mie de Cholera ayant eu lieu Ã  Londres en 1854, et comment John Snow, trouver lâ€™origine de lâ€™Ã©pidÃ©mie, en se concentrant sur les donnÃ©es, et non pas seulement sur les chiffres.\n\n\n  â€œExplaining anomalies often makes your theroy bulletproofâ€ John Rauser\n\n\nUne deuxiÃ¨me partie Ã©tait concentrÃ©e sur le fait dâ€™Ã©tudier les extremitÃ©s sur vos Ã©chantillons de maniÃ¨re Ã  trouver ce qui nâ€™allait pas. Point de vue trÃ¨s instructif.\n\nLa vidÃ©o ci dessus est un â€œmust-seeâ€ de la VÃ©locity.\n\n\n  â€œLook at the extremes and youâ€™ll find things that are brokenâ€ John Rauser\n\n\n\n\n[DevOps] Building Resilient User Experiences, par Mike Brittain (Etsy)\n\nLe message autour de cette prÃ©sentation, est que votre application DOIT sâ€™adapter aux incidents. Si possible faire en sorte que cela ne soit mÃªme pas percu par la plupart de vos internautes. En dÃ©coupant chacune des fonctionnalitÃ©s de votre site, vous devez pouvoir ne pas afficher celle qui ne fonctionne pas correctement sans que cela impact vos utilisateurs (Graceful Degradation).\n\nLes slides sont disponible ici : https://www.slideshare.net/mikebrittain/building-resilient-user-experiences-13461063\n\n\n\n[WebPerf] Predicting User Activity to Make the Web Fast, par Arvind Jain (Google), Dominic Hamon (Google)\n\nLa prÃ©sentation commence avec un rappel sur â€œHow Fast is the web todayâ€. \n En quelques chiffres :\n\n\n  Chrome ~2.3s/5.4s page load time (median/mean)\n  Google Analytics ~2.9s/6.9s page load time (median/mean)\n  Mobile ~4.3s/10.2s page load time (median/mean)\n\n\nDâ€™autres infos sont partagÃ©es venant du trÃ¨s utile HttpArchive â€¦\n\nOn assiste ensuite la prÃ©sentation des fonctionnalitÃ©s de â€œPrefetchâ€ de google et du Prerendering en place dans la barre de recherche de Chrome : â€œOmniboxâ€, ceci ayant pour but de rendre le web encore plus rapide.\n\nTout cela donne des idÃ©Ã©es sur la faÃ§on de prÃ©dire ce que vont faire les internautes, et sur nos gestions dâ€™â€œautocompleteâ€.\n\n\n\n\n\n[WebPerf] Performance Implications of Responsive Web Design, par Jason Grigsby (Cloud Four)\n\nUne autre confÃ©rence que jâ€™attendais grandement, sur le Responsive Web Design. Le sujet est parfaitement maitrisÃ©, et tous les reproches que je peux faire cette techno en ce moment, sont mentionnÃ©s, expliquÃ©s, et certaines solutions ou idÃ©es sont donnÃ©es ! Du tout bon.\n\nA retenir, la mÃ©thode conseillÃ©e qui est de faire du Mobile First Responsive Web design, câ€™est dire commencer par la version mobile, puis faire la version web, et non lâ€™inverse.\n\nLes slides ici : https://speakerdeck.com/u/grigs/p/performance-implications-of-responsive-design\n\nLa confÃ©rence nâ€™Ã©tant pas disponible en vidÃ©o, vous pouvez dÃ©jÃ©couter Jason Grisby lors dâ€™une interview suite sa confÃ©rence en vidÃ©o si dessous.\n\n(CrÃ©dit photo : https://www.flickr.com/photos/stuart-dootson/4024407198/ )\n\n\nJason Grigsby interview la VÃ©locity Conf 2012\n\n\n\n[WebPerf] RUM for Breakfast - Distilling Insights From the Noise, par Buddy Brewer (LogNormal), Philip Tellis (LogNormal, Inc) &amp;amp; Carlos Bueno (Facebook)\n\nRUM aka Real User Monitoring est un terme qui est revenu trÃ¨s rÃ©guliÃ¨rement durant cette Velocity. Nous avions pour cette confÃ©rence notamment, deux personnes de LogNormal dont le crÃ©ateur de Boomerang.js : https://yahoo.github.com/boomerang/doc/ et Carlos Bueno de Facebook.\n\nLa prÃ©sentation expliquait comment mesurer des mÃ©triques de performances venant dâ€™utilisateurs rÃ©el, comment analyser toutes les donnÃ©es, en filtrer le â€œbruitâ€, et quoi en tirer. Le tout Ã©tait trÃ¨s instructif, surtout sur la partie filtrage de donnÃ©es (Band Pass Filtering, IQR Filtering ..).\n\nSlides : https://www.slideshare.net/buddybrewer/rum-for-breakfast-distilling-insights-from-the-noise\n\n\n\n[WebPerf] Rendering Slow? Too Much CSS3? Ask RSlow, par Marcel Duran (Twitter Inc.), David Calhoun (CBS Interactive)\n\nOn retrouve ici une prÃ©sentation assez fun du rÃ©sumÃ© de la conf sous forme de Waterfall (voir photo).\n\nLe talk a ensuite abordÃ© les notions de rendering au niveau CSS avec au dÃ©part un cas dâ€™Ã©tude : RÃ©aliser le logo de ySlow en CSS3 entiÃ¨rement. On observe de maniÃ¨re assez drÃ´le le rendu finale dans les diffÃ©rents navigateurs (Ã©clat de rire gÃ©nÃ©ral sur IE off course). Vous pouvez les retrouver sur les slides ci dessous.\n\nLa confÃ©rence part ensuite sur quelques tests rÃ©alisÃ©s sur Chrome uniquement (Ã  prendre donc avec des pincettes) sur les performances CSS3 de chacun de ces cas :\n\n\n  background-image vs css3 gradient\n   vs css background-image\n  @font-face vs  vs sans-serif\n  animated gif vs css3 spinner\n\n\nLâ€™Ã©tude est intÃ©ressante, et mÃ©riterais dâ€™Ãªtre creusÃ©e sur dâ€™autres navigateurs, mais cela est rendu trÃ¨s difficile par le fait que seul Chrome sait exporter les donnÃ©es de rendu de sa Timeline â€¦\n\nLes slides sont dispo ici : \nhttps://docs.google.com/presentation/d/1b7rdeXYdmL3lmT24GAaC14eOSkq5qt6FM-yLSeFykQk/edit?pli=1\n\n[WebPerf] Time To First Tweet, par Dan Webb, par (Twitter Inc) &amp;amp; Rob Sayre (Twitter)\n\nDan et Rob nous parle performances cotÃ© client chez Twitter, et la rÃ©ecriture du Front-End.\n\nLa notion de Time To First Tweet, correspond au temps de navigation jusquâ€™a lâ€™affichage du premier twiit sur la Timeline. Cette mesure est prise grace Ã  la Navigation Timing Api, supportÃ©e dans IE&amp;gt;=9, Firefox &amp;amp; Chrome notamment.\n\nTwitter Ã  aussi abandonnÃ© progressivement, lâ€™utilisation des hashbangs (les #! dans lâ€™url), pour utiliser la PushState Api, ainsi que le templating cotÃ© client (Mustache.js et Hogan.js) pour repasser sur un templating serveur avec leur migration de Ruby vers Java, avec au final 75% de temps client gagnÃ© sur le 95th Percentile !\n\nConfÃ©rence trÃ¨s intÃ©ressante, notamment sur la maniÃ¨re de charger les Javascripts.\n\nPlus de dÃ©tail sur le blog technique de Twitter : https://engineering.twitter.com/2012/05/improving-performance-on-twittercom.html\n\nLes slides sont disponible : https://speakerdeck.com/u/danwrong/p/time-to-first-tweet\n\nConclusion Day 2 :\n\nEncore une journÃ©e riche en informations et idÃ©es. Le rythme Ã©tant beaucoup plus soutenu, et la fatigue sâ€™accumulant, il nâ€™Ã©tait pas Ã©vident dâ€™Ãªtre Ã  100% dans chaque talk :-)\n\nEn attendant le CR Ops, et celui du Day 3, vous pouvez relire le CR du Day 1 : \nhttps://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n\nP.S : Retrouvez moi sur twitter : @kenny_dee\n\nPlaylist Youtube Velocity US 2012\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 1 (Dev/Webperf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, mobile",
    "url"      : "/cr-velocity-conference-day-1-dev-webperf",
    "date"     : "June 27, 2012",
    "excerpt"  : "Nous voici Ã  Santa Clara, CA, ce lundi 25 juin pour notre premiÃ¨re Velocity Conference (Web Performance &amp;amp; Operations) aux states (nous avions dÃ©jÃ  pu avoir un aperÃ§u lâ€™annÃ©e derniÃ¨re avec la premiÃ¨re Velocity Europe Berlin).\n\n\n\nLâ€™Ã©venement se ...",
  "content"  : "Nous voici Ã  Santa Clara, CA, ce lundi 25 juin pour notre premiÃ¨re Velocity Conference (Web Performance &amp;amp; Operations) aux states (nous avions dÃ©jÃ  pu avoir un aperÃ§u lâ€™annÃ©e derniÃ¨re avec la premiÃ¨re Velocity Europe Berlin).\n\n\n\nLâ€™Ã©venement se situe au Convention Center, et la premiÃ¨re chose que nous remarquons, câ€™est la taille dÃ©mesurÃ©e du lieu ! Et pour cause, 800 personnes sont attendues !\n\nPour cette premiÃ¨re journÃ©e sous le signe des tutoriaux, il y avait entre trois et quatre tracks parallÃ¨les de 90 minutes chacun, dont un rÃ©servÃ© aux sponsors. Dur de faire des choix parmi toutes les confs et le programme allÃ©chant de la journÃ©e !\n\nJe mâ€™oriente donc sur le cotÃ© DÃ©v / WebPerf / Monitoring, pendant que mon collÃ¨gue, Francois, part sur le cotÃ© operations quâ€™il couvrira dans une autre sÃ©rie de CR.\n\n[WebPerf] Understanding and Optimizing Web Performance Metrics, par Bryan McQuade de chez Google\n\nAu menu :\n\n\n  Explication des mÃ©triques de performance orientÃ©es rÃ©seau\n  Fonctionnement du parser HTML5\n  Explication des mÃ©triques de performance orientÃ©es rendu\n  DÃ©mo de Critical Path Explorer (PageSpeed Online)\n  Optimisation de lâ€™affichage perÃ§u utilisateur\n\n\nLes slides parlent dâ€™elles mÃªme et sont disponibles ici : \nhttps://perf-metrics-velocity2012.appspot.com .\n\nElles parcourent lâ€™intÃ©gralitÃ© des notions de WebPerf existantes â€œÃ  jourâ€, dont certaines peu connues comme la Speculative loading, et permettent surtout de comprendre ce quâ€™elles signifient trÃ¨s prÃ©cisÃ©ment.\n\nA retenir aussi le SSL Server Test ici https://www.ssllabs.com/ssltest/\n\nCâ€™est donc un Must Read pour tout ceux que la WebPerf intÃ©resse.\n\n**Page Speed Insights : **\n\nNous avons eu droit ensuite une dÃ©mo trÃ¨s intÃ©ressante de la fonctionnalitÃ© Critical Path Explorer (dÃ©jÃ  entraperÃ§ue en version bÃ©ta Ã  la Velocity Europe), et qui sera je pense lancÃ©e officiellement demain.\n\nEn attendant et pour la tester : \nhttps://developers.google.com/speed/pagespeed/insights?velocity=1\n\nCette fonctionnalitÃ© permet, comme son nom lâ€™indique, de montrer le chemin critique de votre page. Sur les quelques tests que jâ€™ai pu effectuÃ©s, câ€™est trÃ¨s efficace. On apprÃ©cie le dÃ©tail au niveau du waterfall sur lâ€™exÃ©cution des javascripts, lâ€™affichage de â€œqui bloque quoiâ€, ou le render css. A approfondir de toute urgence !\n\n\n\nBryan McQuade (Google)\n\n\nLa Lightning DÃ©mo de Page Speed ayant eu lieu le lendemain\n\n[WebPerf] A Web Perf Dashboard: Up &amp;amp; Running in 90 Minutes, par Cliff Crocker et Aaron Kulick.\n\nLâ€™idÃ©e ici Ã©tait de montrer en 90 minutes avec quels outils obtenir un dashboard orientÃ© WebPerf, qui sera fourni comme une VM Ã  la fin de la session.\n\nAprÃ¨s une longue prÃ©sentation orale dâ€™outils plutÃ´t connus dÃ©sormais comme :\n\n\n  Boomerang.Js\n  WebPageTest instance privÃ©e + API\n  Piwik (clone de Google Analytics)\n  StatsD (collecteur pour Graphite)\n  Graphite\n  REDbot.org\n  cUrl\n  ShowSlow\n  â€¦\n\n\nLes deux confÃ©renciers nous prÃ©sentent un site rÃ©alisÃ© pour lâ€™occasion : â€œSally Squirrelâ€™s Dance Emporiumâ€, hommage aux gifs animÃ©s dâ€™Ã©cureuils dansant, et nous font une dÃ©mo (un peu capricieuse) de leur dashboard basÃ© sur Piwik, alternative Google Analytics avec un systÃ¨me de â€œpluginâ€ visiblement pour aggrÃ©ger un peu le tout.\n\nLâ€™idÃ©e est clairement bonne, le rÃ©sultat ne mâ€™a pas convaincu titre personnel. On vante au dÃ©part de la prÃ©sentation, le faite quâ€™une image bien choisie suffit au monitoring, et quâ€™un dashboard ne doit pas Ãªtre complexe, et au final, on se retrouve avec un dashboard remplis dâ€™images en tout genre, de donnÃ©es tabulaires, â€¦ complexe quoi â€¦\n\nPiwik en alternative Analytics ?\n\nJe doute aussi de la robustesse de Piwik que nous avions dÃ©jÃ  Ã©tudier, et voir des pages listant les temps de latence ou de chargement utilisateur par utilisateur, me fait rÃ©ellement peur avec une audience dÃ©passant la centaine de personnes la journÃ©e â€¦\n\nPour lâ€™anecdote, sur le wiki de Piwik, on lit cette phrase que je vous laisse apprÃ©cier : â€œIf your website has more than a few hundreds visits per day (bravo!), waiting for Piwik to process your data may take a few minutesâ€\n\nJe vous invite tout de mÃªme Ã  lire les slides : \n[https://assets.en.oreilly.com/1/event/79/A%20Web%20Perf%20Dashboard_%20%20Up%20%20Running%20in%2090%20Minutes%20Presentation.pptx](https://assets.en.oreilly.com/1/event/79/A%20Web%20Perf%20Dashboard%20%20Up%20_%20Running%20in%2090%20Minutes%20Presentation.pptx)\n\nAinsi quâ€™Ã  tester la VM mise Ã  disposition, car le travail derriÃ¨re est consÃ©quent, et peut correspondre Ã  certains, ou peut au moins donner des idÃ©es pour dâ€™autres : https://t.co/uLv1fX1A\n\n**A retenir : **\n\nA retenir aussi dans cette prÃ©sentation, tout le bien qui a Ã©tÃ© dit de Graphite (mÃªme si je ne suis plus convaincre la dessus), et de quelques features pour lesquelles jâ€™Ã©tais passÃ©e travers :\n\n\n  Support du SVG ( &amp;amp;format=svg) qui va enfin nous permettre de tester lâ€™enrichissement des graphs par du contenu â€œconnexeâ€ (liste des erreurs 404 sur le graph lorsque lâ€™on clic sur un point, nom du dÃ©veloppeur ayant fait la mise en production etc)\n  Les fonctions de HoltWinter afin dâ€™avoir des tendances hautes et basses pour mieux savoir quand alerter par exemple.\n\n\n[WebPerf] How to Walk Away From Your Outage Looking Like a HERO par Teresa Dietrich (WebMD), Derek Chang (WebMD)\n\nLâ€™une des confÃ©rences que jâ€™attendais beaucoup : le titre annonÃ§ait un talk sur la gestion dâ€™incident avec un cotÃ© humoristique.\n\nLes deux confÃ©renciers ont prÃ©sentÃ©s des templates trÃ¨s complet de gestion dâ€™incident quâ€™ils rÃ©alisent pour des posts-mortems quâ€™on peut retrouver ici : https://www.teresadietrich.net/?page_id=37\n\nPersonnellement, il me parait trÃ¨s important de rÃ©aliser des posts mortems. Mais si câ€™est pour passer plus de temps Ã  rÃ©diger des rapports dâ€™incidents trop complet quâ€™on ne relira jamais quâ€™a en tirer un quelconque bÃ©nÃ©fice, cela me parait inutile.\n\nDu coup, la premiÃ¨re demi-heure a consistÃ© Ã  prÃ©senter ces templates, lire et expliquer quelques incidents ayant eu lieu chez WebMD.\n\nJâ€™ai, comme une bonne partie de la salle, fait lâ€™impasse rapidement : entre le sujet dans lequel je ne suis jamais rentrÃ© ainsi que des slides avec beaucoup de texte illisible passÃ©s les premiers rangs de la trÃ¨s grande salle, je nâ€™ai pas accrochÃ©.\n\nA revoir tÃªte reposÃ©e : https://velocityconf.com/velocity2012/public/schedule/detail/23615 (slides non dispo Ã  lâ€™heure actuelle)\n\n\n\nTeresa Dietrich (WebMD), Derek Chang (WebMD)\n\n[WebPerf] The 90-minutes Mobile optimization life cycle par Hooman Beheshti (VP strangeloop)\n\nConfÃ©rence orientÃ©e WebPerf Mobile.\n\nIci, on retrouve tout ce que jâ€™aime dans les confÃ©rences VÃ©locity :\n\n\n  Un confÃ©rencier avec un grand talent dâ€™orateur : prÃ©cis, drÃ´le et captivant\n  Un sujet trÃ¨s maitrisÃ©\n  Des slides propres et parlantes mÃªme sans avoir assistÃ© au talk\n  Des dÃ©bats lancÃ©s â€¦\n\n\nLâ€™idÃ©e Ã©tait de partir dâ€™un site, au hasard Oreilly.com, puis le site mobile velocityconf.com par la suite, et de dÃ©montrer les Ã©tapes dâ€™optimisation WebPerf, Ã©tape par Ã©tape, avec Ã  chaque fois, ce que lâ€™on souhaite obtenir, ce que lâ€™on obtient rÃ©ellement, et une comparaison vidÃ©o du changement.\n\nDes points ont Ã©tÃ© approfondis comme la gestion du cache, via LocalStorage, du fonctionnement des CDN (pour le mobile), du Pipellining HTTP, de la congestion TCP etc â€¦\n\nBeaucoup dâ€™outils ont aussi Ã©tÃ© mentionnÃ©s pour la WebPerf mobile :\n\n\n  Chrome remote debugging : Http://developers.google.com/chrome/mobile/docs/debugging/\n  iWebInspector for IOS simulator : www.iwebinspector.com\n  Weinre : Remote debugging from the desktop for what the phone is doing : https://people.apache.org/~pmuellr/weinre/\n  Aardwolf : Remote js debugging lexandera.com/aardwolf\n  Mobile Perf Bookmarklet : stevesouders.com/mobileperf/mobileperfbkm.php\n  Pcap2har : Turn packet captures to waterfalls https://pcapperf.appspot.com\n  â€¦\n\n\nBref, un trÃ¨s bon panorama pour la WebPerf mobile avec deux cas concret dâ€™Ã©tude, chacun avec deux approches diffÃ©rentes :\n\n\n  AmÃ©liorer les mÃ©triques de WebPerf pour le site dâ€™Oreilly\n  AmÃ©liorer la perception utilisateur sans regarder les mÃ©triques pour le site mobile de la VÃ©locityconf\n\n\nSlides dispo ici : https://www.strangeloopnetworks.com/blog/the-90-minute-mobile-optimization-life-cycle/\n\nJe vous invite aussi Ã  regarder son interview ci dessous.\n\n\n\n\n\n[Event] Akamai Pool Party\n\nCette journÃ©e touche Ã  sa fin avec une Pool Party extÃ©rieure par Akamai avec un orchestre (qui nous jouÃ© notamment le thÃ¨me de Mario ! TrÃ¨s fun), beaucoup Ã  boire, et beaucoup Ã  manger (lÃ©gumes tremper dans du brie chaud, WTF ?). Lâ€™occasion de rencontrer quelques sponsors et 2 autres franÃ§ais. :-)\n\n\nDevOps drinking session / @jstinson\n\n[Event] Ignite Sessions\n\nA 19h30 avait lieu les Ignite sessions, des confs â€œlightning talksâ€ de 5 minutes sur des sujets divers, certains trÃ¨s intÃ©ressant comme :\n\n\n  le â€œPerceptual Diffâ€ par un ingÃ©nieur de chez Google, pour Ãªtre alertÃ© (par lâ€™intÃ©gration continue) lorsque la page du Service customers de chez Google change. Photos de la prÃ©sentation ici + https://pdiff.sourceforge.net/\n  les #lolops, avec une sÃ©rie de Twitt orientÃ©e Devops Ã  mourir de rire : Voir ici https://www.slideshare.net/cwestin63/lolops-a-years-worth-of-humorous-engineering-tweets\n  â€¦\n\n\nLe concept est vraiment efficace avec des speakers ultra dynamique et pour la majoritÃ© trÃ¨s drÃ´le.\n\nMention spÃ©ciale pour la partie centrale, oÃ¹ 11 personnes de la salle (dont certains speakers, Allspaw et Souders en tÃªte), avait 1 Ã  2 minutes pour improviser sur des slides plutÃ´t trÃ¨s drÃ´le quâ€™ils nâ€™avaient jamais vu.\n\nJâ€™espÃ¨re que les vidÃ©os seront disponible car câ€™Ã©tait juste hilarant au possible. Je ne mâ€™attendais pas Ã  pleurer de rire non stop ici :-)\n\n\n\nConclusion\n\nExcellente premiÃ¨re journÃ©e, dÃ©jÃ  des tonnes dâ€™infos Ã  condenser / retenir, et ce nâ€™Ã©tait que le premier jour !\n\nSinon lâ€™organisation est impeccable, lieu exceptionnel, wifi public qui fonctionne, repas de trÃ¨s bonne qualitÃ© (et table qui plus est), pas mal de multiprises dans les salles, Ã  boire Ã  volontÃ© â€¦ La grande classe !\n\nLes comptes rendus des prochaines journÃ©es et des sessions orientÃ©s Ops suivre ;-)\n\nNâ€™hÃ©sitez pas Ã  faire un maximum de retour sur ce compte rendu, cela nous aidera et nous motivera pour les prochains ;-)\n\nP.S: Retrouvez moi sur Twitter : @kenny_dee\n\n"
} ,
  
  {
    "title"    : "Monitoring applicatif : Pourquoi et comment ?",
    "category" : "",
    "tags"     : " monitoring, graphite, statsd, conference",
    "url"      : "/monitoring-applicatif-pourquoi-et-comment",
    "date"     : "June 26, 2012",
    "excerpt"  : "Voici les slides de la prÃ©sentation que jâ€™ai donnÃ©e au Forum PHP 2012, et au WebEvent 4 :\n\nVous Ãªtes dÃ©veloppeur, chef de projet technique ou mÃªme responsable et vous souhaitez avoir de la visibilitÃ© sur le fonctionnement de vos applicatifs, ou su...",
  "content"  : "Voici les slides de la prÃ©sentation que jâ€™ai donnÃ©e au Forum PHP 2012, et au WebEvent 4 :\n\nVous Ãªtes dÃ©veloppeur, chef de projet technique ou mÃªme responsable et vous souhaitez avoir de la visibilitÃ© sur le fonctionnement de vos applicatifs, ou sur la plateforme sur laquelle ils sont hÃ©bÃ©rgÃ©s ?\n\nNous Ã©tudierons comment, grÃ¢ce Ã  des outils simples (StatD / Graphite / Log BDD) et nos expÃ©riences chez M6Web, mettre en place un monitoring applicatif ultra complet.\n\nCe monitoring vous permettra de retrouver la vue sur vos projets, pour mieux anticiper la charge, detecter la root cause en cas dâ€™incident et connaitre lâ€™Ã©tat de chacun de vos services ..\n\nMonitoring applicatif : Pourquoi et comment ?\n\n \n"
} ,
  
  {
    "title"    : "M6Web au Web Event Lyon #4",
    "category" : "",
    "tags"     : " webevent",
    "url"      : "/m6web-au-web-event-lyon-4",
    "date"     : "June 20, 2012",
    "excerpt"  : "\nUne partie de lâ€™Ã©quipe de M6 Web au webevent de La ferme du Web.\n\n",
  "content"  : "\nUne partie de lâ€™Ã©quipe de M6 Web au webevent de La ferme du Web.\n\n"
} ,
  
  {
    "title"    : "M6 Web Ã©tait prÃ©sent au Forum PHP 2012",
    "category" : "",
    "tags"     : " afup, forumphp, conference",
    "url"      : "/post/24732185644/m6-web-tait-pr-sent-au-forum-php-2012",
    "date"     : "June 9, 2012",
    "excerpt"  : "Voici quelques photos des membres dâ€™M6Web prises lors du Forum PHP 2012.\n\nKenny a animÃ© une confÃ©rence sur le monitoring applicatif.\n\n\n\nMartin, notre CTO, a participÃ© une table ronde.\n\n\n\nOlivier Ã©tait Ã©galement prÃ©sent en tant que membre de lâ€™Afup...",
  "content"  : "Voici quelques photos des membres dâ€™M6Web prises lors du Forum PHP 2012.\n\nKenny a animÃ© une confÃ©rence sur le monitoring applicatif.\n\n\n\nMartin, notre CTO, a participÃ© une table ronde.\n\n\n\nOlivier Ã©tait Ã©galement prÃ©sent en tant que membre de lâ€™Afup\n\n\n\nAinsi que Didier et Julien, dÃ©veloppeurs dans nos Ã©quipes R&amp;amp;D, et trop prÃ©ssÃ©s dâ€™assiter toutes les confÃ©rences pour Ãªtre photographiÃ©s.\n\n"
} ,
  
  {
    "title"    : "M6Web au Forum PHP 2012 et au WebEvent #4",
    "category" : "",
    "tags"     : " php, afup, monitoring, conference",
    "url"      : "/post/24184111542/m6web-au-forum-php-2012-et-au-webevent-4",
    "date"     : "June 1, 2012",
    "excerpt"  : "Cette annÃ©e, M6Web sponsorise deux Ã©vÃ©nements franÃ§ais majeurs dans le monde du Web :\n\n\n  le Web Event Ã  Lyon #4 (https://event.lafermeduweb.net/ au centre de congrÃ¨s de Lyon le 15 juin 2012)\n  et le forumPHP 2012 (https://afup.org/pages/forumphp2...",
  "content"  : "Cette annÃ©e, M6Web sponsorise deux Ã©vÃ©nements franÃ§ais majeurs dans le monde du Web :\n\n\n  le Web Event Ã  Lyon #4 (https://event.lafermeduweb.net/ au centre de congrÃ¨s de Lyon le 15 juin 2012)\n  et le forumPHP 2012 (https://afup.org/pages/forumphp2012/ la CitÃ© universitaire Ã  Paris le 5 &amp;amp; 6 juin 2012) !\n\n\nA cette occasion, M6Web sera bien reprÃ©sentÃ© :\n\n\n  je (Kenny Dits) prÃ©senterais chaque Ã©vÃ¨nement des sessions sur le monitoring applicatif en regard de ce que nous faisons au quotidien chez M6Web. https://afup.org/pages/forumphp2012/sessions.php#632\nhttps://event.lafermeduweb.net/les-sessions#c6\n  Olivier Mansour en tant que Vice PrÃ©sident de lâ€™Afup parlera lors de la Keynote de cloture du Forum Php le 6 juin. https://afup.org/pages/forumphp2012/sessions.php#732\n  Martin Boronski, notre directeur technique participera Ã©galement Ã  la table ronde DSI organisÃ©e par lâ€™AFUP Paris le 6 Juin.\n\n\nRendez-vous lÃ  bas ? ;-)\n\nBanniÃ¨re du Forum PHP 2012\n\n[\n\nBanniÃ¨re du Web Event Lyon\n\n[\n\n"
} ,
  
  {
    "title"    : "Performances PHP chez M6Web",
    "category" : "",
    "tags"     : " graphite, monitoring, nodejs, php, varnish, webperf, conference",
    "url"      : "/post/23671071384/performances-php-chez-m6web",
    "date"     : "May 24, 2012",
    "excerpt"  : "Voici les slides de la prÃ©sentation du 23 mai rÃ©alisÃ©e Ã  lâ€™Epitech de Lyon.\n\nCâ€™est un retour dâ€™expÃ©rience, qui survole un peu tous les axes sur lesquels nous travaillons chez m6web, ayant trait aux optimisations de nos sites.\n\nJâ€™espÃ¨re que certain...",
  "content"  : "Voici les slides de la prÃ©sentation du 23 mai rÃ©alisÃ©e Ã  lâ€™Epitech de Lyon.\n\nCâ€™est un retour dâ€™expÃ©rience, qui survole un peu tous les axes sur lesquels nous travaillons chez m6web, ayant trait aux optimisations de nos sites.\n\nJâ€™espÃ¨re que certains points feront lâ€™objet dâ€™autres articles dans le futur ;-)\n\nPerformances php chez M6Web\n&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;\n\n"
} ,
  
  {
    "title"    : "Lancement du blog technique d&#39;M6Web",
    "category" : "",
    "tags"     : " ",
    "url"      : "/post/23664141031/lancement-du-blog-technique-dm6web",
    "date"     : "May 24, 2012",
    "excerpt"  : "Bienvenue sur le blog de la direction technique de M6 Web.\n\nVous retrouverez ici, une frÃ©quence quâ€™on espÃ¨re des plus rÃ©guliÃ¨res, quelques articles et autres retours dâ€™expÃ©rience de nos Ã©quipes technique.\n\nAttendez vous Ã  manger du PHP, Mysql, Nod...",
  "content"  : "Bienvenue sur le blog de la direction technique de M6 Web.\n\nVous retrouverez ici, une frÃ©quence quâ€™on espÃ¨re des plus rÃ©guliÃ¨res, quelques articles et autres retours dâ€™expÃ©rience de nos Ã©quipes technique.\n\nAttendez vous Ã  manger du PHP, Mysql, Node.js, entendre parler de performance, monitoring, vidÃ©o, html5 etc ;-)\n\nBonne lecture Ã  tous.\n\n"
} 
  
  
  
]
