[
  
  {
    "title"    : "How we improved scroll performance on Smart TV apps",
    "category" : "",
    "tags"     : " TV, performance, javascript, react, web, frontend",
    "url"      : "/2024/11/22/tvjs-scroll-performance-enhancement.html",
    "date"     : "November 22, 2024",
    "excerpt"  : "One of the core experiences of a Bedrock app for the end user is browsing the catalog. Scrolling vertically through blocks of content, and scrolling horizontally through lists of items. However, TVs do not offer high performance and provide poor u...",
  "content"  : "One of the core experiences of a Bedrock app for the end user is browsing the catalog. Scrolling vertically through blocks of content, and scrolling horizontally through lists of items. However, TVs do not offer high performance and provide poor user experience during heavy resource actions. We especially noticed that scrolling horizontally in a list was laggy and unpleasant. This article focuses on performance optimization to enhance the horizontal scroll experience on Smart TVs.\n\n\n\nNote : The GIF above shows a laggy scroll experience on TV. During the videos featured in this article, a x20 CPU throttle has been enforced on the browser, to mimic a low-performance TV device\n\nContext\nOn TV, we scroll horizontally by focusing each item sequentially when the user presses the left or right arrow button on their remote.\n\nScrollable lists can be of various sizes and even include paginated content. In cases of paginated content, the next page is fetched preemptively during scroll, when the focus reaches a certain threshold.\n\nOur old scroll component worked as follows: we would render a whole list of items, in a parent component handling scroll. When scrolling horizontally, the focus would switch to the next item. This would notify the parent component in charge of scroll, that would move the whole list laterally. The movement was computed from the measurements of the focused item, the size of the list, and the size of the container.\n\nThere are multiple chances for improvement in this implementation:\n\n\n  Since every item was rendered in the DOM, moving the whole list was very heavy. Subsequently, a whole page of lists was itself pretty heavy to render.\n  Because the whole list is rendered, fetching a new page means that the new items are immediately all rendered to the DOM, imposing a heavy load by displaying content that is not even on the screen.\n\n\nVirtualization\nTo address the first shortcoming of the initial approach, we introduced virtualization. Virtualization is a technique to render only the items that are visible on the screen.\n\nFor context, the content we display on each list is normalized and stored in a redux store. All the items are available in an array and can be selected by their respective index.\n\nconst ItemComponent = ({ position }) =&amp;gt; {\n    const item = useSelector(selectItemByIndex(position));\n\n    return &amp;lt;Item {...item} /&amp;gt;;\n}\n\nThe virtualized scroll renders items based on a static array, each cell of the array being a slot for the item it‚Äôs going to display.\nconst ScrollComponent = () =&amp;gt; {\n    const SCROLLER_BASE_ARRAY = Array.from(\n        { length: nbItemsToDisplay },\n        (_, index) =&amp;gt; index - 1\n    );\n\n    return SCROLLER_BASE_ARRAY.map(index =&amp;gt; {\n        const [focusOffset, setFocusOffset] = useState(0);\n        // focusOffset is a state updated upon user input: \n        // + 1 when the right arrow is clicked, -1 when the left arrow is clicked\n        const position = index + focusOffset;\n\n\n        return (\n            &amp;lt;ItemComponent\n                key={position}\n                position={position}\n            /&amp;gt;\n        );\n    });\n}\n\n\n\n\nEach cell is connected to the store and uses its own index as selection parameter to get the corresponding item in the store (cell of index 0 gets the first item, cell of index 1 gets the second, etc.)\n\n\n\nAt this point, only a subset of the list is rendered, as many items as the static array has cells.\n\nHorizontal scroll is managed by incrementing the selection index upon user input (e.g., pressing the right arrow key). Using the same array, each cell now selects from the store the item for its index plus an ‚Äúoffset‚Äù that describes how much the list is scrolled.\n\n\n\nBy offsetting the items at every user input (negative offset to move the items to the right and positive offset to move the items to the left), we achieve a visual scroll, with only a subsection of the list displayed on screen.\n\n\n\nOptimised Rendering with React Keys\n\nThe heart of the implementation is to fill each cell with a new item at each scroll. From the point of view of a single cell, when we scroll, the item it displays is new. But we know that the item already existed in the DOM, just one cell over.\n\nThis is where we can leverage React‚Äôs keys mechanism. Items rendered use a key that combines the original cell index with the current scroll offset. These keys help React reconcile the item in cell 1 before render as the item in cell 2 after render as the same item, thus reusing the same DOM node. As a result, we get zero re-renders for the items that are shifting places, significantly reducing the performance impact of a scroll.\n\n\n  \n  ‚òùÔ∏èProfiling during a single scroll right. The only items rendering are the ones with focus change (item losing focus and item gaining focus) and the new item that wasn‚Äôt on the screen. Every other item is unaffected by a horizontal scroll\n\n\n\nOptimised pagination\n\nA nice win from virtualization is the impact on pagination. Only a subset of items is rendered on the screen. Also, the list itself only needs to know about that subset of items since it uses a constant array to display its items. This means that a new page fetched has 0 impact on renders: the new items are added to the store, but the React component itself has no knowledge of that operation and triggers no re-renders.\n\nResults\nNote: measurements presented here are taken with the Chrome DevTools performance tab, with x6 CPU throttle and network connection limited to fast 4G to mimic a low-performance TV device and keep a steady test environment. Times are scripting and rendering times added.\n\nWe can compare a few benchmarks to exhibit the gains from the new scroller.\n\nScrolling right is obviously less expensive now. Here, measurements were taken from a single scroll right, in a 72 items list.\n\n\n  \n    \n      Before\n      After, with new scroll\n    \n  \n  \n    \n      462ms\n      41ms (-91%)\n    \n  \n\n\nBut more closely to the app‚Äôs actual use, here is a scenario measuring the cost of scrolling right through a list of 72 items, with 8 pages fetched during scroll.\n\n\n  \n    \n      Before\n      After\n    \n  \n  \n    \n      11615ms\n      8631ms (-26%)\n    \n  \n\n\nHere, we include everything else a list does when scrolling (fetching new pages, additional handlers‚Ä¶), so the gain is less, but still significant.\n\nScrolling down in a page with lighter lists is also more efficient. Here, measurements were taken during a scroll down 25 lists.\n\n\n  \n    \n      Before\n      After\n    \n  \n  \n    \n      1308ms\n      1038ms (-21%)\n    \n  \n\n\nBeyond benchmarks, on-device tests were also conclusive: the scroll is smoother, we almost eliminated the lag caused by a pagination fetch. Overall, it feels better to scroll through a list.\nConclusion\n\n\n\nThis frontend R&amp;amp;D project successfully addressed the scrolling performance issues on TV. The new scrolling solution dramatically improved performance by limiting re-renders. This optimization ensured a smoother scrolling experience, enhancing usability on TV devices. From this experience, we also moved on to implementing the same virtualization on the horizontal scroll of the catalog, which presented its own challenges but was also a success.\n"
} ,
  
  {
    "title"    : "Insights from Golab Conference 2024, Florence",
    "category" : "",
    "tags"     : " golang, opensource, community, conference, 2024",
    "url"      : "/2024/11/12/golab-florence.html",
    "date"     : "November 12, 2024",
    "excerpt"  : "Attending the Golab conference in Florence has been a thrilling experience for us as developers. Immersing ourselves in talks by some of the most brilliant minds in the Go community gave us a deeper understanding of advanced concepts, practical li...",
  "content"  : "Attending the Golab conference in Florence has been a thrilling experience for us as developers. Immersing ourselves in talks by some of the most brilliant minds in the Go community gave us a deeper understanding of advanced concepts, practical libraries, and the future of Go. We‚Äôve summarized the key takeaways from the talks we attended to share some of the valuable lessons and fresh perspectives we gained.\n\n\n\nKeynote: Go Telemetry Wins\n\nThe opening keynote, delivered by Russ Cox, focused on the importance of telemetry in the Go ecosystem itself. Russ explained how telemetry data helps the Go team make informed decisions to improve the language and its ecosystem, emphasizing the strict measures taken to ensure user privacy. We learned how telemetry is carefully designed to protect sensitive information while still providing valuable insights.\n\n\n\nThe session concluded with a call to action, encouraging everyone to enable telemetry (by running go telemetry on) on their Go installations to support ongoing enhancements and community-driven improvements.\n\nAll data collected are public and could be found here\n\n\n\nInstrumenting Go Apps With OpenTelemetry\n\nThis talk introduced us to OpenTelemetry, an open source solution for instrumenting applications. The speaker introduced the topic with a specific situation, you have instrumenting with datadog and want to switch to another tool (for any reason such as better pricing, UX‚Ä¶).\n\n\n\n\n\nThen he demonstrated how OpenTelemetry can be used to collect and analyze performance data (ie. through Grafana), and its benefits compared to an agent :\n\n  No need to install an agent on the host\n  No cost of install when changing agent\n  Open source so it‚Äôs tool agnostic\n\n\n\n\nAt Bedrock, we mainly use NewRelic for our monitoring, but we are free either to use the NewRelic agent or to use OpenTelemetry to collect traces, metrics and logs.\n\n\n\nWatermill: The Missing Standard Library for Event-Driven Applications\n\nThe Watermill library was highlighted as a game-changer for event-driven architecture in Go. Initially born out of a need to simplify the complexity of building event-driven systems, Watermill has evolved into a mature and widely adopted library. The talk inspired us by demonstrating how adopting Watermill can transform projects needing scalability and resilience, and it has now become a key consideration for us in future architectural designs.\nThe design of this very complete library is thought out in three blocks that can be used independently or not:\n\n\n\nThe first, on which the rest is based, is a simple Publisher/Subscriber brick with several implementations (go channel,kafka,RabbitMQ etc‚Ä¶).\n\nThe second is a Router layer which manages advanced event dispatch processing and adds the possibility of using middleware such as metrics, poison queue, retrying, throttling, etc. .\n\nThe last block is the CQRS management block.\nThe project documentation is well supplied and full of examples that make bootstrapping in existing projects much easier.\nYou will find its presentation materials here.\n‚Äî\n\nLet‚Äôs Go Asynchronous\n\nTom√°≈° Sedl√°ƒçek tackled the common dilemma between synchronous and asynchronous communication.\n\nReal-world examples illustrated how synchronous patterns can lead to bottlenecks, while asynchronous designs promote efficiency. Techniques like load balancing, rate limiting, and implementing retries were demonstrated, showcasing Go‚Äôs flexibility with tools like RabbitMQ and PGQ. It‚Äôs a solid reminder to evaluate communication patterns carefully and how going asynchronous isn‚Äôt as daunting as it sounds.\n\n\n\nHow to Write a Programming Language and Shell in Go with 92% Test Coverage\n\nA fascinating exploration of Elvish, a shell written in Go, broke down how to create a programming language.\n\nBeyond the technical implementation, Qi Xiao provided insights into achieving impressive test coverage and setting up instant CI/CD pipelines. The talk underscored the power of Go in developing robust systems, even for complex challenges like creating a shell.\n\n\n\nFrom Bland to Spiky: How Generics Made My Service Super Robust\n\nThis lively talk explored how generics have brought versatility to Go.\n\nTeea Alarto used vivid examples, transforming generic code to create robust, maintainable services. The presentation not only demystified generics but also highlighted their practical benefits, such as eliminating repetition and enhancing code maintainability.\n\n\n\nTinyGo for Pet Automation\n\nThis session was both fun and practical, demonstrating how TinyGo can be leveraged for pet automation using Arduino.\n\nJosephine Winter presented a compelling case for TinyGo as an accessible platform for embedded programming, opening possibilities beyond traditional web development. Automation projects, whether for pets or other use cases, seem more achievable and engaging with TinyGo.\n\n\n\nGophers Gone Domain-Driven: A Tale of Go and DDD\n\nDomain-Driven Design (DDD) often intimidates developers, but this talk simplified the fundamentals for Go practitioners.\n\nBy exploring domain modeling, bounded contexts, and ubiquitous language, we gained insights into creating a codebase that aligns with business logic.\n\nDavide Imola also discussed using hexagonal architecture to make Go applications scalable and maintainable, providing a practical roadmap for implementing DDD concepts.\nHe provides a simple example of a structured Go project using the DDD and Onion Architecture concepts.\n\n\n\nTo CGo or Not: Cross Compiling a SQLite Driver\n\nThe challenge of cross-compiling without relying on CGo was explored in this technical deep dive.\n\nJan Mercl introduced a CGo-free driver, explaining how transpilers and C runtime emulation make it feasible.\n\nThis was particularly eye-opening for those of us working on projects that need efficient and lightweight database solutions.\n\n\n\nGraphs and Games: Can Go Take a Ticket to Ride?\n\nBlending board games and Go programming, Michele Caci took us on a journey through graph algorithms inspired by ‚ÄúTicket to Ride.‚Äù\n\nHe demonstrated how they implemented a Go-based version of the game and applied algorithms to improve gameplay.\n\nIt was a fun reminder of how computer science concepts can intersect with entertainment, sparking ideas for gamifying our projects.\n\n\n\nDeep Dive into a Go Binary\n\nJes√∫s Espino, Software developer at Mattermost and who loves to deep dive into technical details and previously talked about Go Runtime, answered this question:\n\nWhat lies inside a Go binary?\n\n\n\nJes√∫s unveiled the secrets of compiled Go programs while mirroring The Lord of the Rings making it very pedagogical, starting with ELF format composed of a set of headers, list of sections and segments to the internal mechanisms that make a binary tick.\n\nUnderstanding the anatomy of a Go binary enhanced our appreciation for the Go compiler and the efficient executables it generates.\nYou can view his presentation slides here\n\nHere some of his references:\n\n  In-depth ELF\n  GoReSym\n  Garble\n  The go source code (specifically the linker code)\n\n\n\n\nHow to Punch Holes in Network Infrastructure Using Go\n\nFinally, we delved into the intricacies of peer-to-peer networking. Using the go-libp2p module, the speaker showed how to establish decentralized connections by bypassing traditional network barriers.\n\nThis talk was both technical and visionary, providing us with ideas on how to optimize network performance using Go‚Äôs robust features.\n\nYou will be able to find his presentation materials here.\n\n\n\nClosing Keynote: Go Back to the Future\n\nThe closing keynote was a captivating journey through Go‚Äôs past, present, and future, filled with live demos and unexpected surprises.\n\nThe speaker showcased the evolution of Go while exploring how the language might shape the future of programming. One of the highlights was a live demonstration where the presenter made a drone fly over the audience, all powered and controlled by Go. This thrilling spectacle illustrated the language‚Äôs potential in robotics and automation, leaving everyone inspired and in awe of what Go can achieve.\n\nConclusion\n\nThe Golab conference in Florence provided a rich tapestry of knowledge, sparking inspiration and reaffirming our love for Go. From new architectural insights to innovative uses of Go in domains like automation and networking, we walked away with a toolkit full of ideas to bring back to our development practices. We‚Äôre eager to implement some of these learnings and contribute further to the vibrant Go community!\n\n\n\n"
} ,
  
  {
    "title"    : "Using Fastly&#39;s Compute at-edge to personalize static pre-generated pages",
    "category" : "",
    "tags"     : " edge, fastly, cloud, devops, 2024",
    "url"      : "/2024/11/07/compute-at-edge-personalize-static-pages.html",
    "date"     : "November 7, 2024",
    "excerpt"  : "During the UEFA Euro football cup in June and July 2024, M6 broadcasted several matches. Of course, this competition was available, live, on M6+. For every user joining right from the first second and or for all users hearing their neighbors shout...",
  "content"  : "During the UEFA Euro football cup in June and July 2024, M6 broadcasted several matches. Of course, this competition was available, live, on M6+. For every user joining right from the first second and or for all users hearing their neighbors shouting and wanting to re-watch the action, starting a live video stream had to work!\n\nWhen you use one of the platforms we manage, your goal is to start a video stream. You land on the homepage of a platform like M6+ or Videoland, browse the catalog, register or login and, ultimately, you‚Äôll start a video. It‚Äôs the one thing that causes the most frustration if it fails. And we don‚Äôt want that. We want users to enjoy their experience.\n\nFor Euro, we first worked on shortening the browsing part, to both provide a great experience for users arriving to watch a match, and to lower the load on our platform. We talked about this in How Special Event Page allowed us to handle more than 1 million of users.\n\nWe also worked on reducing load on our backend applications, allowing our platform to handle far more users and load than it‚Äôs used to. This post is the story of how we used compute-at-edge for the first time!\n\nOur backend platform\n\nAll our frontend (web/mobile/TV) applications communicate with our backend components through what we call BFF. This BFF then calls many other backend APIs and returns a layout that contains everything an app needs. This layout will contain lists of rows and blocks when you are browsing the catalog, or most information required to start a stream when you are trying to do so.\n\n\nflowchart LR\n    user1[User 1]\n    user2[User 2]\n    user3[User 3]\n\n    user1 --&amp;gt; bff\n    user2 --&amp;gt; bff\n    user3 --&amp;gt; bff\n\n    bff((BFF))\n\n    bff --&amp;gt; api1\n    bff --&amp;gt; api2\n    bff --&amp;gt; api3\n    bff --&amp;gt; api4\n    bff --&amp;gt; api5\n\n    api1[API-1]\n    api2[API-2]\n    api3[API-3]\n    api4[API-4]\n    api5[API-5]\n\n\nSchema 1: one BFF between frontend apps and backend APIs.\n\n\n\nWith this architecture, BFF receives a lot of requests, and each of these causes up to dozens of requests to other services. This is greatly amplified by the fact that all BFF responses are personalized, which means we cannot cache entire responses.\n\nAs most of our APIs are running in a Kubernetes cluster, you might say ‚Äúuse auto-scaling!‚Äù, and you‚Äôd be kind of right. We are using auto-scaling. But reactive auto-scaling, based on traffic or load metrics, is not fast enough to handle huge spikes in traffic ‚Äì like big football matches can cause. For this, we have developed a pre-scaling mechanism, and even open-sourced it.\n\nBut we didn‚Äôt want to take any chances‚Ä¶\n\nDisabling all personalization?\n\nFirst, a word about an idea we used three years ago: disabling all personalization. It‚Äôs quite easy to implement: ensure BFF doesn‚Äôt do any personalization work, and return a cache-control HTTP header so the CDN in front of the BFF backend application caches layouts and returns them to all users.\n\nWith BFF only generating a few dozen responses every minute, we solved all possible problems with load or scaling and traffic spikes. But this also means cutting many features for end-users, and not displaying personalized ads.\n\nOur customers want to display personalized contents and ads, and users want to get all features from the platform. So, we decided not to enable this mechanism this year.\n\nDoing more work‚Ä¶ in front of the backend app!\n\nSeveral months before the Euro competition, one of our Principal Engineers had done a few prototypes with Fastly‚Äôs compute and datastores at-edge, convinced being able to do more things before the apps would one day prove useful. Well‚Ä¶ We brought back this idea to focus. And our BFF application was already using Fastly as its CDN, what a coincidence ;-)\n\n\n  Fastly allows one to run compute at-edge, on the CDN points-of-presence. Basically, you write code in Go/Rust/Javascript and compile it to WASM.\n  It provides several datastores. The one we used here is called KVStore ‚Äì a basic key-value store.\n\n\nWe reworked our architecture this way:\n\n\nflowchart LR\n    user1[User 1]\n    user2[User 2]\n    user3[User 3]\n\n    subgraph In Front Of BFF\n        new(NEW)\n        datastore[(Datastore)]\n        \n        new -.-&amp;gt; datastore\n    end\n\n    user1 --&amp;gt; new\n    user2 --&amp;gt; new\n    user3 --&amp;gt; new\n\n    new -- cache --&amp;gt; bff\n\n    bff((BFF))\n\n    bff --&amp;gt; api1\n    bff --&amp;gt; api2\n    bff --&amp;gt; api3\n    bff --&amp;gt; api4\n    bff --&amp;gt; api5\n\n    api1[API-1]\n    api2[API-2]\n    api3[API-3]\n    api4[API-4]\n    api5[API-5]\n\n\nSchema 2: doing work in front of the BFF.\n\n\n\nOur goal was to implement a lightweight personalization layer in front of our backend BFF application. The backend application would then only need to return non-personalized layouts ‚Äì and those would be cacheable.\n\nNote: as we have many frontend applications and did not want (and did not have enough time) to update all of them, we searched for a solution that would only require changes somewhere on the backend side.\n\nThis personalization layer would read some basic data (does the current user have a subscription? Did they consent to tracking and analytics?) from a datastore-at-edge and use them to inject identifiers in the cached-non-personalized layout before returning it.\n\nAs BFF was already using a Fastly VCL service as its CDN and a VCL service cannot also do compute, we decided to insert a compute service between the VCL one and the backend, only for the /live/ route. As time was limited and we wanted to confirm how this would work with real users, we chose to implement this approach only for one route and one layout, the one that would be called the most: the layout used to start a live stream.\n\nThis means our CDN architecture was looking like this:\n\n\nflowchart LR\n    users[Users]\n    users --&amp;gt; fastlyVCL\n\n    subgraph Fastly CDN\n        fastlyVCL[FastlyVCL]\n\n        fastlyVCL -- /live --&amp;gt; fastlyCompute\n        fastlyCompute[FastlyCompute]\n\n        datastore[(Datastore)]\n        fastlyCompute -.-&amp;gt; datastore\n    end\n\n    fastlyVCL -- /* --&amp;gt; bff\n    fastlyCompute -- &quot;read(with cache)&quot; --&amp;gt; bff\n\n    bff((BFF))\n\n\nSchema 3: chaining a VCL and a compute service.\n\n\n\nBefore starting to actually implement this, we talked with our contacts at Fastly, to confirm this approach made sense to them and their systems would be able to handle the load and traffic spikes we were expecting. They validated the concept, and noted we should shard our data over several KVStores, as each KVStore is limited to 1000 writes/second and 5000 reads/second ‚Äì good catch!\n\nWe then started implementing this approach, first with our compute-at-edge code doing static replacements, then with loading random data from the KVStores, and finally with loading the real data. Between each step, we ran synthetic load-tests, to ensure everything was running smoothly.\n\nAlso, we ensured from day-1 we would not have to throw all this ‚Äúdo more work in front of BFF‚Äù approach away, if Fastly was not able to handle our needs. We had a fallback in mind: deploying this inside our Kubernetes cluster as a Go application and storing data in DynamoDB. Most of the code would still have worked.\n\nGetting the data to the datastore-at-edge\n\nReading data from KVStores and using it to personalize the live layout being out of the way, it was time to think about how to get data into those KVStores.\n\nWe had to synchronize pieces of data from two different backend APIs to the at-edge KVStores. First is our ‚Äúusers‚Äù API, for GDPR consents. And the second is our ‚Äúsubscription‚Äù API. Both store their data in DynamoDB.\n\nSeveral months before, one of our Principal Engineers had done a few demos and prototypes showing how to use DynamoDB Streams and Lambda the right way and proving asynchronous is not necessarily slow. This helped not start from scratch here, having confidence working with DDB Streams and Lambda would be OK, and providing some code foundations.\n\nSo, in both ‚Äúusers‚Äù and ‚Äúsubscriptions‚Äù projects, we added a DynamoDB Stream on the tables used to store the relevant data. Those streams are read from a couple of Lambda functions (with retries, batches bisect, dead-letters queues‚Ä¶ all natively handled by AWS). And those functions call Fastly‚Äôs KVStore API to insert/update/delete data there. We did not forget to deal with the 1000 RPS per KVStore limit.\n\n\nflowchart LR\n    subgraph &quot;API-2 (AWS)&quot;\n        api2[API-2]\n        ddb2[(DynamoDB)]\n        api2 --&amp;gt; ddb2\n        ddb2 -- DDBStream --&amp;gt; lambda2\n        lambda2(Lambda)\n    end\n\n    subgraph &quot;API-4 (AWS)&quot;\n        api4[API-4]\n        ddb4[(DynamoDB)]\n        api4 --&amp;gt; ddb4\n        ddb4 -- DDBStream --&amp;gt; lambda4\n        lambda4(Lambda)\n    end\n\n    lambda2 --&amp;gt; datastore\n    lambda4 --&amp;gt; datastore\n\n    subgraph &quot;Fastly CDN&quot;\n        datastore[(Datastore)]\n    end\n\n\nSchema 4: pushing data asynchronously to Fastly‚Äôs KVStores.\n\n\n\nWith this mechanism, data in the KVstores is updated after 1 or 2 seconds (we could speed things up a little by not using batching when reading from DynamoDB Streams), which is fine for this use-case.\n\nIn addition to this synchronization mechanism, we also implemented a full import process, used to initialize data for all users and subscriptions ‚Äì and to fix a few edge-cases with odd data we didn‚Äôt anticipate.\n\nBetter resiliency\n\nLooking at the architecture schema shared before, one of our Principal Engineers noticed if the BFF backend component is down (possibly because of an overload caused by too many users browsing the catalog), it will not return non-personalized layouts. And Compute, at-edge, will not be able to do its personalization work.\n\nSo, we chose to asynchronously pre-generate the non-personalized layouts, and store them on Amazon S3. S3 would then be used as origin by Fastly Compute.\n\n\nflowchart LR\n    users[Users]\n    users --&amp;gt; fastlyVCL\n\n    subgraph Fastly CDN\n        fastlyVCL[FastlyVCL]\n\n        fastlyVCL -- /live --&amp;gt; fastlyCompute\n        fastlyCompute[FastlyCompute]\n\n        datastore[(Datastore)]\n        fastlyCompute -.-&amp;gt; datastore\n    end\n\n    fastlyCompute -- &quot;read(with cache)&quot; --&amp;gt; s3\n    s3[(S3 Bucket)]\n\n    fastlyVCL -- /* --&amp;gt; bff\n    \n    bff((BFF))\n    bff -- &quot;Generate staticfiles every X minutes&quot; --&amp;gt; s3\n\n\nSchema 5: storing non-personalized layouts on Amazon S3.\n\n\n\nOf course, doing this requires a bit more development work. We‚Äôve had to setup a background cronjob to generate static layouts and store them on S3. But, keeping in mind our ‚Äúusers must be able to start a stream‚Äù goal, we estimated the potential gain on resiliency was worth it. Also, we already had a process to generate static layouts and push them to S3, so it wasn‚Äôt that much additional work.\n\nThe Special Event Page helped ensure users would not have to actually call most our backend APIs at all between the homepage and starting a live stream.\n\nLoad-testing and real life\n\nWe carefully load-tested this solution, every step of the way while implementing it. Once fully implemented, we load-tested it again and again, to ensure it would handle as much traffic as we were expecting to get during high-stakes matches.\n\nDoing those load-tests and analyzing their results with our contacts at Fastly helped us identify three points we quickly fixed:\n\n\n  At first, our KVStores‚Äô primary location was in the US. Fastly reconfigured our account to have them primarily located in the EU, gaining several dozen milliseconds of latency each time new users would do their first read.\n  We compiled our Golang code to WASM with both Tinygo and Biggo. One produces WASM that used more CPU, and the other produces WASM that used more RAM. In the end, we followed Fastly‚Äôs recommendations, considering they know better than us what resource could be a bottleneck for their platform.\n  The first time we ran tests in our production environment, results were not great. Far worse than in our staging environment, in fact. Well, we had not paid for the Compute option in our Fastly‚Äôs production account yet, and it was configured with lower rate-limits than in staging üòÉ\n\n\nDuring these load-tests as well as during real events later, we monitored a few basic metrics: number of calls per second, CPU and RAM usage, latency and error-rate.\n\n\n\nSchema 6: monitoring Fastly‚Äôs Compute during a load-test.\n\n\n\nThe high number of requests-per second we reached during our load-tests proved Fastly Compute is a viable approach for this workload, and for some others we are already thinking about migrating.\n\nOn our backend‚Äôs application side, we also checked the number of calls per second was going down while it was going up on Compute at-edge. In practice, it went down to 0 for the /live/ route, and remained stable or even went up for other routes, as there were more users browsing the catalog.\n\n\n\nSchema 7: Backend (Kubernetes) and Fastly Compute for /live/, before and during a match.\n\nConclusion\n\nWith this approach to generating personalized layouts at-edge for the /live/ route, users of our platform have been able to enjoy the competition without any hiccup!\n\n\n  We personalized millions of live layout on the CDN at-edge, using Fastly Compute, enabling both users and customers to experience pretty much all features of the platform.\n  Our backend BFF and other APIs have not been overloaded. They have been fully operational to serve requests for users browsing the catalog or starting videos on demand.\n\n\nAnd this proved edge computing is a viable option to implement some features of our platform. More than this, though, it proved we can separate our BFF software into several smaller parts, which is one of the major ideas we will implement next year while re-architecturing it.\n\nWhat are the next steps, then?\n\n\n  First, use Compute at-edge to serve a couple other highly-solicited routes.\n  And, for the /live/ route, implement a few use-cases that were not required for this competition, in order to use Fastly Compute for this route all the time!\n\n"
} ,
  
  {
    "title"    : "We love speed 2024 ‚ù§Ô∏è",
    "category" : "",
    "tags"     : " performance, conference, webperf, javascript, react, web, frontend",
    "url"      : "/2024/10/29/we-love-speed-2024.html",
    "date"     : "October 29, 2024",
    "excerpt"  : "Nous avons eu la chance de participer √† la conf√©rence We love speed, une conf√©rence annuelle ax√©e sur la\nperformance du web. C‚Äôest un domaine qui nous passionne et nous sommes tr√®s content d‚Äôavoir pu y assister.\nLe th√®me de cette √©dition, c‚Äôest l‚Äô...",
  "content"  : "Nous avons eu la chance de participer √† la conf√©rence We love speed, une conf√©rence annuelle ax√©e sur la\nperformance du web. C‚Äôest un domaine qui nous passionne et nous sommes tr√®s content d‚Äôavoir pu y assister.\nLe th√®me de cette √©dition, c‚Äôest l‚ÄôINP. En effet, cette m√©trique de performance a √©t√© ajout√©e aux core web vitals par\nGoogle r√©cemment.\nL‚Äôobjectif de cette m√©trique est de refl√©ter l‚Äôexp√©rience utilisateur en mesurant la r√©activit√© d‚Äôune application.\nElle observe le temps entre une action utilisateur et une r√©ponse visuelle de notre interface.\n\n\n\nHTMX, le nouvel atout pour vos projets SSR - Ewen Quimerc‚Äòh\n\nLors de ce talk, nous avons d√©couvert un outil tr√®s int√©ressant. Il s‚Äôagit de HTMX¬†: une biblioth√®que Javascript qui\npermet d‚Äôajouter des fonctionnalit√©s de type SPA (Single Page Application) √† une application web classique et de fa√ßon\nnon intrusive. Par exemple, on peut surcharger les liens &amp;lt;a&amp;gt; pour qu‚Äôils chargent une nouvelle page en AJAX gr√¢ce √† un\nattribut ajout√© au HTML. Ce mode de fonctionnement est tr√®s int√©ressant, car il permet de garder une application web\nclassique avec tous ses comportements le temps que HTMX se charge. C‚Äôest-a-dire que si HTMX venait √† ne pas d√©marrer,\nvotre application web se comporterait de la m√™me mani√®re, mais sans les am√©liorations de temps d‚Äôinteraction.\n\nHTMX surcharge la mani√®re dont vos liens et images vont √™tre charg√©s par le navigateur.\nAinsi, lors de la prochaine interaction, ce dernier sera d√©j√† pr√™t √† servir les ressources.\nLe principe de HTMX consiste √† ajouter des balises HTML sp√©cifiques dans le DOM qu‚Äôil va lire et en d√©duire les\ncomportements √† son chargement.\nCette manipulation est appel√©e le ‚ÄúDOM morphing‚Äù. Gr√¢ce √† ce processus le temps de chargement est r√©duit et on √©vite\nl‚Äôeffet ‚Äúblink‚Äù (page blanche lors du chargement de la page).\nIl est √† noter que ces comportements ne sont qu‚Äôun embellissement propos√© par HTMX, il est tout √† fait possible\nd‚Äôajouter par exemple l‚Äôattribut preload sur une balise a pour demander le chargement en avance du lien par le\nnavigateur.\n\n\n  \n\nComme nous utilisons React pour notre application, l&#39;utilisation de HTMX n&#39;est pas vraiment utile.\nIl est d√©j√† possible avec React de pr√©charger les ressources en avance. Mais √ßa reste un outil int√©ressant...\n\n\n\n\n  \n...effectivement, HTMX semble √™tre int√©ressant, mais on se retrouve √† ajouter\nbeaucoup d&#39;attributs dans le HTML. √áa peut le rendre le markup moins lisible. Et en plus, √ßa donne l&#39;impression de recoder les comportements du navigateur.\n\n\n\n\nReact / Next vs INP - Jean-Pierre Vincent\n\nLe deuxi√®me talk a une place particuli√®re dans notre c≈ìur ‚ù§Ô∏è puisqu‚Äôil a √©t√© donn√© par notre cher Jean-Pierre Vincent,\nqui a audit√© les performances du web de Bedrock, il y a deux ans.\nLors de ce talk, Jean-Pierre nous a donn√© la feuille de route pour √©viter au mieux la d√©ferlante de Javascript que vos\nutilisateurs re√ßoivent au chargement de votre site.\n\n\n\nL‚ÄôINP (Interaction to Next Paint) est une m√©trique qui mesure le temps entre une interaction utilisateur et le prochain\nrendu du navigateur. L‚Äôid√©e g√©n√©rale est de pouvoir mesurer l‚Äôincapacit√© du navigateur √† r√©agir. Apr√®s avoir r√©cup√©r√©\ndes mesures, il est bon de se rappeler qu‚Äôil y a un biais de selection pour les donn√©es de Crux. Pour rappel, Crux est\nune base de donn√©es qui contient des m√©triques de performance de sites web collect√©es par Google.\nEn effet, il n‚Äôest calcul√© que sur les appareils Google (c‚Äôest le principe). Une fois qu‚Äôon a r√©colt√© des m√©triques de\nperformance de nos utilisateurs, si on veut travailler sur notre site web et avoir une bonne id√©e du ressenti de nos\nutilisateurs, l‚Äôid√©al est de tester avec un v√©ritable Samsung S8 par exemple. Le S8 est un appareil sur lequel on a\nbeaucoup de donn√©es et qui repr√©sente √† l‚Äôheure actuelle une bonne repr√©sentation des capacit√©s de l‚Äôutilisateur moyen.\nL‚ÄôINP est une m√©trique qui peut √™tre influenc√©e par des interactions qui ne sont pas pr√©vues par les devs. Par exemple,\non a √©t√© √©tonn√©s de constater que lorsque les temps de chargement sont un poil trop longs √† leur go√ªt, les utilisateurs\nse mettent √† cliquer partout ü§∑ C‚Äôest pourquoi il est important de se baser sur des donn√©es r√©elles.\n\n\n\nParmi les bonnes pratiques qu‚Äôon peut appliquer d√®s maintenant, et qui je dois le dire m‚Äôa paru un peu contre-intuitif :\nfaire passer Babel sur les node_modules.\nEn fait, du point de vue d‚Äôun d√©veloppeur, on peut se dire que cela va augmenter les temps de build drastiquement, et on\naurait s√ªrement raison. Mais en fait, il faut voir le b√©n√©fice qu‚Äôil y a derri√®re. Si on personnalise les r√®gles Babel\nafin qu‚Äôelles correspondent aux navigateurs de nos utilisateurs, on √©vite des transformations inutiles qui\naugmenteraient le poids de nos fichiers Javascript.\n\nUne nouvelle fonctionnalit√© de React appel√©e RSC (React Server Components) permet de combiner le rendu c√¥t√© serveur avec\nl‚Äôinteractivit√© c√¥t√© client.\nLes RSC aident √† r√©duire la taille du Javascript dans le navigateur ce qui permet d‚Äôam√©liorer le temps d‚Äôinteraction et\ndonc l‚Äôexp√©rience utilisateur. Vous l‚Äôaurez compris, c‚Äôest l‚Äôennemi n¬∞1 de Jean-Pierre (et de vos navigateurs) !\nLe principe est de rendre les composants c√¥t√© serveur et de faire en sorte que ces derniers ne rendent que du HTML, qui\nne sera pas hydrat√© c√¥t√© client.\nL‚Äô√©tape de r√©hydratation est une √©tape importante et trop souvent sous-estim√©e. Il s‚Äôagit d‚Äôune nouveaut√© de React qui\nest prometteuse et qui est d√©j√† pr√©sente dans Next.js.\n\nPour nous montrer un exemple concret d‚Äôabus de JavaScript : il a montr√© du code Bedrock üòÖ.\nIl s‚Äôagit d‚Äôun FlameGraph du rendu de notre &amp;lt;Footer /&amp;gt; c√¥t√© app web. Il y a une quantit√© cons√©quente de JS car nous\nfaisions ce qu‚Äôon appelle du CSS-in-JS.\nVous l‚Äôavez devin√©, c‚Äôest la partie ‚Äúin-JS‚Äù qui pose un probl√®me. Cela signifie que pour appliquer du style sur notre\nsite, c‚Äôest le Javascript qui s‚Äôen charge. Or dans un composant, comme le &amp;lt;Footer /&amp;gt;, il y a beaucoup d‚Äô√©l√©ments et\nchacun va avoir besoin de son propre style. Si l‚Äôid√©e de colocaliser le CSS dans le JS n‚Äôest pas nocive en soi, le plus\ngros probl√®me √©tait l‚Äôutilisation de Styled-Components qui calcule le style au moment\ndu rendu, le rendant donc plus long. FYI¬†: Entre temps, nous avons chez Bedrock entam√© une migration pour quitter\nStyled-Components au profit de Linaria pour le projet web\net Vanilla Extract pour le projet smart TV.\n\n\n\nAutre information qui nous concerne chez Bedrock, au moment d‚Äô√©crire ces lignes, nous sommes en train de mettre en\nproduction la migration de React 17 vers React 18 sur le projet web.\nD‚Äôapr√®s les retours d‚Äôexp√©rience de Jean-Pierre, cette version de React aura un impact positif sur l‚ÄôINP car il permet\nde faire moins de render.\n\nEnfin, Jean-Pierre nous laisse avec un ultime conseil pour que nos applications web soient p√©rennes : ‚ÄúMonitore (au\nmoins une fois dans ta vie) l‚Äôorigine des INP avec un vrai utilisateur.‚Äù\n\n\n  \n\nJ&#39;ai bien aim√© ce talk ! J&#39;ai trouv√© que sa pr√©sentation √©tait tr√®s accessible, il a su vulgariser des concepts et rendre un sujet fastidieux (la performance) int√©ressant üëè\n\n\n\nD√©bogage des performances √† l‚Äôaide des DevTools : Mise en pratique approfondie - Umar Hansa\n\nAvoir un ≈ìil sur ses performances est essentiel pour avancer dans la bonne direction et s‚Äôassurer qu‚Äôon\nfournit √† nos utilisateurs une exp√©rience optimale. Fort heureusement pour nous les devs, on est bien lotis avec de tr√®s\nbons outils. Il suffit d‚Äôouvrir les Chrome DevTools pour s‚Äôen rendre compte. Ce talk nous a pr√©sent√© comment bien\nutiliser les DevTools pour mesurer les performances de nos applications web et se mettre √† la place de\nnos utilisateurs. Par exemple, on peut brider son r√©seau et son CPU pour simuler une connexion 3G et un CPU lent. Dans\ncette pr√©sentation, on nous a quand m√™me rappel√© que les DevTools ne sont pas une solution miracle, il est important de\ntester sur de vrais devices pour le ressenti.\n\nPour ce qui est de l‚Äôinterpr√©tation des donn√©es, une myriade d‚Äôoutils est √† notre disposition pour nous aider √†\ncomprendre ce que nous voyons. Par exemple, on peut ajouter des annotations dans le flamegraph comme des labels, des\ndiagrammes ou encore des plages de temps. On peut aussi mettre en place des custom tracks pour suivre des √©v√©nements\nsp√©cifiques. Au sein de notre application, on peut utiliser l‚ÄôAPI User Timing pour ajouter des points de rep√®re dans\nnotre code et ainsi mieux comprendre ce qui se passe au d√©clenchement d‚Äô√©v√©nements sp√©cifiques.\n\nWeb Performance Testing - Estela Franco\n\nNous avons √©galement eu un talk sur l‚Äôint√©gration de Lighthouse, un outil de Google pour mesurer la performance de nos\napplications web, dans une CI. Cela permet de d√©tecter les probl√®mes de performance avant qu‚Äôils ne soient d√©ploy√©s en\nproduction. Il est possible de mettre des warnings, voire des erreurs emp√™chant de merger, si l‚Äôapplication ne respecte\npas les standards que nous nous sommes fix√©s. L‚Äôid√©e est de s‚Äôassurer que la performance de\nnotre application web est toujours au top et ne se d√©grade pas dans le temps.\n\n\n\n  \n\nOn envisage de l&#39;ajouter au projet smart TV, mais plus pour g√©n√©rer un rapport de performance quotidien plut√¥t que de le faire pour chaque push ou merge. \n\n\n\nComment les navigateurs chargent VRAIMENT les pages web - Robin Marx\n\nDans ce qui est probablement le talk le plus technique de la journ√©e, on nous a expliqu√© comment les navigateurs\nchargent les diff√©rentes ressources n√©cessaires √† l‚Äôaffichage d‚Äôune page web. Plus sp√©cifiquement, on nous a expliqu√©\ncomment les navigateurs interpr√®tent le HTML pour d√©terminer quelles ressources charger en priorit√©.\n\nLe talk √©tait tr√®s int√©ressant, mais la conclusion est un peu frustrante : il est pour le moment impossible de pr√©voir\nl‚Äôordre de chargement des ressources par le navigateur √† partir du m√™me HTML. En effet, les navigateurs ont des\ncomportements diff√©rents entre eux et m√™me par version ü§Ø. Chrome a, par exemple, un comportement tr√®s diff√©rent cette\nann√©e\npar rapport √† deux ans en arri√®re.\n\nM√™me si on est tent√© de vouloir contr√¥ler le chargement des ressources, il est important de se rappeler que le\nnavigateur\nest tr√®s bien optimis√© pour charger les ressources de mani√®re efficace. Il est donc pr√©f√©rable de laisser le navigateur\nfaire son travail plut√¥t que de vouloir le contr√¥ler. L‚Äôattribut preload est un bon exemple de ce que l‚Äôon peut faire\npour aider le navigateur √† charger les ressources de mani√®re plus efficace. Il faut cependant l‚Äôutiliser avec parcimonie\net de mani√®re chirurgicale pour ne pas interf√©rer avec le travail du navigateur.\n\n\n\nMais que fait la police ? - Eroan Boyer\n\nPour finir, on a eu un talk sur les polices de caract√®res. Elles sont essentielles pour l‚Äôidentit√© visuelle de nos\napplications web, mais elles peuvent aussi √™tre une source de probl√®mes de performance. En effet, les polices de\ncaract√®res peuvent √™tre tr√®s lourdes et ralentir le chargement de nos pages. Il est donc important de bien les choisir\net de les optimiser pour garantir une bonne performance. Il existe plusieurs techniques pour optimiser les polices,\nnotamment en r√©alisant un subset de la police pour ne t√©l√©charger que les glyphes dont on a besoin. (En fran√ßais, on a\nbesoin que de 165 glyphes, compar√© √† 528 pour le latin).\nIl existe des outils pour nous aider √† r√©aliser ces subsets\ncomme¬†: Font Subsetter, fontTools ou\nGlyphanger.\n\n  \nAttention √† ne pas abuser des subsets, car cela peut entra√Æner des probl√®mes de lisibilit√© du texte. Le fameux tü†âfu .\n\n\n\n\n\n\nIl est aussi possible de minimiser le nombre de fichiers en utilisant des polices variables. Un bon exemple est la\npolice Roboto Flex, qui est customisable et permet ainsi de pouvoir r√©duire le nombre de fichiers n√©cessaires √† charger.\nIl est l√† aussi, possible de s√©lectionner les variations que l‚Äôon souhaite pour r√©duire encore plus le poids de la\npolice.\n\nConclusion\n\nCette ann√©e, l‚Äôaccent a √©t√© mis sur l‚ÄôINP et la mani√®re de l‚Äôam√©liorer. Il est important de garder en t√™te que l‚ÄôINP est\nune m√©trique qui mesure l‚Äôexp√©rience utilisateur, il est donc essentiel de la garder √† l‚Äô≈ìil. Il est bon de\nrappeler que la performance est plus une habitude √† prendre qu‚Äôun constat √† r√©aliser. Une application performante, c‚Äôest\nune exp√©rience utilisateur am√©lior√©e et des utilisateurs satisfaits !\n\nDe notre c√¥t√©, nous sommes rentr√©s avec quelques id√©es √† mettre en place dans nos projets √† Bedrock, notamment¬†:\n\n\n  Etudier la possibilit√© d‚Äôajouter des React Server Components pour r√©duire le poids de notre JS\n  Mettre en place des tests de performance dans notre CI avec Lighthouse CI\n  V√©rifier que nos polices de caract√®res sont bien optimis√©es pour la performance\n\n\nNotre √©quipe est ressortie de cette conf√©rence ravie et avec de nouvelles id√©es qui vont surement nous suivre sur nos\nprojets personnels aussi. La We love speed ‚ù§Ô∏è est une conf√©rence √† ne pas manquer pour tous les passionn√©s de\nperformance web, on vous recommande chaudement d‚Äôy assister si vous en avez l‚Äôoccasion !\n\n"
} ,
  
  {
    "title"    : "Enhancing Production Monitoring with New Relic",
    "category" : "",
    "tags"     : " monitoring, production, newrelic, shared practice, alerting",
    "url"      : "/2024/10/03/enhancing-production-monitoring-with-newrelic.html",
    "date"     : "October 3, 2024",
    "excerpt"  : "At Bedrock, we develop a streaming solution tailored for European media companies. Our application is a customizable white-label product used by millions across Europe. Bedrock‚Äôs clients have high expectations for stability, early incident detecti...",
  "content"  : "At Bedrock, we develop a streaming solution tailored for European media companies. Our application is a customizable white-label product used by millions across Europe. Bedrock‚Äôs clients have high expectations for stability, early incident detection, quick resolution, and effective communication during those times. Naturally, our goal is to deliver a stable product, and we dedicate significant effort toward this objective.\n\nHowever, about two and a half years ago, our mobile team faced serious challenges in monitoring production. The tagging systems were pushing events into a tool that developers were not kin to use. The graphs were not intuitive and difficult to generate, making them hard to interpret. Each team across the company was managing its own monitoring practices. Monitoring was largely manual.\nToo many problems were brought to the attention of the team by a complaint of our client to the support team. We knew we had to make a change.\n\nSeveral organization-wide measures were implemented (like tools to streamline incident reporting and improved communication processes with our clients), but on the tech side, we also needed to play our part.\n\nSetting Our Goals and Expectations\n\nWe began by defining our goals:\n\n\n  Confidence in Releases: We needed certainty that what we shipped was functioning as expected.\n  Standardized Practices: It was crucial to align monitoring practices and tagging plans across teams.\n  Early Problem Detection: Our goal was to identify issues as early as possible.\n  Quick Data for Support Teams: We aimed to swiftly provide support with relevant metrics, such as the number of users impacted and the duration of incidents.\n  Developer Adoption: The selected tool had to be user-friendly, offering easy graph creation, visually appealing layouts, straightforward querying, and an ergonomic interface.\n  Automated Alerts: We needed a system that could automatically alert us of potential issues.\n  Configuration Portability: Regularly exporting our dashboards and alerts configuration was key, ensuring we could redeploy the setup if needed.\n\n\nBedrock chose New Relic. It‚Äôs important to note that this is not the only monitoring tool available, and we‚Äôre not claiming that others can‚Äôt meet our needs (we didn‚Äôt explore them deeply enough to say). What we do know is that New Relic has been widely adopted across our tech teams and has proven to be an effective solution to reach our requirements.\n\nOur Implementation Path\n\nHere‚Äôs the approach we took:\n\n\n  Shared Documentation: We created unified documentation for technical tagging plans across front-end teams, organized by domain and feature.\n  Common Dashboards: We developed shared dashboards for the Android and iOS teams, organized by domain (e.g., one for the Player, one for the Core, one for User Life Cycle, and one for iOS-specific features).\n  Automated Notifications: A Slack notification system was set up to alert teams whenever the documentation changes, ensuring everyone stays informed.\n\n\nAdvanced Queries with New Relic Insights\n\nNew Relic Insights, powered by NRQL (New Relic Query Language), allows developers to create powerful queries to extract specific information. Below, we illustrate how various NRQL queries translate into different types of graphs and charts on our dashboard:\n\nExample 1: Count Query with Line chart\n\n SELECT count(*) \n FROM Actions \n WHERE actionName = &#39;LoginAction&#39; \n FACET status, authenticationMethod \n TIMESERIES auto \n\n\nThis query counts the number of occurrences of the ‚ÄòLoginAction‚Äô event, faceted by the status and authenticationMethod fields. The TIMESERIES auto clause visualizes this count over time, showing trends or spikes in login activity.\n\nHow It Renders: The line chart displays the count of login actions over time, with different lines for each combination of status and authentication method (Google, Apple, Email). This allows us to track how login activity changes over time and to compare different types of logins or statuses.\n\n\n\nExample 2: Count Query with Pie chart\n\n SELECT count(*), WHERE status = &#39;error&#39; \n FROM Actions \n WHERE actionName = &#39;LoginAction&#39; \n FACET authenticationMethod, errorCode \n\n\nThis query counts the number of login actions that resulted in errors, faceting by authenticationMethod (Google, Apple, Email) and errorCode. It helps identify which authentication methods and error codes are most common.\n\nHow It Renders: The pie chart visualizes the distribution of login errors across different authentication methods and error codes. Each slice represents the proportion of errors associated with a particular method or error code, making it easy to see which are most problematic.\n\n\n\nExample 3: Percentage Query with Billboard chart\n\n SELECT 100 - percentage(count(*), WHERE status = &#39;error&#39;)  \n as `Successful login` \n FROM Actions \n WHERE actionName = &#39;LoginAction&#39; \n SINCE 2 hours ago \n COMPARE WITH 1 day ago  \n\n\nThis query calculates the percentage of successful login actions (i.e., those that did not result in errors) within the last two hours, compared to the same period one day ago. It helps track success rates and identify any improvements or declines in login performance.\n\nHow It Renders: The billboard chart prominently displays the percentage of successful logins as a large, bold number. The comparison with the previous day is also displayed, allowing for quick assessment of whether the success rate has improved or worsened.\n\n\n\nExample 4: Funnel Query\n\n   SELECT funnel(sessionId AS session, \n   WHERE name = &#39;EmailValidationStatus&#39; AND isVerified IS false AS &#39;email not verified&#39;, \n   WHERE name = &#39;EmailNotVerifiedPageOpe&#39; AS &#39;email validation page opened&#39;), \n   WHERE name = &#39;LayoutPageOpen‚Äô AS &#39;one layout event is called &#39;) \n   FROM Actions \n\n\nThis funnel query tracks user sessions through key steps in an email verification process. It identifies users with unverified emails and checks if they opened the email validation page. This helps monitor conversion rates and potential drop-offs in the user journey.\n\nHow It Renders: The funnel chart visualizes the sequence of user actions, showing the number of users at each step of the email verification process. This makes it easy to see where users drop off and which steps might need improvement.\n\n\n\nConfiguring Alerts with New Relic\n\nWhen setting up New Relic alerts, we used both static and anomaly thresholds based on the feature‚Äôs behaviour:\n\n\n  \n    Static Thresholds: Applied to features with predictable, consistent performance, where specific limits are well-defined.\nFor example, we used static thresholds for the Login/Register and Consent features, where behaviour is stable and deviations are easily identifiable.\n  \n  \n    Anomaly Detection: Used for features with dynamic patterns where a fixed threshold isn‚Äôt suitable. \nWe applied anomaly detection for example to the Payment feature since transaction patterns can vary significantly based on many factors.\n  \n\n\nThis combination allows us to accurately monitor both stable and variable features.\n\nAlert Policies\n\nAn alert policy determines who should be notified when an alert is triggered. It defines notification channels, user groups, and actions to take. To create an alert policy:\n\n\n  Go to ‚ÄúAlerts &amp;amp; AI‚Äù in your New Relic dashboard.\n  Create a new alert policy and define the actions to take when an alert is triggered.\n  Associate the policy with the channels or emails that should receive the alert notifications.\n\n\nAlert Conditions\n\nAlert conditions define the thresholds at which an alert should be triggered. To configure an alert condition:\n\n\n  In your New Relic dashboard, go to ‚ÄúAlerts &amp;amp; AI.‚Äù\n  Create a new alert condition by specifying criteria such as response time or error rate.\n  Define the thresholds and conditions necessary to trigger the alert. You can use anomaly detection or static thresholds depending on your feature needs.\n  Associate this condition with the specific alert policy you have defined.\n\n\nCode of alerts with Terraform\n\nHere‚Äôs a simple example of Terraform code to configure an alert condition that triggers when a spike in login errors (critical or warning) is detected:\n\n  resource &quot;newrelic_nrql_alert_condition&quot; &quot;authentication_login_error&quot; { \n  account_id                   = &quot;account_id&quot; \n  policy_id                    = &quot;policy_id&quot; \n  type                         = &quot;static&quot; \n  name                         = &quot;authentication_login_error&quot; \n  description                  = &quot;This alert fires when the % of login errors gets too high in a given time&quot; \n  enabled                      = true \n  violation_time_limit_seconds = 259200 \n\n  nrql { \n    query = &quot;SELECT percentage(count(*), WHERE status = &#39;error&#39;) * IF(count(*)&amp;lt;100, 0, 1) FROM Actions WHERE actionName = &#39;LoginAction&#39;&quot; \n  } \n\n  critical { \n    operator              = &quot;above&quot; \n    threshold             = 10 \n    threshold_duration    = 3600 \n    threshold_occurrences = &quot;all&quot; \n    \n  } \n\n  warning { \n    operator              = &quot;above&quot; \n    threshold             = 8 \n    threshold_duration    = 3600 \n    threshold_occurrences = &quot;all&quot; \n\n  } \n\n  fill_option        = &quot;none&quot; \n  aggregation_window = 3600 \n  aggregation_method = &quot;event_flow&quot; \n  aggregation_delay  = 600 \n} \n\n\nExplanation:\n\nThis Terraform script configures an NRQL static alert condition for monitoring login errors. The alert triggers if the percentage of login errors exceeds a threshold. The NRQL query calculates the error percentage and multiplies it by 0 if the event count is less than 100 (to avoid false positives from low activity). The critical condition triggers if the error rate is above 10% for at least 1 hour. The warning condition triggers if the error rate is above 8% for the same duration.  The script also sets up an aggregation window and delay to smooth out any short-lived spikes. Once this condition is met, an alert is sent to the Slack channel configured in the previous steps.\n\nBrighter Outlook with Enhanced Monitoring\n\nNow, with our new monitoring solution in place, we‚Äôre seeing a significant improvement in how we handle critical issues. Alerts are delivered instantly, allowing us to respond to problems as they arise. Even when we receive warnings, we can act proactively to prevent potential issues from escalating.\n\nThis immediate and precise alerting system ensures that our dedicated teams whether they handle backend issues or other areas are promptly informed and can address concerns before they impact our users. This proactive approach not only enhances our overall performance but also improves our operational efficiency.\n\nOverall, this solution has proven to be more effective and practical for our project, enabling us to maintain higher levels of stability and deliver a better experience for our clients. We‚Äôre confident that this improvement will continue to support our success and growth moving forward.\n\nReference:\nSteal this dashboard\n\n"
} ,
  
  {
    "title"    : "How to manage hundreds of view templates?",
    "category" : "",
    "tags"     : " android, versioning, design, atomic design, design system",
    "url"      : "/2024/08/22/how-to-manage-hundreds-of-templates.html",
    "date"     : "August 22, 2024",
    "excerpt"  : "Let‚Äôs dive together into the depths of a large scale template versioning for a white label streaming application.\n\nThe context\n\nToday, at Bedrock, we are providing a streaming service on:\n\n  11 common CTV platforms: Tizen, Philips, WebOS, Hisense,...",
  "content"  : "Let‚Äôs dive together into the depths of a large scale template versioning for a white label streaming application.\n\nThe context\n\nToday, at Bedrock, we are providing a streaming service on:\n\n  11 common CTV platforms: Tizen, Philips, WebOS, Hisense, Panasonic, Orange, Xbox, PS4, Orsay, Smart Alliance, WhaleOS\n  6 Android platforms: mobile, AndroidTV, Bytel, Free, SFR, FireTV\n  2 Apple platforms: iOS / tvOS\n  1 Web platform\n  a lot of specific devices\n\n\nEach of these platforms has their own guidelines and usage (interaction, screen sizes, etc) and is managed by a dedicated team in our organization. They also have their own native design components which our implementations rely on to offer the best possible experience.\n\nBedrock has several clients (M6+, Videoland and others), that all have their dedicated application on each platform. But they also have their own design expectations and branding guidelines.\nWe provide them with a design system: a set of visual assets they can use to render pages. These components follow the atomic design model, which means that each layer is a composition of the previous one. Here is the list of the layers with their name and some items that belong to it:\n\n  14 tokens: base element that defines the brand (ex: Colors, FontSystem, Radius, Shadow, Breakpoints, Treatments‚Ä¶)\n  9 atoms: simplest design item that cannot be decomposed (ex: Icons, Avatar, CheckBox, ProgressBar, ServiceIcon, Separator‚Ä¶)\n  62 molecules: group of atoms that forms a visual unity (ex: Card, Poster, HorizontalCard, Button, CTA, Portrait, Totem‚Ä¶)\n  12 organisms: complex visual item or part of the interface (ex: Jumbotron, Hero, Solo, Banner‚Ä¶)\n\n\nHereunder are some sample screens composed of design system components for different customers:\n\n\n  \n    \n      \n      \n      \n    \n  \n\n\nThis results in approximately one hundred design elements per client. However, not all components are always specific, and an inheritance system is in place to allow clients to reuse default atoms, molecules, or organisms while applying the tokens corresponding to their brand.\n\nThe objectives and expectations regarding the design vary greatly among clients, resulting in numerous graphical element evolutions. Managing the creation and progression of these elements through different processes is a major challenge in tracking designs across each platform. These demands sometimes lead to different integrations. Whether or not the default design is inherited is crucial information to avoid manually comparing the app designs with documentation, which can lead to misunderstandings and wasted time during validation and approval processes.\nTo achieve all these objectives, we need to be able to carefully follow the evolution of the design system on each plateform.\n\nOne versioning to rule them all\n\nTherefore, implementing a versioning system became essential to continue enriching and evolving our client‚Äôs design systems.\nTo ensure its interest and effectiveness, this system had to address several issues:\n\n  Be specific per platform and per client: evolution progress does not occur at the same pace across all platforms. This also allows for comparing design progress between platforms.\n  Allow visualizing the inheritance between the default design system and the client‚Äôs specific part on the same platform. We also wanted the ability to add comments on implementation details.\n  Be directly accessible with each build to stay in touch with the application it represents. On mobile, many builds are generated daily, making it difficult to track the arrival of new features.\n  Stay up-to-date with the constant evolutions of the design system to maintain the source of truth.\n\n\nExemple of a Design System release note:\n\n\n\nInitially, the design team started by versioning its releases and all available components, which are accessible to technical teams in our online design documentation (hosted on zeroheight). This versioning is common to all platforms and all clients if there are no specificities for the component in question. Ultimately, versioning helps product and technical teams track the delivery progress of new designs. The version number follows these rules:\n\n\n  \n    \n      Code status\n      Stage\n      Rule\n      Example\n    \n  \n  \n    \n      First release\n      New collection\n      1.0.0\n      1.0.0\n    \n    \n      Documentation update without impact on the anatomy of design element\n      Documentation update\n      Increment the third digit\n      1.0.1\n    \n    \n      New backward-compatible feature\n      Minor release\n      Increment the 2nd digit and reset the third\n      1.1.0\n    \n    \n      Change on element that breaks backward compatibility\n      Major release\n      Increment the first digit and reset the second and the third\n      2.0.0\n    \n  \n\n\nEach frontend team were then responsible for implementing an equivalent versioning system representing the state of their platform. This was done through a feature team including all front-end teams, design system managers, and the product team. This organization brought numerous benefits:\n\n  Facilitating the synchronization and homogeneity of product templates\n  Sharing development challenges\n  Limiting versioning differences between all fronts\n  Sharing industrialization ideas\n\n\nCase study: Android versioning challenges\n\nFor Android, we sometimes have different component implementations between mobile and TV, evolving at different paces. This required two separate versionings to represent them. Moreover, we currently use comments to track the migration to Google‚Äôs new view system (Compose), which is happening alongside graphical evolutions.\nEach client has its versioning file containing all the components available on the targeted platform. Regarding component inheritance from the default design system, there are two possibilities:\n\n  The component is inherited and thus has no specificities and no version in the client‚Äôs versioning file, using the generic Bedrock customer version\n  It has its own implementation and its associated version\n\n\n\n  \n    \n      Default Versioning file\n      Customer Versioning file\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nFrom these versioning files, we have been able to generate reports for each platform containing only the available graphical elements and their version, whether inherited or not. To track their growing number, we opted for automatic generation of these reports with each build using a Gradle Plugin integrated on our continuous deployment and integration (CI/CD): Bitrise. Now, they are generated using a Mkdocs plugin to be published online automatically with our technical documentation.\n\nAutomatically generated template versioning report for M6+:\n\n\n\nConclusion\n\nDesign system versioning reports offer a quick and exhaustive overview of the design system at any given moment for any of our customers and simplifies the QA teams validation work. It is now an essential tool for tracking design progress across all supported platforms.\n\nHowever, improvements are always possible:\n\n  Currently, developers are responsible for updating the versioning file when adding or modifying graphical elements, which can lead to errors and omissions. To avoid this, we would like to link the concrete implementation of the component and its version in the versioning file. But as we are still using the android legacy view system, component can be a class but also some xml style or theme or even resource files. This should be easier after the migration to the new Android view system Compose.\n  Access through Bitrise is not ideal. We currently centralize all generated builds (using CI as well) in Airtable. A link would be preferable and more visible.\n\n\nThese last improvements will perhaps be the subject of a follow-up to this article. Thank you for reading, we hope you found some useful information that can help you to better follow the graphical evolution of your project. Do not hesitate to contact us if you have some questions or suggestions.\n"
} ,
  
  {
    "title"    : "JS Nation x React Summit 2024 highlights",
    "category" : "",
    "tags"     : " conference, frontend, react, javascript",
    "url"      : "/2024/08/20/js-nation-react-summit-2024.html",
    "date"     : "August 20, 2024",
    "excerpt"  : "The Frontend Bedrock teams were present at the 2024 edition of the JS Nation and React Summit conferences in Amsterdam on 13 &amp;amp; 14 June. We would like to highlight and share some of the subjects we find relevant about our favourite language and...",
  "content"  : "The Frontend Bedrock teams were present at the 2024 edition of the JS Nation and React Summit conferences in Amsterdam on 13 &amp;amp; 14 June. We would like to highlight and share some of the subjects we find relevant about our favourite language and framework üôÇ Feel free to explore them further if you are interested.\n\n\n  üìπ You can watch all the video recordings of the JS Nation and the React Summit talks.\n\n\nLessons for Building Resilient Codebases - Alex Moldovan\n\n\n  üìπ Watch the video\n\n\nOne of my favourite types of presentation, with practical things that you can then apply in your daily life as a developer.\n\nAlex sums up his speech in 5 points:\n\n\n  Accept imperfections in your codebase: perfect is the enemy of good, shortcuts and exceptions can be taken if they are well documented.\n  Colocating code according to concerns is a way of improving readability and therefore maintainability and efficiency.\n  Reusability is not always the better choice vs duplication, you should find the right balance, abstraction should only come when a piece of code is proven to be a long term solution.\n  Readability and understandability of the code is very important for the future developers that will maintain your code: leave comments, break conditions with more than 3 members using intermediate variables, etc.\n  Use Typescript intensively and try to handle all the possible states of your data.\n\n\nAt Bedrock, we‚Äôve always tried to apply a maximum of best practices including some addressed here by Alex, to ensure the maintainability of our web codebase. It seems to be working pretty good since this JS project is now 10 years old!\n\nWhy Your Performance Work Is Not Seen - Vinicius Dallacqua\n\n\n  üìπ Watch the video\n\n\nVinicius addresses an important subject for us, as frontend engineers. We are all concerned about performance and we want to deliver the most optimized product, but we often come up against major obstacles in this area. We already talked about this topic last year when we attended the We Love Speed conference.\n\nTo demonstrate the worth of working on performances, Lab tools (like Lighthouse) &amp;amp; RUM (Real User Monitoring) have to be used together. Collected data must be analyzed, optimized and framed to make connections with product concerns. Performance needs to matter to engineering and product for a sustainable governance model.\n\nAs in many areas, the key is to set realistic and achievable objectives and to proceed by iteration: monitor, measure, report and repeat.\n\nInstall Nothing: App UIs With Native Browser APIs - Scott Tolinski\n\n\n  üìπ Watch the video\n\n\nScott reveals that we can use the browser‚Äôs native APIs to build certain UI components easily, whereas we‚Äôve sometimes been using complex JS to do this for years.\n\nHe shows for example:\n\n\n  How to build modals with the &amp;lt;dialog&amp;gt; element\n  How to easily create animated transitions with View Transition API or with @starting-style\n  How to create a menu with the &amp;lt;popover&amp;gt; element\n  How to build slideshows with scroll-snap CSS properties\n\n\nCheck all these tips out and much more besides on Scott‚Äôs site!\n\nTesting: do more with less - Eugene Fidelin\n\n\n  üìπ Watch the video\n\n\nTesting has been a guide and a real challenge since we began the JS web project at Bedrock in 2015. We are convinced that this is an important part of the sustainability and reliability of the codebase. As a result, we listened to Eugene‚Äôs point of view with curiosity.\n\nDORA metrics should be adopted to measure how good your are for shipping software with 4 indicators:\n\n\n  Deployment frequency\n  Lead time for changes\n  Change failure rate\n  Time to restore service\n\n\nThe first three are affected by the testing strategy. The Testing Trophy can help to write the right tests given the return on investment of the different forms.\n\n\n\nEugene describes the various steps to build a reliable testing strategy:\n\n\n  step 0: enable static linters and type checks\n  step 1: create integration tests for all happy and non-happy flows\n  step 2: write unit tests for reusable parts and code without coverage\n  step 3: write very few e2e tests only for the most business critical flows\n  step 4: use metrics, tracing, logging to identify anomalies on production\n\n\nIn Bedrock‚Äôs frontend teams, we are aligned with these strategy:\n\n\n  Static testing is a compulsory stage in any project start-up.\n  We have a lot of unit and integration tests (which can be viewed as e2e tests, but we only test the frontend part with mocked backend and 3rd parties).\n  A manual homologation phase before deploying any new release run e2e tests: this is a current project to automate most of these tests to save time in the process.\n  Production monitoring is one of our mantras: our former boss used to say that ‚Äúa non-monitored project is not in production‚Äù.\n\n\nA tweet from Guillermo Rauch can perfectly conclude this topic:\n\n\n  Write tests. Not too many. Mostly integration.\n\n\nFacing Frontend‚Äôs Existencial Crisis - Ryan Carniato\n\n\n  üìπ Watch the video\n\n\nRyan shows why JS frameworks, especially React, have recently evolved using new concepts to reduce the runtime at page load.\n\nThe size of pages has been increasing over the years. In particular because SPAs have gained ground in the web world. SPAs are costly because of:\n\n\n  code execution and evaluation,\n  code bundle size,\n  payload (HTML &amp;amp; data) size.\n\n\nAt the same time, devices have improved their performance, but not all of them. Connectivity can also be a limiting factor, depending on the context of use. As a result, we are faced with a real fact: an app with client-side rendering is slower to display and interact than an app with server-side rendering.\n\nIn order to reduce each of these three costly aspects, Ryan mentions three concepts that new versions of recent JS frameworks, such as React 19, are developing:\n\n\n  islands: splitting the page in several blocks, and render those which never change on server-side only,\n  server components: close to the island concept but with the ability to handle state persistence over the client navigation,\n  resumability: the ability to defer the hydratation of components until an interaction is requested.\n\n\nIf you want more details about concepts developed by Ryan, you can read his article on this subject.\n\nInvisible Hand of React Performance - Ivan Akulov\n\n\n  üìπ Watch the video\n\n\nLike Charlotte Isambert‚Äôs explanation of how Suspense works, this talk takes us into the inner workings of React. It is often interesting to understand how the library you are using works under the hood so that you can use it properly.\n\nBy comparing behaviors between the latest React versions, Ivan explains:\n\n\n  How useEffect has optimized the browser layout and paint calculation flow comparing to componentDidMount\n  How setState batching have been optimized in React 18\n  Why the freeze experience if the user interacts during the hydratation phase is much better by using Suspense.\n\n\nWhy You Should Use Redux in 2024 - Mark Erikson\n\n\n  üìπ Watch the video\n\n\nAt Bedrock, we have been using Redux since 2016, and we continue to use it. So Mark‚Äôs arguments will certainly confirm our choice!\n\nMark is the creator of Redux Toolkit. He begins by detailing the pros and cons of using Redux in relation to various aspects:\n\n\n  Flux-style indirection via dispatching actions instead of direct state mutations\n  A single global centralized store\n  State updates via reducers and slices\n\n\nThen Mark presents the arguments why he thinks using Redux in 2024 is a good choice:\n\n\n  Provides a consistent architecture pattern for apps\n  Better understanding of what‚Äôs happening in the app\n  Widely used\n  Well documented\n  Better update behavior than React Context\n  Redux Toolkit provides built-in tools standard use cases\n  RTK Query data fetching and caching layer\n  Works great with Typescript\n  Designed to work with React but still UI-agnostic\n\n\nSince 2016 we have been trying at Bedrock to streamline the way we use Redux in JS projects by applying some best practices.\nWe also have been adopting Redux Toolkit for some time with this in mind. And we are always open to new ways of managing state in apps. For example, we are currently testing XState to manage the state of our player.\n\nCase Study: Building Accessible Reusable React Components at GitHub - Siddharth Kshetrapal\n\n\n  üìπ Watch the video\n\n\nThe talk by Sid focuses on building accessible React components, and emphasises on the importance of using the correct HTML elements and ARIA roles to enhance web accessibility.\n\nSid highlights specific examples, such as navigating tab lists and handling conditional checkboxes, explaining how ARIA roles and properties can make web interfaces more accessible, especially for screen reader users.\n\nWhat we remember from this talk:\n\n\n  Importance of using correct HTML elements and ARIA roles for accessibility\n  Implementation of ARIA roles like tablist, tab, and aria-selected\n  Significance of keyboard navigation and ARIA states/properties in accessibility\n  Design considerations for accessibility, including using aria-disabled over disabled\n\n\nWhat‚Äôs next for us\n\nWe came away from these conferences with the feeling that we had adopted the right approach in the past few years for our JS projects, whether in terms of architecture, maintainability or testing strategy. However, we saw that we shouldn‚Äôt miss the next steps regarding accessibility (an improvement project is underway for the end of year) and performances, especially taking into account the new way of rendering server-side with React 19 (while we pioneered the use of React with SSR in 2014). We will be sure to keep you informed of our progress!\n"
} ,
  
  {
    "title"    : "How Special Event Page allowed us to handle more than 1 million of users",
    "category" : "",
    "tags"     : " resilience, frontend, back-for-front, 2024",
    "url"      : "/2024/08/07/euro-resiliency-special-event-page.html",
    "date"     : "August 7, 2024",
    "excerpt"  : "Earlier this year took place the Euro soccer competition, spanning over a month and with thousands of people tuning all over the world to watch the matches. One of our customers, M6+, was streaming several of these games - And during the competiti...",
  "content"  : "Earlier this year took place the Euro soccer competition, spanning over a month and with thousands of people tuning all over the world to watch the matches. One of our customers, M6+, was streaming several of these games - And during the competition, hundreds of thousands of browsers, phones and TVs were able to seamlessly stream the matches with no major issue at all. It was, however, no easy feat to reach that state. How did we do that? What were the challenges we faced, the solutions we envisioned? In this article, we‚Äôll discuss one of the features we developed specifically for the Euro: something that we called the ‚ÄúSpecial Event Page‚Äù.\n\nThe need behind the Special Event Page\n\nThis feature was actually first developed back in 2021, for a similar reason: yet another Euro soccer tournament! Back then, it was a very simple feature: display a page to the user, once per session and prior to any kind of backend call, that would prompt them to go to the football. Here‚Äôs what it looked like:\n\n\n\nIt was a very simple page (no background, only the two teams and two buttons were displayed), which was only developed for the Web clients, but it served its purpose.\nFor every soccer evening, we would handle far more traffic than what we usually handled. If each one of these users would load a personalized homepage, search for a program, navigate around the app and do all kind of other actions, then it wouldn‚Äôt take long for our backend servers to explode under the pressure, especially as most users who want to watch a football match arrive ‚Äúat the same time‚Äù, over a few minutes before it starts, which makes it difficult for our servers to scale accordingly. Each user that saw that Special Event page and clicked on the Live button was one that was immediately directed to their content, without any kind of backend call. It was a very simple solution, it worked very well, felt great for our users, and it allowed us to alleviate a lot of the pressure our backend APIs would have otherwise faced.\n\nAt the same time, the Special Event page was very useful for our end users. Enabled a few minutes before the match, and staying available for the entire duration, it allowed any new user that was arriving to the platform to quickly be able to reach the corresponding live, without spending time searching for it beforehand, directly reducing friction time.\n\nHowever, this was far from a perfect solution, as it presented a few issues that we needed to fix in order to be ready for the Euro 2024:\n\n  This page was entirely managed by the Web team. Our customer did not have any input on it, if they wanted to make a change they‚Äôd have to request it, and we‚Äôd then have to integrate it in our web application, and then deploy it, preventing any last-minute modifications. We needed a process that could be documented, followed, and would allow lightning-fast modifications to the Special Event page.\n  As it was only managed by the Web team, the traffic generated by phones and TVs did not benefit from this feature. This was a major issue, as these devices could prove to be a lot of traffic. For example, usage of mobile devices suddenly spiked at every goal, with a lot of users in the street taking out their phone to check out what happened when they heard screams around them. We needed a new solution that could impact all platforms, not just the Web.\n  It was overall not pretty üòÖ We ourselves weren‚Äôt really huge fans of the old page, we wanted a new one that‚Äôd be more ‚ú® shiny ‚ú®\n\n\nWith all of these in mind, in preparation for the Euro of this year, we began to create what would be the new Special Event Page.\n\nAttack plan\n\nOne of our first goals was to make sure that most platforms (with a target goal of more than 90% of them) would get access to that feature. If we wanted to be able to sustain as much traffic as possible without hiccup, we‚Äôd have to reach as many users as possible in one shot, and not spend time working on device-specific implementations. For this reason, we decided to opt for a backend-first strategy: ideally the work on different fronts would be minimal, and our backend would be responsible to handle and answer the Special Event page whenever it is needed.\n\nAfter consideration, we decided to go for a Cookie-based solution. When one of our fronts would request the homepage, for example, the request would first be caught by our CDN. It‚Äôd check for the existence of a given cookie in that request. If the cookie was present, then the user already saw the special event page and we‚Äôd let the request go through. If the cookie was not present, then we‚Äôd return the Special Event page to the user, along with a new Set-Cookie header that would save that cookie to the user‚Äôs device, preventing them from seeing the Special Event page again for a given duration.\nThe Special event page content would be a static, non-personalized version of a page that resembled one our backend API could return for the homepage, hosted in the cloud. We‚Äôd then be able to edit that page on the fly without bothering the frontend teams.\n\n\n\nWith this solution, we also reaped another benefit: phone apps. When a new version of the application is released, not everyone always updates theirs. We could require an update, but it‚Äôs not really a good experience for a user to be prompted to update their app ever-so-often (and it can translate into a slight portion of our audience choosing to uninstall the app instead), so that‚Äôs a mean we wish to avoid as much as possible. With the solution described above, managing everything using nothing but cookies and http headers meant that not a single line of code needed to be written by our frontend teams, so aside from one small change to enable cookie storage (That we could plan for long in advance), no new version was needed!\n\nHowever, it wasn‚Äôt perfect either.\n\nWeb challenges : Cookies and SSR\n\nThe first issue we encountered, we probably should‚Äôve guessed earlier that it would happen: cross-domain cookies. While cookies work wonders when they are set by the same domain that‚Äôs using them, here, our cookies were set on 6play.fr by the domain 6cloud.fr. At first, when we tested locally, this wasn‚Äôt an issue.\n\nHowever, nowadays, as a mean to ensure users privacy, most browsers block these kind of cookies to prevent cross-domain tracking. While our cookies weren‚Äôt trying to gather anything from our users, they were blocked nonetheless! With no existing way of indicating that this was a functional cookie (as it‚Äôd otherwise probably be abused by these exact tracking tools that browsers aim to block üòÖ), we had to find another solution specifically for the Web. We ended up having to code a second logic on the Web code directly, detecting whenever a Special Event page was displayed and setting a cookie from the website itself, to prevent this third-party cookie blocking. Even though this required a bit more code, this ended up working like a charm for the web.\n\nThe second issue we encountered was with Server-Side Rendering, that the Web (again!) use plenty (we talked about this a while ago, right here!). When a page is requested, the SSR first renders the page on the server, and serves it to the client: it allows a user to see an immediate result, while their own device is processing the page. That result is also served to web crawlers such as Google SEO robots! However, in our architecture, SSR responses are not user-specific, and we can‚Äôt detect cookies of each user on the server side. As a result, the server would always respond the Special Event Page, and every user would see it for a split second before the client-side code takes over again. Moreover, it‚Äôd completely break our SEO! To prevent that issue, we chose to whitelist the IP addresses of our servers, in order to specify to our CDN not to send it the Special Event page under any circumstances.\n\nFinal result\n\nWith these small issues now behind us, the Special Event was ready to be released. We prepared the static files, configured the last details for the cookie duration, and voil√†!\n\n\n\nAs soon as the feature came live, we could instantly see the impact it had on our traffic, with the pressure on our servers diminishing drastically.\n\n\n\n\nWhile the Special Event page was a success, enabling our users to reach the soccer matches with nothing more than one single click, while at the same time allowing us to reduce a lot of the incoming traffic on our backend servers, it was not the only feature that we developed to prepare for such a massive event. We also worked on quite a few topics, such as the layout at edge, or a huge workshop on a way to load-test efficiently our services. We will communicate about these at some point in the future, so keep an eye out for this!\n\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #20",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2024/07/26/bedrock-dev-facts-20.html",
    "date"     : "July 26, 2024",
    "excerpt"  : "Bien que cela fasse pr√®s de 18 mois que nous n‚Äôavons pas publi√© une nouvelle fourn√©e de Dev Facts, nos √©quipes ont continu√©s √† alimenter la pile des perles des d√©veloppeurs.\nEt du coup, double fourn√©e !!\n\nTester, c‚Äôest douter\n\n  \n    A : Attention...",
  "content"  : "Bien que cela fasse pr√®s de 18 mois que nous n‚Äôavons pas publi√© une nouvelle fourn√©e de Dev Facts, nos √©quipes ont continu√©s √† alimenter la pile des perles des d√©veloppeurs.\nEt du coup, double fourn√©e !!\n\nTester, c‚Äôest douter\n\n  \n    A : Attention, je vais peut-√™tre (je l‚Äôesp√®re) faire sonner [la prod]\n    B : A, toujours l√† pour utiliser la prod comme son bac √† sable\n  \n\n\nJ‚Äôai appris √† lire, je ne souhaite √ßa √† personne\n\n  Nan mais en plus je sais m√™me pas ce qu‚Äôil faut faire ! Le ticket il est l√†, y a plein de mots !\n\n\nProfessionnel jusqu‚Äô√† l‚Äôassiette\n\n  \n    A : Mais moi quand je commande ma salade moulin, je fais gaffe a prendre des trucs qui vont ensembles ! Genre, chou rouge et betterave, c‚Äôest des couleurs compl√©mentaires !\n    B : Ah bah tout de suite on voit le dev qui se d√©brouille en CSS ici\n  \n\n\nLa mutualisation\n\n  c‚Äôest l‚Äôavantage d‚Äôavoir une solution core, tu retrouveras s√ªrement les m√™mes bugs chez les autres clients\n\n\nLes doigts devraient suffir\n\n  C‚Äôest le moment o√π je croise les fesses\n\n\nLe d√©bat, mode d‚Äôemploi\n\n  \n    A : en fait c‚Äô√©tait vachement mieux\n    B : ben non\n    A : ¬Ø\\(„ÉÑ)/¬Ø\n    C :  j‚Äôaime les d√©bats argument√©s ü§£\n  \n\n\nQuand les admins d√©couvrent la vie r√©elle\n\n  Super faut respecter les r√®gles quoi\n\n\nQuand il faut √©teindre un client\n\n  J‚Äôai l‚Äôimpression de supprimer mon skyblog\n\n\nC‚Äôest un peu plus moins bien\n\n  C‚Äôest cute dans le c√¥t√© d√©gueulasse\n\n\nLe t√©l√©travail v2\n\n  \n    A : Il fait plus chaud dans les couloirs que dans les bureaux.\n    B : On a qu‚Äô√† d√©placer les bureaux dans les couloirs. On fera du t√©l√©pr√©sentiel\n  \n\n\nL‚Äôentrainement, c‚Äôest important\n\n  Mon temps de r√©action en r√©union n‚Äôest plus tr√®s bon depuis que j‚Äôai arr√™t√© de jouer a StarCraft.\n\n\nUn d√©veloppeur, un vrai, avec pleins de doigts\n\n  Je fais pas un copier-coller, je retape doigt par doigt\n\n\nL‚Äô√©chelle sociale en entreprise\n\n  Un alternant : Je peux avoir un stagiaire moi aussi ?\n\n\nQuand le respect s‚Äôest enfui\n\n  Vas-y r√©p√®te, je n‚Äô√©coutais pas\n\n\nOu alors, c‚Äôest le 9√®me jour de la semaine\n\n  C‚Äôest le 5 le dernier vendredi du mois ?\n\n\nQuand √ßa √©choue vraiment\n\n  Au lieu d‚Äô√©chouer avec succ√®s √ßa √©choue avec √©chec\n\n\nPas vu, pas pris\n\n  C‚Äôest une feature potentiellement pas impactante tant que tu ne l‚Äôutilises pas\n\n\nC‚Äôest pas faux\n\n  Ca sert √† rien d‚Äôavoir un truc qui sert √† rien\n\n\nL‚Äôing√©niosit√©\n\n  Ah ? Ma molette est en panne ‚Ä¶. tant pis je vais scroller au clic\n\n\nC‚Äôest pas faux, bis\n\n  C‚Äôest cool les choses bien\n\n\nIls ne savaient pas que c‚Äô√©tait impossible, alors ils l‚Äôont fait\n\n  On est oblig√©s de pouvoir puisque c‚Äôest la seule solution\n\n\nIl n‚Äôa visiblement pas eu son stagiaire\n\n  Un alternant: Je vois pas mon code si je pleure\n\n\nVous faites quoi mercredi soir ?\n\n  \n    J‚Äôai une nouvelle passion pur√©e de piments qui m‚Äôa bien occup√©e durant le dernier week-end\n  \n\n  J‚Äôadore ce genre de phrases. Je suis 100% susceptible d‚Äôen faire sur plein de sujets cuisine. Mais √† chaque fois je ne peux m‚Äôemp√©cher de penser qu‚Äôon pourrait √™tre invit√© √† un diner de cons avec ce genre de phrase üòÇ\n\n\nDe la sagesse, il faut faire preuve\n\n  La vie est un breaking-change\n\n\nLe positivisme\n\n  ‚ÄúNon‚Äù, c‚Äôest pas n√©gatif\n\n\nA la pointe de la technologie\n\n  On a commenc√© les travaux de refonte auj üòÑ. (injection de dependance, puis on enchaine sur l‚Äôutilisation des ‚Äúnouvelles‚Äù api apple (qui datent d‚Äôil y a 4 ans d√©j√† üòÑ))\n\n\nL‚Äôinflation, c‚Äôest compliqu√©\n\n  Ouais y a l‚Äôinflation, c‚Äôest bien pour √ßa que tu me gonfles ?\n\n\nLa quadrature du cercle new gen\n\n  Je suis entrain de tourner en rond dans un triangle\n\n\nBut why ?\n\n  Parce que parler lascivement a une chaise, j‚Äôai jamais tent√©\n\n\nExemple tr√®s parlant\n\n  C‚Äôest √©crit plus petit que derri√®re les paquets de c√©r√©ales\n\n\nIl fait aussi du th√© ?\n\n  Je n‚Äôai pas la science infuse, je suis juste un connard qui fait des commentaires.\n\n\nC‚Äôest pas faux, ter\n\n  Alors c‚Äôest moche, parce qu‚Äôen vrai, c‚Äôest pas tr√®s beau\n\n\nJe ne dirais pas que √ßa a √©chou√©, je dirais que √ßa n‚Äôa pas march√©\n\n  C‚Äôest pas du DDOS, c‚Äôest un stress test pas vraiment sous contr√¥le\n\n\nThe yes need the no, to win, against the no\n\n  Ca peut √™tre long √† √©crire tr√®s vite\n\n\nAvoir une passion, c‚Äôest important\n\n  Casser les couilles, c‚Äôest ma passion\n\n\nLe trou noir de la productivit√©\n\n  ‚ÄòTain je sais pas si c‚Äôest votre ilot, mais j‚Äôai vraiment l‚Äôimpression de rien foutre ici\n\n\nUn chef, c‚Äôest fait pour cheffer\n\n  \n    A : C‚Äôest marrant mais on a jamais fait de pair ensemble !\n    B : En m√™me temps, je dev pas‚Ä¶ et toi non plus‚Ä¶\n  \n\n\nL‚Äôagenda, c‚Äôest so 2010\n\n  \n    Un lead √† un membre (appelons le T) de son √©quipe : Bon, tu nous rejoins ?\n    T : ah on a r√©union ?\n    Le lead : ben on est en retro surtout l√†\n  \n\n\nTout d√©pend de la zone\n\n  \n    A : Tout depend de la policy\n    B : et pas de la gendarmerie ?\n  \n\n\nRTFM\n\n  Putain lire la doc √ßa sert !\n\n\nPour un peu qu‚Äôil soit cach√©, on est foutu\n\n  le jwt actuel expire le  26/11/5189 20:26:26. (j‚Äôesp√®re que √ßa suffira)\n\n\nOn m‚Äôa dit de regarder s‚Äôil y a des erreurs, pas s‚Äôil marche\n\n  \n    A : Dites, quelqu‚Äôun arrive a acc√©der a notre outil de monitoring ?\n    B : Non, mais je crois que Vendredi dernier il √©tait d√©j√† cass√©\n    C : ‚Ä¶ Mais tu as mis RAS sur ton rapport de Monitoring ?\n  \n\n\nC‚Äôest pour engager un engagement\n\n  Il faut qu‚Äôon sache ce qu‚Äôil y a contractuellement dans le contrat‚Ä¶\n\n\nApr√®s le no code, le code √† emporter\n\n  \n    A :  On se fait un point ‚Äúplan d‚Äôactions‚Äù si tu veux.\n    B : Pourquoi pas maintenant ! Tu es sur place ?\n    A : Non, √† emporter !\n  \n\n\nIl n‚Äôa pas l‚Äôhabitude\n\n  Oulaaa, mon PC il aime pas quand je travaille\n\n\nC‚Äôest pas faux, quarter\n\n  Dell c‚Äôest le Apple de Windows\n\n\nEt tout √ßa, pour pas cher\n\n  Avec rien, tu ne fais pas grand chose\n\n\nDu coup, √ßa ne fait pas grand chose ?\n\n  Ca allait bien jusqu‚Äô√† ce qu‚Äôon ne fasse rien\n\n\nQuand tu d√©veloppes √† base de TNT\n\n  L√† on pourrait tout casser sans rien faire marcher\n\n\nSudo make me a sandwich\n\n  \n    A : Ma commande marche pas\n    B : Ca me dit un truc, je regarde sur StackOverflow‚Ä¶ Ah oui tient le lien vers le site est violet\n    A : Et du coup ?\n    B : Rajoute un ‚Äìforce.\n  \n\n\nEt si on utilise des carottes jaunes ?\n\n  J‚Äôai n√©anmoins le sentiment que le client compare des carottes et des patates. Par hasard au d√©but les carottes et les patates (douces) avaient la m√™me couleur, et que maintenant qu‚Äôon a des vraies patates, ca marche plus\n\n\nOptimisme, d√©finition\n\n  Pour le burndown, si on enl√®ve tous les jours ou √ßa stagne, √ßa descend\n\n\nSi, mais tu les comprends plus tard, quand tu n‚Äôen as plus besoin\n\n  De toutes fa√ßon les blagues de front on ne peux pas les comprendre, elles sont asynchrone\n\n\nEn plus, il faut lire des choses\n\n  \n    A : J‚Äôai r√©solu notre probl√®me\n    B : Ah cool, c‚Äô√©tait du coup ?\n    A : C‚Äô√©tait une mauvaise lecture de la doc\n    B : Ouais mais leurs docs elles sont trop longues !\n  \n\n\nLe legacy de demain, qui a √©t√© fait hier, mais vu d‚Äôaujourd‚Äôhui\n\n  C‚Äôest un legacy futur\n\n\n√áa nous est tous arriv√©\n\n  Putain, je viens de faire un commit que j‚Äôai appel√© --amend\n\n\nLe pianiste\n\n  C‚Äôest mes doigts qui connaissent mon mot de passe, c‚Äôest pas moi\n\n\n√áa explique des choses ‚Ä¶\n\n  Hier j‚Äôai utilis√© mes comp√©tences de dev front, j‚Äôai fait un g√¢teau au chocolat\n\n\nC‚Äôest presque √ßa\n\n  J‚Äôveux pas faire des features sur la com√®te\n\n\nDu coup, on passe la prod en staging ?\n\n  La validation a conclut que √ßa marchait en prod mais pas en staging\n\n\nEn signal de fum√©e ?\n\n  Je sais pas comment je vais dev, ce qui est branch√© a mon √©cran c‚Äôest une cigarette √©lectronique\n\n\nC‚Äô√©tait au tour du chien de surveiller l‚Äôalerting ?\n\n  \n    A : C‚Äôest quoi qui sonne comme √ßa ?\n    B : C‚Äôest le collier de mon chien quand il bouge la t√™te\n    A : Je voulais dire dans notre alerting\n  \n\n\nMerci\n\n  \n    A : On sait faire un test atoum\n    B : √Ä tes souhaits\n  \n\n\nIl √©choue rapidement peut-√™tre ?\n\n  Putain le code est tellement optimis√© qu‚Äôil marche plus\n\n\nMais comme personne ne lit la doc ‚Ä¶\n\n  C‚Äôest pas de la magie, c‚Äôest document√©\n\n\nUne histoire de mar(s|c)h(e)\n\n  Si on Unmarshal puis Marshal, √ßa doit marcher\n\n\n4G ou 5G ?\n\n  Avez-vous un cable Wi-fi √† me pr√™ter ?\n\n\nNous avons affaire √† un serial killer\n\n  \n    A : He is on AirTiTi (RTT)\n    B : Quoi ?\n    A : En RTT\n    B : Oh, on AirTiTi\n  \n\n\nYou have to be curious\n\n  On a la salle bi curieux\n\n\nOn a dit que le chef est l√† pour cheffer\n\n  J‚Äôaimerais bien un assistant, genre un team lead\n\n\nC‚Äôest beau ou beau ?\n\n  Je t‚Äôai demand√© de constater √† quel point c‚Äôest beau, pas de me dire que c‚Äôest moche\n\n\nQuand tu vas trop loin dans l‚Äôintimit√© des gens\n\n  Sachez que Mark Zuckerberg transpire beaucoup.\n\n\nLes solutions les plus simples\n\n  Mon script qui sort mon PC de veille c‚Äôest mon chien qui passe vers le PC‚ÄØen bougeant la souris\n\n\nEncore quelqu‚Äôun qui n‚Äôa pas lu la doc avant\n\n  \n    On va tous mourrir, le BOv3 aussi, mais lui c‚Äôest son jour. J‚Äôai dit pareil pour ma mamie\n    Moi aussi, mais j‚Äôai pas pu la red√©marrer apr√®s\n    T‚Äôas oubli√© de la recompiler\n    üò± J‚Äôai pas recompil√© mamie !\n  \n\n\nAu moins, il est indispensable\n\n  Moi avec le temps, je me suis sp√©cialis√© dans les sujets qu‚Äôaiment pas le reste de la team\n\n\nEt pour prendre un wget ?\n\n  T‚Äôas pas d‚Äôami prends un Curly, t‚Äôas pas d‚Äô√¢me prends un curl\n\n\nC‚Äôest pas faux, quinquies\n\n  Notre monitoring n‚Äôest fiable que si on peut s‚Äôy fier\n\n\nLynchez le !\n\n  Mais quoi ? Zelda c‚Äôest pas le personnage principal qu‚Äôon joue dans Zelda\n\n\nEt le meilleur pour la fin\n\n  Y a peu de temps j‚Äô√©tais en train de faire du python, l√† je reviens sur du JS. Et du coup je suis en train de debugger mon site, quand d‚Äôun coup sans que je comprenne pourquoi √† chaque fois que je le reload il imprime une feuille de papier. J‚Äôai mis une heure √† chercher avant de voir que par r√©flexe, j‚Äôai debugg√© mon code en utilisant la commande print au lieu de faire un console.log\n\n"
} ,
  
  {
    "title"    : "Bedrock GopherCon Berlin 2024",
    "category" : "",
    "tags"     : " golang, opensource, community, conference, 2024",
    "url"      : "/2024/07/18/gophercon-eu-berlin-2024.html",
    "date"     : "July 18, 2024",
    "excerpt"  : "The 2024 GopherCon Europe took place in Berlin. Four of us had the opportunity to take part in the event in-person while\nothers could attend the talks virtually.\n\nTalks\n\nThe Business of Go - Cameron Balahan Go Product Manager\n\nCameron Balahan, a P...",
  "content"  : "The 2024 GopherCon Europe took place in Berlin. Four of us had the opportunity to take part in the event in-person while\nothers could attend the talks virtually.\n\nTalks\n\nThe Business of Go - Cameron Balahan Go Product Manager\n\nCameron Balahan, a Product Manager at the Golang Team, kicked off GopherCon 2024 with a compelling talk on the business\nside of Go. He shared insights into the strategic decisions that shape the development of the Go language, emphasizing\nthe importance of balancing innovation with stability to meet the needs of both enterprise users and the broader\ndeveloper community.\n\n\n\n\nMemory Optimization - Diana Shevchenko\n\nDiana Shevchenko from Datadog delivered an enlightening session on memory management in Go, particularly focusing on how\ndata on the heap is stored in chunks known as mspans. She highlighted the benefits of this approach, such as efficient\nmemory access and CPU cache optimization, while also discussing potential pitfalls like logical grouping and code\nreadability challenges.\n\nKey Takeaways:\n\n  Benefits: Efficient memory access, CPU cache optimization.\n  Challenges: Logical grouping, code readability, versioning, and backward compatibility.\n\n\nDomain Driven Design - Robert Laszcak\n\nRobert, a principal engineer, shared valuable insights on simplifying Go projects using Domain-Driven Design (DDD). He\nhighlighted a recurring challenge: project complexity. Both accidental complexity, stemming from over-engineering, and\nessential complexity, inherent in each feature, were identified as major hurdles. Ignoring essential complexity often\nleads to increased accidental complexity, making projects cumbersome and prone to becoming legacy systems quickly.\n\n\n\nTo illustrate the problem, the engineer introduced a fictional company, Bing Mate, which developed an API for issuing\ninvoices and processing payments. Initially, the company delivered new features weekly. However, a year later, despite\nmaintaining a healthy codebase, they struggled to implement similar features promptly, triggering concerns about their\nproductivity.\n\nThe Initial Solution: Microservices and Kubernetes\n\nBing Mate attempted to solve their issue by hiring more developers and transitioning from a monolithic application to a\nmicroservices architecture on Kubernetes. Despite achieving the ideal Kubernetes setup, they still faced slow software\ndelivery. This led to a realization that the complexity of their microservices architecture might be the root cause.\n\nIn this Fake company, Emily conducted an investigation. She discovered that Bing Mate had split their monolithic\napplication by database tables, resulting in closely related services that increased complexity. The solution proposed\nwas to merge these related services into a more cohesive microservice, or a ‚Äúmicro-monolith‚Äù, reducing the need for\nextensive inter-service communication. To manage the complexity of this new service, Domain-Driven Design (DDD) was\nintroduced.\n\nKey DDD Patterns To Take Home\n\n\n  \n    Always Keep Valid State in Memory:\nThe first pattern emphasizes maintaining valid state within the application. By encapsulating the state and ensuring\nthat only valid data can be instantiated, the integrity of the application is preserved. This is achieved by using\nprivate properties and constructors that enforce validation rules, ensuring that no invalid state can exist in memory.\n  \n  \n    Keep Domain Database-Agnostic:\nThe second pattern involves separating domain logic from database logic. By using the repository pattern, interactions\nwith the database are abstracted, allowing the domain logic to remain clean and focused on business rules. This\nseparation not only makes the code more maintainable but also facilitates testing and potential database changes in the\nfuture.\n  \n  \n    Reflect Business Logic in Code:\nThe third pattern advocates for aligning code with business terminology. By using the same language and terms that\nbusiness stakeholders use, the code becomes more readable and understandable for non-technical team members. This\nalignment enhances communication and ensures that the software accurately represents business requirements.\n  \n\n\nConclusion\n\nWhile these patterns provide a solid starting point, they represent just a fraction of what DDD offers. Domain-Driven\nDesign encompasses a wide array of techniques that address various aspects of software development, from architecture to\nrequirement gathering. By integrating these patterns into their projects, developers can create more maintainable and\nscalable systems, ultimately leading to more efficient software delivery.\n\n\n\nThe engineer concluded the talk by emphasizing the importance of understanding and utilizing DDD, especially in complex\nprojects. They also provided materials and resources for further learning, encouraging developers to explore and\nimplement DDD in their own projects.\n\nDB Connection Pool - Agniva De Sarker\n\nAgniva De Sarker of Mattermost delivered a detailed presentation on the intricacies of database connection pooling in Go.\n\nHe emphasized the importance of using an efficient database schema for logical database operations and discussed the\nlimitations of pgbouncer. Agniva highlighted that the ideal solution is a single pool serving multiple requests,\nalthough Mattermost itself is not designed to be multi-tenant.\n\nHighlights\n\n\n  Key Concepts: Efficient database schema, pgbouncer.\n  Project: Mattermost‚Äôs Perseus project was discontinued due to security concerns.\n  Resource: Mattermost Perseus Project\n\n\nAgniva‚Äôs insights into database connection pooling underscored the importance of an efficiently designed database\nschema, which defines how data is logically organized within a relational database. This includes logical constraints\nsuch as table names, fields, data types, and the relationships between these entities, all crucial for optimal\nperformance and scalability in Go applications.\n\n\n\n\n\nAI Application in Go - Travis Cline\n\nTravis Cline introduced LangChainGo, a library designed to integrate Go with the generative AI ecosystem. This library\nsimplifies writing LLM-based programs in Go, supporting various models like ChatGPT. Travis outlined current\ncapabilities and future plans for LangChainGo, including core simplification, advanced agent support, and deeper\nintegration with tools like LangSmith.\n\nCapabilities\n\n\n  Structured Output\n  Tool Calling\n  Vector Database (Sequoia)\n\n\nFuture Plans\n\n\n  Core Simplification\n  Documentation\n  Advanced Agent Support\n  Deeper Integration\n\n\n\n\n\n\nConcurrent Go - Raghav Roy\n\nRaghav Roy introduced the TLA+ specification language, a formal method for verifying distributed and concurrent systems.\n\nHe demonstrated how TLA+ can be used to model and verify concurrent Go programs, ensuring that they are free from unexpected behaviors.\n\n\n\nTechnical Docs - Hila Fish\n\nHila Fish provided a comprehensive guide on creating effective technical documentation. She stressed the importance of\nvarious document types, including system design briefs, on-call runbooks, and project planning documents. Hila also\ndiscussed the benefits of thorough documentation, such as reducing work volume and enabling self-service, thereby\nincreasing developer velocity.\n\nDocumentation Types\n\n\n  System Logical Design/Brief\n  On-Call Runbooks\n  Code README\n  Onboarding Docs\n  Project Planning Docs\n  Docs as Code\n  Slack Pinned Messages\n  Slack bot\n\n\nGeneral Guidelines\n\n\n  Know Your Audience: Tailor documentation for internal maintainers or external users. Make sure people understand it\n  Decide/Abide by Documentation Type: Use markdown for docs as code, integrate diagrams, and ensure CI/CD validations.\n\n\n\n\nHer tips aim to have a well-documented code and not a self-documented one. She recommends that we should not feel forced\nto write everything but at least documente whatever we can and keep it simple.\n\nBelow are some of the suggested links shared to help us:\n\n  Go Official Style Guide\n  Google Style Guide\n  Technical Writing Tips by Hila Fish\n\n\nFrames &amp;amp; Pointers - Felix Geisend√∂rfer\n\nFelix Geisend√∂rfer‚Äôs session focused on performance profiling and tracing tools in Go, including pprof, frame pointers,\nand various tracing utilities like Gotraceui and Traceutils. He demonstrated how these tools can help developers\nunderstand and optimize their Go applications.\n\nTools Highlighted\n\n\n  pprof\n  Frame Pointers\n  Gotraceui\n  Traceutils\n\n\n\n\n\n\nSecuring Containers - Zvonimir Pavlinovic\n\nZvonimir Pavlinovic discussed container security, introducing govulncheck, a vulnerability scanning tool developed by\nthe Go team. He presented data on vulnerability findings in containers and discussed the capabilities of Scalibr, a\nsoftware composition analysis library that works across different types of binaries.\n\nKey Points\n\n\n  Tool: govulncheck\n  Vulnerability Findings: Detected symbols in 54% of containers, mostly Go vulnerabilities.\n  Library: Scalibr, supports various binary types.\n\n\nResources\n\n\n  govulncheck\n  Scalibr\n\n\nAnti Patterns - Rabieh Fashwall\n\nRabieh proposes some good practices:\n\n\n  when using generics is relevant\n  that can be resumed in ‚Äúdon‚Äôt reinvent the wheel‚Äù i.e. use native go functions instead of homemade code can have\nimpact on performances. He shares simple examples and validates them with simple benchmarks.\n\n\n\n\n\nConclusion\n\nThese two days were a great chance to meet people involved in the language‚Äôs development and learn more about a wide\nrange of topics related to the Go language. Because the event was so well-organized, we were able to exchange in general\nbenevolence with a large number of GO users.\n\nOur only regret is the somewhat short duration of the talks, which occasionally prevented us from digging deeper into\ncertain subjects and the lack of a Q&amp;amp;A period with the speakers at the conclusion of their presentation though you can\nstill ask them questions all along the event.\n"
} ,
  
  {
    "title"    : "QCon London 2024",
    "category" : "",
    "tags"     : " cloud, devops, conference, 2024",
    "url"      : "/2024/04/17/qcon-london-2024.html",
    "date"     : "April 17, 2024",
    "excerpt"  : "We really enjoyed this QCon conference in London. It‚Äôs based on 5 tracks a day, covering resilience, scalability, architecture, monitoring, performance, management and all the subjects that speak to tech companies. None of these tracks are sponsor...",
  "content"  : "We really enjoyed this QCon conference in London. It‚Äôs based on 5 tracks a day, covering resilience, scalability, architecture, monitoring, performance, management and all the subjects that speak to tech companies. None of these tracks are sponsored: they‚Äôre all about feedback, with no marketing whatsoever. There is a 6th track where sponsorship is permitted, and it‚Äôs the only one. There aren‚Äôt many sponsorship stands, and you go to this conference to talk to your peers first and foremost.\n\nWe were able to focus on feedback from teams facing similar problems to those we face at Bedrock Streaming. It was an inspiring conference, which reinforced some of our choices and gave us ideas for others.\n\nThis article will summarise what we have learned from each theme.\n\nPerformance\n\nNot all optimisations are necessarily relevant, and it is important to measure their benefits and compare their costs. Some make the code more complex, while others lead to weaknesses in resilience. It‚Äôs all about compromise.\n\nThink about the main case, rather than anticipating all the possible exceptions, which can complicate the use in very isolated cases.\n\nProfiling is the key to your journey to optimization. However with one tool you are sure where problems lie but with a second one (or more) doubt creeps in‚Ä¶ It is interesting to associate benchmarks before and after each optimization. And beware as the famous Pareto Principle,-aka the 80-20 rule-, as it turns out easy fixes are often the most efficient.\n\nYou can‚Äôt perform profiling? No problem just follow metrics and identify when a code change impacts your performances. What again? I can‚Äôt hear you with all this noisy data! Oh yes you are right, noise in your data is indeed a pain in the backend but you have to deal with it as it is commonplace. I know, this is scary, because you don‚Äôt come from a data engineering world but not worries, statistics are not so hard to tame. Go take a look at Change Point Detection, a non-parametric test which helps you test if these changes you saw in your metrics are relevant.\n\nLet‚Äôs now deep-dive in a more opaque layer of our system composed by the kernel. Here we have to admit that the heuristics at stake while for example choosing on which CPUs or GPUs to execute are not easily tweakables, despite the fact that it was shown they indeed impact overall performances.\n\nOne of the promising solutions is the famous tool cilium/eBPF which allows you to directly reprogram certain behaviours of your kernel, especially the network (for the cilium part), without having to recompile the kernel.\n\nHowever, some think that regarding these kernel issues we are stuck in a local maximum and it will require a mindset shift to fully overcome these hardware performance issues.\n\nScaling\n\nAround scaling, we attended both technical and organisational talks.\nOn the technical side, there are a number of relevant ideas. Always contain your costs - especially in the Cloud, with all its hidden expenses. Aiming for the highest possible SLO is a higher priority than cutting costs.\n\nSome incidents are an accumulation of many changes and are not linked to one team/one change/one regression. Those problems lead to long investigations and cannot be resolved by a simple rollback. This counterpart comes naturally with innovation.\n\nArchitects or equivalent must always coordinate internal exchanges: timeout durations, number and relevance of retries, etc.\n\nAlso, standardising tools brings more benefits than choosing a highly optimised tool that is different for each team. Try to reuse code and don‚Äôt reinvent the wheel.\n\nSeveral presenters spoke of asynchronous and ‚Äòreal-time‚Äô scalability (meaning millions of users in a matter of seconds). The bottom line for all of them is that there is no way of scaling the necessary resources so quickly: everyone pre-scales. The pre-scaling itself can take a long time, so for a particular event (such as a sports match), resources need to be launched several hours in advance. Asynchronous processing can help to spread out all the non-critical calculations over time.\n\nDon‚Äôt forget to be patient! It really seems to go against the tide, yet designing long-time running APIs (meaning APIs that can handle waiting) really seems to be the key of success when trying to address scalability issues. Likewise try as much as possible not to overlap your APIs field of expertise.\n\nOn the organisational level, architects of all kinds must identify or create links between all elements in the ecosystem.\n\nThe architect‚Äôs vision is to understand the choices and make compromises between the teams so that the whole is coherent. There is no lock-in. Every choice brings both positives and negatives.\n\nSustainability\n\nIncluding sustainability in our business processes is a recent but no less important necessity. And do you know what? As it turns out, it often means cutting costs, increasing performances and resiliency!\n\n\n\nWhat we can sum up is that we have the power to act at our own level. Choose services that auto-scale, prefer ARM instances, select partners who measure and try to improve their Carbon emissions. In many cases, Serverless or Compute@Edge can be a very good choice for applications that can take huge load spikes as much as a few requests (quickly scalable and extremely elastic). When possible, try to run jobs during the night, when load spikes are over.\n\nThe aim is to use energy when it is really needed: switching off dev and staging environments when employees are resting. Limit over-provisioning of resources and pre-scaling times.\nAll cloud providers include a sustainability section in their ‚Äòwell-architected‚Äô programs, a section that is independent of performance and costs, even though it provides co-benefits with those two subjects.\n\nIt really is in companies‚Äô interests to move in this direction.\n\nManagement, People and  Process\n\nMaybe first keep in mind that decisions are not right or bad, they just are taken by a person or a quorum based on the context they had on a given time. And let‚Äôs face it, your organisation will need to change over time especially if you begin as a start-up and now need to scale-up (oh an interesting topic of scaling coming back!).\n\nAs for scaling your infrastructure, you need to realise that you will need to make compromises. For example, Trainline shared with us that their three pillars are: Alignment, Productivity and Code Quality. You can‚Äôt perform in the three at the same time, you need to choose which one(s) to let go a little in order to achieve the company‚Äôs goal. Importantly what is working at a given time in a certain situation could not be appropriate anymore months later.\n\nPlatform Engineering\n\nThe last day of the conference offered a dedicated track on platform engineering, where we listened to the insights shared by the speakers. As they delved into their own experiences and learnings, we found striking parallels with our own journey at Bedrock over the past few years.\n\n\n\nOne of the most resonant themes was the importance of treating the platform as a product: a perspective shift that has significantly influenced our approach. By emphasising the need for a smoother experience that balances consistency, performance, security, and development speed, the speakers reinforced the notion that our platform isn‚Äôt just a set of tools; it‚Äôs a crucial enabler for our developers‚Äô success.\n\nThe concept of providing a golden path for developers stood out as particularly impactful. We‚Äôve come to realise that by streamlining the onboarding process and offering clear guidelines, we can empower developers to navigate the platform with confidence and efficiency. This approach not only accelerates time-to-market for new features but also fosters a sense of trust between the platform team and its users.\n\nAnother key takeaway was the importance of communication and trust-building within the organisation. We were reminded that trust is the currency of collaboration, and effective communication is essential for maintaining it. Clear documentation, transparent processes, and accessible points of contact were highlighted as critical components of this effort.\n\nSurveys emerged as a valuable tool for gathering feedback and gauging user satisfaction, although the speakers acknowledged the challenges in ensuring high engagement and confidence in the results. However, they emphasised that even imperfect data can provide valuable insights when interpreted thoughtfully and supplemented with other forms of feedback, such as user interviews and proactive communication.\n\nThe speakers also shared their experiences with empowering developers to take ownership of certain tasks, such as fixing issues surfaced by conformity checks and contributing features that solved their own challenges back up to the general platform to benefit the whole organisation. This aligns closely with our own DevOps principles, emphasising collaboration and shared responsibility across teams.\n\nAs we reflect on the insights shared by the conference speakers, we‚Äôre reminded of the importance of continuous improvement and iteration in our platform engineering efforts. By learning from our mistakes, embracing new perspectives, and staying attuned to the needs of our users, we can continue to evolve our platform into a truly indispensable resource for our development community.\n\nConclusion\n\nIt was our first QCon for most of us. We appreciated the transparency of the speakers and the commitment of the organisers, who made the event a success on every level: the intensity of the conferences, the quality of the talks, the really good and varied food and the discreet presence of the sponsors.\n\nWithout being very technical, this conference is a source of inspiration for us, as it focuses on feedback and encourages discussion with our peers.\n\n"
} ,
  
  {
    "title"    : "KubeCon Europe 2024, Paris",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, 2024",
    "url"      : "/2024/04/09/kubecon-europe-2024.html",
    "date"     : "April 9, 2024",
    "excerpt"  : "Two weeks ago, six of us were at KubeCon in Paris. For a few, it was their first KubeCon! For one, it was the fourth one. Since Copenhagen in 2018, which was before we started moving to AWS and Kubernetes, so much has changed ‚Äì and so much has not...",
  "content"  : "Two weeks ago, six of us were at KubeCon in Paris. For a few, it was their first KubeCon! For one, it was the fourth one. Since Copenhagen in 2018, which was before we started moving to AWS and Kubernetes, so much has changed ‚Äì and so much has not ;-)\n\nBig ideas\n\nFirst of all, these four days clearly highlighted some wide topics.\n\nAI and GPUs everywhere\n\nSeems like ‚ÄúAI‚Äù was the impossible-to-miss subject vendors were trying to push, this year! Still, while many are running experiments, not many have deployed and maintained workloads in production yet.\n\nLike many, we have started using a few ‚ÄúAI‚Äù tools and are working on some ideas (not hosting them ourselves yet, though), but we can‚Äôt help but feel ‚ÄúAI‚Äù and especially the ‚ÄúGenerative AI‚Äù term is a bit over-used and some solutions we‚Äôve seen were closer to good-old-ML.\n\nAs we are not planning on running our GenAI workloads on Kubernetes anytime soon, we haven‚Äôt been to many talks on this matter. Still, between re:Invent last year and KubeCon this year, you‚Äôll find plenty of contents if that‚Äôs your thing üí™.\n\nPlatform engineering is the new DevOps?\n\nNext up, looks like DevOps is dead and Platform Engineering is the new DevOps!\n\nOn one hand, we clearly see having everything related to all your projects in one place brings tremendous value (and we have a couple dashboards that go this way). Same with providing a common basis (Terraform, Golang Skeleton, Helm chart) most projects build upon instead of re-inventing it again and again.\n\nOn the other hand, we don‚Äôt think, today, ‚Äúbeing able to create a new service in one click and 15 seconds‚Äù is a goal we should aim for. We‚Äôd rather rationalize micro-services and reduce network overhead. Remember, distributed systems are ‚Äúeasy‚Äù.\n\nStill, Backstage has gained in traction over the past two years, and it would be the first tool we‚Äôd try if (or when) we‚Äôd wanted to invest on this path.\n\nGreen Computing\n\n‚è©¬†Heating Pools with Cloud Power: A New Wave in Green Computing - Saiyam Pathak, Civo &amp;amp; Mark Bjornsgaard, Deep Green Technologies\n\nNowadays, data centers‚Äô electricity consumption uses 2% of the world‚Äôs electricity and the demand is growing exponentially due to more and more data centers being built. A very large percentage of future consumption will be dedicated to AI through high consumption GPUs.\n\nIn this context, Deep Green is embracing a generational shift as a company building a new kind of data centers. They aim to build small edge data centers as close as possible to heat consumers in order to recapture and distribute the heat. Indeed, in classic data centers, a huge chunk of energy is wasted into heat not being used. Chance with datacenter is that a very large amount of heat is produced in the same place (in comparison with other kind of industry). Deep Green‚Äôs computers are immersed into biodegradable oil which captures generated heat. Then, heat is transferred through heat exchangers to targets which can be swimming pools, private houses, offices‚Ä¶ The process is simple and known for decades, but the smart move is to associate small edge data centers to heat consumers.\n\n\n\nAnd what about CNCF and kubernetes? CNCF tools are allowing them to make this work as efficient as possible by:\n\n\n  monitoring heat consumption and carbon emissions with tools like Kepler\n  adapting workloads sizes\n  managing intelligent workload scheduling: scheduling the workload to the datacenter which is actually requiring to heat up a particular consumer (using ArgoCD deployment, for instance)\n\n\nScaling depending on energy usage of instances\n\nIn recent years, sustainable IT has become a major issue for companies. At Bedrock streaming, we are committed to reducing the environmental impact of our platforms. One of the first steps in this transition is to understand the impact of our applications.\n\nAt kubeCon 2024, we were able to discover a number of interesting tools that we‚Äôll be testing in the near future.\n\nKepler\n\n‚è©¬†Sustainable Computing: Measuring Application Energy Consumption in Kubernetes Environments with Kepler\n\nKepler is a CNCF Sandbox project that uses eBPF to collect performance counters and other system statistics. Kepler estimates energy consumption per pod based on this data and exports it as Prometheus metrics.\n\n\n\nPEAKS\n\n‚è©¬†Empowering Efficiency: PEAKS - Orchestrating Power-Aware Kubernetes Scheduling\n\nPeaks aims to optimize the energy consumption of a kubernetes cluster during scheduling. Peaks relies on pre-trained ML models vs energy consumption to predict the most suitable nodes. The tool is not yet available, but development is progressing and a Kubernetes Enhancement Proposal will be opened in the sig-scheduling project.\n\nBoth tools represent significant advances towards more sustainable computing, and reflect the Kubernetes community‚Äôs commitment to responsible innovation.\n\nFinops\n\nReducing ‚Äì or optimizing ‚Äì infrastructure costs has been a recurring topic at KubeCon for as long as we remember. Every year, there are talks about basic tools like HPA or ideas like using Spot instances (like we do, for 100% our worker nodes!). This time, Karpenter has clearly become a leader.\n\nThe basics being covered, costs are still too high, and reducing them is getting even more complex than before as more stuff is deployed to Kubernetes. Thinks GPU-based or long-running workloads, for example.\n\nWe are beginning to see more and more approaches using machine learning to forecast how much resources will be needed at a given time, depending on past data. We, like others, have implemented our own pre-scaling mechanism ‚Äì not based on ML yet, though.\n\nKubernetes and Developer experience\n\nDevOps and Ops, who manipulate Kubernetes and its objects everyday, we tend to forget: Kubernetes is not (perceived as) easy for our developers colleagues, who only write manifests every once in a while or use kubectl when their app fails.\n\nDuring ‚ÄúDevelopers Demand UX for K8s‚Äù, Mairin Duffy and Conor Cowman presented the results of an UX research they did amongst Kubernetes users and identified the main issues developers encounter with it. Debugging network issues, YAML (indentation, lack of validation, clean export), the infamous Crash Loop (and disappering logs), CLI vs GUI‚Ä¶ Yeah, we‚Äôve had all of those!\n\nWe have partially solved some of these issues, both with technical solutions (Helm + Helmfile, reusable Github Action workflows) and training sessions, but debugging is still a pain¬†:-/\n\nThings we are currently working on\n\nKeda\n\n‚è©¬†Scaling New Heights with KEDA: Performance, Extensions, and Beyond - Jorge Turrado, SCRM Lidl International Hub &amp;amp; Zbynek Roubalik, Kedify\n\nIn our ongoing quest to improve the efficiency of our Kubernetes clusters, we recently opted for Kubernetes Event-driven Autoscaling (KEDA), a project supported by the Cloud Native Computing Foundation (CNCF) and enjoying growing adoption by the community. This choice is in line with our desire to go further than the capabilities offered by our previous tool, the Prometheus adapter, thanks in particular to more refined scaling management based on various external metrics.\n\nOne of KEDA‚Äôs real strengths is its ability to adjust scaling via ‚Äòscaling modifiers‚Äô, allowing precise adjustment of the number of pods as required based on a calculation. At Bedrock Streaming this will enable us to refine our prescaling strategy, previously based on an external metric provided by a go service, we‚Äôll be able to upgrade our system to something simpler and more precise. Maybe evolve from a simple calculation of minimum pods and multipliers, to a more sophisticated approach.\n\nThe presentation by Zbynek Roubalik and Jorge Turrado, KEDA maintainers, highlighted KEDA‚Äôs effectiveness in meeting various load requirements through the use of a variety of external metrics, making scaling more accurate.\n\nCilium\n\nCilium (and eBPF) were well represented at KubeCon, with a number of conferences on the subject.\n\nThis came at just the right time, as we are currently in the process of migrating to this CNI. This one, based on eBPF (with all that that implies) seemed to us to be a good replacement for our current CNI.\n\nWith cilium: no more KubeProxy, and no more IPtables, which for large clusters is a major performance factor.\n\nCilium also enables network traffic to be filtered with NetworkPolicies inbound and outbound.\n\nAnother feature we particularly like is ‚Äúcluster-mesh‚Äù, which allows you to communicate between several clusters in a fairly simple way, at least on paper üòä! (conference available ‚è©¬†here)\n\nWe were also able to see new features such as GatewayAPI support from version 1.15, and the use of stateDB from version 1.16 to manage the state of cilium objects (and therefore reconciliation in the event of inconsistent state (conference available ‚è©¬†here)).\n\nTools and idea we want to try üí™\n\nKubeCon is also a great way to discover or re-discover new technologies or tools. We won‚Äôt share an exhaustive list (see you there next year!), but here are a few we are excited about:\n\n\n  WebAssembly (especially SpinKube): we are running some workloads at-edge on WASM and see a huge potential there. WASM in Kubernetes is intriguing, maybe as a way to reduce pods startup time?\n  OpenFeature: for feature-flipping, A/B testing‚Ä¶ Glad to see the emergence of an open standard!\n  Working with multiple Kubernetes clusters: not something we think we need for now, but after hearing about Federation years ago, Karmada seems to be the current tool for this.\n  mirrord: to develop locally as-if inside a Kubernetes cluster, forwarding network and file accesses.\n\n\nCrik &amp;amp; Criu: creating snapshots for interrupted pods\n\n‚è©¬†The Party Must Go on - Resume Pods After Spot Instance Shut Down - Muvaffak Onu≈ü, QA Wolf\n\nDuring the conference held by Muaffak Onu≈ü of the CNCF, an innovative solution was highlighted to mitigate the instability of spot instances: the CRIK project, which leverages CRIU to provide backup and restore functionality for Kubernetes pods. By leveraging CRIU, CRIK can capture the state of processes within a pod before a spot instance shutdown, and then restore them, making it easier to resume work after a spot reclaim for exemple.\n\nA kubernetes controller is deployed, which monitors the state of nodes. When a pod with the required criu configuration is going to be deleted it consults the controller to check if the node is going to be deleted to dump all it‚Äôs processes, file descriptors etc‚Ä¶ Before being restored on another one.\n\nThis technology is particularly relevant for Bedrock Streaming, where our entire Kubernetes clusters are based on EC2 spot instances. The use of CRIK could transform the way we manage extended jobs, particularly for our machine learning / AI teams, by ensuring the continuity of jobs lasting several hours despite instance interruptions. The integration of CRIK promises to substantially improve the resilience and efficiency of our cloud operations. We plan to test Crik at one of our future r&amp;amp;d days.\n\nCilium as cluster-mesh\n\n‚è©¬†Simplifying Multi-Cluster and Multi-Cloud Deployments with Cilium - Liz Rice, Isovalent\n\nOne thing that caught our attention was the native Cluster-Mesh feature which could allow us to connect multiple clusters between them with little effort and potentially multiple cloud providers between them.\n\nTo allow each Cilium‚Äôs components to communicate between them an internal load-balancer is deployed in each cluster.\n\nThere are some prerequisites to allow cluster mesh though:\n\n\n  CIDR block must not overlapped\n  Each cluster needs to have their own unique ID and name\n  All clusters need to use Cilium as their CNI\n  Cluster Mesh option has to be enabled on each cluster\n\n\nLiz Rice did a live presentation of cluster mesh capabilities with some use cases and functionalities like:\n\n\n  Creation of global services\n  The ability to setup affinity rules for local cluster and remote one, to prefer the usage of local cluster pods and fallback to remote if needed.\n  Setup of network policies to cluster level with the Cilium‚Äôs ability to add custom labels to pods to identify the cluster they‚Äôre running on.\n\n\nEven if for now the usage of multi-cluster through different cloud providers is not a common use case the fact that Isovalent succeeded to make it that simple is very impressive\n\nCilium is now the new standard and we are glad to have chosen it on our clusters.\n\nOpenFGA\n\n‚è©¬†Federated IAM for Kubernetes with OpenFGA - Jonathan Whitaker, Okta\n\nAmong the presentations that stood out for us, the one from OpenFGA really grabbed our attention. OpenFGA, or Fine Grained Authorization, is an open source project that promises to transform the way we manage authorization and identity federation in modern applications.\n\nOpenFGA is a universal authorization solution that enables complex authorizations to be modeled in a granular way. Inspired by Google‚Äôs Zanzibar project, OpenFGA offers a developer-friendly API, while guaranteeing performance and security.\n\nOpenFGA gives us the ability to define detailed access policies, which is crucial for identity and access management in dynamic environments like Kubernetes.\n\nThe OpenFGA tool represents a major advance in the field of IT security. Its flexibility, performance and open approach make it an ideal candidate for companies looking to implement robust identity federation. The presentation at KubeCon 2024 only confirmed the immense potential of this tool.\n\nThings that are no longer ‚Äúproblems‚Äù\n\n\n  Basic reactive auto-scaling. Still challenges: using &amp;gt;50% of resources, predictive pre-scaling, long-running tasks.\n  Basic Kubernetes observability. Still challenges: observability at large-scale and/or for large distributed systems that go beyond Kubernetes.\n  Using EC2 Spot instances ‚Äì we‚Äôve been doing it for years for 100% our worker nodes. Now, with Karpenter, others seem to be doing it more and more.\n\n\nConclusion\n\nWhy do we keep going to KubeCon ‚Äì and other conferences? Well, three goals:\n\n\n  discovering new approaches and tools that might help us better serve our customers in the future;\n  confirming the solutions we are working with are used by many others in the community, which (probably) means they are (still) the right tools;\n  and learning how to do some things better every day.\n\n\nAnd it‚Äôs funny how each of us noticed different matters, depending on which parts of our platform we work on and depending on our experience and knowledge in Cloud Native solutions!\n\nIf we were to suggest a few things to KubeCon‚Äôs organizers:\n\n\n  30 minutes talks is sometimes a bit short and longer talks could go deeper into technical details;\n  rooms were not big enough for popular talks ‚Äì and we‚Äôre talking about ‚Äútech‚Äù talks¬†;-)\n  food üòµ. I mean, we‚Äôre French üòÖ\n\n\nOnce again, we come back from KubeCon full of energy and ideas, ready for an exciting new year!\n"
} ,
  
  {
    "title"    : "Mon premier jeu sur BGA #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/mon-premier-jeu-sur-bga",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Introduction au jeu de soci√©t√© Velonimo et √† la plateforme de jeux en ligne Board Game Arena, suivie d‚Äôun retour d‚Äôexp√©rience de l‚Äôimpl√©mentation de ce jeu sur cette plateforme en tant que d√©veloppeur Web.\n\n\nPar Oliver Th√©bault\n",
  "content"  : "\n  Introduction au jeu de soci√©t√© Velonimo et √† la plateforme de jeux en ligne Board Game Arena, suivie d‚Äôun retour d‚Äôexp√©rience de l‚Äôimpl√©mentation de ce jeu sur cette plateforme en tant que d√©veloppeur Web.\n\n\nPar Oliver Th√©bault\n"
} ,
  
  {
    "title"    : "Le LFT du LFT - PUB LFT #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/le-lft-du-lft",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Comment aider les LFTs ?\n\n\nPar la LFTeam\n",
  "content"  : "\n  Comment aider les LFTs ?\n\n\nPar la LFTeam\n"
} ,
  
  {
    "title"    : "Le leader imposteur #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/le-leader-imposteur",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Je ne suis pas expert de mon m√©tier et ce n‚Äôest pas parce que j‚Äôen parle facilement que je pense le devenir un jour.\nJe n‚Äôai jamais su √™tre un leader, et tous les leaders m‚Äôimpressionnent.\nJe pense que les autres sont meilleurs que moi et que j...",
  "content"  : "\n  Je ne suis pas expert de mon m√©tier et ce n‚Äôest pas parce que j‚Äôen parle facilement que je pense le devenir un jour.\nJe n‚Äôai jamais su √™tre un leader, et tous les leaders m‚Äôimpressionnent.\nJe pense que les autres sont meilleurs que moi et que je ne pourrai pas √™tre leur √©gal.\nJ‚Äôai √©t√© nomm√© par chance √† un poste de leader alors que je n‚Äôen avais pas la l√©gitimit√©.\nEt‚Ä¶je n‚Äôai jamais eu confiance en moi (j‚Äôaurais peut-√™tre du commencer par √ßa)\n\n\n\n  Ceci est une histoire, MON histoire, et je vais vous parler de mon syndrome de l‚Äôimposteur.\nComment vivre avec, mes conseils, mes craintes et mes peurs‚Ä¶mais surtout comment vous pouvez vous aider et aider ceux qui le poss√®de √©galement.\n\n\nPar Mathieu Mure\n"
} ,
  
  {
    "title"    : "Du code √† l&#39;image : Aller et Retour #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/du-code-a-l-image-aller-et-retour",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Pourquoi l‚Äôintelligence artificielle doit-elle s‚Äôentra√Æner pour devenir meilleure, et comment fait-elle ? C‚Äôest quoi un r√©seau de neurones ? Comment Dall-E arrive-t-il √† dessiner de si belles images ?\n\n\n\n  Depuis peu, l‚Äôintelligence artificiell...",
  "content"  : "\n  Pourquoi l‚Äôintelligence artificielle doit-elle s‚Äôentra√Æner pour devenir meilleure, et comment fait-elle ? C‚Äôest quoi un r√©seau de neurones ? Comment Dall-E arrive-t-il √† dessiner de si belles images ?\n\n\n\n  Depuis peu, l‚Äôintelligence artificielle fait √©norm√©ment parler d‚Äôelle. Pourtant, derri√®re les termes de ‚ÄúDeep learning‚Äù et de ‚ÄúR√©seau de neurones‚Äù, il est rare de comprendre exactement ce qui se passe dans la machine. Et si on explorait √ßa ?\n\n\n\n  D√©couvrons ensemble, dans une pr√©sentation ne demandant strictement aucune comp√©tence technique, les bases de l‚Äôapprentissage de ces intelligences, afin de comprendre un peu plus ce qui se passe derri√®re les rouages et toute cette magie !\n\n\n\n  N‚Äôh√©sitez surtout pas √† venir m√™me sans la moindre connaissance technique, le but de cette conf√©rence est que m√™me votre grand-m√®re puisse y venir, et en ressortir en √©tant capable d‚Äôexpliquer ce qu‚Äôest une IA !\n\n\nPar Etienne Doyon\n"
} ,
  
  {
    "title"    : "Comment j&#39;ai retrouv√© le sens de la vie gr√¢ce √† WebAssembly #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-j-ai-retrouve-le-sens-de-la-vie-grace-a-web-assembly",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Le jeu de la vie est un dr√¥le de jeu qui a la particularit√© de n‚Äôavoir pas de joueur. Il se joue de lui-m√™me et produit des configurations qui semblent √©voluer de mani√®re autonome, sans intervention ext√©rieure. A partir de r√®gles tr√®s basiques,...",
  "content"  : "\n  Le jeu de la vie est un dr√¥le de jeu qui a la particularit√© de n‚Äôavoir pas de joueur. Il se joue de lui-m√™me et produit des configurations qui semblent √©voluer de mani√®re autonome, sans intervention ext√©rieure. A partir de r√®gles tr√®s basiques, des structures d‚Äôune tr√®s grande complexit√© peuvent √©merger d‚Äôune mani√®re qui √©voque l‚Äôapparition de la vie sur Terre √† partir d‚Äô√©l√©ments inertes, d‚Äôo√π le nom myst√©rieux de jeu de la vie.\n\n\n\n  Comme ce jeu se joue id√©alement sur des plateaux sans limite de taille, la question des performances de l‚Äôimpl√©mentation est capitale. Pour du d√©veloppement web, cet exemple permet √† la fois de voir les limites de JavaScript pour effectuer un grand nombre de calculs et de pr√©senter une solution compl√©mentaire √† la r√©duction de la complexit√© algorithmique : une impl√©mentation en WebAssembly qui permet de d√©l√©guer la charge de calcul √† un langage compil√© plus performant, Rust.\n\n\nPar Th√©o Gianella\n"
} ,
  
  {
    "title"    : "Au-del√† des industries : Le pouvoir de l&#39;√©tat d&#39;esprit #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/au-dela-des-industries-le-pouvoir-de-l-etat-d-esprit",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Je souhaite explorer l‚Äôid√©e selon laquelle le succ√®s n‚Äôest pas d√©fini par une industrie, une discipline ou un secteur sp√©cifique, mais repose plut√¥t sur notre √©tat d‚Äôesprit. √Ä travers mes exp√©riences personnelles vari√©es - en cosm√©tique, market...",
  "content"  : "\n  Je souhaite explorer l‚Äôid√©e selon laquelle le succ√®s n‚Äôest pas d√©fini par une industrie, une discipline ou un secteur sp√©cifique, mais repose plut√¥t sur notre √©tat d‚Äôesprit. √Ä travers mes exp√©riences personnelles vari√©es - en cosm√©tique, marketing, e-commerce, ainsi que dans le domaine des bars, restaurants et de l‚Äôindustrie - je mets en avant l‚Äôid√©e que les principes du succ√®s sont universels. Cette pr√©sentation souligne l‚Äôimportance de la mentalit√©, de la pers√©v√©rance, des strat√©gies adopt√©es et d‚Äôune m√©thode, ind√©pendamment du secteur professionnel. \nL‚Äôobjectif est d‚Äôinspirer et de red√©finir la perception du succ√®s, en d√©passant les barri√®res sp√©cifiques √† chaque domaine.\n\n\n\n  En r√©sum√©, c‚Äôest une approche universelle vers la r√©ussite qui est essentielle, quel que soit le secteur d‚Äôactivit√©\n\n\nPar Serge Menassa\n"
} ,
  
  {
    "title"    : "Journal de l&#39;alternant - Comment j&#39;ai perdu mes d√©pendances pnpm",
    "category" : "",
    "tags"     : " pnpm, hoisting, node_modules, alternant",
    "url"      : "/2024/01/10/journal-de-l-alternant-comment-jai-perdu-mes-dependences-pnpm.html",
    "date"     : "January 10, 2024",
    "excerpt"  : "√Ä Bedrock, on m‚Äôa charg√© de faire un POC (proof of concept) pour tester les avantages et les limites d‚Äôun double run entre notre app c√¥t√© web (sur une base maison React Server Side Rendering) en d√©l√©guant des pages progressivement vers une app Nex...",
  "content"  : "√Ä Bedrock, on m‚Äôa charg√© de faire un POC (proof of concept) pour tester les avantages et les limites d‚Äôun double run entre notre app c√¥t√© web (sur une base maison React Server Side Rendering) en d√©l√©guant des pages progressivement vers une app Next.js. √âtant tout nouveau dans le dev et encore plus nouveau sur le projet, ma vie ces derniers temps est une suite d‚Äôobstacles, d‚Äôessais, d‚Äôerreurs et de triomphes (pas toujours, mais souvent) bien m√©rit√©s. Je suis habitu√© √† faire des erreurs plus lunaires les unes que les autres, mais je vais m‚Äôattarder dans cet article sur une erreur qui m‚Äôa retourn√© le cerveau. Au menu : erreurs soudaines, d√©pendances disparues et d√©sespoir‚Ä¶ Bonne lecture.\n\nJe suis alternant depuis un an √† Bedrock et je travaille pour la premi√®re fois sur notre projet web interne. C‚Äôest un projet qui est tr√®s complexe, avec lequel vient √©norm√©ment d‚Äôhistorique (premi√®res briques √©crite en 2014) et dont la lecture du code rel√®ve parfois autant de l‚Äôhistoire que du d√©veloppement.\n\nEn route pour l‚Äôaventure\n\n\n  D‚Äôailleurs √† Bedrock, si on arrive √† maintenir notre application web dans la dur√©e, c‚Äôest gr√¢ce aux bonnes pratiques qu‚Äôon essaie de respecter au mieux.\n\n\nEn bref, je n‚Äôai qu‚Äôune connaissance tr√®s superficielle du projet et des outils qu‚Äôil int√®gre.\n\nDans mes habitudes de code, il peut parfois m‚Äôarriver d‚Äôoublier de v√©rifier que le code que j‚Äô√©cris ne vienne pas casser les tests en place dans le code. Heureusement, notre CI qui nous est ch√®re ne manque jamais de me rappeler mon manque de rigueur. Cette fois-l√†, je casse un test √† cause d‚Äôune erreur tellement anodine que je ne parviens pas √† m‚Äôen rappeler. Je peux juste vous dire que j‚Äôai eu le r√©flexe d‚Äôaller dans mon terminal de lancer le runner de test jest √† l‚Äôaide de notre package manager pnpm dans une commande qui ressemble √† : pnpm test TEST_QUI_CASSE. Le test est rouge pour une raison qui me semble venir d‚Äôun probl√®me de d√©pendances. Ayant beaucoup tritur√© mes node_modules, je me dis que repartir sur des bases propres ne devrait pas faire de mal au projet. Je d√©cide donc, sans savoir ce qui m‚Äôattend, de lancer l‚Äôinnocente commande : pnpm install\n\nJ‚Äôobserve que pnpm fait son travail, met √† jour des d√©pendances, je devais effectivement avoir jou√© un peu trop avec mes node_modules.\n\nJe relance le test et l√† quelle ne fut pas ma surprise quand mon terminal, sans trembler, m‚Äôa affich√© Command: &quot;jest&quot; not found.\n\nJe commence √† penser que je ne viens pas seulement de casser un test, mais j‚Äôai √©galement cass√© jest. √Ä ce moment-l√†, je venais de ressortir d‚Äôune bataille avec des d√©pendances et donc je venais de me familiariser avec le node_modules .pnpm et autre .bin . C‚Äôest dans ce dernier dossier que je me rends compte qu‚Äôeffectivement, il y manque l‚Äôex√©cutable jest.\n\nEn fait, il y manque √©galement d‚Äôautres outils que je m‚Äôattendais √† trouver comme prettier et eslint.\n\nJe me dis que la port√©e de mon probl√®me vient de s‚Äô√©tendre de jest √† mes node_modules. ü´†\n\nD√©sesp√©r√©, je tente une recherche globale des mots cl√©s : prettier et eslint. Je finis par trouver une correspondance int√©ressante dans le fichier .npmrc.\n\nVoil√† √† quoi ressemble le fichier √† ce moment-l√† :\npublic-hoist-pattern[]=*@testing-library/jest-dom*\npublic-hoist-pattern[]=*@testing-library/react*\npublic-hoist-pattern[]=*@testing-library/user-event*\npublic-hoist-pattern[]=*enzyme*\npublic-hoist-pattern[]=*jest*\npublic-hoist-pattern[]=*redux-mock-store*\npublic-hoist-pattern[]=*eslint*\npublic-hoist-pattern[]=*prettier*\n\nJe peux sentir qu‚Äôil s‚Äôagit d‚Äôune v√©ritable piste parce que dans ce fichier sont list√©es toutes les d√©pendances qui sont cass√©es sur ma machine.\n\nComprendre la configuration de pnpm\nHoisting des d√©pendances\nPour comprendre la configuration public-hoist-pattern il faut d‚Äôabord comprendre comment sont form√©s les node_modules par pnpm. Il ne va mettre dans le dossier node_modules en racine uniquement les d√©pendances directes du projet, toutes les sous-d√©pendances seront plac√©es dans un dossier cach√© .pnpm et un lien symbolique sera cr√©√©.\n\n\n  Je vous invite √† lire la documentation √©crite par pnpm afin de comprendre leur syst√®me de d√©pendances.\n\n\nCela peut parfois poser des probl√®mes avec des libraries qui utilisent des d√©pendances fant√¥mes. C‚Äôest pourquoi pnpm laisse quand m√™me du contr√¥le sur ce comportement.\n\n\n  On parle de d√©pendance fant√¥me pour d√©signer toutes les d√©pendances qui ne sont pas d√©sign√©es dans le package.json root mais qui sont quand m√™me n√©cessaire pour le bon fonctionnement de l‚Äôapplication.\n\n\npublic-hoist-pattern permet d‚Äôindiquer les d√©pendances qu‚Äôon veut forcer √† √™tre dans le dossier node_modules racine plut√¥t que dansnode_modules/.pnpm.\n\nLa ligne public-hoist-pattern[]=*jest* veut donc dire qu‚Äôon ajoute jest aux d√©pendances qui sont accessibles depuis la racine et ainsi l‚Äôex√©cutable dans node_modules/.bin . Cela permet par exemple de d√©l√©guer la configuration et l‚Äôimport de jest dans un package enfant du repository.\n\nRetour √† l‚Äôhistoire‚Ä¶ let‚Äôs debug\nA cet instant je suis convaincu que c‚Äôest le fichier .npmrc qui est responsable de l‚Äôerreur Command: &quot;jest&quot; not found. Je ne vois rien d‚Äôanormal dans ce fichier qui pourrait me mettre la puce √† l‚Äôoreille, c‚Äôest alors que je me dis que peut √™tre pnpm ne lit pas la bonne configuration. En lisant la documentation, je tombe sur la commande parfaite : pnpm config get. Cette commande permet d‚Äôafficher la configuration que r√©sout pnpm. La sortie de cette commande m‚Äôa mis sur une nouvelle piste puisque c‚Äôest l√† que j‚Äôai vu appara√Ætre la ligne probl√©matique : shamefully-hoist=false.\n\nJe tente de chercher dans le projet o√π est √©crite cette ligne. Aucune trace de cette maudite ligne. Je retourne tout le projet √† la recherche d‚Äôune ligne de code qui pourrait ajouter cette ligne de configuration. Je me mets √† lire toute la documentation pnpm pour pouvoir comprendre d‚Äôo√π cette ligne peut venir. Apr√®s avoir d√©sinstall√© et r√©install√© pnpm, node et red√©marr√© mon PC, je tente dans un dernier espoir de cr√©er un dossier test-a-laide dans lequel je reclone le projet. Malheureusement, rien n‚Äôy fait.\n\nC‚Äôest √† ce moment que je me dis que si le probl√®me ne vient pas de mes outils ni de la configuration locale, il faut peut-√™tre que j‚Äôaille chercher dans ma configuration globale. En effet, en ouvrant cette dite configuration ~/.npmrc, je m‚Äôaper√ßois que c‚Äôest de l√† que vient la ligne shamefully-hoist=false. C‚Äôest un soulagement, j‚Äôai enfin trouv√© d‚Äôo√π cette ligne mystique venait.\n\n\n  Je suis encore √† la recherche de la r√©ponse √† la question : pourquoi diable, ai-je mis cette configuration dans mon .npmrc global. Je pense me souvenir l‚Äôavoir fait en me disant que je voulais m‚Äôassurer que pnpm se comporte en faisant des symlinks (l‚Äôintention n‚Äô√©tait pas mauvaise, mais la cons√©quence pas joyeuse).\n\n\nOn peut lire dans la documentation de pnpm que : Setting shamefully-hoist to true is the same as setting public-hoist-pattern to *.\n\nEn d‚Äôautres termes shamefully-hoist √† une influence sur le hoisting de toutes les d√©pendances du projet.\n\nJ‚Äôai deux probl√®mes avec la documentation √† ce sujet :\n\n\n  Tout d‚Äôabord, il n‚Äôest pas explicit√© le cas inverse √† savoir si on met shamefully-hoist=false alors √ßa revient √† √©craser toutes les configurations de public-hoist-pattern\n  Le comportement, qu‚Äôil soit un bug ou un cas √† la marge, de la configuration globale de shamefully-hoist qui √©crase la configuration locale de public-hoist-pattern n‚Äôest pas sp√©cifi√©\n\n\nBref, apr√®s avoir d√©duit que c‚Äô√©tait cette ligne qui cassait mon hoisting, je retire la ligne et je lance un pnpm install. Bingo ! Je r√©cup√®re toutes mes d√©pendances perdues.\n\nEnseignements\nJ‚Äôessaie a posteriori de d√©chiffrer pourquoi j‚Äôai eu ce probl√®me et comment faire en sorte que cela n‚Äôarrive pas. Je pense √™tre tomb√© sur un comportement √©trange de pnpm. Je ne sais pas s‚Äôil s‚Äôagit d‚Äôun bug ou d‚Äôune feature. En effet, intuitivement, j‚Äôaurais tendance √† dire qu‚Äôune configuration globale de shamefully-hoist ne devrait pas override la configuration locale de public-hoist-pattern. Je suis pr√™t √† entendre que le comportement est attendu et voulu, mais dans ce cas je pense qu‚Äôun peu plus de documentation √† ce sujet ne peinera personne. √Ä cet √©gard j‚Äôai ouvert une issue sur le Github de pnpm.\n\nJe retire plusieurs enseignements de cette aventure :\n\n  Douter de la configuration qui est lue par les outils\n  La documentation ne contient pas toujours tous les comportements\n  Il faut penser √† voir plus loin que son fichier local de config et penser aux potentielles surcharges‚Ä¶\n\n"
} ,
  
  {
    "title"    : "Bedrock aux API Days Paris (2023)",
    "category" : "",
    "tags"     : " conference, paris, tech, api, eda",
    "url"      : "/api-days-paris-2023",
    "date"     : "December 20, 2023",
    "excerpt"  : "Cette ann√©e Bedrock a envoy√© 7 de ses collaborateurs et collaboratrices (i.e. nous, les auteurs de cet article) √† l‚Äô√©dition 2023 des conf√©rences ‚ÄúAPI Days‚Äù √† Paris.\n\nL‚Äô√©v√©nement a eu lieu au CNIT √† La D√©fense (juste en face du march√© de No√´l) et a...",
  "content"  : "Cette ann√©e Bedrock a envoy√© 7 de ses collaborateurs et collaboratrices (i.e. nous, les auteurs de cet article) √† l‚Äô√©dition 2023 des conf√©rences ‚ÄúAPI Days‚Äù √† Paris.\n\nL‚Äô√©v√©nement a eu lieu au CNIT √† La D√©fense (juste en face du march√© de No√´l) et a dur√© 3 jours, du Mercredi 06/12/23 au Vendredi 08/12/23.\n\n\n\nEn plus des 11 paires de chaussettes diff√©rentes üß¶ que nous avons r√©ussi √† d√©busquer en parlant aux diff√©rents partenaires sur place‚Ä¶ En tout, ce sont plus de 100 talks, r√©partis dans 9 salles, qui nous ont √©t√© pr√©sent√©s.\n\nVoici, un r√©sum√© de quelques-uns des talks que nous souhaitions mentionner sur ce blog üëá\n\nMaking the Most of Your OpenAPI Spec\n\n\n  Conf√©rence pr√©sent√©e par Alexander Broadbent (Principal Engineer - SAPI)\n\n\nCette conf√©rence expliquait, en d√©tail, une technique ‚Äúdesign-first‚Äù permettant d‚Äô√©radiquer les erreurs de ‚Äúd√©synchronisation‚Äù entre la documentation d‚Äôune API et son comportement r√©el, tout en g√©n√©rant une partie du code.\n\n\n\n\n\nCette technique peut se r√©sumer en quelques points :\n\n  la documentation OpenAPI est la source de v√©rit√© et d√©crit l‚Äôint√©gralit√© des endpoints de l‚Äôapplication (celle-ci peut √™tre fragment√©e en plusieurs fichiers)\n  un outil (redocly) regroupe tous les documents OpenAPI dans un m√™me fichier\n  un outil (openapi-typescript) g√©n√®re le typage Typescript correspondant au fichier OpenAPI\n  un outil (fastify-openapi-glue) applique le typage Typescript g√©n√©r√© sur le code des diff√©rents endpoints de l‚ÄôAPI Fastify afin de s‚Äôassurer que le code produit par les d√©veloppeurs est conforme √† la documentation\n\n\nEt pour les d√©tails d‚Äôimpl√©mentation, le repository GitHub de la d√©mo d‚ÄôAlexander est disponible ici : https://github.com/AlexBroadbent/openapi-demo\n\nForget TypeScript, Choose Rust to build Robust, Fast and Cheap APIs\n\n\n  Conf√©rence pr√©sent√©e par Zacaria Chtatar (Backend Software Engineer - HaveSomeCode)\n\n\nCette conf√©rence au titre subversif expliquait pourquoi Zacaria, d√©veloppeur Typescript/NodeJS √† ClubMed, en est arriv√© √† s‚Äôint√©resser tr√®s fortement au langage de programmation Rust‚Ä¶ Au point d‚Äôen faire la promotion, en anglais (respect), aux API Days.\n\nLa premi√®re partie de sa conf√©rence parlait du langage de programmation Typescript, en dressant une liste de ses qualit√©s (fullstack, tr√®s largement d√©ploy√© en entreprise, √©cosyst√®me riche, ‚Ä¶) et de ses d√©fauts (gestion d‚Äôerreur optionnelle, typage √©ph√©m√®re, runtime principal peu performant, ‚Ä¶). Cette premi√®re partie s‚Äôest achev√©e par un message clair : ‚ÄúTypescript is not enough‚Äù.\n\n\n\nLa suite et fin de la pr√©sentation, quant √† elle, √©tait une introduction √† Rust.\n\n\n\nBien que nous √©tions surpris de voir que le langage de programmation pr√©f√©r√© des d√©veloppeurs de ces 8 derni√®res ann√©es, et sur lequel les plus grosses entreprises tech du monde misent aujourd‚Äôhui, avait encore besoin d‚Äô√™tre mis en avant en 2023‚Ä¶ Zacaria a effectivement eu raison d‚Äôen remettre une couche, car encore trop peu d‚Äôentreprises fran√ßaises ont pris conscience des avantages qu‚Äôoffre Rust.\n\nCependant, cette 2√®me partie de conf√©rence avait un go√ªt de d√©j√† vu pour nous car un de nos collaborateurs a d√©j√† donn√© le m√™me genre de conf√©rence √† Bedrock durant nos LFT 2 ann√©es auparavant (en partant de PHP plut√¥t que de Typescript)‚Ä¶ Ceci √©tant, gr√¢ce √† nos 2 ans d‚Äôexp√©rience de Rust en production et pour les raisons √©voqu√©es par Zacaria durant son talk, Bedrock est en mesure de t√©moigner sa satisfaction d‚Äôavoir pris le temps de r√©√©crire certaines des API les plus critiques de Bedrock en Rust. Et nous esp√©rons que de nombreuses entreprises oseront suivre notre exemple, afin de permettre aux d√©veloppeurs soucieux de la qualit√© de leur travail comme Zacaria, de disposer de ce merveilleux langage de programmation.\n\nEt si vous ne comprenez toujours pas l‚Äôint√©r√™t de Rust, le talk de Zacaria est disponible en replay sur Youtube.\n\nReal-Life REST API Versioning: Strategies and Best Practices\n\n\n  Conf√©rence pr√©sent√©e par Alexandre Touret (Senior Software Architect - Worldline)\n\n\nLa gestion des versions dans le contexte des API au sein d‚Äôune entreprise peut souvent s‚Äôav√©rer √™tre un d√©fi complexe et d√©licat. En effet, les interfaces de programmation applicative (API) constituent le c≈ìur des interactions entre diff√©rents services et applications au sein d‚Äôun √©cosyst√®me num√©rique. La n√©cessit√© de versionner ces API devient imp√©rative pour garantir une √©volution harmonieuse tout en pr√©servant la compatibilit√© avec les syst√®mes existants.\n\nWorldline fait partie des entreprises avec une architecture technique complexe, exposant √† des clients finaux un mod√®le m√©tier en constante √©volution.\nCe probl√®me fait tr√®s fortement √©cho avec le business model de Bedrock.\n\nDans sa conf√©rence, Alexandre a pr√©sent√© son retour d‚Äôexp√©rience sur comment versionner une API. Et comment l‚Äôexercice est loin d‚Äô√™tre un long fleuve tranquille. Voici les points cl√©s que nous avons retenus :\n\nToutes les APIs ne n√©cessitent pas d‚Äô√™tre versionn√©es\n\nLe versioning c‚Äôest compliqu√©. C‚Äôest un fait. Autant l‚Äôappliquer sur des APIs o√π ce principe est vraiment utile. Il est effectivement peut-√™tre moins utile de versionner une API interne, non expos√©e sur internet ou avec un p√©rim√®tre fonctionnel tr√®s limit√© ou stable.\nVoici les questions propos√©es par Alexandre qui peuvent nous aider dans notre d√©cision : ai-je besoin de versionner ? Combien de versions dois-je g√©rer en parall√®le ? Est-ce que ma plateforme est compatible ? Quels sont les impacts sur ma configuration, mon mod√®le de donn√©es, mon syst√®me d‚Äôauthentification (Alexandre a bien insist√© sur ce point. Il ne faut pas n√©gliger l‚Äôauthentification dans la probl√©matique du versioning) ?\n\nLe versioning s‚Äôapplique aussi sur une architecture en micro service\n\nOn a tendance √† croire que seules ‚Äúles grosses API‚Äù sont concern√©es par le versioning mais Alexandre nous a montr√© qu‚Äôil n‚Äôen est rien. Un micro service, aussi micro (voir macro) qu‚Äôil soit, g√®re √† lui tout seul un p√©rim√®tre fonctionnel. Il est donc l√©gitime qu‚Äôil puisse √©voluer au fil des versions.\n\nComment g√©rer le versioning\n\nPlusieurs solutions, directement dans l‚ÄôURL, via header (plus facile avec une API existante)\nAlexandre a fortement d√©conseill√© d‚Äôutiliser le versioning par content type. √Ä la fois peu lisible et difficilement maintenable.\n\n\n\nL‚Äôimpact du versioning\n\nVersionner une API change en profondeur nos mani√®res de travailler. Et ce √† plusieurs niveaux :\n\n  Le code source, la technique. Il est ind√©niable que le code source ainsi que l‚Äôarchitecture technique de vos projets s‚Äôen verra impact√©s. L‚Äôimpact ne s‚Äôarr√™te pas au code en lui-m√™me, mais sur tout ce qui gravite autour. Nos bases de donn√©es, nos scripts, nos configurations serveur ou nos images docker par exemple\n  Le produit. Nos API exposent le m√©tier de notre produit. L‚Äôimpact n‚Äôest donc pas que technique, mais aussi fonctionnel / produit. Une √©volution de version technique entraine des breaking change et inversement. Il est donc tr√®s important, les √©quipes et produit travaillent de pair pour √©voluer ensemble\n  La livraison. Avoir plusieurs versions d‚Äôun API complexifie la mise en production de cette derni√®re. Il faudra tr√®s certainement revoir nos pipelines d‚Äôint√©gration et de d√©ploiement. Ce point est, lui aussi, √† ne pas n√©gliger et n√©cessitera un travail commun au sein des √©quipes techniques\n\n\nL‚Äôobservabilit√©\n\nAlexandre nous a aussi parl√© de l‚Äôimportance d‚Äôavoir de la visibilit√© sur ce qu‚Äôil se passe en production, et tout particuli√®rement √† la maille des diff√©rentes versions de nos API.\n\nL‚Äôobservabilit√© est quelque chose de plus en plus r√©pandu dans notre m√©tier. Mais le versioning porte le concept encore un cran au-dessus.\n\nIl est primordial d‚Äô√™tre capable d‚Äôassurer une bonne observabilit√© de nos APIs √† la maille :\n\n  De la version\n  Des clients (via des API Keys d√©di√©es)\n\n\nUne bonne observabilit√© est la cl√© pour d√©finir une bonne strat√©gie et √™tre capable d‚Äôanticiper le d√© commissionnement de versions obsol√®tes. Ce point est selon moi tr√®s important.\n\nIl est tr√®s (tr√®s) compliqu√© de maintenir un nombre trop √©lev√© de versions pour une m√™me API. Sans parler d‚Äôobsolescence programm√©e, il faut trouver le bon niveau de maintenance pour √©viter de tomber dans le pi√®ge de la dette technique. Pas facile comme sujet !\n\nPour finir, je tiens √† remercier Alexandre pour sa conf√©rence vraiment int√©ressante. Bourr√©e d‚Äôexemples concrets. J‚Äôai vraiment senti une vraie ma√Ætrise du sujet. Bravo ! Cette conf√©rence est mon coup de c≈ìur de l‚ÄôAPI Days Paris 2023.\n\nOur Ongoing Journey From REST To GraphQL On Android\n\n\n  Conf√©rence pr√©sent√©e par Julien Salvi (Lead Android Engineer - Aircall)\n\n\nDurant cette pr√©sentation, Julien Salvi, Lead Android Engineer chez Aircall nous fait un retour d‚Äôexp√©rience sur la migration de l‚Äô√©quipe Android d‚Äôune API REST √† une API GraphQL. Ce choix a √©t√© motiv√© par plusieurs raisons :\n\n  Une probl√©matique globale sur le scaling de leur API REST\n  L‚Äôefficacit√© et l‚Äôagr√©gation des donn√©es des API GraphQL\n  La recherche d‚Äôune alternative Serverless\n  L‚Äôobjectif de limiter les perturbations pour les clients\n\n\nLeur aventure d√©bute mi 2020 et est toujours en cours.\n\n\n\nPour r√©pondre √† ces demandes, les √©quipes ont dirig√© leur choix vers GraphQL pour cr√©er leur nouvelles API, qui a plusieurs avantages selon Julien notamment:\n\n  La possibilit√© pour les clients de r√©cup√©rer seulement les donn√©es dont ils ont besoin, cela √©vite l‚Äôover-fetching et l‚Äôunder-fetching\n  Les clients peuvent r√©cup√©rer de multiples ressources dans une seule requ√™te\n  GraphQL propose un moyen d‚Äô√©tablir une connection constante entre le client et le serveur, ce qui augmente la scalabilit√© en temps r√©el\n  Le fort typage de GraphQL permet une communication claire, r√©duisant ainsi les erreurs\n  L‚Äôam√©lioration de la performance globale gr√¢ce au batching des requ√™tes\n\n\nL‚Äô√©quipe Android utilise le service AppSync d‚ÄôAWS, facilitant le filtrage, permetttant de faire du realtime, assurer une scalabilit√© et une bonne int√©gration avec ElasticSearch et DynamoDB.\n\nApr√®s les premi√®res migrations vers les API GraphQL, le conf√©rencier insiste sur l‚Äôimportance du monitoring, qui chez eux est pr√©sent que ce soit pour les queries ou les mutations.\n\nVoici les points √† retenir de leur exp√©rience\n\n\n\nPour finir, revenons sur un de leur point √† surveiller, Julien nous √©voque l‚Äôimportance de la collaboration entre les √©quipes front et backend qui est √©galement selon nous tr√®s importante, notamment pour optimiser l‚Äôefficacit√© des API. On peut citer comme actions par exemple, se mettre d‚Äôaccord sur les meilleurs timeout √† adopter sur les API ou aussi cr√©er les sch√©mas OpenApi ensemble.\n\nWhy API Contracts matters\n\n\n  Conf√©rence pr√©sent√©e par St√©ve Sfartz (Principal Architect - Cisco)\n\n\nCette pr√©sentation par le Principal Architect de Cisco nous explique pourquoi, dans une strat√©gie API First, le besoin d‚Äôavoir des contrats ainsi qu‚Äôun cycle de vie de l‚ÄôAPI est primordial pour la coh√©rence du syst√®me.\n\nIls formalisent leur contrats d‚ÄôAPI via OpenAPI Sp√©cification, un standard pour les contrats d‚ÄôAPI REST, en compl√©ment de documents OpenAPI, pour former la d√©finition de l‚ÄôAPI. A c√¥t√© de cette d√©finition, on trouve la gestion du cycle de vie (lifecycle) de l‚ÄôAPI, pour informer des deprecated, du changelog et des Breaking Changes lors des versions majeures (semantic versionning).\n\n\n\nLors de la mise en place de ces contrats pour les API √† cisco, une qualit√© (qu‚Äôils appellent aussi Health Contract) y a √©t√© associ√©e pour avoir une vue d‚Äôensemble de la documentation des API. Ayant environ 2000 API, cette qualit√© ne peut pas √™tre √©valu√©e √† la main au cas par cas, et passe donc par des outils d‚Äôanalyse tels qu‚Äôun linter Spectral, pour √©viter les erreurs et automatiser la g√©n√©ration de ce statut.\n\n\n\nVient ensuite la gestion du drift entre la documentation et le code (par exemple si une annotation est oubli√©e, une route non document√©e) : la v√©rification du drift doit √™tre faite lors de la CI/CD.\n\nLa conf√©rence se conclut sur une question simple : ‚ÄúQuelle est la source de v√©rit√© pour vos API ?‚Äù. La r√©ponse est bien √©videmment le code, mais une documentation g√©n√©r√©e automatiquement permet justement de s‚Äôen rapprocher grandement. A Bedrock, nos API GraphQL ont leur documentation (sch√©ma) g√©n√©r√© automatiquement √† partir du code lors du merge d‚Äôune PR, ce qui permet d‚Äô√©viter les oublis de mise √† jour.\n\nLet‚Äôs bring science into API docs\n\n\n  Conf√©rence pr√©sent√©e par Lana Novikova (Technical Writer - JetBrains)\n\n\nAu cours de cette conf√©rence, Lana Novikova explore comment fusionner les principes scientifiques avec une communication technique efficace dans la documentation des API. \nElle partage √©galement ses connaissances sur la fa√ßon dont les d√©veloppeurs et d√©veloppeuses consomment l‚Äôinformation en ligne, en mettant en lumi√®re les liens avec diff√©rents styles cognitifs, le tout appuy√© par des articles scientifiques. Elle explique de mani√®re concr√®te comment ces principes peuvent √™tre appliqu√©s dans la documentation des API et comment ils peuvent contribuer √† am√©liorer l‚Äôexp√©rience des d√©veloppeurs et d√©veloppeuses.\nSa conf√©rence est une extension d‚Äôun pr√©c√©dent talk qu‚Äôelle a donn√© en 2022 √† la ‚ÄúWrite The Docs Australia‚Äù\n\nLa premi√®re √©tude que nous pr√©sente Lana s‚Äôintitule ‚ÄúPatterns of Knowledge in API Reference Documentation‚Äù. \nElle parle de la nature et de l‚Äôorganisation des connaissances contenues dans la documentation de r√©f√©rence de centaines d‚ÄôAPI au sein de deux plateformes technologiques : Java SDK 6 et .NET 4.0. L‚Äô√©tude a, entre autres, consist√© √† √©laborer une taxonomie des types de connaissances et a pu dresser la liste de 12 types de connaissances distinctes dans la documentation de l‚ÄôAPI :\n\n\n\n√Ä travers cette √©tude, nous pouvons donc √©valuer le contenu de la documentation de notre API en fonction des types de connaissances et ainsi d√©velopper des mod√®les de documentation adapt√©s aux connaissances commun√©ment associ√©es aux diff√©rents types de composants de l‚Äôapi. De plus, aujourd‚Äôhui, des projets comme the good docs project existent et proposent des templates de documentation bas√©s sur ces donn√©es scientifiques.\n\nLa deuxi√®me √©tude expos√©e dans cette conf√©rence a comme titre ‚ÄúHow Developers Use API Documentation: An Observation Study‚Äù. Sa m√©thodologie consiste √† l‚Äôobservation active, via des screencasts et des protocoles verbaux, des activit√©s des personnes participantes pendant le test. Les chercheurs et chercheuses ont √©valu√© le taux de r√©ussite, le temps pass√© sur les t√¢ches et l‚Äôutilisation de la documentation et des cat√©gories de contenu. L‚Äôobjectif principal est d‚Äôobserver comment les d√©veloppeurs et d√©veloppeuses abordent les t√¢ches avec une API qu‚Äôelles ne connaissent pas. Il s‚Äôagit √©galement d‚Äôanalyser comment les d√©veloppeurs et d√©veloppeuses utilisent les ressources d‚Äôinformation propos√©es par la documentation de l‚ÄôAPI. Cela permet de caract√©riser les strat√©gies adopt√©es par les d√©veloppeurs et d√©veloppeuses lorsqu‚Äôelles commencent √† travailler avec une nouvelle API. La conclusion que Lana nous partage est qu‚Äôen moyenne, les personnes participantes ont utilis√© la documentation de l‚ÄôAPI environ 49 % du temps (Min : 31 %, Max : 68 %). La cat√©gorie de contenu √† laquelle il est fait r√©f√©rence le plus souvent est ‚ÄúAPI reference‚Äù, suivie de ‚ÄúRecipes page‚Äù.\n\n\n\n\n\nIl se d√©gage que le temps que les personnes participantes consacrent aux diff√©rentes cat√©gories de contenu varie consid√©rablement d‚Äôune personne √† l‚Äôautre. Sur la base de ces donn√©es, les chercheurs et chercheuses ont d√©fini trois types de personnages de d√©veloppeurs logiciels √† la recherche d‚Äôinformations ainsi que leurs approches lorsqu‚Äôils op√®rent celles-ci: Systematic learners, Opportunistic learners et Pragmatic learners. Pour les personnes curieuses d‚Äôapprofondir le sujet, ces personae sont bas√©s sur une autre √©tude intitul√©e ‚ÄúWhat is an end user software engineer?‚Äù.\n\nLana Novikova conclut en mettant l‚Äôaccent sur le fait qu‚Äôil faut respecter les diff√©rentes strat√©gies adopt√©es par les d√©veloppeurs et les d√©veloppeuses lorsqu‚Äôelles abordent une nouvelle API et nous propose d‚Äôappliquer ces conseils :\n\n  Pour les ‚Äúopportunistic learners‚Äù, donner des exemples de code complets et exhaustifs en donnant la possibilit√© de masquer tout le reste et de relier le texte au code.\n  Pour les ‚Äúsystematic learners‚Äù, fournir des informations importantes de mani√®re redondante et donner des connaissances de base pertinentes.\n  Pour les ‚Äúpragmatic learners‚Äù (et les autres), donner un moyen technique pour commencer √† utiliser une API.\n\n\nJe tiens personnellement √† souligner qu‚Äôil est rare d‚Äôassister √† une conf√©rence aussi compl√®te se basant sur autant de donn√©es scientifiques. Je ressors de cette conf√©rence agr√©ablement surpris de la qualit√© de tout ce qui a √©t√© propos√© et des ressources mises √† disposition. Je vous laisse avec un lien contenant toutes les slides de la pr√©sentation de Lana Novikova qui expliquera bien mieux le propos que mon r√©sum√©. Bravo √† elle et √† son travail, en esp√©rant voir de plus en plus de conf√©rences de ce genre √† l‚Äôavenir.\n\nLe mot de la fin\n\nSi on vous a donn√© envie d‚Äôen savoir plus :\n\n  le site officiel de la conf√©rence\n  la majorit√© des talks sont disponibles sur Youtube\n  certains speakers ont mis √† disposition les slides de leur talk\n\n\nBonnes f√™tes de fin d‚Äôann√©e !\n\n\n\n"
} ,
  
  {
    "title"    : "Bedrock at 2023 AWS re:Invent Las Vegas",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, aws, re:invent, las vegas, 2023",
    "url"      : "/2023/12/18/aws-reinvent-lasvegas-2023.html",
    "date"     : "December 18, 2023",
    "excerpt"  : "AWS re:Invent 2023 was a showcase for GenAI. It was the announcement of Amazon Q, Amazon‚Äôs new AI assistant, that attracted the most interest, designed to meet the specific needs of businesses.\n\nAlongside these announcements, we had the opportunit...",
  "content"  : "AWS re:Invent 2023 was a showcase for GenAI. It was the announcement of Amazon Q, Amazon‚Äôs new AI assistant, that attracted the most interest, designed to meet the specific needs of businesses.\n\nAlongside these announcements, we had the opportunity to attend talks by some of the major players in streaming, such as Prime Video, Peacock, Netflix and Spotify. Their presentations offered valuable insights into their challenges, successes and lessons learned, enriching our own understanding of the sector.\n\nHow Amazon scales resilience to new heights\n\n\n\nOlga Hall, Director of Live Events Availability &amp;amp; Resilience at Amazon Prime Video, and Lauren Domb, Chief Technologist, WWP Federal Financial Services, WW Chaos Engineering Lead at AWS, discussed the importance of resilience in the streaming industry.\n\nThey highlighted the high cost of downtime and the impact on companies‚Äô ability to serve their customers. We‚Äôre talking about an average of $300,000 per hour across the industry.\n\nIn their view, preparing engineering teams for peak loads and streaming is very similar to preparing sports teams for major events. So they created a ‚Äúresilience playbook‚Äù, a series of strategies and tactics inspired by the most successful sports teams, to help their teams become resilient in the face of the unpredictable.\n\nThey also shared their experience of broadcasting Thursday night soccer matches on Prime Video, highlighting how they had to manage unpredictable conditions, such as weather-related match delays, which extended the duration of the peak workload.\n\nAvailability is considered the number 1 feature at Amazon Prime Video. All projects and workflows are listed, budgeted and included in the list of expected features, with availability always at the top of the list.\nTo guarantee the availability of their services, they run load tests three times a week in each region, scaled for peak usage. This enables them to detect bugs before they affect users, and ensure that their systems are ready to handle the highest loads.\n\nAt Bedrock Streaming, we share this focus on resilience. Our applications incorporate circuit breakers, we run load tests very regularly and practice chaos engineering. \nThese practices at Amazon Prime Video offer an interesting perspective on how we might further improve our resilience at Bedrock Streaming. We were particularly impressed by the resilience score per application approach. This is definitely something we‚Äôll be exploring in conjunction with a feature flipping by service approach.\n\nSurviving overloads: How Amazon Prime Day avoids congestion collapse\n\nJim Roskind, Distinguished Engineer at Amazon.com, and Ankit Chadha Solution Architect at AWS shared their experiences and knowledge at the conference. Formerly of Google, where he oversaw Google Chrome metrics and performance, Jim now works for Amazon with a singular mission: to buy less of AWS products. This approach, aimed at reducing IT costs, is supported by AWS, which wants to teach all its customers how to optimize their spending.\n\nHowever, this quest for efficiency and cost reduction is not without risks. One of Jim‚Äôs main concerns is congestion collapse, a phenomenon that can lead to a drop in productivity and even paralyze a system. To help us understand this phenomenon, Jim presented a series of pragmatic examples and theories on congestion collapse.\n\nWhat is congestion collapse?\n\nCongestion collapse occurs when demand exceeds a system‚Äôs capacity. This leads to the build-up of queues, reduced productivity and, in extreme cases, the complete cessation of productive work. This phenomenon is not uncommon and can occur in a variety of situations, from highways to web servers.\n\nJim shared some examples to illustrate this phenomenon, the main one was about \nAmazon Prime Day 2018. Even giants like Amazon aren‚Äôt immune to congestion collapse. \nHe hadn‚Äôt anticipated customer interest in a particular product. They had a massive demand for a product display and the service ended up being unavailable. At this point, traffic increased again sharply as customers began reloading the page. Above all, it highlights the fact that, as their services are slow to respond, the answers are no longer relevant, leaving us with a system that is 100% loaded and no longer doing anything useful.\n\nThese examples show that congestion collapse is a real problem that requires constant attention and planning.\n\nAt bedrock we‚Äôve also encountered congestion collapse. For example, during busy events such as soccer matches, we‚Äôve already encountered the case where a massive influx of users would cause the platform to become unavailable, at which point all the users would press F5 at the same time, drastically increasing the traffic on an already struggling platform. Moreover, requests are queued up and we end up answering requests issued 1 or 2 min earlier, and therefore answering people who have surely already left.\n\nJim mentions this issue, which has happened at amazon.com.\n\nThey were able to implement 2 solutions in particular.\n\n\n  \n    The first, when the servers can no longer respond, is to display a page with a message warning the user to wait a while and try again in a while. This had a surprisingly noticeable effect. Users were no longer repeatedly pressing the F5 button.\n  \n  \n    Secondly, they have implemented mechanisms to detect massive retries and thus avoid transmitting traffic to their backend. In particular, they have implemented this in the WAF service.\n  \n\n\nAt Bedrock Streaming we already display a page in case of trouble, but we can improve it to suggest to the user to wait before retrying. Moreover, we use Cloudfront and WAF on almost all our services. We have a few rules on WAF that allow us to deny traffic that seems illegitimate, but we‚Äôre going to work on a new rule to avoid transmitting untimely user retries in the event of an overloaded system.\n\nNetflix caching\n‚ÄúWho in this room is a netflix user?‚Äù (90% of the room raises its hand): no doubt Prudhviraj Karumanchi, software engineer &amp;amp; Sriram Rangarajan, Senior Distributed Systems Engineer at Netflix, conference speakers, know how to introduce their talk and remind us that they are the market giants. During their conference, they presented how Netflix uses the EVCache solution for multi-region cache replication.\n\nNetflix likes to say that one of its missions is to spread joy. This involves two aspects: offering users a fully personalized homepage and \nbenefiting from a scalable, low-cost architecture (so that Netflix‚Äôs techies are happy too).\n\n\n\nThis conference addressed many of the issues we are familiar with: a home page that calls on a large number of microservices to display content to the user, scaling issues, cost management‚Ä¶\n\nThe heart of the conference detailed the architecture implemented by Netflix teams to replicate cache across multiple regions using EVCache, Kafka and SQS.\n\nIt was also interesting to see that at Netflix, as with us, it‚Äôs important to build with costs in mind: after analyzing the costs of their inter-region traffic, they finally decided to remove their network load-balancer to make their architecture more cost-efficient.\n\nA long part of the conference was dedicated to the observability of the replication stack: our teams have also done a lot of work on this issue in recent years, so it was interesting to compare our practices on the subject of observability.\n\nWhile we don‚Äôt yet work on the same scale as Netflix, attending this conference allowed us to reinforce our idea that caching is essential in the architecture of a platform such as Bedrock Streaming. And it gives us new ways for reflection‚Ä¶\n\nTakeaways from Reinvent2023. The newcomers‚Äô point of view\n\nIt‚Äôs often said that ‚Äúwhat happens in Vegas stays in Vegas‚Äù, but for some of us, this edition of Re:invent was our first time at a conference of this magnitude. So it‚Äôs hard not to share some of our feedback with you!\n\nFirst of all, a word about Vegas, the city of excess which hosts Re:invent every year, and which has been transformed for us into an open-air R&amp;amp;D ground! From Fremont Street to the brand-new Sphere, there are screens EVERYWHERE in this city, which has aroused our curiosity as video streaming professionals‚Ä¶ \n\n\nOn the eve of Reinvent, the city is transformed: tourists give way to speakers from all over the world, advertising screens talk of nothing but cloud solutions, scalability and artificial intelligence‚Ä¶ It‚Äôs an incredible phenomenon to behold!\n\nWith over 70,000 participants, nothing can be left to chance in the organization of Reinvent. It‚Äôs impressive to see (Vegas traffic jams aside) how smoothly the whole conference runs. A sort of ‚Äúhuman load-balancing‚Äù has even been arranged so that each speaker can have lunch in record time!\n\nThe keynotes organized during Reinvent (5 in all) are events within events! The expected crowds are so huge that several overflow rooms are planned in addition to the main ballroom where the speaker is based.\nWe were able to attend two of them: Adam Zelipsky‚Äôs (CEO of aws) - for which the speakers were greeted by a fantastic rock band at 7:30 a.m., a guaranteed wake-up call - and Werner Vogels‚Äô (CPO &amp;amp; VP of amazon.com), where this time the welcome was provided by a fantastic string quartet.\n\nWhile Adam Zelisky‚Äôs conference was undoubtedly a masterpiece (production, new services announced, influential clients such as Lidia Fonseca, chief digital and technology officer at Pfizer, speaking), it was Dr. Vogels‚Äô conference that impressed us the most.\n\nBased on the laws of the ‚Äúfrugal architect‚Äù, this keynote spoke to everyone: technicians, business people, product managers‚Ä¶ It took up the elementary concepts of what should motivate our design of IT solutions: cost awareness, the indispensable balance between commercial and technical needs, or the danger of never questioning oneself, quoting Grace Hopper: ‚ÄúOne of the most dangerous phrases in the English language is: ‚ÄòWe‚Äôve always done it this way‚Äô‚Äù\nWe encourage you to watch the replay of this keynote, a must-see!\n\n\n\nBeyond the gigantism of the event, let‚Äôs look at what we‚Äôve taken away from our participation in Reinvent. Attending a conference is always an opportunity to take a step back on our current work themes: we attended many conferences on resilience, scalability and new architectures:\n\n  it enabled us to compare our practices with those of major players in the sector (Prime, Spotify, Peacock‚Ä¶),\n  to transpose these themes into a context completely different from our own (‚Äú1.5 million requests per second‚Äîa story from the Brazilian elections‚Äù)\n  or to appreciate the work we‚Äôve done over the year, which sometimes goes beyond what‚Äôs presented at conferences (‚ÄúUse new IAM Access Analyzer features on your journey to least privilege‚Äù)\n\n\nFor all these reasons, no doubt Bedrock Streaming teams will be betting on Reinvent again next year!\n"
} ,
  
  {
    "title"    : "Android 14 is out",
    "category" : "",
    "tags"     : " android, mobile, google, 14",
    "url"      : "/2023/12/05/android-14.html",
    "date"     : "December 5, 2023",
    "excerpt"  : "Here‚Äôs what it means for users and developers.\n\nWith each new OS version, new things, upgrades, deprecations and changes are introduced, affecting the way we use and develop our apps.\nGoogle keeps going in the direction of more privacy, more acces...",
  "content"  : "Here‚Äôs what it means for users and developers.\n\nWith each new OS version, new things, upgrades, deprecations and changes are introduced, affecting the way we use and develop our apps.\nGoogle keeps going in the direction of more privacy, more accessibility and more control over what the apps can do to maximize security and integrity.\nAndroid 14 is no exception and here‚Äôs what I compiled on different topics that I will try to vulgarize to keep everyone on board.\n\nTechnical\n\nTechnical changes build over features and APIs already introduced in previous versions, mostly Android 12 and 13.\nThey tend to modernize tools by catching up with some Java features and semantics, helping manufacturers and improving the developers‚Äô IDE to embrace those changes.\nDue to the nature of the changes, this is the topic that has to remain‚Ä¶technical, sorry for that.\n\n\n  Mobile screens are getting bigger with more ratios to support, we‚Äôre moving further and further away from the binary world of phone vs tablet. To ensure the best experience on this wide range of devices, Android 14 introduces the Large Screen Compatibility Mode to help manufacturers improve the experience on their devices.\n  Updates to OpenJDK17 may require a bit of attention from apps using Regex that are not close enough to openJDK‚Äôs new semantics, throwing exceptions when confronted to an invalid groupe reference.\n  Generating a UUID from a string sees the validation become stricter and will now lead to exceptions due to deserialization issues. More than ever, it‚Äôs time to unit test UUID generation.\n  A bit of additional ruling may be needed to fix Proguard issues when shrinking / obfuscating code involving the ClassValue class coming with API34.\n  The new Back APIs are now strengthened by built-in animations and support for custom ones.\n  Making the ForegroundService type explicit is now mandatory, if the implementation was already properly done back in the Android 10 days when it was introduced, congratulations, nothing to do here.\n  Foreground services are also encouraged to be migrated to user-initiated jobs. A new RUN_USER_INITIATED_JOBS permission is introduced and new methods on the JobInfo builder allow to set the userInitiated() status along with the estimated amount of bytes the job will expect from the network. Scheduling the job is now done with the app foregrounded and the notification icon system remains the same so the user knows something is going on even if the app is backgrounded post launch.\n\n\nBattery and performance\n\nWithout a single ounce of surprise, Google continues its effort to improve battery life and takes steps towards sanctioning bad actors that publish battery-draining or unstable apps.\nToday, not crashing is no longer enough, developers should take steps to push their app to their full potential and that means power management and performance monitoring.\n\n\n  Bad behaviours like ANRs (screen freezes) or background crashes now more aggressively flag the guilty apps and put them at the bottom of the priority list where apps are fighting for resources, meaning they‚Äôll also be the first to go if the system needs some. No more filtering out ANRs and non-fatal crashes on Crashlytics, everything matters now.\n  While on the subject of fighting for resources, let‚Äôs also note that now, context-registered broadcasts are now queued when the app is backgrounded and the system will deliver them when the app is awake or system conditions allow it.\n  Another change to the cached state (aka when the app is backgrounded) impacts background tasks that can no longer be triggered unless one of the app components is awake. This change pushes devs to use framework‚Äôs JobScheduler and WorkManager more as they aren‚Äôt impacted by this change.\n  Still with Jobscheduler, jobs don‚Äôt just fail silently anymore if they don‚Äôt respond in time but trigger an ANR, it is advised to move to WorkManager with its out of the box async support.\n  If a job requires a special network state to be triggered, the ACCESS_NETWORK_STATE permission is now mandatory. Without it, a SecurityException will be raised.\n  Intents keep getting more and more headache prone as the implicit and pending intents now can only be delivered to exported components. If you need to reach an unexported component, explicit intent is your go-to solution. Note that mutable pending intents now need to specify a component or it will throw an exception.\n\n\nNotifications\n\nFinding the right balance between informative presence and in-your-face nuisance has always been a challenge for notifications and it seems Google keeps pushing to make them less invasive and easier for the user to dismiss or delay them.\n\n\n  The Fullscreen Intent notifications that we see when our clock rings or when we receive a phone call are luckily already rarely used.\nThey are now more restricted and available only to apps declaring Call or Alarm features, meaning we shouldn‚Äôt see bad actors abusing this feature that would allow them to bypass the lock screen amongst other things.\n  Non-dismissible foreground notifications are now dismissible in some cases but will remain non-dismissible\n    \n      on top of the Lock Screen to prevent it from being swiped by anyone accessing a device behind the owner‚Äôs back.\n      from the Clear all feature to prevent misclicks.\n    \n  \n\n\nPrivacy and security\n\nThis is, once again without surprise, where a lot of the changes happen and it is aligned with Google‚Äôs vision and goals when it comes to give users back the control of their data and permissions.\nSome of them seem so obvious that it‚Äôs surprising to see them in action only now. Maybe the EU pressure with GDPR starts to pay off? Maybe‚Ä¶\n\n\n  Android 14 introduces new places where the data sharing purposes are displayed. Until now, we could only check them from the PlayStore app page. \nNow, it will also be displayed in the runtime permission popups, starting with those related to location to remind why the data is necessary and with whom it will be shared.\n  It will now be impossible to install apps that don‚Äôt target at least the API 23 to prevent bad actors from exploiting security breaches discovered inside older Android versions.\nBe aware that installed apps won‚Äôt be removed and the system won‚Äôt warn you when starting one of those apps, maybe a new feature for Android 15?\n  Dynamic Code Loading now requires to flag the file as read-only to avoid any tampering or code injection. In any case, DCL should be avoided when possible and only trusted files should obviously be loaded this way.\n  When saving a file inside the app storage, the system attributes to the file an owner id, this id being the app package name that saved it. \nThis feature allows apps to know which file they can open without requesting the external read permission. The issue was that by querying this id, other apps could access the owner ids that weren‚Äôt them and deducting the owner‚Äôs installed apps list. \nTo fix this, the name is now redacted, increasing again a little bit the user data protection, the list of the installed apps being considered a sensitive data by Google.\n  If an app features screen or audio recording, it is now required to be granted the user consent to do so before each session start and therefore be able to handle permission denied scenarios.\n  Zip files are also impacted as a fixed vulnerability with the path transversal reading now triggers an exception if some characters are found inside it. (Contains .. Or starts with /)\n  Even though already required, the BLUETOOTH_CONNECT permission was not yet enforced to access the profile state, it is now the case.\n  Users are no more required to grant access to all images or videos to share or display a single media, Android 14 now upgrades the permission popup with an option to select only the media the app is allowed to access.\n  Apps can now react to a user screenshot event, they can‚Äôt manipulate the content but developers can now add a callback bound to the activity lifecycle. \nSensitive screens should still be protected with the secure flag.\n  Starting activities from the background with a pending intent or through another app in the foreground now requires the app to opt-in to this feature inside said activity and is no longer a default behaviour.\n\n\nAccessibility\n\nIt is no secret that mobile devices are now owned by more and more people every year, which includes people with a range of disabilities or personalities that may make an app usage more challenging.\nAndroid 14 helps them with new and upgraded features to ease their journey with a mobile device.\n\n\n  A step is taken towards low-vision users‚Äô direction, the changes and impacts to the font scaling should be negligible to developers already properly using SP as their size units but a full testing pass with the scaling enabled should be scheduled to be safe and tweak improvable screens.\n  New tools inside Android studio are added to help developers handling per-app language more efficiently and easily.\n  Grammatical Inflection API is introduced, offering developers working on apps with gendered languages new tools. It adds a layer of complexity to the strings files by having three gender-files by gendered language. In those files are added only the strings affected by gender inflections like Vous √™tes d√©connect√© for masculine, Vous √™tes d√©connect√©e for feminine or La d√©connection est effective for neutral in french. More work for developers and translators but an overall better experience for users.\n\n\n\n\nAll in all, Android 14 is an update faithful to the Google roadmap. \nUsers today are very different than users 10 years ago. They care more about their data and their privacy; the Mobile ecosystem and business is also a lot more professional.\nIt‚Äôs important for us developers to be aware of those changes in order to continuously improve the experience, be it related to our core business or simply to keep the user engaged in a safe environment.\n\nWhen this article is released, Android 14 should be freshly out and developer teams hands deep in the migration tasks.\nI hope you enjoyed the information and see you soon for more Android related articles!\n\n\n  Changes potentially affecting all apps\n  Changes affecting apps targetting Android 14\n  New features introduced by Android 14\n  APIs changelog\n  Overview\n\n"
} ,
  
  {
    "title"    : "STOP √† l&#39;espionnage ! Comment dispara√Ætre d&#39;internet ? #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/stop-a-l-espionnage-comment-disparaitre-d-internet",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  Vous en avez marre de vous sentir traqu√© sur Internet ? üòí Nous aussi ! Mais est-il r√©ellement possible de nos jours de devenir un v√©ritable ninja digital ? ü•∑üèª Et si pour prot√©ger votre vie priv√©e en ligne, il fallait tout d‚Äôabord comprendre qui...",
  "content"  : "\n  Vous en avez marre de vous sentir traqu√© sur Internet ? üòí Nous aussi ! Mais est-il r√©ellement possible de nos jours de devenir un v√©ritable ninja digital ? ü•∑üèª Et si pour prot√©ger votre vie priv√©e en ligne, il fallait tout d‚Äôabord comprendre qui a acc√®s √† vos donn√©es et ce qu‚Äôils peuvent faire avec ?\n\n\n\n  Nous allons vous apprendre comment √™tre un vrai pro de la confidentialit√© et de la s√©curit√© des donn√©es, mais surtout comment devenir anonyme en ligne et √©viter les curieux‚Ä¶ üëÄ Sortez vos loupes et suivez-nous dans cette enqu√™te pour reprendre le contr√¥le de votre vie num√©rique !\n\n\n\n  Et si vous √™tes chanceux, nous vous d√©voilerons peut-√™tre quelques secrets de ninja pour √©chapper aux espions ! Venez nous rejoindre et apprenez comment devenir le ma√Ætre du camouflage num√©rique ! üò∂‚Äçüå´Ô∏è\n\n\nPar Etienne Idoux &amp;amp; Micka√´l Alves\n"
} ,
  
  {
    "title"    : "Le futur du web est sur la p√©riph√©rie du r√©seau #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/le-futur-du-web-est-sur-la-peripherie-du-reseau",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  Le mot est apparu sur toutes les documentations ces derniers mois : les cloud providers comme Vercel, Netlify ou AWS proposent leur version des ‚ÄúEdge Functions‚Äù, alors que les frameworks Javascript comme Next, Nuxt ou encore Astro mettent en av...",
  "content"  : "\n  Le mot est apparu sur toutes les documentations ces derniers mois : les cloud providers comme Vercel, Netlify ou AWS proposent leur version des ‚ÄúEdge Functions‚Äù, alors que les frameworks Javascript comme Next, Nuxt ou encore Astro mettent en avant leur support des ‚ÄúEdge API Routes‚Äù ou du ‚ÄúSSR on the Edge‚Äù. Mais qu‚Äôest-ce donc que tout cela ?\n\n\n\n  √Ä l‚Äôinstar des CDNs pour les fichiers statiques, ce nouveau paradigme consiste √† ex√©cuter le code serveur au plus pr√®s des utilisateurs (‚Äúthe Edge‚Äù). On peut ainsi obtenir du contenu dynamique √† la vitesse du statique, avec des usages comme le SSR, l‚Äôauthentification ou l‚ÄôA/B Testing √† la p√©riph√©rie du r√©seau.\n\n\n\n  Je vous propose de d√©couvrir, chiffres √† l‚Äôappui, les performances que l‚Äôon peut atteindre sur certains cas d‚Äôutilisation, et les situations dans lesquelles il n‚Äôest au contraire pas int√©ressant de l‚Äôutiliser. Avec √ßa, plus d‚Äôexcuses si votre site n‚Äôest pas ‚Äúblazing fast‚Äù.\n\n\nPar Julien Sulpis\n"
} ,
  
  {
    "title"    : "Jeux vid√©o, websocket et binaire: temps r√©el efficace pour navigateur #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/jeux-video-websocket-et-binaire-temps-reel-efficace-pour-navigateur",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  Connaissez vous les websockets ? La techno id√©ale pour coder des exp√©riences temps r√©el dans le navigateur et que j‚Äôutilise dans tout mes projets collaboratifs et jeux multijoueur ?\n\n\n\n  Je vous propose de nous int√©resser √† son fonctionnement, ...",
  "content"  : "\n  Connaissez vous les websockets ? La techno id√©ale pour coder des exp√©riences temps r√©el dans le navigateur et que j‚Äôutilise dans tout mes projets collaboratifs et jeux multijoueur ?\n\n\n\n  Je vous propose de nous int√©resser √† son fonctionnement, et √† la fa√ßon dont on peut mettre en place cette communication client/serveur performante, en √©changeant directement en binaire.\n\n\n\n  On s‚Äôamusera √† les voir prendre vie tous ensemble dans une d√©mo live‚Ä¶en GO !\n\n\nPar Thomas Jarrand\n"
} ,
  
  {
    "title"    : "Comment ne pas jeter son application Frontend tout les deux ans ? #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-ne-pas-jeter-son-application-frontend-tout-les-deux-ans",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  Bonnes pratiques pour la maintenance d‚Äôune application web\n\n\n\n  Refaire son front tous les 2 ans, c‚Äôest devenu une pratique plut√¥t courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la m√™me codeb...",
  "content"  : "\n  Bonnes pratiques pour la maintenance d‚Äôune application web\n\n\n\n  Refaire son front tous les 2 ans, c‚Äôest devenu une pratique plut√¥t courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la m√™me codebase et √ßa depuis plus de 7 ans! En plus, ce n‚Äôest pas une petite application puisqu‚Äôil s‚Äôagit de 6play et de salto.\n\n\n\n  Vous pourriez vous dire: ‚ÄúOh les pauvres, maintenir une application vieille de presque 10 ans √ßa doit √™tre un enfer !‚Äù\n\n\n\n  Rassurez-vous, ce n‚Äôest pas le cas ! Nous avons tous travaill√© sur des projets bien moins vieux mais sur lesquels le d√©veloppement de nouvelles fonctionnalit√©s √©tait bien plus p√©nible. Quel est notre secret ? C‚Äôest ce que vous allez d√©couvrir pendant ce talk.\n\n\n\n  Automatisation des t√¢ches courantes, gestion de la dette, testing et architecture seront des sujets abord√©s.\n\n\nPar Florent Dubost &amp;amp; Antoine Caron\n"
} ,
  
  {
    "title"    : "Comment faire de votre vie un BlackFriday permanent #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft",
    "url"      : "/comment-faire-de-votre-vie-un-blackfriday-permanent",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  A travers ce talk, j‚Äôaimerai vous initier √† l‚Äôint√©r√™t de la n√©gociation et les diff√©rentes techniques que l‚Äôon peut utiliser pour bien n√©gocier.\nL‚Äôid√©e est de vous donner des cl√©s simples et efficaces pour aborder une situation fr√©quente qui pe...",
  "content"  : "\n  A travers ce talk, j‚Äôaimerai vous initier √† l‚Äôint√©r√™t de la n√©gociation et les diff√©rentes techniques que l‚Äôon peut utiliser pour bien n√©gocier.\nL‚Äôid√©e est de vous donner des cl√©s simples et efficaces pour aborder une situation fr√©quente qui peut parfois √™tre source de stress ou de tensions.\n\n\nPar Thomas Sontag\n"
} ,
  
  {
    "title"    : "Swift Concurrency in a Nutshell",
    "category" : "",
    "tags"     : " swift, apple, xcode",
    "url"      : "/2023/11/14/swift-concurrency-in-a-nutshell.html",
    "date"     : "November 14, 2023",
    "excerpt"  : "As modern apps grow in complexity and features, the need for multitasking to enhance the user experience becomes evident. Whether processing large datasets or querying multiple systems over the network, concurrency is essential.\n\nThis article pres...",
  "content"  : "As modern apps grow in complexity and features, the need for multitasking to enhance the user experience becomes evident. Whether processing large datasets or querying multiple systems over the network, concurrency is essential.\n\nThis article presents a concise, yet comprehensive overview of Swift‚Äôs Concurrency, highlighting its key features and core concepts. Swift‚Äôs approach to concurrency provides several benefits:\n\n\n  Simplified code that‚Äôs easier to reason about and maintain\n  A noticeable reduction in bugs and performance issues\n  Ensured app responsiveness\n\n\nBefore delving into Swift‚Äôs concurrency paradigms,  let‚Äôs familiarize ourselves with foundational terminology.\n\nConcurrency\n\nConcurrency is about structuring your code so that tasks can be executed independently. It provides mechanisms for synchronization, communication, and coordination between units of work to avoid race conditions and ensure proper execution. However, concurrency doesn‚Äôt imply parallel execution; the actual mode of execution is determined separately.\n\nDesigning your code effectively for concurrency makes adding parallelism nearly free.\n\nParallelism\n\nParallelism is the simultaneous execution of tasks across multiple processing units, guaranteeing genuine concurrent progression of operations. It‚Äôs a specific form of concurrency where tasks are actually executed at the same time.\n\nStructured Concurrency\n\nTraditionally, developers had to manually manage threads, locks, and callbacks, leading to code that is difficult to manage and error prone. Even with a lot of discipline, it was really hard to get right as the cognitive load was so high.\n\nStructured concurrency is a programming paradigm providing a higher level of abstraction, allowing you to manage concurrency in a structured and organized way. It simplifies the task management and their dependencies, making it easier to write correct and efficient concurrent code.\n\nSwift Concurrency\n\nOne prime objective of Swift is safety, by removing undefined behaviors such as null pointer, array out-of-bounds, and integer overflows. Until recently, multithreading remained a weak spot in Swift‚Äôs safety features. Developers had to rely on Grand Central Dispatch, which wasn‚Äôt inherently designed to help with concurrency-related pitfalls like thread explosion.\n\nSwift Concurrency fills this gap, enhancing the language‚Äôs overall safety by integrating the Task abstraction from Structured Concurrency, the async/await pattern and Actors for data isolation.\n\nTask\n\nWith Swift Concurrency, Tasks become the primary unit of work and offer three core functionalities:\n\n\n  Carry scheduling information such as priority\n  Act as handles for task management\n  Hold user-defined and task-local data\n\n\nThese attributes make tasks the cornerstone that guides the execution model in running, prioritizing, and suspending or canceling jobs. Every asynchronous function operates within a task. Tasks also serve as the entry point for synchronous functions to execute asynchronous code.\n\nChild Tasks\n\nA child task is a task spawned by another task, known as the parent task. Child tasks inherit some properties from their parent, such as priority levels, but are their own individual units of work that can be scheduled independently. One important characteristic of child tasks is their lifetime is tied to their parent task; if the parent task is cancelled, all its child tasks are also cancelled. This ensures a structured way to manage and reason about concurrent tasks in your code. However, cancellations do not propagate upward, requiring parent tasks to manually check the status of their child tasks.\n\nChild Tasks are created using Task Groups as we will see later.\n\nasync / await\n\nThe async/await pattern simplifies asynchronous code development, allowing a sequential-like structure, akin to traditional synchronous functions.\n\nUse the async keyword to mark functions that perform asynchronous work.\n\nfunc performRemoteOperation(_ url: URL) async throws -&amp;gt; ResultType\n\n\nThe await keyword indicates potential suspension points in your code, which are necessary for running async functions. These markers also offer developers insight into the behavior and control flow of asynchronous operations. At these suspension points, the system can pause the current task to await the completion of an asynchronous operation.\n\nfunc processRemoteData() async throws -&amp;gt; Resource {\n    let data = try await performRemoteOperation() // waiting for performRemoteOperation() to complete\n    let resource = await process(data)\n    return resource\n}\n\n\nError propagation\n\nAs you may have noticed in the previous examples, Swift‚Äôs concurrency model seamlessly integrates with the language‚Äôs native error-handling mechanism. This brings several advantages over the old completion-based concurrency:\n\n\n  Clarity: Errors are propagated in a way that is consistent with how they are handled in synchronous Swift code. This means you don‚Äôt have to learn a new error-handling paradigm when moving to concurrent code.\n  Safety: Because errors can be propagated and caught, you can handle exceptional conditions gracefully, making your concurrent code more robust.\n  Maintainability: With explicit error types and propagation, debugging and maintaining concurrent code becomes easier. You can clearly understand what types of errors your asynchronous functions can throw and handle them appropriately.\n\n\nActors\n\nSwift‚Äôs Structured Concurrency is designed to address data races in concurrency for functions and closures. However, working concurrently usually involve dealing with shared mutable state, requiring tedious manual synchronization.\n\nTo address this, Swift introduces Actors, a new reference type designed to encapsulate states within a specific concurrency domain, ensuring data isolation and thread-safe operations. Actors not only enhances safety and efficiency but also align with Swift‚Äôs established patterns and features.\n\nTo create an Actor, just use the keyword actor.\n\nactor MessageThread {\n    let playerTag: String\n    var messages: [String]\n\n    init(playerTag: String, previousMessages: [String]) {\n        self.playerTag = playerTag\n        self.messages = previousMessages\n    }\n}\n\n\nActors are similar to class, the main difference is that they protect their mutable data from data races by implementing Actor Isolation.\n\nActor Isolation\n\nActor Isolation enforces that any mutable properties managed by an actor can only be modified using self.\n\nextension MessageThread {\n    func send(_ message: String, to other: MessageThread) {}\n        messages.append(message)\n        other.messages.append(message) ... // error: trying to access another actor mutable property\n        print(other.playerTag) // works fine as read only\n    }\n}\n\n\nIn the example above, the compiler complains when trying to modify the mutable property of another actor (cross-actor reference). However, accessing read-only properties poses no issue.\n\nTo address this, you can introduce another function allowing the other MessageThread actor to modify its own state.\n\nextension MessageThread {\n    func send(_ message: String, to other: MessageThread) async {\n        messages.append(message)\n        await other.receive(message)\n    }\n    \n    func receive(_ message: String) {\n        messages.append(message)\n    } \n}\n\n\nWith these modifications:\n\n\n  The send function is now async, because of the await suspension point required to call the receive function in the other actor‚Äôs asynchronous context.\n  While the receive function isn‚Äôt explicitly marked as async (since it doesn‚Äôt have suspension points and operates synchronously), actor isolation in Swift ensures functions behave as implicitly async when invoked from outside their own actor‚Äôs context.\n\n\nActors ensure safe execution by maintaining their own dedicated serial executor internally. Messages sent to an actor are termed partial tasks. While processing these tasks, the order of their execution is not strictly guaranteed, as priorities of partial tasks influence the sequence in which they are tackled.\n\nLastly, you can do a cross-actor reference on a mutable property with an asynchronous call as long as it‚Äôs read only.\n\nfunc getMessages(thread: MessageThread) async {\n    print(await thread.messages) // works\n}\n\n\nSendable\n\nFinally, to make Actors truly isolated we need to prevent cross-actor references from inadvertently sharing mutable state. The Sendable protocol was introduced to ensure that types shared across actor boundaries don‚Äôt introduce data races. This protocol doesn‚Äôt provide or dictate specific code behavior, but is leveraged by the compiler to ensure the safety of the concurrent code.\n\nHere are types that can conform to Sendable (some implicitely do):\n\n\n  Value types\n  Actors\n  final classes with immutable and sendable properties (and without superclass).\n  Functions and closures when using the @Sendable attribute.\n\n\nFor a detailed explanation, please refer to the official Apple documentation.\n\nGlobal Actor\n\nGlobal actors are Actors providing a way to extend actor isolation to global and static variables, safeguarding them from concurrent access issues. Global actor can be referenced from anywhere in the program. A common global actor is the MainActor which allows you to execute your code on the main thread.\n\nIn Practice\n\nTheory covered, let‚Äôs dive into practical use-cases.\n\nCall Async Functions Sequentially\n\nWhile calling functions sequentially is straightforward in synchronous code, achieving the same in asynchronous code used to be cumbersome, often leading to the Pyramid of doom. Swift‚Äôs concurrency model radically simplifies this by using the async/await paradigm.\n\nfunc fetchInfo() async throws -&amp;gt; UserInfo {...}\nfunc fetchImg() async -&amp;gt; ProfileImage {...}\nfunc fetchAct() async -&amp;gt; UserActivity {...}\nfunc saveDB(_ info: UserInfo, _ img: ProfileImage, _ act: UserActivity) async throws {...}\n\nfunc backupUserProfile() async throws {\n    let info = try await fetchInfo()\n    let img = await fetchImg()\n    let act = await fetchAct()\n    try await saveDB(info, img, act)\n}\n\n\nThe use of await ensures each async function completes before the next starts. This sequential execution offers the readability of synchronous code while retaining the benefits of asynchronicity.\n\nCall Async Functions in Parallel\n\nWhen async functions are independent, running them in parallel can save time.  async let allows you to achieve this with minimal code changes. Consider the previous example, modified to execute tasks concurrently:\n\nfunc backupUserProfile() async throws {\n    async let info = try fetchInfo()\n    async let img = fetchImg()\n    async let act = fetchAct()\n    try await saveDB(info, img, act) // Await the results of async let tasks\n}\n\n\nasync let spawns child tasks, sets placeholders on the variables, and allows the code to continue running until it needs the results, which are obtained using await at the end of the function.\n\nCall Async Functions from a Synchronous Function\n\nTask serves as a bridge between synchronous and asynchronous code, enabling you to use async-await without requiring the entire function chain to be asynchronous.\n\nfunc onSavePressed() {\n    Task {\n        do {\n            try await backupUserProfile()\n        } catch {\n            print(&quot;Error backing up profile: \\(error.localizedDescription)&quot;)\n        }\n    }\n}\n\n\nAn alternative is Task.detached. This creates a new top-level task and decouples it from its originating context, allowing it to operate on a different Actor and with a different priority. A typical scenario involves initiating a task from the main thread to execute it on a different thread.\n\nTerminology: Unstructured Concurrency\n\nCreating a standalone Task is known as an Unstructured Task, as it lacks both a parent task and child tasks.\n\nUnstructured Tasks are useful for:\n\n\n  Calling a task from a non-async context\n  Tasks that must persist beyond a specific scope\n\n\nNote: Swift‚Äôs use of the terms Structured and Unstructured Concurrency relates only to the hierarchy of Tasks and should not be confused with the broader concept of Structured Concurrency described in the introduction.\n\nQuoting the swift documentation.\n\n\n  Structured concurrency: Tasks arranged in a hierarchy. Each task in a task group has the same parent task, and each task can have child tasks. Although you take on some of the responsibility for correctness, the explicit parent-child relationships between tasks let Swift handle some behaviors like propagating cancellation for you, and lets Swift detect some errors at compile time.\n\n  Unstructured concurrency: Unlike tasks that are part of a task group, an unstructured task doesn‚Äôt have a parent task. You have complete flexibility to manage unstructured tasks in whatever way your program needs, but you‚Äôre also completely responsible for their correctness.\n\n\nParallel Processing with Task Groups\n\nWhile async let may suffice for handling a limited number of tasks, Task Groups are recommended when a structured approach to parallelism is desired. Here‚Äôs an example that employs Task Groups along with an accumulator to safely process an array of data in parallel.\n\nlet processedData = await withTaskGroup(of: Data.self, returning: [Data].self) { taskGroup in \t\n    // Create a new Task within the Task Group for each item   \n    for item in items {\n        taskGroup.addTask(priority: .background) { // Create a new Task within the Task Group\n            await process(item)\n        }\n    }\n\n    var allData: [Data] = []\n    // Asynchronously collect the task results as they complete\n    for await result in taskGroup {\n        allData.append(result)\n    }\n\n    return allData\n}\n\n\nThis code initializes a Task Group and spawns a child task for each item with .background priority. Then an AsyncSequence for await loop asynchronously collects and stores the task results in the allData accumulator as they complete.\n\nCooperative Cancellation\n\nTo enable cancellation within Task Groups, tasks must be built for Cooperative Cancellation, which means the task periodically checks whether it should terminate early. Two methods can be used to check if a task has been cancelled:\n\n\n  \n    try Task.checkCancellation() throws an error if the current Task is cancelled..\n  \n  \n    if Task.isCancelled { break } returns true if the Task is cancelled. Note that this approach might produce partial outputs, which should be documented.\n  \n\n\ntaskGroup.addTask(priority: .background) {\n    if Task.isCancelled { return nil } // Return empty or default Data\n    await process(item)\n}\n\n\nReference and Cancel a Task\n\nUntil now, we‚Äôve only used tasks for running isolated asynchronous operations. However, there are scenarios where maintaining a task reference for potential cancellation is beneficial, as shown in the following static sales dashboard example.\n\nclass SalesDataViewController: UIViewController {\n    private var processingTask: Task&amp;lt;Void, Never&amp;gt;?\n\n    override func viewWillAppear(_ animated: Bool) {\n        super.viewWillAppear(animated)\n\n        guard processingTask == nil else { return }\n\n        processingTask = Task {\n            do {\n                let rawData = try await fetchSales()\n              \tlet chartData = await process(rawData)\n                await showChartData(chartData)\n            } catch {\n                handleError(error)\n            }\n\n            processingTask = nil\n        }\n    }\n\n    override func viewDidDisappear(_ animated: Bool) {\n        super.viewDidDisappear(animated)\n        processingTask?.cancel()\n        processingTask = nil\n    }\n}\n\n\nIn this SalesDataViewController class, we create and keep a reference to a new Task for fetching and processing sales data. If the user exits the view before the task completes, the task is canceled, preventing task accumulation during repeated view transitions.\n\nConvert completion based API to async functions with Continuation\n\nSometimes you encounter legacy APIs not designed to work with Swift‚Äôs Concurrency model, often the case with Objective-C-based APIs. Swift offers a solution via Continuation.\n\nContinuation wraps old-style block-based code and adapts it for use in an async function. This enables you to return values or throw errors within that function. Here‚Äôs how to apply this with HealthKit as an example:\n\nimport HealthKit\n\nfunc getWorkouts() async throws -&amp;gt; [HKWorkout] {\n    return try await withCheckedThrowingContinuation { continuation in\n        let query = HKSampleQuery(\n            sampleType: HKObjectType.workoutType(),\n            predicate: nil,\n            limit: HKObjectQueryNoLimit,\n            sortDescriptors: nil\n        ) { query, results, error in\n            if let error = error {\n                continuation.resume(throwing: HealthError.myError)\n            } else {\n                guard let results = results as? [HKWorkout] else {\n                    continuation.resume(throwing: HealthError.wrongType)\n                    return\n                }\n                continuation.resume(returning: results)\n            }\n        }\n        HKHealthStore().execute(query)\n    }\n}\n\nAlways ensure to resume a Continuation exactly once; failing to do so can lead to indefinite suspension of the task, resulting in a memory leak, as per Apple‚Äôs guidelines. Resuming multiple times is considered undefined behavior and should be avoided.\n\nExecuting Async Code on Main Thread with MainActor\n\nYou can use the MainActor to execute code on the main thread via three ways:\n\nAnnotate your code with @MainActor\n\nApply the @MainActor attribute to properties, functions and classes.\n\nclass MyClass {\n    @MainActor var image: Data // Update occurs on the main thread\n  \n    @MainActor func updateUI() async {\n        // this is now called on the main thread\n    }\n}\n\n// class properties and functions are now run on the MainActor\n@MainActor class MyClass {\n    var image: Data\n  \n    func updateUI() async { }\n}\n\n\nUse @MainActor in Task closures\n\nIncorporate @MainActor within a Task to switch its execution context to the main thread.\n\nTask { @MainActor in \n    // Code runs on the main thread\n}\n\n\nUse MainActor.run\n\nUse MainActor.run within any Task or asynchronous function to force main-thread execution.\n\nTask {\n    let data = await fetchAndProcessData()\n    await MainActor.run {\n        // Executed on main thread\n        await updateUI(with: data)\n    }\n}\n\n\nTips and pitfalls\n\nTask Cheat sheet\n\nFor quick reference, here‚Äôs a table taken from Explore structured concurrency in Swift WWDC session.\n\n\n  \n    \n      ¬†\n      Launched by\n      Launchable from\n      Lifetime\n      Cancellation\n      Inherits from origin\n    \n  \n  \n    \n      async-let tasks\n      async let x\n      async functions\n      scoped to statement\n      automatic\n      priority, task-local values\n    \n    \n      Group tasks\n      group.async\n      withTaskGroup\n      scoped to task group\n      automatic\n      priority, task-local values\n    \n    \n      Unstructured tasks\n      Task\n      anywhere\n      unscoped\n      via Task\n      priority, task-local values, actor\n    \n    \n      Detached tasks\n      Task.detached\n      anywhere\n      unscoped\n      via Task\n      nothing\n    \n  \n\n\nAsync Protocol Conformance\n\nWhen defining a protocol with async functions, you can conform to the protocol by implementing a synchronous function too.\n\nprotocol MyProtocol {\n    func processData() async\n}\n\nstruct TypeA: MyProtocol {\n    func processData() async\n}\n\nstruct TypeB: MyProtocol {\n    func processData() // also valid\n}\n\n\nReentrancy\n\nIn Swift concurrency, Reentrancy refers to the situation where a suspended block of code resumes execution at a later time. Upon resumption, the mutable state of your code is not guaranteed to remain the same as it was before suspension, posing potential risks of unintended side effects.\n\nTask Suspension and Unowned References\n\nIn Swift‚Äôs concurrency model, a Task strongly retains any reference to self, potentially extending the object‚Äôs lifecycle unexpectedly, especially if tasks remain active after their parent objects have been deallocated. To mitigate this, developers often employ weak self. However, introducing a suspension point using await within a Task can reintroduce issues associated with unowned references.\n\nclass MyClass {\n    unowned var dataStorage: DataStorage!\n    \n    func refreshData() {\n        Task { [weak self] in\n            guard let self = self else { return } // temporarily retains self\n            \n            let newData = loadDataFromDisk()\n            self.dataStorage = newData // Safe\n        }\n    }\n}\n\n\nIn this example, the code behaves as expected because it executes atomically. If self is available, it is temporarily retained, and newData is updated synchronously.\n\nHowever, introducing a suspension point can lead to issues similar to those encountered when neglecting to check for a weak self.\n\nclass MyClass {\n    unowned var dataStorage: DataStorage!\n    \n    func refreshData() {\n        Task { [weak self] in\n            guard let self = self else { return } // temporarily retains self\n            \n            let newData = await downloadData() // suspension point\n            self.dataStorage = newData // random crash\n        }\n    }\n}\n\n\nHere, if the task suspends during the await, nothing prevents dataStorage‚Äôs owner from being deallocated. When the task resumes, attempting to access the unowned property can result in a fatal error since dataStorage is no longer in memory.\n\nActor Reentrancy\n\nActor Reentrancy is a complex behavior that occurs when an actor method makes an asynchronous call, and while waiting for that call to complete, the actor processes other tasks. This can lead to unexpected states within the actor due to interleaved execution of its methods.\n\nactor Counter {\n    var value = 0\n\n    func increment() {\n        value += 1\n    }\n\n    func process() async {\n        increment()\n        print(value) // 1\n        await doLongProcessing() // suspension point\n        print(value) // Unpredictable output (1?)\n    }\n}\n\n\nIn this example, while process() is awaiting the completion of doLongProcessing(), there‚Äôs an opportunity for another task to call increment(). This undermines the expectation that an actor‚Äôs state remains consistent within a given method. So, the second print(value) may output an unpredictable result, illustrating the challenge of managing mutable state in an actor with reentrant behavior.\n\nAsync Function Execution Contexts\n\nContrary to the behavior in Grand Central Dispatch (GCD), where all code executed within the scope of a block is performed on the same thread, Swift‚Äôs concurrency model executes any async function on a global executor unless explicitly specified otherwise, such as with the @MainActor annotation.\n\nNote: a .task{} in SwiftUI runs implicitely on the MainActor when set within the body of a SwiftUI View.\n\nstruct MyView: View {\n  var body: some View {\n    ...\n    .task {\n      // Code within this block is executed on the Main Actor.\n      print(&quot;hello&quot;)\n      // Executed on a Global Executor despite being called from the Main Actor.\n      await fetchData()\n      // Executed on the Main Actor because we explicitly used @MainActor below.\n      await updateUI() \n    }\n  }\n\n\n  func fetchData() async { ... }\n  @MainActor func updateUI() async { ... }\n}\n\n\nConclusion\n\nAs we have seen, Swift Concurrency is a huge step forward in terms of safety and code maintainability. I hope you enjoyed reading this article and learned a few tricks. Dive in, experiment, and harness the power of Swift concurrency. Happy coding!\n\nFurther Reading &amp;amp; References\n\n\n  How async/await works internally in Swift\n  The Bleeding Edge of Swift Concurrency\n  Structured concurrency\n  Async/await\n  Async let\n  Actors\n  Global Actors\n  Concurrency is not Parallelism\n  How to determine where code runs in Swift Concurrency\n  Your Brain üß† on Swift Concurrency - iOS Conf SG 2023\n  Where View.task gets its main-actor isolation from\n\n"
} ,
  
  {
    "title"    : "Bedrock au Forum PHP 2023",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2023/10/20/forum-php-afup-2023.html",
    "date"     : "October 20, 2023",
    "excerpt"  : "Cette ann√©e encore, Bedrock √©tait pr√©sent au Forum PHP pour cette √©dition 2023.\n\nNous √©tions sept au total : trois organisateurs, trois visiteurs et une conf√©renci√®re, Pauline Rambaud.\n\nLes conf√©rences que l‚Äôon retient\n\nWhy is PHP still awesome in...",
  "content"  : "Cette ann√©e encore, Bedrock √©tait pr√©sent au Forum PHP pour cette √©dition 2023.\n\nNous √©tions sept au total : trois organisateurs, trois visiteurs et une conf√©renci√®re, Pauline Rambaud.\n\nLes conf√©rences que l‚Äôon retient\n\nWhy is PHP still awesome in 2023 ?\n\n\n  Conf√©rence pr√©sent√©e par Frank Karlitschek, co-fondateur de NextCloud\n\n\nLa conf√©rence s‚Äôest divis√©e en deux parties que l‚Äôon pourrait r√©sumer de la\nmani√®re suivante :\n\n  Une pr√©sentation assez d√©taill√©e de ce qu‚Äôest NextCloud\n  Pourquoi le choix de PHPüêò pour NextCloud ?\n\n\nNextCloud\n\nNextCloud est un outil comparable √† Google Workspace, car il permet la gestion\nde fichiers partag√©s, de documents de type Office, et propose aussi des outils\nde conversations textuelles et d‚Äôappels vid√©os.\n\nIl existe en version desktop bien s√ªr, mais aussi en version mobile.\nOn peut noter qu‚Äôil est utilis√© par le Gouvernement Fran√ßais, ou encore\nl‚ÄôUnion Europ√©enne.\n\nEn somme, il constitue aujourd‚Äôhui une alternative cr√©dible √† ses concurrents\nAm√©ricains ou Chinois.\n\nUne de ses diff√©rences majeure toutefois est qu‚Äôil est open-source.\n\nPourquoi choisir PHP pour NextCloud ?\n\nLes raisons que Frank nous donne sont les suivantes :\n\n\n  PHPüêò est facile √† d√©ployer\n  Il est ind√©pendant (il n‚Äôappartient pas √† Google)\n  Son isolation des process garantie une bonne scalabilit√©\n  Il propose une bonne courbe d‚Äôapprentissage\n  Il √©volue depuis longtemps et est donc robuste\n  Il a derri√®re lui une grosse communaut√© de d√©veloppeur\n  Un important √©cosyst√®me d‚Äôint√©gration et de frameworks\n\n\nIl √©voque toutefois quelques limites qui subsistent √† son sens :\n\n\n  il reste (de moins en moins) facile d‚Äô√©crire du code non s√©curis√©, malgr√©\nle syst√®me de typehinting ou les enums\n  il reste des incoh√©rences, par exemple dans les array functions\n  il reste tr√®s limit√© dans la programmation fonctionnelle\n\n\nToutefois, Franck conclura sur le fait que sans PHP, NextCloud n‚Äôaurait\njamais √©t√© possible.\n\nAugmentez votre couverture : supprimez des tests\n\n\n\n\n  Conf√©rence pr√©sent√©e par Baptiste Langlade\n\n\nAvez-vous d√©j√† √©t√© confront√© √† la probl√©matique de l‚Äôaugmentation exponentielle\ndu nombre de tests de votre projet, et par cons√©quent, de l‚Äôaugmentation du temps\nd‚Äôex√©cution de vos tests ?\n\nCe fut le cas de Baptiste dans l‚ÄôApplication de gestion de Documents √† laquelle\nil a particip√©.\n\nVive le hasard\n\nSa r√©ponse √† ce probl√®me fut la suivante : jouer des tests au hasard, autrement\ndit faire du Property Based Testing.\n\nL‚Äôid√©e est de g√©n√©rer al√©atoirement des donn√©es pour couvrir le plus de cas\npossible.\n\nEnsuite, on √©crit un test si un bug survient.\n\nCette approche se base sur la fameuse loi de Murphy qui veut que lorsqu‚Äôun probl√®me\ndoit survenir, il arrive toujours trop t√¥t.\n\nOn mise donc sur l‚Äôal√©atoire pour faire remonter les bugs plus rapidement.\nAinsi, on d√©termine que pour tout ensemble de donn√©e X, l‚Äôensemble des tests\ndoit √™tre vrai.\n\nBlackbox\n\nDe cette approche est n√©e Blackbox, une\nlibrairie compatible PHPUnit permettant de faciliter et d‚Äôautomatiser la mise\nen place de tests bas√©e sur le Property Based Testing.\n\nElle permet notamment de faire en sorte que les jeux de donn√©es que l‚Äôon va\nins√©rer en input peuvent √™tre vraiment divers et vari√©s, et provoquer des\ncas critiques non r√©pertori√©s.\n\nR√©sultat : plus on joue les tests, plus notre confiance grandit dans l‚Äôapplication.\n\nComment contribuer √† PHP en 2023 ? Georges Banyard\n\n\n\n\n  Conf√©rence pr√©sent√©e par Georges Banyard\n\n\nTout d‚Äôabord Georges Banyard a introduit la notion de compilation minimale de PHP en C, pour cela il a utilis√© son blog\net nous a pr√©sent√© les choses importantes √† connaitre.\n\nPuis, il a fait la pr√©sentation du code source de PHP et du moteur de recherche qu‚Äôil utilise.\n\nEnsuite, il a commenc√© √† nous montrer quelques ressources auxquelles se r√©ferer si nous d√©sirons contribuer √† PHP.\n\nRappelons que PHP est un langage de programmation open-source et que toute personne d√©sireuse de l‚Äôam√©liorer peut\nproposer une impl√©mentation.\n\nLes ressources que nous pourrions utiliser si notre souhait √©tait de cr√©er une nouvelle fonction dans PHP seraient :\n\n\n  des articles sur Zend (https://www.zend.com/resources/writing-php-extensions)\n  le PHP internal books (https://www.phpinternalsbook.com/)\n  ou bien encore la ‚ÄúRoom 11‚Äù sur Stackoverflow o√π se retrouve de nombreux contributeurs PHP.\n\n\nGeorges Banyard est ensuite pass√© √† la pratique et a cod√© en direct une nouvelle fonction de tri dans un\ntableau array_search. Le but de cette conf√©rence √©tait de d√©mystifier la contribution √† PHP et de montrer aux gens que\nfinalement ce n‚Äôest pas si compliqu√©, il suffit de plonger dedans !\n\nUtilisez la biblioth√®que standard PHP (SPL) au quotidien\n\n\n\n\n  Conf√©rence pr√©sent√©e par Florian Merle\n\n\nLa biblioth√®que SPL √ßa vous dit quelque\nchose ? En r√©alit√©, vous l‚Äôutilisez d√©j√† au quotidien √† travers les Exceptions\nou bien la fonction spl_autoload_register par exemple, mais Florian est venu\nnous parler de certains aspects moins connus et pourtant tr√®s utiles.\n\nLes structures de donn√©es\n\nLa plus √©vidente et la plus connue, qui a sans doute contribu√© √† la facilit√©\nd‚Äôacc√®s de PHPüêò est bien √©videmment l‚Äôarray. Mais si son avantage majeur\nest le fait qu‚Äôil soit multi-usage, il se r√©v√®le en r√©alit√© assez peu optimis√©\npour les gros volumes de donn√©es.\n\nD‚Äôautres structures telles que les listes doublements cha√Æn√©es\n(SplDoublyLinkedList),\nou les Heaps (SplHeap,\nSplPriorityQueue ont\n√©t√© abord√©es, mais on constate rapidement qu‚Äôelles pr√©sentent l‚Äôinconv√©nient majeur\nd‚Äôavoir de mauvaises performances par rapport √† array et des nommages de m√©thode\npeu intuitif.\n\nPHP DS\n\nUne alternative int√©ressante est DS,\nqui ne pr√©sente pas de probl√®me de gestion de priorit√© et de meilleures performances.\n\nFlorian nous a pr√©sent√© notamment DS\\PriorityQueue\net DS\\Vector, qui constitue une\nalternative int√©ressante √† array.\n\nLes iterators\n\nEnfin, nous avons vu les iterators de la SPl avec :\n\n\n  IteratorAggregate\nqui permet d‚Äôappliquer un traitement √©ventuel sur le tableau\n  AppendIterator\nqui permet d‚Äôins√©rer d‚Äôautres it√©rateurs\n  IteratorIterator\nqui renvoie un autre Iterator\n  InfiniteIterator\nqui permet de boucler √† l‚Äôinfini (pensez √† la fonction lecture en boucle de votre player audio)\n  CallbackFilterIterator\nqui permet de filtrer les donn√©es √† l‚Äôaide d‚Äôune callback.\n\n\nApprendre √† apprendre : petit dev deviendra grand - Aline Leroy\n\n\n  Conf√©rence pr√©sent√©e par Aline Leroy\n\n\nAline Leroy nous parle ici de sa reconversion, et des diff√©rentes exp√©riences qu‚Äôelle a v√©cues au cours de son\napprentissage du code. Elle nous donne plein d‚Äôastuces et de cl√©s pour am√©liorer notre fa√ßon d‚Äôapprendre et de\ntravailler notre plasticit√© c√©r√©brale. Tout d‚Äôabord, il faut g√©rer son temps, alterner concentration et dispersion, et\ncomprendre qu‚Äôapprendre c‚Äôest cr√©er des liens et des images mentales.\n\nAline Leroy nous conseille de commencer par faire une introspection afin de d√©terminer quels sont nos points forts et\nfaibles, quels sont nos objectifs et puis pour comprendre son propre fonctionnement. Ensuite, il faut faire preuve de\ncuriosit√© et ne pas se limiter √† un domaine (pensez cr√©ation de lien, plus le champ est grand, plus vous allez faire\ndes connexions). Nous pourrons aussi choisir de travailler par d√©coupage, une partie des choses √† apprendre, nous allons\ndevoir travailler notre m√©moire. Il est aussi important que l‚Äôapprentissage soit une d√©marche active, prise de notes,\nrecherche, ne pas cat√©goriser et surtout, c‚Äôest un processus qui demande de la r√©gularit√©.\n\nElle insiste sur l‚Äôaspect bien-√™tre qui est tr√®s important pour apprendre, se mettre au calme, couper ses notifications\net puis on prend des pauses, car c‚Äôest l√† que tout le travail de liaison neuronale se fait. Une pause sportive et encore\nplus recommand√©e, bien s√ªr ces temps de dispersion doivent √™tre sur un temps maitris√©.\n\nYou Build It, You Run It, l‚Äôobservabilit√© pour les devs\n\n\n\n\n  Conf√©rence pr√©sent√©e par Smaine Milianni, d√©veloppeur chez Yousign\n\n\nL‚Äôobservabilit√© est quelque chose de tr√®s important chez Bedrock, c‚Äôest pourquoi cette conf√©rence √©tait tr√®s\nint√©ressante.\n\nSmaine a d√©coup√© sa conf√©rence en posant trois questions : pourquoi, comment et qui ?\n\nPourquoi faire de l‚Äôobservabilit√© ?\n\nUne application va forc√©ment planter √† un moment, nous dit Smaine pour r√©pondre √† sa premi√®re question.\n\nL‚Äôobservabilit√© permet non seulement de diminuer les risques que cela arrive, mais aussi d‚Äôanticiper les probl√®mes en\namont.\n\nAu-del√† des plantages, l‚Äôobservabilit√© ouvre la porte √† d‚Äôautres formes d‚Äôanalyses :\n\n  d√©tecter les probl√®mes/changements de performance\n  capter les comportements inhabituels\n  ou simplement s‚Äôassurer qu‚Äôun syst√®me fonctionne correctement\n\n\nComment fait-on ?\n\nHabituellement, l‚Äôobservabilit√© s‚Äôappuie sur trois piliers : les logs, les m√©triques et les traces, auxquels Smaine\najoute un quatri√®me : les alertes.\n\nPour r√©sumer ces diff√©rents piliers :\n\n  les logs doivent √™tre lus, et donc √™tre disponibles dans un outil d√©di√©\n  les m√©triques permettent d‚Äôobserver l‚Äô√©tat de sant√© d‚Äôune application\n    \n      elles peuvent √™tre techniques (CPU, temps de r√©ponse d‚Äôune URL, etc)\n      ou m√©tier (nombre de ventes par jour, nombre d‚Äôinscriptions, etc)\n    \n  \n  les traces permettent de suivre le comportement d‚Äôune application\n    \n      par exemple pour une requ√™te HTTP, on aurait le temps pass√© dans chaque couche technique (base de donn√©es, un\ncontroller PHP, lecture de cache, etc)\n    \n  \n  Et enfin, les alertes\n    \n      elles ne doivent √™tre envoy√©es qu‚Äôen cas de probl√®me (pour √©viter de lasser les lecteurs)\n      elles peuvent se baser sur les m√©triques (m√™me m√©tier)\n      elles peuvent √™tre li√©es aux logs et aux traces pour aider √† les comprendre\n    \n  \n\n\nSmaine nous a √©galement rapidement parl√© des post mortems, qui sont essentiels apr√®s un incident pour apprendre des\nerreurs et ne pas les reproduire.\n\nQui est responsable de la mise en place ?\n\nL‚Äôentreprise de Smaine, Yousign, a une organisation sous forme de squads.\nUn squad est compos√© de plusieurs m√©tiers : dev, devops, PO, designer, chef de projet, etc.\n\nIls sont alors responsables √† la fois de leur p√©rim√®tre, mais aussi de son observabilit√©. En gardant cette\nresponsabilit√© au sein d‚Äôun squad, les √©quipes sont plus attentives et r√©actives en cas d‚Äôincident.\n\nLes femmes et le num√©rique\n\n\n\n\n  Conf√©rence pr√©sent√©e par Isabelle Collet\n\n\nIsabelle Collet est ancienne d√©veloppeuse et sociologue √©panouie comme elle le dit elle-m√™me. Ce qui lui donne mati√®re √†\nnous expliquer beaucoup de choses sur ‚Äúle choix des femmes de ne pas venir dans la tech‚Äù. Et bien, oui, pourquoi\nsont-elles r√©ticentes ?\n\nTout d‚Äôabord, Isabelle Collet nous parle de pays qui sont des contre-exemples, comme en Malaisie o√π les femmes sont la\nmajorit√© des postes informatiques et de d√©veloppement, elles sont aussi responsables d‚ÄôUniversit√©. Globalement, dans\ncertains pays d‚ÄôAsie, les femmes sont fortement repr√©sent√©es dans le secteur tertiaire, et pourquoi ?\nCar ces m√©tiers sont consid√©r√©s comme des ‚Äúm√©tiers de femmes‚Äù, non salissant, pas physique et surtout possibilit√© de\nt√©l√©travail ce qui leur permet de s‚Äôoccuper de leur famille.\n\nPuis gr√¢ce √† un petit retour dans le pass√©, nous d√©couvrons que les femmes au d√©part √©taient tr√®s pr√©sentes dans\nl‚Äôinformatique et plus pr√©cis√©ment dans le d√©veloppement. Les hommes √©taient charg√©s de la conception des ordinateurs,\nce qui √©tait bien vu dans la soci√©t√©, mais la programmation √©tait un poste consid√©r√© comme inf√©rieur. Ce qui va changer\ncela est l‚Äôapparition du micro-ordinateur qui va inverser la tendance et les hommes vont devenir plus nombreux √† coder\net les femmes vont progressivement ‚Äúdispara√Ætre‚Äù du paysage informatique.\n\nPour rem√©dier √† cela, Isabelle Collet nous pr√©sente deux initiatives universitaires o√π pour r√©√©quilibrer le taux\nhommes/femmes des quotas ont √©t√© impos√©s pendant une dizaine d‚Äôann√©es. Aujourd‚Äôhui l‚Äô√©quilibre de candidats se fait\nnaturellement.\n\nEn conclusion, les choses √©voluent si on veut bien les faire √©voluer. Il faut continuer les efforts d‚Äôinclusion et la\nlutte pour que les femmes soient plus nombreuses dans la tech.\n\nConclusion\n\nCette ann√©e, la fresque LEGO a c√©l√©br√© la diversit√© des membres de l‚ÄôAFUP et bien s√ªr le PHP.\n\nCe forum f√ªt l‚Äôoccasion d‚Äô√©changer avec de nombreuses personnes, de d√©couvrir des sujets aussi bien techniques que\nsoci√©taux. On ne peut que f√©liciter les conf√©rencier¬∑e¬∑s et les b√©n√©voles pour un Forum PHP encore tr√®s r√©ussi !\n\n\n"
} ,
  
  {
    "title"    : "Siteswap: jongler avec les maths #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/siteswap-jongler-avec-les-maths",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  Pr√©sentation des concepts de base du Siteswap, la notation math√©matique qui permet de d√©crire les patterns/figures de jonglerie. D√©finition, exemples basiques, limites, comment d√©terminer si une s√©quence est valide‚Ä¶ avec d√©monstration en direct...",
  "content"  : "\n  Pr√©sentation des concepts de base du Siteswap, la notation math√©matique qui permet de d√©crire les patterns/figures de jonglerie. D√©finition, exemples basiques, limites, comment d√©terminer si une s√©quence est valide‚Ä¶ avec d√©monstration en direct de certaines figures.\n\n\nPar Damien Krieger\n"
} ,
  
  {
    "title"    : "Nourrir nos IA #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/nourrir-nos-ia",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  ChatGPT, c‚Äôest magique. L‚ÄôIA, c‚Äôest futur de l‚Äôhomme. Les r√©seaux neuronaux, c‚Äôest la vie. Vraiment ?\n\n\n\n  Je vous propose de voir un peu ce qui se cache derri√®re l‚ÄôIA par le biais de celleux qui la construisent : les travailleur¬∑se¬∑s du digita...",
  "content"  : "\n  ChatGPT, c‚Äôest magique. L‚ÄôIA, c‚Äôest futur de l‚Äôhomme. Les r√©seaux neuronaux, c‚Äôest la vie. Vraiment ?\n\n\n\n  Je vous propose de voir un peu ce qui se cache derri√®re l‚ÄôIA par le biais de celleux qui la construisent : les travailleur¬∑se¬∑s du digital. Oui, j‚Äôai bien digital et vous verrez que ce terme n‚Äôa rien d‚Äôabusif.\n\n\n\n  √Ä la fin, vous aurez une vision plus pr√©cise de ce qui se trame dans le monde pas si magique des IA.\n\n\nPar Nastasia Saby\n"
} ,
  
  {
    "title"    : "Mentors: super-h√©ros ou super-vilains ? #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/mentors-super-heros-ou-super-vilains",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  D√©veloppeuse junior, j‚Äôai eu du mal √† trouver ma place et faire mes preuves. √âtudiante, vous m‚Äôaviez dit : ‚ÄúDeviens PO ou chef de projet !‚Äù. Je ne voulais pas d‚Äôun m√©tier fonctionnel. Je me suis fix√©e un but, r√©ussir l√† o√π vous, profs, maitres ...",
  "content"  : "\n  D√©veloppeuse junior, j‚Äôai eu du mal √† trouver ma place et faire mes preuves. √âtudiante, vous m‚Äôaviez dit : ‚ÄúDeviens PO ou chef de projet !‚Äù. Je ne voulais pas d‚Äôun m√©tier fonctionnel. Je me suis fix√©e un but, r√©ussir l√† o√π vous, profs, maitres de stage, ne vouliez pas que j‚Äôaille. Des rencontres, cauchemardesques comme merveilleuses, tout au long de mon apprentissage, m‚Äôont aid√©e √† confirmer mon choix professionnel : √™tre d√©veloppeuse ! Comment mes mentors m‚Äôont guid√©e‚ÄØ? Je partagerai des actions et des comportements qui m‚Äôont incit√©e √† lutter pour atteindre mes objectifs.\n\n\n\n  Vous aussi, vous avez le pouvoir de changer une vie !\n\n\nPar Pauline Rambaud\n"
} ,
  
  {
    "title"    : "Le Don du Sang (et +) #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/le-don-du-sang",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  Donner son sang, c‚Äôest unanimement consid√©r√© comme une bonne action\n\n\n\n  Mais pourquoi en fait ? √áa vient d‚Äôo√π ? √áa sert √† quoi ? C‚Äôest dangereux ?? \nEt concr√®tement, √ßa se passe comment ?\n\n\n\n  Dans ce talk, on va raconter un peu l‚Äôhistoire du ...",
  "content"  : "\n  Donner son sang, c‚Äôest unanimement consid√©r√© comme une bonne action\n\n\n\n  Mais pourquoi en fait ? √áa vient d‚Äôo√π ? √áa sert √† quoi ? C‚Äôest dangereux ?? \nEt concr√®tement, √ßa se passe comment ?\n\n\n\n  Dans ce talk, on va raconter un peu l‚Äôhistoire du don du sang dans le monde, puis rentrer dans le concret avec le processus de don et les diff√©rents types de dons. Et surtout, on va voir √† quoi √ßa sert de donner (pour soi, et pour les autres). Enfin on verra un peu √† travers le monde comment √ßa se passe aussi !\n\n\nPar Quentin Nambot\n"
} ,
  
  {
    "title"    : "La travers√©e du Finnmarksvidda: carnet de bord d‚Äôune aventure glaciale #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-traversee-du-finnmarksvidda-carnet-de-bord-d-une-aventure-glaciale",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  Carnet de voyage de ma travers√©e en solitaire du plateau du Finnmarksvidda, en Norv√®ge, entre Alta et Karasjok, en mars 2023.\n\n\nPar Sylvain Guyon\n",
  "content"  : "\n  Carnet de voyage de ma travers√©e en solitaire du plateau du Finnmarksvidda, en Norv√®ge, entre Alta et Karasjok, en mars 2023.\n\n\nPar Sylvain Guyon\n"
} ,
  
  {
    "title"    : "Json au service des devs #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/json-au-service-des-devs",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  √âditer un CV, imprimer des fiches de recette, √©crire une lettre de motivation, r√©diger un rapport ‚Ä¶ Tant de choses fastidieuses √† √©crire et chiantes √† mettre en page (surtout quand on ne sait pas utiliser Word). C‚Äôest pourquoi j‚Äôai d√©velopp√© un...",
  "content"  : "\n  √âditer un CV, imprimer des fiches de recette, √©crire une lettre de motivation, r√©diger un rapport ‚Ä¶ Tant de choses fastidieuses √† √©crire et chiantes √† mettre en page (surtout quand on ne sait pas utiliser Word). C‚Äôest pourquoi j‚Äôai d√©velopp√© un outil pour m‚Äôaider √† faire tout √ßa en utilisant uniquement du JSON !\n\n\nPar Julie Nginn\n"
} ,
  
  {
    "title"    : "GopherCon UK 2023 highlights",
    "category" : "",
    "tags"     : " conference, london, tech, golang, go",
    "url"      : "/2023/08/12/gophercon-uk-london-2023.html",
    "date"     : "August 12, 2023",
    "excerpt"  : "Hello there! I‚Äôm Pierre-Alain, a senior back-end developer at Bedrock and I had the opportunity to go to London for the GopherCon UK.\n\nI travelled by train from Lyon to London, TGV and Eurostar. Train journey not only aligned with my commitment to...",
  "content"  : "Hello there! I‚Äôm Pierre-Alain, a senior back-end developer at Bedrock and I had the opportunity to go to London for the GopherCon UK.\n\nI travelled by train from Lyon to London, TGV and Eurostar. Train journey not only aligned with my commitment to minimize my carbon footprint for this trip, but is also a enjoyable travel choice. While the cost was higher than taking a plane, I would like to thank Bedrock for enabling me to take this option.\n\n\n\nWhat is GopherCon UK ?\n\nGopherCon UK 2023 spans three days, commencing with a workshop day on August 16th, followed by two conference days featuring multiple tracks on August 17th and 18th. The event takes place at The Brewery, at Barbican, situated in the heart of the City of London.\nWe were over 500 attendees, including delegates, speakers, and sponsors. This event is all about sharing the latest in Go programming with lots of networking, good food and drinks.\n\nWorkshop day (Practical GO for developers)\nIt was a first time for me to attend a workshop at an event like GopherCon. As a new Gopher, my exposure to writing Go code had been limited. However, I was looking forward to it.\n\nIn the following lines, I‚Äôll be sharing the insights I gained during the workshop ‚Äî knowledge I‚Äôm excited to share to both you and my colleagues:\n\n  \n    Because of some unicode characters, you should not use len() on a string in case you have emojis, hieroglyph or symbol (¬£‰∏ñÁïå). It is better to use tf8.RuneCountInString()\n  \n  use fmt.Printf(...) for debugging\n      package main\n\n  import &quot;fmt&quot;\n\n  func main() {\n      s := struct {\n          E int\n          A string\n      }{1, &quot;string&quot;}\n\n      fmt.Printf(&quot;%#v, %+v, (%T)&quot;, s, s, s)\n      // output : struct { E int; A string }{E:1, A:&quot;string&quot;},\n      //          {E:1 A:string},\n      //          (struct { E int; A string })\n  }\n    \n  \n  \n    `http://site.name`  gives a raw string which can be useful when you have slashes in your string, to avoid having to escape them.\n  \n  I liked this image which gives an idea of computer latency compared to a human.\n    Computer Latency at a Human ScaleSource: @ProwessConsult (2017) https://t.co/5Uhw5nCzUJ pic.twitter.com/YlVYnm3nGH&amp;mdash; Josh Jordan (@jordancurve) March 20, 2019\n    \n  \n  a well known bug also known as the for-loop gotcha where the i in the loop is not incrementing correctly.\n      for i := 0; i &amp;lt; 5; i++ {\n      go func() {\n          fmt.Printf(&quot;gr:%d\\n&quot;, i)\n      }()\n\n      //fix 1\n      go func(n int) {\n          fmt.Printf(&quot;gr:%d\\n&quot;, n)\n      }(i)\n\n      // fix 2\n      i := i\n      go func() {\n          fmt.Printf(&quot;gr:%d\\n&quot;, i)\n      }()\n  }\n    \n  \n  Goroutine channel tricks\n    \n      \n        send/receive will block until opposite operation (*)\n          \n            buffered channel of cap n has n non-blocking sends\n          \n        \n        receive from a closed channel will return the zero value without blocking\n        you can use val,ok := &amp;lt;- ch as second left variable to know if value comes from channel or not\n        send or close to a closed channel will panic\n        send or receive on a nil channel will block forever‚Ä¶\n      \n    \n  \n  \n    doc.go is used to add documentation to packages, example below:\n\n      /*\n  Package nope does nothing as Go package.\n\n  ...\n  */\n  package nope\n    \n  \n  example_test.go on a package, gives example tests to other developers\n\n\npackage nope_test\n\nimport (\n  &quot;fmt&quot;\n\n  &quot;github.com/shipt/nlp&quot;\n)\n\nfunc ExampleTokenize() {\n  nothing := nope.SayNothing()\n  fmt.Println(nothing)\n\n  // Output:\n  // &quot;nothing&quot;\n}\n\n\n\n  \n    you can create a testdata folder to store fixtures which would be avoided by compiler\n  \n  \n    we should use testify/require, testify/mock and testify/suites evil laugh\n  \n\n\nWorkshop given by Miki Tebeka, Ardan Labs was really great and gave the opportunity to compare myself to what is expected from a new Gopher, and so far I would say I‚Äôm doing OK. I am now eager to tackle more projects in Go at Bedrock. \\o/\n\nDay one\n\nUpon arrival, I was greeted with a delightful British-style Gopher plush, serving as a warm and wonderful welcome gift.\n\n\n\t\n\n\nThen, I enjoyed a truly good breakfast (kudos to @Formal for the exceptional coffee baristas) and engaged in conversations with the sponsor booths positioned within the venue, and even entered their raffle giveaways.\n\nScaling Coffee with Goroutines (workshop tutorial)\n\nSadie Freeman‚Äôs enlightening tutorial walks us through the process of taking advantages from goroutines to achieve scalable codebase and platform effectively.\n\nThe coffee shop challenge\n\nImagine a coffee shop aiming to do:\n\n  a lot of coffee\n  for a lot of people\n  as fast as possible\n\n\nFor a coffee to be considered complete, three essential actions were required:\n\n  Accept payment\n  Steam milk\n  Make espresso\n\n\nEach of these actions takes approximately 2 seconds to execute.\n\nTo break it down:\n\n  Serving one customer requires 2 seconds per action, totaling 6 seconds.\n  Serving three customers extends the total time to 18 seconds.\n\n\nScaling for Speed\n\nInitial attempts to introduce goroutines seem straightforward, by merely appending ‚Äúgo‚Äù before a method:\n\n go MakeCoffee()\n\n\nHowever, for three customers, this results in a staggeringly brief 49 microseconds, practically an impossible feat. The solution wasn‚Äôt as straightforward as it may seem.\n\nEnter the sync.WaitGroup\nsync.WaitGroup()\n\ndefer wg.Done()\n\nwg.Wait()\n\n\nNonetheless, even with this approach, we still find ourselves waiting for each coffee to complete before initiating the next one. Consequently, serving three customers still demands 6 seconds.\n\nOptimization beckons; each action could potentially transform into its own goroutine. Ultimately, with this approach, serving three customers takes a mere 2 seconds, finally achieving efficient customer service.\n\nScaling for Load\n\nEnter containerization! Deployment onto Kubernetes through Docker containers ensues. The coffee shop pods are allocated finite CPU and Memory resources.\n\nConsider a scenario where 300 customers arrive simultaneously on our webservice. This influx of customers causes the pod to become memory-intensive, ultimately leading to an Out of Memory (OOM) kill.\n\nBut what happens when the challenge escalates to 3000 customers? This needs vertical scaling: allocating more resources. However, this approach can become prohibitively expensive.\n\nEnter the pragmatic solution of horizontal scaling - opening more pods (coffee shops). With 10 pods, each handling 300 customers, accommodating 3000 customers in about 2 seconds becomes feasible.\n\nWhile this tutorial doesn‚Äôt delve into the topic, an intriguing possibility would be to split responsibilities in different pod(payment, milk, espresso) for further optimization and efficiency.\n\nThe 7 Deadly Sins for Gophers\n\nJohn Gregory provided invaluable insights into the pitfalls to avoid for Gophers, whether seasoned or new. Here‚Äôs a condensed overview of the key takeaways:\n\nLush\nThe urge to rush into production without due consideration. Concurrency might entice you with the ‚Äúgo‚Äù keyword, but remember:\n\n  Use goroutines judiciously, only when necessary.\n\n\nWrath\nSubstituting ‚Äúpanic‚Äù for proper error handling:\n\n  ‚ÄúUsing a wall to stop a car instead of the brakes.‚Äù\n\n  Embrace defensive coding practices.\n\n  Reserved methods should be utilized outside user runtime; employ them at startup.\n\n\nGreed\nThe desire to future-proof everything:\n\n  Avoid over-engineering; simplicity often prevails.\n\n\nSloth\nFocusing on ‚Äúwhat‚Äù rather than ‚Äúwhy‚Äù in comments:\n\n  Enhance error handling with contextual information. Seek semantic context.\n\n  Utilize fmt.Errorf(&quot;this did not work: %w&quot;, err) for enhanced error messages.\n\n\nGluttony\nUnnecessary reliance on frameworks:\n\n  Beware of vulnerabilities.\n\n  Begin with simplicity.\n\n  Embrace libraries with robust support.\n\n\nEnvy\nForcing patterns where they aren‚Äôt required:\n\n  Interfaces aren‚Äôt always necessary.\n\n  Explore ‚ÄúEffective Go‚Äù principles.\n\n\nPride\nAssuming you possess the ultimate knowledge:\n\n  Avoid artificially restricting API access.\n\n\nJohn Gregory‚Äôs wisdom shines a light on the potential pitfalls Gophers may encounter. Steering clear of these seven deadly sins can lead to more effective and resilient Go programming practices.\n\n\nSocial event with food and drinks (responsibly)\nI (also) enjoyed video game, table tennis, a magician and a DJ !\n\n\n\nDay two\n\nI started this second day of conferences by having a Cup of tea (with milk) to get myself in the shoes of an English Gopher :)\n\nState of the Go Nation!\nCameron Balahan, Product Manager of the Go team, took us on a journey through the past, present, and future of Go. He shared insights into the language‚Äôs evolution, growth, and its roadmap ahead.\n\nGoing Back in Time\n\n  2007: Go‚Äôs inception within Google.\n  2009: The pivotal moment when Go was opensourced.\n  2012: The groundbreaking release of Go 1.0, marking its maturity.\n\n\nThe Stability of Go 1.0\nBalahan emphasized the continuity of Go 1.0, ensuring backward compatibility with all Go 1.* versions.\n\nRefinement and Progress\n\n  2015: Go‚Äôs strides in version 1.5:\n    \n      Advancements in the compiler.\n      Introduction of a new garbage collector.\n      Adoption of semi-annual releases.\n    \n  \n\n\nThe Flourishing Ecosystem of 2018\n\n  2018: Go‚Äôs ecosystem flourished, introducing key enhancements:\n    \n      Go modules with caching and checksum features.\n      A focus on creating a secure platform.\n    \n  \n\n\nForever Go 1.*\nBalahan said Go 2 ‚Äúwill never happen‚Äù, as keeping Go 1 program working with all Go 1.* version is their aim from the start.\n\nToday‚Äôs Triumphs\nIn the present:\n\n  Go‚Äôs user base has multiplied by four since 2018.\n  High levels of user satisfaction persist among the Go community.\n\n\nThe Path Ahead\nLooking forward:\n\n  Addressing the loop variable issue. (mostly done in Go 1.21)\n  Enhancing onboarding experiences.\n  Strengthening vulnerability management.\n\n\nA Bonus for the Dev Community\nIn the spirit of improving security, Balahan encourages developers to integrate the new Govulncheck tool into their CI pipelines. This tool developed by Go team stands as a sentinel, safeguarding against potential vulnerabilities within Go packages.\n\nEfficient Debugging and Logging with OpenTelemetry in Go\nIn the past, our debugging approach was straightforward: Log everything, locally or remotely. However, the landscape has evolved, favoring a combination of logs, traces, and metrics to illuminate the path.\n\nA quote I really liked:\n\n\n  ‚ÄúLogs are for your future self, left by your past self :)‚Äù\n\n\nWhile logs remain indispensable, their role can now be enhanced.\n\n\n  \n    Absence of Request Context - Log entries often contain detailed information about microservices, functions, or applications, yet they can fall short in providing the broader request context.\n  \n  \n    Scale Demands Complexity - To grasp an application‚Äôs normal functioning, collating and analyzing numerous log entries becomes vital. This can require extensive indexing and complex tools to achieve at scale.\n  \n  \n    Disparity Across Teams and Services - Uniformity in logs isn‚Äôt guaranteed across teams, adding complexity when attempting to link them cohesively.\n  \n\n\nEnter Distributed Tracing:\nA game-changer in the debugging realm, distributed tracing offers a comprehensive view of a request‚Äôs lifecycle. It stands as a potent tool to combat production issues effectively. With end-to-end visibility, distributed tracing is meant to level up your debugging power!\nWe, Bedrock, already been using distributed tracings for a while in our PHP codebase, which allow us to rapidly debug and understand problems in production when incidents happen ;)\n\nThe Hacker‚Äôs Guide to JWT Security\nJSON Web Tokens (JWTs) come with their own set of caveats that need to be carefully considered to ensure a robust security level.\n\nThe Pronunciation Puzzle: How to Pronounce JWT?\nBefore we dive into the security aspects of JWTs, let‚Äôs clarify a common query: how do you pronounce JWT? According to the standards outlined in RFC 7519, JWT is pronounced as ‚Äújot‚Äù. So, the next time you discuss JWTs, remember it‚Äôs neither three separate letters nor ‚ÄúJWT Token‚Äù as some say in France‚Ä¶, but a concise ‚Äújot‚Äù.\n\nThe Encoding vs. Encryption Distinction\nOne fundamental fact about JWTs is that they are designed for encoding, not encryption. This means that the data contained within a JWT is base64url encoded, which allows easy transmission between parties, but it‚Äôs not encrypted in a way that prevents unauthorized access to the actual content.\n\n1. The ‚Äúnone‚Äù Algorithm\nIn scenarios where the backend fails to verify the algorithm before checking the claims within the JWT, an attacker can exploit this weakness and craft a forged token. This issue lies not just in the hands of the RFC specifications but also in implementation.\n\nTo address this concern, it‚Äôs imperative to implement rigorous checks at the backend level. Verification of the algorithm should precede the verification of claims. This approach ensures that only tokens with valid algorithms are processed, mitigating the risk of unauthorized access.\n\n2. HS256: Password/Key Cracking Vulnerability\nThis algorithm employs a shared secret key for both encoding and decoding, making it susceptible to password or key cracking attacks. What‚Äôs particularly concerning is that even a single compromised token can lead to offline attacks, where no communication with the server is necessary while craking.\n\nTo counter this threat, regularly update and rotate the secret keys, ensuring that even if one key is compromised, the potential damage is limited.\n\n3. Man in the Middle on Internal Networks\nIn a man-in-the-middle attack scenario, an attacker can intercept and manipulate JWTs exchanged within an unsecured network, potentially gaining unauthorized access.\n\nTo minimise this risk, it‚Äôs recommended to adopt secure communication protocols such as HTTPS. Additionally, implementing strict network segmentation and proper access controls can minimize the attack surface within internal networks.\n\n4. XSS Vulnerabilities: Safeguarding Tokens\nStoring JWTs in local storage, which can be accessed by scripts, worsen this risk to counter XSS threats: consider using cookies to store JWTs. Cookies are less susceptible to XSS attacks, as their content cannot be directly accessed by Javascript scripts.\n\nBy adopting this practice, the risk of token theft through XSS is substantially reduced.\n\nTo sum up, Bedrock already adapted these principles for a while, but nevertheless it was great to remind myself to think about security and vulnerabilities around JWTs.\n\nHow NOT to Write a Test in Go\n\nAmir Malka stated that adhering to these practices elevates Go testing from trivial to masterful. Effective testing enhances code reliability and development predictability. Embracing these guidelines lays the foundation for successful software projects.\n\nWhy Testing?\n\n  Tests uncover bugs and validate code changes.\n  Tests build confidence in code correctness.\n  Tests serve as living documentation, illustrating code behavior.\n\n\nKey Testing Principles\n\n  Fixture Placement: Organize test fixtures in a dedicated testdata directory, avoided by compiler.\n  Structured Tests: Embrace table-driven tests for readability and comprehensive coverage.\n  Parallelism: Use caution with parallel tests to avoid unintended interactions.\n  Test Suites: Employ testing.M for related tests and control setup/teardown with TestMain.\n  Categorization: Use build tags or environment variables to categorize and skip tests.\n  Concurrency Safety: Detect data races with the -race flag during testing.\n  Effective Mocking: Benefit from embedded interfaces for accurate mocks.\n  Randomizing Tests: Introduce randomness with -shuffle flag to uncover hidden issues.\n  Benchmarks: Gauge code performance with testing.B, setup/teardown carefully.\n  Test Automation: Automate tests in CI pipelines to maintain consistent quality.\n  Code Coverage: Use -cover flag to monitor and improve test coverage.\n\n\nComing from PHP, I never thought about parallelism or Benchmarks while writing tests on a daily basis. All of these testing principles will help me and hopefully my team (when they would have read this post!) to write test the ‚ÄúGo way‚Äù.\n\nConclusion\nIn closing, my journey through GopherCon UK 2023 has been as exciting as tiring for my first Golang Conference. I thank a lot Bedrock for affording me the opportunity to partake in this experience. Participating in this event helped me to dive deeper into the world of Go and will enhance my professional growth.\n\nTo the organizers of GopherCon UK 2023, your planning and dedication have resulted in a seamless event. Every aspect has been orchestrated to perfection. Congrats to you ! Also, thanks to the sponsors, for continuing to take part in these events during financially hard times.\n\nAs I return to my daily routine armed with the knowledge gained, I am excited to share these learnings with my colleagues and hope to contribute even more effectively to my team. GopherCon UK 2023 has not only expanded my knowledge but has also strengthened my enthusiasm for Go.\n\nGopherCon UK 2024, hopefully, here I come (with other of my colleagues) ! Cheers!\n"
} ,
  
  {
    "title"    : "Bedrock at We Love Speed 2023",
    "category" : "",
    "tags"     : " conference, paris, tech, webperf",
    "url"      : "/2023/08/11/we-love-speed-2023.html",
    "date"     : "August 11, 2023",
    "excerpt"  : "The Frontend Bedrock teams were present at the 2023 edition of the We Love Speed conference in Paris on May 10. Its aim is to share as widely as possible knowledge and experience in the field of web performance.\n\nWebperf, or web performance, refer...",
  "content"  : "The Frontend Bedrock teams were present at the 2023 edition of the We Love Speed conference in Paris on May 10. Its aim is to share as widely as possible knowledge and experience in the field of web performance.\n\nWebperf, or web performance, refers to the set of techniques and best practices aimed at optimizing the loading speed and user experience (UX) of websites. It covers various aspects such as page loading time, interface responsiveness, animation fluidity, server request processing capacity, etc.\n\nWebperf is still a major challenge in today‚Äôs web industry, influencing UX, website visibility and overall commercial success. This article summarizes the topics discussed at this year‚Äôs We Love Speed conference and gives an overview of the 2023 approach to web performance.\n\nTable of contents\n\n\n  Why optimize your sites‚Äô webperf?\n    \n      It‚Äôs all about money ü§ë\n      What about SEO?\n    \n  \n  How do you take up performance projects?\n    \n      Build a long-term strategy based on usages\n      Take cognitive biases into account to prioritise optimisations\n      Performance must be a company-wide concern\n    \n  \n  What are the tools to measure and improve performance?\n    \n      Use the right tools to understand performance issues‚Ä¶\n      ‚Ä¶and the right approach to fix them\n      MPA vs SPA\n    \n  \n  Implement performance improvements\n    \n      Using bfcache\n      Preloading, Preconnecting and HTML\n      Lazy‚Ä¶\n      Taking care of the Consent Management Platform (CMP)\n      Self-host blocking thirdparties\n      Use Server-side Rendering (SSR) and optimize it!\n      103 Early hints\n    \n  \n  Webperf at Bedrock\n\n\nWhy optimize your sites‚Äô webperf?\n\nIt‚Äôs all about money ü§ë\n\nImproving the webperf just for the technical challenge is not interesting in a company‚Äôs perspective and can‚Äôt make the approach durable. Boris Schapira in his talk ‚ÄúParlons de valeur‚Äù üá´üá∑ shows how important it is to talk about value to make optimisation projects part of an organisation‚Äôs strategy.\n\nWeb performance is essential, as it has a direct impact on user satisfaction, engagement and the overall success of a website. Fast performance enables users to access information quickly, interact without delay and navigate smoothly. On the other hand, slow loading times, frozen interfaces or high response times can lead to user frustration, lower engagement and higher bounce rates.\n\nA lot of articles on the net have proved the impact of the webperf on users‚Äô engagement.\n\n\n  Here is a great article from Google or hundreds of case studies from WPO stats, a site listing case studies demonstrating the impact of web performance optimization.\n\n  Reducing perceived waiting time by 40% increased search engine traffic and sign-ups by 15% at Pinterest. For Mobify, each 100ms reduction in homepage loading speed yields an average increase in annual sales of almost $380,000!\n\n\nSo, depending on the site‚Äôs field, it‚Äôs relatively easy to link the webperf approach to an objective of user views or loyalty, but also with the brand image. And the company‚Äôs governance will be able to quickly measure the benefits in terms of return on investment.\n\nWhat about SEO?\n\nMoreover, search engines such as Google also give a relative importance on website performance in their ranking algorithms. More optimized sites often benefit from better positioning in search results, which can have an impact on their visibility and traffic.\n\nHowever, Philippe Yonnet‚Äôs talk ‚ÄúQuel est le v√©ritable impact des probl√®mes de web performance sur le SEO‚ÄØ?‚Äù üá´üá∑ reveals that actually the Google webperf scoring is in reality quite low in all the parameters taken into account in the ranking algorithm. Relevant content is always the first factor in top positioning. As it was the case with responsive design a few years ago, the Google‚Äôs communication around webperf is aimed more at encouraging good practices in the web community than at penalizing slow sites.\n\nHow do you take up performance projects?\n\nBuild a long-term strategy based on usages\n\nYou need a strategy and a speech! Boris Schapira üá´üá∑ defines 5 steps:\n\n\n  Identify the sticking point\n  Associate a value to the project for the business\n  Invest time for the optimizations\n  Evaluate the ROI\n  Sustain the approach over time (it should never be a one shot project)\n\n\n\n  We now universally use the Google‚Äôs Core Web Vitals KPI (CWV) to model the UX:\n\n  \n    Do users have a stable visual response? ‚û°Ô∏è it‚Äôs measured through the Cumulative Layout Shift (CLS)\n    Do users quickly see content they can trust? ‚û°Ô∏è it‚Äôs measured through the Largest Contentful Paint (LCP)\n    Can users quickly interact with the page in a qualitative way? ‚û°Ô∏è it‚Äôs measured through the First Input Delay (FID)\n  \n\n\nBut these KPI are a limited reflection of the UX, and it is hard to link it to a representative value. It‚Äôs possible to have a super-fast LCP and still have a feeling of slowness or discomfort on the user‚Äôs side. For example, a page that quickly displays the thumbnail of a film (LCP criterion) but takes a long time to display the film‚Äôs cast list (which is the content the user wants) won‚Äôt be great for the UX.\n\nYou need to correlate these measures with usages. There is no magic solution for this but segmenting sessions is a good practice to focus on relevant optimizations:\n\n\n  take a period of time where the measure is representative (e.g. on our streaming sites, the activity period is mainly in the evening)\n  take into account the fact that different actions or events on your site may not involve the same users (so not the same devices, etc.)\n  take ‚Äúshort‚Äù indicators to measure the impact of the performance (e.g. if users are on a product list, monitor the viewing of the product page and not the purchase of the product)\n  take into account the market you‚Äôre addressing (e.g. food click &amp;amp; collect users are more likely to wait the loading of the next page than users on a retail site)\n\n\nTake cognitive biases into account to prioritise optimisations\n\nWhen it comes to prioritising performance subjects, there are several strategies to consider. Philip Tellis in his talk ‚ÄúUnderstanding Cognitive Biases in Performance Measurement‚Äù üá¨üáß shed light on the psychological aspect of performance, focusing on cognitive biases.\n\nPhilip made a powerful statement: ‚ÄúIf you have a brain, you have a bias‚Äù. Let‚Äôs explore three examples that demonstrate how biases can impact your work and how you can leverage them to your advantage.\n\nSlowest is the norm\n\nThe negativity bias reveals that negative experiences tend to leave a stronger impression than positive ones, even if their intensity is the same. This is crucial to keep in mind when optimizing your app‚Äôs performance. Ideally, the slowest page should not be more than 15 times slower than the fastest page. Users might perceive the slower page as the norm for your app.\n\nTo counteract this bias, it‚Äôs important to be transparent with your users. You can display a message indicating that the page is slow and assure them that efforts are underway to resolve the issue. Interestingly, slow pages have been associated with a 38% increase in heart rate, similar to watching a horror movie. By prioritizing performance and being honest with your users, you can mitigate stress and anxiety.\n\nTake care at the beginning and end\n\nThe serial position effect highlights that people tend to remember the beginning and end of an experience more vividly. This is particularly relevant for our work in developing applications for Smart TVs, where performance challenges arise in two areas: page loading and remote control navigation. Taking this bias into account, we‚Äôve learned that investing more time in optimizing page load times can significantly increase end-user satisfaction. Furthermore, statistics show that a 500ms increase in delay leads to a 26% rise in frustration.\n\nTake care at the beginning üòÖ\n\nThe escalation of commitment bias refers to the inclination to persist in an endeavor once a significant investment of money, effort, or time has been made. Applying this bias to our app development, we recognize the importance of ensuring that the initial pages users encounter are fast, as this positively influences user retention. There is a direct correlation between a smooth initial experience and increased user engagement.\n\nThese are just three examples among many others that Philip presented in his thought-provoking talk. It was an insightful perspective on performance, viewed through the lens of our cognitive biases. We all possess biases, and as developers, our objective is to acknowledge them and harness their power to inform the future updates of our apps.\n\nPerformance must be a company-wide concern\n\nWhile optimizing specific parts of your app is essential, it‚Äôs important to remember that you work in a company composed of diverse individuals with varying roles, perspectives, and concerns. Therefore, it‚Äôs crucial to extend performance awareness across all departments of your organization.\n\nLa Redoute, an international clothing retailer, provides an excellent example of addressing this issue by creating a performance community within their company through the talk ‚ÄúComment construire une communaut√© Web Perf dans son orga ?‚Äù üá´üá∑.\n\nTo provide some context, La Redoute faces not only performance issues but also challenges related to synchronization and contributions on their platform. To tackle these challenges, they established a community that brings together individuals from different departments, including technical, marketing, finance, design, product, SEO, and more.\n\nIt‚Äôs vital to ensure that performance is not solely the responsibility of one department, as each department can contribute valuable insights and support that ultimately benefit the entire company.\n\nThis community meets weekly to exchange knowledge and share updates on performance-related topics. After six months, they began witnessing the initial benefits, and within one year, they deemed the community mature. They continue to hold these meetings, recognizing that the objective is not to prevent all issues but rather to minimize their impact and respond swiftly.\n\nLa Redoute‚Äôs approach is truly inspiring and serves as a great example of fostering collaboration and cross-departmental engagement to address performance challenges.\n\nWhat are the tools to measure and improve performance?\n\nWhen analyzing a website‚Äôs performance, it‚Äôs essential to take several key criteria into account in order to become the ‚ÄúSherlock Holmes of Web Performance‚Äù as would say Ludovic Lefebvre üá´üá∑. In addition of the CWV mentioned above, there are other commonly used performance analysis criteria:\n\n\n\nTo measure/evaluate your site‚Äôs performance according to these criteria, there are 3 types of sources: Local (devtools), Synthetic (CI/CD / online scan tools), RUM (Real User Monitoring).\n\n\n\nUse the right tools to understand performance issues‚Ä¶\n\nJean-Pierre Vincent gives more details about how to measure performance in his talk ‚ÄúMesure ou meurs : diagnostics rapides‚Äù üá´üá∑, but here is a summary of the key tools:\n\n\n  Local development tools - devtools\n    \n      Chrome &amp;amp; Firefox DevTools: a set of development tools integrated into the browser, for analyzing performance, debugging JavaScript code, inspecting DOM elements, etc.\n      Lighthouse DevTools (Chrome): a Chrome DevTools extension that provides automated audits to evaluate performance, accessibility, SEO optimization, etc.\n    \n  \n  Synthetic testing - CI/CD and online analysis tools\n    \n      WebPageTest: an online tool for evaluating website performance by running tests on different browser configurations, network connections and geographical locations.\n      Lighthouse CI: a Lighthouse-based continuous integration (CI) solution that automates web performance and quality audits with every deployment.\n    \n  \n  Real User Monitoring\n    \n      New Relic: an application performance monitoring platform that provides real-time visibility into the performance of your web applications, including usage metrics, load times, etc.\n      Grafana: a data visualization platform that can be used to display and analyze performance monitoring data collected from a variety of sources, including RUM tools.\n    \n  \n\n\n\n\nIt‚Äôs important to note that each type of analysis data has its own advantages and is used in specific contexts. Local data is useful for real-time development and debugging, synthetic data enables performance to be compared and optimized in a reproducible way, while RUM data, just as explained by Tim Vereecke in his ‚ÄúNoise Canceling RUM‚Äù talk üá¨üáß, provides a true view of UX. It is generally recommended to use a combination of these different data sources to identify/target problems or opportunities for performance improvement.\n\n‚Ä¶and the right approach to fix them\n\nOnce you‚Äôve identified the relevant performance criteria and used the analysis tools to measure your website‚Äôs current performance, it‚Äôs essential to stabilize the various criteria before setting improvement targets and finally implement said improvements. Here are the steps to follow:\n\n\n  Stabilize criteria\n    \n      Analyze the results of the performance measurements for each criterion you‚Äôve identified. Identify areas where improvements are needed, and problems that affect performance stability. For example, LCP depends on the appearance of the widest element on the page. It‚Äôs important that this element is always the same on the same page type, otherwise there will be no consistent result and optimization will be impossible.\n      Examine performance metrics over a sufficiently long period of time to detect trends and variations. This will help you understand whether performance problems are constant or sporadic.\n      Identify factors that could influence performance, such as code changes, server updates, new features, content contributions, etc.\n    \n  \n  Setting improvement targets\n    \n      Determine the performance levels you want to achieve for each criterion, based on benchmarks, best practices or objectives specific to your application.\n      Define measurable and realistic objectives for each performance criterion. For example, you could aim to reduce page load time by 20%, improve server response time by less than 200 ms, or reduce bounce rate by 15%.\n      Prioritize goals according to their impact on your website‚Äôs UX and business objectives (see above section about cognitive biases).\n    \n  \n  Plan and implement improvements\n    \n      Identify the specific actions to be taken to achieve each improvement objective. This may include code optimizations, server configuration adjustments, resource optimizations, caching, etc.\n      Establish an action plan for each objective, defining the necessary steps, responsibilities, deadlines and resources required.\n      Implement improvements iteratively, and measure performance regularly to assess the impact of changes.\n      Repeat the process of analyzing, stabilizing and improving performance iteratively to continue optimizing your website.\n    \n  \n\n\n\n  ‚ö†Ô∏è It‚Äôs important to bear in mind that website performance is often an ongoing process, as needs, functionalities and conditions change over time. It is therefore advisable to regularly monitor performance and continue to identify new opportunities for improvement to deliver a superior UX.\n\n\nMPA vs SPA\n\nThe performance possibilities are not the same between Single Pages Applications (SPA) and Multiple Pages Applications (MPA).\n\nMPAs are simple to measure: each page is independent. The first page loads resources and then caches certain resources. It‚Äôs therefore important to be able to distinguish between measurements based on a user‚Äôs first visit and those based on browsing (which is much faster).\n\nSPAs are more difficult, because apart from the initial page load, there is other page load. Initial loading is often heavier (because of JS framework), and therefore slower. Page changes can‚Äôt be measured simply since only the initial load is taken into account. Research is currently underway in browsers to improve the collection of information on page changes in SPAs. This new metric is called soft-navigation and is currently being tested on Chrome and specified by web organizations. Yoav Weiss explains this in his talk ‚ÄúSoft Navigations are hard!!‚Äù üá¨üáß.\n\nImplement performance improvements\n\nThere are many technical tips for improving performance. While the list is far from exhaustive, we can go here through some of those mentioned at the conference.\n\nUsing bfcache\n\nAt the time of writing, around 10% of a given user‚Äôs navigation consists of clicking on the ‚ÄúGo back‚Äù button of their browser. More often than not, however, these users will then need to wait for the page to load again, despite the fact that the browser already did that work a few seconds beforehand.\nIn order to help reduce that waiting time, browsers recently began to release what‚Äôs called the Back/Forward cache (Also known as bfcache).\n\nLike its name suggests, it caches the page the user is leaving, so that if they go back using their browser features, they‚Äôll see the cached page instead. Barry Pollard talks about the bfcache in the ‚ÄúTop Core Web Vitals Recommendations for 2023‚Äù talk üá¨üáß.\n\nPreloading, Preconnecting and HTML\n\nAnother possible improvement explained by Barry Pollard is to use HTML properties to preload and preconnect resources. He explains here that there is one important rule with HTML and the way it‚Äôs processed by web browsers: it is parsed line by line, and it can be paused.\n\nWhen a browser is trying to render a web page, if it runs across something trying to call an url (such as loading a CSS file), then it‚Äôll wait until said CSS file is fully loaded, before being able to keep going.\nHowever this behavior can be somewhat altered by using the rel HTML property to prepare some of that stuff ahead. For example, rel=‚Äùpreload‚Äù will tell the browser to start loading said CSS asynchronously, as soon as a user opens the page. For an API, it will be the rel=‚Äùpreconnect‚Äù that will allow the website to start the handshake with an API as soon as possible.\n\nIf a resource needs to be even higher-priority, there is the fetchpriority attribute which will make the browser fetch that resource as soon as possible. If used correctly, it can have immense benefit on LCP.\n\nLazy‚Ä¶\n\nImages, and especially high-quality ones, can take up a lot of a website‚Äôs loading time, and are often considered the Largest Content of a website, dictating the LCP value. However, usually a user isn‚Äôt seeing every image of a website at the same time. That‚Äôs where we can use lazy-loading.\n\nThe goal of lazy-loading is to make sure that a user is only loading the images they are seeing. Here again exists an HTML attribute that can take care of everything, named loading. It can receive the lazy value in order to only load an image when approaching the screen.\n\nHowever, that lazy-loading property is not limited to images only. A component can be lazyloaded as well, such as a chat component that doesn‚Äôt need to load until the user clicks on the chat icon.\n\nTaking care of the Consent Management Platform (CMP)\n\nA lot of websites, us included, are using a lot of third parties. We are not judging their usefulness here, we need them. This is why Andy Davies dedicates an entire talk about how to tame the speed impact of 3rd-party tags üá¨üáß.\n\nAs he explains, each new thirdparty comes with its own compounding issues. Adding another domain and thus another need to connect to a distant API, reheating the connection‚Ä¶ A lot of these actions are taking network time, time that is actively needed to load a lot more data for a website.\n\nIt is important to understand how our thirdparties work, and the limitations they inherently have. For example, we CANNOT, and should NEVER try to preconnect to them. Preconnecting to a thirdparty discloses a user IP address, which according to GDPR laws at the time of writing, is considered a personnel data. If our user did not consent to share their data, we should never try to connect in the first place.\n\nTherefore, a big hurdle is that we need to collect the user‚Äôs consent to be able to connect to our thirdparties API. The first improvement to make is therefore to optimize the CMP, to make sure it loads fast and it shares the user‚Äôs consent with us as soon as possible.\n\nSelf-host blocking thirdparties\n\nSometimes, thirdparties can be needed to display anything at all to the user (e.g. for A/B testing). Since our user‚Äôs browser has limited network capabilities, it will try to prioritize its requests, and requests that are going to another domain are far down the line. For this exact reason, Andy Davies, in his talk üá¨üáß, recommends to self-host these thirdparties, so that they are on the same domain as our website and are prioritized by the browser.\n\nUse Server-Side Rendering (SSR) and optimize it!\n\n\n  ‚ÑπÔ∏è This is far from an easy to implement solution, and may require you to change your framework altogether.\n\n\nImplementing SSR is a solution that is so powerful that it had a dedicated talk, where K√©vin Raynel and Marting Guiller explained how Lazy Hydrate, Never Hydrate, and Resumable JS üá´üá∑ can be used to improve Web Performance.\n\nBy allowing the code of an application to be pre-rendered in a server before being sent to a browser, you can dramatically decrease the time it takes for a user to be able to interact with the page. At Bedrock, we understood the power of SSR very early on, when we implemented it with React in 2015 for the new JS version of 6play.fr! Since then, we haven‚Äôt looked back, but we will need to fine-tune the hydratation to keep the best.\n\n103 Early Hints\n\n\n  Do note that this is an experimental feature ‚ö†Ô∏è\n\n\nIn his excellent Web Protocols for Frontend Developers talk üá¨üáß, Robin Marx details extensively what is called the 103 Early Hints feature and how it can be used to improve your performance.\n\n103 Early Hints is a status code used to send preliminary headers in a response, providing the client with potential resource information before the main response is sent. This enables the browser to optimize its call flow.\n\nThere are many other implementation tips covered by the conference, such as the new Speculation Rules API, but we can‚Äôt talk about everything here üòâ\n\nWebperf at Bedrock\n\nBedrock‚Äôs technical teams have long been aware of the importance of web performance. Since we set up our first full JS frontend in 2015, we have been implementing best practices (SSR, lazy loading, prefetching, code spitting, image optimization, etc.) and we try to carry out regular optimization projects.\n\nBut we know that we still have a lot to do in this area, particularly in terms of long-term monitoring and company-wide concern.\n\nFollowing this conference, there are lots of things we‚Äôd like to try out, such as using the fetchpriority attribute to improve our LCP, optimizing the hydration of our SSR (perhaps with the use of React Server Components) or studying the new experimental features.\n\nOn our TV platforms developed in JS, hardware constraints are forcing us to go even further in optimizing performance, particularly in terms of memory usage. For example, a project is underway to use a global lazy loading to improve the management of rows of programs and videos. To be continued‚Ä¶\n\nKeep at it! üí™\n\nAll in all, performance is definitely anything but an easy task to do. There is no magic formula that will make all your problems go away. Even if you manage to hold a long workshop of several weeks (or even months!) that will finally fix your performance issues, if it does not become a habit in your developer teams, it will never be over. Performance is a habit, built through practice and good communication. But it can be achieved.\n\nAuthors: Etienne Doyon, Alexandre Gory, Maxime Blanc, Florent Dubost\n"
} ,
  
  {
    "title"    : "Bedrock √† la GopherCon EU (2023)",
    "category" : "",
    "tags"     : " conference, berlin, tech, go",
    "url"      : "/2023/07/10/gophercon-eu-2023-a-berlin.html",
    "date"     : "July 10, 2023",
    "excerpt"  : "Depuis maintenant presque 1 an, la verticale Backend de Bedrock, s‚Äôouvre √† d‚Äôautres langages de programmation que PHP, √†\nsavoir Golang et Rust.\nC‚Äôest pourquoi cette ann√©e, pour la 1√®re fois, 6 de nos coll√®gues ont particip√© (2 en pr√©sentiel, 4 √†\nd...",
  "content"  : "Depuis maintenant presque 1 an, la verticale Backend de Bedrock, s‚Äôouvre √† d‚Äôautres langages de programmation que PHP, √†\nsavoir Golang et Rust.\nC‚Äôest pourquoi cette ann√©e, pour la 1√®re fois, 6 de nos coll√®gues ont particip√© (2 en pr√©sentiel, 4 √†\ndistance) √† la GopherCon EU ayant lieu √† Berlin.\n\n\n\nLa GopherCon EU, c‚Äôest un peu comme le Forum PHP, mais pour le Go et √† un niveau international. √Ä cette √©dition, environ\n600 participants √©taient pr√©sents sur place ou √† distance depuis les 4 coins du monde (Br√©sil, √âtats-Unis, Afrique du\nNord, Europe, Asie, Australie‚Ä¶) et bien s√ªr d‚Äôautres fran√ßais.es.\n\nElle se d√©roule sur 1 semaine enti√®re :\n\n\n  Jour 1 : Visite de Berlin et table ronde\n  Jour 2 : Atelier\n  Jour 3 et 4 : Conf√©rences\n  Jour 5 : Interview avec les UX de l‚Äô√©quipe Go\n\n\nL‚Äô√©v√®nement avait lieu dans un espace assez typique de l‚ÄôAllemagne, un ‚ÄúBiergarten‚Äù que l‚Äôon pourrait traduire par\n‚ÄúBrasserie en plein air‚Äù. On vous rassure tout de suite pas de bi√®re pendant les conf√©rences ;)\n\nLes conf√©rences se d√©roulaient principalement dans la salle de concert du Biergarten, cependant l‚Äôapr√®s-midi, il y avait\nune 2·µâ track qui se tenait dans un entrep√¥t de ventes aux ench√®res.\n\n\n\nLieux atypiques, pour nous, bonne ambiance, des gophers.euses tr√®s sympas, amicaux, respecteux.ses, tout pour assister √†\ndes conf√©rences fantastiques.\n\nNous ne pouvons vous pr√©senter l‚Äôensemble des conf√©rences, mais en voici quelques-unes nous ayant marqu√©s pour vous\ndonner envie d‚Äôen voir davantage :\n\nKeynote - State of the Go Nation\n\nLes deux jours de conf√©rences ont d√©marr√© par une Keynote donn√©e par Cameron Balahan,\nle Product Lead pour le Go chez Google.\n\nCette keynote a √©t√© l‚Äôoccasion de revenir sur l‚Äôhistorique de Go en tant que plateforme depuis sa cr√©ation en 2007\njusqu‚Äô√† aujourd‚Äôhui avec l‚Äôarriv√©e prochaine de la version 1.21 en ao√ªt :\n\n\n  2007 : Cr√©ation de go par Google (utilis√© √† 20% sur les projets Google)\n  2009 : Go devient open source\n  2012 : Sortie de Go 1.0, avec pour objectif de construire une plateforme stable et compatible dans le temps\n  2015 : Sortie de Go 1.5, qui apporte une augmentation des performances (via la mise en place du ¬´ low latency garbage collection ¬ª), un compilateur et runtime √©crits en Go\n  2018 : Introduction des modules. Am√©lioration de la s√©curit√©. Nouveaut√©s : SBOM, fuzzing\n  2022 : Introduction des generics sans breaking changes\n\n\nUne fois cet historique pr√©sent√©, Cameron a parl√© de l‚Äôavenir du Go et de son √©cosyst√®me dans les ann√©es √† venir. Pour\nl‚Äô√©quipe de d√©veloppement (dont plusieurs membres √©taient pr√©sents), le Go n‚Äôest pas juste un langage de programmation\nmais tout un √©cosyst√®me : des outils pour les IDE (le plugin Go pour VS Code est maintenu par la team), la gestion des\nd√©pendances, les syst√®mes de test, le formatting, le profiling, la CLI, la r√©tro-compatibilit√©, la documentation web et\nbien s√ªr le langage lui-m√™me.\n\nLa r√©tro-compatibilit√© est un point sur lequel il a beaucoup insist√©, en parlant notamment de l‚Äôajout des generics\ndans la version 1.18, et ce, sans aucun breaking change. Cet ajout est pour la Core\nTeam la modification la plus complexe ayant eu lieu sur le langage, et la plus complexe qu‚Äôil n‚Äôy aura jamais. Tout √ßa\npour dire qu‚Äôils ne voient pas de raison √† ce qu‚Äôil existe un jour un Go 2.0 et que le langage restera donc toujours\nr√©tro-compatible dans ses futures versions.\n\nUseful Functional-Options Tricks For Better Libraries\n\n\n\nJulien Cretel nous a pr√©sent√© le pattern functional options √† travers l‚Äôexemple d‚Äôune\npetite librairie de gestion de CORS.\n\nPour faire cette premi√®re librairie, une premi√®re approche ‚Äúclassique‚Äù serait d‚Äôutiliser une fonction de cr√©ation de\ncors avec des param√®tres pour chacune des options.\n\nIl note plusieurs inconv√©nients :\n\n  Pas tr√®s ergonomique\n  Pas assez expressif\n  Et pas extensible\n\n\nfcors.NewCORS([]string{&quot;https://example.com&quot;}, 0, nil)\n\n\nUne premi√®re alternative est l‚Äôutilisation d‚Äôune struct Config qui contiendrait toutes les options possibles et de passer\ncette struct en param√®tre :\n\ntype Config struct {\n  Origins []string\n  MaxAgeInSeconds uint\n  RequestHeaders []string\n}\n\nfunc NewCORS(cfg Config) *Middleware\n\n\nC‚Äôest plus extensible, mais certains inconv√©nients restent :\n\n\n  pas de possibilit√© de ne pas passer un param√®tre\n  peu expressif\n\n\nIl propose alors une autre possibilit√© : le functional options :\n\nfunc NewCORS(opts ...Option) *Middleware\n\nfunc FromOrigins(origins ...string) Option\nfunc MaxAgeInSeconds(delta uint) Option\nfunc WithRequestHeaders(names ...string) Option\n\n\nL‚Äôid√©e est d‚Äôutiliser des constructeurs nomm√©s pour instancier des options selon les besoins et de passer ses options en\nparam√®tre !\n\nLes avantages qu‚Äôil y voit sont :\n\n  le syst√®me est facilement extensible\n  c‚Äôest beaucoup plus expressif\n\n\nIl a ensuite d√©taill√© certaines astuces pour aller plus loin avec ce pattern :\n\n  ¬´ When order doesn‚Äôt matter, users are happier ¬ª\n  ¬´ Multiple calls to the same option ¬ª\n  ¬´ Immutability ¬ª\n\n\nVous pouvez retrouver les slides de sa pr√©sentation ici\net cerise sur le g√¢teau, une librairie qui sert d‚Äôexemple existe : https://github.com/jub0bs/fcors\n\nTowards modern development of cloud applications with service weaver\n\n\n\nCette conf√©rence, pr√©sent√©e par Robert Grandl, est int√©ressante pour une entreprise\nqui commence √† utiliser Go. On peut se poser la question : Faut-il partir tout de suite sur des micro-services ou bien\ncommencer par un monolithe ?\n\nL‚Äôid√©e g√©n√©rale de Service Weaver est de permettre aux d√©veloppeurs de se concentrer sur le\nd√©veloppement de leur application sans se soucier de cette question d‚Äôarchitecture.\n\nGr√¢ce √† ce framework, une application peut √™tre d√©velopp√©e comme une sorte de monolithe via des modules Go qui\ncommuniquent via des interfaces. Le framework permet ensuite de d√©ployer cette application de deux fa√ßons diff√©rentes :\n\n\n  un monolithe, o√π les modules communiquent via des appels directs dans un seul fichier binaire final\n  des micro-services, o√π les modules communiquent via des appels r√©seaux (gRPC) et sont d√©ploy√©s dans des containers\ns√©par√©s\n\n\nDans le cas d‚Äôun d√©ploiement en Micro-services, le framework prend enti√®rement en charge la communication entre les\nmodules, sans aucun impact sur le code ou le d√©veloppeur.\n\nGo Sync or Go Home: Advanced Concurrency Techniques for Better Go Programs\n\nYarden Laifenfeld, Software Engineer chez Rookout, nous a pr√©sent√©e les fonctionnalit√©s moins connues des packages\nsync et x/sync.\n\nVoici les diff√©rents sous packages pr√©sent√©s lors de cette conf√©rence :\n\nsync - Wait Group\n\nWaitGroup va permettre d‚Äôattendre que toutes les t√¢ches soient termin√©es.\nSon utilisation simplifie l‚Äôimpl√©mentation, la lecture du code, mais aussi la performance compar√©e au couple\ngoroutines/channels.\n\nx/sync - Error Group\n\nErrorGroup permet de prendre en charge la propagation des erreurs, si une t√¢che renvoie une erreur, la t√¢che\nprincipale peut agir en fonction.\n\nx/sync - Single Flight (Do)\n\nDo de singleflight permet de ne pas ex√©cuter deux fois la m√™me t√¢che avec la m√™me valeur d‚Äôentr√©e. Si une entr√©e\nsimilaire arrive avant la fin de la pr√©c√©dente, cette entr√©e va attendre le r√©sultat de la pr√©c√©dente et retourner le\nm√™me r√©sultat faisant ainsi gagner en performance.\n\nYarden nous a indiqu√© que Kubernetes utilise ce m√©canisme dans le cached token authenticator depuis maintenant\nquelque temps.\n\nNous allons regarder dans les prochaines semaines si nous avons la possibilit√© d‚Äôutiliser ces patterns dans notre\ncodebase actuelle et l‚Äôavoir √† l‚Äôesprit pour nos futurs d√©veloppements.\n\nPour conclure\n\nBedrock participe depuis longtemps directement ou indirectement √† des √©v√©nements li√©s aux diff√©rentes technologies\nutilis√©es en interne.\n\nPar exemple en 2023 : Android Makers,\nl‚ÄôAFUP Day,\nle MiXiT,\nl‚ÄôAWS Summit,\nKubernetes Community Days\net Vue Amsterdam.\n\nC‚Äôest donc dans la m√™me logique que Bedrock a choisi de faire participer certains membres du backend √† la GopherCon.\n\nParmi les 6 coll√®gues pr√©sents √† la conf√©rence, certains viennent de commencer √† utiliser le langage de programmation Go\ntandis que d‚Äôautres commencent √† avoir une certaine exp√©rience. Ce f√ªt donc l‚Äôoccasion d‚Äô√©changer, de partager et de\nd√©couvrir, aussi bien entre nous qu‚Äôavec les autres participants, diff√©rents sujets au sein de l‚Äô√©cosyst√®me Go, qui\nprend une part croissante dans notre plateforme.\n\n\n"
} ,
  
  {
    "title"    : "You Need a Custom Gradle Plugin, and Here‚Äôs Why",
    "category" : "",
    "tags"     : " android, gradle, plugin",
    "url"      : "/2023/07/07/gradle-convention-plugins.html",
    "date"     : "July 7, 2023",
    "excerpt"  : "In the last couple of years, Gradle has been encouraging developers to work towards modularizing their projects. Of course, when effectively implemented, this approach offers several advantages, with build parallelization being a significant facto...",
  "content"  : "In the last couple of years, Gradle has been encouraging developers to work towards modularizing their projects. Of course, when effectively implemented, this approach offers several advantages, with build parallelization being a significant factor.\n\nBut splitting your Android project into many modules has a major drawback, at first: you need to write a build file for each of them.\n\nThe naive approach\n\nOne might be tempted to create ‚Äúcommon‚Äù Groovy files (also known as ‚Äúscript plugins‚Äù) and import them into each module. We can also define some properties in the root project, which can then be used in each subproject.\n\napply plugin: &#39;com.android.library&#39;\napply plugin: &#39;kotlin-android&#39;\napply plugin: &#39;kotlin-kapt&#39;\n\n// This imports a Gradle file which we can use everywhere\napply from: rootDir.path + &#39;/lib-common.gradle&#39;\n\nandroid {\n    // compileSdkVersion is defined in the root project\n    compileSdkVersion rootProject.ext.compileSdkVersion\n\n    defaultConfig {\n        minSdkVersion rootProject.ext.minSdkVersion\n        targetSdkVersion rootProject.ext.targetSdkVersion\n\n        consumerProguardFiles &#39;proguard-rules.pro&#39;\n        testInstrumentationRunner &quot;androidx.test.runner.AndroidJUnitRunner&quot;\n    }\n\n    compileOptions {\n        sourceCompatibility rootProject.ext.sourceCompatibility\n        targetCompatibility rootProject.ext.targetCompatibility\n    }\n\n    kotlinOptions {\n        jvmTarget = rootProject.ext.kotlinJvmTarget\n        freeCompilerArgs += rootProject.ext.kotlinCompilerArgs\n    }\n}\n\ndependencies {\n    // Dependencies are defined in a map in the root project\n    def dep = rootProject.ext.dependencies\n    implementation dep.&#39;androidx.core:core-ktx&#39;\n    implementation dep.&#39;androidx.paging:paging-runtime-ktx&#39;\n    implementation dep.&#39;com.squareup.okhttp3:okhttp&#39;\n\n    rootProject.applyTestDependenciesOn(dependencies)\n    rootProject.applyToothpickDependenciesOn(dependencies)\n}\n\n\nThis is the approach we were using before moving to a better system. These few following drawbacks made it obsolete and not recommended by Gradle maintainers.\n\n\n  Script plugins need to be imported individually for each module in your project. This means that your heap will grow a lot, and this approach will scale terribly on a project with many modules.\n  Relying on the rootProject in your modules‚àíor relying on subprojects from your root project, for that matter‚àíwill add unwanted dependencies between your modules, which will in turn defeat optimization mechanisms designed by Gradle, such as configuration-on-demand or configuration cache. These are made to help bring down the time Gradle spends configuring your project (i.e. reading the configuration and building the task graph) each time you build; it goes without saying that getting this time to decrease will make for happier and more productive developers.\n\n\nIn addition to these issues, we wanted to start modularizing much of our project. We already had about 150 modules, but we planned on making many more soon, so this would be a good time to find a future-proof architecture. Plus, this was a good opportunity to clear some tech debt: cleaning unused dependencies, moving to a version catalog‚Ä¶\n\nModern problems call for modern solutions\n\nCentralizing version management\n\nA significant challenge we faced, which is also common in the industry, is managing dependencies and versions across the entire project. Hard-coding the version of okhttp for every module is not recommended, as it can be tedious and error-prone.\n\nThere are several known solutions to this problem, such as storing versions in the root project or using a buildSrc script. But not only are some solutions bad for your build performance (see: reliance on the root project), almost all of them share an insoluble issue: tooling support.\n\nThere are multiple ways to be informed when your dependencies can be upgraded. You can rely on your IDE to highlight your outdated dependencies, which it does by trying to look for some string that‚Ä¶ looks like a Gradle . You can also rely on a tool like Renovate, which does the same thing on your CI. In either case, you probably could use a standard solution, where there is some kind of standard to declare your centralized dependencies, which both humans and machines can rely on consistently.\n\nTo solve this problem, Gradle introduced the version catalog:\n\n[versions]\nandroidCompileSdk = &quot;33&quot;\nandroidGradlePlugin = &quot;7.4.2&quot;\njvm = &quot;17&quot;\n\n[libraries]\nandroid-billingclient-core = { module = &quot;com.android.billingclient:billing&quot;, version.ref = &quot;billing&quot; }\nandroid-billingclient-ktx = { module = &quot;com.android.billingclient:billing-ktx&quot;, version.ref = &quot;billing&quot; }\nandroid-gradle = { module = &quot;com.android.tools.build:gradle&quot;, version.ref = &quot;androidGradlePlugin&quot; }\nandroid-installreferrer = { module = &quot;com.android.installreferrer:installreferrer&quot;, version = &quot;2.2&quot; }\nandroid-tools-desugar-jdk-libs = { module = &quot;com.android.tools:desugar_jdk_libs&quot;, version = &quot;1.1.5&quot; }\nandroid-tools-lint-api = { module = &quot;com.android.tools.lint:lint-api&quot;, version.ref = &quot;lint&quot; }\n\n[plugins]\nandroid-app = { id = &quot;com.android.application&quot;, version.ref = &quot;androidGradlePlugin&quot; }\nandroid-library = { id = &quot;com.android.library&quot;, version.ref = &quot;androidGradlePlugin&quot; }\n\n\nThis format has been great, even for a project as big as ours. It‚Äôs flexible: you can now store library versions, but plugin versions as well, and even just plain versions, which you can get from your custom plugin later on!\n\nAnd it‚Äôs a standard format, so it works out of the box with tools like Renovate or Android Studio.\n\nCode reuse\n\nThe direction of Gradle best practices in our industry is evident, with numerous talks and blog posts from big tech companies and even Gradle itself emphasizing the use of convention plugins.\n\nWhile the name may sound intimidating, convention plugins are actually pretty straightforward. They are Gradle plugins that can be applied to each module, ensuring consistent configuration across all of them.\n\nConvention plugins offer the advantages of build scripts and the elimination of duplicate configuration, all without the need for a dependency on the root project. The convention plugin is an isolated project, which could be stored in your monorepo, but could very well be stored in a completely different place. Unlike build scripts, it‚Äôs compiled and instantiated only once, and is then √ó*called** once for each module.\n\nCreating a convention plugin is similar to creating any custom Gradle plugin. If you haven‚Äôt had to do this yet, it looks like this:\n\n// settings.gradle\n// ‚Ä¶\nincludeBuild &#39;gradle-plugins/convention-plugin&#39;\n\n\nThis will include your convention plugin alongside your main project at build time, so you will be able to use its result for your main project‚Äôs build system.\n\nYou‚Äôll need a simple settings.gradle(.kts) file for your plugin. If your plugin is located in your monorepo, it will be very useful to be able to access its Version Catalog, so you can even share your dependency versions in the build files of your plugin.\n\ndependencyResolutionManagement {\n    versionCatalogs {\n        libs {\n            from(files(&quot;../../gradle/libs.versions.toml&quot;))\n        }\n    }\n\n    repositories {\n        google()\n        mavenCentral()\n        gradlePluginPortal()\n    }\n}\n\nrootProject.name = &#39;gradle-plugin-convention&#39;\n\n\nThen, you need a build.gradle(.kts) configuration script for your custom plugin. In order to configure other modules with the Android Gradle Plugin (AGP), for example, you will need access to the AGP‚Äôs classpath at build time in your plugin. You might be tempted to apply the AGP as a plugin, but you actually need to import it as an implementation.\n\ngroup = &quot;com.bedrockstreaming&quot;\nversion = &quot;1.0-SNAPSHOT&quot;\n\nplugins {\n    // This is a Gradle plugin written in Kotlin, import the Gradle Kotlin DSL\n    `kotlin-dsl`\n}\n\njava {\n    toolchain {\n        // This sets the JVM version needed to build this project.\n        // Notice that we set this version in the Version Catalog, and we can use it here!\n        languageVersion.set(JavaLanguageVersion.of(libs.versions.jvm.get()))\n    }\n}\n\ngradlePlugin {\n    plugins {\n        // Your custom plugin&#39;s module can actually contain many plugins.\n        // Create as many as you need - if you have multiple application modules, \n        // it might be useful to at least create one for library modules,\n        // and one for application modules.\n\n        create(&quot;androidMobileAppPlugin&quot;) {\n            id = &quot;com.bedrockstreaming.convention.application.mobile&quot;\n            implementationClass = &quot;com.bedrockstreaming.gradle.convention.android.application.AndroidMobileApplicationPlugin&quot;\n        }\n\n        create(&quot;androidLibraryPlugin&quot;) {\n            id = &quot;com.bedrockstreaming.convention.library.android&quot;\n            implementationClass = &quot;com.bedrockstreaming.gradle.convention.android.library.AndroidLibraryPlugin&quot;\n        }\n\n        create(&quot;jvmLibraryPlugin&quot;) {\n            id = &quot;com.bedrockstreaming.convention.library.jvm&quot;\n            implementationClass = &quot;com.bedrockstreaming.gradle.convention.jvm.JvmLibraryPlugin&quot;\n        }\n    }\n}\n\ndependencies {\n    // Note that we add the AGP and Kotlin plugin as implementations, which is unusual.\n    implementation(libs.android.gradle)\n    implementation(libs.kotlin.gradle)\n}\n\n\nThen, you‚Äôll need an extension, which is Gradle speak to describe a configuration interface. Each option you will add to your extension will be usable from your module‚Äôs build.gradle(.kts). This is one of the most powerful advantages of custom plugins: you can reuse code and still make it configurable!\n\nabstract class BaseConventionPluginExtension {\n\n    internal abstract val enableCompose: Property&amp;lt;Boolean&amp;gt;\n\n    /**\n     * Enable Jetpack Compose on this module, and add core libraries.\n     */\n    fun composeToolkit() {\n        enableCompose.set(true)\n    }\n\n    // ‚Ä¶\n}\n\n\nThen, it‚Äôs time to create the actual plugin class, the entry point for Gradle (specified in implementationClass above).\n\npackage com.bedrockstreaming.gradle.convention.android.library\n\nimport org.gradle.api.Plugin\nimport org.gradle.api.Project\nimport org.gradle.kotlin.dsl.create\n\nclass AndroidLibraryPlugin : Plugin&amp;lt;Project&amp;gt; {\n\n    override fun apply(target: Project) {\n        // This is where we declare that our extension will be available in a bedrock {} block.\n        val extension = target.extensions.create&amp;lt;AndroidLibraryExtension&amp;gt;(&quot;bedrock&quot;)\n        // ‚Ä¶\n    }\n}\n\n\nThat‚Äôs it for boilerplate! You‚Äôre free to architect the internals of your Gradle plugin however you want, but this Plugin::apply method will be the entry point for your configuration code. It will be called for each module on which your plugin has been applied.\n\nFor example, here‚Äôs how you might apply the com.android.library plugin to your module, and configure it:\n\nfun apply(target: Project) = with(target) {\n    // getPluginId is an extension function that reads the plugin ID from the version catalog\n    apply(plugin = getPluginId(&quot;android.library&quot;))\n\n    configure&amp;lt;LibraryExtension&amp;gt; {\n        compileSdk = getVersion(&quot;androidCompileSdk&quot;).toInt()\n    }\n\n    androidComponents.finalizeDsl {\n        configure&amp;lt;LibraryExtension&amp;gt; {\n            defaultConfig {\n                minSdk = getVersion(&quot;androidMinSdk&quot;).toInt()\n                consumerProguardFiles(&quot;proguard-rules.pro&quot;)\n            }\n        }\n    }\n}\n\n\nYou can reuse this principle and apply it to all your common configuration blocks. You can automatically add dependencies, add some unit testing configuration, set the correct JDK toolchain, build flags, and even configure other third-party plugins with the same mechanism. The sky is the limit!\n\nEnd result\n\nRemember our old build file, with its included Groovy scripts, referenced root project, custom extension functions? Here‚Äôs what it looks like now!\n\nplugins {\n    alias(libs.plugins.bedrock.library.android)\n}\n\nbedrock {\n    moshi(codegen: true)\n    composeToolkit()\n    unitTests()\n}\n\ndependencies {\n    implementation(libs.androidx.core.ktx)\n    implementation(libs.androidx.paging.runtime.ktx)\n}\n\n\nMuch nicer, isn‚Äôt it? ü§©\n\nIn summary\n\nThe scalability of our project has been significantly improved through the migration from included build scripts and root project dependencies. Although writing custom Gradle plugins can initially pose challenges due to the potential for frustrating errors resulting from a minor misunderstanding of the Gradle API, once you are set up, the maintenance becomes much easier. It feels more rewarding to work in harmony with Gradle, rather than working against the optimizations introduced with each Gradle update, knowing that we can automatically benefit from them. The version catalogs provide a convenient method for organizing dependencies, and the fact that our tooling recognizes the format is a significant advantage.\n\nIn conclusion, for developers working on medium-to-large Gradle projects, whether in the Android realm or elsewhere, I highly recommend exploring the use of convention plugins. Mastering them is not as difficult as it may seem, and they provide effective solutions to address real challenges that we all face day-to-day.\n\nCover image ¬© Isis Petroni\n"
} ,
  
  {
    "title"    : "Deux jours √† Android Makers by Droidcon 2023",
    "category" : "",
    "tags"     : " android, mobile, conference, makers",
    "url"      : "/2023/06/19/android-makers-23.html",
    "date"     : "June 19, 2023",
    "excerpt"  : "Il y a quelques semaines d√©j√†, nous avons pu nous rendre √† LA conf√©rence annuelle Android en France : Android Makers. Conf√©rence qui s‚Äôassocie tout juste avec une initiative un peu plus internationale qui est DroidCon (cf. notre pr√©c√©dent article,...",
  "content"  : "Il y a quelques semaines d√©j√†, nous avons pu nous rendre √† LA conf√©rence annuelle Android en France : Android Makers. Conf√©rence qui s‚Äôassocie tout juste avec une initiative un peu plus internationale qui est DroidCon (cf. notre pr√©c√©dent article, par exemple).\nL‚Äôoccasion d‚Äôassister √† des conf√©rences de speakers du monde entier mais √©galement de networker et revoir avec plaisir beaucoup de t√™tes connues !\n\nVoici p√™le-m√™le nos retours et les apprentissages que nous avons pu collecter durant ces 2 jours intenses !\n\nPar @Antoine Pitel\n\nSi on devait retenir une chose de la conf√©rence The Rise and Fall of Feature Teams de Danny Preussler, on pourrait dire en r√©sum√© que ‚ÄúLes d√©veloppeurs ont besoin d‚Äôautres d√©veloppeurs de la m√™me technologie pour s‚Äô√©panouir‚Äù. Le format en Feature Team peut facilement tomber dans le pi√®ge de l‚Äôisolement du d√©veloppeur sur sa technologie. Il est fondamentale de s‚Äôassurer soit de multiplier les profils de m√™me technos dans une team, soit d‚Äôorganiser un partage de connaissance et du pair programming tr√®s r√©current.\n\n\n\n\n\nLa conf√©rence Practical ADB usage to enhance your life! de Benjamin Kadel √©tait, pour moi, d√©finitivement la plus passionnante (au sens propre du terme passionn√©). L‚Äôusage d‚ÄôADB pour optimiser le travail quotidien en d√©veloppement comme en test me parait ultra efficace. On a pu y d√©couvrir notamment une astuce particuli√®rement utile : il est possible via ADB de nettoyer, refuser ou accepter les permissions demand√©es au framework par l‚Äôapp. Un gros gain de temps quand on doit d√©velopper sur une feature qui n√©cessite une permission sp√©cifique (et le statut de celle-ci) .\n\n\n\n\n\nJe ne me suis jamais vraiment pench√© sur Android Auto, shame on me üòâ Mais la conf√©rence ‚ÄúGoing on a road trip with Android Auto‚Äù de Carlos Mota nous a ouvert un tout nouvel univers de jeu que j‚Äôai h√¢te d‚Äôexplorer chez Bedrock ! En effet gr√¢ce aux derni√®res √©volutions d‚ÄôAndroid Auto il est d√©sormais possible de publier, comme avant, des services audio (radio, podcast,..), mais d√©sormais aussi des services vid√©o ! L‚Äôacc√®s vid√©o n‚Äô√©tant disponible qu‚Äô√† l‚Äôarr√™t du v√©hicule, information que le framework Android Automotive fourni √† travers la classe CarUxRestrictions du package android.car.drivingstate et sa m√©thode isRequiresDistractionOptimization().\n\n\n\n\n\nIl est tellement plaisant, mais rare, d‚Äôassister √† des conf√©rences qui parlent de CD et de publication ! En cela la conf√©rence How to ship apps better, faster, stronger de Fabien Devos √©tait passionnante et pleine d‚Äôapprentissage √† diffuser au plus grand nombre ! J‚Äôen retiendrai 2 de mon c√¥t√© :\n\n  La notion de ‚Äúrelease Train‚Äù et cette m√©taphore du train qui part √† heure fixe de mani√®re r√©guli√®re. Avec de plus l‚Äôinfo que la p√©riodicit√© hebdomadaire semble √™tre adapt√©e aux projets qui collaborent avec des stores comme App Store et Google Play Store ;\n  L‚Äôabsolue n√©cessit√© de disposer d‚Äôun syst√®me de feature flipping robuste et couvrant le maximum du fonctionnel du service. Un vaste chantier !\n\n\n\n\nPar @Baptiste Candellier\n\nComme tous les ans, Android Makers nous propose des talks vari√©s, des derni√®res nouveaut√©s de Jetpack Compose au management d‚Äô√©quipe, en passant par l‚Äôhabituelle keynote humoristique de Romain Guy et Chet Haase. J‚Äôai s√©lectionn√© pour vous mes talks pr√©f√©r√©s.\n\nLe talk 90s Website ‚Ä¶ in 2023 on mobile in Compose ‚Ä¶ for science de Maia Grotepass a √©t√© pour moi le plus original, intriguant et int√©ressant de la conf√©rence. Maia nous a guid√© √† travers son projet de c≈ìur : reproduire, gr√¢ce √† Jetpack Compose, le look-and-feel d‚Äôun site web des ann√©es 90. Un m√©lange de technologique moderne, qui est √©crit pour tourner √† la fois sur Android mais √©galement sur desktop, gr√¢ce aux efforts de Jetbrains sur le multi-plateforme. Un projet qui pourrait sembler futile au premier abord, mais Maia nous plonge dans son parcours nostalgique tout en nous expliquant de mani√®re p√©dagogique les APIs d‚Äôanimation et de dessin de Compose, que nous avons assez peu souvent l‚Äôoccasion d‚Äôutiliser dans des projets professionnels, qui se reposent souvent sur des composants pr√©-con√ßus. √Ä voir, que vous soyez amateur¬∑ice de web old-school ou de Canvas.\n\n\n\n\n\nForging the path from monolith to multi-module app, par Marco Gomiero, s√©duira les adeptes d‚Äôarchitecture. Ce talk vante non seulement les avantages d‚Äôun projet Gradle multi-modules, mais nous d√©taille tous les choix architecturaux qui en d√©coulent, de fa√ßon subjective. Un retour d‚Äôexp√©rience sur ce sujet, qui est une trend relativement r√©cente dans l‚Äô√©cosyst√®me Android, est tr√®s int√©ressant et permet non seulement de guider nos d√©cisions d‚Äôavenir, mais aussi de regarder d‚Äôun nouvel ≈ìil nos propres d√©cisions architecturales. Marco nous propose un apercu de sujets tels que les types de modules, la gestion de la navigation, les version catalogs, les convention plugins ‚Äî autant de sujets qui sont √† l‚Äô√©tat de l‚Äôart des projets Gradle, et qui m√©rient bien un partage d‚Äôexp√©rience !\n\nMarco apporte un bon √©quilibre en nous montrant le travail de son √©quipe sur l‚Äôapp Tier, tout en nuan√ßant sur le fait que l‚Äôarchitecture n‚Äôest pas une science exacte. On retiendra cette citation frappante :\n\n\n  Sometimes the ‚Äúbest decision‚Ñ¢‚Äù is not the best one\n\n\n\n\n\n\nOn termine pour ma part avec un talk qui parle encore d‚Äôarchitecture, mais cette fois en ce qui concerne la migration vers Compose. Un Design System, √ßa se Compose !, avec Jean-Baptiste Vincey et Julie Gl√©onnec, nous pr√©sente la direction prise par les √©quipes de Deezer en ce qui concerne la migration de leur design system vers Jetpack Compose. C‚Äôest un sujet d‚Äôactualit√© et qui aura tr√®s certainement des ramifications sur les ann√©es √† venir ‚Äî Bedrock a entam√© cette ann√©e, j‚Äô√©tais donc curieux de conna√Ætre les approches prises par d‚Äôautres √©quipes travaillant sur de grosses applications avec des design systems bas√©s sur des vues XML.\n\nLes √©quipes de Deezer ont choisi de r√©√©crire enti√®rement leur impl√©mentation du design system en Compose, et de migrer leur app √©cran par √©cran. Un choix qui a des avantages - la facilit√© de migration des nouveaux √©crans, l‚Äôabsence de legacy dans les nouveaux composants, mais aussi des inconv√©nients comme la n√©cessit√© de garder √† jour deux versions des composants jusqu‚Äô√† la migration compl√®te de l‚Äôapp. Deezer nous expose dans cette pr√©sentation passionnante leurs choix et leur chemin vers Compose.\n\n\n"
} ,
  
  {
    "title"    : "Bedrock √† l&#39;AFUP Day Lyon (2023)",
    "category" : "",
    "tags"     : " conference, lyon, tech, php, afup",
    "url"      : "/2023/06/06/afup-day-lyon-2023.html",
    "date"     : "June 6, 2023",
    "excerpt"  : "Comme √† l‚Äôaccoutum√©e, les √©quipes backend de Bedrock √©taient pr√©sentes le 12 mai 2023 √† CPE (au Campus Lyontech) pour une nouvelle √©dition de l‚ÄôAFUP Day.\n\nPour suivre les conf√©rences qui gravitaient autour de PHP bien s√ªr, mais aussi pour soutenir...",
  "content"  : "Comme √† l‚Äôaccoutum√©e, les √©quipes backend de Bedrock √©taient pr√©sentes le 12 mai 2023 √† CPE (au Campus Lyontech) pour une nouvelle √©dition de l‚ÄôAFUP Day.\n\nPour suivre les conf√©rences qui gravitaient autour de PHP bien s√ªr, mais aussi pour soutenir notre coll√®gue Pauline Rambaud qui √©tait la star du jour (pour Bedrock du moins) ü§© !\n\nEt nous n‚Äôoublions pas non plus, les √¢mes braves de Bedrock qui ont rev√™ti le maillot bleu de l‚ÄôAFUP et qui ont donn√© de leur personne pour nous accueillir dans d‚Äôexcellentes conditions.\n\nEnfin bref (üêò), voici le petit r√©cap habituel des diff√©rentes conf√©rences de cette journ√©e üëá\n\nTirer parti du composant ExpressionLanguage de Symfony, laissez les utilisateurs finaux √™tre cr√©atifs !\n\n\n  Conf√©rence pr√©sent√©e par Florian MERLE et Mathias ARLAUD\n\n\nDans leur talk, Florian et Mathias nous parlent du composant Symfony Expression Language. \nCe composant fournit au d√©veloppeur un moteur d‚Äôexpressions et permet l‚Äôutilisation de ces derni√®res dans la configuration d‚Äôun projet ou bien encore comme fondation d‚Äôun moteur de r√®gles m√©tiers. \nSi ce composant vous est inconnu, sachez qu‚Äôil est lui-m√™me utilis√© par Symfony : par exemple pour la s√©curit√© sur les r√®gles de validation des routes.\n\nFlorian et Mathias nous familiarisent avec le fonctionnement du composant : on peut d√©finir des expressions simples qui ne requi√®rent pas d‚Äô√™tre compil√©es en PHP (ex¬†: ‚Äú1+1‚Äù) et √† l‚Äôinverse des expressions compil√©es en PHP qui vont correspondre √† des r√®gles personnalis√©es d√©finies au sein de notre projet (‚Äúest_eligible_a_une_promo(client)‚Äù). \nPour mieux comprendre comment tout cela fonctionne en interne, nous avons ensuite une pr√©sentation de comment les expressions sont interpr√©t√©es gr√¢ce √† de l‚Äôanalyse lexicale et le principe d‚Äôarbre syntaxique.\n\nPour finir ce talk, nous avons le droit √† une d√©monstration du composant comme moteur de r√®gles m√©tiers. \nOn nous pr√©sente un petit site e-commerce o√π les administrateurs peuvent, via un formulaire, utiliser les r√®gles m√©tiers d√©finies dans le projet et les ajuster pour correspondre au mieux √† leurs besoins.\n\nMentors : super-h√©ros ou super-vilains ?\n\n\n  Conf√©rence pr√©sent√©e par Pauline RAMBAUD\n\n\nOn a choisi de ne rien vous d√©voiler sur cette conf√©rence donn√©e par notre coll√®gue Pauline, il fallait √™tre pr√©sent pour savourer l‚Äôinstant üòâ‚Ä¶ \nCeci dit, Pauline sera s√ªrement ravie d‚Äôen parler avec vous sur Twitter.\n\nMonades : paradigme unique pour la programmation\n\n\n  Conf√©rence pr√©sent√©e par Baptiste LANGLADE\n\n\nDans cette conf√©rence, Baptiste nous a fait un rapide tour d‚Äôhorizon des solutions existantes pour faire de l‚Äôasynchrone en PHP. \nLe conf√©rencier nous a expliqu√© qu‚Äôil √©tait difficile aujourd‚Äôhui de faire du synchrone et de l‚Äôasynchrone de la m√™me mani√®re, facilement.\n\nC‚Äôest pour cela que le speaker a d√©velopp√© le composant Sequence. \nGr√¢ce √† ce projet et sa d√©mo, nous avons pu voir comment, sans changer le code (hormis une d√©pendance), nous pouvons faire des appels synchrones ou asynchrones.\n\nNous avons, au passage, eu la pr√©sentation du composant Filesystem qui apporte des outils int√©ressants pour la manipulation de fichiers.\n\nLe Z√©ro Downtime Deployment en pratique\n\n\n  Conf√©rence pr√©sent√©e par Sma√Øne MILIANNI\n\n\nSma√Øne nous a pr√©sent√© la logique √† avoir lorsque l‚Äôon veut faire du ZDD avec deux exemples et les limites de cette pratique.\n\nLes deux r√®gles d‚Äôor √† respecter concernant les changements sont :\n\n  √™tre r√©trocompatibles, c‚Äôest-√†-dire que les nouvelles modifications doivent fonctionner avec l‚Äôexistant. \nSi le d√©ploiement fail cela ne doit pas emp√™cher la version actuelle de fonctionner.\n  √™tre livr√© par release\n\n\nCela implique de repenser la fa√ßon de construire une nouveaut√© et pour illustrer cela, Sma√Øne a donn√© l‚Äôexemple de l‚Äôajout d‚Äôune colonne not null dans une base de donn√©es.\n\nObjectif : ajouter une colonne not null dans une table\n\nD√©coupage en deux releases :\n\n  Release 1\n    \n      ajout d‚Äôune colonne A null dans la table\n      mise √† jour de l‚Äôentit√© concern√©e dans le code initialis√© √† null\n    \n  \n  Release 2\n    \n      mettre √† jour les lignes sans valeur dans la colonne A avec une valeur par d√©faut\n      ajouter la contrainte NOT NULL sur la colonne A\n      mettre √† jour l‚Äôentit√© en sp√©cifiant l‚Äôattribut comme non nullable\n    \n  \n\n\nAfin de s‚Äôassurer que chaque release peut fonctionner avec l‚Äôexistant et anticiper les impacts que pourraient avoir les modifications, il ne faut pas h√©siter √† tester de fa√ßon automatique ou manuelle.\n\nSuite √† l‚Äôexemple, Sma√Øne a pr√©sent√© les limites du ZDD qui sont :\n\n  cette pratique a du sens si vous faites des releases fr√©quemment\n  un changement majeur n√©cessitera toujours une maintenance\n  tant que l‚Äôensemble des releases pr√©vues pour faire une modification ne sont pas en production, le syst√®me est consid√©r√© comme instable\n  cette pratique n√©cessite de penser et de concevoir diff√©remment les releases.\n\n\nPour conclure, le speaker a donn√© quelques cl√©s pour se lancer dans le ZDD :\n\n  former ses √©quipes\n  documenter les processus mis en place\n  it√©rer et s‚Äôam√©liorer avec chaque exp√©rience\n  tester et encore tester\n\n\nD√©mystifions les pratiques du Software craftsmanship !\n\n\n  Conf√©rence pr√©sent√©e par Thomas BOILEAU\n\n\nApr√®s une rapide pr√©sentation des diff√©rentes pratiques autour du craftsmanship (Test Driven Development, Domain Driven Development, Clean Architecture, Architecture Hexagonale ‚Ä¶), Thomas nous raconte comment il est tomb√© dans le pi√®ge du Gatekeeping.\n\nQu‚Äôest-ce que le Gatekeeping ? D‚Äôapr√®s lui (cf. photo), c‚Äôest l‚Äôart de marquer son appartenance √† un groupe en excluant les autres.\n\n\n\nTout commence avec une remarque lue sur Internet, du style ‚ÄúSi tu ne fais pas de TDD, alors tu n‚Äôes pas un vrai d√©veloppeur‚Äù, qui le complexera au point de le pousser √† √©tudier et appliquer cette pratique jusqu‚Äô√† son tour devenir l‚Äôauteur de ce genre de remarque.\n\nEn consid√©rant le craftsmanship comme la solution universelle et en l‚Äôappliquant de mani√®re dogmatique, il se retrouve √† proposer des solutions inadapt√©es √† ses projets et donc √† nuire √† ses clients.\n\nL‚Äôobjet de cette conf√©rence sera donc de nous parler de comment il a su se remettre en question et sortir de l‚Äôimpasse.\n\nComment faire pour ne plus √™tre un gatekeeper ? \nThomas nous parle alors de prendre du recul sur soi, d‚Äô√™tre pragmatique et bien s√ªr de savoir faire preuve d‚Äôhumilit√©. \nMais il existe aussi des pratiques reconnues dans notre milieu telles que l‚ÄôEgoless programming, le Pair Programming, ou tout simplement reconna√Ætre √† chacun le droit √† l‚Äôerreur et conna√Ætre ses propres limites.\n\nComment refondre un legacy sans cris et sans larmes - Retour d‚Äôexp√©rience et bonnes pratiques\n\n\n  Conf√©rence pr√©sent√©e par Kevin BALICOT\n\n\n√Ä travers son retour d‚Äôexp√©rience d‚Äôune refonte d‚Äôun tr√®s vieux projet PHP, Kevin BALICOT nous a offert sa recette d‚Äôune refonte progressive sans cris ni larmes :\n\n\n  Lister tous les probl√®mes de l‚Äôapplication\n  D√©finir une strat√©gie et des objectifs\n  Faire un inventaire de l‚Äôapplication\n  Mettre en place un Golden Master\n  Mettre des outils d‚Äôanalyse de code\n  Impl√©menter des Design Pattern et des architectures\n  Consolider les choix avec des ADR et du Pair Programming\n  Tester !\n\n\nSi vous souhaitez approfondir un de ces points, vous pouvez sans doute lui demander directement sur Twitter.\n\nLe travail invisible en entreprise : le cas du glue work\n\n\n  Conf√©rence pr√©sent√©e par Camille CASTILLO\n\n\nNous avons d√©couvert le concept du ‚Äúglue work‚Äù lors de la premi√®re conf√©rence de Camille.\n\nEnfin, nous avons un terme pour d√©crire cette id√©e que nous avions tous.tes en t√™te, mais qui manquait d‚Äôune d√©finition concr√®te.\n\nLe ‚Äúglue work‚Äù repr√©sente toutes ces t√¢ches accomplies par les employ√©.e.s, notamment les d√©veloppeur.euse.s, lors de leur travail quotidien, qui ne sont g√©n√©ralement pas facilement quantifiables et rarement valoris√©es par l‚Äôentreprise.\n\nCamille a identifi√© trois cat√©gories de ‚Äúglue work‚Äù : social, manag√©rial et technique.\n\nPar exemple, organiser une sortie au restaurant favorise les liens sociaux. Planifier une r√©union avec des clients renforce les relations professionnelles. Et effectuer une veille et proposer de nouveaux outils de d√©veloppement rel√®ve de la dimension technique.\n\nMalheureusement, en effectuant ces t√¢ches essentielles √† la vie de l‚Äôentreprise et m√™me √† sa productivit√©, les employ√©.e.s consacrent logiquement moins de temps √† leurs t√¢ches principales, comme le d√©veloppement.\n\nCela peut devenir probl√©matique si l‚Äôentreprise ne reconna√Æt pas la valeur de ces activit√©s.\n\nAlors quelles solutions pour prendre en consid√©ration le glue work ?\n\nTout d‚Äôabord, le rep√©rer et se porter volontaire.\n\nUn manager peut aussi veiller √† r√©partir ces t√¢ches.\n\nCamille conclut en indiquant que le glue work est n√©cessaire √† l‚Äôentreprise, qu‚Äôil faut l‚Äôidentifier et √™tre acteur.ice.s chacun √† son niveau pour le faire reconna√Ætre.\n\nTransformer efficacement du JSON en structure PHP fortement typ√©e\n\n\n  Conf√©rence pr√©sent√©e par Romain CANON\n\n\nUne chouette conf√©rence qui pr√©sentait la librairie d‚ÄôObject Mapping pour PHP Valinor, permettant de tirer parti au maximum des types PHP au runtime.\n\nD‚Äôailleurs suite √† √ßa, certaines de nos √©quipes ont commenc√© √† l‚Äôutiliser √† Bedrock‚Ä¶ Peut-√™tre un prochain article de REX √† pr√©voir üòâ\n\nLes instruments des devs augment√©¬∑e¬∑s\n\n\n  Conf√©rence pr√©sent√©e par Gabriel PILLET\n\n\nOn a fini cette journ√©e en beaut√©, par une vue d‚Äôensemble des diff√©rents outils permettant d√®s aujourd‚Äôhui d‚Äô√©pauler les d√©veloppeurs dans leur travail quotidien.\n\nDe PHPStan √† GPT-4 en passant par GitHub Copilot, cette conf√©rence, dont les slides √©taient habill√©es d‚Äôimages g√©n√©r√©es par une IA, nous a bien fait comprendre qu‚Äôon a tout int√©r√™t √† accueillir ces nouveaux outils, si on souhaite d√©cupler notre productivit√© ü§û.\n\n√Ä l‚Äôann√©e prochaine !\n\n\n"
} ,
  
  {
    "title"    : "Et si vos prochaines vacances se passaient √† v√©lo ? #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/vacances-a-velo",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  La randonn√©e √† v√©lo, c‚Äôest √©conomique, √©cologique, relaxant, d√©paysant et c‚Äôest plus facile d‚Äôacc√®s que ce que vous pourriez pensez. Alors on y va ? Un talk pour bien d√©marrer, faire d√©couvrir, chasser les id√©es re√ßues, partager des astuces et ...",
  "content"  : "\n  La randonn√©e √† v√©lo, c‚Äôest √©conomique, √©cologique, relaxant, d√©paysant et c‚Äôest plus facile d‚Äôacc√®s que ce que vous pourriez pensez. Alors on y va ? Un talk pour bien d√©marrer, faire d√©couvrir, chasser les id√©es re√ßues, partager des astuces et donner l‚Äôenvie de p√©daler !\n\n\nPar Thomas Jarrand\n"
} ,
  
  {
    "title"    : "Situations conflictuelles : et si vous sortiez de la spirale infernale ? #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/situations-conflictuelles",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  Pers√©cuteur, victime, sauveur‚Ä¶ lequel √™tes-vous ?\n\n  La situation suivante vous parle ?\n\n  ‚ÄúJ‚Äôen ai marre ! Quand je demande de la revue sur mon travail, les gens de mon √©quipe ne me donnent jamais de retours et je dois toujours les relancer au...",
  "content"  : "\n  Pers√©cuteur, victime, sauveur‚Ä¶ lequel √™tes-vous ?\n\n  La situation suivante vous parle ?\n\n  ‚ÄúJ‚Äôen ai marre ! Quand je demande de la revue sur mon travail, les gens de mon √©quipe ne me donnent jamais de retours et je dois toujours les relancer au moins 10 fois avant d‚Äôobtenir une r√©ponse‚Ä¶ Heureusement que Michel, mon ancien coll√®gue super sympa qui est maintenant dans l‚Äô√©quipe Warrior continue √† me r√©pondre, lui !‚Äù\n\n  Oui ? C‚Äôest normal. Inconsciemment, nous jouons les r√¥les de pers√©cuteur, victime ou sauveur. Et nos interactions en souffrent : rien de constructif ne peut √©merger de cette spirale infernale.\n\n  Apprenons √† identifier ces r√¥les pour comprendre leurs effets sur nos relations avec nos coll√®gues, puis nous explorerons comment en sortir pour des interactions plus constructives et positives. Mettons fin √† cette spirale infernale !\n\n\nPar Elodie Perrin\n"
} ,
  
  {
    "title"    : "Du CSS aux shaders WebGL : panorama des techniques d&#39;animation en 2023 #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/panorama-css-animations",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  Parfois subtiles, parfois pro√©minentes, les animations sont un √©l√©ment essentiel pour une exp√©rience utilisateur agr√©able. Mais une fois qu‚Äôon a imagin√© des animations plus ou moins folles, vient la question fatidique du ‚Äúcomment fait-on √ßa ?‚Äù,...",
  "content"  : "\n  Parfois subtiles, parfois pro√©minentes, les animations sont un √©l√©ment essentiel pour une exp√©rience utilisateur agr√©able. Mais une fois qu‚Äôon a imagin√© des animations plus ou moins folles, vient la question fatidique du ‚Äúcomment fait-on √ßa ?‚Äù, √† laquelle j‚Äôaimerais vous aider √† r√©pondre.\n\n  Pour pouvoir choisir la technique la mieux adapt√©e √† chaque animation, il vaut mieux avoir une bo√Æte √† outils la plus compl√®te possible. J‚Äôaimerais vous aider √† construire la v√¥tre en vous pr√©sentant un panel le plus large possible de techniques d‚Äôanimation sur le web : des APIs natives classiques (CSS, Web Animation API) aux plus complexes (Canvas API) en passant par les librairies sp√©cialis√©es (FLIP, Lottie, Framer Motion, Rive‚Ä¶). Nous finirons avec WebGL et ses shaders GLSL, qui feraient trembler plus d‚Äôun d√©veloppeur mais dont on retrouve les effets impressionnants sur tous les sites r√©compens√©s aux Awwwards.\n\n  Je passerai rapidement sur toutes ces techniques, en comparant leurs performances et utilisations possibles, afin de vous laisser la libert√© d‚Äôexplorer plus en profondeur celles qui vous int√©ressent.\n\n\nPr√©sent√© par Julien Sulpis.\n"
} ,
  
  {
    "title"    : "OZINT - Vos traces vous trahissent ! #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/ozint-lft",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  L‚ÄôOpen Source Intelligence (OSINT), ou le renseignement en sources ouvertes (ROSO) en fran√ßais, est un ensemble de m√©thodologies et de pratiques destin√©s √† la collecte et l‚Äôanalyse d‚Äôinformations publiques et l√©gales en ayant pour objectif de r...",
  "content"  : "\n  L‚ÄôOpen Source Intelligence (OSINT), ou le renseignement en sources ouvertes (ROSO) en fran√ßais, est un ensemble de m√©thodologies et de pratiques destin√©s √† la collecte et l‚Äôanalyse d‚Äôinformations publiques et l√©gales en ayant pour objectif de r√©pondre √† des questions ou de faire des choix. L‚ÄôOSINT est utilis√© dans le monde du public comme dans le monde du priv√©, et dans plusieurs domaines, incluant : l‚Äôintelligence √©conomique, le journalisme, les services de renseignement, la recherche scientifique, la cybers√©curit√©, ou encore la lutte contre la criminalit√©.\n\n  L‚Äôobjectif de cette conf√©rence serait de mettre en lumi√®re cette discipline mal connue, mais √©galement d‚Äôutiliser les m√©thodes OSINT pour effectuer une d√©monstration de sensibilisation sur le sujet des donn√©es personnelles.\n\n\nPar Alexis Martins\n"
} ,
  
  {
    "title"    : "Cr√©er son association #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/creer-son-association",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  √áa fait maintenant plus de deux ans que j‚Äôai cr√©√© mon association sportive. J‚Äôaimerais √† travers ce talk, faire un retour d‚Äôexp√©rience sur mon aventure associative.\n\n  Le format serait ax√© autour d‚Äôun fil rouge concernant ma propre exp√©rience a...",
  "content"  : "\n  √áa fait maintenant plus de deux ans que j‚Äôai cr√©√© mon association sportive. J‚Äôaimerais √† travers ce talk, faire un retour d‚Äôexp√©rience sur mon aventure associative.\n\n  Le format serait ax√© autour d‚Äôun fil rouge concernant ma propre exp√©rience avec mon association ‚ÄúFit for All‚Äù Tout en g√©n√©ralisant des tips et des ‚Äúchoses √† savoir/connaitre‚Äù pour monter un association.\n\n  Je fais ce talk sur toutes les choses que j‚Äôaurais aim√© savoir avant de me lancer ‚Äúdans le grand bassin‚Äù\n\n\nPr√©sent√© par Guillaume Tr√©m√©.\n"
} ,
  
  {
    "title"    : "C√©l√©brons nos r√©ussites gr√¢ce au Brag Document ! #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/brag-document",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  ¬´ Quelles sont tes r√©ussites du dernier sprint ? Qu‚Äôas-tu aim√© faire? ¬ª Si, comme pour moi, vos r√©ussites s‚Äô√©vaporent de votre m√©moire comme un mojito au soleil, alors le Brag Document est fait pour vous.\n\n  Gr√¢ce au Brag Document, listez vos r...",
  "content"  : "\n  ¬´ Quelles sont tes r√©ussites du dernier sprint ? Qu‚Äôas-tu aim√© faire? ¬ª Si, comme pour moi, vos r√©ussites s‚Äô√©vaporent de votre m√©moire comme un mojito au soleil, alors le Brag Document est fait pour vous.\n\n  Gr√¢ce au Brag Document, listez vos r√©ussites, ce que vous avez appris, appr√©ci√© faire, bref, tout ce qui vous a sembl√© important. Il peut √™tre partag√© avec votre manager pour faire le point sur votre progression, ou tout simplement pour r√©pondre √† ces questions : ¬´ De quoi √™tes-vous fier(e) ? Qu‚Äôest ce que vous aimeriez faire plus ? ¬ª\n\n  Regardons un exemple de template pour initier son propre Brag Document. Puis je vous proposerai un exemple de mise en place du Brag Document au sein d‚Äôune √©quipe (Picsou), aupr√®s de chaque coll√®gue mais aussi pour la team en elle-m√™me. N‚Äôoublions plus nos r√©ussites !\n\n\nPar Anne-Laure de Boissieu.\n"
} ,
  
  {
    "title"    : "Bedrock √† MiXiT Lyon (2023)",
    "category" : "",
    "tags"     : " conference, lyon, tech, agilit√©",
    "url"      : "/2023/04/25/mixit-lyon-2023.html",
    "date"     : "April 25, 2023",
    "excerpt"  : "Nous √©tions pr√©sent les 13 et 14 avril 2023 √† CPE pour l‚Äô√©dition de MiXiT 2023, pour suivre les \nconf√©rences bien s√ªr et pour soutenir les coll√®gues qui donnaient une conf√©rence !\n\nEt si vos prochaines vacances se passaient √† v√©lo ?\n\nConf√©rence pr...",
  "content"  : "Nous √©tions pr√©sent les 13 et 14 avril 2023 √† CPE pour l‚Äô√©dition de MiXiT 2023, pour suivre les \nconf√©rences bien s√ªr et pour soutenir les coll√®gues qui donnaient une conf√©rence !\n\nEt si vos prochaines vacances se passaient √† v√©lo ?\n\nConf√©rence pr√©sent√©e par Thomas Jarrand\n\n\nJay-Z, Maths and Signals ! How to clone Shazam üéß\n\nConf√©rence pr√©sent√©e par Moustapha Agack\n\n\nBienvenue dans le monde merveilleux des syst√®mes distribu√©s !\n\nConf√©rence pr√©sent√©e par Pascal Martin\n\n\nRemotion : le 7√®me art √† port√©e de composants web et d‚ÄôAPI üé¨\n\nConf√©rence pr√©sent√©e par Antoine Caron et Mickael Alves\n\n\n\nConversations avec ChatGPT: illusion ou r√©alit√©?\n\nConf√©rence pr√©sent√©e par Marie-Alice Blete\n\nMarie-Alice a pris le temps de r√©expliquer ce qu‚Äôest ChatGPT et pourquoi ce g√©n√©rateur de texte \nne garantira jamais la v√©racit√© des informations fournies. En effet, si les r√©ponses de ChatGPT \npeuvent √™tre tr√®s cr√©dibles, le contenu propos√© n‚Äôest en aucun cas v√©rifi√© \npuisqu‚Äôil s‚Äôagit d‚Äôune suite de mots les plus probables. \nEn guise d‚Äôillustration, Marie-Alice a demand√© √† ChatGPT de se rendre sur son terminal Linux, et \nde taper des commandes afin de cloner le projet chatGPT 4. Celui-ci s‚Äôex√©cute. Mais la \nretranscription n‚Äôest pas la r√©alit√©, c‚Äôest ce qu‚Äôil y aurait probablement eu dans le terminal. Un \nexemple bluffant !\nPour Marie-Alice, tout le monde peut √™tre tromp√©. De nouveau √† titre d‚Äôexemple, elle cite le cas \nd‚Äôun expert en IT qui a demand√© √† ChatGPT de r√©sumer un article d‚Äôune revue scientifique. \nChatGPT a propos√© un r√©sum√© imagin√© √† partir du titre - il n‚Äôa pas acc√®s √† l‚Äôarticle. Ce r√©sum√© \n√©tait si convaincant que l‚Äôexpert a cru que ChatGPT avait acc√®s √† l‚Äôarticle.\n\nCette conf√©rence a remis en place les attentes qu‚Äôon pouvait avoir √† propos de ChatGPT, qui n‚Äôest qu‚Äôun outil avec ses limites.\n\nVoir les slides\n\nFresque anti-sexisme\n\nAtelier pr√©sent√© par Sara Dufour\n\nCet atelier, √† l‚Äôimage de la fresque du climat, nous a permis de construire, en plusieurs √©tapes,\net par groupe de 5-6 personnes, un tableau d√©peignant la vie actuelle.\nLes d√©bats √©taient constructifs et les faits, appuy√©s par les statistiques de l‚ÄôINSEE, √©taient effrayants.\nL‚Äôatelier √©tait tr√®s int√©ressant, mais un peu d√©moralisant de voir qu‚Äôen 2023,\nil y a encore beaucoup √† faire pour diminuer le sexisme aussi bien dans le milieu professionnel que priv√©.\n\nLego Flow Game : le Waterfall, le Scrum et le Kanban tu diff√©rencieras !\n\nAtelier pr√©sent√© par Fanny Klauk\n\nCet atelier nous a permis de (re)voir les diff√©rentes organisations suivantes : le Waterfall, le Scrum, le Kanban.\nNous avons form√© des √©quipes de 5 personnes et utilis√© des Lego :heart_eyes: pour illustrer les taches de constructions.\nChaque personne jouait un r√¥le particulier qui repr√©sente les diff√©rents r√¥les d‚Äôune √©quipe de \nd√©veloppement standard.\n\nL‚Äôobjectif √©tait de construire le plus de mod√®les d‚Äôun calendrier de l‚Äôavent Lego, donc des \nmod√®les petits et (en g√©n√©ral) assez simples, nous avions 6 minutes par type d‚Äôorganisation. Les \nr√¥les √©taient les suivants :\n\n  Analyste : Prend une porte (du calendrier de l‚Äôavent), note le num√©ro de la porte (jour o√π le calendrier sera ouvert) sur une carte et accroche avec un trombone la carte sur le trombone. Il g√®re la priorit√© en fonction du num√©ro de la carte.\n  Fournisseur : Prend la carte et va chercher le sachet correspondant. Il n‚Äôy a aucune indication sur le sachet qui permet de savoir lequel prendre\n  R√©alisateur : Construit le mod√®le\n  Testeur : V√©rifie que tout est ok\n  Pilote : Remplit un tableau de suivi des diff√©rents postes\n\n\nPremi√®re √©tape, le Waterfall : chaque r√¥le devait finir son travail pour 5 cartes avant de passer au suivant.\nBilan, au bout des 6 minutes, l‚Äôanalyste a fini son travail, le fournisseur est toujours en train de chercher des sachets, et les autres attendent et sont frustr√©s, car ils ne peuvent rien faire d‚Äôautre.\n\nDeuxi√®me √©tape, le Scrum : chaque r√¥le peut faire passer une carte au prochain d√®s que son \ntravail sur celle-ci est finie.\n3 it√©rations de 2 minutes seront faites pour illustrer les sprints et une estimation est faite avant chaque it√©ration.\nBilan : entre chaque it√©ration, nous avions une petite r√©trospective, celle-ci permettait de nous am√©liorer au tour d‚Äôapr√®s.\nAu final, nous avons pu terminer 4 cartes sur 5.\n\nDerni√®re √©tape, le Kanban : comme pour le Scrum, une carte peut passer √† l‚Äô√©tape d‚Äôapr√®s, d√®s \nqu‚Äôelle est termin√©e.\nPas d‚Äôestimation par contre, pas le droit d‚Äôavoir plus de 2 cartes √† la m√™me √©tape (pas de stock).\nBilan : l√† aussi, nous avons fait des petites r√©trospectives pour nous am√©liorer entre chaque it√©ration.\nAu final, nous avons pu terminer 6 cartes (un bug est survenu en cours de route). Le Kanban a \npermis une avanc√©e plus importante, mais le suivi par le pilote √©tait plus compliqu√©, car les cartes bougeaient tr√®s vite.\n\nPersonnellement, j‚Äôai appr√©ci√© travailler en Kanban et suis content de travailler avec la m√©thode ScrumBan chez Bedrock, cette derni√®re m√©langeant le Scrum et le Kanban.\n\nA l‚Äôann√©e prochaine !\n\n\n\n"
} ,
  
  {
    "title"    : "Bedrock √† l&#39;AWS Summit Paris 2023",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, aws, summit, paris, 2023",
    "url"      : "/2023/04/20/aws-summit-paris-2023.html",
    "date"     : "April 20, 2023",
    "excerpt"  : "L‚ÄôAWS Summit Paris 2023 s‚Äôest d√©roul√© le 4 avril. C‚Äô√©tait pour nous l‚Äôoccasion de d√©couvrir les derni√®res innovations au c≈ìur des services AWS, comme la solution d‚ÄôIA d‚Äôaide au d√©veloppement nomm√©e CodeWhisperer. De plus, Pascal Martin, Principal ...",
  "content"  : "L‚ÄôAWS Summit Paris 2023 s‚Äôest d√©roul√© le 4 avril. C‚Äô√©tait pour nous l‚Äôoccasion de d√©couvrir les derni√®res innovations au c≈ìur des services AWS, comme la solution d‚ÄôIA d‚Äôaide au d√©veloppement nomm√©e CodeWhisperer. De plus, Pascal Martin, Principal Engineer, y assistait aussi en tant que speaker pour partager notre exp√©rience en conception et maintenance de Syst√®mes Distribu√©s.\nEn plus des deux points pr√©c√©demment cit√©s, nous verrons aussi comment eTF1 s‚Äôest pr√©par√© pour la Coupe du Monde de la FIFA 2022, ou de souverainet√© et de son application chez AWS.\n\n√Ä vos c√¥t√©s pour les grands moments : AWS, TF1 et la Coupe du Monde de la FIFA 2022\n\nConf√©rence pr√©sent√©e par :\n\n  Imane Zeroual - Senior Technical Account Manager, AWS\n  Djamel Arichi, Head of Managed Services and Support, eTF1\n  Ali Oubabiz, Head of Digital Infrastructure, eTF1\n  Remy Pinsonneau, Architecte, eTF1\n\n\n\n\neTF1 partage son retour d‚Äôexp√©rience sur la Coupe du Monde de foot 2022 et les d√©fis surmont√©s pour que leur plateforme de replay myTF1 propose une parfaite exp√©rience utilisateur tout au long de l‚Äô√©v√©nement.\n\nLa pr√©sentation, coanim√©e par Imane, Senior Technical Manager de chez AWS, permet aussi d‚Äôen apprendre un peu plus sur le programme IEM d‚Äôaccompagnement de clients AWS lors d‚Äô√©v√©nements critiques. Nous avons d‚Äôailleurs d√©j√† exploit√© ce programme chez Bedrock Streaming.\nChallenge technique pour les √©quipes eTF1, la Coupe du Monde de football 2022 a battu plusieurs records de la plateforme, dont des pics √† plus de 2,4 Millions d‚Äôutilisateurs simultan√©s. L‚Äô√©v√©nement a √©t√© pr√©par√© en collaboration avec les √©quipes d‚ÄôAWS pour adapter les infrastructures √† recevoir de fortes charges.\n\nTrois points critiques identifi√©s :\n\n  Authent/backend, les millions d‚Äôutilisateurs vont s‚Äôauthentifier dans une fen√™tre de 15 minutes.\n  Delivery vid√©o, tout au long de l‚Äô√©v√©nement une forte charge, constante, est attendue.\n  Publicit√©, pic de charge tr√®s important mais ponctuel.\n\n\nDes sc√©narios de tests de performances ont √©t√© effectu√©s √† l‚Äôaide de K6, pour chacun des points. La pr√©production a servi d‚Äôenvironnement de test, avant d‚Äôeffectuer une validation finale sur la production. Du travail a √©t√© √©galement r√©alis√© sur les services AWS : par exemple, les tables DynamoDB ont √©t√© bascul√©es en OnDemand afin de profiter de l‚Äô√©lasticit√© plus rapide du service, malgr√© les co√ªts suppl√©mentaires, compar√© au mode provisionn√©.\nAu niveau des clusters Kubernetes, les applications ont √©t√© redimensionn√©es √† la hausse (m√©moire, cpu, HPA) pour anticiper les pics de charge et ne pas seulement se reposer sur du scaling r√©actif.\n\nLors de la comp√©tition, une War Room √©tait ouverte suivant l‚Äôimportance des matchs. Elle √©tait compos√©e d‚Äôintervenants AWS gr√¢ce au programme IEM, de personnels techniques eTF1 et de membres du service management pour pouvoir r√©agir en cas d‚Äôimpr√©vus. \nLa War Room a d‚Äôailleurs √©t√© mise √† contribution puisque la plateforme √† subi des attaques DDOS pendant certains matchs. Le CDN Cloudfront et WAF ont permis de les contenir.\n\nChez Bedrock Streaming, nous √©tions curieux de ce retour d‚Äôexp√©rience : nous avons pr√©par√© ce m√™me type d‚Äô√©v√©nement lors de l‚ÄôEuro de football 2020. Les d√©fis √† surmonter sont les m√™mes que ceux que nous avions rencontr√©s et nous sommes arriv√©s √† des conclusions similaires dans nos choix techniques. Nous avions d‚Äôailleurs d√©velopp√© un outil pour r√©pondre au probl√®me de scalabilit√© dans kubernetes durant l‚ÄôEuro 2020 et que nous utilisons toujours aujourd‚Äôhui, un article de blog √† ce sujet est disponible ici.\n\nComment bien d√©buter avec Amazon CodeWhisperer\n\nConf√©rence pr√©sent√©e par :\n\n  S√©bastien Butreau, Senior Partner Solutions Architect, AWS\n  S√©bastien Grazzini, Principal Solutions Architect, AWS\n\n\n\n\nAmazon annonce une sortie grand public, prochaine, de son assistant de d√©veloppement par IA CodeWhisperer (update: depuis, CodeWhisperer est pass√© GA).\nNous avons eu droit √† une d√©monstration de l‚Äôoutil. En quelques minutes et seulement √† l‚Äôaide de quelques commentaires, les deux pr√©sentateurs ont produit un script python capable de prendre en entr√©e un r√©pertoire de photos et donner en sortie un JSON qui, pour chaque photo, donnait le nom de la c√©l√©brit√© pr√©sente dessus.\n\nChez Bedrock Streaming, nous pensons qu‚Äôil est tr√®s important de suivre ce nouveau tournant que prend l‚Äôaide au d√©veloppement via l‚ÄôIA depuis quelques mois. Nous pr√©voyons de tester lors de nos journ√©es R&amp;amp;D Github Copilot et Amazon CodeWhisperer.\n\nL‚Äôoutil d‚ÄôAmazon a quelques atouts, notamment la fonctionnalit√© de suivi des r√©f√©rences qui permet de savoir si du code propos√© est similaire √† du code utilis√© pour l‚Äôapprentissage et peut-√™tre prot√©g√© par une licence incompatible avec notre usage. De plus, l‚Äôint√©gration du SDK Amazon est assez pouss√©e et cela prend tout son sens, notamment lors du d√©veloppement pour des lambdas AWS o√π l‚Äôoutil semble tr√®s performant.\n\nBienvenue dans le Monde Merveilleux des Syst√®mes Distribu√©s\n\nCette ann√©e encore, nous avons eu la chance de pouvoir partager notre exp√©rience, lors d‚Äôune conf√©rence donn√©e par Pascal, Principal Engineer et AWS Hero : ¬´ Bienvenue dans le Monde Merveilleux des Syst√®mes Distribu√©s ¬ª\n\n\n\nPourquoi s‚Äôemb√™ter avec des Syst√®mes distribu√©s ? Comment en tirer profit ? Quels dangers ? Scalabilit√©, coordination et r√©silience : trois grands axes pour ce talk, bas√© sur l‚Äôexp√©rience acquise par les √©quipes Bedrock, tant infra que devs, depuis plusieurs ann√©es.\n\nEn tant que speaker, pouvoir partager avec notre communaut√© est toujours aussi agr√©able ! Et, dans le public, il √©tait assez int√©ressant d‚Äôentendre les r√©actions de nos voisins lorsque Pascal racontait certaines anecdotes ou pr√©sentait certains concepts. Les probl√©matiques que nous rencontrons dans nos m√©tiers, nous sommes nombreux √† les rencontrer, et c‚Äôest tout l‚Äôint√©r√™t des √©v√©nements comme AWS Summit : apprendre les uns des autres !\n\nCette pr√©sentation n‚Äôa malheureusement pas √©t√© enregistr√©e lors du Summit, mais Pascal l‚Äôa redonn√©e depuis √† MixIT, o√π elle a √©t√© enregistr√©e ‚Äì et les vid√©os devraient √™tre bient√¥t publi√©es ;-)\n\nLa souverainet√© des donn√©es chez AWS\n\nUne des conf√©rences portait sur les th√®mes de la Souverainet√© dans le Cloud AWS et du R√®glement europ√©en G√©n√©ral sur la Protection des Donn√©es (RGPD). Lors de cette pr√©sentation, Stephan Hadinger (Directeur de la Technologie chez AWS) a expos√© le cadre de ce r√®glement et sa mise en application au sein de l‚Äôinfrastructure AWS. C‚Äôest cette partie qui √©tait, d‚Äôapr√®s nous, la plus int√©ressante, √©tant donn√©e sa dimension technique.\n\nRGPD est un regroupement de r√®gles qui r√©gissent et prot√®gent les droits des r√©sidents d‚ÄôUnion Europ√©enne. Il porte sur le respect de la confidentialit√© et la protection des donn√©es personnelles. Toute entreprise exer√ßant dans l‚ÄôUE y est soumise. Dans le cas pr√©sent, la RGPD couvre √† la fois les clients AWS (comme Bedrock) et les utilisateurs finaux (comme les utilisateurs des services Bedrock).\n\nChez AWS, la Souverainet√© est synonyme d‚Äôautonomie strat√©gique et s‚Äôexprime de la fa√ßon suivante :\n\n  la possession des donn√©es clients : tous les clients AWS ont le contr√¥le de leurs donn√©es et applications, et nous verrons comment ;\n  le choix de la localisation des donn√©es, via la possibilit√© d‚Äôh√©berger l‚Äôint√©gralit√© des donn√©es sur le territoire de son choix, en France par exemple ;\n  l‚Äôacc√®s au meilleur de la technologie, qui favorise l‚Äôinnovation ;\n  et la possibilit√© de changer de solution (pas de lock-in).\n\n\nLes clients sont les seuls possesseurs de leurs donn√©es, ils en ont le contr√¥le total : AWS n‚Äôa aucun droit d‚Äôusage des donn√©es de leurs clients. De plus, AWS n‚Äôa pas acc√®s aux donn√©es et ne d√©place pas (g√©ographiquement) les donn√©es de ses clients.\n\nL‚Äôimpl√©mentation technique de ces concepts repose, entre autres, sur le chiffrement syst√©matique des donn√©es. AWS Nitro est une des briques d‚Äôarchitecture qui en est responsable pour les EC2 (depuis 2013 pour la partie r√©seau). Nitro permet le chiffrement de toute la cha√Æne de donn√©es (r√©seau, volumes de stockage) et comprend plusieurs composants :\n\n  Carte Nitro d√©di√©e au √©changes externes (r√©seau + acc√®s aux EBS, stockage persistant)\n  Carte Nitro pour le stockage local (stockage temporaire attach√© √† l‚Äôh√¥te)\n  Hyperviseur Nitro (il s‚Äôagit d‚Äôun hyperviseur bas√© sur linux KVM, mais grandement modifi√© pour les besoins, pas de sshd, pas de systemd, pas de couche r√©seau)\n  Puce de s√©curit√© Nitro (qui emp√™che le client d‚Äôavoir acc√®s aux composants de l‚Äôh√¥te, proc√®de √† la mise √† jour des firmwares des composants du serveur et g√®re le s√©cure boot afin de contr√¥ler l‚Äô√©tat des firmwares des composants avant de d√©marrer l‚Äôh√¥te).\n\n\n\n\nAu del√† du chiffrement dont il est principalement question ici, Nitro permet aussi de grandement augmenter les performances des EC2 en limitant l‚Äôimpact de l‚Äôhyperviseur sur le CPU utilis√© par les clients. Dans le cas d‚Äôune virtualisation classique, toutes les t√¢ches list√©es ci-dessus sont effectu√©es par le processeur lui-m√™me, grignotant ainsi de la puissance des machines. Ici, Nitro permet de d√©charger le CPU de ces t√¢ches en le rendant ainsi d√©di√© aux EC2.\n\nAWS utilise aussi des solutions telles que Key Management Service (KMS) pour chiffrer les donn√©es de plus d‚Äôune centaine de ses services. Il s‚Äôagit l√† aussi d‚Äôun syst√®me de protection des donn√©es des utilisateurs : seul l‚Äôop√©rateur poss√©dant la cl√© de chiffrement est capable de lire les donn√©es de ces services. \nUne version √©tendue de KMS est m√™me disponible pour les clients les plus soucieux de la protection de leurs donn√©es : External Key Stores. XKS est un dispositif physique pouvant √™tre h√©berg√© en dehors des locaux d‚ÄôAWS. Il est m√™me capable de se ‚Äúd√©fendre‚Äù contre les attaques physiques en proc√©dant √† l‚Äôeffacement des cl√©s lors d‚Äôune tentative d‚Äôintrusion physique. Il s‚Äôagit probablement de l‚Äôultime impl√©mentation de s√©curit√© et de souverainet√© chez AWS.\n\n\n\nTout au long de cette conf√©rence, on a bien senti que le but d‚ÄôAWS, afin de respecter les donn√©es de ses usagers, √©tait de faire en sorte de ne pas pouvoir acc√©der aux donn√©es de ses clients.\n\nLe mot de la fin\n\nL‚ÄôAWS Summit comportait plus d‚Äôune centaine de sessions et nous avons juste eu l‚Äôoccasion d‚Äôeffleurer le contenu propos√© lors de cette journ√©e.\n\nNous avons commenc√© √† migrer vers Le Cloud en 2018 et notre premier AWS Summit Paris √©tait en 2019 ‚Äì nous y avions d‚Äôailleurs d√©j√† parl√© de cette migration au cours d‚Äôun autre √©v√©nement.\n\nDepuis, que de chemin parcouru ! Cette ann√©e, nous pensions moins √† Kubernetes, √† DynamoDB ou aux optimisations de co√ªts, sur lesquels nous avons bien boss√© depuis 2019. Notre attention √©tait plus attir√©e vers des sujets que nous avons commenc√© √† travailler plus r√©cemment et o√π nous avons encore des challenges majeurs, comme les approches pleinement serverless ;-)\n"
} ,
  
  {
    "title"    : "Bedrock au Kubernetes Community Days France 2023",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, rex",
    "url"      : "/2023/04/03/kubernetes-community-days.html",
    "date"     : "April 3, 2023",
    "excerpt"  : "La premi√®re √©dition de KCD (Kubernetes Community Days) en France s‚Äôest d√©roul√©e le 7 mars au Centre Pompidou et rassemblant pr√®s de 1000 participants pour une belle journ√©e de conf√©rences.\n\n\n\nKCD a rassembl√© les communaut√©s tech fran√ßaises pour ce...",
  "content"  : "La premi√®re √©dition de KCD (Kubernetes Community Days) en France s‚Äôest d√©roul√©e le 7 mars au Centre Pompidou et rassemblant pr√®s de 1000 participants pour une belle journ√©e de conf√©rences.\n\n\n\nKCD a rassembl√© les communaut√©s tech fran√ßaises pour cette journ√©e de partage d‚Äôexpertise et de retours d‚Äôexp√©rience autour de Kubernetes et des technologies Cloud Native et DevOps.\n\nSolomon Hykes, son acolyte J√©rome Petazzoni et l‚Äô√âducation Nationale ont pr√©sent√© la keynote d‚Äôouverture.\nCette premi√®re keynote a permis d‚Äôintroduire le projet Santorin du Minist√®re de l‚Äô√âducation. C‚Äôest un syst√®me d‚Äôaide √† la correction et √† la notation pour lequel ils utilisent 3 clusters afin d‚Äôanalyser 5 millions de copies.\n\n\n  \n\nSolomon Hykes &amp;amp; J√©r√¥me Petazzoni\n\n\nDes grands acteurs de la tech en France tels que Scaleway, OVHCloud, Shadow, eTF1, Back Market, vpTech, Doctolib, Deezer, Carrefour et l‚Äô√âducation Nationale √©taient pr√©sents pour rapporter leurs exp√©riences. \nLes trois salles nomm√©es aux couleurs du drapeau fran√ßais √©taient disponibles tout au long de la journ√©e pour accueillir la quarantaine de conf√©rences organis√©es par KCD.\n\nLa plus-value d‚Äôun portail d√©veloppeur chez Back Market \n\nConf√©rence pr√©sent√©e par :\n\n\n  Sami Farhat - Backend Engineer\n\n\nBack Market, entreprise fran√ßaise de commerce √©lectronique, est venue nous parler de leur impl√©mentation \nd‚Äôun ‚ÄúDevPortal‚Äù bas√© sur le projet Backstage.io.\n\n\n\nLa cr√©ation de ce portail d√©veloppeur √† √©t√© initi√© en 2021 suite au projet de mise √† l‚Äô√©chelle et de passage en microservices de leur applications.\n\nInitialement, chaque nouveau service √©tait cr√©√© manuellement et n√©cessitait du travail dans les √©quipes d‚Äôinfrastructure.\nEn plus de demander du travail lors de leur cr√©ation, les services n‚Äô√©taient donc pas syst√©matiquement cr√©√©s avec les m√™mes bases de codes et pouvaient diff√©rer dans leurs impl√©mentation.\n\nLe but √©tait donc d‚Äôobtenir une vue centralis√©e sur les projets et de permettre aux d√©veloppeurs de cr√©er de nouveaux services eux-m√™mes.\n\n\n\nLa cr√©ation de ce portail √† √©galement permis √† Back Market d‚Äôinitier l‚Äôutilisation d‚Äôun mod√®le pour la cr√©ation de services, ainsi d‚Äôuniformiser les architectures et de faciliter le passage en microservices.\n\nIls ont √©galement impl√©ment√© une vue relationnelle concernant les projets et les √©quipes qui y sont associ√©es.\n\n\n\nEnfin, pour trouver les projets prioritaires pour la migration en microservices, ils ont cr√©√© une vue nomm√©e Coupling scores :\n\n\n\nC‚Äôest une vue qui permet d‚Äôobtenir la liste des applications monolithiques avec le taux de couplage le plus √©lev√©.\n\nLe replay de cette conf√©rence est disponible ici.\n\nVPC dans k8s : Pas aussi simple que √ßa en a l‚Äôair \n\nConf√©rence pr√©sent√©e par :\n\n\n  Louis Portay - Ing√©nieur DevOps Kapsule Scaleway\n\n\nAu tour de Scaleway qui nous ont pr√©sent√© comment ils ont impl√©ment√© les VPC priv√©s dans le service Kaspule, leur Kubernetes manag√©.\nC‚Äôest un besoin qui s‚Äôest pr√©sent√© afin d‚Äô√©viter que les √©changes inter-noeuds transitent via le r√©seau public.\n\n\n\nPour utiliser le r√©seau priv√© dans Kaspule, ils ont ajout√© une interface nomm√©e ‚Äúkapsule0‚Äù sur les instances utilis√©es dans la cr√©ation du cluster. Cette interface est ensuite attach√©e √† Cilium dans le cluster.\n\n\n\nCette fonctionnalit√© est actuellement en b√™ta, elle sera bient√¥t disponible de mani√®re r√©gionale.\n\nParmi les impl√©mentations futures, Scaleway pr√©voit de proposer la possibilit√© de retirer l‚Äôinterface r√©seau publique afin que tous les √©changes entre Kubelet et le Control Plane passent √©galement via le r√©seau priv√©.\n\nLe replay de cette conf√©rence est disponible ici.\n\nKubernetes the not so hard Veepee way \n\nConf√©rence pr√©sent√©e par :\n\n\n  Lo√Øc Blot - Lead SRE Veepee\n  Micka√´l Todorovic - Tribe Lead SRE Veepee\n\n\nLo√Øc et Micka√´l de Veepee sont venus pr√©senter l‚Äô√©volution des infrastructures utilis√©es par l‚Äôentreprise.\nInitialement, avant 2019, ils comptaient plus de 10 000 machines virtuelles dans leur parc.\nCes machines h√©bergaient les applications de Veepee via diverses technologies :\n\n  Swarm\n  Rancher\n  Hashicorp Nomad\n  Docker compose\n  LXC\n\n\nEn 2019, pour anticiper la gestion de la vente des tickets pour le concert de C√©line Dion, ils ont choisi de migrer leurs services sur des clusters Kubernetes.\nLa premi√®re infrastructure √©tait manag√©e via Ansible, ils utilisaient Traefik et un cert manager home made.\n\n\n\nAujourd‚Äôhui, ils fournissent un produit Container as a Service nomm√© Starfish. C‚Äôest un outil qu‚Äôils ont √©crit en Go et qui permet de g√©rer les applications des √©quipes Veepee. Ils utilisent √©galement Gitlab et ArgoCD.\n\nLe replay de cette conf√©rence est disponible ici.\n\nConclusion\n\nLa majorit√© de conf√©rences auxquelles j‚Äôai assist√© √©taient des retours d‚Äôexp√©rience. C‚Äô√©tait particuli√®rement int√©ressant car en plus de la pr√©sentation d‚Äôune technologie, nous avons un retour d√©taill√© sur l‚Äôusage de cette derni√®re.\n\nMerci √† tout les speakers pour leur partage de connaissances et aux organisateurs de KCD France.\n"
} ,
  
  {
    "title"    : "Twitch: du streaming mais pas en lit de pierre #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/twitch-streaming",
    "date"     : "March 31, 2023",
    "excerpt"  : "Twitch: du streaming mais pas en lit de pierre.\nPr√©sent√© par Quentin GILLIE.\n",
  "content"  : "Twitch: du streaming mais pas en lit de pierre.\nPr√©sent√© par Quentin GILLIE.\n"
} ,
  
  {
    "title"    : "REX-Shape Up, un LFT dont vous √™tes les h√©ros #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/rex-shape-up",
    "date"     : "March 31, 2023",
    "excerpt"  : "REX-Shape Up, un LFT dont vous √™tes les h√©ros. \nPr√©sent√© par Pierre-Thomas GUILLOT.\n",
  "content"  : "REX-Shape Up, un LFT dont vous √™tes les h√©ros. \nPr√©sent√© par Pierre-Thomas GUILLOT.\n"
} ,
  
  {
    "title"    : "Retour Conf√©rence Vue Amsterdam 2023",
    "category" : "",
    "tags"     : " node, Node, vue, vuex, pinia, vite, Vitest, TypeScript, developer, javascript",
    "url"      : "/2023/03/31/retour-vue-amsterdam-2023.html",
    "date"     : "March 31, 2023",
    "excerpt"  : "C‚Äôest dans le Theater Amsterdam que se sont d√©roul√©s ces deux jours de VueJS Amsterdam, √©v√©nement faisant partie de la JSWorld Conference, durant toute la semaine.\n\nDe nombreux sponsors √©taient l√† pour l‚Äôoccasion, ainsi que des √©coles comme VueMas...",
  "content"  : "C‚Äôest dans le Theater Amsterdam que se sont d√©roul√©s ces deux jours de VueJS Amsterdam, √©v√©nement faisant partie de la JSWorld Conference, durant toute la semaine.\n\nDe nombreux sponsors √©taient l√† pour l‚Äôoccasion, ainsi que des √©coles comme VueMastery ou VueSchool (proposant une toute nouvelle certification Vue), et des partenaires plus connus comme Storyblok ou Nuxt Labs.\n\n\n\nL‚Äôambiance √©tait au rendez-vous d√®s le d√©but avec une conf√©rence en musique, avec Tim Benniks guitare en main !\n\nNous avons ensuite pu profiter de conf√©rences aussi nombreuses que vari√©es. De l‚Äôaccessibilit√© √† la gestion de l‚Äôinterface en Vue du leader mondial du transport de marchandises Maersk, en passant par les tests et le guide ultime pour publier un package NPM !\n\nSans oublier la grande famille Nuxt qui √©tait (presque) au complet !\n\nL‚Äôensemble des conf√©rences est visible sur la cha√Æne youtube du JSWorld Conference.\n\nState of Vuenion\n\nEvan You (cr√©ateur de Vue et Vite) a pr√©sent√© un √©tat des lieux et des derni√®res avanc√©es de Vue, √©paul√© par Alex Kyriakidis (fondateur de VueSchool).\n\n\n\nCommen√ßant par un retour sur les nombreuses nouveaut√©s de ces trois derni√®res ann√©es, pour les plus connues Vue 3, Vite, Vitest et Pinia, Evan a d‚Äôabord fait un focus sur la position de Vue 3 en tant que version par d√©faut depuis le 7 f√©vrier 2022.\n\nIl a aussi √©voqu√© les avanc√©es relatives √† Vue 2.7 (dont l‚Äôint√©gration de la Composition API, du v-bind CSS, etc.) permettant de rapprocher les exp√©riences d√©veloppeurs entre Vue 2 et Vue 3.\n\nEvan a ensuite pr√©sent√© les derniers travaux sur la version core de Vue en version 3. Principalement orient√©s sur des am√©liorations concernant la facilit√© d‚Äôutilisation, l‚Äôacc√©l√©ration des tests (avec Vitest) ainsi que la vitesse de build (avec Rollup).\n\nEnfin, Evan a pr√©sent√© les projets √† venir pour le core. La disparition de la Reactivity Transform jug√©e trop risqu√©e, l‚Äôam√©lioration du server side rendering et la cr√©ation d‚Äôun ‚Äúvapor mode‚Äù, une nouvelle mani√®re de compiler les Single Files Components en optimisant l‚Äôutilisation d‚Äôun Virtual DOM. Ce dernier permet d‚Äôapprocher la vitesse de compilation d‚Äôune application en JS vanilla, en gardant la puissance du framework !\n\n\n  üì∫ Conf√©rence sur Youtube\n\n\nLa fin de vie de Vue 2\n\n\n\nApr√®s avoir pr√©sent√© l‚Äô√©tat de l‚Äô√©cosyst√®me qui gravite autour de Vue et Vite lors de sa conf√©rence ‚ÄúState of the Vuenion 2023‚Äù, Evan You a termin√© en d√©voilant la date de fin de vie de Vue 2 au 31 d√©cembre 2023.\n\nApr√®s cette date, la version arr√™tera de recevoir des mises √† jour ; il sera n√©anmoins possible de b√©n√©ficier de mise √† jour payante, bien qu‚Äôil soit vivement conseill√© de migrer vers la derni√®re version du framework.\n\nUne page dans la documentation a √©t√© mise en place, pr√©sentant les options qui s‚Äôoffrent aux d√©veloppeurs et aux organismes qui n‚Äôont pas encore migr√© leurs applications.\n\nIl √©tait une fois‚Ä¶ Histoire\n\n\n\nGuillaume Chau, un des membres de l‚Äô√©quipe de d√©veloppement de Vue, a pr√©sent√© l‚Äôoutil Histoire durant une petite demi-heure.\n\nL‚Äôoutil est dans la m√™me veine que Storybook. La plupart du temps, il sert √† afficher et documenter des composants d‚Äôun design system en compl√®te isolation.\n\nContrairement √† d‚Äôautres outils, Histoire est pens√© pour s‚Äôint√©grer parfaitement dans son environnement de d√©veloppement, de fa√ßon √† ce que l‚Äô√©criture des ‚Äústories‚Äù s‚Äôapparente le plus possible √† l‚Äô√©criture et √† l‚Äôutilisation native de composants Vue.\n\nHistoire utilise Vite ce qui lui permet de s‚Äôint√©grer dans un projet qui l‚Äôutilise d√©j√† avec tr√®s peu de configurations suppl√©mentaires.\n\n\n  üì∫ Conf√©rence sur Youtube\n\n\nUne autre histoire de‚Ä¶ migration !\n\n\n\nLa soci√©t√© Maersk, sp√©cialiste dans la logistique des transports, nous a pr√©sent√© son application destin√©e entre autres, √† la gestion des conteneurs maritimes. Une occasion pour nous faire part de leur processus de migration de Vue 2 vers Vue 3 !\n\nR√©alisant la m√™me migration de version √† Bedrock Streaming sur la partie backoffice, nous constatons que nous partageons beaucoup de similitudes !\n\nVous trouverez un article d√©di√© sur la migration Vue 2 vers Vue 3 √† Bedrock en suivant ce lien ! üéâ\n\n\n  üì∫ Conf√©rence sur Youtube\n\n\n‚ÄúLet‚Äôs Build A Virtual DOM‚Äù\n\n\n\nCertaines conf√©rences √©taient aussi l‚Äôoccasion de rappeler des fondamentaux. Beaucoup de d√©veloppeurs sont conscients que Vue utilise un syst√®me de DOM virtuel pour g√©n√©rer ses pages mais peu savent vraiment ce que cela signifie.\n\nLa conf√©rence de Marc Backes a permis de d√©mystifier cela en d√©veloppant sur sc√®ne un DOM virtuel simple √† travers plusieurs cas pratiques.\n\nCe dernier a d‚Äôailleurs publi√© le code √©crit sur son Github : https://github.com/themarcba/vue-vdom.\n\n\n  üì∫ Conf√©rence sur Youtube\n\n\nLe guide complet du packaging des librairies\n\n\n\nCette conf√©rence √©tait pr√©sent√©e par Bjorn Lu (core team member de Astro, Vite et Svelte), et expliquait comment cr√©er un package d‚Äôune librairie et le publier ‚Äúpresque‚Äù sans peine.\n\nEn prenant l‚Äôexemple d‚Äôune librairie extr√™mement simple, proposant une fonction d‚Äôaddition, Bjorn a parcouru les √©tapes de la cr√©ation de ce package progressivement, en prenant en compte le fonctionnement d‚ÄôESModules, l‚Äôajout du typage, puis le support de CommonJS pour les utilisations sur des anciennes versions de node et de l‚Äôexport en parall√®le des version ESM et CJS.\n\nIl pr√©sente ensuite les outils de build les plus courants ainsi que quelques outsiders, puis prend l‚Äôexemple de Tsup pour montrer une commande de build.\n\nUne solution int√©ressante pour typer utilisant JSDoc et simplifiant beaucoup les √©tapes du build completera sa conf√©rence, \npour enfin terminer par une s√©rie d‚Äôoutils et de ‚Äúdo and don‚Äôt‚Äù tr√®s pratiques dont publint.dev fait parti.\n\n\n  üì∫ Conf√©rence sur Youtube\n\n\nConclusion\n\n\n\nCe s√©jour √† Amsterdam pour assister √† cette conf√©rence de deux jours a √©t√© enrichissant √† bien des √©gards.\nNon seulement la ville est belle et agr√©able √† visiter, mais la d√©couverte d‚Äôoutils prometteurs r√©pondant √† nos besoins a √©galement √©t√© un v√©ritable apport pour notre entreprise.\n\nPour en d√©couvrir plus\n\n\n  Site VueJS Amsterdam\n  Gallerie photos\n  Replay des conf√©rences sur Youtube\n\n"
} ,
  
  {
    "title"    : "Montres bracelets, le guide pratique de l&#39;amateur d&#39;horlogerie #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/guide-pratique-amateur-horlogerie",
    "date"     : "March 31, 2023",
    "excerpt"  : "Montres bracelets, le guide pratique de l‚Äôamateur d‚Äôhorlogerie.\nPr√©sent√© par Rafi PANOYAN.\n",
  "content"  : "Montres bracelets, le guide pratique de l‚Äôamateur d‚Äôhorlogerie.\nPr√©sent√© par Rafi PANOYAN.\n"
} ,
  
  {
    "title"    : "Comment g√©rer des journ√©es de 35h #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-gerer-des-journees-de-35h",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment g√©rer des journ√©es de 35h\nPr√©sent√© par Sylvain GOUGOUZIAN.\n",
  "content"  : "Comment g√©rer des journ√©es de 35h\nPr√©sent√© par Sylvain GOUGOUZIAN.\n"
} ,
  
  {
    "title"    : "Comment (enfin) sortir vos side projects #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-enfin-sortir-vos-side-projects",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment (enfin) sortir vos side projects.\nPr√©sent√© par Thomas JARRAND.\n",
  "content"  : "Comment (enfin) sortir vos side projects.\nPr√©sent√© par Thomas JARRAND.\n"
} ,
  
  {
    "title"    : "Comment j&#39;ai r√©ussi √† capturer la couleur et quelle est sa signification ? #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/capture-et-signification-de-la-couleur",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment j‚Äôai r√©ussi √† capturer la couleur et quelle est sa signification ?\nPr√©sent√© par Hugo DETANG.\n",
  "content"  : "Comment j‚Äôai r√©ussi √† capturer la couleur et quelle est sa signification ?\nPr√©sent√© par Hugo DETANG.\n"
} ,
  
  {
    "title"    : "Couleur, Typographie, Logo: Analyse dune charte graphique #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/analyse-charte-graphique",
    "date"     : "March 31, 2023",
    "excerpt"  : "Couleur, Typographie, Logo: Analyse dune charte graphique. \nPr√©sent√© par Sylvain MASSON.\n",
  "content"  : "Couleur, Typographie, Logo: Analyse dune charte graphique. \nPr√©sent√© par Sylvain MASSON.\n"
} ,
  
  {
    "title"    : "De Node.js 10 √† Node.js 18, nous avons rattrap√© 8 ans de retard et de dette technique",
    "category" : "",
    "tags"     : " node, Node.js, vue, Vue.js, vuex, pinia, vite, Vite.js, Vitest, TypeScript, developer retention, migration",
    "url"      : "/2023/03/25/de-node-js-10-a-node-js-18-nous-avons-rattrape-8-ans-de-retard-et-de-dette-technique-et-seule-une-approche-progressive-et-incrementale-etait-viable.html",
    "date"     : "March 25, 2023",
    "excerpt"  : "Difficile de faire √©voluer des applications et am√©liorer une stack si l‚Äôensemble est bas√© sur une version obsol√®te de Node.js‚Ä¶ Dans cet article, nous verrons comment nous avons r√©ussi √† migrer vers une version r√©cente et maintenue de Node.js gr√¢ce...",
  "content"  : "Difficile de faire √©voluer des applications et am√©liorer une stack si l‚Äôensemble est bas√© sur une version obsol√®te de Node.js‚Ä¶ Dans cet article, nous verrons comment nous avons r√©ussi √† migrer vers une version r√©cente et maintenue de Node.js gr√¢ce √† une approche progressive et incr√©mentale.\n\n\n  Contexte g√©n√©ral et fonctionnel\n  Contexte technique\n  Objectif\n  Une premi√®re strat√©gie probl√©matique : la m√©thode ‚Äúrhinoc√©ros‚Äù ü¶è\n  La strat√©gie gagnante : une migration progressive üì∂    \n      Motivation\n      Plan d‚Äôaction\n    \n  \n  Difficult√©s rencontr√©es    \n      Non d√©coupage des √©tapes de migration\n      M√©connaissance de Typescript\n      Suppression pr√©cipit√©e de librairies obsol√®tes\n      Non anticipation de la complexit√© li√©e √† certaines d√©pendances\n      Entretien des applications legacy en m√™me temps\n    \n  \n  Autres avantages    \n      Uniformisation des technologies au sein de la soci√©t√©\n      Attractivit√© et r√©tention des d√©veloppeurs\n    \n  \n  Conclusion\n\n\nContexte g√©n√©ral et fonctionnel\n\nBedrock streaming est une co-entreprise (joint-venture) cr√©√©e en 2020 par M6 Group et RTL Group, permettant √† 7 diffuseurs et soci√©t√©s de m√©dias dans 5 pays d‚ÄôEurope de divertir 45 millions d‚Äôutilisateurs chaque jour, sur tous les √©crans.\n\nPour g√©rer tous leurs utilisateurs ainsi que leurs contenus, notamment vid√©os, les clients de Bedrock Streaming acc√®dent chacun √† une constellation d‚Äôapplications au sein d‚Äôun back-office centralis√© (appel√© BO par la suite).\n\nContexte technique\n\nDe part sa conception initiale, le BO est une application monorepo. Elle fournit (√† elle-m√™me donc), des donn√©es via une API Symfony 4 (PHP 7.4), consomm√©es uniquement par :\n\n\n  des applications Vue.js 1 et Vue.js 2 g√©r√©es par la team backend (qui historiquement maintient le frontend de quelques applications) ;\n  des applications Vue.js 2 g√©r√©es par la team frontend.\n\n\nLe tout, dans un environnement Node.js 10.\n\nObjectif\n\nNode.js 10 est arriv√© en fin de vie le 30 avril 2021. Il n‚Äôest donc plus maintenu, que ce soit en terme de fonctionnalit√©s ou en terme de s√©curit√©. Naturellement, toutes les d√©pendances JS migrent progressivement vers un support des versions de Node.js sup√©rieures, et abandonnent le support de cette version 10 devenue obsol√®te.\n\nIl s‚Äôagit donc de migrer la version de Node.js vers une version sup√©rieure, dans l‚Äôid√©al LTS afin de se pr√©munir d‚Äôune obsolescence pr√©matur√©e. Dans un premier temps, Node.js 12.\n\nVoici plusieurs raisons qui poussent √† migrer Node.js :\n\n\n  Nouvelles fonctionnalit√©s (e.g. nouvelle impl√©mentation pour l‚ÄôES6 Module Support exp√©rimental, source : https://nodejs.medium.com/announcing-a-new-experimental-modules-1be8d2d6c2ff ) ;\n  Abandon de fonctionnalit√©s d√©faillantes (e.g. via d√©pr√©ciation) ;\n  Performance (e.g. mise √† jour de V8 engine, source : https://nodejs.medium.com/introducing-node-js-12-76c41a1b3f3f ) ;\n  S√©curit√© (e.g. mise √† jour de TLS, source : https://nodejs.medium.com/introducing-node-js-12-76c41a1b3f3f ) ;\n  √âvolutions des d√©pendances externes. (e.g. Cypress qui abandonne les versions de Node.js non maintenues et qui requiert Node.js 14, 16 ou 18+, source : https://docs.cypress.io/guides/references/changelog#12-0-0).\n\n\nUne premi√®re strat√©gie probl√©matique : la m√©thode ‚Äúrhinoc√©ros‚Äù ü¶è\n\nLa d√©cision a √©t√© prise de migrer le repository de Node.js 10 vers Node.js 12 en d√©but d‚Äôann√©e 2021.\n\nEmpiriquement, cette m√©thode a montr√© plusieurs limites :\n\n\n  m√™me si la compilation semblait bien se d√©rouler, des erreurs apparaissaient au moment de l‚Äôaffichage de l‚ÄôUI ‚û° Il semblait donc n√©cessaire de parcourir l‚Äôint√©gralit√© des √©crans afin de d√©celer toutes les anomalies possibles ‚û° Le travail de la QA √©tait alors cons√©quent ;\n  m√™me lorsqu‚Äôune anomalie est corrig√©e, une nouvelle peut apparaitre ‚û° Il fallait re-parcourir les √©crans concern√©s (par exemple, apr√®s avoir corrig√© une anomalie qui emp√™che l‚Äôapparition d‚Äôune modale, de nouvelles anomalies peuvent √™tre d√©cel√©es au niveau des fonctionnalit√©s que permet cette modale) ‚û° Le travail de la QA augmentait de fa√ßon exponentielle au fil des corrections d‚Äôanomalies ;\n  des dizaines voire centaines de d√©pendances dans le projet √©taient d√©pendantes de Node.js 10 sans √™tre encore compatibles avec Node.js 12 ‚û° Il s‚Äôagissait donc de faire le point sur celles-ci, pour trouver des √©quivalents compatibles.\n\n\nApr√®s plusieurs mois, bien que bon nombre d‚Äôanomalies avaient pu √™tre corrig√©es, la situation stagnait et la fin ne semblait pas plus proche qu‚Äôau d√©but.\n\nLes raisons de l‚Äô√©chec :\n\n\n  L‚Äôanciennet√© de certaines applications. Certaines d‚Äôentre elles avaient plus de 8 ans d‚Äôexistence. En n‚Äôayant subi que quelques corrections seulement. Les connaissances fonctionnelles et techniques s‚Äô√©taient donc estomp√©es naturellement, en raison d‚Äôune absence de documentation (autant fonctionnelle que technique). Il s‚Äôagit l√† des dettes fonctionnelle et technique. Lorsqu‚Äôelles sont l√†, elles sont relativement simples √† identifier. Mais c‚Äôest d√©j√† trop tard‚Ä¶ ;\n  L‚Äôabsence de mise √† jour des technologies. Certaines technologies devenues obsol√®tes (jQuery 1.9, Vue.js 1, Bootstrap 2.3) imposait non plus un refactor li√© √† une migration, mais une v√©ritable refonte ;\n  L‚Äôabsence de tests. La couverture de tests √©tait alors faible voire nulle. Migrer sans r√©gression relevait alors d‚Äôune chance non maitrisable ;\n  La fa√ßon dont la migration a √©t√© lanc√©e √©tait trop t√©m√©raire : c‚Äôest la m√©thode rhinoc√©ros.\n    \n      cr√©ation d‚Äôune nouvelle branche (et d‚Äôune PR pour cette branche)\n      suppression de Node.js 10 et installation de Node.js 12\n      correction de toutes les anomalies qui apparaissent !\n    \n  \n\n\nCe fonctionnement peut marcher pour des p√©rim√®tres techniques plus petits ou du moins dont les contours sont pr√©cis√©ment marqu√©s ;\n\nL‚Äôorganisation en √©quipe devenait compliqu√©e. Au fur et √† mesure des d√©couvertes des anomalies au sein d‚Äôune seule et unique PR, il devenait difficile de suivre tous les sujets, sans d√©coupage pr√©cis et rigoureux.\n\nFace √† cette situation, dont les d√©veloppeurs et testeurs ne semblaient plus voir le bout, il a √©t√© d√©cid√© d‚Äôemployer une autre strat√©gie.\n\nLa strat√©gie gagnante : une migration progressive üì∂\n\nDe part un essoufflement des d√©veloppeurs et une nouvelle √©nergie insuffl√©e par des d√©parts et arriv√©es dans l‚Äô√©quipe, une nouvelle strat√©gie a √©merg√©. Face √† l‚Äô√©chec de la premi√®re, il a √©t√© propos√© plus simplement de partir sur des bases saines, afin de migrer les applications sur des fondations plus solides car maitris√©es.\n\nPlus techniquement, cela s‚Äôest traduit par :\n\n\n  Cr√©ation d‚Äôun nouveau r√©pertoire modern-apps/ dans le monorepo.\n  Mise en place d‚Äôune architecture bas√©e sur Node.js 16 (Oui oui, Node.js 16 directement ! Il s‚Äôagissait de la version LTS en cours en date de d√©but 2022.) dans ce r√©pertoire seulement.\n  Migration des applications du BO, une par une, vers une stack plus moderne. En date de d√©but 2023, cette migration est toujours en cours.\n\n\nMotivation\n\nLa motivation √©tait principalement port√©e par :\n\n\n  une volont√© forte d‚Äôabandonner des outils et technologies vieillissantes voire obsol√®tes ;\n  une pression engendr√©e par l‚Äô√©volution rapide des technologies :\n    \n      Node.js sort une version LTS tous les ans ;\n      Vue.js 3 venait de sortir et l‚Äôeffort des d√©veloppeurs du framework allait se porter plut√¥t sur cette version que sur la version 2.\n    \n  \n  une pression engendr√©e par les autres √©quipes de la soci√©t√© qui, elles, √©taient √† jour (pour certaines), dont celle qui proposait des outils JS et TS dont l‚Äô√©quipe pourrait avoir l‚Äôusage, comme par exemple une librairie de configuration pour eslint coupl√© √† vue ;\n  une excitation li√©e √† l‚Äôutilisation d‚Äôune stack r√©cente et de cutting-edge tools.\n\n\nPlan d‚Äôaction\n\nCette page blanche a n√©cessit√© un plan d‚Äôaction que voici :\n\n\n  Cr√©ation d‚Äôune application simplissime en guise de PoC, afin de montrer la viabilit√© d‚Äôun travail sous Node.js 16 dans une sous-partie du projet en parall√®le d‚Äôun travail toujours actif sous Node.js 10 dans le reste du projet.\n  Mise en place d‚Äôune certaines DX vis-√†-vis des linters et formatters notamment (ainsi que d‚Äôextensions d‚ÄôIDE), par l‚Äôapplication de r√®gles simples mais strictes, qui √©vitent aux d√©veloppeurs les t√¢ches sans plus-value, comme ajuster manuellement l‚Äôindentation ou ajouter les points-virgules.\n  Migration des librairies internes au monorepo.\n  Migration du design system, ainsi que des outils aff√©rents (Storybook).\n  Migration d‚Äôune premi√®re application, la plus simple possible. L‚Äôobjectif √©tait alors de se rendre compte tr√®s concr√®tement des √©tapes de migration d‚Äôune application, afin d‚Äôen tirer une documentation exploitable pour les futures applications. Il en est ressorti que la majeure partie du travail consistait √† refactor le code avec les nouvelles technologies choisies, en l‚Äôoccurrence :\n    \n      Vue.js 3 et sa Composition API (framework JS),\n      Vite (serveur de dev et de build),\n      Pinia (global state management),\n      Vitest (framework de test unitaire),\n      Cypress dans ses derni√®res versions (framework de test end-to-end)\n      aussi et surtout Typescript (langage de programmation, sur-couche √† JS).\n    \n  \n  Migration du processus de build et d‚Äôint√©gration aux templates backend (via notamment une extension Twig impl√©ment√©e par nos soins, ViteAppExtension.php)\n  Mise en place d‚Äôune CI pour ces nouvelles applications, calqu√©e sur celle des anciennes applications : linting, tests pour celles qui en avaient, d√©ploiement en preview, etc.\n\n\nEn quelques mois seulement, il a √©t√© possible d‚Äôobtenir un r√©sultat concret. Le r√©pertoire modern-apps/ a √©t√© initi√© en f√©vrier 2022, et d√®s avril de la m√™me ann√©e, une premi√®re application migr√©e √©tait livr√©e en production. Et cela, avec un seul d√©veloppeur √† plein temps sur le sujet.\n\nDifficult√©s rencontr√©es\n\nCette seconde strat√©gie n‚Äôa bien s√ªr pas √©t√© sans encombre. Voici les principales difficult√©s rencontr√©es, dont l‚Äô√©quipe a su se pr√©munir au fil du temps.\n\nNon d√©coupage des √©tapes de migration\n\nLors de la migration d‚Äôune des premi√®res applications dont la complexit√© √©tait l√©g√®rement sup√©rieure aux pr√©c√©dentes, nous nous sommes retrouv√©s embourb√©s dans une multitude de bugs techniques et fonctionnels. En effet, migrer implique plusieurs changements qui n‚Äôont pas n√©cessairement de rapport les uns avec les autres :\n\n\n  ajouter des types TS\n  migrer la librairie de Global State Management de vuex vers pinia\n  migrer la Global API de Vue (de new Vue() vers createApp())\n  migrer de l‚ÄôOptions API vers la Composition API de Vue\n  etc.\n\n\nSi tous ces changements sont op√©r√©s en m√™me temps, comment r√©agir lors de l‚Äôapparition d‚Äôune anomalie ? Comment traquer efficacement cette anomalie ?\n\n\n  Solution adopt√©e\n\n  Nous avons d√©cid√© de d√©couper plus finement nos d√©veloppements. Une PR doit concerner un p√©rim√®tre r√©duit et bien d√©fini. Par exemple, la PR de migration de la librairie de Global State Management ne doit comporter que des modifications √† ce sujet, et doit fournir une application fonctionnelle dont les tests passent.\n\n\nM√©connaissance de Typescript\n\n\n  TypeScript is a strongly typed programming language that builds on JavaScript, giving you better tooling at any scale.\n\n\nSource : https://www.typescriptlang.org/\n\nCe langage de programmation, bien que son adoption parmi les d√©veloppeurs JS explose, s‚Äôest av√©r√© une compl√®te nouveaut√© dans l‚Äô√©quipe. Il peut √™tre tentant d‚Äô√©crire des any partout, ou de supprimer le strict mode‚Ä¶\n\n\n  Solution adopt√©e\n\n  Nous avons d√©cid√© d‚Äôint√©grer TS progressivement sans se mettre trop de pression quant √† l‚Äôint√©gralit√© du typage de nos applications. Typescript permet justement cette int√©gration progressive aux projets.\n\n  Un tr√®s gros progr√®s a aussi √©t√© r√©alis√© gr√¢ce √† la g√©n√©ration automatique des types TS √† partir de l‚ÄôAPI (gr√¢ce √† l‚Äôintrospection system de GraphQL). Les donn√©es re√ßues du backend se voyaient alors avoir une structure directement exploitable dans le frontend.\n\n\nSuppression pr√©cipit√©e de librairies obsol√®tes\n\nLors du d√©coupage des √©tapes de migration, une probl√©matique est apparue. Par exemple, si nous souhaitons migrer de vuex vers pinia dans un second temps, comment faire pour que l‚Äôapplication reste fonctionnelle avec vuex dans le premier temps ?\n\n\n  Solution adopt√©e\n\n  Nous avons d√©cid√© de conserver certaines librairies, le temps de la migration des applications. Il peut √™tre tentant de vouloir supprimer imm√©diatement ce qui nous semble obsol√®te, mais ces √©l√©ments ne seront vraiment obsol√®tes que lorsque toutes les applications seront migr√©es ; mais pas le temps qu‚Äôelles le soient.\n\n\nNon anticipation de la complexit√© li√©e √† certaines d√©pendances\n\nBien que cet aspect n‚Äô√©tait pas une surprise, certaines librairies ont apport√© plus de difficult√©s que d‚Äôautres lors de la migration. Par exemple, l‚Äôint√©gration de Vue 3 et la Composition API impliquait la mont√©e de version de vee-validate, un librairie de validation de formulaire. Il s‚Äôest av√©r√© que l‚Äôimpl√©mentation impos√©e √©tait radicalement diff√©rente de la version pr√©c√©dente (compatible avec Vue 2 et l‚ÄôOptions API), moins intuitive et plus complexe.\n\n\n  Solution adopt√©e\n\n  Ce cas de figure n‚Äôest pas vraiment impressionnant car nous nous y attendions. Nous avons d√©cid√© dans un premier temps d‚Äôeffectuer une certaine veille technique, afin de remettre en cause le choix initial de cette librairie. Il s‚Äôest av√©r√© que nous l‚Äôavons conserv√©e, ce qui amenait dans un second temps une mont√©e en comp√©tence quant √† l‚Äôutilisation de celle-ci, en vue de son int√©gration.\n\n\nEntretien des applications legacy en m√™me temps\n\nUne application donn√©e pouvait se retrouver d‚Äôune part en cours de migration, et d‚Äôautre part devoir recevoir une √©volution ou une correction d‚Äôanomalie.\n\n\n  Solution adopt√©e\n\n  Le choix et l‚Äôordre des applications √† migrer a √©t√© choisi en fonction des priorit√©s en cours. Nous avons choisi de migrer en premier les applications qui ne subissaient que tr√®s peu de modifications. Par la suite, et encore aujourd‚Äôhui, nous livrons en production rapidement chaque application migr√©e, afin de ne pas avoir √† maintenir plusieurs versions en m√™me temps (la version legacy √©tant tout de m√™me conserv√©e le temps de s‚Äôassurer que la version moderne tourne correctement en production aupr√®s des clients). Dans les tr√®s rares cas o√π une application en cours de migration devait recevoir une √©volution ou une correction d‚Äôanomalie, nous la traitions dans les 2 versions.\n\n\nAutres avantages\n\nUniformisation des technologies au sein de la soci√©t√©\n\nAu sein de Bedrock, le back-office n‚Äôest pas la seule application. Il existe aussi des applications frontend sur les m√™mes technologies pour adresser l‚Äô√©cran web ou les t√©l√©visions connect√©es. Bien que le framework utilis√© pour celles-ci soit React.js et non Vue.js, l‚Äôoutillage peut √™tre uniformis√© entre les projets et les √©quipes. La migration a permis de pr√©parer le terrain pour mettre en place ces outils : TypeScript, PNPM, etc.\n\nAttractivit√© et r√©tention des d√©veloppeurs\n\nCette migration g√©n√©rale permet de mettre en place une stack r√©solument plus moderne et d‚Äôutiliser des outils et technologies plus r√©cents. N‚Äôest-ce pas l√† un argument fort pour attirer des nouveaux d√©veloppeurs et retenir ceux d√©j√† en place ? Dans l‚Äô√©quipe, plusieurs personnes ont √©mis des doutes sur leur volont√© de rester dans la soci√©t√© si la d√©cision de migrer, et donc d‚Äôint√©grer des technologies plus √† jour, n‚Äôavait pas √©t√© prise. En date de d√©but 2023, il fait peu de doutes que les projets en Vue 3 sont plus attractifs que les projets en Vue 2‚Ä¶\n\nConclusion\n\nEn fin de compte, cette approche progressive et incr√©mentale, toujours en cours, permet de maintenir dans un r√©pertoire bien d√©fini une stack r√©cente dont les mises √† jour sont simples car petites. Par exemple, nous avons r√©cemment migr√© de Node.js 16 vers Node.js 18‚Ä¶ en quelques jours !\n\nCette grande aventure, toujours en cours, nous a permis de vraiment prendre conscience qu‚Äôil faut entretenir certes les applications mais aussi les versions des frameworks et outils ! Utiliser un nouvel outil ou une nouvelle technologie est un choix fort qu‚Äôil faut √™tre capable d‚Äôassumer dans le temps.\n\nIl peut paraitre frustrant d‚Äôentretenir des outils, sans gagner en performance ni en productivit√© mais seulement pour ne pas devenir obsol√®te. Mettre l‚Äôaccent sur ces points, tout en sachant bien jauger jusqu‚Äôo√π doivent aller ces upgrades, est la marque d‚Äôun certain professionnalisme.\n\nIl est vrai que dans l‚Äôimm√©diat, la valeur ajout√©e pour le client est mod√©r√©e : les gains restent tr√®s techniques, notamment en termes de stabilit√© et de performances. Ce n‚Äôest que plus tard que les gains se feront concr√®tement sentir : plus d‚Äôefficacit√© et de productivit√© pour les √©volutions, et plus de fiabilit√©.\n\nIl est aussi important de savoir reconnaitre qu‚Äôune technologie utilis√©e (parfois avec fiert√© √† ses d√©buts) est devenue obsol√®te, et qu‚Äôil faut s‚Äôen d√©barrasser pendant qu‚Äôil est encore temps.\n"
} ,
  
  {
    "title"    : "La gamification contre le legacy",
    "category" : "",
    "tags"     : " infra, legacy, retour d'exp√©rience",
    "url"      : "/2023/03/20/La-gamification-contre-le-legacy.html",
    "date"     : "March 20, 2023",
    "excerpt"  : "Ce que vous ne voulez pas voir dans vos backlogs‚Ä¶\n\nElles sont l√†, tapies dans l‚Äôombre de la colonne ‚ÄúTo do‚Äù de vos backlogs, attendant que leur heure vienne. √Ä chaque backlog refinement, vous vous demandez s‚Äôil ne faut pas tout simplement les annu...",
  "content"  : "Ce que vous ne voulez pas voir dans vos backlogs‚Ä¶\n\nElles sont l√†, tapies dans l‚Äôombre de la colonne ‚ÄúTo do‚Äù de vos backlogs, attendant que leur heure vienne. √Ä chaque backlog refinement, vous vous demandez s‚Äôil ne faut pas tout simplement les annuler, puisque personne ne les prend en charge‚Ä¶ De quoi parle-t-on ? De ces user stories qui existent dans le backlog de chaque √©quipe technique, pour traiter ‚Äúun jour‚Äù un sujet legacy. Ces petits aides-m√©moire de sujets ‚Äú√† ne pas oublier‚Äù qui nous poursuivent mais ne sont que peu souvent trait√©s, faute de priorisation.\n\n\nUn exemple de backlog legacy\n\n\nClean de code mort, mont√©es de versions de layers Terraform, projets de refactoring jamais d√©but√©s‚Ä¶ autant de sujets p√©nibles √† traiter qui n√©cessitent du temps‚Ä¶ et de la r√©silience. Parce que bien souvent, d√©buter l‚Äôun de ces sujets revient √† s‚Äôattaquer √† toutes les d√©pendances li√©es, √† g√©rer tous les impacts. Et parce qu‚Äôil s‚Äôagit aussi de t√¢ches redondantes, non-automatisables, n‚Äôapportant quasiment aucune valeur business imm√©diatement mesurable.. Du ‚Äúrun‚Äù, pur et simple. Dans le Slack de Bedrock, il y a un emoji tout trouv√© pour ce type de t√¢che : \n\nBien s√ªr, on parvient parfois √† d√©gager du temps pour s‚Äôatteler √† ces user stories. Mais il faut souvent plus d‚Äôun sprint pour en venir √† bout, et l‚Äô√©quipe en charge de leur r√©alisation peut rapidement se d√©courager devant l‚Äôampleur et le caract√®re r√©p√©titif de la t√¢che.\n\nNos √©quipes Ops et DevOps sont responsables de 23 repositories Terraform. Lorsqu‚Äôil a √©t√© n√©cessaire d‚Äôupgrader tous nos layers en version 1.x, nous nous sommes d‚Äôabord donn√© pour consigne que chaque personne qui tombait sur un layer obsol√®te devait le mettre √† jour avant de poursuivre son travail. Oui mais voil√†, mettre √† jour un layer √ßa ne se fait pas en deux minutes, et bien souvent on refuse d‚Äôabandonner ce sur quoi on travaillait jusqu‚Äôalors pour mettre √† jour sa version de Terraform. La consigne a alors √©volu√© : pour chaque layer √† mettre √† jour, on cr√©√© une US en colonne ‚Äúto do‚Äù‚Ä¶ Vous voyez o√π l‚Äôon veut en venir ? üòè\n\nPour tenter de venir √† bout de ces sujets legacy que l‚Äôon tra√Æne comme des boulets, nous avons mis en place depuis octobre 2022 les ‚ÄúJeudis du fun‚Äù, dont l‚Äôorganisation est prise en charge par la facilitatrice agile et la Project Manager Officer (PMO) du service Infrastructure (autrices de cet article).\n\n\nLogo de la 1√®re √©dition du ‚Äújeudi du fun‚Äù\n\n\n√âradiquer en gamifiant, challengeant, s‚Äôentraidant.\n\nL‚Äôid√©e est simple : faire travailler ensemble, sur une journ√©e, les cinq √©quipes de la verticale (trois √©quipes de SysAdmins et deux √©quipes DevOps) pour faire avancer un sujet legacy. Au cours de cette journ√©e, les profils et les membres d‚Äô√©quipe seront mix√©s, afin de ne pas travailler avec les m√™mes coll√®gues qu‚Äôau quotidien. Leads, principal engineer, seniors et juniors : tout le monde participe √† la corv√©e !\n\nIl est difficile de convoquer 25 personnes sur une journ√©e en leur disant que la journ√©e est banalis√©e pour traiter des t√¢ches p√©nibles. Elles viendraient √† reculons. Deux axes ont √©t√© choisis pour faire de ces journ√©es des journ√©es ‚Äúparticuli√®res‚Äù :\n\n\n  Gamifier certains moments cl√©s de la journ√©e : la d√©couverte du sujet, la composition des √©quipes, la remise en jambe du d√©but d‚Äôapr√®s-midi‚Ä¶\n  Challenger les participants pour ne pas simplement leur demander de traiter du legacy, mais bien d‚Äô√™tre la meilleure √©quipe pour traiter du legacy. Celle qui ira le plus loin, qui en fera le plus.\n  (Un troisi√®me axe, plus convivial, est choisi pour la fin de journ√©e : partager un verre tous ensemble.)\n\n\n\nLe jeu de d√©couverte du sujet de la 3√®me √©dition : Ansible\n\n\nLors de la 1√®re √©dition, en octobre 2022, nous avons propos√© aux √©quipes un grand th√®me : le repository ‚ÄúSysAdmin/Terraform‚Äù, centre n√©vralgique du travail de l‚ÄôInfra. Il y a beaucoup √† faire : les fameux upgrades de layers, du refactoring de code pour industrialiser nos process, des PRs ouvertes et rest√©es en suspens depuis de nombreux mois‚Ä¶ chacun peut y trouver son compte. Chacune des six √©quipes compos√©es ce jour-l√† disposait de dix minutes pour d√©finir √† quel chantier elle s‚Äôattaquerait durant la journ√©e. A l‚Äôissue de ces dix minutes, le repr√©sentant de l‚Äô√©quipe devait pr√©senter aux autres le sujet choisi et l‚Äôindicateur qui permettrait de juger si le travail a √©t√© accompli ou non, en fin de journ√©e. L‚Äô√©quipe ayant propos√© le sujet le plus ambitieux s‚Äôest vue attribuer des points bonus, rentrant en compte pour le calcul du score final.\n\nPour la seconde √©dition le mois suivant, le sujet √©tait impos√© : toutes les √©quipes avaient pour objectif de mieux s√©curiser les secrets contenus dans le code Bedrock. L‚Äô√©quipe qui en traiterait le plus grand nombre l‚Äôemporterait.\n\nLors de la derni√®re √©dition, en f√©vrier dernier, la comp√©tition reposait √©galement sur le nombre de points gagn√©s par chaque √©quipe en fin de journ√©e. Nous avons attribu√© un nombre de points √† chaque t√¢che pouvant √™tre trait√©e dans la journ√©e, en fonction de sa complexit√© et/ou de sa priorit√©. Chaque √©quipe pouvait s‚Äôorganiser librement : choisir plusieurs petites t√¢ches ou deux plus importantes‚Ä¶\n\nBien s√ªr, pour que la comp√©tition soit totale, chaque √©dition du jeudi du fun se termine par une remise de prix : distribution de goodies, de cartes ‚Äúbonus‚Äù ou ‚Äúmalus‚Äù valables dans nos ‚Äúvrais‚Äù sprints, de gourmandises‚Ä¶ il faut que la r√©compense soit r√©elle pour que les participants se prennent au jeu.\n\n\nExemple de lot pouvant √™tre remport√© lors du ‚ÄúJeudi du fun‚Äù\n\n\nLe risque avec la comp√©tition, c‚Äôest de se laisser d√©border : gagner co√ªte que co√ªte, ajouter des points √† son compteur en faisant du ‚Äúquick &amp;amp; dirty‚Äù. Jusqu‚Äô√† pr√©sent, la comp√©tition dans la verticale Infra est rest√©e bon enfant : les √©quipes se d√©fient entre elles tout au long de la journ√©e, des points ‚Äúbonus‚Äù sont r√©clam√©s aux organisatrices au moindre pr√©texte‚Ä¶ mais personne ne perd de vue l‚Äôobjectif principal : venir √† bout du sujet.\n\nLes Jeudis du Fun reposent donc sur le challenge et le jeu. Mais nous avions sous-estim√© un autre axe nous permettant de faire de ces journ√©es un succ√®s : l‚Äôentraide. A chaque √©dition, les retours les plus enthousiastes portent sur le fait de passer une journ√©e √† travailler en cross-team. SysAdmins et DevOps apprennent les uns des autres, les juniors ont l‚Äôoccasion de former des leads‚Ä¶ et chacun √©largit son spectre de comp√©tences. Au-del√† du fait de venir √† bout de sujets legacy, l‚Äô√©mulation engendr√©e par ces journ√©es justifie √† elle-seule leur organisation.\n\nEt puis, quitte √† faire des jeudis du fun des journ√©es particuli√®res, autant y aller franchement : certains membres de nos √©quipes n‚Äôh√©sitent pas √† venir d√©guis√©s pour ajouter une dose de fun. Vous avez crois√© une licorne, Pikachu ou un plombier dans l‚Äôopen space de Bedrock ? Aucun doute, c‚Äô√©tait un jeudi ! Un dress code a m√™me √©t√© d√©fini lors de l‚Äô√©dition de f√©vrier 2023.\n\nIt√©rer, et corriger nos erreurs √† chaque √©dition\n\nTrois √©ditions du ‚Äújeudi du fun‚Äù ont √©t√© organis√©es jusqu‚Äô√† pr√©sent. √Ä la fin de chaque √©dition, les organisatrices recueillent le feed-back des participantes et participants, afin de corriger ce qui doit l‚Äô√™tre et de capitaliser sur ce qui a march√©. Voici le premier bilan que nous pouvons en tirer.\n\nDe l‚Äôimportance du choix du sujet\n\nLe succ√®s de la journ√©e repose sur le choix du sujet. En choisissant un sujet f√©d√©rateur, comme lors de notre premi√®re √©dition, et en laissant le soin √† chaque √©quipe de d√©finir quel chantier elle souhaitait mener, nous partions gagnantes. Le repo Sysadmin/Terraform sur lequel nous avons travaill√© lors de cette journ√©e est un point de douleur pour l‚Äôensemble de nos √©quipes : chacun des participants a compris l‚Äôint√©r√™t de jouer le jeu et de retrousser ses manches. Les √©quipes ont m√™me eu du mal √† cl√¥turer la journ√©e, car elles voulaient finir ce qu‚Äôelles avaient commenc√©.\n\n\nAu cours de la 1√®re journ√©e du ‚ÄúJeudi du fun‚Äù\n\n\nLors de la seconde √©dition en revanche, le sujet de cette √©dition a mis la journ√©e en p√©ril. Nous avions demand√© aux √©quipes d‚Äôajouter un niveau de s√©curit√© √† l‚Äôensemble des secrets contenus dans la codebase de Bedrock. Cela a suscit√© quelques difficult√©s :\n\n  Tout d‚Äôabord, il s‚Äôagissait de trouver une m√©thode pour identifier tous les secrets concern√©s. Toutes les √©quipes du jeudi du fun ont alors planch√© sur ce sujet, en utilisant des m√©thodes et outils diff√©rents. Au final, nous ne sommes parvenus que tardivement (2h apr√®s le lancement de la journ√©e) √† nous mettre d‚Äôaccord sur une m√©thodologie. Autant de temps perdu que nous aurions pu consacrer au c≈ìur du sujet, la s√©curisation des secrets.\n  En nous attaquant √† l‚Äôensemble des secrets de Bedrock, nous touchions forc√©ment √† des repositories projets dont nous ne sommes pas les code owners. Ce n‚Äôest pas une v√©ritable difficult√© en soi, puisqu‚Äôau quotidien, nous intervenons fr√©quemment dans ces repos projets pour accompagner les √©quipes devs. En revanche, l‚Äôajout d‚Äôun niveau de s√©curit√© suppl√©mentaire sur des secrets implique de pouvoir tester, puis de merger nos modifications. Impossible de r√©aliser ces actions sans les √©quipes back et front responsables des projets, ou sans impacter leur travail. Notre p√©rim√®tre d‚Äôintervention lors de cette journ√©e √† √©t√© consid√©rablement limit√©.\n\n\nLa complexit√© du sujet et le constat de notre incapacit√© √† avancer lors de cette journ√©e ont rapidement conduit √† un d√©couragement des troupes. Nous sommes tout de m√™me ressortis de cette √©dition avec des points positifs :\n\n  Une meilleure visibilit√© sur le p√©rim√®tre de s√©curisation √† couvrir, en d√©finissant le nombre de secrets concern√©s,\n  Un workflow visant √† d√©tecter √† l‚Äôavenir tout nouveau secret concern√©\n  ‚Ä¶ et la n√©cessit√© de mieux d√©finir les guidelines pour le choix du sujet !\n\n\nEntendu pendant la 2nde √©dition du jeudi du fun üòÖ\n\n\n  üëßüèª : ‚ÄúAlors, qu‚Äôest-ce que tu fais de beau ?‚Äù\n\n  üë¶ : ‚ÄúJe souffre‚Äù\n\n\nCes guidelines nous ont aid√© √† d√©finir le choix de la th√©matique de la 3√®me √©dition du jeudi du fun. Le sujet devait r√©pondre √† ces crit√®res :\n\n  √ätre r√©alisable en une journ√©e,\n  Permettre de terminer / acc√©l√©rer un projet ou d‚Äô√©radiquer du legacy,\n  √ätre dans le p√©rim√®tre dont l‚Äôinfra est le code owner,\n  Et √™tre ‚Äúmorcelable‚Äù en sous-p√©rim√®tres, un pour chaque √©quipe.\n  Enfin, l‚Äôavanc√©e du sujet doit √™tre mesurable.\n\n\nPour l‚Äô√©dition de f√©vrier 2023, nous avons donc ‚Äújou√©‚Äù avec la migration Ansible en cours de r√©alisation dans l‚Äôune de nos √©quipes de SysAdmins. 45 r√¥les Ansible restaient √† migrer vers notre nouveau template Ansible, utilis√© pour d√©ployer nos machines on-prem : il y a du travail pour tout le monde, c‚Äôest parti !\n\nEt finalement, est-ce que √ßa marche ?\n\nApr√®s trois √©ditions, il nous semble n√©cessaire de prendre un peu de recul pour analyser si ces journ√©es portent leur fruit. Les √©quipes sont ravies de travailler ensemble, certes, mais l‚Äôobjectif principal est-il rempli ? Les jeudis du fun permettent-ils de venir √† bout de sujets legacy ?\n\nLa premi√®re √©dition a fortement contribu√© √† √©radiquer du legacy : nous avons mis √† jour la quasi-totalit√© des layers Terraform, nous avons merg√© ou ferm√© l‚Äôenti√®ret√© des PRs, et nous avons initi√© des travaux de rework. Cependant, nous n‚Äôavions pas d√©fini d‚Äôindicateurs de r√©ussite assez fiables lors de cette premi√®re it√©ration pour quantifier r√©ellement le travail accompli. Si toute la Verticale partage le sentiment d‚Äôavoir avanc√© lors de cette journ√©e, nous ne savons pas le mesurer finement.\n\n\nCapture d‚Äô√©cran du repo sysadmin/terraform au cours de la 1√®re √©dition du ‚ÄúJeudi du fun‚Äù\n\n\nPour pallier cette difficult√©, nous avions d√©fini un indicateur de suivi tr√®s simple pour la seconde √©dition du jeudi du fun : nombre de secrets √† traiter / nombre de secrets trait√©s. Ainsi, nous savons que, lors de cette (difficile) journ√©e, nous avons trait√© environ un quart du p√©rim√®tre.\n\nAu lancement de la 3√®me √©dition du jeudi du fun, nous avions 45 r√¥les √† migrer vers notre nouveau template Ansible. √Ä l‚Äôissue de cette journ√©e, l‚Äô√©quipe responsable du sujet n‚Äôen avait plus que 10 √† traiter. La mutualisation de nos forces a port√© ses fruits !\n\nInsuffisants lors de la premi√®re √©dition, les indicateurs de suivi mis en place dans les √©ditions suivantes sont cruciaux pour √©valuer le ROI de ces journ√©es de travail ‚Äúparticuli√®res‚Äù.\n\nLes coulisses du jeudi du fun\n\nLes jeudis du fun sont organis√©s par deux personnes au sein de la verticale infra. Si les s√©ances de pr√©paration de cette journ√©e (qui d√©butent environ 3 semaines avant la tenue de l‚Äô√©v√©nement) sont source de beaucoup de rires, il n‚Äôemp√™che qu‚Äôelles doivent √©galement r√©pondre √† certaines probl√©matiques.\n\nS‚Äôadapter aux habitudes de travail de chacun\n\nEn premier lieu, nous devons organiser une journ√©e √† laquelle tous les membres de nos √©quipes puissent prendre part, qu‚Äôils soient au bureau ou en t√©l√©travail. Tous les moments de la journ√©e doivent tenir compte de cet √©l√©ment, qu‚Äôil s‚Äôagisse des phases de travail en petits groupes, des sessions en pl√©ni√®re (25 personnes) comme le lancement de la journ√©e, la remise des prix ou les diff√©rents jeux qui ponctuent ces jeudis.\n\nLes phases de travail en √©quipe sont les plus simples √† g√©rer : nos √©quipes ont d√©j√† l‚Äôhabitude au quotidien de travailler avec des coll√®gues √† distance. Tout le monde se connecte sur une room de visioconf√©rence, et le tour est jou√©.\n\n\nTeam mixte pr√©sentiel / distanciel lors du 1er ‚Äújeudi du fun‚Äù\n\n\nLes moments en pl√©ni√®re sont en revanche plus d√©licats √† g√©rer, car le brouhaha d‚Äôune vingtaine de personnes rassembl√©es dans une m√™me pi√®ce reste difficilement audible pour les personnes √† distance. Un prochain challenge pourrait √™tre d‚Äôorganiser un jeudi du fun 100% distanciel.\n\nIl est √©galement n√©cessaire de tenir compte de la fa√ßon de travailler de chacun : si certaines personnes sont capables de travailler en faisant fi du bruit d‚Äôun open space, d‚Äôautres ont besoin de plus de calme. √Ä chaque √©dition, nous tentons d‚Äôorganiser le jeudi du fun sous diff√©rentes formes, pour tenir compte des besoins de chacun, mais nous n‚Äôavons pas encore trouv√© la solution id√©ale.\n\nLors de la premi√®re √©dition, nous √©tions tous rassembl√©s dans le m√™me open space, sans dispositif particulier pour les personnes ayant besoin d‚Äôun environnement silencieux, et cette journ√©e leur a √©t√© difficile √† supporter. De nombreuses autres √©quipes de Bedrock avec qui nous partageons d‚Äôhabitude cet open space √©taient en d√©placement ce jour-l√†, ce qui a n√©anmoins permis de limiter nos nuisances sonores √† notre seule verticale.\n\nPour la seconde √©dition, nous avions r√©serv√© un open space dans les locaux de Bedrock pour ne pas prendre le risque de d√©ranger les autres √©quipes : l‚Äôambiance y a √©t√© d‚Äôautant plus conviviale mais n‚Äôa apport√© aucun mieux aux personnes ayant besoin de tranquillit√© pour travailler.\n\nLors de notre derni√®re √©dition, nous avons tent√© une approche hybride : la plupart des √©quipes √©taient rassembl√©es dans un m√™me open space, et pour les personnes ayant besoin de s‚Äôisoler, une salle de r√©union avait √©t√© r√©serv√©e pour l‚Äôoccasion. Il semble que cette organisation a apport√© un mieux pour les personnes souffrant du bruit avec un √©cueil cependant : elles √©taient isol√©es des autres √©quipes tout au long de la journ√©e, et le jeudi du fun repose (aussi) sur l‚Äô√©mulation collective‚Ä¶\n\nLes autres limites de l‚Äôorganisation\n\nAu fil des √©ditions, nous avons rencontr√©, en tant qu‚Äôorganisatrices, deux autres limites.\n\nLa premi√®re touche au choix du sujet. Si la d√©finition de la th√©matique de la premi√®re journ√©e a √©t√© √©vidente car le repository sysadmin/terraform est source de complaintes quotidiennes, tr√®s vite, nous avons eu besoin d‚Äôaide pour d√©finir les sujets des √©ditions suivantes.  \nEn effet, il est difficile pour nous d‚Äôappr√©hender un sujet dans sa globalit√© : y aura-t‚Äôil du travail pour chaque √©quipe ? Le sujet est-il accessible pour tous nos profils, sans mont√©e en comp√©tence pr√©alable ? Quelles sont concr√®tement les actions √† conduire pour venir √† bout d‚Äôun sujet ? Pour pallier √† ce probl√®me, nous avons r√©alis√© un tour de passe-passe : l‚Äô√©quipe qui remporte le jeudi du fun gagne le droit de d√©finir avec nous le sujet de l‚Äô√©dition suivante. Et √ßa fonctionne ! Les gagnants participent avec plaisir au choix du prochain sujet de torture de fun !\n\nLa seconde limite concerne la r√©currence de l‚Äô√©v√©nement. Initialement, nous avions pr√©vu d‚Äôorganiser un jeudi du fun par mois, pour venir √† bout rapidement de nos sujets legacy. Apr√®s les deux premi√®res √©ditions (organis√©es en octobre et novembre 2022), nous nous sommes aper√ßues que nous perdrions le fun de cette journ√©e si elle revenait trop fr√©quemment. Pour que cet √©v√©nement reste une journ√©e de travail particuli√®re √† laquelle les personnes participent avec plaisir, nous avons fait le choix d‚Äôopter pour un format trimestriel.\n\nNext steps et prochains d√©fis\n\nD‚Äôautres am√©liorations restent √† apporter, notamment autour de la gestion du reste √† faire. Comment finir correctement les travaux initi√©s dans cette journ√©e, afin de ne pas cr√©er de nouvelles user stories legacy ? Ce point est tout aussi important que celui sur le travail accompli au cours de ces journ√©es. Entamer un rework et le laisser en chantier g√©n√®re au moins autant de frustration que le manque de temps pour traiter du legacy.\n\nN√©anmoins, apr√®s trois √©ditions du jeudi du fun, il nous semblait important de partager notre exp√©rience, ne serait-ce que pour convaincre des √©quipes de devs de Bedrock de venir jouer avec nous lors d‚Äôune prochaine √©dition !\n\n\nLes participants du Jeudi du fun\n\n\n\n\nPour vous donner un aper√ßu de comment se d√©roulent ces fameux jeudis, voici grosso modo le programme d‚Äôune journ√©e :\n\n\n  \n    ‚è∞ 9h00 Petit d√©jeuner convivial (car c‚Äôest tr√®s important de commencer une telle journ√©e en prenant des forces)\n  \n  \n    ‚è∞ 9h30 D√©but officiel de la journ√©e : on se retrouve en pl√©ni√®re, dans une grande salle de r√©union, avec tous les participants et on (r√©)explique le contexte de la journ√©e ainsi que le programme. \nOn commence avec un petit jeu (5 minutes maximum) qui sert √† deviner le sujet du jour. Les sujets sont toujours gard√©s secrets jusqu‚Äôau lancement de la journ√©e, ce qui donne lieu √† toutes sortes d‚Äôhypoth√®ses les jours qui pr√©c√®dent (‚ÄúOui, oui, bien s√ªr on va recoder toute notre plateforme dans un autre langage jeudi‚Äù).On fait monter la pression !  \nL‚Äôobjectif de ce premier jeu est d‚Äô√©nergiser un maximum nos coll√®gues et de leur permettre de commencer √† se projeter sur ce qu‚Äôils vont pouvoir y faire. Le jeu change √† chaque fois, pour garder un effet de surprise. \nEnsuite, vient le temps de r√©v√©ler la constitution des √©quipes qui changent elles aussi √† chaque √©dition afin de permettre √† chaque personne de c√¥toyer de nouveaux coll√®gues.\n  \n  \n    ‚è∞ 10h00 Les √©quipes partent travailler sur le sujet du jour, √† leurs postes de travail\n  \n  \n    ‚è∞ 12h30 - 13h30 D√©jeuner\n  \n\n\n\n\n\n  \n    ‚è∞ 13h30 Jeu de reprise (facultatif) : on se retrouve autour d‚Äôun blind test ou un gartic phone, histoire de passer un bon moment et de se remettre en jambe pour l‚Äôapr√®s-midi. C‚Äôest un court moment de team building qui est tr√®s appr√©ci√© la plupart du temps (sauf lorsque les √©quipes ne veulent pas perdre un minute pour venir √† bout de leur objectif !)\n  \n  \n    ‚è∞ 14h00 Les √©quipes reprennent le travail initi√© le matin et essayent de finir un maximum de choses\n  \n  \n    ‚è∞ 17h30 On se retrouve en pl√©ni√®re pour le d√©brief de la journ√©e : on fait le point sur le travail accompli, le d√©compte des points gagn√©s par chaque √©quipe et on fait le fameux podium ainsi que la remise des prix. \nOn r√©cup√®re √† chaud les premiers retours des participants.\n  \n  \n    ‚è∞ 18h00 Le verre de l‚Äôamiti√©\n  \n\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #19",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2023/03/13/bedrock-dev-facts-19.html",
    "date"     : "March 13, 2023",
    "excerpt"  : "La fin de l‚Äôhiver approche, il est temps de faire un bilan ! Quelles b√™tises le froid aura-t-il apport√©es parmi les devs ? ‚ùÑÔ∏è\n\nLa confiance ü§û\n\n\n\nMieux qu‚Äôun readme\n\n\n  Quand le mec t‚Äôexplique la solution, et finit par :\n\n  ‚ÄúEnfin √ßa c‚Äôest si mon c...",
  "content"  : "La fin de l‚Äôhiver approche, il est temps de faire un bilan ! Quelles b√™tises le froid aura-t-il apport√©es parmi les devs ? ‚ùÑÔ∏è\n\nLa confiance ü§û\n\n\n\nMieux qu‚Äôun readme\n\n\n  Quand le mec t‚Äôexplique la solution, et finit par :\n\n  ‚ÄúEnfin √ßa c‚Äôest si mon code a bien continu√© d‚Äô√™tre copi√© coll√© partout‚Äù\n\n\nUne √©thique, moi ?\n\n\n  Moi je peux mettre du code d√©gueulasse un peu partout, c‚Äôest pas un probl√®me !\n\n\nLe Socrate des temps modernes\n\n\n  La vie est un Spike\n\n\nOn apprend de ses erreurs\n\n\n  Set up a reminder ‚Äú@myself ne jamais dire ‚Äòje finis aujourd‚Äôhui‚Äô‚Äù in this channel at 9h45AM every day.\n\n\nEn tout bien tout honneur ‚ù§Ô∏è\n\n\n  Ah bah go, mets-moi en dur si tu veux.\n\n\nGIT 101\n\n\n  J‚Äôen ai connu certains, √† chaque fois qu‚Äôils avaient un conflit sur leur branche, ils supprimaient le repo avant de le re-cloner\n\n\nTen seconds before disaster\n\n\n  Le cache, c‚Äôest nul !\n\n\nshoeHasHole: boolean üëü\n\n\n  A : ‚Äòtin j‚Äôai un trou dans ma chaussure\n\n  B : Tu es s√ªr que c‚Äôest pas un false ?\n\n\nOn a tous un env de test. Certains ont aussi un env de prod.\n\n\n\nPeur de rien, sauf d‚Äôune chose‚Ä¶\n\n\n  A : √áa finit en devs qui se reconvertissent Boulanger √ßa.\n\n  B : Certes, mais l‚Äôinverse est vrai aussi, il arrive que des Boulangers se reconvertissent apr√®s √™tre devenus allergiques √† la Farine.\n\n  A : C‚Äôest pour √ßa que je ne me reconvertirai pas en Barman‚Ä¶ trop peur‚Ä¶\n\n\nLa confiance, second √©pisode ü§û\n\n\n  C‚Äôest pas n‚Äôimporte quoi, juste un peu yolo !\n\n\nComme de l‚Äôeau de roche trouble !\n\n\n  Franchement, je trouve √ßa clair ! Mais je comprends pas..\n\n\nDe rares g√©nies üí°\n\n\n  On est peut-√™tre des lumi√®res, mais √ßa ne veut pas dire qu‚Äôon est tous allum√©s !\n\n\nCassage de prod dans 3‚Ä¶ 2‚Ä¶ 1‚Ä¶\n\n\n  J‚Äôle sens bien l√†. J‚Äôle sens bien bien bien.\n\n\nIt‚Äôs not a bug, it‚Äôs a feature\n\n\n  J‚Äôai v√©rifi√©, le bug marchait bien.\n\n\nToujours lire les petites lignes üîé\n\n\n  Tout* marche du coup !\n\n  (*pour l‚Äôinstant)\n\n\nMiaou üê±üìà\n\n\n  A : Je th√©orise que le chat ne miaule devant la porte que pour savoir s‚Äôil pourrait passer quand il aura envie.\n\n  B : Ouah ton chat il fait du monitoring de la porte !\n\n\nFacile comme tout !\n\n\n  TKT ! tu mets ton JSON dans le yaml et √ßa ira !\n\n\nUne grande histoire d‚Äôamour, √©pisode 1\n\n\n  Moi, j‚Äôadore le JSON\n\n\nUne grande histoire d‚Äôamour, √©pisode 2\n\n\n  Le mail et le DNS c‚Äôest ma grande passion\n\n\nPartir comme un roi üëë\n\n\n\nLes grandes questions de la vie ü•ê\n\n\n  D‚Äôailleurs c‚Äôest LinkedIn ou pain au linked ?\n\n\nL‚Äôhumour pour les nuls\n\n\n  A : Pouffe de rire\n\n  B : Tout va bien ?\n\n  A : D√©sol√©, je viens de relire ma vanne\n\n\nLes progr√®s de l‚ÄôIA ü§ñ\n\n\n  A : Alors B, c‚Äôest quoi le format de date php de la constante de format ‚Äòc‚Äô ?\n\n  B : Tu m‚Äôas pris pour chatGPT ou quoi ?\n\n\nUn stagiaire en d√©tresse\n\n\n  TLDR: √Ä l‚Äôaide svp\n\n\nLa s√©curit√© pour les nuls\n\n\n  Brian is in the Keychain.\n\n\nPromis dans le contexte c‚Äôest vrai\n\n\n  On doit afficher des ronds, alors c‚Äôest mieux s‚Äôils nous envoient des carr√©s.\n\n\nüò≥\n\n\n  (Au pire, si on a la main sur une regexp, c‚Äôest d√©j√† plus qu‚Äôil n‚Äôen faut pour me faire r√™ver)\n\n\nLa s√©lection naturelle\n\n\n  Je suis d‚Äôaccord que l√† il y a un bug, mais c‚Äôest un bug parce que je suis con !\n\n\nüçµ\n\n\n  Je suis en train de me rappeler de mon weekend, et spoiler mettre du rhum dans son th√© ce n‚Äôest pas une bonne id√©e.\n\n\nError : Task completed successfully\n\n\n\nLa confiance, 3.0 ü§û\n\n\n  Coucou, aujourd‚Äôhui, je p√®te la reco (en prod), mais c‚Äôest sous contr√¥le.\n\n\nUn instant de r√©alisme\n\n\n  Personnellement, je sais pas ce que je fous en d√©veloppeur !\n\n\nThomas the train üöÜ\n\n\n\nTurlututu chapeau pointu !\n\n\n  On aurait d√ª dire c‚Äôest ‚Äúchapeau perch√©‚Äù.\n\n\nMieux qu‚Äôun rappel automatique ü§Ø\n\n\n  A : Du coup, tu as envoy√© un mail ?\n\n  B : Pas encore non ! J‚Äôattendais d‚Äôy penser !\n\n\n"
} ,
  
  {
    "title"    : "Why is Transit Gateway service not right for us?",
    "category" : "",
    "tags"     : " on-premise, cloud, aws, network",
    "url"      : "/2023/03/02/aws_transit_gateway.html",
    "date"     : "March 2, 2023",
    "excerpt"  : "Managing the network of many interconnected AWS accounts can quickly lead to having a messy network architecture.\nTransit Gateway (TGW) service seems to be the way out of this. So how do you know if TGW is right for you?\n\nThis blog post will intro...",
  "content"  : "Managing the network of many interconnected AWS accounts can quickly lead to having a messy network architecture.\nTransit Gateway (TGW) service seems to be the way out of this. So how do you know if TGW is right for you?\n\nThis blog post will introduce how the service works and explain why we chose not to carry on with our migration to AWS Transit Gateway.\n\nTransit Gateway‚Äôs backstory\n\nTransit Gateway is a network transit hub that connects multiple VPCs and On-Premises sites to allows control traffic between them.\nIt was created to provide a new approach of network implementation on AWS and to make network administration smoother.\n\nVPC peering is a point-to-point connection between 2 VPCs.\nIt is a great example of complex network management because it adds a new topology to the network architecture.\nOn this diagram you can see an example of VPC peering usage. It‚Äôs not that messy yet but at scale it will be.\n\n\n\nBy acting as a ‚Äúcloud router‚Äù, TGW centralizes network connections and takes control of packet forwarding between VPCs.\n\n\n\nVPC peerings are not required anymore, we go back to a simpler star network topology thanks to Transit Gateway which really does address the complexity and restrictions of VPC peerings.\nAt that point, TGW seems to be the perfect answer for a simpler network architecture.\n\nWhat about TGW at Bedrock?\n\nWe operate more than 20 different AWS accounts for our customers‚Äô platforms. Each account has a VPC with at least 3 private and 3 public subnets. We also manage AWS accounts for internal tools like ECR repositories, monitoring tools and shared s3 buckets. We configured Site-to-Site VPNs from On-Premises infrastructure to all the VPCs in these accounts.\n\nFrom the creation of new AWS accounts to deploying the tenants‚Äô platform, onboarding a new customer requires a lot of work and time.\n\nConfiguring VPCs Site-to-Site VPN is one of the steps that requires a lot of work. This is why we were interested in Transit Gateway at first.\n\nProof of concept\n\nWe created a production like Proof of Concept infrastructure using three AWS accounts, two different regions, multiple VPCs and a single Site-to-Site VPN from TGW to On-Premises firewall.\n\nHow did we test TGW?\n\nWe started by trying to split routing domains.\nCentralizing network connections also means (with correct ACLs or Security Groups) that VPCs can reach all other VPCs. We want to control that.\nTransit Gateway attachments read their routes in the TGW route table they are associated to. This is how we manage routing domains.\nWe create a Transit Gateway routing table and create routes for target networks.\nTGW attachments are able to propagate routes in a route table if we want to. But because of routing domains, we can‚Äôt use that option and we have to add routes manually (attachments only read routes in the route table).\n\nThen we tested Transit Gateway peering.\nTGW is a regional service, this means that we need to have a TGW for each active AWS region. We use TGW peering to interconnect them.\nWe expected to have some way to propagate routes dynamically in the Transit Gateway peering route table. But it is not possible.\n\nThe last thing we tested is migrating from VPC Site-to-Site VPN to TGW VPN.\nBecause of the amount of VPC Site-to-Site VPN we have, it was important for us to know if we could get a minimal down time on On-Premises to VPC connections when migrating to the Transit Gateway VPN.\nThis process requires a lot of time because routes have to be deleted and created manually on each side.\n\nEven if we noticed some pain points, tests went well. So we decided to initiate the migration to the Transit Gateway service.\n\nWhy did we choose to rollback?\n\nEverything was okay at first, we successfully migrated two VPC Site-to-Site VPN to our Transit Gateway VPN.\n\nBut then previous pain points became barriers:\n\n  creating and managing routing domains is possible, but makes it impossible to use dynamic route propagation\n  there is not option to propagate routes in VPC route table, they all have to be created manually\n  data transfer cost is too high (and multiplied by the number of region on which you deployed TGW if your packets go through all these regions)\n  migrating to Transit Gateway requires a planned maintenance because there is a network downtime\n\n\nWe took some time to talk about what to do next and concluded that migrating to Transit Gateway will just move the complexity of configuring VPC Site-to-Site VPNs to configuring TGW attachments and routes.\n\nAWS support did not suggest enough solutions to the problems we faced, so we decided to rollback to VPC Site-to-Site VPNs.\n"
} ,
  
  {
    "title"    : "Projet XState",
    "category" : "",
    "tags"     : " xstate, lyonjs, meetup, react, javascript, conference",
    "url"      : "/2023/02/08/projet-xstate.html",
    "date"     : "February 8, 2023",
    "excerpt"  : "Dans une application frontend moderne, la gestion d‚Äô√©tat est un √©l√©ment central de son bon fonctionnement. Malgr√© les nombreuses librairies disponibles (Redux, MobX, Recoil‚Ä¶), cette tache reste complexe √† r√©aliser et il est facile de perdre le con...",
  "content"  : "Dans une application frontend moderne, la gestion d‚Äô√©tat est un √©l√©ment central de son bon fonctionnement. Malgr√© les nombreuses librairies disponibles (Redux, MobX, Recoil‚Ä¶), cette tache reste complexe √† r√©aliser et il est facile de perdre le contr√¥le.\n\nDans l‚Äôobjectif de rester maitre de son application, je vous propose de d√©couvrir XState, une librairie reposant sur le concept de machine √† √©tats. Si l‚Äôoutil ne fait pas tout, le concept de machine √† √©tat aide grandement √† concevoir une application r√©siliente.\n\nPour pr√©senter au mieux les concepts, la th√©orie sera suivie de pratique au travers d‚Äôun live coding.\n"
} ,
  
  {
    "title"    : "The time I tried to build a Second Brain #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/the-time-i-tried-to-build-a-second-brain",
    "date"     : "January 27, 2023",
    "excerpt"  : "The time I tried to build a Second Brain\nPr√©sent√© par Sylvain ZOCCARATO.\n",
  "content"  : "The time I tried to build a Second Brain\nPr√©sent√© par Sylvain ZOCCARATO.\n"
} ,
  
  {
    "title"    : "Projet XState #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/projet-xstate",
    "date"     : "January 27, 2023",
    "excerpt"  : "Projet XState.\nPr√©sent√© par Maxime BLANC.\n",
  "content"  : "Projet XState.\nPr√©sent√© par Maxime BLANC.\n"
} ,
  
  {
    "title"    : "Qu‚Äôest-ce que l‚Äôoignon dans le Web ? #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/loignon-dans-le-web",
    "date"     : "January 27, 2023",
    "excerpt"  : "Qu‚Äôest-ce que l‚Äôoignon dans le Web ?\nPr√©sent√© par Etienne Doyon.\n",
  "content"  : "Qu‚Äôest-ce que l‚Äôoignon dans le Web ?\nPr√©sent√© par Etienne Doyon.\n"
} ,
  
  {
    "title"    : "La philo et les livres : mes compagnons de route pour les d√©fis sportifs #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-philo-et-les-livres",
    "date"     : "January 27, 2023",
    "excerpt"  : "La philo et les livres : mes compagnons de route pour les d√©fis sportifs\nPr√©sent√© par Chiara PETTINELLI.\n",
  "content"  : "La philo et les livres : mes compagnons de route pour les d√©fis sportifs\nPr√©sent√© par Chiara PETTINELLI.\n"
} ,
  
  {
    "title"    : "La culture Hip-Hop #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-culture-hip-hop",
    "date"     : "January 27, 2023",
    "excerpt"  : "La culture Hip-Hop\nPr√©sent√© par Pascal HALTER.\n",
  "content"  : "La culture Hip-Hop\nPr√©sent√© par Pascal HALTER.\n"
} ,
  
  {
    "title"    : "La photographie de paysages nocturnes #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/photographie-de-paysages-nocturnes",
    "date"     : "January 27, 2023",
    "excerpt"  : "La photographie de paysages nocturnes\nPr√©sent√© par Camille NIEL.\n",
  "content"  : "La photographie de paysages nocturnes\nPr√©sent√© par Camille NIEL.\n"
} ,
  
  {
    "title"    : "Le festival de cannes de sa naissance √† aujourd‚Äôhui #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/festival-de-cannes-de-sa-naissance-a-aujourdhui",
    "date"     : "January 27, 2023",
    "excerpt"  : "Le festival de cannes de sa naissance √† aujourd‚Äôhui\nPr√©sent√© par Maxime LE MOAL.\n",
  "content"  : "Le festival de cannes de sa naissance √† aujourd‚Äôhui\nPr√©sent√© par Maxime LE MOAL.\n"
} ,
  
  {
    "title"    : "A journey into connected TVs industrialisation process, Part 1",
    "category" : "",
    "tags"     : " ",
    "url"      : "/2023/01/10/bedrock-app-launcher.html",
    "date"     : "January 10, 2023",
    "excerpt"  : "At Bedrock, we build and run streaming applications on a wide variety of OTT devices (more than 60 different ecosystems). While testing and experimenting is easy on web and mobile devices, even for non-developers, it‚Äôs not as easy for Connected TV...",
  "content"  : "At Bedrock, we build and run streaming applications on a wide variety of OTT devices (more than 60 different ecosystems). While testing and experimenting is easy on web and mobile devices, even for non-developers, it‚Äôs not as easy for Connected TV (CTV). In this article, you‚Äôll discover how all of our employees can now access testing and pre-release environments on TV devices, with ease and without any technical knowledge.\n\nBedrock TvJS Project\n\nHow does it work ?\n\nTo address the growing number of CTVs vendors in the market, we have a one-and-only monorepo project named ‚ÄúTVJS‚Äù. It is a React application which we can deploy almost everywhere almost anywhere with the same code, UI and UX. The magic part? There isn‚Äôt much manufacturer-specific code in that application, most of those particularities are handled by our homemade JS library named PELO (Platform Easy Life Officer). For non-French readers, ‚Äúp√©lo‚Äù is a Lyon/Grenoble city slang to designate ‚Äúsomeone‚Äù.\n\n\n\nIn a few words, PELO is a set of libraries showing a unified front API for TV developers, so they don‚Äôt have to keep in mind every TV specific details and custom APIs (like lifecycle, keyboard, storage handling, and more). PELO also provides several CLI tools allowing the use of proprietary manufacturer SDKs, with a common shared API.\n\nThere are at least two ways to deploy a TV application:\n\n  A fully packaged solution, where all application files and resources are stored on the TV. Everytime you want to update it, application you have to go through the manufacturer QA process. Doing so, you can develop either a web application that will run through the TV‚Äôs Web Engine, or a native TV application.\n  The hosted solution, where the TV packaged application only redirects to a web application that you are responsible for. It grants much more flexibility, and delivery speed, as deployment and propagation of a new version are almost instantaneous.\n\n\nWe chose the second way as we are addressing a big number of devices and need all the flexibility we can have for deployments ‚Äì and, sadly, for rollbacks too. Therefore, we host and deploy our CTV applications like any other website and we control the TV Browser Engine to navigate to specific domain names.\n\nThree teams are working on this project, on the same repository:\n\n  a team dedicated to Core features (like catalog, user lifecycle)\n  a team dedicated to Player features (video playback and advertising)\n  and a team supporting legacy devices\n\n\nDeveloper teams are supported by a QA team. It is responsible for functional quality assurance on Pull Requests and pre-releases. Quality assurance designates any processes to ensure a service meets its quality requirements in terms of experience, stability, ‚Ä¶\n\nDevelop &amp;amp; release process\n\nWe do our maximum to ensure the best quality of service and experience of what we deliver to our customers and their end-users. We have a strong culture of automated testing &amp;amp; tech reviewing which allows us to deliver almost without a sweat‚Ä¶ Still, at our scale, missing a bug means a bad experience for thousands or millions of people! And that‚Äôs something we won‚Äôt accept without a fight!\n\n\n  One of Bedrock‚Äôs Values is: ROCK-SOLID, ALWAYS\n\n\nAs a consequence, we also have dedicated QA teams testing our work for a subset of device models and versions, before it is being merged to the main codebase, and before going to production as part of a release. They are doing so by connecting TVs to specific environments that are deployed on-demand: previews and staging.\n\nLet‚Äôs show off a little bit: at the beginning of 2022, thanks to the TVJS project, we were able to deploy production code to 7 manufacturers, and 38 device versions, meaning 266 combinations to check before launching a release into production! And these numbers are ever increasing!\n\nMy wish: make testing environments easily accessible\n\nWe love showing-off a bit over the applications we deliver on such a huge number of device models, but that doesn‚Äôt go with ease nor without pain.\n\nTesting a specific environment on a device was not possible for non-project members (other teams, support, business &amp;amp; product teams, managers ‚Ä¶). Starting a preview or a staging application requires a deep understanding of the project, proprietary SDKs (even with our PELO CLI), shell, Git commands and advanced knowledge of how devices work in Developer Mode. This was a major issue: it causes interruptions for developers, slows delivery down, reduces our Time To Market.\n\nQA teams assigned to the project know its basics, they can use PELO CLI and proprietary SDKs, but cannot debug issues they may encounter with such tools: they have to ask developers to take actions for them (as this is not their core job). Using those tools is also time-consuming and time is of the essence when running quality checks while preparing a release.\n\nMany teams also want to start environments by themselves, to test their own developments on back-end services, to investigate when a customer creates a support ticket, ‚Ä¶\nThe most important of them are Video teams, responsible for video encoding, transcoding and packaging: they are constantly testing new streams and features, and need a way to test their content by themselves, without asking around for a TVJS developer.\n\nOur answer: The Launcher App !\n\nWhat does it do ?\n\nI‚Äôve developed a TV application to quickly and efficiently start a specific environment. Using the TV remote, people can select the wanted environment and be redirected to it instantly, having the app like they would with the specific app installed.\n\nTyping long texts is painful for TV users. So, when selecting the preview environment, it shows another set of options where users can input a specific PR number. A background process will ask our Github if it knows the PR number, if it is deployed on the selected customer/manufacturer and will pre-fill the branch name. If not specified, it will default back to our master preview that is updated whenever we merge code to the master branch.\n\n\n\nTechnical Architecture\n\nThe launcher is part of the TVJS monorepo, developed using React and re-using modules and packages for UI and Navigation allowing it to have minimum maintenance cost.\n\nFor the first iterations of development of the launcher, I hosted it on AWS Amplify, but the Core team quickly integrated it back to a regular production deployment process we have at Bedrock.\n\nAn automatic process builds the javascript bundle and assets and sends everything to AWS S3. The launcher will then be served through Fastly CDN. We build and deploy a unique launcher per compatible manufacturer on their own domain names (as-of-writing, Samsung Tizen, LG webOS and Hisense). For security reasons, those Fastly services are only accessible from our office networks.\n\n\n\nUnreliability of launcher app installation\n\nI‚Äôm proud of this launcher and it is already saving loads of time for our QA teams ! They love it, as it helps them focus on their primary role: ensuring service &amp;amp; experience quality. Still, installing the launcher application on every device in our office is a huge amount of work! And, unfortunately, not a persistent one.\n\nTo develop and test apps on live devices, we need to set them in ‚ÄúDeveloper Mode‚Äù. And each manufacturer has its own way, more or less time-consuming. Worse, whenever Developer Mode expires, all applications installed during this time are uninstalled from the device! Which means we have to install the launcher again after a brief period of time.\n\n  Tizen Developer Mode\n  LG webOS Developer Mode\n\n\nThat period of time varies. For Samsung Tizen, we‚Äôre not absolutely sure, but it‚Äôs almost a month. For LG webOS, it is 50 hours if you don‚Äôt extend the Developer Mode or if you connect another TV with the same Developer Account.\n\nSpecifically for LG, I did set up a CRON that automatically extends the Developer Mode, but sometimes it is being disconnected without reason‚Ä¶ Or a mishandling by team members can cause the CRON to fail.\n\n\n\n\n\nTherefore, we aren‚Äôt 100% sure the launcher application will be up and ready on all the office devices when work begins in the morning, which means developers will have to manually re-install the launcher when asked by another Bedrock employee. It generates frustration for both QA and developers as they are wasting precious time to re-install the launcher.\n\nDon‚Äôt worry though, I already have a couple of ideas to ensure the installation becomes reliable! I‚Äôll talk more about these it in a future article.\n\nConclusion\n\nAny Bedrock employee can now start an office CTV, use the launcher app, select customer and environment, hit Let‚Äôs Go and access the environment they need to work!\n\nWhat we started to measure, and hopefully we‚Äôll have more refined metrics over the next months, is the time QA teams are gaining per day. They needed an average of 15 minutes to start up a TV, set up the Developer Mode, and install the wanted app through CLI. They are validating 5 PRs per day, on 2 different devices at minimum, they almost gain one hour per day. That means our Time To Market is faster, and our QA teams have more time to do exploratory testing as well as refining their tests and writing more automated tests. Something that is not as measurable as time, is the enhanced peace of mind for them to go to work every morning knowing they have a tool designed for them to focus on their core work.\n\nThis has improved the QA team overall velocity! And it makes the whole project more accessible for any employee. However, there is still room for improvement regarding launcher deployment and stability over time, and this is something I will cover in our next article.\n\nI hope you liked this article and it helped you if you‚Äôre trying to achieve something similar!\n"
} ,
  
  {
    "title"    : "How Micro-Services changed our caching architecture",
    "category" : "",
    "tags"     : " on-premise, cloud, cdn, varnish, aws, cloud, fastly, varnish-operator, cloudfront, alb",
    "url"      : "/2022/12/23/varnish-operator.html",
    "date"     : "December 23, 2022",
    "excerpt"  : "At Bedrock we use Cloudfront or Fastly for two different reason. To protect our applications from potential Distributed Denial of Service Attack. And to provide a layer of cache in front of our applications. No need to go down to the app for an ea...",
  "content"  : "At Bedrock we use Cloudfront or Fastly for two different reason. To protect our applications from potential Distributed Denial of Service Attack. And to provide a layer of cache in front of our applications. No need to go down to the app for an easily cacheable response.\n\n\n\n\n\nAt least that is what we thought in 2018 when we were migrating from on premise to the Cloud.\n\nAt that time we had a Varnish instance caching everything at the border  of our on premise infrastructure. All the applications were running either on virtual machines or on bare metal servers. Those applications were mostly called by the end-user‚Äôs browser. Whenever an application called another application it did it through Varnish.\n\nThis is ideal if applications are mostly called from the outside world. The Varnish instance caches all cacheable content, and it does not cost too much time as it was in the same Data Center.\n\n\n\n\nIn 2023, we think otherwise. We have now a KOps managed Kubernetes cluster running on EC2 spot instances in private subnets at AWS. As we migrated to the cloud we also embarked on the journey of splitting monolith into smaller more manageable microservices.\n\nWith less monoliths the Bedrock product is more resilient and easier to scale but it changes the topologies of network calls. Before there were far more calls coming from the internet from end-users browsers. Now with the new architecture coming into place inter-app requests have increased.\n\nOne solution would be to directly call the ingress of the applications, staying inside the cluster but without the benefit of caching as it is handled by the CDN. This would lead to unsustainable increase in CPU usage, and probably very little gain in terms of response time.\n\nA better solution for us would be to have the caching of CDN inside the cluster. This would enable us to have fast response time and little to no increase in CPU usage.\n\nEnter Varnish-Operator\n\nWe tested the project IBM/Varnish-Operator. This project allows us to create Custom Resources for Kubernetes handled by the Varnish-Operator. This object is called a VarnishCluster, the configuration is pretty simple to get started. This enables us to have a caching layer, between the Ingress-Controller and the Application.\n\n\n\n\n\nVarnishCluster also uses Varnish Configuration Language (VCL) which we are pretty familiar with since we use Varnish On-Premise since 2015, and developers use it regularly to configure Fastly distribution.\n\nBy adding cache using VarnishCluster to an application that is not fully cacheable, we almost divided it‚Äôs average response time by two. It is not a surprise as inter api calls used to look like the following graph:\n\n\n\n\nWe changed parameters in the application after adding VarnishCluster so that it calls other app inside the cluster like in the following graph:\n\n\n\n\nA few details\n\nBefore I wrap this up, here are a few details about the implementations.\n\nAs you will be able to read in the Varnish documentation:\n\n  ‚ÄúBy default Varnish will use 100 megabytes of malloc(3) storage for caching objects, if you want to cache more than that, you should look at the -s argument.‚Äù\n\n\nSo if you give many Gigs of memory to your Varnish container it won‚Äôt be attributed to the Varnish process. You can set it with the argument -s storage=malloc,&amp;lt;Number&amp;gt;.\n\nAs we use only Spot nodes that can be terminated by AWS at any moment with only 2 minutes notice, we want to give more resilience to our Varnish Clusters pod as cache is stored in RAM memory.\nYou lose all your cache at each restart of the Varnish Container.\n\nWe configured podAntiAffinity between application pods and VarnishClusters‚Äô to avoid scheduling those pods on the same node and be vulnerable to reclaims.\n\nWe added a podDisruptionBudget to avoid losing all our pods at the same time. We also customized the VCL a bit to make Varnish serve stale content in case our application is unreachable.\n\nWe also added a Prometheus Service Monitor to make sure all Varnish metrics would be scraped by Victoria Metrics.\n\nIn the Future\n\nIn next versions we would like to add the possibility to configure PriorityClass of VarnishClusters pod. PriorityClasses are used to order workloads priority.\nIn a context of scaling and of scarcity of resources, the scheduler will evict pods of lower priority to make room for the pod it is trying to schedule.\n\nFor now our VarnishCluster‚Äôs pods have the PriorityClass by default but it is more critical than any other applications as it holds a cache in its memory.\n\nAlso we do not have logs of Varnish. We would like to be able to stream VarnishLog content into Loki. This would be super useful to debug and to investigate if we ever encounter bugs or unexpected behaviors.\n\nConclusion\n\nAverage response time going down, red bar is when we pushed it in production\n\n\n\nWith the generalization of microservices, Bedrock needed to rethink its architecture to optimize not only for browser to API calls but also for more API to API usage. By adding VarnishCluster in front of our applications and calling them directly from inside the cluster we improved significantly the Bedrock product.\n\nThe Github project is still young and lacks important features, we hope with this article to help draw attention to this project and potential contributors.\n\n\n\n"
} ,
  
  {
    "title"    : "Nos retours sur l&#39;HAProxyConf Paris 2022",
    "category" : "",
    "tags"     : " haproxy, haproxyconf, conference",
    "url"      : "/2022/12/23/haproxyconf-paris-2022.html",
    "date"     : "December 23, 2022",
    "excerpt"  : "Bedrock √©tait pr√©sent lors de la Conf√©rence HAProxy qui se d√©roulait √† Paris en novembre 2022 : en tant que speaker, avec la pr√©sentation de Vincent Gallissot, mais aussi en tant que spectateur. Cet article relate les points forts qui nous ont mar...",
  "content"  : "Bedrock √©tait pr√©sent lors de la Conf√©rence HAProxy qui se d√©roulait √† Paris en novembre 2022 : en tant que speaker, avec la pr√©sentation de Vincent Gallissot, mais aussi en tant que spectateur. Cet article relate les points forts qui nous ont marqu√©s.\n\nLa pr√©sentation de Vincent Gallissot, Lead Cloud Architect chez Bedrock, mettait en valeur l‚Äôusage d‚ÄôHAProxy en tant que brique essentielle de notre infrastructure. Chez Bedrock, nous d√©veloppons et maintenons une plateforme de streaming qui a √©t√© migr√©e dans le Cloud en 2019. Cette pr√©sentation √©tait grandement inspir√©e de l‚Äôarticle intitul√© ‚ÄúScaling Bedrock video delivery to 50 million users‚Äù, dans lequel vous trouverez pl√©thore d‚Äôinformations concernant nos utilisations d‚ÄôHAProxy.\n\n\n\nSommaire\n\n\n  Ce que des millions de requ√™tes par seconde signifient en termes de co√ªt et d‚Äô√©conomie d‚Äô√©nergie\n  Un outil pour les gouverner tous\n  Vous reprendrez bien un peu de p√©taoctets?\n\n\nCe que des millions de requ√™tes par seconde signifient en termes de co√ªt et d‚Äô√©conomie d‚Äô√©nergie.\n\nLa keynote d‚Äôouverture avait pour orateur Willy Tarreau, le Lead Developer d‚ÄôHAProxy.\nAu travers d‚Äôune d√©monstration concr√®te m√©langeant software et hardware, l‚Äôobjectif √©tait de :\n\n  transmettre l‚Äôid√©e qu‚Äôajouter une brique logicielle dans un syst√®me ne le d√©grade pas pour autant, bien au contraire\n  sensibiliser l‚Äôaudience quant √† la consommation d‚Äô√©nergie de nos syst√®mes\n\n\nContexte technique et premi√®res am√©liorations\n\nPour ce premier cas d‚Äô√©tude, Willy Tarreau nous pr√©sente le cas d‚Äôun service de vente en ligne.\n\nLa stack technique est compos√©e de PHP / pgSQL (NodeJS + Symfony) et les images sont stock√©es en base de donn√©es. C‚Äôest cette architecture qui sera mise √† l‚Äô√©preuve lors des tests de charge √† venir.\n\nDans un premier temps, plusieurs am√©liorations (sans HAProxy) sont propos√©es. Il peut s‚Äôagir d‚Äôun simple rappel, voir d‚Äôun pro-tip d‚Äôarchitecture pour les plus novices : Les images en base de donn√©es, c‚Äôest une mauvaise id√©e.\n\nEn les d√©pla√ßant vers un CDN, le syst√®me peut rapidement et simplement doubler ses performances, la base de donn√©es √©tant un goulot d‚Äô√©tranglement. La taille des pages peut √™tre optimis√©e via l‚Äôactivation de l‚Äôoption http ‚Äúgzip‚Äù. Les informations de sessions sont elles aussi enregistr√©es en base de donn√©es. Afin d‚Äôam√©liorer les performances, il est possible d‚Äôajouter du caching via des outils tels que Memcache.\n\nSuite √† cela, une premi√®re am√©lioration d‚Äôarchitecture serait d‚Äôajouter un NLB (Network Load Balancer) en amont du syst√®me qui distribuerait les requ√™tes entrantes vers plusieurs unit√©s de calculs.\n\n\n\nSch√©ma d‚Äôarchitecture, premi√®re version\n\nDans le cas pr√©sent, les requ√™tes entrantes sont distribu√©es de fa√ßon al√©atoire entre les diff√©rentes unit√©s de traitement. Chacun de ces backends se connectant √† la m√™me et unique base de donn√©es.\nLe benchmark ci-dessous (efficacit√©, au sens nombre de requ√™tes trait√©es en fonction du nombre d‚Äôunit√©s de calcul), ne montre pas une croissance lin√©aire. Il s‚Äôagit d‚Äôune courbe tendant vers une pente nulle (voir n√©gative pour les plus grosses architectures).\n\n\n\nGraphique repr√©sentant l‚Äôefficacit√© du syst√®me en fonction du nombre de backends\n\nComment expliquer que cette architecture ne scale pas lin√©airement ?\n\nMalgr√© les am√©liorations apport√©es pour les sessions gr√¢ce au cache, il subsiste encore un probl√®me.\n\nLe NLB est un composant qui ne fait que r√©partir la charge sans tenir compte de l‚Äôhistorique des requ√™tes. En effet, celui-ci va distribuer la charge d‚Äôentr√©e al√©atoirement vers les backends.\nChaque backend re√ßoit des requ√™tes provenant de n‚Äôimporte quel utilisateur impliquant alors un cache-miss tr√®s √©lev√© : l‚Äôutilisateur est rarement trouv√© dans le cache, ce qui g√©n√®re une requ√™te suppl√©mentaire en base de donn√©es et d√©grade les performances en plus de consommer inutilement des ressources.\n\nEt si nous ajoutons HAProxy √† notre syst√®me ?\n\nC‚Äôest ici qu‚Äôentre en jeu HAProxy en rempla√ßant le NLB. Pour cela, pas besoin d‚Äôun foudre de guerre en termes de ressources.\n\nLes tests ont √©t√© effectu√©s sur une machine ARM Breadbee cadenc√©e √† 1 GHz et poss√©dant 64 Mo de RAM. Nous verrons √©galement par la suite qu‚Äôon pourrait m√™me se passer d‚Äôune machine suppl√©mentaire.\n\nLe but d‚ÄôHAProxy est de sp√©cialiser les caches des backends et plus globalement de forcer les sessions utilisateurs vers les m√™mes backends.\n\nPour cela, HAProxy effectue une inspection de la couche 7 du trafic et renvoie toutes les requ√™tes d‚Äôun m√™me utilisateur sur une m√™me machine en r√©duisant ainsi les cache-miss aux seuls cas des nouveaux clients se connectant √† la plateforme. Ainsi, le nombre d‚Äôappels √† la base de donn√©es pour r√©cup√©rer les informations de session est drastiquement r√©duit, la majorit√© d‚Äôentre elles √©tant stock√©es en cache.\n\nAutre fonctionnalit√© de taille : HAProxy limite le nombre de requ√™tes faites en parall√®le sur un m√™me backend, ce qui limite les locks de processus et les temps d‚Äôattente. Ceci a pour cons√©quence directe de r√©duire la consommation CPU.\n\nCes deux am√©liorations permettent √† l‚Äôapplication de scaler de fa√ßon beaucoup plus lin√©aire, tout en r√©duisant les consommations CPU et √©nerg√©tiques inutiles. Globalement, les performances initiales sont largement d√©pass√©es avec deux fois moins de backends.\n\nA partir de quand est-il int√©ressant de franchir le pas ?\n\nMaintenant que les b√©n√©fices d‚ÄôHAProxy ont √©t√© pr√©sent√©s, la prochaine √©tape est de se demander : quand est-ce qu‚Äôon se lance ? La question est consid√©r√©e en termes de performance, mais aussi sous un angle p√©cunier.\nSi HAProxy peut √™tre int√©gr√© sans augmenter les co√ªts du syst√®me, c‚Äôest encore mieux.\n\nAjouter HAProxy dans un syst√®me compos√© d‚Äôun seul backend n‚Äôapporte pas de b√©n√©fice : il n‚Äôy a pas de load-balancing possible. Avec deux backends, si on divise le besoin de processing par deux, nous n‚Äôavons plus qu‚Äôun seul backend et donc pas de load-balancing possible.\nC‚Äôest en fait √† partir de 4 backends que l‚Äôajout d‚Äôun HAProxy en entr√©e devient int√©ressant :\n\n  en retirant 2 serveurs de nos backends en conservant une puissance √©quivalente (cf les tests ci-dessus)\n  et en recyclant un des deux backends retir√©s en h√¥te pour HAProxy\nEn fin de compte, pour une m√™me puissance de traitement, un backend est retir√© ce qui permet de r√©duire les co√ªts de fonctionnement. Ce principe s‚Äôapplique √©galement sur un grand nombre de backends.\n\n\nC‚Äôest l√† que prend tout son sens l‚Äôexpression qui avait √©t√© utilis√©e pour conclure cette keynote : ‚ÄúHAProxy is a free software running on free hardware‚Äù.\n\nChez Bedrock, nous appliquons aussi ces diff√©rentes techniques de Consistent Hashing en entr√©e de notre CDN vid√©o. Nos caches vid√©os sont sp√©cialis√©s et chaque utilisateur est redirig√© vers un unique backend lors de la lecture d‚Äôune vid√©o.\nPour en savoir plus, vous pouvez consulter notre article au sujet du Consistent Hashing.\n\nUn outil pour les gouverner tous\n\nDans notre activit√© en informatique, nous sommes amen√©s √† d√©livrer de plus en plus rapidement des applications, des mises √† jour, etc‚Ä¶ Nous avons donc adopt√© la philosophie DevOps et tout un panel d‚Äôoutils autour de celle-ci afin de s√©curiser, monitorer et automatiser chaque √©tape de nos pipelines de livraison.\n\nLe cas de figure du load balancing est int√©ressant dans ce type d‚Äôorganisation, il est essentiel d‚Äôexposer de nouvelles applications sur les environnements de production mais √©tant donn√© que la ma√Ætrise de cet outil requiert une compr√©hension du r√©seau, la responsabilit√© incombe souvent √† l‚Äô√©quipe Ops de le g√©rer.\n\nVous souhaitez mieux g√©rer votre flotte HAProxy ?\n\nAnjelko Iharos, directeur de l‚Äôing√©nierie √† HAProxy Technologies nous a pr√©sent√© leur nouvel outil d‚Äôautomatisation : HAProxy Fusion Control Plane, packag√© dans la version entreprise de HAProxy.\n\nCelui-ci va amener une nouvelle interface enrichie afin de g√©rer toutes les instances HAProxy et les outils gravitant autour de ces derni√®res.\n\nOn peut citer :\n\n  La possibilit√© pour les d√©veloppeurs de router eux-m√™me leurs applications sans avoir besoin d‚Äôun Ops dans leurs pipelines de CI via l‚ÄôAPI Fusion.\n  G√©rer les WAF de HAProxy de mani√®re centralis√©e et r√©percuter cette configuration sur un ensemble de clusters/instances.\n  Permettre aux Ops de g√©rer la structure de leurs load balancers, ajouter de nouvelles instances, g√©rer les certificats SSL, le tuning des performances depuis un seul point d‚Äôentr√©e.\n\n\nEst-ce r√©silient ?\n\nFusion Control Plane est livr√© avec tout un set de features int√©ressantes pour assurer sa maintenabilit√© et sa r√©silience :\n\n  Une pleine observabilit√© avec une application unifi√©e de r√©cup√©ration de logs, m√©triques et rapports dans la m√™me interface. L‚Äôexport de ces data est possible, notamment pour les transposer dans un dashboard tiers (Grafana, par exemple).\n  Un syst√®me de RBAC permettant de mieux g√©rer les p√©rim√®tres de chacune des √©quipes dans le control plane.\n  La gestion centralis√©e de la configuration, la validation des configurations et le bot management. La partie WAF est packag√©e avec OWASP (communaut√© publiant des recommandations pour la s√©curisation des applications web) ModSecurity Core Rule Set (CRS) pour la d√©tection des vuln√©rabilit√©s. Dans le cadre d‚Äôun cluster un syst√®me de failover automatique avec auto-√©lection du leader (√† la mani√®re de GOSSIP avec Consul).\n\n\nUne vue de l‚Äôavenir ?\n\nAujourd‚Äôhui, Fusion Control Plane limite son scope √† HAProxy Entreprise et Community Edition, les IngressController ne sont pour le moment pas encore support√©s.\n\nIl n‚Äôest pas encore pleinement compatible avec les features offertes par AWS (Gestion des ASG et de Route53) mais c‚Äôest en cours de d√©veloppement chez HAProxy Technologies.\n\nLe produit semble prometteur et int√©ressant. Les possibilit√©s qu‚Äôil nous offre pour laisser la main aux d√©veloppeurs sur la mise en place de routes vers leurs applications c√¥t√© on-premise est vraiment un gros plus, mais il nous manque pour le moment le support de l‚ÄôIngressController HAProxy utilis√© sur nos cluster Kubernetes, ce qui nous emp√™che d‚Äôen profiter au maximum.\n\nVous reprendrez bien un peu de p√©taoctets ?\n\nChez Bedrock, un √©l√©ment central de notre m√©tier est de fournir de la vid√©o √† nos utilisateurs. (Incroyable pour une boite qui fait de la VOD hein? üòÄ).\n\nPour ce faire nous avons nos propres serveurs CDN h√©berg√©s sur Paris, en compl√©ment des CDN publics comme Cloudfront ou Fastly. Cette ann√©e nous avons servis plusieurs centaines de PB de donn√©es via nos serveurs et nous esp√©rons pouvoir au moins doubler ce trafic l‚Äôann√©e prochaine !\n\nNotre architecture CDN est constitu√©e d‚Äôun logiciel appel√© LBCDN qui ‚Äúload-balance‚Äù la charge sur les CDN, on-prem et publics, en redirigeant un utilisateur vers un serveur CDN sp√©cifique.\nNos serveurs en eux-m√™mes sont bas√©s sur Nginx avec une configuration assez simple en direct IO sur de gros SSD.\n\nLa HAproxy conf 2022 nous a pas mal inspir√©s pour r√©pondre √† nos probl√©matiques avec ces deux conf√©rences :\n\n  Boost your web apps with HAProxy and Varnish, by J√©r√©my Lecour CTO of Evolix:Video\n  Was That really HAProxy, by Ricardo Nabinger Sanchez performance engineer at Taghos: Video\n\n\nCes deux pr√©sentations font √©tat d‚Äôune architecture sur les CDN int√©ressante o√π HAProxy est utilis√© pour mettre ‚Äúen sandwich‚Äù l‚Äôoutil (ou les outils) faisant fonction de CDN.\nL‚Äôarchitecture pr√©sent√©e semble permettre une configuration bien plus fine que ce que nous avons actuellement avec seulement Nginx.\n\nPar exemple, sur nos CDN on-prem nous devons aujourd‚Äôhui utiliser une astuce pour que Nginx puisse dynamiquement aller r√©soudre le nom de domaine du backend sur lequel il source ses fichiers. Cela est d√©j√† un peu dommage de ne pas avoir de m√©canisme disponible nativement. De plus, ce m√©canisme est difficile √† coupler avec d‚Äôautres permettant d‚Äôavoir du fail-over par exemple.\n\nC‚Äôest ici qu‚ÄôHAProxy pourrait intervenir pour r√©soudre notre probl√©matique car il nous permet d‚Äôavoir du fail over et des tests plus fins sur l‚Äô√©tat de sant√© des backends.\n\nDe plus, nous sommes en train de tester une solution de second-tier de CDN qui, du fait de la complexit√© ajout√©e √† notre architecture de CDN, profiterait beaucoup d‚Äôune plus grande finesse de configuration.\n\n‚ÄúMais attends, tu n‚Äôas parl√© que de HAProxy en backend l√†, tu triches un peu non? C‚Äôest pas un sandwich c‚Äôest une tartine de HAProxy l√†!‚Äù\nTout √† fait, notre cas d‚Äôusage actuel n‚Äôa pas forc√©ment besoin d‚Äôun HAProxy en frontal de Nginx.\n\nMAIS!\n\nC‚Äôest l√† que les conf√©rences sont int√©ressantes car elles montrent que l‚Äôon peut mixer les backends.\nDans la conf√©rence pr√©sent√©e par Ricardo, l‚Äôutilisation de deux backends (Varnish et hyper-cache) sur un m√™me serveur est permise par un HAProxy. Cela permet de profiter de la compl√©mentarit√© de ces services.\nDans notre cas, nous n‚Äôavons pas besoin de cela mais une autre conf√©rence nous a mis la puce √† l‚Äôoreille : Writing HAProxy Filters in Rust, by Aleksandr Orlenko.\nCela pourrait nous permettre, avec un HAProxy en frontal, d‚Äôagr√©ger plus finement les mesures de performances du serveur afin d‚Äôoptimiser l‚Äôusage de ses ressources, ou d√©porter une partie du trafic sur un serveur moins charg√©, ou encore de r√©cup√©rer une partie des traitements actuellement effectu√©s par le LBCDN.\n\nAjouter cette fonctionnalit√© serait la belle cerise au kirsch au sommet de ce sandwich de HAProxy.\n\n\n\n‚ÄúIl est bizarre ton sandwich‚Äù\n\n‚ÄúBon d‚Äôaccord, c‚Äôest plut√¥t un g√¢teau √† √©tages.‚Äù\n\n‚ÄúOk c‚Äôest mieux, mais je pr√©f√®re les macarons de la HAProxy Conf 2022 quand m√™me.‚Äù\n\nA une prochaine fois !\n\nLa HAProxyConf, c‚Äô√©tait deux jours de conf√©rences avec des orateurs venus de tous les coins du globe.\nUne belle occasion pour nous d‚Äôen apprendre plus sur un outil que nous utilisons quotidiennement chez Bedrock.\nDans cet article, nous n‚Äôavons pas pu faire mention de tout ce qui nous a int√©ress√©. Nous pourrions notamment citer les tr√®s int√©ressantes conf√©rences au sujet de :\n\n  Docker et leur utilisation de l‚Äôoutil Keda\n  Ou encore de SoundCloud et leurs mesures anti-DDOS\n\n\nCette conf√©rence √©tait aussi l‚Äôoccasion d‚Äô√©changer avec l‚Äô√©quipe HAProxy autour de sujets techniques qui nous concernent, de voir que nous utilisions d√©j√† certaines bonnes pratiques, mais aussi que nous avions de quoi nous am√©liorer.\n\nSuite √† cette conf√©rence, c‚Äôest HAProxy Fusion que nous attendons le plus. Fusion s‚Äôannonce comme l‚Äôoutil id√©al pour manager une flotte d‚ÄôHAProxy. Jusqu‚Äô√† pr√©sent, nous devions utiliser une solution maison HSDO, fonctionnelle, mais tr√®s probablement moins bien int√©gr√©e qu‚Äôun outil directement fourni par HAProxy.\n"
} ,
  
  {
    "title"    : "Trophy Hunter #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/trophy-hunter",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Les chasseurs qui ne font de mal √† personne d‚Äôautres qu‚Äô√† eux-m√™mes üòª\n\n\nPr√©sent√© par Oliver Th√©bault.\n",
  "content"  : "\n  Les chasseurs qui ne font de mal √† personne d‚Äôautres qu‚Äô√† eux-m√™mes üòª\n\n\nPr√©sent√© par Oliver Th√©bault.\n"
} ,
  
  {
    "title"    : "Trois patterns avanc√©s pour am√©liorer la r√©silience d‚Äôune application #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/trois-patterns-avances-pour-ameliorer-la-resilience-une-application",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Apr√®s la pr√©sentation g√©n√©rale d‚Äôil y a quelques mois (vid√©o publique), plongeons plus en profondeur dans trois approches qui aident √† am√©liorer la r√©silience d‚Äôune plateforme.\n\n  Nous parlerons, aujourd‚Äôhui, de :\n\n  Random Jitter ;\n\n  de Cell-...",
  "content"  : "\n  Apr√®s la pr√©sentation g√©n√©rale d‚Äôil y a quelques mois (vid√©o publique), plongeons plus en profondeur dans trois approches qui aident √† am√©liorer la r√©silience d‚Äôune plateforme.\n\n  Nous parlerons, aujourd‚Äôhui, de :\n\n  Random Jitter ;\n\n  de Cell-based architecture ;\n\n  et de Shuffle Sharding.\n\n  Avec quelques sch√©mas √† ma fa√ßon üòç\n\n\nPr√©sent√© par Pascal Martin.\n"
} ,
  
  {
    "title"    : "To be or not to be, ou quelques r√©flexions sur la dette technique et humaine #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/to-be-or-not-to-be",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Et si on parlait dette technique ? Oui, vous savez bien, tous les trucs qu‚Äôon laisse un peu de c√¥t√© pour de bonnes ou de mauvaises raisons. Parceque on aura toujours le temps de voir √ßa plus tard, parcequ‚Äôil n‚Äôy a pas de temps pour s‚Äôy consacre...",
  "content"  : "\n  Et si on parlait dette technique ? Oui, vous savez bien, tous les trucs qu‚Äôon laisse un peu de c√¥t√© pour de bonnes ou de mauvaises raisons. Parceque on aura toujours le temps de voir √ßa plus tard, parcequ‚Äôil n‚Äôy a pas de temps pour s‚Äôy consacrer, parceque qu‚Äôapr√®s tout, le produit ne fonctionne pas si mal que √ßa au fond, parceque cela co√ªte cher ! Mais au fond de vous ‚Ä¶. En bon professionnel √™tes vous vraiment en accord avec cette vision ? Etes vous ok pour sacrifier le produit ?\n\n  Maintenant, imaginons que ‚Äúle produit‚Äù, ce soit vous et transposons tout cela √† l‚ÄôHumain je vous propose de d√©couvrir ce que pourrait √™tre la ou les diff√©rentes dettes technique qui peuvent nous freiner. La dette qui peut nous faire ‚Äúbugger‚Äù et nous faire dysfonctionner. Et comment on peut connaitre ou reconnaitre quelques sch√©mas, avoir quelques pistes √† mettre en oeuvre pour au final √™tre une meilleure version de soi m√™me et pour mieux travailler avec soi et les autres.\n\n\nPr√©sent√© par Emmanuel Herve.\n"
} ,
  
  {
    "title"    : "git log --since=486-09-16T00:00:00-00:00 --until=1453-05-29T00:00:00 #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/reconstitution-medieval",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Reconstitution medieval et autres joyeuset√©s üèπ\n\n\nPr√©sent√© par Olivier Janin et Thomas Briset.\n",
  "content"  : "\n  Reconstitution medieval et autres joyeuset√©s üèπ\n\n\nPr√©sent√© par Olivier Janin et Thomas Briset.\n"
} ,
  
  {
    "title"    : "40 min pour (tenter de) comprendre l&#39;informatique quantique #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/quarante-min-pour-comprendre-informatique-quantique",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  C‚Äôest une technologie qui d√©fie l‚Äôimagination ü§Ø\n\n  Elle exploite des propri√©t√©s √©tranges de la mati√®re et serait capable de r√©soudre en quelques minutes des calculs qui prennent aujourd‚Äôhui des ann√©es ‚ö°Ô∏è Elle fait r√™ver les physiciens, les gran...",
  "content"  : "\n  C‚Äôest une technologie qui d√©fie l‚Äôimagination ü§Ø\n\n  Elle exploite des propri√©t√©s √©tranges de la mati√®re et serait capable de r√©soudre en quelques minutes des calculs qui prennent aujourd‚Äôhui des ann√©es ‚ö°Ô∏è Elle fait r√™ver les physiciens, les grands groupes et m√™me les gouvernements qui investissent massivement dans la recherche quantique üí∏\n\n  Mais qu‚Äôest-ce que l‚Äôinformatique quantique ? Pourquoi serait-elle r√©volutionnaire ?\n\n  Je vous propose une introduction aux enjeux de l‚Äôordinateur quantique, un peu de th√©orie et un exemple d‚Äôalgorithme quantique Pr√©parez-vous √† voir le monde en 12 dimensions, et c‚Äôest parti !\n\n\nPr√©sent√© par Gabriel Forien.\n"
} ,
  
  {
    "title"    : "Fabriquer sa table avec plateau live edge : tout ce qu&#39;il faut faire... ou pas ! #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/fabriquer-sa-table-avec-plateau-live-edge-tout-ce-quil-faut-faire-ou-pas",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Fabriquer sa table avec plateau live edge : tout ce qu‚Äôil faut faire‚Ä¶ ou pas !\n\n\nPr√©sent√© par Timoth√© Crespy.\n",
  "content"  : "\n  Fabriquer sa table avec plateau live edge : tout ce qu‚Äôil faut faire‚Ä¶ ou pas !\n\n\nPr√©sent√© par Timoth√© Crespy.\n"
} ,
  
  {
    "title"    : "√âcrire un livre, mais pourquoi faire ? #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/ecrire-un-livre-mais-pourquoi-faire",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  √âcrire un livre vous rendra-t-il riche et c√©l√®bre ? Peut-√™tre ou peut-√™tre pas.\n\n  En janvier dernier, j‚Äôai publi√© un ouvrage sur Spark, framework data, aux √âditions ENI. Je vous propose de vous raconter cette aventure.\n\n  De cette mani√®re-l√†, ...",
  "content"  : "\n  √âcrire un livre vous rendra-t-il riche et c√©l√®bre ? Peut-√™tre ou peut-√™tre pas.\n\n  En janvier dernier, j‚Äôai publi√© un ouvrage sur Spark, framework data, aux √âditions ENI. Je vous propose de vous raconter cette aventure.\n\n  De cette mani√®re-l√†, vous aurez une vision des b√©n√©fices (ou pas) que vous pouvez tirer en √©crivant un livre.\n\n\nPr√©sent√© par Nastasia Saby.\n"
} ,
  
  {
    "title"    : "Docteur qui ? #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/docteur-qui",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Pr√©sentation de la s√©rie TV iconique, geek et britannique qu‚Äôest Doctor Who.\n\n  D√©couvrez :\n\n  De quoi parle Doctor Who ?\n\n  Pourquoi cette vieille s√©rie est culte ?\n\n  Par o√π commencer si on veut regarder\n\n\nPr√©sent√© par Sarah Ha√Øm-Lubczanski.\n",
  "content"  : "\n  Pr√©sentation de la s√©rie TV iconique, geek et britannique qu‚Äôest Doctor Who.\n\n  D√©couvrez :\n\n  De quoi parle Doctor Who ?\n\n  Pourquoi cette vieille s√©rie est culte ?\n\n  Par o√π commencer si on veut regarder\n\n\nPr√©sent√© par Sarah Ha√Øm-Lubczanski.\n"
} ,
  
  {
    "title"    : "How many DynamoDB RCU and WCU should we reserve to achieve maximum cost reductions, when our workloads are changing all the time?",
    "category" : "",
    "tags"     : " aws, dynamodb, finops",
    "url"      : "/2022/11/22/dynamodb-reservations.html",
    "date"     : "November 22, 2022",
    "excerpt"  : "Many of the microservices in our VOD and Replay platform use DynamoDB as their database.\nPerformance is very good if the data is architected for it, scalability is reasonably fast, and the serverless aspect offloads a lot of the administration and...",
  "content"  : "Many of the microservices in our VOD and Replay platform use DynamoDB as their database.\nPerformance is very good if the data is architected for it, scalability is reasonably fast, and the serverless aspect offloads a lot of the administration and hosting work. Whether it‚Äôs performance, resilience or time-to-market, DynamoDB helps us achieve our business goals.\n\nThat said, when we spend several hundred thousand dollars on DynamoDB every year, any optimization is good for us!\n\nWith DynamoDB, committing to a certain capacity for a year can help reduce costs ‚Äì up to 50% savings on that capacity. But how do we know how much to reserve when traffic on our platform varies throughout the day?\n\n\n\nTable of Contents\n\n\n  DynamoDB: a not always obvious cost model!\n  How many WCUs and RCUs do we consume?\n  In theory: how much should we reserve, to achieve maximum savings?\n  In practice: let‚Äôs calculate how much to reserve!\n  Finally, let‚Äôs create those reservations!\n  After reserving, viewing the costs\n  Conclusion\n\n\nDynamoDB: a not always obvious cost model!\n\nTo skip all the theory about how DynamoDB is priced and WCUs, RCU, on-demand and provisionned billing modes, click here‚Ä¶\n\n\n  DynamoDB is serverless1!\n\n\nBut, as with many AWS services, you have to think for a while before you really understand DynamoDB costs‚Ä¶\n\nOut of scope costs\n\nWe pay for the volume of data stored, the volume of data backed up. These costs are outside the scope of this article and I won‚Äôt talk about them again today. They are not zero, however, and can even be a significant part of your bill ‚Äì for example, if you store large data for a long time2 in DynamoDB. Something you probably shouldn‚Äôt do!\n\nWCUs and RCUs\n\nEach DynamoDB table can be configured in either on-demand or provisioned billing mode.\n\nIn the second case, we pay for RCUs (Read Capacity Units) and WCUs (Write Capacity Units), depending on the capacity we provision for each table.\nReservations only matter for these RCUs and WCUs, in purple in the screenshot below.\n\n\n\nOver the past year, our WCU and RCU costs in provisioned mode represent about half of our DynamoDB costs.\nStorage and backups have costs that we consider negligible today.\n\nAnd, from a financial standpoint, we work with far too many pay-per-request tables3 for my taste.\n\nThe documentation will tell you more, but in very broad terms:\n\n\n  One WCU is consumed when writing one line of data. Or for each 1 KB block written.\n  One RCU is consumed to read one line of data. Or for each 4 KB block read.\n  In eventually-consistent read mode, only 1/2 RCU is consumed to read one line of data. Or for each 4 KB block.\n  Transactional mode costs twice as much.\n\n\nAs you can imagine, the first optimization is to store only what is necessary and to request DynamoDB in the way that best meets the needs of the application, including consistency and costs. Developing a data schema that efficiently meets the needs of the application is crucial. I highly recommend you read Alex DeBrie‚Äôs very good ‚ÄúThe DynamoDB Book‚Äù! Financial optimization based on reservations should ‚Äì and can ‚Äì only come afterwards, when usage patterns have been dealt with.\n\nThe on-demand / pay-per-request mode\n\nIn on-demand mode, we theoretically don‚Äôt have to worry about scalability, DynamoDB handles it for us4.\n\nIn this mode, we pay for each RCU and WCU we consume. If we don‚Äôt use DynamoDB, we don‚Äôt pay. If we use DynamoDB, we pay.\nThe counterpart is that RCUs and WCUs are more expensive in this mode than in the one presented below.\n\nThis mode is therefore very practical, in my opinion, in two cases:\n\n\n  In an environment where we only perform a few queries from time to time (dev, staging).\n  For tables that are usually not used much, but receive large and sudden peaks of requests at certain times.\n\n\nThis mode is not adapted, especially because costs are too high:\n\n\n  For tables where consumption is stable or varies slowly. Typically, tables for which usage follows our daily traffic wave, which is gentle enough on most applications for a reactive auto-scaling mechanism to meet our needs.\n\n\nProvisioned mode\n\nIn provisioned mode, we configure how many RCUs and WCUs we want and we pay for that number of RCUs and WCUs ‚Äì no matter if we consume them or not.\nThis billing mode is therefore less flexible than on-demand. On the other hand, RCUs and WCUs are less expensive.\n\nIn provisioned mode, we can set up an auto-scaler on the RCUs and WCUs of the tables that need it. It will dynamically reconfigure the provisioned RCUs and WCUs for those tables, to approximate the actual usage. With an auto-scaler, we can pay as close as possible to our actual consumption, at the provisioned price, which is lower than the on-demand one.\nHowever, scale-out is not instantaneous: it takes several minutes to detect it needs to act, and then up to several minutes (especially on a large table) to do so. Also, scale-in can only be triggered around once per hour. For more detailed information, read the documentation and the quota page.\n\nThis mode is especially recommended, in my opinion and considering our workloads:\n\n\n  As often as possible, since each RCU and WCU costs much less than in on-demand mode.\n\n\nThis mode is not suitable:\n\n\n  On tables where consumption varies very abruptly.\n\n\nIn provisioned mode, reservations\n\nBy agreeing to pay for a certain amount of RCU and WCU for one year (or even three years in some regions), these RCU and WCU become even cheaper: up to ~50%5 cheaper than in default-provisioned mode.\nReserving capacity is a great way to considerably reduce the cost of read/write operations on DynamoDB!\n\nReservations lock us for one year. We will pay for the reserved RCUs and WCUs, whether we use them or not.\nIt is therefore important to calculate correctly the reservations to be made.\n\nAlso, we pay a part of the total yearly amount at the beginning of the commitment (= ‚Äúupfront‚Äù), which means we must be able to invest a certain amount in advance.\nThe other part is spread over all the months of the commitment period.\n\nAs a consequence, the big question, to which the rest of this document tries to answer, is: ‚Äúhow many RCU and WCU should we reserve to keep our costs as low as possible?‚Äù\nWhen our consumption varies throughout the day, this calculation is pretty fun¬†;-)\n\nReservations are global to an AWS account, or even to all accounts on a consolidated bill6.\n\n‚Üí Reserved pricing is documented on the page of ‚Äúprovisioned‚Äù pricing.\n‚Üí You can also read this whitepaper.\n\nHow many WCUs and RCUs do we consume?\n\nFor the rest of our reasoning and this article, we only count the consumption in provisioned mode (and exclude on-demand), since that‚Äôs where we can play with reservations.\nAlso, we count provisioned WCU and RCU and not what is actually consumed ‚Äì so beware of any potential waste.\n\nOn the DynamoDB Web Console home screen, we can see, for an account and a region, how many WCUs and RCUs are provisioned at the current time:\n\n\n\nBut these numbers only give a view at a given instant, in a single AWS account and in a single region.\nWe deploy our platform across dozens of accounts and multiple regions, with traffic that changes throughout the day, so this is not enough.\n\nTable WCUs/RCUs\n\nFor a global view of all tables in an account in a region, we can query Cloudwatch Metrics, analyzing the ProvisionedWriteCapacityUnits or ProvisionedReadCapacityUnits metrics:\n\n\n\nThe Stacked Area view shows, at any given time, the total WCUs (or RCUs) provisioned for all of our tables, in an account and a region.\n\nWCU/RCU of GSI\n\nWe also need to count the WCUs/RCUs of the Global Secondary Indexes ‚Äì and these are different metrics! Or, at least, the metrics are shown in a different category in the Cloudwatch web console.\n\n\n\nSo, in total‚Ä¶\n\nTo get the total, you have to consider this metric for the tables and for the Global Secondary Indexes! In the Cloudwatch console, you have to search in two categories.\nGraphing it all :\n\n\n\nOf course, this is to be looked at for WCUs, but also for RCUs, following exactly the same principle.\nAnd, again, we‚Äôre working in multiple accounts and regions.\n\nIn theory: how much should we reserve, to achieve maximum savings?\n\nOnce we know how much capacity we‚Äôre actually using, we can move on to reservations.\n\nBut the calculation would be far too easy if our usage was flat!\nIn reality, thanks to auto-scaling, our provisioned capacity follows our usual traffic pattern: a wave.\n\nAnd, two things:\n\n\n  if we reserve more than we provision, we‚Äôll waste money.\n  if we reserve less than we provision, we won‚Äôt save as much as we could.\n\n\nReserve at the bottom of the wave\n\nA first idea is to reserve the lowest value we provision throughout the day: what we provision at the bottom of our traffic wave, at night.\n\nIn this case, we are not wasting money, as we always provision 100% or more of our reservation.\nBut we are probably minimizing our savings, since we are provisioning more than the reservation, all day long.\n\nReserve at the top of the wave\n\nA second idea, kind of the opposite, is to reserve the highest value we provision throughout the day.\nThis way, we will never pay the full rate for any WCU/RCU.\n\nBut, in this case, we will be wasting a lot of money, since all day long we will be provisioning less than our reservation.\nThis is a bad idea.\n\nReserve ‚Äúin the middle‚Äù, thanks to careful calculations\n\nNow, the real solution: calculate the right value:\n\n\n  Less than the highest value, to minimize waste.\n  And more than the lowest value, to optimize savings.\n\n\nIn practice: let‚Äôs calculate how much to reserve!\n\nManipulating metrics in Cloudwatch, for visualization, may be acceptable, although we rarely do it since we use other stacks for our metrics. And aggregating metrics from multiple accounts should be feasible (we haven‚Äôt tried it).\nBut for calculations, it is not enough.\n\nExporting metrics\n\nAs a first step, we exported the metrics visualized above, to be able to manipulate them in another tool ‚Äì in a spreadsheet, for example.\nTo export these metrics from Cloudwatch, we can query its API. We need to do this for all accounts and for each table, which is complicated to do manually.\n\nTo simplify the task, we started working with a script that exports this data to a CSV file.\nSpecifically, this script exports one data point per hour: the number of WCUs or RCUs actually provisioned during that hour.\n\nRunning this script for a representative week, we have enough data to calculate the ideal reservations.\n\n\n  üóìÔ∏è Representative week?\nOf course, we have to be careful to choose the week we focus on.\nIf we work with data from a week with a huge unexplained peak of traffic, the results of our calculation will fit that week, but not so much to the rest of the year!\n\n\nA Google Spreadsheet calculation\n\nImporting this data into a Google Spreadsheet, we get two columns: a date+time and a number of WCUs.\nAnd this is for each one-hour range during an entire week:\n\n\n\n\n  ‚ÑπÔ∏è Only twelve hours\nHere, I only reproduce twelve rows corresponding to twelve hours, but keep in mind that there are actually 168 rows in my spreadsheet: one row per hour, 24 hours per day, for 7 days.\nAlso, the values used for this article are all simulated, to avoid sharing sensitive information, but they scrupulously respect the shape of our traffic and usage wave.\n\n\nThe next step is to integrate the cost of these WCUs.\nEasy anough, we multiply the number of WCUs by the cost of a WCU in Paris, i.e. $0.000772.\nAnd the sum of the cost of each line gives us the total cost, without reservation:\n\n\n\nThe calculations, on an assumption\n\nNow, let‚Äôs assume, for the time being, that we reserve 25,000 WCUs:\n\n\n  The upfront, each hour, is $5.07991.\n  And, each hour, we also have to pay $3.82500 for this capacity, since the upfront is only partial.\n\n\nIn addition:\n\n\n  During some hours, when we consume less than 25,000 WCU, we will not pay anything extra.\n  During some other hours, when we consume more than 25,000 WCU, we will have to pay a supplement, at the full provisioned rate.\n\n\nAdding these data, we obtain a different hourly cost, often lower than the one determined above.\nAnd, therefore, we get a lower total cost as well:\n\n\n\nWith this hypothesis of a 25,000 WCU reservation, over these twelve hours, we would pay 135 dollars instead of 229 dollars without reservation.\nWe would then realize 40.96% savings!\n\nThe calculations, until we find the right value\n\nOf course, during the hours when we consume less than 25,000 WCU, we are wasting capacity: we are paying for it, without using it.\n\n\n\nThe goal of the game is to find the right number of WCUs to reserve: we want to reduce the total cost as much as possible, maximizing the percentage of savings.\n\nTo do so, we try different values for the number of WCUs reserved, until we find the one that maximizes the percentage of savings:\n\n\n\nHere‚Äôs the same thing as a graph:\n\n\n\nHere, over these twelve hours, the optimal approach would be to reserve 23,000 WCU.\n\n\n  üí™ Getting real: an entire week\nIn reality, we perform exactly the same calculation and we follow this very same logic, on 168 lines of data, corresponding to a representative week.\n\n\nEasier calculations?\n\nThe first year we tried to reserve capacity, we quickly wrote a script to collect the data from Cloudwatch and export it as CSV.\n\nWe still haven‚Äôt, after three or four years now, written a program that would perform the calculations based on this data to come up with the right value for the number of WCUs or RCUs to reserve.\nAs a matter of facts, copying and pasting data from the CSV export to a spreadsheet only takes a minute, we reuse the same year after year, and its visual aspect is nice!\n\nAlso, we only do these calculations and reservations twice a year, so we don‚Äôt spend too much time working on this, while still refining more often than once each year.\nEach time, the process takes two of us7 about two hours, or one day per year in total‚Ä¶ And the most time-consuming part is talking to our colleagues who are heavy DynamoDB users, and asking them ‚Äúare you planning to reduce the consumption of your project over the coming year?‚Äù\n\nFinally, let‚Äôs create those reservations!\n\nWe calculated how many WCUs and how many RCUs we should reserve to achieve the best possible savings, hoping the week we chose to base our calculations on was actually a representative week.\n\nA commitment: be careful‚Ä¶\n\nA reservation commits us to pay for a year, whether we use this capacity or not.\n\nSo, it‚Äôs always a good idea to take a moment to validate with our colleagues that they are not planning to use less DynamoDB in the near future.\nOf course, the answer is often partly ‚Äúit depends‚Äù, since usage depends on new projects as well as on the traffic on our platforms, but if we can already anticipate the next planned optimizations, it‚Äôs always a good thing.\n\nIn November 2022, we can only open DynamoDB reservations for one year if we work in the AWS Paris region.\nOther regions (us-east-1 for example) allow reservations for three years, which means more substantial savings. On the other hand, would we be willing to commit for three years and lose a major advantage of The Cloud, its flexibility?\n\nWhich account to reserve on?\n\nThe documentation says (emphasis mine):\n\n\n  If you have multiple accounts linked with consolidated billing, reserved capacity units purchased either at the payer account level or linked account level are shared with all accounts connected to the payer account.\nReserved capacity is applied first to the account that purchased it and then any unused capacity is applied to other linked accounts.\n\n\nWe have configured our AWS accounts to have a single payer account.\nWe have decided to make all our reservations in this account and they are applied to the child accounts without discrimination.\nThis applies to DynamoDB but also to RDS, EC2, Elasticache‚Ä¶\n\nReserving!\n\nTo reserve, we go through the AWS DynamoDB Web console, in our payer account, in the region where these reservations will be used.\n\nOn this screen, you can see how many WCUs and RCUs we have already reserved.\nSince we make several reservations during the year, the reservations already in progress are to be subtracted from the values calculated above!\n\n\n\nTo create a new reservation, click on ‚ÄúPurchase reserved capacity‚Äù and fill in the form¬†;-)\n\n\n\nAfter reserving, viewing the costs\n\nOnce the reservations are made, in AWS Cost Explorer, the upfront cost is clearly visible.\nIt is charged at once, on the day we opened the reservation:\n\n\n\nTo have a daily view of WCU/RCU costs (reserved + provisioned in addition to reservations), remember to fill in ‚ÄúShow costs as: Amortized costs‚Äù to smooth the monthly price of reservations over all days of the month:\n\n\n\n\n  Reservations and one payer account\nSince reservations, which cover the bulk of our DynamoDB costs, are made on our payer account, the bulk of our DynamoDB costs go back to this account‚Ä¶ And not to the tenant/environment accounts.\nGood luck tracking costs and allocating them to projects and teams üí™\n\n\nConclusion\n\nWe work with DynamoDB a lot, for several dozen microservices, and we face several types of infrastructure costs: on-demand reads/writes, provisioned reads/writes, storage, backups.\nIn exchange for a loss of flexibility and through reservations that commit us for a year, AWS allows us to reduce the cost of provisioned reads/writes.\n\nDetermining how much to reserve, in the face of a constantly changing load, is not easy.\nWe need to have a certain vision on the evolution of usage, over a year, and must accept to lose flexibility.\nAnd we need to find the right values to reserve for read and write capacity.\n\nWith three or four years of hindsight, by making reservations twice a year and by following the method detailed in this article, we realize savings of about 30% to 35% on our read and write capacity in provisioned mode.\nOn our scale, this saving represents several tens of thousands of dollars per year ‚Äì which is great, considering we only spend a few hours working on this every six months!\n\n\n\n  \n    \n      DynamoDB is one of the most serverless services we use and I like it a lot. Still, there are a few admin tasks left in our hands. Typically, we have to specify the capacity we need and configure an auto-scaler. We also have to enable encryption, backups, to setup permissions ‚Äì and to check all this is done, for all tables, managed by many teams.¬†&amp;#8617;\n    \n    \n      If you do store a lot of data for a long time in DynamoDB, take a look at Standard-IA, it might help you reduce costs.¬†&amp;#8617;\n    \n    \n      Why do we use pay-per-request so much? Well, in short, because this mode is more flexible than the provisioned one, and several of our projects are willing to pay much more in exchange for this flexibility.¬†&amp;#8617;\n    \n    \n      DynamoDB in on-demand mode and scalability: in practice, AWS hides what‚Äôs going on, but doesn‚Äôt scale to infinity instantly either.¬†&amp;#8617;\n    \n    \n      50% is kind of the maximum possible saving we can achieve if our usage is flat and we reserve exactly what we provision. Flat usage might be what you see on your applications, but it‚Äôs not how our platform works!¬†&amp;#8617;\n    \n    \n      At Bedrock, we have a dedicated billing account ‚Äì a ‚Äúpayer account‚Äù ‚Äì that aggregates costs from all our other accounts. Reservations are also shared amongst all (whitelisted) accounts that have a shared payer account.¬†&amp;#8617;\n    \n    \n      For these kind of calculations and reservations, we usually work in pair, as this involves large amounts of money. Lowering risk of doing a costly mistake is quite a good idea.¬†&amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Ce que nous retenons de la droidcon London 2022",
    "category" : "",
    "tags"     : " android, droidcon, conference",
    "url"      : "/2022/11/22/droidcon-london-2022.html",
    "date"     : "November 22, 2022",
    "excerpt"  : "La communaut√© Android a apport√© le soleil sur Londres les 27 et 28 octobre 2022. La droidcon London a r√©uni plus de 1400 d√©veloppeurs autour de l‚Äô√©cosyst√®me Android, de ses outils et enjeux actuels. Jetpack Compose, √©videmment, mais aussi Gradle, ...",
  "content"  : "La communaut√© Android a apport√© le soleil sur Londres les 27 et 28 octobre 2022. La droidcon London a r√©uni plus de 1400 d√©veloppeurs autour de l‚Äô√©cosyst√®me Android, de ses outils et enjeux actuels. Jetpack Compose, √©videmment, mais aussi Gradle, modularisation, optimisation et autres sujets plus divers ont √©t√© abord√©s lors de ce rendez-vous incontournable pour la communaut√©.\n\n\n\n\n  √áa compile ? - Rafi Panoyan    \n      Vous reprendrez bien un peu de Gradle Enterprise ?\n      Dessine-moi un module\n      Trucs et astuces\n    \n  \n  Design the world - Damien Cuny    \n      To Compose\n      Design System\n      Vers l‚Äôinfini et au-del√†\n    \n  \n  La gestion des erreurs - David Yim    \n      V√©rification des entr√©es\n      Le type Either\n      Kotlin Result\n      Conclusion\n    \n  \n  √Ä la prochaine !\n\n\n√áa compile ? - Rafi Panoyan\n\nLes sujets de compilation ont tenu une place tr√®s importante lors de cette √©dition de la droidcon Londres 2022. \nQu‚Äôil s‚Äôagisse d‚Äôoptimiser ses temps de compilation, de repenser la cr√©ation de modules et des d√©pendances entre eux, de factoriser les logiques des scripts de compilation, \nnous avons eu une emphase claire sur l‚Äôimportance d‚Äôadresser ces sujets.\n\nVous reprendrez bien un peu de Gradle Enterprise ?\n\nNelson Osacky, qui travaille chez Gradle, a pr√©sent√© tous les outils que la formule Gradle Entreprise met √† disposition des d√©veloppeurs pour analyser en d√©tail les compilations.\n\nVous voulez v√©rifier que la compilation incr√©mentale est bien appliqu√©e partout o√π cela est possible ? Un script permet de comparer, dans des conditions reproductibles, \nles entr√©es et sorties de vos builds, et analyse les t√¢ches emp√™chant ce m√©canisme central dans la r√©duction des temps de compilation.\n\nVous voulez vous assurer que Gradle est bien capable de retrouver le cache de vos t√¢ches sur un m√™me poste ou bien depuis le cloud ? \nL√† aussi des outils vous permettent d‚Äôidentifier pr√©cisemment les points qui ne tirent pas parti de ces m√©canismes.\n\nOn regrettera que ces outils soient disponibles uniquement pour la formule payante de Gradle. Cependant, les scans Gradle sont, eux,\ngratuits et illimit√©s, et permettent tout de m√™me de mesurer et comparer des compilations et ainsi suivre l‚Äôimpact des diff√©rentes optimisations que vous pourriez apporter.\n\nDessine-moi un module\n\nLa modularisation ayant un impact sur les temps de compilation, plusieurs conf√©rences ont abord√© ce sujet tr√®s en vogue dans la communaut√© Android.\n\nUn point de vue int√©ressant de Josef Raska nous invite √† nous poser la question de la pertinence de modulariser selon le contexte. \nNe pas suivre une tendance mais se poser la question de l‚Äôutilit√© d‚Äôun nouveau module, et encore plus de ses d√©pendances avec les autres modules. \nVoil√† des propos qui invitent √† mesurer concr√®tement l‚Äôimpact du chantier de la modularisation dans nos applications.\n\nAinsi, si on peut penser que modulariser permet de r√©duire les temps de compilation (en tirant parti de la parall√©lisation des t√¢ches par exemple), \nun chemin de d√©pendances trop long entre le module initial et la d√©pendance la plus profonde va entra√Æner une augmentation du temps de compilation.\n\nVigilance, donc, sur les ‚Äúhubs de d√©pendances‚Äù (ces d√©pendances dont beaucoup de modules ont besoin, et qui ont besoin de beaucoup de modules).\n\n\n  \n  1. Hub de d√©pendances\n\n\n\nDe la m√™me mani√®re, un chemin de d√©pendances de trop grande profondeur ne permettra pas de tirer parti de la parall√©lisation des t√¢ches de compilation.\nSur le sch√©ma ci-dessous, on peut voir qu‚Äôun chemin de profondeur 4 existe pour aller du module applicatif vers le module le plus bas dans la hi√©rarchie.\n\n\n  \n  2. Profondeur de d√©pendances\n\n\n\nJosef Raska propose le sch√©ma suivant avec un d√©coupage API/impl√©mentation afin de r√©duire au maximum cette profondeur, et ainsi compiler plus rapidement.\n\n\n  \n  3. Profondeur de d√©pendances r√©duite\n\n\n\nAndroid Studio et son analyse de d√©pendances peut √™tre tr√®s utile pour v√©rifier et mesurer cela.\nJosef Raska a d‚Äôailleurs cr√©√© un plugin Gradle afin de sp√©cifier ces r√®gles √† l‚Äôechelle d‚Äôun projet et de s‚Äôassurer qu‚Äôelles soient respect√©es : modules-graph-assert.\n\nTrucs et astuces\n\nApr√®s ces conseils tr√®s avis√©s mais structurellement chronophages √† mettre en place (surtout sur de gros projets d√©j√† cr√©√©s), d‚Äôautres conf√©renciers se sont plut√¥t tourn√©s vers les ‚Äúquick-win‚Äù. Des changements peu co√ªteux, aux gains plus modestes mais qui s‚Äôadditionnent, il en existe quelques-uns.\n\nAinsi, si Gradle nous permet d‚Äôactiver des fonctionnalit√©s de caching (org.gradle.unsafe.configuration-cache=true pour gagner du temps lors de la phase de configuration par exemple), il est aussi possible de d√©sactiver des fonctionnalit√©s du plugin Android si elles ne nous sont pas utiles.\n\nVoici une petite liste des propri√©t√©s qui sont activ√©es par d√©faut, m√™me lorsqu‚Äôelles ne sont pas utilis√©es dans les modules :\nandroid {\n  buildFeatures {\n    buildConfig false\n    aidl false\n    renderScript false\n    resValues false\n    shaders false\n  }\n}\n\n\nSi vous n‚Äôutilisez pas les valeurs li√©es √† la configuration de votre compilation, ne g√©n√©rez pas de BuildConfig.\nSi vous n‚Äôavez pas de resources dans votre module, d√©sactivez la g√©n√©ration de resValue !\n\nRetrouvez ici la liste de ces fonctionnalit√©s, leur utilit√© et leurs valeurs par d√©faut : BuildFeatures.\n\nDesign the world - Damien Cuny\n\nIl y a un peu plus d‚Äôun an sortait la version 1.0 de Jetpack Compose, le nouveau toolkit d√©claratif pour la cr√©ation d‚Äôinterfaces Android. D‚Äôautre part, le design system Material Design 3 vient de sortir en version stable et son impl√©mentation Compose Material sont √©galement disponibles.\nAvec tout cela, le design a, cette ann√©e encore, tenu une place de choix dans l‚Äôagenda de cette droidcon 2022 √† Londres.\nMais comment utiliser tout cela correctement ? Comment s‚Äôen servir pour impl√©menter un design system personnalis√© ? Jusqu‚Äôo√π peut-on aller ?\nAutant de questions auxquelles ont tent√© de r√©pondre les nombreuses pr√©sentations sur le sujet.\n\nTo Compose\n\nCompose facilite beaucoup de choses dans l‚Äôimpl√©mentation et le maintien d‚Äôinterfaces sur Android. Cependant, cela n√©cessite de r√©apprendre √† faire certaines choses que l‚Äôon ma√Ætrise d√©j√† avec le syst√®me de View.\nDessiner dans un canvas en est une, et Himanshu Singh dans sa pr√©sentation ‚ÄúComposing in your canvas‚Äù, nous montre les pi√®ges √† √©viter pour r√©aliser cela avec Compose.\n\nLa recomposition peut √©galement √™tre source de probl√®mes et de latences si Compose est mal utilis√©. Dans sa pr√©sentation ‚ÄúUnderstanding recomposition performance pitfalls‚Äù, Jossi Wolf et Andrei Shikov nous donnent, √† partir d‚Äôun exemple concret, les meilleures astuces pour l‚Äôutiliser √† bon escient.\n\nDesign System\n\nEn faisant le parall√®le avec la saga √©pique de JRR Tolkien, Daniel Beleza, dans sa pr√©sentation ‚ÄúOne design system to rule them all‚Äù, nous explique comment il a r√©ussi, tout en se passant de Material design, √† unifier et automatiser son propre design system.\nCela demande, √©videmment, une collaboration totale de la part de l‚Äô√©quipe de design, mais une fois cette int√©gration faite, les b√©n√©fices et l‚Äôautonomie se ressentent de part et d‚Äôautre.\nDes outils tel que Figma, Kotlin Poet ou des plugins Android Studio custom lui ont permis d‚Äôautomatiser ensuite ce processus.\n\nMaterial Design est un design system. Il a l‚Äôavantage d‚Äô√™tre bien document√©, uniforme et r√©guli√®rement enrichi. De plus, il est d√©j√† impl√©ment√© dans l‚Äôancien syst√®me de View Android et plus r√©cemment dans Jetpack Compose avec Compose Material.\n\nUne des diff√©rences majeures entre Compose et le syst√®me de View sur Android est son d√©coupage. Dans Compose, Material n‚Äôest impl√©ment√© et n‚Äôappara√Æt que dans la partie la plus hautes alors que dans le syst√®me de View, son impl√©mentation est r√©partie dans toutes les couches de la librairie.\nIl est donc assez complexe de se passer de Material avec le syst√®me de View mais cela est compl√©tement envisageable, voire recommand√©, dans certains cas avec Compose.\n\n\n\nPour illustrer cela Sebastiano Poggi (la moiti√© de Coding with the italians) est venu nous pr√©senter, dans ‚ÄúCompose beyond Material‚Äù, les questions √† se poser avant de se lancer dans son design system et comment le package Foundation de Compose peut nous aider.\n\nPour terminer il nous donne de nombreux conseils concrets sur l‚Äôimpl√©mentation de composants sans Material. Le principal, rejoint la pr√©sentation d‚Äôintroduction de cette droidcon, ‚ÄúThe Silver Bullet Syndrome Director‚Äôs Cut - Complexity Strikes Back!‚Äù, un bon design system est un design system qui correspond √† nos besoins et qui y r√©pond le plus simplement possible.\n\nVers l‚Äôinfini et au-del√†\n\nChris Bane et Nacho Lopez dans leur pr√©sentation ‚ÄúBranching out Jetpack Compose‚Äù, nous ont racont√© comment l‚Äôaventure du passage √† Compose s‚Äôest d√©roul√©e chez Twitter, une des premi√®res apps √† l‚Äôadopter.\nAvec une code base aussi cons√©quente (plus de 1000 modules, dont 300 pour le design, r√©partis sur plus de 30 √©quipes), ils ont d√ª progressivement convaincre les √©quipes, les former et les accompagner.\nLa question de continuer √† utiliser Material Design s‚Äôest √©galement pos√©e chez eux. Ils l‚Äôont dans un premier temps conserv√© pour faciliter le passage sur Compose, pour finalement le retirer compl√®tement en se basant, eux aussi, sur le package Foundation.\nLeur pr√©sentation r√©sume bien l‚Äôensemble des √©tapes et des questions par lesquelles ils sont pass√©s pour accomplir cette transition.\n\nAfin de remettre les choses en perspective, Ash Davies nous rappelle que Compose est un simple pattern de d√©veloppement multiplateforme. De ce fait, il peux √™tre appliqu√© √† autre chose qu‚Äô√† de l‚ÄôUI comme le propose Jetpack Compose. Il nous explique dans ‚ÄúDemystifying Molecule: Running Your Own Compositions For Fun And Profit‚Äù, comment l‚Äôappliquer √† la couche domaine d‚Äôun projet pour le ‚ÄúFun‚Äù.\n\nLa gestion des erreurs - David Yim\n\nLa gestion des erreurs a √©t√© le sujet de plusieurs pr√©sentations √† la droidcon. Ces pr√©sentations avaient pour objectif de servir de piq√ªre de rappel sur l‚Äôimportance de bien prendre en compte ce probl√®me concernant tous les d√©veloppeurs. Aujourd‚Äôhui, nous avons tous les outils pour g√©rer facilement nos erreurs. Cependant, par paresse et comme nous pr√©f√©rons penser de mani√®re positive, nous ne pensons souvent qu‚Äôaux cas de succ√®s et les cas d‚Äôerreurs sont souvent brouillons voire ne sont m√™me pas sp√©cifi√©s.\n\nLes speakers m‚Äôont marqu√© avec un exemple de mauvaise gestion d‚Äôerreur qui a co√ªt√© plusieurs centaines de milliers de dollars. L‚Äôexemple parlait d‚Äôune faille sur le site japonais de 7-Eleven, une cha√Æne de sup√©rettes, dont elle a √©t√© victime. Dans la base de donn√©es de ce projet, les d√©veloppeurs ont ajout√© un champ ‚Äúdate de naissance‚Äù comme nullable. Plus tard, ce champ est devenu non-nullable. Par paresse, le d√©veloppeur qui a rendu ce champ non-nullable a mis par d√©faut un 1er janvier 2019 sur cette date lorsqu‚Äôelle n‚Äô√©tait pas renseign√©e, simplement pour satisfaire son compilateur. Le probl√®me est que ce champ fut plus tard utilis√© dans la fonctionnalit√© de mot de passe oubli√© du site. En utilisant la date par d√©faut du 1er janvier 2019, un hacker a pu r√©cup√©rer des comptes utilisateurs et voler des informations bancaires. Cet exemple m‚Äôa marqu√© par l‚Äôhabitude que nous avons en tant que d√©veloppeur de nous soucier que de satisfaire notre compilateur plut√¥t que de vraiment discuter de solutions r√©fl√©chies √† nos probl√®mes techniques.\n\nPlusieurs m√©thodes de gestion des erreurs existent et les speakers en ont pr√©sent√©s quelques-unes.\n\nV√©rification des entr√©es\n\nL‚Äôune des m√©thode pour √™tre certain de ne pas avoir de probl√®me est de v√©rifier les donn√©es que l‚Äôon re√ßoit. Prenons un exemple simple :\n\ndata class User(val email: String)\n\n\nRien n‚Äôemp√™che d‚Äôinstancier cette classe de la mani√®re suivante :\n\nval user = User(email = &quot;&quot;)\n\n\nCela peut cr√©er des probl√®mes par le futur, alors qu‚Äôil y a un moyen d‚Äô√©viter cela\n\nclass Email(value: String) {\n    val value: String =\n        if (value.isEmpty() || !Regex(EMAIL_FORMAT).matches(value)) {\n            throw Exception(&quot;Email format is not correct&quot;)\n        } else {\n            value\n        }\n}\n\ndata class User(private val email: Email)\n\n\nCette m√©thode peut para√Ætre un peu exag√©r√©e dans cet exemple. Mais dans un contexte o√π la classe User serait utilis√©e par un grand nombre d‚Äô√©quipes et que les r√®gles m√©tier de l‚ÄôEmail serait complexe, cette m√©thode prendrait tout son sens pour √©viter d‚Äôavoir de mauvaises surprises !\n\nLe type Either\n\nLe type Either est un moyen de diff√©rencier les cas de succ√®s des cas d‚Äôerreurs. Il est disponible dans la librairie Arrow ou facilement reproduisible :\n\nsealed class Either&amp;lt;out A, out B&amp;gt; {\n    class Left&amp;lt;A&amp;gt;(val value: A): Either&amp;lt;A, Nothing&amp;gt;()\n    class Right&amp;lt;B&amp;gt;(val value: B): Either&amp;lt;Nothing, B&amp;gt;()\n}\n\n\nL‚Äôutilisation de ce type est qu‚Äôil est soit un type A, soit un type B. On peut ainsi d√©finir par exemple que le A est un cas de succ√®s et que le type B est un cas d‚Äôerreur.\n\ndata class User(val name: String)\n\nvar either: Either&amp;lt;User, Exception&amp;gt;\n\nwhen (either) {\n    is Either.Left -&amp;gt; {\n        println(&quot;The user is called ${(either as Either.Left&amp;lt;User&amp;gt;).value.name}&quot;)\n    }\n    is Either.Right -&amp;gt; {\n        (either as Either.Right&amp;lt;Exception&amp;gt;).value.printStackTrace()\n    }\n}\n\n\nGr√¢ce √† ce type, on peut par exemple savoir si un appel √† une API a r√©ussi ou non, ce qui nous permet de g√©rer plus facilement nos cas d‚Äôerreurs.\n\nKotlin Result\n\nLa classe Result est similaire au type Either et a pour avantage d‚Äô√™tre directement inclue dans Kotlin et que l‚Äôon n‚Äôa pas √† se synchroniser pour savoir si le c√¥t√© gauche est le cas de succ√®s ou d‚Äôerreur.\n\ndata class User(val name: String)\n\nvar result: Result&amp;lt;User&amp;gt;\n\nwhen {\n    result.isSuccess -&amp;gt; {\n        println(&quot;The user is called ${result.getOrNull()?.name}&quot;)\n    }\n    result.isFailure -&amp;gt; {\n        result.exceptionOrNull()?.printStackTrace()\n    }\n}\n\nOU\n\nresult.onSuccess { user -&amp;gt;\n    println(&quot;The user is called ${user.name}&quot;)\n}.onFailure { throwable -&amp;gt;\n    throwable.printStackTrace()\n}\n\n\nConclusion\n\nPlusieurs m√©thodes existent pour prendre en compte nos cas d‚Äôerreurs. Laquelle est la meilleure ? Eh bien vous vous y attendez s√ªrement, mais √ßa d√©pend ! On choisira une m√©thode ou une autre selon ce qui nous arrange par rapport √† la situation, nos choix d‚Äôoutils techniques ou nos effectifs. L‚Äôimportant √©tant de prendre en compte ces cas d‚Äôerreurs et de ne pas laisser leur r√©solution au hasard. Les cas d‚Äôerreurs ne sont en fait que d‚Äôautres usecases de l‚Äôutilisateur et souvent ne sont pas des edgecase. Ils m√©ritent donc d‚Äô√™tre tout autant r√©fl√©chis et sp√©cifi√©s que les cas de succ√®s !\n\n√Ä la prochaine !\n\nIl est toujours int√©ressant de mesurer l‚Äôengouement pour tel ou tel sujet dans la communaut√© Android en analysant les pr√©sentations lors des diff√©rentes conf√©rences technologiques.\n\nSans aucun doute, cette droidcon √©tait sous le signe de Jetpack Compose, qui b√©n√©ficie d‚Äôun suivi et d‚Äôun engagement fort de Google et de toute la communaut√©.\nTout l‚Äôenjeu ici est de rester au contact des innovations et de l‚Äô√©volution de la plateforme Android, et Jetpack Compose offre un d√©fi que nous avons commenc√© √† relever chez Bedrock.\n\nNous attendons avec impatience de voir o√π va Android, et avons √† coeur de participer √† cette aventure qui nous lie tous !\n\n\n"
} ,
  
  {
    "title"    : "Ce que nous avons retenu de la SymfonyCon - Disneyland Paris 2022",
    "category" : "",
    "tags"     : " conferences, backend, symfony, php",
    "url"      : "/2022/11/17/symfonycon.html",
    "date"     : "November 17, 2022",
    "excerpt"  : "En cette fin d‚Äôann√©e, une petite √©quipe de chez Bedrock a assist√© au grand retour de la SymfonyCon 2022 apr√®s 3 ans d‚Äôabsence. \nNous avons eu la chance de d√©couvrir les nouveaut√©s li√©es √† Symfony 6.2 et d‚Äôassister aux conf√©rences sur de nombreux s...",
  "content"  : "En cette fin d‚Äôann√©e, une petite √©quipe de chez Bedrock a assist√© au grand retour de la SymfonyCon 2022 apr√®s 3 ans d‚Äôabsence. \nNous avons eu la chance de d√©couvrir les nouveaut√©s li√©es √† Symfony 6.2 et d‚Äôassister aux conf√©rences sur de nombreux sujets techs √† Disneyland.\nLa keynote pr√©sent√©e par Fabien Potencier, le cr√©ateur de Symfony, nous donne un avant-go√ªt des nouvelles fonctionnalit√©s qui seront pr√©sentes dans la future version de Symfony.\nAu programme, un nouveau composant Webhooks ainsi que l‚Äô√©volution du composant Mailer.\n\nUnleashing the power of lazy objects in PHP\n\nNicolas Grekas, habitu√© de la sc√®ne PHP et membre de la Symfony Core Team, nous a pr√©sent√© les diff√©rentes fa√ßons d‚Äôutiliser les lazy objects en PHP et plus sp√©cifiquement √† l‚Äôaide de Symfony.¬†\n\nLes lazy objects sont des objets instanci√©s vides qui peuplent leurs propri√©t√©s eux-m√™mes seulement quand ils sont utilis√©s.\nIls sont utiles lors de l‚Äôinstanciation de lourds objets peu appel√©s, cela permet de faire du lazy loading.\n\nNicolas nous a aussi pr√©sent√© deux nouveaux traits prochainement disponibles sur Symfony 6.2 permettant de travailler avec ces objets.\nLes VirtualInheritanceProxies et les GhostsObjects sont deux nouvelles possibilit√©s pour impl√©menter plus facilement des lazy objects en plus du ValueHolder.\n\nAdvanced Git magic\n\nPauline Vos nous repr√©sente GIT en dehors de son usage quotidien, comment aller plus loin que les pull, commit, push et merge habituels, elle nous livre donc une pr√©sentation sur une m√©thode de debug en utilisant GIT.\n\nRetour dans un premier temps sur l‚Äôimportance des commits dit ‚Äúatomic‚Äù avec comme r√®gles :\n\n  Chaque commit se r√©sume √† un fix ou une feature\n  Chaque commit doit fonctionner (tous les tests doivent passer)\n  Chaque message et description doivent √™tre clairs et concis\n\n\nPauline introduit ensuite la commande git rebase -i qui permet un rebase interactif servant notamment √† r√©√©crire notre historique.\n\n\n\nVient ensuite l‚Äôutilisation de la commande git reflog, commande avec laquelle nous pouvons obtenir le d√©tail des commandes lanc√©es sur la branche, elle peut de ce fait √™tre utile pour r√©parer une erreur.\n\nComment utiliser toutes ces commandes GIT pour debugger ?\nUne d√©monstration de la commande git bisect et de toutes ses options qui permettent d‚Äôidentifier le commit qui a introduit le bug en faisant une recherche dichotomique.\n\nPauline pousse la r√©flexion plus loin en alliant la commande git bisect avec un script de debug ou un test unitaire.\n\n‚ÄúWrite it, push it and find where it breaks‚Äù\n\nAfin de tirer parti de cette m√©thode, il est important que chaque commit soit fonctionnel, tous les tests doivent donc passer.\n\nVous pouvez √©galement retrouver un article de Pauline √† ce sujet sur son blog personnel.\n\nSchr√∂dinger‚Äôs SQL - The SQL inside the Doctrine box\n\nAu d√©but de sa conf√©rence Claudio Zizza insiste sur le fait de bien conna√Ætre nos bases de donn√©es et le langage utilis√© pour les requ√™ter, il √©voque notamment les diff√©rences entre MySQL et PostgreSQL. Ensuite, il nous a parl√© de Doctrine ORM, des fonctions que nous appr√©cions tant, car elles nous facilitent la vie. \nPuis, il a rappel√© l‚Äôimportance de savoir faire des requ√™tes SQL m√™me si nous utilisons Doctrine. Comprendre le SQL peut nous permettre d‚Äôoptimiser notre utilisation de Doctrine et donc de mieux appr√©hender son fonctionnement. \nIl insiste aussi sur le fait que SQL ‚Äúseul‚Äù est bien plus puissant que DQL (Doctrine Query Language). \nPour terminer, Claudio nous a donn√© quelques recommandations de lecture pour apprendre le SQL ainsi qu‚Äôun site permettant de tester nos requ√™tes.\n\nAdvanced Test Driven Development\n\nDurant cette conf√©rence, Diego Aguiar, d√©veloppeur de chez SymfonyCasts nous rappelle ce qu‚Äôest le TDD (Test Driven Development), son histoire, les diff√©rentes techniques ainsi que des astuces afin de nous d√©bloquer et bien s√ªr les cas o√π il ne semble pas utile d‚Äôutiliser le TDD.\n\n√Ä retenir, le TDD est une discipline, il faut beaucoup d‚Äôentra√Ænement et r√©p√©ter continuellement ces exercices.\n\n\n\n‚ÄúFake it till you make it‚Äù, il s‚Äôagit d‚Äôabord d‚Äô√©crire son code de test en utilisant par exemple des assertions, des tests avec diff√©rentes sorties et ensuite de produire le code qui va r√©soudre ces tests.\n\n\n\n‚Äú Write your test code, produce it and repeat.‚Äù\n\nPourquoi nous retrouvons nous parfois bloqu√©s ?\n\n  Les tests √©crits sont peut-√™tre faux\n  Les tests ne sont pas assez segment√©s\n  Le code √©crit est peut-√™tre trop sp√©cifique\n\n\nComment se d√©bloquer ?\n\n  Continuer et trouver un test plus simple\n  Refactoriser le code en production qui met en difficult√©\n  √âcrire les diff√©rents use-cases\n  Passer outre les tests un instant\n\n\nLes cas les plus favorables au TDD sont les nouvelles fonctionnalit√©s qui n‚Äôont pas de lien avec du code legacy. En ce qui concerne les cas non pertinent au TDD, nous retrouvons les cas de configuration, de d√©couverte de code et de requ√™tes.\n\nPour conclure, Diego nous rappelle que le TDD est bien √©videmment plus un outil qu‚Äôune r√®gle.\n\nDynamic Validation With Symfony\n\nTout en se basant sur les √©volutions des Pok√©mon, Marion Hurteau a introduit le principe de validation dynamique. Par exemple, v√©rifier que le nom de notre Pok√©mon contient bien 10 caract√®res, ou encore les diff√©rentes r√®gles d‚Äô√©volution en fonction du type de Pok√©mon. \n√Ä l‚Äôaide d‚Äôexemples de code qui sont disponibles sur son repo Git, elle a pass√© en revue les fa√ßons d‚Äôimpl√©menter des validations √† l‚Äôaide du composant Symfony Validator.\nAu fil de sa pr√©sentation, la complexit√© des contraintes cro√Æt ce qui permet de voir un √©ventail de possibilit√©s.\n\nFrom monolith to decoupled‚Ä¶wait, why is that one getting bigger?!?\nLors de cette conf√©rence, Shawna Spoor est venue nous parler de comment d√©couper un monolithe en une multitude de microservices gr√¢ce au ‚ÄúStrangler Fig Pat‚Äù. Elle a commenc√© par nous rappeler les avantages et les inconv√©nients des microservices compar√© √† un monolithe.\n\nSuite √† cela, elle nous a donn√© les diff√©rentes √©tapes pour d√©couper une application monolithe en micro-services et cela sans jamais arr√™ter le d√©veloppement de nouvelles features:\n\n  Choisir une fonctionnalit√© qui peut √™tre d√©coup√©e\n  Cr√©√© le nouveau Service\n  D√©placer le trafic vers le nouveau service\n  Recommencer jusqu‚Äô√† la disparition du monolithe\n\n\nOn peut r√©sumer ce pattern via l‚Äôimage ci-dessous, le tronc repr√©sente le monolithe et les branches qui l‚Äô√©tranglent lentement correspondent aux micro-services.\n\n\nFrom a legacy Monolith to a Symfony Service Oriented Architecture with zero downtime\nLors de la conf√©rence pr√©sent√©e par Cl√©ment Bertillon, nous avons pu voir comment son √©quipe a transform√© leur ancienne application monolithe compos√©e de milliers de fichiers PHP en un monorepo d√©compos√© en micro-services en utilisant le Strangler Fig pattern et cela sans aucune rupture de service ni arr√™ter le d√©veloppement de nouvelles features.\n\nDe mani√®re tr√®s simplifi√©e, ils ont install√© Symfony, mis le code legacy dans un dossier √† la racine du projet, le routeur symfony permet d‚Äôacc√©der au nouveau micro-service tout en redirigeant vers le legacy si aucun contr√¥leur n‚Äôa √©t√© trouv√©. Il a conclu avec les r√®gles d‚Äôor et comment analyser les performances via Blackfire.\nCes deux conf√©rences sur le Strangler fig paterne, mon permis de mettre en place un micro projet dans un de nos projets, tout en le cloisonnant du code parent (r√®gles d‚Äôor v√©rifi√© gr√¢ce √† l‚Äôoutil pr√©sent√© deptrac). Ce principe nous permettra de le transformer en micro service tr√®s facilement.\n\nPHPStan: Advanced Types\nCette conf√©rence centr√©e sur l‚Äôoutil d‚Äôanalyse statique de code : PHPStan, a √©t√© pr√©sent√©e par son cr√©ateur Ond≈ôej Mirtes.\nIl a commenc√© par nous rappeler quelle est la diff√©rence entre un langage compil√© et un langage interpr√©t√©, le premier ne se compile pas s‚Äôil y a des erreurs alors que le second ne plante qu‚Äô√† l‚Äôex√©cution. Le but de PHPStan est de nous aider √† identifier toutes les erreurs sans avoir besoin d‚Äôex√©cuter le code.\n\n\n\nCet outil analyse toutes les fonctions, les propri√©t√©s, le typage PHP, mais aussi la PHPDoc. Ond≈ôej nous a ensuite parl√© de tous les types PHPStan avec des exemples, en voici quelques un qui ont marqu√© notre attention :\n\n  non-empty-array, non-empty-string\n  literal-string\n  integer-range, integer-mask, integer-maskof\n  conditional return types, union types, intersection types ‚Ä¶\n\n\nIl finit en nous rappelant que l‚Äôutilisation de @var est une mauvaise pratique et qu‚Äôil vaut mieux renforcer le typage quitte √† modifier la documentation des vendor via les Stub files.\n\nGNAP: The future of OAuth\n[Slides]\n\nRobin Chalas @chalas_r nous a pr√©sent√© GNAP (Grant Negotiation and Authorization Protocol) : une initiative pour d√©velopper la prochaine g√©n√©ration de protocoles d‚Äôautorisation.\nPour mieux comprendre les enjeux, nous sommes repartis de l‚Äôhistorique d‚Äôoauth, ses √©volutions et ses √©cueils. Le constat √©tant que m√™me si de nombreux probl√®mes connus ont √©t√© r√©solus, aujourd‚Äôhui, pour bien utiliser OAuth 2, il faut lire une douzaine de RFC et s‚Äôassurer qu‚Äôelles soient pertinentes pour les diff√©rents cas d‚Äôutilisation.\n\nL‚Äôaugmentation de la complexit√© du protocole d√©grade l‚Äôexp√©rience du d√©veloppeur, ce qui va √† l‚Äôencontre de son objectif principal qui est la simplicit√© pour les d√©veloppeurs de clients.\n\nGNAP (prononc√© ‚Äúnap‚Äù) est une compl√®te r√©√©criture afin de r√©pondre aux besoins en s√©curit√© des applications modernes¬†:\n\n  Pens√© pour tous les clients, plateformes (pas uniquement web, possibilit√©s de deeplinking mobile par exemple)\n  les interactions sont un concept cl√©\n  Du chiffrement partout et des m√©canismes de rotation extensibles\n  Plusieurs Access Tokens / grant request\n  Gestion de l‚Äôidentit√© int√©gr√©e\n  Plus developer friendly\n  Pas r√©trocompatible avec OAUTH2\n\n\nCe protocole est toujours √† l‚Äô√©tat de brouillon, le groupe de travail a √©t√© mont√© en octobre 2020 et lors du dernier rassemblement (nov. 2022), aucune modification du protocole n‚Äôa √©t√© act√©e. Le speaker conclut sur la n√©cessit√© de commencer √† travailler sur l‚Äôimpl√©mentation de ce protocole dans l‚Äô√©cosyst√®me PHP afin de supporter ce nouveau standard dont la finalisation ne devrait plus tarder. \nPour aller plus loin https://oauth.xyz/\n\nA self-training journey to the certification Symfony\nCette conf√©rence traite de la m√©thodologie et des bonnes pratiques pour obtenir la fameuse certification Symfony.\nEn effet, la conf√©renci√®re, Camille Jouan, nous pr√©sente sa mani√®re de pr√©parer l‚Äôexamen.\nElle commence par √©noncer son plan d‚Äôaction :\n\n  Rassembler un maximum d‚Äôinformations (Symfony doc, site pour la pr√©paration √† la certification, etc)\n  Organiser un plan autour du quoi/comment/pourquoi\n  Faire une timeline avec les √©tapes pr√©vues\n  Se fixer un objectif dans le temps\n\n\nL‚Äôid√©e est d‚Äôaccepter que cela ne sera pas parfait et que des retouches vont y √™tre apport√©es\n\nPour Camille, l‚Äôid√©al serait de pouvoir faire un test blanc au bout d‚Äôun certain temps de pr√©paration sans ‚Äúgrandes convictions‚Äù : le but √©tant de se familiariser avec l‚Äôexercice.\nSi les fonds sont disponibles, un training est propos√© par Sensiolabs.\nEn fonction de cet examen blanc, ajuster son plan, se concentrer sur des parties qui doivent √™tre approfondies.\nUn autre point sur lequel la conf√©renci√®re a insist√© est le monitoring : r√©guli√®rement faire un bilan sur son avanc√©e pour s‚Äôadapter.\nDes outils comme Trello, Excel, Google permettent d‚Äôen avoir une vision globale.\n\nIl est important de parler de ce projet autour de soi, notamment aupr√®s de ses proches pour avoir du soutien, mais √©galement aupr√®s de son entreprise qui peut √©ventuellement proposer une subvention et ou un am√©nagement du temps de travail.\n\nElle conclut son intervention par un dernier conseil : cette m√©thodologie est adaptable √† la vie quotidienne et peut √™tre utile dans d‚Äôautres situations.\n\nNotre retour d‚Äôexp√©rience\nCette nouvelle √©dition de la SymfonyCon nous a permis de d√©couvrir ou d‚Äôapprofondir certaines connaissances. \nNous pouvons aussi nous rendre compte de notre travail quotidien et prendre du recul sur celui-ci.\nCette exp√©rience anglophone √©tait tr√®s enrichissante et les conf√©rences propos√©es √©taient vari√©es.\nIl y avait de la r√©solution de probl√®mes techniques, des retours d‚Äôexp√©riences ou encore de la t√©l√©m√©trie.\n\n\n"
} ,
  
  {
    "title"    : "Un onboarding facilit√© gr√¢ce √† la revue de code!",
    "category" : "",
    "tags"     : " team",
    "url"      : "/2022/11/15/onboarding-revue-code.html",
    "date"     : "November 15, 2022",
    "excerpt"  : "Au sein des √©quipes de d√©veloppement, une activit√© bien connue est celle de la revue de code, et \nplus \npr√©cis√©ment de la revue du delta du code. Il s‚Äôagit de l‚Äôinspection par nos \npairs du code propos√© par nos soins, qui se trouve ainsi comment√© ...",
  "content"  : "Au sein des √©quipes de d√©veloppement, une activit√© bien connue est celle de la revue de code, et \nplus \npr√©cis√©ment de la revue du delta du code. Il s‚Äôagit de l‚Äôinspection par nos \npairs du code propos√© par nos soins, qui se trouve ainsi comment√© pour r√©pondre aux \nexigences de qualit√© de l‚Äô√©quipe et du projet.\n\nLes risques d‚Äôincompr√©hensions inh√©rents √† la communication √©crite, de malentendus\nou encore les remarques malheureuses peuvent rendre cet exercice redout√© tant par les \nrelecteurs et relectrices \nque \npar celles et ceux dont le code est relu.\n\nAvant d‚Äôarriver chez Bedrock, j‚Äô√©tais un peu \ninqui√®te. Je savais d√©j√† que les 9 personnes de ma future √©quipe font tou(te)s de la revue de code. \nComment √©changerons-nous? Saurai-je faire ‚Äúbonne impression‚Äù via mes commentaires \nsur leur code ?\n\nEn arrivant, j‚Äôai √©t√© tr√®s \nagr√©ablement surprise de d√©couvrir que l‚Äô√©quipe applique un \nstandard, celui des conventions de commentaires, ou conventional comments. Gr√¢ce √† cela, mon \nonboarding a √©t√© grandement facilit√© et j‚Äôai d√©couvert une fa√ßon plus efficace d‚Äô√©crire mes \ncommentaires !\n\nDisclaimer : cet article est inspir√© de ma conf√©rence ‚ÄúRevue de code : on n‚Äôest pas \nvenu-e-s pour\nsouffrir !‚Äù donn√©e √† l‚Äôoccasion du meet-up anniversaire Duchess chez Dataiku en 2022 et au Forum\nPHP 2022.\n\n\n\nPetit rappel : pourquoi faisons-nous des revues de code ?\n\nPasser en revue le code propos√© par ses co-√©quipier(e)s est largement r√©pandu dans les \n√©quipes de d√©veloppeurs et d√©veloppeuses. Bien s√ªr, la qualit√© du code en elle-m√™me se trouve \nam√©lior√©e \ncar chacun apporte un regard neuf sur ce qui est propos√©, mais ce n‚Äôest pas tout. La \nrevue de code est √©galement une fa√ßon de nous tenir inform√©(e) de \nl‚Äôimpl√©mentation de nouvelles features, d‚Äôapprendre autant du m√©tier que de la \ntechnique et enfin, d‚Äôapprendre √† travailler ensemble.\n\n\n\nVoici une \npetite liste non exhaustive de l‚Äôint√©r√™t de la revue de code :\n\n\n  Am√©liorer la qualit√© et la lisibilit√© du code gr√¢ce aux remarques de toutes les\npersonnes de l‚Äô√©quipe\n  Appliquer les standards adopt√©s par l‚Äô√©quipe (et les apprendre !)\n  D√©tecter et corriger les √©ventuels bugs fonctionnels\n  Favoriser la collaboration en √©quipe\n  Former les d√©veloppeurs et d√©veloppeuses au fur et √† mesure des remarques\n  Partager les responsabilit√©s : en approuvant une pull request ou une merge request, nous\nsommes responsables en tant qu‚Äô√©quipe du code ajout√©/modifi√© au tronc commun !\n\n\n‚Ä¶Et parfois, on souffre\n\n\n\nMais parfois, ce n‚Äôest pas tout rose. Les commentaires qu‚Äôon laisse peuvent vexer. On \npeut nous-m√™me √™tre vex√©. Car certains jours, on peut manquer d‚Äôempathie. On peut avoir \nl‚Äôimpression d‚Äô√™tre plus comp√©tent(e) en \ncritiquant les autres, on veut se rassurer en se montrant plus qualifi√©(e). On peut √©galement \n√™tre habitu√©(e) √† une culture de la comp√©tition, nous poussant ainsi √† faire des remarques d√©sagr√©ables √† nos pairs.\n\nComment le formatage de commentaire a-t-il am√©lior√© mon arriv√©e dans l‚Äô√©quipe ?\n\nLa standardisation des commentaires a √©norm√©ment am√©lior√© mon int√©gration dans \nl‚Äô√©quipe. En effet, gr√¢ce √† cela :\n\n\n  J‚Äôai pu rapidement me rendre compte de ce qui √©tait bloquant / non bloquant et ainsi me \nconcentrer sur les actions essentielles et prioritaires √† mener;\n  je n‚Äôai pas eu √† me poser de questions sur le ton employ√© par mes coll√®gues ni sur leur \nintention;\n  j‚Äôai pu rapidement faire moi-m√™me des revues de code sans craindre d‚Äô√™tre mal comprise;\n  j‚Äôai eu des retours qui m‚Äôont permis de progresser sur la connaissance du fonctionnel et \ndes standards de la team.\n\n\nAm√©liorer sa posture\n\nAvant de parler du standard, je vous propose de nous interroger sur \nnotre posture en tant que d√©veloppeur et d√©veloppeuse. Recevoir ou donner des commentaires, ce \nn‚Äôest pas ais√© pour tous. Notre ego peut interf√©rer et d√©grader la qualit√© de nos \n√©changes avec nos coll√®gues. Aussi, avant de chercher √† formater nos commentaires, nous pouvons \nnous interroger sur leur contenu.\n\n\n\nL‚ÄôEgoless Programming, propos√© par Gerald Weinberg en 1971 dans son livre The Psychology of \nComputer Programming, pr√©sente une dizaine de commandements pour nous \naider √† progresser.\n\nLe principe est le suivant : r√©duire au minimum les facteurs personnels lors des interactions \navec ses pairs, pour favoriser le travail en √©quipe et produire le maximum de qualit√©.\n\n\n  Critiquez le code au lieu des personnes,\n  Soyez factuels sur le code,\n  N‚Äôattaquez jamais les personnes.\n\n\nJe vous recommande de regarder cette excellente conf√©rence sur l‚Äôegoless programming, o√π Olivier \nThelu prend le temps de revenir sur tous les concepts :\n\n\nLes 10 commandements de la programmation sans &amp;eacute;go - Olivier Thelu - MiXiT 2022 from MiXiT on Vimeo.\n\nUne autre excellente conf√©rence de Kim La√Ø Trinh, lead d√©veloppeur, et son ‚ÄúAuto-critique de la \nrevue de code\nbienveillante‚Äù.\n\n\n\nFormatez vos commentaires !\n\nUne fois qu‚Äôon a adopt√© une posture qui nous aide √† mieux recevoir et donner des commentaires dans le cadre de nos revues de code, on peut r√©fl√©chir √† la fa√ßon dont on les formate.\n\nGr√¢ce au standard des conventional comments, nous disposons d‚Äôune convention pour √©crire des \ncommentaires clairs et visuels et limiter les incompr√©hensions. Chacun de nous est invit√© \n√† r√©fl√©chir √† l‚Äôintention de son commentaire avant de l‚Äô√©crire.\n\nPar exemple, avec ce commentaire qui peut pr√™ter √† confusion (le OMG qui signifie ‚ÄúOh my god‚Äù \npeut √™tre autant interpr√©t√© comme quelque chose de n√©gatif que de positif, notamment ici puisque \nnous n‚Äôavons pas le contexte üòà) :\n\n\n\nCe commentaire peut √™tre pr√©fix√© par praise, ce qui signifie √©loge. Cela change radicalement \nle ton du commentaire.\n\n\n\nVoici un autre exemple laconique : Poubelle.\n\n\n\nCelui-ci peut √™tre am√©lior√© en √©tant pr√©fix√© par l‚Äô√©tiquette nitpick, qui signifie \n‚Äútatillonner‚Äù, ce qui diminue √©galement son ton dramatique. De plus, l‚Äôurgence peut √™tre \nindiqu√©e (ici, non-bloquant) et le contexte est d√©crit et peut √™tre exploit√© gr√¢ce √† un patch \nproposant un code de remplacement.\n\n\n\nLa compr√©hension du commentaire est facilit√©e par l‚Äôeffort fourni pour ajouter le maximum de\ncontexte possible. On gagne en lisibilit√© gr√¢ce √† la cat√©gorisation (√©tiquette), qui nous permet\n√©galement d‚Äôimm√©diatement savoir de quoi on parle. Par exemple, une question ne sera pas lue\nde la m√™me fa√ßon qu‚Äôune suggestion !\n\nLa contextualisation permet de savoir si on traite le retour imm√©diatement ou si on ouvre une\nnouvelle pull request plus tard, pour r√©medier au point soulev√©. On limite ainsi les\nquiproquos ou les pertes de temps sur des actions non prioritaires.\nEt surtout, on limite les mauvaises impressions sur le ton employ√©.\n\nDescription du standard\n\n&amp;lt;label&amp;gt; [decorations]: &amp;lt;subject&amp;gt;\n\n[discussion]\n\n\n\n  √©tiquette (label) : ‚Äú√©tiquette‚Äù pour signifier de quel genre de commentaire il s‚Äôagit\n  sujet (subject) : le commentaire en lui-m√™me\n  contexte suppl√©mentaire (decorations) (optionnel)  : labels suppl√©mentaires pour donner plus d‚Äôindications (entre parenth√®ses, s√©par√©s par des virgules).\nExemple : non-blocking, blocking, test ‚Ä¶\n  discussion (optional) : contexte, raisonnement ou tout autre √©l√©ment pour aider √† \ncomprendre le ¬´ pourquoi ¬ª et les ¬´ prochaines √©tapes ¬ª pour r√©soudre le commentaire\n\n\nLes labels\n\nVoici la liste de labels ou √©tiquettes, extraits du standard :\n\n\n  praise\n  nitpick\n  suggestion\n  issue\n  todo\n  question\n  thought\n  chore\n  typo\n  polish\n  quibble\n\n\nL‚Äô√©quipe est, bien entendu, libre de choisir ou d‚Äôinventer ses labels ! Chez nous, le choix a √©t√© \nfait de respecter le standard tel qu‚Äôil est propos√©, mais cela pourrait √™tre rediscut√© si besoin.\n\nVoici quelques d√©finitions (pour le reste, se r√©f√©rer au site du standard):\n\nPraise (√©loge)\n\nGr√¢ce √† ce commentaire, on souligne quelque chose de positif, on encourage la personne. Bien \nentendu, pas de second degr√© !\n\nSuggestion (suggestion)\n\nLes suggestions sont la majorit√© des commentaires qu‚Äôon laisse, en g√©n√©ral. Il s‚Äôagit \nd‚Äôam√©liorations √† apporter au sujet actuel. On cherchera √† √™tre explicite et clair,\n\n\n  expliquer en quoi il s‚Äôagit d‚Äôune am√©lioration;\n  utiliser des patchs;\n  utiliser des d√©corations blocking ou non-blocking.\n\n\nIssue (probl√®me)\n\nGr√¢ce aux issues, on met en √©vidence des probl√®mes sp√©cifiques. Id√©alement, on couple ce \ncommentaire avec une Suggestion.\n\nThought (pens√©e)\n\nLes pens√©es sont des id√©es qui surgissent lors de la relecture du code. Celles-ci ne sont pas \nbloquantes par nature, mais sont extr√™mement pr√©cieuses, car elles peuvent conduire √† des \npossibilit√©s de mentorat.\n\nAppropriez-vous la m√©thode !\n\nBien entendu, vous n‚Äô√™tes pas oblig√© d‚Äôutiliser toute la liste de labels propos√©e par le \nstandard. Vous pouvez en choisir quelques uns, ou bien carr√©ment vous en inspirer et cr√©er les \nv√¥tres. C‚Äôest le choix qu‚Äôont fait Camille et son √©quipe, qu‚Äôelle d√©crit dans cet excellent \narticle \nsur les conventional Comments et l‚Äôutilisation des emojis.\n\nAinsi, l‚Äô√©quipe a port√© son choix sur une liste d‚Äô√©tiquettes illustr√©es par des emojis, qui \ntraduisent √† fois l‚Äôintention du commentaire et son urgence.\nVoici quelques exemples, tir√©s de l‚Äôarticle :\n\nü•ú peanuts\n‚ùì question\nüí¨ discussion\nüö® alerte\nüö´ no-go\nüëè bravo\n‚ö†Ô∏è warning\n‚ò†Ô∏è bad idea\n‚ú® magic\nüî• burn-it-all\n\nQuelques autres bonnes pratiques d‚Äôonboarding\n\nBien entendu, il y a plein d‚Äôautres fa√ßons d‚Äôaider vos nouveaux d√©veloppeurs ou\nnouvelles d√©veloppeuses √† d√©couvrir le code. Voici quelques autres id√©es :\n\n\n  \n    On peut se familiariser avec le workflow d‚Äôune publication de pull request ou de merge \nrequest en faisant une petite modification (ajouter son nom dans un fichier, par exemple ?);\n  \n  \n    on peut √™tre accompagn√©(e) d‚Äôun ‚Äúbuddy‚Äù qui nous est assign√© √† l‚Äôarriv√©e dans l‚Äôentreprise avec \nqui on fait les premi√®res revues de code en direct, et pas par √©crit.\n  \n\n\nUne derni√®re bonne pratique tr√®s largement r√©pandue : si les √©changes par commentaires sont trop \nnombreux sur une m√™me pull request, pourquoi ne pas se retrouver directement et r√©soudre en pair \nles \npoints discut√©s ?\n"
} ,
  
  {
    "title"    : "Forum PHP 2022 - L‚Äô√©l√©phant bleu n‚Äôa pas peur de la souris aux grandes oreilles",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2022/10/28/forum-php-afup-2022.html",
    "date"     : "October 28, 2022",
    "excerpt"  : "Pour cette √©dition du Forum PHP qui s‚Äôest d√©roul√©e √† Disneyland Paris, Bedrock a vu les choses en grand : premi√®re fois\nsponsor Or sur un forum et pas moins de 32 Bedrockien(ne)s pr√©sent(e)s dont 4 en tant que conf√©rencier(e)s !\nCette ann√©e encore...",
  "content"  : "Pour cette √©dition du Forum PHP qui s‚Äôest d√©roul√©e √† Disneyland Paris, Bedrock a vu les choses en grand : premi√®re fois\nsponsor Or sur un forum et pas moins de 32 Bedrockien(ne)s pr√©sent(e)s dont 4 en tant que conf√©rencier(e)s !\nCette ann√©e encore, le forum a √©t√© un moment privil√©gi√© pour les √©changes, le partage et vous avez √©t√© nombreux(ses) √†\nvenir nous rencontrer sur notre stand et nous avons √©t√© ravis de pouvoir √©changer avec vous.\n\nLa richesse et la diversit√© des conf√©rences ont fait de cette √©dition une grande r√©ussite. Nous ne pouvons\nmalheureusement pas aborder toutes les conf√©rences dans cet article, mais voici une s√©lection de 10 d‚Äôentre elles.\n\n\n\nBedrock au Forum PHP\n\nGrande premi√®re pour nous cette fois-ci : Bedrock avait un stand sur le forum PHP !\nNotre √©quipe technique, ainsi que des membres de l‚Äô√©quipe RH a particip√© √† la pr√©sentation de\nl‚Äôentreprise. Nous avons pu, tous ensemble, vous accueillir sur le stand pour r√©pondre aux questions sur notre activit√©,\nnotre organisation et la vie dans l‚Äôentreprise.\n\nDe leurs propres mots :\n\n\n  ‚ÄúNotre pr√©sence au forum fut une r√©ussite collective, qui nous a permis de nous faire conna√Ætre et mettre en lumi√®re BEDROCK en tant que structure √† part enti√®re, et non plus en tant que ¬´ M6 Web ¬ª.\nNos √©quipes √©taient ravies que Bedrock soit sponsor et ont pu se challenger sur de r√©els ‚Äúcasses-t√™tes‚Äù et non pas sur leur code.\nDurant ces deux journ√©es, le stand Bedrock a √©t√© un point de rendez-vous agr√©able pour nos √©quipes et pour les autres participants curieux d‚Äôapprendre √† nous conna√Ætre.\nCette participation, au-del√† d‚Äô√™tre tr√®s formatrice pour la communaut√© Bedrock, a m√™me parfois √©t√© assimil√©e √† un ¬´ team building ¬ª pour reprendre les termes de nos collaborateurs, leur permettant de se retrouver dans un autre environnement et de participer √† l‚Äôattractivit√© de leur entreprise.\nLes √©quipes Backend &amp;amp; RH √©taient fi√®res de repr√©senter Bedrock en participant √† la vie du stand et de r√©pondre aux questions des passants.\n\n\n\n\nLes conf√©rences qu‚Äôon retient\n\nJour 1\n\nThe PHP Foundation: The past, the present, and the future - Sebastian BERGMANN, Roman PRONSKIY\n\n\n  ‚ÄúPHP c‚Äôest simple, suffit de d√©gommer un mec, et √ßa n‚Äôexiste plus‚Ä¶‚Äù\n                                  \t- Chuck Norris -\n\n\nCette vision √† peine exag√©r√©e de l‚Äôami Chuck √©tait assez proche de la r√©alit√© jusqu‚Äô√† l‚Äôann√©e derni√®re. En effet,\njusque-l√†, l‚Äô√©crasante majorit√© de la connaissance internet et des impl√©mentations des features de PHP reposaient sur\nles √©paules de Nikita Popov et Dmitry Stogov. Lesquels faisaient cela de mani√®re non-professionnelle, comme un hobby,\nalors que PHP est aujourd‚Äôhui utilis√© pour environ 70% des sites internets.\n\nTout ceci est r√©sum√© dans cet article (en anglais).\n\nCe qui nous am√®ne √† un principe bien connu et tr√®s probl√©matique de bon nombre d‚Äôentreprises : le Bus Factor.\n\nBus Factor\n\nCe principe identifie le nombre de personnes qui doivent √™tre renvers√©es par un bus pour que la connaissance d‚Äôun\nprojet disparaisse et que ce dernier soit r√©ellement mis en p√©ril. Appliqu√© √† PHP, cela signifie que si deux\npersonnes avaient √©t√© renvers√©es par un bus ou avaient crois√© le chemin de Chuck, l‚Äôensemble de la communaut√© PHP\naurait bien transpir√©.\n\nLa PHP Foundation\n\nEn 2021, Nikita Popov annonce son souhait de r√©duire drastiquement sa contribution au langage, et c‚Äôest l√† que\nla PHP Fundation est fond√©e autour de personnalit√©s comme Sebastian BERGMAN et\nRoman PRONSKIY qui ont anim√© cette conf√©rence √† laquelle nous avons eu la chance d‚Äôassister.\n\nL‚Äôobjectif est simple : r√©colter des fonds pour pouvoir r√©mun√©rer des personnes qui font √©voluer le langage et\nr√©partir la connaissance dudit langage entre un maximum de personnes pour minimiser le Bus Factor\n\nL‚Äôaspect juridique et financier de la chose est d√©l√©gu√© √† une structure\nnomm√©e Open Collective, dont l‚Äôactivit√© est 100% transparente.\nLes derniers chiffres font ainsi valoir qu‚Äôapr√®s d√©ductions des charges, il reste cette ann√©e environ 580 000$ pour la\nfondation.\n\nLes fonds proviennent majoritairement de donations r√©partis comme suit :\n\n  77% provenant d‚Äôentreprises (JetBrains, Livesport, Symfony‚Ä¶)\n  23% provenant d‚Äôindividus\n\n\nParmi les actuels core contributeurs on retrouve :\n\n  Arnaud Le Blanc\n  Derick Rethans\n  George P. Banyard\n  Ilija Tovilo\n  Jakub Zelenka\n  M√°t√© Kocsis\n\n\nLe futur de PHP ?\n\nAvec cette strat√©gie de contractualisation des core-contributeurs et le souci de partager la connaissance, on est en\ndroit d‚Äôesp√©rer que le risque de mise en p√©ril du langage soit fortement r√©duit. Qui plus est, l‚Äôarriv√©e prochaine de\nla version 8.2 de PHP semble montrer que la nouvelle √©quipe a su faire avancer le projet, pour notre plus grand plaisir.\n\nWatch the clock - Andreas HEIGHL\n\nDans cette conf√©rence, Andreas aborde la douloureuse probl√©matique du temps en PHP\net plus pr√©cis√©ment le cauchemar que sont les tests sur la notion de temps r√©el.\n\nAfin de r√©pondre √† cette probl√©matique, il √©voque et explique en d√©tail tout le cheminement intellectuel pour la mise en\nplace de la PSR-20. Elle donnera la possibilit√©, gr√¢ce √† une ‚Äúsimple‚Äù interface, de\ng√©rer plus facilement le temps r√©el et les tests associ√©s. Malheureusement, la PSR-20 est actuellement encore √† l‚Äô√©tat\nde brouillon et ne sera probablement pas mise √† disposition avant longtemps.\n\nAfin de nous faire patienter, Andreas a d√©velopp√© sa propre impl√©mentation de l‚Äôinterface Clock qui sera propos√©e dans\nla PSR-20 : https://packagist.org/packages/stella-maris/clock.\n\nComment √™tre bien onboard√© en tant que d√©veloppeuse junior reconvertie ? - Am√©lie ABDALLAH\n\nAm√©lie, alternante et en reconversion, a donn√© sa premi√®re conf√©rence pour faire un retour d‚Äôexp√©rience des onboardings\nqu‚Äôelle a v√©cu dans ses 2 premi√®res entreprises.\n\nPour sa premi√®re exp√©rience en tant que d√©veloppeuse elle se retrouve √† devoir coder d√®s le premier jour. Une fois\npass√©e l‚Äôeuphorie, je rappelle qu‚Äôelle est en reconversion et sur-motiv√©e, elle se rend vite compte de tout ce qui ne\nva pas :\n\n\n  personne pour la former que ce soit techniquement ou concernant le m√©tier\n  aucune pr√©sentation des autres √©quipes\n  aucun accompagnement de la part des sup√©rieurs hi√©rarchiques\n  seule sur un projet o√π les r√®gles m√©tiers semblent complexes\n\n\nElle se sent rapidement perdue et son moral et sa motivation tombent en fl√®che.\n\nConcernant sa seconde entreprise, c‚Äôest tout l‚Äôinverse :\n\n\n  pas de code pour les alternants/reconverti(e)s avant 2 semaines\n  un syst√®me de marrainage/parrainage\n  un repo avec des exercices et bonnes pratiques pour progresser\n  une rencontre avec toutes les √©quipes pour se conna√Ætre mais surtout pour bien comprendre les diff√©rents m√©tiers de ses coll√®gues\n\n\nAm√©lie conclut en incitant toutes les entreprises √† envisager des candidat(e)s en reconversion qui, √† la condition\nd‚Äô√™tre bien accompagn√©(e)s, seront sur-motiv√©(e)s et plein(e)s d‚Äô√©nergie.\n\nCela nous permet de faire l‚Äô√©tat des lieux de notre processus d‚Äôonboarding. M√™me si on peut trouver des pistes\nd‚Äôam√©lioration, comme par exemple avoir une pr√©sentation des autres √©quipes et diff√©rents m√©tiers bien plus t√¥t, on\nconstate que nous avons d√©j√† mis en place beaucoup de bonnes pratiques :\n\n\n  calendrier d‚Äôonboarding clair avec installation de la machine, pr√©sentation de l‚Äô√©quipe rejoint par la nouvelle personne, des locaux, des projets, etc.\n  syst√®me de marrainage/parrainage,\n  des formations disponibles sur les diff√©rentes technologies utilis√©es\n  ce qu‚Äôon appelle la Bedrock Academy, qui donne une vision globale des m√©tiers de l‚Äôentreprise\n  un rapport d‚Äô√©tonnement pr√©sent√© par le nouvel arrivant qui nous permet d‚Äôam√©liorer notre onboarding en continu\n\n\nFFI : De nouveaux horizons pour PHP - Pierre PELISSET\n\nChez Karafun, Pierre Pelisset nous pr√©sente un moyen de d√©passer les fronti√®res habituelles de PHP.\n\nLa mise en place de bar √† karaok√© n√©cessite en effet de manipuler du mat√©riel sp√©cifique. La transmission de donn√©es par\nUSB reste le moyen le plus simple de faire communiquer des syst√®mes, mais nativement rien n‚Äôexiste en PHP. Jusqu‚Äô√†\npr√©sent les √©quipes de Karafun utilisaient un script Python avec PySerial pour r√©pondre √† ce besoin.\n\nDepuis PHP 7.4, il est possible d‚Äôutiliser les Foreign Function Interface (FFI)\npour appeler directement des libraires C compil√©es depuis le code PHP. En se basant sur les bonnes librairies, il a √©t√©\nainsi possible pour les √©quipes de Karafun d‚Äôutiliser PHP dans la totalit√© de leur stack, et de remplacer leurs scripts\nPython.\n\nPierre nous pr√©sente ensuite la librairie php-termios qu‚Äôil a impl√©ment√©e\npour permettre d‚Äôutiliser Termios, une librairie de manipulation de terminal POSIX √©crite en C, depuis PHP.\n\nL‚Äôinterfa√ßage reste facile tant qu‚Äôil s‚Äôagit de types de donn√©es simples (comme des entiers), mais devient beaucoup plus\ncomplexe lorsqu‚Äôil s‚Äôagit de manipuler des structures de donn√©es avanc√©es (cha√Ænes de caract√®res, objets, ‚Ä¶).\n\nId√©alement, il faut cr√©er des classes PHP ressemblant √† la librairie C (m√™mes noms de fonction, de constante, ‚Ä¶), afin\nde faciliter sa manipulation.\n\nL‚Äôutilisation des FFI introduit de nouvelles difficult√©s li√©es g√©n√©ralement aux programmes compil√©s. Ainsi, dans le cas\nde Termios, les constantes utilis√©es varient selon la plateforme (Linux, MacOS, ‚Ä¶). Il a donc fallu utiliser quelques\nastuces pour les copier dynamiquement dans le code PHP. De m√™me, pour distribuer le binaire compil√©, il faut prendre en\ncompte l‚Äôarchitecture CPU de la machine ex√©cutant le code.\n\nOpenTelemetry : vers un standard pour surveiller vos applications - Benoit Viguier\n\nLors de sa conf√©rence, Benoit Viguier (d√©veloppeur chez Platform.sh au sein de l‚Äô√©quipe de Blackfire.io) est venu parler\nd‚Äôun standard de monitoring soutenu par la Cloud Native Computing Foundation : OpenTelemetry. Il a d‚Äôabord commenc√© par\nnous rappeler pourquoi nous faisons du monitoring.\n\nLe monitoring nous permet de savoir si nos services fonctionnent correctement (conforme au SLA|SLO), et si cela n‚Äôest\npas le cas, de savoir pourquoi cela ne fonctionne pas. Pour faire cela, il nous a pr√©sent√© les diff√©rentes solutions de\nmonitoring qui existent, de la plus simple √† mettre en place (Analytics), au plus|trop d√©taill√© (logs) mais aussi via\ndes metrics qui eux demandent une plus grande int√©gration.\n\nEnsuite vient la pr√©sentation des promesses de ce nouveau standard (OpenTelemetry) et sa volont√© d‚Äôuniformiser les trois\npiliers du monitoring : logs, metrics et les traces, avec la volont√© de rendre interop√©rable ces donn√©es collect√©es avec\nn‚Äôimporte quel service et ce qu‚Äôimporte le langage.\n\n\n\nAfin de permettre cette interop√©rabilit√© pour la collecte, le traitement et l‚Äôenvoi des donn√©es entre nos applications\net nos APMs, OpenTelemetry propose l‚Äôutilisation d‚Äôun collecteur. Ce collecteur poss√®de trois composants par lesquels\nnotre donn√©e va transiter :\n\n\n  Receiver - G√®re la r√©cup√©ration des donn√©es dans le collecteur. Il fonctionne aussi bien avec une m√©canique de push que de pull et supporte nativement les protocoles HTTP et gRPC.\n  Processor - Permet le traitement des donn√©es avant l‚Äôenvoi aux diff√©rents outils de monitoring.\n  Exporter - Envoie les donn√©es (via push ou pull) aux outils de monitoring.\n\n\nConcernant l‚Äôutilisation du standard OpenTelemetry en PHP, il existe aujourd‚Äôhui un SDK qui, cependant, ne poss√®de\npas beaucoup de fonctionnalit√©s.\n\nMalgr√© son jeune √¢ge (release v1.0 en 2021), OpenTelemetry a encore de beaux jours devant lui si la contribution et le\nsoutien de la communaut√© continuent. Il est important de rappeler que le standard n‚Äôest pas encore finalis√©, par exemple\nla fonctionnalit√© ‚Äúlogging‚Äù est encore en cours de d√©veloppement et n‚Äôest disponible qu‚Äôen ‚Äúdraft‚Äù.\n\nJour 2\n\nTypage en PHP, comment √ßa fonctionne ? - George BANYARD\n\nLors de cette dissection du typage PHP, George Banyard, d√©veloppeur salari√© de la PHP Foundation, nous a expliqu√© (ou du\nmoins tent√©) comment fonctionne le typage PHP gr√¢ce √† des formules math√©matiques. Mais avant de nous faire peur avec les\nformules, il nous d‚Äôabord rappel√© les diff√©rents types existants et futurs (PHP 8.1 et 8.2) et comment ils sont\nrepr√©sent√©s en C (zend_type, _zval_struct, _zend_class_entry)\n\n\n  Types Primitifs\n  Types d√©finis en espace utilisateur (classe, interfaces, enum)\n  Types Litt√©raux (false, true)\n  Type callable\n  \n    \n      \n        \n          Types Compos√©s (A&amp;amp;B, A\n          B, Forme Normale Disjonctive)\n        \n      \n    \n  \n  Alias de Type (PHP 8.2)\n\n\nEnsuite, il nous a expliqu√© le principe de substitution de Liskov de diff√©rente mani√®re.\n\n  Si œÜ(x) est une propri√©t√© d√©montrable pour tout objet x de\ntype T, alors œÜ(y) est vraie pour tout objet y de type S tel que\nS est un sous-type de T\n\n\nMath√©matique, en C mais aussi en image :\n\n\n\nEn version tr√®s simplifi√©e, la substitution de Liskov permet de substituer un type par un autre type s‚Äôil est\nmieux-disant. C‚Äôest par exemple sur ce principe que sont fond√©es la co-variance et la contra-variance en PHP.\n\nApr√®s toutes ces informations, George nous a fait r√™ver avec les nouveaut√©s qui pourraient arriver dans notre langage pr√©f√©r√© :\n\n\n  La possibilit√© de d√©finir nos alias de type (numeric qui serait int ou float)\n  Pouvoir d√©finir un typage pour les param√®tres et le retour des callable directement dans la fonction : foo(fn&amp;lt;int,string&amp;gt;:bool $callable) {}\n  Le param√®tre in-out qui permet de v√©rifier que le type ne change pas entre l‚Äôentr√©e et la sortie d‚Äôune fonction, utile notamment dans le cas de passage par r√©f√©rence\n  Pouvoir cr√©er des types g√©n√©riques (par exemple class Collection&amp;lt;string&amp;gt;)\n\n\nVous pouvez retrouver les slides de la conf√©rence ici\n\nProt√©ger votre application avec l‚Äôen-t√™te HTTP de s√©curit√© ‚ÄúContent Security Policy‚Äù - Laurent BRUNET\n\nDurant ce talk, Laurent BRUNET nous a rappel√© l‚Äôimportance de s√©curiser les sites internet en nous parlant des attaques\nles plus utilis√©es et dont on pourrait se d√©fendre en exploitant correctement le header de r√©ponse Content-Security-Policy (aka. CSP).\n\nConcr√®tement, le header CSP est d√©livr√© par le serveur en m√™me temps que le HTML d‚Äôune page web et contraint les\nnavigateurs √† s‚Äôassurer que le contenu de la page respecte les r√®gles de s√©curit√© d√©finies gr√¢ce √† de nombreuses directives.\n\nIl existe 2 variantes de ce header qui peuvent √™tre utilis√©es conjointement :\n\n\n  une forme bloquante qui emp√™chera tous contenus illicites d‚Äô√™tre charg√©s par le navigateur\nContent-Security-Policy: &amp;lt;directive1&amp;gt; ;  &amp;lt;directive2&amp;gt; ;  &amp;lt;directiveN&amp;gt;\n  une forme non-bloquante qui permet uniquement aux d√©veloppeurs d‚Äô√™tre avertis si du contenu illicite est pr√©sent sur la page\nContent-Security-Policy-Report-Only: &amp;lt;directive1&amp;gt; ;  &amp;lt;directive2&amp;gt; ;  &amp;lt;directiveN&amp;gt;\n\n\nEt voici quelques unes des failles de s√©curit√© √©voqu√©es durant ce talk, chacune accompagn√©e d‚Äôune des directives permettant de s‚Äôen prot√©ger :\n\n\n  on peut se prot√©ger des attaques XSS (e.g. injection de scripts dans une page web permettant d‚Äôexploiter les acc√®s d‚Äôun utilisateur) en whitelistant les scripts autoris√©s √† s‚Äôex√©cuter dans votre application avec script-src &#39;nonce-MonIDDeScript&#39; &#39;sha256-MonHashDeScript&#39; &#39;strict-dynamic&#39; https: &#39;unsafe-inline&#39; *.example.com ;\n  le Clickjacking (e.g. injection d‚Äôiframe invisible dans laquelle l‚Äôutilisateur va cliquer sans le savoir) peut √™tre facilement contr√© en whitelistant vos iframes et celles de vos partenaires avec frame-ancestors &#39;self&#39; https://example.com ;\n  la faille Magecart (e.g. r√©cup√©ration des donn√©es bancaires en les envoyant vers un nom de domaine pirate difficile √† d√©tecter) dispara√Ætra en whitelistant les noms de domaines utilis√©s dans des appels AJAX avec connect-src &#39;self&#39; https://example.com ;\n\n\nSi le sujet vous int√©resse, n‚Äôh√©sitez pas √† consulter le talk de Laurent d√®s qu‚Äôil sera disponible en replay. En\nattendant, profitez de la documentation du MDN et sachez\nque Google fournit un service en ligne pour inspecter et valider les CSP de vos\npages web.\n\nTester √† travers OpenAPI, ou comment valider votre documentation ! - St√©phane Hulard\n\nSt√©phane Hulard a commenc√© sa conf√©rence par nous rappeler ce qu‚Äôest OpenAPI : il s‚Äôagit\nd‚Äôune initiative qui vise √† normaliser et standardiser la description d‚ÄôAPIs. Cela sert √† l‚Äôinterop√©rabilit√©,\nl‚Äôautomatisation et la fiabilit√©.\n\nIl faut voir la documentation comme une sp√©cification de notre API. ‚ÄúUne documentation n‚Äôa de sens que si elle refl√®te\nl‚Äô√©tat actuel de l‚Äôapplication.‚Äù Rien de mieux donc que d‚Äôint√©grer la validation de notre documentation par rapport √†\nnotre code, et inversement : que notre documentation valide notre code !\n\n\n\nthephpleague nous propose une solution pour faire √ßa : openapi-psr7-validator.\nCe paquet peut valider les messages PSR-7 par rapport aux sp√©cifications OpenAPI (3.0.x) exprim√©es en YAML ou JSON.\n\nCe paquet se base sur les PHP Standards Recommendations (PSR) qui sont des textes d√©crivant une mani√®re commune de\nr√©soudre un probl√®me sp√©cifique. De cette fa√ßon, les projets qui suivent ces recommandations auront une excellente\ninterop√©rabilit√© en suivant les m√™mes recommandations et contrats.\n\nSt√©phane nous parle ensuite de la librairie Raven qu‚Äôil vient de publier. Raven a\npour but de faciliter la validation au travers de la documentation, de s‚Äôappuyer sur de vraies donn√©es pour valider les\nrequ√™tes et r√©ponses ainsi que valider le comportement de l‚ÄôAPI test√©e. La librairie n‚Äôen est qu‚Äô√† ses d√©buts, quelques\nissues sont remont√©es sur le repository, n‚Äôh√©sitez pas √† contribuer !\n\nSt√©phane a mis √† disposition les slides de sa conf√©rence ici\n\nFrankenPHP, dans les entrailles de l‚Äôinterpr√©teur PHP, de machines virtuelles et des threads - K√©vin DUNGLAS\n\nChez Bedrock, nous utilisons majoritairement nginx et php-fpm pour servir nos applications. Nous avons aussi fait des\nessais avec Road Runner comme alternative.\n\nDurant cette conf√©rence, K√©vin DUNGLAS nous a pr√©sent√© une nouvelle alternative √©crite en Go et nomm√©e frankenphp.\nChouette effet d‚Äôannonce au passage, FrankenPHP a √©t√© ouvert au public en direct durant la conf√©rence.\n\nEn plus d‚Äôavoir un logo tr√®s mignon, ce nouveau serveur d‚Äôapplication PHP promet un gain de performance en gardant en\nm√©moire l‚Äôapplication charg√©e. Autre argument int√©ressant, la facilit√© d‚Äôinstallation et d‚Äôutilisation.\n\nBien qu‚Äôil a plusieurs fois pr√©cis√© que ce n‚Äô√©tait pas pr√™t pour la production, cette nouvelle approche semble\nprometteuse. Il est probable que nous ne tardions pas √† l‚Äôessayer pour voir s‚Äôil est possible de gagner en performance.\n\nLes conf√©renciers Bedrock\n\n\n\nCette ann√©e, ce ne sont pas moins de 4 pr√©sentations qui √©taient donn√©es par des personnes de chez Bedrock.\n\nVous pourrez les trouver tr√®s bient√¥t en replay. Les liens seront partag√©s sur ce blog et les vid√©os disponibles\nsur la cha√Æne youtube de l‚Äôafup.\n\nEn attendant, et pour rappel, il s‚Äôagissait des conf√©rences suivantes :\n\nComprenez comment PHP fonctionne, vos applications marcheront mieux - Pascal MARTIN\n\n√Ä l‚Äô√©chelle √† laquelle nous travaillons, avec des millions de personnes sur nos plateformes tous les jours, nous d√©couvrons, atteignons, voire d√©passons r√©guli√®rement des limites de l‚Äôapproche traditionnelle de PHP et de php-fpm. Au cours de ce talk, Pascal souhaitait partager notre exp√©rience de travail avec PHP, sur des sujets souvent peu connus par les d√©veloppeurs et d√©veloppeuses, pour aider le public √† cr√©er des applications qui r√©pondent mieux aux attentes de leur public.\n\n\n  Pour ex√©cuter du code, PHP consomme du processeur et de la m√©moire. Quand une requ√™te HTTP arrive, un processus php-fpm lui est d√©di√©. Mais ces ressources sont limit√©es. Et, m√™me dans le Cloud ou en serverless, scaler prend du temps et les co√ªts s‚Äôenvolent !\n\n\n\n  Savez-vous combien de CPU et de RAM votre application r√©clame ? Et pendant quelle dur√©e ? Si non ou sans comprendre ¬´ pourquoi ¬ª, difficile de d√©velopper efficacement et de dimensionner un h√©bergement p√©renne ! Peut-√™tre que √ßa marche‚Ä¶ Sur votre poste. Ou pendant un moment, en gaspillant de l‚Äôargent et des ressources. Mais l‚Äôexp√©rience prouve que, t√¥t ou tard, ces questions vous rattraperont.\n\n\n\n  Cycle de vie de PHP, communication entre nginx et php-fpm, approche shared-nothing, compilation et cache d‚Äôopcodes, gestion interne de la m√©moire ou m√™me architecture logicielle et debugging‚Ä¶ Pour qu‚Äôune application r√©ponde aux attentes de son public, nous devons comprendre comment PHP fonctionne !\n\n\nSauve un-e d√©v, √©cris une doc ! - Sarah HA√èM-LUBCZANSKI\n\n\n  Vous √™tes d√©veloppeur ou d√©veloppeuse PHP : vous aimez programmer, r√©fl√©chir. Vous aimez cr√©er des applications ou des biblioth√®ques de qualit√©. Mais pourquoi personne ne les utilise ? Parce que votre documentation n‚Äôest pas √† la hauteur !\n\n\n\n  Justement : je suis Technical Writer et mon m√©tier est de vous aider √† valoriser votre logiciel aupr√®s de ses utilisateurs et utilisatrices, √† travers une bonne doc. Comprenons comment architecturer, concevoir et r√©diger votre contenu. D√©couvrons les outils qui vous procureront une aide pr√©cieuse. Enfin, facilitons sa mise √† jour pour qu‚Äôelle soit p√©renne.\n\n\n\n  Dor√©navant, vous saurez identifier les passages oblig√©s et ceux o√π vous pouvez gagner du temps.\n\n\nRevue de code : on n‚Äôest pas venu pour souffrir ! - Anne-Laure DE BOISSIEU\n\n\n  J‚Äôai rejoint ma nouvelle √©quipe il y a 6 mois, avec une appr√©hension. Comment allais-je vivre les revues de code par des coll√®gues que je ne connais pas encore ? Incompr√©hensions, malentendus : la communication √©crite rend cet exercice tr√®s d√©licat. Vous avez √©t√© bless√©-e par un commentaire ? Etait-il vraiment mal intentionn√© ? Vous avez bless√© quelqu‚Äôun sans le vouloir, √† cause d‚Äôune tournure maladroite ?\n\n\n\n  Dans mon √©quipe, j‚Äôai d√©couvert un cadre qui m‚Äôa permis de me sentir bien accueillie d√®s mon arriv√©e. En adoptant une posture et une convention bien adapt√©e, on peut largement diminuer le risque de mal se comprendre. Non seulement on communique mieux, mais on am√©liore la qualit√© globale du projet.\n\n\n\n  Vous n‚Äôaurez plus aucune raison de souffrir !\n\n\nBFF, notre best friend forever pour faire plein d‚Äôapplications frontend ? - Valentin CLARAS\n\n\n  Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application √©tant d√©ploy√©e sur de nombreux appareils (ordinateur, mobile, set top box, tv connect√©e, consoles de jeux, tv stick etc ‚Ä¶). Il √©tait devenu tr√®s difficile de g√©rer la cr√©ation et l‚Äô√©volution de ces nombreuses applications qui requ√™taient et formataient chacune elles-m√™mes les donn√©es dont elles avaient besoin.\n\n\n\n  Pour cela, en 2018, nous avons d√©cid√© de nous lancer dans la cr√©ation d‚Äôun Back For Front afin d‚Äôunifier et faciliter les interactions backend et frontend. Cette conf√©rence fut l‚Äôoccasion de passer en revue :\n  \n    les concepts du Back For Front\n    l‚Äôarchitecture api-gateway et micro service mise en place\n    les gains fonctionnels et la v√©locit√© gagn√©e\n    les diff√©rents m√©canismes d√©velopp√©s pour absorber les importants pics de charge (r√©silience, circuit breaker, fallbacks etc.)\n    les impacts techniques et organisationnels d‚Äôune telle architecture\n  \n\n\n\n  Aujourd‚Äôhui notre API Gateway BFF op√®re 92 frontends d√©livrant 1.5 milliards de vid√©os par an pour 45 millions d‚Äôutilisateurs actifs (MaU).\nVous pourrez trouver un compl√©ment d‚Äôinformations au sujet de notre BFF dans la suite d‚Äôarticles d√©di√©.\n\n\nPour conclure\n\nNous sommes revenus la t√™te pleine de nouvelles id√©es. Ces deux jours de conf√©rences nous ont permis de montrer le savoir faire pr√©sent chez Bedrock et nous avons aussi pu nous inspirer des connaissances d‚Äôautres personnes. Les formats vari√©s r√©pondaient aux go√ªts de chacun(e) et ont rendu ce forum unique.\n\nLes nombreuses activit√©s propos√©es entre chaque conf√©rence permettaient d‚Äô√©changer entre pairs et comme toujours la communaut√© a √©t√© mise √† l‚Äôhonneur avec la construction d‚Äôune fresque LEGO repr√©sentant tous les logos des antennes de l‚ÄôAFUP.\n\nMerci √† tou(te)s les conf√©rencier(e)s pour leur travail incroyable et merci l‚ÄôAFUP pour l‚Äôorganisation de ce superbe √©v√®nement !\n\n\n\nVivement l‚Äôann√©e prochaine !\n"
} ,
  
  {
    "title"    : "Sauve un-e d√©v, √©cris une doc !",
    "category" : "",
    "tags"     : " conference, afup, forumphp, doc",
    "url"      : "/2022/10/13/sauve-une-dev-ecris-une-doc.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Vous √™tes d√©veloppeur ou d√©veloppeuse PHP : vous aimez programmer, r√©fl√©chir. Vous aimez cr√©er des applications ou des biblioth√®ques de qualit√©. Mais pourquoi personne ne les utilise ? Parce que votre documentation n‚Äôest pas √† la hauteur !\n\nJustem...",
  "content"  : "Vous √™tes d√©veloppeur ou d√©veloppeuse PHP : vous aimez programmer, r√©fl√©chir. Vous aimez cr√©er des applications ou des biblioth√®ques de qualit√©. Mais pourquoi personne ne les utilise ? Parce que votre documentation n‚Äôest pas √† la hauteur !\n\nJustement : je suis Technical Writer et mon m√©tier est de vous aider √† valoriser votre logiciel aupr√®s de ses utilisateurs et utilisatrices, √† travers une bonne doc. Comprenons comment architecturer, concevoir et r√©diger votre contenu. D√©couvrons les outils qui vous procurerons une aide pr√©cieuse. Enfin, facilitons sa mise √† jour pour qu‚Äôelle soit p√©renne.\n\nDor√©navant, vous saurez identifier les passages oblig√©s et ceux o√π vous pouvez gagner du temps.\n"
} ,
  
  {
    "title"    : "Revue de code : on n‚Äôest pas venu pour souffrir¬†!",
    "category" : "",
    "tags"     : " conference, afup, forumphp, revue",
    "url"      : "/2022/10/13/revue-de-code-on-n-est-pas-venu-pour-souffrir.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "J‚Äôai rejoint ma nouvelle √©quipe il y a 6 mois, avec une appr√©hension. Comment allais-je vivre les revues de code par des coll√®gues que je ne connais pas encore ? Incompr√©hensions, malentendus : la communication √©crite rend cet exercice tr√®s d√©lica...",
  "content"  : "J‚Äôai rejoint ma nouvelle √©quipe il y a 6 mois, avec une appr√©hension. Comment allais-je vivre les revues de code par des coll√®gues que je ne connais pas encore ? Incompr√©hensions, malentendus : la communication √©crite rend cet exercice tr√®s d√©licat. Vous avez √©t√© bless√©-e par un commentaire ? Etait-il vraiment mal intentionn√© ? Vous avez bless√© quelqu‚Äôun sans le vouloir, √† cause d‚Äôune tournure maladroite ?\n\nDans mon √©quipe, j‚Äôai d√©couvert un cadre qui m‚Äôa permis de me sentir bien accueillie d√®s mon arriv√©e. En adoptant une posture et une convention bien adapt√©e, on peut largement diminuer le risque de mal se comprendre. Non seulement on communique mieux, mais on am√©liore la qualit√© globale du projet.\n\nVous n‚Äôaurez plus aucune raison de souffrir !\n"
} ,
  
  {
    "title"    : "Comprenez comment PHP fonctionne, vos applications marcheront mieux, Forum PHP 2022",
    "category" : "",
    "tags"     : " conference, afup, forumphp, php",
    "url"      : "/2022/10/13/comprenez-comment-php-fonctionne-vos-applications-marcheront-mieux.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Pour ex√©cuter du code, PHP consomme du processeur et de la m√©moire. Quand une requ√™te HTTP arrive, un processus php-fpm lui est d√©di√©. Mais ces ressources sont limit√©es. Et, m√™me dans Le Cloud ou en serverless, scaler prend du temps et les co√ªts s...",
  "content"  : "Pour ex√©cuter du code, PHP consomme du processeur et de la m√©moire. Quand une requ√™te HTTP arrive, un processus php-fpm lui est d√©di√©. Mais ces ressources sont limit√©es. Et, m√™me dans Le Cloud ou en serverless, scaler prend du temps et les co√ªts s‚Äôenvolent !\n\nSavez-vous combien de CPU et de RAM votre application r√©clame ? Et pendant quelle dur√©e ? Si non ou sans comprendre ¬´ pourquoi ¬ª, difficile de d√©velopper efficacement et de dimensionner un h√©bergement p√©renne ! Peut-√™tre que √ßa marche‚Ä¶ Sur votre poste. Ou pendant un moment, en gaspillant de l‚Äôargent et des ressources. Mais l‚Äôexp√©rience prouve que, t√¥t ou tard, ces questions vous rattraperont.\n\nCycle de vie de PHP, communication entre nginx et php-fpm, approche shared-nothing, compilation et cache d‚Äôopcodes, gestion interne de la m√©moire ou m√™me architecture logicielle et debugging‚Ä¶ Pour qu‚Äôune application r√©ponde aux attentes de son public, nous devons comprendre comment PHP fonctionne !\n"
} ,
  
  {
    "title"    : "BFF, notre best friend forever pour faire plein d‚Äôapplications frontend¬†?",
    "category" : "",
    "tags"     : " conference, afup, forumphp, php, bff",
    "url"      : "/2022/10/13/bff-notre-best-friend-forever-pour-faire-plein-d-applications-frontend.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application √©tant d√©ploy√©e sur de nombreux appareils (ordinateur, mobile, set top box, tv connect√©, consoles de jeux, t...",
  "content"  : "Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application √©tant d√©ploy√©e sur de nombreux appareils (ordinateur, mobile, set top box, tv connect√©, consoles de jeux, tv stick etc ‚Ä¶). Il √©tait devenu tr√®s difficile de g√©rer la cr√©ation et l‚Äô√©volution de ces nombreuses applications qui requ√™taient et formataient chacune elles-m√™mes les donn√©es dont elles avaient besoin.\n\nPour cela, en 2018, nous avons d√©cid√© de nous lancer dans la cr√©ation d‚Äôun Back For Front afin d‚Äôunifier et faciliter les interactions backend et frontend. Au cours de cette conf√©rence nous passerons en revue :\n\n\n  les concepts du back for front\n  l‚Äôarchitecture api-gateway et micro service mise en place\n  Les gains fonctionnels et la v√©locit√©s gagn√©e\n  les diff√©rents m√©canismes d√©velopp√©s pour absorber les importants pic de charge (r√©silience, circuit breaker, fallbacks etc.)\n  les impacts techniques et organisationnels d‚Äôune telle architecture\n\n\nAujourd‚Äôhui notre API Gateway BFF op√®re 92 frontends d√©livrant 1.5 milliards de vid√©os par an pour 45 millions d‚Äôutilisateurs actifs (MaU). Venez d√©couvrir notre retour d‚Äôexp√©rience sur la mise en place d‚Äôun tel projet.\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #18",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/10/07/bedrock-dev-facts-18.html",
    "date"     : "October 7, 2022",
    "excerpt"  : "C‚Äôest maintenant l‚Äôautomne üçÅüéÉ, on vous propose les devfacts de cette fin d‚Äô√©t√© et il y a du lourd !\n\nCogito ergo sum\n\n\n  Le seul truc que je ‚Äúsais‚Äù c‚Äôest ‚Äúon paye 200 par mois pour avoir un direct connect‚Äù ; mais peut-√™tre que je sais que je ne sa...",
  "content"  : "C‚Äôest maintenant l‚Äôautomne üçÅüéÉ, on vous propose les devfacts de cette fin d‚Äô√©t√© et il y a du lourd !\n\nCogito ergo sum\n\n\n  Le seul truc que je ‚Äúsais‚Äù c‚Äôest ‚Äúon paye 200 par mois pour avoir un direct connect‚Äù ; mais peut-√™tre que je sais que je ne sais rien.\n\n\nüëãüî•üôäüéâ\n\n\n  Je te fais confiance, je te laisserai mettre des emoji sur ma tombe\n\n\nLa priorit√© c‚Äôest important\n\n\n  C‚Äôest plus important que des trucs moins importants.\n\n\nBiafine n√©cessaire\n\nUn QA voulant faire une vanne au morning\n\n  Le QA: ‚ÄúQu‚Äôest-ce qui a deux lettres et qui marche pas?‚Äù\n\n  R√©ponse du dev: ‚ÄúLa QA ?‚Äù\n\n\n3615 croissants\n\nParlant d‚Äôune √©volution majeure\n\n  Est-ce que c‚Äôest OK pour vous pour qu‚Äôon MEP aujourd‚Äôhui ou pas ?\n\n  √Ä priori pas de contre-indications. Peut-√™tre acheter directement une boulangerie ?\n\n\nSugar !\n\n\n  C‚Äôest du sucre syntaxique\n\n  Quand il y a trop de sucre, tu risques du diab√®te\n\n  Surtout que le diab√®te syntaxique, c‚Äôest le pire\n\n\nLA metric\n\n\n  Je crois qu‚Äôon est bon, on a pas plus d‚Äôerreur que d‚Äôuser.\n\n\nLe verre √† moiti√© plein\n\n\n  Du coup √ßa a cass√© je suppose?\n\n  Ouais, enfin pas totalement, presque un succ√®s.\n\n\nLa montagne √ßa vous gagne\n\nEn parlant de son ascension du Ventoux √† v√©lo:\n\n\n  ‚ÄúMoi je pensais vraiment que j‚Äô√©tais pr√™t‚Äù\n\n\nMais pourquoi ?\n\nEssayant de comprendre pourquoi une commande met beaucoup de temps sur sa machine.\n\n\n  Je suis sur master, branche √† jour, yarn r√©alis√© juste avant, et je pense pas que mon PC soit particuli√®rement atteint de maladie cong√©nitale\n(Enfin, j‚Äôesp√®re pas?)\n\n\nRead me please\n\n\n  Tout est dans le README (que les gens liront pas de toute fa√ßon mais bon).\n\n\nLes √©toiles ‚≠êÔ∏è\n\n\n  √ätre responsable d‚Äôun incident, c‚Äôest pas marrant c‚Äôest s√ªr. \nMais √ßa forme !\nCasser la prod et la r√©parer √ßa donne une √©toile sur le maillot, et je peux te dire qu‚Äôon est plusieurs ici √† avoir plus d‚Äô√©toiles que le drapeau am√©ricain.\n\n\nReview main dans la main\n\n\n  La peer review, c‚Äôest la meilleure des reviews.\n\n\nPatterns\n\n\n  A : ‚ÄúJe pense que je vais devoir faire un singleton pour √ßa‚Ä¶‚Äù\n\n  B : ‚ÄúSinon, tu devrais pouvoir aussi faire des multitons ?‚Äù\n\n  C : ‚ÄúOn appelle √ßa une bo√Æte de conserve.‚Äù\n\n\n‡∂∏\n\n\n  √áa devait √™tre en base64 des Maya\n\n\nTime is relative\n\n\n  Je pense que la premi√®re fois ce sera long ‚Ä¶et apr√®s ce ne sera pas long, mais ce sera long quand m√™me.\n\n\n:bim:\n\n\n  Alors l‚Äôintelligence elle existe d√©j√†, elle s‚Äôappelle vous-m√™me‚Ä¶ mais actuellement elle n‚Äôest pas disponible.\n\n\n‚òïÔ∏è\n\n\n  Tu peux mettre autant de code brouette dans tes pull request que de sucre dans ton caf√© : L‚Äôimportant, c‚Äôest que √ßa soit bien dilu√©. \nMoi, je ne sucre pas mon caf√©.\n\n\nüîç\n\n\n  Normalement j‚Äôutilise un vrai IDE. Mais je ne sais pas monter le zoom sur PHPStorm\n\n\nEncore une histoire de date\n\n\n  C‚Äôest une fois par semaine, les releases mensuelles ?\n\n"
} ,
  
  {
    "title"    : "API Platform Conference 2022",
    "category" : "",
    "tags"     : " conferences, backend, api, php",
    "url"      : "/2022/10/07/api-platform-conference-2022.html",
    "date"     : "October 7, 2022",
    "excerpt"  : "En cette p√©riode de rentr√©e, Bedrock participait √† l‚ÄôAPI Platform Conference 2022, o√π nous avons eu le plaisir d‚Äôassister √† une partie des conf√©rences propos√©es. Un grand merci √† toutes les personnes chez Les-Tilleuls.coop pour l‚Äôorganisation de c...",
  "content"  : "En cette p√©riode de rentr√©e, Bedrock participait √† l‚ÄôAPI Platform Conference 2022, o√π nous avons eu le plaisir d‚Äôassister √† une partie des conf√©rences propos√©es. Un grand merci √† toutes les personnes chez Les-Tilleuls.coop pour l‚Äôorganisation de cet √©v√®nement !\nPour cette seconde √©dition, le programme √©tait r√©parti sur deux jours, les 15 &amp;amp; 16 septembre 2022.\n\nEn introduction √† cette conf√©rence, K√©vin Dunglas, cr√©ateur d‚ÄôAPI Platform, a mis en ligne la version 3.0.0 du framework en nous pr√©sentant certaines nouvelles fonctionnalit√©s d√©velopp√©es telles que le support natif de XDebug. Il a profit√© de l‚Äôoccasion pour pr√©senter un petit historique d‚ÄôAPI Platform.\n\nDomain-driven design with API Platform 3\n\nLors de cette conf√©rence, Robin Chalas et Mathias Arlaud nous ont parl√© de l‚Äôutilisation d‚ÄôAPI Platform dans le cadre du Domain Driven Development et de l‚ÄôArchitecture Hexagonale.\nLes pr√©sentateurs ont commenc√© cette pr√©sentation par plusieurs rappels et pr√©sentations sur des sujets comme :\n\n  Domain Driven Design\n  Structures hexagonales\n  CQRS\n\n\nCes rappels ont permis d‚Äôenchainer sur l‚Äôutilisation du framework dans ce contexte √† travers un exemple de projet DDD utilisant API Platform 3 et suivant l‚Äôarchitecture hexagonale.\n\nIls expliquent comment impl√©menter API Platform dans notre code en d√©taillant plusieurs points :\n\n  L‚Äôimpl√©mentation des providers c√¥t√© query\n  L‚Äôimpl√©mentation des processors c√¥t√© command\n  Le syst√®me d‚Äôop√©ration\n  Les providers/processors qui appellent l‚Äôapp via les bus\n\n\nComment Alice Garden‚Äôs g√®re-t-elle son code m√©tier via les √©v√®nements\n\nNous avons pu assister √† la conf√©rence ‚ÄúComment Alice Garden‚Äôs g√®re-t-elle le code m√©tier via des √©v√®nements‚ÄØ?‚Äù propos√©e par leur technical architect Nicolas Lemahieu. Tout d‚Äôabord, il nous a pr√©sent√© le contexte de son entreprise Alice Garden‚Äôs qui fait de la vente de mobilier d‚Äôext√©rieur. En se basant sur la stack technique d√©j√† pr√©sente : Symfony, RabbitMQ, MariaDB et sans tout refondre, comment faire pour mieux g√©rer le code m√©tier actuellement √©parpill√© un peu partout dans le code.\n\nIls utilisaient beaucoup de subscribers Doctrine, ce qui entra√Æne plusieurs probl√®mes :\n\n  Des subscribers nombreux = plus de logique au flush\n  Code plus difficile √† maintenir et √† comprendre\n  L‚Äôaugmentation du risque de boucles infinies implique que chaque changement entra√Æne des boucles sur le UnitOfWork\n  Duplication de code\n  Code fortement coupl√© √† Doctrine et manque de typage\n  Du c√¥t√© du profiler Symfony, cela devient compliqu√© aussi d√®s qu‚Äôon commence √† en avoir beaucoup\n  Les tests sont compliqu√©s :\n    \n      Unitaires quasi impossibles,\n      Fonctionnels possibles, mais demandent beaucoup de ressources en temps et donc d‚Äôargent\n    \n  \n\n\nNicolas Lemahieu a donc pr√©sent√© les diff√©rentes solutions envisag√©es ainsi que leurs avantages et inconv√©nients :\n\n  Domain Driven Development : s√©paration tr√®s nette du m√©tier et de l‚Äôinfra, mais demande beaucoup trop d‚Äôeffort √† mettre en place, car aucune correspondance avec les bundles d√©j√† existants (risque de r√©gression trop haut, co√ªt de d√©veloppement trop grand‚Ä¶)\n  Garder les √©v√®nements et s‚Äôaffranchir de Doctrine : c‚Äôest la solution qui a √©t√© retenue parce qu‚Äôelle permettait de r√©utiliser un maximum de l‚Äôexistant\n\n\nPuis, nous avons pu apprendre comment impl√©menter cette solution :\n\n  Cr√©ation d‚Äôune abstraction suppl√©mentaire ‚ÄúBusinessObject‚Äù : nouveau dossier Business dans src o√π :\n    \n      Entit√© = objet m√©tier\n      M√©thodes des entit√©s = r√®gles m√©tier\n      Toutes les interfaces entit√© √©tendent BusinessObjectInterface\n    \n  \n  Classes abstraites dans Business\n  Impl√©mentation d‚Äôevents custom pour chaque objet m√©tier et par type d‚Äôevent\n  Event provider : fournit les √©v√®nements qui sont mis dans une collection puis tag dans les services\n\n\nEn conclusion, chez Alice Garden‚Äôs, ils ont r√©ussi √† n‚Äôavoir qu‚Äôun seul Doctrine Subscriber, donc une seule boucle de UnitOfWork et des tests facilit√©s, car c‚Äôest seulement du PHP. La d√©pendance √† Doctrine est √©limin√©e et il suffira de d√©placer le provider pour adapter le code √† une autre infrastructure.\n\nR√©utiliser et partager vos op√©rations personnalis√©es avec API Platform\n\nGr√¢ce √† Hubert Lenoir et J√©r√©my Jarri√© de l‚Äôentreprise SensioLabs, nous avons pu apprendre √† r√©utiliser et partager les op√©rations avec API Platform. Ils utilisent 3 API REST faites avec API Platform v3. Des op√©rations g√©n√©riques comme ‚Äúliker‚Äù peuvent s‚Äôappliquer sur des ressources diff√©rentes (articles, photos, pages, etc.), on peut donc r√©utiliser du code.\n\nLes principes pour faire cette utilisation g√©n√©rique de code sont simples :\n\n  Un seul contr√¥leur pour plusieurs ressources = un contr√¥leur de comportement\n  Une interface pour que les entit√©s puissent adopter ce comportement\n  Trait pour les m√©thodes du comportement (1 √† n comportements, donc classe abstraite ou interface impossible)\n  Ajouter des services interm√©diaires\n\n\nIl survient un seul probl√®me avec ce pattern : la duplication des annotations API Platform. La solution est d‚Äôajouter des d√©corations (design pattern decorator) sur les m√©tadatas d‚ÄôAPI Platform. Il est donc simple de cr√©er des comportements ind√©pendants des ressources qui pourront √™tre facilement r√©utilis√©s. Ce projet √©tait encore √† l‚Äô√©tat de POC, mais J√©r√©my et Hubert allaient mettre √† jour la version plus aboutie sur le repository GitHub.\n\nLa revue de code est un art\n\nSmaine Milianni a propos√© une conf√©rence sur les conseils √† suivre afin d‚Äôeffectuer une revue de code. Nous avons pu r√©aliser qu‚Äôau sein de Bedrock nous appliquions d√©j√† de nombreux principes.\n\nNous appliquons d√©j√† :\n\n  Un template de PR pour d√©crire les caract√©ristiques du bug ou de l‚ÄôUS :\n    \n      Quoi, pourquoi, comment, comment tester, etc.\n    \n  \n  Des noms de commits explicites\n  La bienveillance\n  Le challenge du code des autres personnes\n  La connaissance des nombreux concepts de code (SOLID, KISS‚Ä¶)\n  Faire du pair review ou mob review\n  Un request bot d√©j√† en place\n  Tester et ne pas se fier uniquement √† la lecture du code\n\n\nNous avons aussi pu prendre du recul et noter des conseils √† appliquer. Chacun a pu transmettre ses id√©es √† son √©quipe. Le rappel qu‚Äôune revue c‚Äôest aussi souligner le positif et pas seulement challenger le code. Cette conf√©rence est tr√®s concr√®te et facilement applicable √† Bedrock.\n\nFighting impostor syndrome: a practical handbook\n\nLors de cette conf√©rence, Marine Gandy commence sa pr√©sentation par d√©finir ce que le syndrome de l‚Äôimposteur est : une peur de l‚Äô√©chec, la crainte que quelqu‚Äôun dise que nous ne sommes pas capables, mais aussi le sentiment de ne pas m√©riter de r√©ussir.\nElle nous explique que ce terme √©tait tout d‚Äôabord attribu√© exclusivement aux femmes, mais qu‚Äôapr√®s une √©tude montrant que 70% de la population √©tait touch√©e, il se serait g√©n√©ralis√© √† tous les genres.\nMarine Gandy nous √©nonce que la tech est tr√®s touch√©e par ce ph√©nom√®ne pour plusieurs raisons comme le fait qu‚Äôil y ait beaucoup de renouveau dans ce corps de m√©tier et que nous avons vite l‚Äôimpression de retourner √† nos d√©buts lorsque nous changeons de techno, cr√©ant ainsi un sentiment d‚Äôinstabilit√©. \nDans ce contexte, Marine nous parle de l‚Äôeffet Julien Lepers en prenant pour exemple le fait de se trouver dans une √©quipe de personnes bien plus exp√©riment√©es que nous o√π la situation influe sur la personne.\nPour finir, la conf√©renci√®re nous pr√©sente plusieurs pistes √† suivre pour √©viter ou minimiser ce genre de sentiment :\n\n  Arr√™ter de se comparer aux autres\n  Se challenger sur de nouveaux domaines pour se rendre compte qu‚Äôon peut toujours apprendre\n  Travailler sur ses faiblesses pour permettre de se sentir plus comp√©tent\n\n\nMon combat contre l‚Äôarachnophobie\n\nJ√©r√¥me Tanghe, par son arachnophobie, nous a expliqu√© comment il est arriv√© √† contribuer √† API Platform afin d‚Äôajouter une option pour cacher la mascotte Webby. Et c‚Äôest ce dont il nous a parl√©, √† savoir comment bien d√©marrer sa premi√®re pull request pour contribuer au logiciel libre. La premi√®re √©tape √©tant de trouver le bon repository qui nous conviendrait parmi les projets existants. Dans le but d‚Äôidentifier un sujet sur lequel contribuer, il ne faut pas h√©siter √† utiliser la fonctionnalit√© des tags sur les issues, par exemple le tag hacktoberfest dans le cadre d‚ÄôAPI Platform. Une fois le sujet trouv√©, il faut maintenant identifier la branche de base √† partir de laquelle faire sa pull request, cela peut s‚Äôagir d‚Äôune version sp√©cifique choisie ou m√™me de la branche principale. La contribution au logiciel libre ou √† l‚Äôopen source ne passe pas uniquement par des pull requests uniquement bas√©s sur le code. Il est √©galement possible de tester les pr√©versions (release-candidate), signaler des bugs, faire des suggestions, am√©liorer la documentation ou encore r√©diger des traductions. Enfin, il est conseill√© de prendre en compte chaque retour sur d‚Äôautres pull requests, cela permet notamment de d√©couvrir les principes et standards du projet.\n\nPourquoi je n‚Äôutilise pas API Platform\n\nFr√©d√©ric Bouchery, s‚Äôest d√©cid√©ment perdu en se retrouvant √† pr√©senter cette conf√©rence √† l‚ÄôAPI Platform conference 2022. Malgr√© tout, cela lui a permis de nous partager son introspection : Mais au fait, pourquoi il ne s‚Äôen sert pas ?\nDans cette premi√®re partie de sa conf√©rence, Fr√©d√©ric n‚Äôh√©site pas √† utiliser beaucoup de sarcasme. Il nous explique qu‚Äôil ne s‚Äôen sert pas, car API Platform est √©crit en PHP et pour lui, c‚Äôest une technologie vieillissante qui ne devrait pas tarder √† rejoindre Cobolt. √âgalement parce qu‚ÄôAPI Platform utilise Symfony, alors que tout le monde le sait tr√®s bien, enfin surtout les Google Trend, Laravel est plus utilis√© dans le monde. De plus, Fr√©d√©ric n‚Äôaime pas la magie et API Platform en est rempli : s√©rialisation et d√©s√©rialisation √† tout va alors que lui est capable de faire une API en seulement quelques lignes avec du PHP pur sans artifice. Enfin, il reproche √† API Platform de devenir compliqu√© √† utiliser si le projet qui se base dessus est complexe, trop de personnalisation et de configurations doivent √™tre effectu√©es. \nDans cette deuxi√®me partie, Fr√©d√©ric fait tomber son masque sarcastique et d√©cide de revenir sur les points qu‚Äôil a abord√©s pr√©c√©demment :\n\n  Les tendances concernant un langage ne sont pas des bons indicateurs, en effet imaginer le futur ou la mort de PHP via des statistiques dont certaines bas√©es sur l‚Äôopinion publique n‚Äôest pas une bonne fa√ßon de faire\n  PHP, c‚Äôest aujourd‚Äôhui 75% des sites du monde entier et 54% parmi le top 1000 des sites internet fr√©quemment utilis√©s\n  Malgr√© tout, il nous conseille de ne pas √™tre mono technologique non plus. S‚Äôint√©resser √† d‚Äôautres langages est une bonne chose\n  API Plateform a quand m√™me de bonnes performances : 99% de r√©ponses avec une moyenne de 91‚Äâms sur 5 000 requ√™tes compar√©es √† son code PHP pur avec une moyenne de 21‚Äâms pour 5 000 requ√™tes √©galement sachant que son code ne prend pas en compte la s√©curit√©\n  Le framework Laravel utilisant des composants Symfony, il est d√®s lors difficile de les comparer. M√™me si factuellement Laravel est plus utilis√© dans le monde, on n‚Äôutilise pas du Laravel ou du Symfony pour les m√™mes raisons et c‚Äôest une bonne chose que les deux coexistent ensemble\n  Il pensait qu‚Äôavec la complexit√© de ses projets, il √©tait trop dur de passer √† API Platform, mais il s‚Äôest rendu compte que ce n‚Äô√©tait pas n√©cessairement vrai\n\n\nEn conclusion de sa conf√©rence et sans aucun sarcasme, Fr√©d√©ric n‚Äôh√©site pas √† se livrer √† nous et finit par nous dire qu‚Äôil va finalement utiliser API Platform 3 pour un projet.\n\nWhat‚Äôs New in Caddy, the webserver of API Platform\n\nFrancis Lavoie nous a pr√©sent√© plusieurs nouveaut√©s dans Caddy, un webserver √©crit en Go ayant beaucoup de fonctionnalit√©s activ√©es par d√©faut et fourni avec API Platform dans l‚Äôinstallation de base.\n\nParmi les nouveaut√©s pr√©sent√©es, il nous a notamment parl√© d‚Äôam√©liorations au niveau des request matchers avec des matchers r√©utilisables, des expressions et des fonctions. Il a ensuite parl√© d‚Äôune gestion native de Authelia permettant de d√©l√©guer facilement l‚Äôauthentification depuis la configuration du serveur.\nEnfin, la directive file_server peut maintenant servir des fichiers provenant d‚Äôautres sources que le syst√®me de fichiers local, par exemple depuis un bucket S3.\n\nCertaines fonctionnalit√©s sont d√©sormais activ√©es par d√©faut comme HTTP/3 et la suppression du log des headers d‚Äôauthentification o√π il est √©galement possible de cr√©er des filtres pour retirer d‚Äôautres informations.\n\nWebAuthn : se d√©barrasser des mots de passe. D√©finitivement.\n\nFlorent Morselli nous fait une proposition : il est de plus en plus possible aujourd‚Äôhui de se passer compl√®tement des mots de passe.\n\nIl a commenc√© par nous rappeler les probl√®mes r√©currents : mots de passe trop faibles et/ou trop courts, r√©utilisation sur plusieurs sites, la multiplication des fuites de bases de donn√©es, etc.\n\nLa solution propos√©e : WebAuthn, un standard d‚Äôauthentification multifacteur permettant d‚Äôidentifier les utilisateurs via des donn√©es biom√©triques, des cl√©s physiques ou sans aucune information apr√®s la premi√®re authentification sur un appareil.\n\nC√¥t√© impl√©mentation, Florent nous a pr√©sent√© deux projets : un bundle Symfony pour le c√¥t√© backend et un composant Symfony UX pour le frontend.\n\nPHP WebSockets, or how to communicate with clients in real-time\n\nHabituellement connue pour faire des conf√©rences sur Git, Pauline Vos nous a fait une d√©mo en live de l‚Äôutilisation des WebSockets en PHP.\n\nElle a commenc√© par une rapide explication de diff√©rents protocoles de communication en temps r√©el existant : WebRTC chez Google, Mercure chez Symfony et Livewire chez Laravel.\nLes WebSockets √©tant de simples tunnels √† donn√©es, ces protocoles permettent de les enrichir de diverses fonctionnalit√©s : identification, structures de messages, reconnexion auto, etc.\n\nVient ensuite la d√©mo qui consistait en une mini webapp de tombola en ligne. Elle a √©t√© d√©coup√©e en diff√©rentes √©tapes (pr√©par√©es dans des branches Git) avec, pour chaque √©tape, une pr√©sentation du code et des tests en live via l‚Äôoutil WebSocketKing.\nPour l‚Äô√©tape finale, un QR code a √©t√© affich√© √† l‚Äô√©cran pour permettre aux spectateurs et spectatrices de participer en live. Le hasard a voulu que le nom tir√© soit Antoine Bluchet, le contributeur principal d‚ÄôAPI Platform !\n\nComment (re)mettre la tech au service du bien commun ?\n\nPour conclure ces deux jours de conf√©rences intenses en savoir et en √©motions, anim√© par Gr√©gory Copin¬†: H√©l√®ne Marchois, Paul Andrieux et K√©vin Dunglas nous ont propos√© une excellente table ronde riche en id√©es et porteuse (d‚Äôun peu) d‚Äôespoir. Le sujet √©tant de savoir s‚Äôil est possible de faire √©voluer la tech dans le but de rejoindre les objectifs de d√©part du logiciel libre.\n\nL‚Äôapparition du mouvement du logiciel libre puis celle du web se sont b√¢ties sur de grands espoirs et de beaux objectifs¬†: √©mancipation des individus, partage des connaissances √† l‚Äô√©chelle plan√©taire, libert√© d‚Äôexpression, constructions de bien commun appartenant √† toutes et tous et maintenues collectivement. Malheureusement, force est de constater que le web comme le logiciel libre ont √©t√© d√©tourn√©s de leurs objectifs de base et que les id√©aux¬†qu‚Äôils portaient ont √©t√© bien mis √† mal¬†: surveillance de masse, capitalisation de ces biens communs et pr√©carisation des individus et des libert√©s.\n\nK√©vin nous explique ensuite la diff√©rence entre logiciel libre et open source : API Platform est un logiciel libre plus qu‚Äôopen source, m√™me si techniquement, c‚Äôest les deux. Historiquement, le logiciel libre est apparu dans le but de cr√©er un bien commun pour l‚Äôhumanit√© et s‚Äôest √©largi avec, notamment, la notion de commons via Wikip√©dia. La diff√©rence avec l‚Äôopen source est que si le code est disponible, ce n‚Äôest pas uniquement pour b√¢tir tout et n‚Äôimporte quoi avec, mais c‚Äôest un code qui porte des valeurs et qui a pour but de faire en sorte que tout le monde sans distinction puisse facilement cr√©er de nouveaux outils qui puissent √™tre partag√©s, qui appartiennent √† un ensemble de personnes et qui vont socialiser le travail qui est r√©alis√© en commun l√†-dessus. Le but du logiciel libre √† la base, c‚Äôest de faire en sorte que ces valeurs de transparence, de d√©mocraties, de partage de connaissance s‚Äô√©tendent via le logiciel √† l‚Äôensemble de la soci√©t√©. Donc si vous aussi, vous voulez utiliser un logiciel libre, la condition est que, vous aussi, vous devez faire quelque chose qui sert l‚Äôhumanit√© : cr√©er un bien commun et mettre aussi √† disposition le code source du logiciel. En l‚Äôoccurrence, API Platform, est une licence permissive, c‚Äôest-√†-dire qu‚Äôil est possible de faire tout et n‚Äôimporte quoi avec, mais ce n‚Äôest pas le cas pour le logiciel Mercure par exemple, o√π si vous l‚Äôutilisez et le modifiez, vous √™tes oblig√© de redistribuer les √©l√©ments.\n\nQuant √† l‚Äôopen source, c‚Äôest une initiative qui est arriv√©e bien apr√®s le logiciel libre et est une offensive de multinationale de la technologie qui veut d√©politiser le mouvement du logiciel libre. Le point de d√©part √©tant que, techniquement, c‚Äôest tr√®s int√©ressant de cr√©er du logiciel ensemble, de partager les co√ªts de maintenance entre diff√©rentes entreprises ou personnes et c‚Äôest surtout tr√®s int√©ressant d‚Äôavoir acc√®s au secret de fabrication pour les choses qui ont peu de valeur ajout√©e. Mais l‚Äôobjectif final √©tant de capitaliser, faire du business et capter la valeur sur ce qui a une tr√®s forte valeur ajout√©e. Par exemple, pour macOS, toutes les briques de bases sont compl√®tement libres, d√©velopp√©es par une communaut√© de personne, d‚Äôentreprise et essentiellement beaucoup de b√©n√©voles et d‚Äôhobbyiste. Et dans ce cas-l√†, ce qui a une extr√™me valeur ajout√©e, c‚Äôest l‚ÄôUI au-dessus du mat√©riel ou encore les jolis outils qui co√ªtent une fortune. Ce qui permet √† Apple d‚Äô√™tre la boite la plus riche du monde en r√©utilisant le travail de personnes qui n‚Äôont pas fait √ßa pour macOS √† la base.\n\nLes trois personnes intervenantes repr√©sentant chacune une SCOP, la table ronde s‚Äôest ensuite naturellement tourn√©e vers le lien entre le logiciel libre et le mouvement coop√©ratif. Le lien √©tant la vision politique du logiciel libre via son socle de valeur¬†: libert√©, transparence, gouvernance partag√©e et coop√©ration. On retrouve cet esprit de transparence, de fonctionnement d√©mocratique et de fonctionnement par coop√©ration √† l‚Äôint√©rieur de la SCOP et entre les diff√©rentes SCOP.\nS‚Äôen est ajout√©e la question du sens par rapport √† son travail. Effectivement, le logiciel libre, comme le mouvement coop√©ratif, redonne du sens, principalement car cela ouvre le champ des possibles en vue des enjeux climatiques et sociaux actuels. M√™me si l‚Äôon vit dans une soci√©t√© qui est r√©gie par le profit, la comp√©tition f√©roce et le pouvoir, il existe des possibilit√©s de s‚Äôorganiser autrement et qui fonctionne quand m√™me √† une √©chelle cons√©quente, bien qu‚Äôencore insuffisante. Des actions individuelles existent et sont possibles. Pour cela, nous vous recommandons de regarder la conf√©rence d‚ÄôH√©l√®ne √† l‚ÄôAPI Platform Conference de l‚Äôann√©e derni√®re qu‚Äôelle r√©sume et √©toffe lors de cette table ronde.\n\nEt bien s√ªr, quand cela sera possible, nous vous encourageons fortement de regarder le replay de cette conf√©rence (sur la chaine des Tilleuls) qui redonne un peu d‚Äôespoir quant aux futurs des organisations d√©mocratiques de nos m√©tiers.\n\nConclusion\nMerci √† toutes et tous les speakers, √† API Platform ainsi qu‚Äôaux Tilleuls-coop pour cet √©v√®nement ! Nous avons pu en apprendre plus sur API Platform et revenir la t√™te pleine d‚Äôid√©es pour nos projets futurs et pr√©sents ! √Ä l‚Äôann√©e prochaine peut-√™tre !\n"
} ,
  
  {
    "title"    : "Turn off your fracking notifications #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/turn-off-your-notifications",
    "date"     : "September 30, 2022",
    "excerpt"  : "Turn off your fracking notifications.\nPr√©sent√© par Fabien Dumas.\n",
  "content"  : "Turn off your fracking notifications.\nPr√©sent√© par Fabien Dumas.\n"
} ,
  
  {
    "title"    : "Nearby interaction, Airtags or how your iPhone shares your location #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/nearby-interaction-airtags",
    "date"     : "September 30, 2022",
    "excerpt"  : "D√©couvrez comment votre iPhone partage votre localisation.\nPr√©sent√© par Oleksandr Balystky.\n",
  "content"  : "D√©couvrez comment votre iPhone partage votre localisation.\nPr√©sent√© par Oleksandr Balystky.\n"
} ,
  
  {
    "title"    : "La facilitation spectacle : Entre artifices et intention #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-facilitation-spectacle",
    "date"     : "September 30, 2022",
    "excerpt"  : "La facilitation spectacle : Entre artifices et intention.\nPr√©sent√© par Camille Cousin &amp;amp; Marie-Andr√©e Jolibois.\n",
  "content"  : "La facilitation spectacle : Entre artifices et intention.\nPr√©sent√© par Camille Cousin &amp;amp; Marie-Andr√©e Jolibois.\n"
} ,
  
  {
    "title"    : "Courir: la voi(e/x) du fondeur #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/courir-la-voie-du-fondeur",
    "date"     : "September 30, 2022",
    "excerpt"  : "Courir: la voi(e/x) du fondeur\nPr√©sent√© par Thomas Sontag.\n",
  "content"  : "Courir: la voi(e/x) du fondeur\nPr√©sent√© par Thomas Sontag.\n"
} ,
  
  {
    "title"    : "Comment cloner Shazam ! #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-cloner-shazam",
    "date"     : "September 30, 2022",
    "excerpt"  : "D√©couvrez comment cloner Shazam!\nPr√©sent√© par Moustapha Agack.\n",
  "content"  : "D√©couvrez comment cloner Shazam!\nPr√©sent√© par Moustapha Agack.\n"
} ,
  
  {
    "title"    : "Chaos engineering dans le frontend #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/chaos-engineering-frontend",
    "date"     : "September 30, 2022",
    "excerpt"  : "D√©couvrez comment faire du chaos engineering dans le frontend.\nPr√©sent√© par Thibaud Courtoison.\n",
  "content"  : "D√©couvrez comment faire du chaos engineering dans le frontend.\nPr√©sent√© par Thibaud Courtoison.\n"
} ,
  
  {
    "title"    : "Bullet journal #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/bullet-journal",
    "date"     : "September 30, 2022",
    "excerpt"  : "Bullet Journal.\nPr√©sent√© par B√©n√©dicte Garcia.\n",
  "content"  : "Bullet Journal.\nPr√©sent√© par B√©n√©dicte Garcia.\n"
} ,
  
  {
    "title"    : "Subtitles, open captions, closed captions, SDH, oh my!",
    "category" : "",
    "tags"     : " streaming, subtitles, captions, video, player",
    "url"      : "/2022/09/20/captioning-in-the-streaming-world.html",
    "date"     : "September 20, 2022",
    "excerpt"  : "Subtitles, open captions, closed captions, SDH, oh my!\n\nWait, what? Subtitles and captions are not the same? Have you noticed the popular ¬´ cc ¬ª logo in a player you use, standing for ¬´ Closed Captions ¬ª but you have never heard of Open Captions? ...",
  "content"  : "Subtitles, open captions, closed captions, SDH, oh my!\n\nWait, what? Subtitles and captions are not the same? Have you noticed the popular ¬´ cc ¬ª logo in a player you use, standing for ¬´ Closed Captions ¬ª but you have never heard of Open Captions? In this article we are going to dive into the different textual representations used in the streaming world.\n\nSubtitles vs Captions: a matter of accessibility\n\nSince both terms are often mixed up, in this post we are going to explain in details the different types of subtitles and captions.\n\nSubtitles exist in order to help the viewer understand the spoken language in the content being watched, assuming that the viewer can hear. This is the important part. You can think of subtitles as the closest translation of what is being said, textually represented on the screen.\n\nClosed Captions\n\nClosed Captions on the other hand, assume that the viewer is deaf (or hard of hearing) hence cannot understand what is being said, regardless of the spoken language. For this reason and contrary to subtitles, it will describe spoken dialogues as well as all important audio information such as music, sounds, speaker information when it makes sense (for example narrated content). In terms of appearance, closed captions are usually white text on a black background. An important note is that they are not supported through digital connections such as HDMI.\n\nSubtitles and closed captions are separate files that provide information for the receiver to decode. They‚Äôre not part of the stream and both can be turned off.\n\nSubtitles for the Deaf and Hard of Hearing\n\nSubtitles for the Deaf and Hard of Hearing, known as SDH, are a combination between subtitles and closed captions. They can be in the same language of the video original audio and bring some additional non-spoken information (speaker identification, sound effects, etc.) and/or be translated. This makes the content accessible for the deaf and hard of hearing who can read and understand foreign languages.\n\nOpen Captions\n\nThe most important difference between Closed and Open Captions is that Open Captions are always visible and cannot be turned off. Think of them as ¬´ burned ¬ª in the video stream: they are not a separate file. Because of this, quality and readability may be affected. Open captions are widely used on social media for retention. Since there is a high chance that the end user is scrolling through content without sound, ensuring the display of text on the video helps catch and retain attention. In other cases where closed captions cannot be used for example if you have no control on the media player that will play your file, you may provide open captions to be sure to display a textual translation. The downside might be that part of the audience dislikes the superfluous text burned in the stream.\n\nForced subtitles\n\nThere is often a misconception around forced subtitles (sometimes referred as forced narratives) as they are mistaken with open captions. The name ¬´ forced ¬ª might suggest that they are burned in the video stream like open captions but there is a difference. Forced subtitles are actually distributed in a separate file and, despite their name, are not necessarily displayed. On our platform, if a subtitle or closed captions track is selected by the user, forced subtitles will not show up. We will come back to this later.\n\nActually, forced subtitles are a text representation of a communication element like a spoken dialogue, specify a character ID that are not described in the original (or dubbed) audio stream. A common example would be to translate alien language. Despite watching a movie in your native (or any language that does not require you to activate subtitles), you would not be able to understand. That‚Äôs where forced subtitles come into play and ensure that you have a textual representation of what is being said even if you set subtitles to off, hence the ¬´ forced ¬ª attribute.\n\nHowever, imagine you are French and watch a Spanish show for instance. If you set the subtitles to French in order to be able to understand the content, forced subtitles won‚Äôt show up since you already have a textual representation of the content. Same goes for closed captions: if set to off, forced subtitles will display, if any. Otherwise, they won‚Äôt show up. To ensure better user experience, forced subtitles content should be included in all other tracks (regular subtitles, SDH, CC).\n\nIf available, forced subtitles should be displayed in the preferred language set up by the user.\n\nTo sum up, here is a table comparing the different technologies:\n\n\n  \n    \n      ¬†\n      Subtitles\n      SDH\n      Closed Captions\n      Open Captions\n      Forced Subtitles\n    \n  \n  \n    \n      Can be turned off\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ¬†\n      *\n    \n    \n      Appearance\n      Varies\n      Varies\n      Usually white text / black background\n      Varies\n      Varies\n    \n    \n      Position\n      Bottom third, centered\n      Bottom third, centered\n      Varies\n      Varies\n      Bottom third, centered\n    \n    \n      HDMI Supported\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ¬†\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n    \n    \n      Describes music and sounds\n      ¬†\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ¬†\n    \n    \n      Describes speaker ID\n      ¬†\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ¬†\n    \n    \n      Available in source language\n      ¬†\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n      ‚úîÔ∏è\n    \n  \n\n\n*: Forced subtitles do not show up in the track selection tool making them impossible to turn on or off. However, the business rules of the media platform will take care of displaying them, if needed.\n"
} ,
  
  {
    "title"    : "Monitoring at scale with Victoria Metrics",
    "category" : "",
    "tags"     : " k8s, kubernetes, monitoring, prometheus, scaling, victoriametrics, cardinality",
    "url"      : "/2022/09/06/monitoring-at-scale-with-victoriametrics.html",
    "date"     : "September 6, 2022",
    "excerpt"  : "Monitoring at Bedrock :\nAt Bedrock Streaming, a large part of our applications are hosted on Kubernetes clusters, others use the EC2 service from AWS and a small part are hosted on ‚ÄúOnPremise‚Äù servers.\n\nFrom 2018 until January 2022, we used Promet...",
  "content"  : "Monitoring at Bedrock :\nAt Bedrock Streaming, a large part of our applications are hosted on Kubernetes clusters, others use the EC2 service from AWS and a small part are hosted on ‚ÄúOnPremise‚Äù servers.\n\nFrom 2018 until January 2022, we used Prometheus to monitor all these platforms, because Prometheus met all our needs: keeping control over our monitoring solution and supporting service discovery, which is essential in environments such as Kubernetes or AWS EC2. Prometheus also supports custom exporters we developed internally.\n\nOver the years, our business has grown significantly, so the load on our platforms has increased. Indirectly, the load on our Prometheus instances has also increased, to the point where certain limitations have become too much for us. This is why we have changed our monitoring/alerting stack.\n\nLimits of Prometheus\nPrometheus does not have a native High Availability mode: to have high availability, we had to duplicate our Prometheus instances. This implies that our targets were ‚Äúscrapped‚Äù by all our Prometheus instances (same for our rules and records).\nTo avoid this, we had to use sharding, but this made the infrastructure more complex. More information on this subject in this documentation from the Prometheus operator\n\nPrometheus is not designed to store metrics on a long-term basis, as mentioned in the documentation :\n\n&amp;gt; Prometheus‚Äôs local storage is not intended to be durable long-term storage; external solutions offer extended retention and data durability.\n\nPrometheus‚Äôs local storage is not intended to be durable long-term storage; external solutions offer extended retention and data durability.\nWe worked around this limitation by using Victoria Metrics (VMCluster) as a LongTermStorage via the remote_write protocol\n\nAll processes (scrapping, ingest, storage, etc.) were, until now, managed in the same ‚Äúprometheus‚Äù instance, which implied a less flexible and vertical scaling only (since recently a Prometheus agent is available for the ‚Äúscrapping‚Äù part).\n\nThe RAM and CPU usage of a Prometheus instance is correlated to the number of metrics (and their cardinality) it has to manage. In our case, several Prometheus instances consumed more than 64 GB of RAM and 26 CPUs each, in order to absorb our peak loads. In a Kubernetes cluster, this high resources consumption can cause problems, especially for scheduling.\n\nThe Write-Ahead Log (WAL) system can cause rather slow restarts if the Prometheus instance runs out of RAM and can cause the Prometheus instance to hang for varying lengths of time. During the replay of the WAL, Prometheus doesn‚Äôt scrape anything, thus there is no alerting and no way of knowing if something is going on.\n\nThe cardinality of metrics\nWhen our Kubernetes clusters manage a large number of pods, a constraint quickly appears: cardinality.\n\n&amp;gt; The cardinality of a metric is the number of TimeSeries of that metric with single-valued labels.\n\n\n\nIn the example above, the status_code label has a cardinality of 5, app has a cardinality of 2 and the overall cardinality of the server_reponses metric is 10.\n\nIn this example, any Prometheus instance can handle this cardinality, but if you add for example the label pod_name or client_IP (or both) to the server_reponses metric, the cardinality increases for each different clients calls and for each pod.\n\nYou should read the excellent article from ‚ÄúRobust Perception‚Äù for more details on this subject.\n\nAt Bedrock the high cardinality metrics come from our HAProxy ingress. For our needs, we retrieve several labels like the name of the ingress pod as well as its IP address, but more importantly the name and IP address of the destination pod. In a cluster that can grow to more than 15,000 pods, the combination of unique labels (cardinality) is very significant for some of our ingress metrics.\n\nWe found that Prometheus performed poorly when we had multiple metrics with high cardinalities (&amp;gt; 100,000), and resulted in over-consumption of RAM.\n\nDuring a high load event, Prometheus could consume up to 200 GB of RAM before being OOMKilled. When this happened, we would go completely blind as we had no metrics or alerting.\nThis also impacts us on scalability in our Kubernetes clusters, as we use CustomMetrics very heavily in HPAs to scale the number of pods in our applications.\n\nRAM and CPU consumption of our prometheus instances (the red lines represent the reboots of our instances, we also see a loss of metrics)\n\n\n\nPrometheus is still a good solution, which has served its purpose well for several years, but we have reached its limits in our production environments.\n\nReplacing Prometheus?\n\nWe spent time optimizing Prometheus to absorb the amount of metrics and their cardinality, in particular by either directly removing high cardinality metrics if they were totally unused, or by removing the labels of certain metrics that caused high cardinalities.\nWe have also optimized the Prometheus configuration directly, as well as the maximum IOPS of our EBS. The RAM and CPU consumption of Prometheus is linked to the number of metrics to manage and their cardinality. But we always have more traffic and therefore always more pods in our clusters: we should have perpetually increased Prometheus instances resources. This was a problem for scalability and costs.\n\nCan we replace a critical tool like this? What are our short, medium and long term needs? How can we optimize costs? And especially in what timeframe?\nThe emergency of recent incidents forced us to exclude solutions such as Thanos and Cortex. Testing these solutions completely would have required too much time, which we did not have.\n\nIt is also important to consider that we were already using Victoria Metrics, but only for the Long Term Storage part, without any problems.\nCould replacing Prometheus with a stack based entirely on Victoria Metrics overcome the limitations we had with Prometheus?\nHigh availability and fault tolerance is well-supported, their documentation explains how to manage this.\n\nManaging long-term data is possible, as we were already doing it.\nVictoria Metrics is built around a set of microservices. Each one is built in order to serve a specific job, and each supports vertical and especially horizontal scaling (with sharding). A very important point when used in a Kubernetes environment.\n\nIn addition, Victoria Metrics seemed to handle high cardinality metrics better (see article on this subject). It is also possible to do rate limiting on the number of Time Series to be ingested:\n\nCPU and RAM consumption is lower with better performance than with Prometheus and even other TSDBs, several comparative articles on this subject have already been published:\n\n  Remote Write Storage Wars\n  Prometheus vs VictoriaMetrics benchmark on node_exporter metrics\n  When size matters ‚Äî benchmarking VictoriaMetrics vs Timescale and InfluxDB\n  Comparing Thanos to VictoriaMetrics cluster\n\n\nWe also wanted to keep the Prometheus language: PromQL in order to keep our Grafana dashboards and all our Prometheus alerts. Even though Victoria Metrics offers its own MetricsQL language, it is perfectly compatible with PromQL.\n\nYou can see the main features of Victoria Metrics as well as various case studies in their documentation.\n\nPOC of Victoria Metrics\nWe wanted to validate the performance and consumption of a stack entirely based on Victoria Metrics, the results were really encouraging.\n\nTest environment :\n\n  1500 web app pods\n  250 Haproxy Ingress pods (metric with high cardinality enabled)\n  3700 scrapped targets\n\n\nComparative table between Prometheus and Victoria Metrics :\n\n\n  \n    \n      ¬†\n      Prometheus\n      Victoria Metrics\n    \n  \n  \n    \n      CPU consumption\n      26\n      8\n    \n    \n      RAM consumption\n      30Go\n      11Go\n    \n    \n      New TimeSeries / min\n      50K\n      6.5M\n    \n    \n      Max active TimeSeries\n      7M\n      91M\n    \n    \n      Max cardinality\n      4 metrics &amp;gt; 100K\n      10+ metrics &amp;gt; 1M\n    \n  \n\n\nGraph on the CPU consumption of Victoria Metrics components\n\n\nNumber of active ‚ÄúTimeSeries‚Äù in Victoria Metrics\n\n\nOur benchmark persuaded us to use Victoria Metrics as a replacement for Prometheus.\n\nImplementation of Victoria Metrics :\nWe used the official victoria-metrics-k8s-stack Helm chart which is based on an operator. This chart Helm permits to deploy a complete monitoring and alerting stack in a Kubernetes cluster.\n\nA VMCluster (Insert, Select, Storage) is deployed to manage access to metrics. The collection of metrics (push/pull) from exporters in Prometheus format is handled by the VMagent. Its configuration is done in the form of a Prometheus configuration file. It is able to :\n\n  Manage the relabeling of metrics.\n  Temporarily store the metrics it has collected if the VMCluster is unavailable or not able to send the metrics to the VMCluster.\n  Limit the cardinality of metrics.\n\n\nOne of the advantages of using this Helm chart is that it will deploy essential components to properly monitor a Kubernetes cluster such as Kube-state-metrics or prometheus-node-exporter, but also scraping configurations for services such as Kubelet, KubeApiServer, KubeControllerManager, KubeDNS, KubeEtcd, KubeScheduler, KubeProxy\n\nAlerting is also managed via a VMAlert component, which will execute the alerting and recording rules set by VictoriaMetrics. Notifications are managed by an Alertmanager which is also deployable via this chart.\n\nOne of the advantages of using this Helm chart is that it will deploy essential components to properly monitor a Kubernetes cluster such as Kube-state-metrics or prometheus-node-exporter, but also scraping configurations for services such as Kubelet, KubeApiServer, KubeControllerManager, KubeDNS, KubeEtcd, KubeScheduler, KubeProxy\n\nThis is what our monitoring and alerting stack based on this Helm chart looks like.\n\n\nResumption of the history\nWe wanted to keep historical metrics of our Kubernetes clusters. Victoria Metrics provides a tool to manage the export and import of data from different TSDB: vmctl.\n\nIn order not to overload our monitoring stack, we splitted the exports into smaller or larger time ranges, depending on the history of the cluster. For clusters with little activity and therefore few metrics, exports/imports were split day by day, for others we had to use smaller time slots.\nA home-made bash script launched several kubernetes jobs simultaneously and took care of restarting one of them as soon as another one ended.\n\nBelow an extract of the definition of our Kubernetes job with the arguments we used to do our history transfer by time range:\n\n      containers:\n      - name: vmctl\n        image: victoriametrics/vmctl\n        resources:\n          requests:\n            cpu: &quot;1&quot;\n        args:\n          - vm-native\n          - --vm-native-src-addr=http://victoria-metrics-cluster-vmselect.monitoring.svc.cluster.local.:8481/select/001/prometheus\n          - --vm-native-dst-addr=http://vminsert-victoria-metrics-k8s-stack.monitoring.svc.cluster.local.:8480/insert/000/prometheus\n          - --vm-native-filter-match={__name__!~&quot;_vm.*&quot;}\n          - --vm-native-filter-time-start=&quot;{ { start } }&quot;\n          - --vm-native-filter-time-end=&quot;{ { end } }&quot;\n      restartPolicy: Never\n\n\nFeedback after months of use\nSince we have been using our new monitoring stack, we have encountered a few bugs (as with all solutions).\nMost of the time, these were not impactful, except for one that caused us a production incident.\nWe had an overconsumption of RAM of VMStorage which was fixed in version 1.76. I would like to highlight the responsiveness of the VictoriaMetrics team, whether on slack or on GitHub: I have had several discussions with them on various subjects, and they have always been reactive\n\nVictoria Metrics regularly releases new versions, including performance improvements and new features. The changelog will give you an idea of the latest improvements and their frequency.\n\nVictoria Metrics has an Enterprise version that adds some features, including one that we are interested in but have not yet tested: downsampling.\nWe have configured a one-year retention for each of our Kubernetes clusters, and on some clusters that‚Äôs mean more than 7 TB of data per VMStorage.\n\nThe downsampling allows you to configure how many metrics you want to keep per time interval.\n\nIn this example: -downsampling.period=24h:10s,1w:30s,30d:1m,360d:5m, (assuming we collect metrics every 5 seconds) we only keep:\n\n  one measurement point every 10 seconds beyond 24 hours (instead of one point every 5 seconds)\n  one measurement point every 30 seconds beyond 7 days\n  one measurement point every minute beyond 30 days\n  one measurement point every 5 minutes beyond one year\n\n\nIt is rarely necessary to keep all the measurements of our metrics on such a long scale, when we want to retrieve measurements that are several months old, it is usually to see a trend and not all the measurements.\nWith this option, we could greatly reduce the storage used by our metrics.\n\nConclusion\nThrough this article, you have discovered why and how we migrated our monitoring stack of our Kubernetes clusters at Bedrock from Prometheus to Victoria Metrics.\n\nThis was an important and critical subject for us, as monitoring is a critical need.\nNow our monitoring stack, based entirely on Victoria Metrics, is robust and capable of absorbing large load peaks.\n\nHere are some indicators of the victoria metrics stack performance of one of our Kubernetes clusters during last 6 months:\n\n  active time series: up to 39 million (average 7.4M)\n  total number of datapoints: 12 trillion\n  ingestion rate : up to 1.3 million new samples per second (average 227K)\n  churn rate : up to 117 Million new time series per day (average 30.6 Million)\n  disk usage (data + index): 15 TB\n  sample rate : up to 4.99M (average 343K)\n  scrape target : up to 49K (average 4.4K)\n\n\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Is machine learning a unicorn hiding a series of if and else?",
    "category" : "",
    "tags"     : " machine learning, Data Science",
    "url"      : "/2022/09/05/machine-learning-if-else.html",
    "date"     : "September 5, 2022",
    "excerpt"  : "Recently, a colleague asked me:\n\n\n  All good with your if and else machine learning system?\n\n\nIt was a joke but this one made me think.\n\nThis is a running gag: machine learning is only a series of if and else.\n\n\n\nBeyond the joke, it is true?\n\nYes!...",
  "content"  : "Recently, a colleague asked me:\n\n\n  All good with your if and else machine learning system?\n\n\nIt was a joke but this one made me think.\n\nThis is a running gag: machine learning is only a series of if and else.\n\n\n\nBeyond the joke, it is true?\n\nYes! ‚Ä¶and no. As always, it depends.\n\nQuick answer: Machine learning is a bunch of mathematical and statistical operations. Sometimes, the operations you use can be translated into if and else clauses, and sometimes not. But you never write the series of if and else yourself.\n\nA recap of machine learning\nThe idea of machine learning is: you have some data, and you apply an algorithm to these to detect a pattern. You put this pattern into a function.\n\n\n\nThen, you‚Äôll be able to use this function on new data to extract new information.\n\nA decision tree with a series of if and else\n\nThere are different types of machine learning. If you decide to build a decision tree (a famous way to do machine learning) to know the form of a diamond, you‚Äôll get something like that:\n\n\n\nIf you translate it with code, you‚Äôll get something like that:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    if size is high:\n        return red plate\n    else:\n        return grey pentagon\n\n\nThen, yes, you can see that here, you have a series of if and else.\n\nAnd decision trees are used a lot in machine learning. Most of the time, you don‚Äôt use decision trees directly but forests of decision trees in the Random Forest algorithm or a series of decision trees in the Gradient Boosted Trees algorithm.\n\nBut, many algorithms in machine learning are just the generation of plain mathematical formulas\n\nLet‚Äôs take another famous way to do machine learning: a neural network. What you‚Äôll get at the end is more something like that:\n\na*10+b*15+c*16+20‚Ä¶\n\n\nThen, the process doesn‚Äôt try to find a series of if and else, but a mathematical formula.\n\nI would like to finish with a last example: recommender systems. There are many ways to build a recommendation system. One which is well known is matrix factorization.\n\nMatrix factorization, what?\n\nI won‚Äôt explain deeply what it is about, but as a sum up, it‚Äôs a manipulation of matrices. It comes from linear algebra.\nHere is a definition: Matrix decomposition.\n\nThe result is something like that:\n\nVector A * Vector B\n\n\nAs a result, yes, you have types of machine learning that will generate a series of if and else. But, you have also plenty of algorithms that try to find the variables of an equation or vectors.\n\nYou never write the series of if and else yourself\n\nLet‚Äôs go back to the decision tree. As you‚Äôve seen previously, the result could be translated as a series of if and else.\n\nBut, you don‚Äôt write directly this code. You generate it using‚Ä¶ mathematical operations. Yes, again!\n\nAs an example, you can get the result of a decision tree using an optimisation algorithm with the Shannon Entropy formula:\n\n\n\nLet‚Äôs suppose you want to guess the form (pentagon or plate) of a diamond according to its attributes. You have three diamonds:\n\n\n  \n    \n      carat\n      size\n      form\n    \n  \n  \n    \n      high\n      small\n      plate\n    \n    \n      low\n      high\n      plate\n    \n    \n      low\n      small\n      pentagon\n    \n  \n\n\nThe process is the following:\n\n  The data is split randomly: a random if statement is created like if carat is high\n  The process checks if it helps to generate a more accurate view of the data: by doing this if, are the data separated correctly? Do we have pentagons mostly from one side and plates from another?\n\n\nTo be able to know if the data are separated correctly, the Shannon entropy formula is used\n\n\n  if yes, the process keeps the if carat is high\n  if not, it generates another one\n\n\nThen, by keeping the if you get something like that:\n\n\n\nThe translation with a code is:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    #The process doesn&#39;t know yet how to handle that\n\n\nNote that you have a branch (below low) with a plate and a pentagon. It corresponds to the else where the process doesn‚Äôt know what to put yet.\n\n\n  So, the data below low is split randomly: another if is created\n  The process checks if it helps to generate a more accurate view of the data\n\n\n\n  if yes, the process keeps the new if\n  if not, it generates another one\n\n\nBy keeping the new if, you get another branch:\n\n\n\nThe translation with a code is:\nif size is high:\n    return red plate\nelse:\n    return grey pentagon\n\n\nAt the end, you get a final tree decision:\n\n\nwith a final code:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    if size is high:\n        return red plate\n    else:\n        return grey pentagon\n\n\nOf course, in real life, data are more complicated and the process must iterate a lot until getting the perfect tree. The process used is an optimisation algorithm. This is the part called learning in machine learning.\n\nMathematical optimization [‚Ä¶] is the selection of a best element, with regard to some criterion, from some set of available alternatives (definition from Wikipedia)\n\nIf you want to know how the Shannon entropy works with mathematical formulas, you‚Äôve got this article: Classification in machine learning - Example of Decision Tree with Shannon Entropy\n\nThen as a result, yes, you can have machine learning algorithms that will build a series of if and else. But to generate it, you‚Äôll use mathematical operations.\n\nNote that for other algorithms such as the matrix factorisation or neural networks, you don‚Äôt use a process with the Shannon entropy formula, but other optimisation algorithms that don‚Äôt generate a series of if and else but, as previously seen, vectors or formulas.\n\nSo why do we sometimes say that machine learning is a bunch of if and else statements?\n\nTo my opinion, because of expert systems. They are the ancestors of machine learning in artificial intelligence.\n\nArtificial intelligence is a way to simulate human cognitive abilities. In the history of artificial intelligence, people thought that they would be able to target that with expert systems. These are big series of hardcoded rules and then‚Ä¶ of if and else.\n\nConclusion\nTo conclude, most of the time, machine learning is not a series of if and else. It‚Äôs just mathematics and for some techniques, they are very old. I‚Äôm thinking of linear regressions or Bayesian probabilities. These were used long before the existence of computers.\n\nPhoto of the unicorn by Stephen Leonardi on Unsplash\n"
} ,
  
  {
    "title"    : "Using a circuit breaker to spare the API we are calling",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, resiliency, circuit-breaker",
    "url"      : "/2022/09/02/backend-circuit-breaker.html",
    "date"     : "September 2, 2022",
    "excerpt"  : "Hi! We‚Äôre going to start our fourth article about Bedrock‚Äôs API gateway.\nToday we will talk about the circuit breaker pattern, what it is, and how we‚Äôre using it.\n\nThe Circuit Breaker Pattern\n\nWith this pattern, our API Gateway detects errors when...",
  "content"  : "Hi! We‚Äôre going to start our fourth article about Bedrock‚Äôs API gateway.\nToday we will talk about the circuit breaker pattern, what it is, and how we‚Äôre using it.\n\nThe Circuit Breaker Pattern\n\nWith this pattern, our API Gateway detects errors when calling its dependencies.\nIt will stop calling them if a given threshold (ratio of errors) is crossed.\n\nThe circuit breaker allows us to spare the dependencies in difficulty, but also avoid taking time to do something that will most likely fail.\n\nYou‚Äôll find a more detailed explanation about the circuit breaker on Martin FOWLER‚Äôs blog.\n\nWhere to use it?\n\nAs soon as a service call is not mandatory for our BFF to answer something that a frontend application can read, then we can use the circuit breaker pattern.\n\nIf an API cannot handle a sudden increase in traffic (for example: it‚Äôs not scaling fast enough or its database starts to throttle), it‚Äôs better to stop calling it temporarily.\nWhen the right timeouts are configured, an API throttling will result in an error, as seen in the previous article\n\nHere are some examples:\n\nVideo progress information\n\nDisplaying a video progress bar is useful for end users, but it‚Äôs better to not display this information instead of risking the entire page to not be displayed!\nIf the service that stores video viewing sessions is (slowing) down, we can stop asking for this information and stop displaying the video progress bar.\n\n\n\nUser geolocation\n\nThe geolocation service allows us to know where the end user is in the world. Based on this information we lock some area restricted contents.\nIf this service goes down for some reason, we will stop calling it, and instead use a default area matching the area of our customer as it‚Äôs the majority case.\n\nImplementation and configuration\n\nSo far we‚Äôre only using the circuit breaker pattern with HTTP calls.\nThis is made possible thanks to the Ganesha library, and its Guzzle middleware.\n\nThe Guzzle middleware is created as a service within the Symfony service definitions.\nIt‚Äôs then injected into our HttpClientFactory that will handle the creation of all the different clients.\nThe responsibility of using the circuit breaker falls on each service that will create a http client.\n\nAckintosh\\Ganesha\\GuzzleMiddleware:\n    factory: [&#39;@...Infrastructure\\HttpClient\\CircuitBreaker\\CircuitBreakerMiddlewareFactory&#39;, &#39;buildWithRateStrategy&#39;]\n    arguments:\n        $timeWindow: 60\n        $failureRateThreshold: 40\n        $minimumRequests: 10\n        $intervalToHalfOpen: 60\n\n\nMonitoring the circuit breaker\n\nAt Bedrock, we‚Äôre used to monitor everything. The circuit breaker makes no exception to this rule.\nUsually we store time spent and response code for every outgoing http call.\nTo see when the circuit breaker is open, we catch the ganesha‚Äôs RejectedException to save a dedicated 666 http status.\n\nThis allows us to look for the exact number of calls avoided.\nBelow lies an example of a monitoring chart showing some errors happening during a usual night.\n\n\n\nWe also have to query slower services that often trigger our circuit breaker because they cannot answer in the short timeout we impose.\nThereafter, the same monitoring chart including such services.\n\n\n\nGoing further\n\nSo far, we have identified two areas for improvements described below.\n\nDifferent configurations\n\nWe‚Äôre only using a single configuration for the circuit breaker.\nWe should allow each service to choose from a named list of configurations when creating a client, similarly to the different guzzle configuration we are using.\nThe main obstacle is a lack of hindsight which prevent us to have fine-tuned values.\nThis is something that will definitively be improved over time as we monitor over long period.\n\nStaled cache when the circuit breaker is open\n\nFor many editorial contents, we‚Äôre using a staled cache version of the data as a fallback.\nTo do so, we‚Äôre using another guzzle middleware.\n\nSadly, the two middlewares don‚Äôt work together. We have to chose which one to use based on the criticality of the content and the API behind. \nThis is something that we aim at solving with a bit of R&amp;amp;D.\n\nConclusion\n\nIn today‚Äôs post we‚Äôve seen our usage of the circuit breaker pattern.\nIt allows us to spare the services we are calling, and avoid slowing us down in case of throttling.\n\nNext time, we will talk about our ultimate layer of protection to ensure the BFF always responds something readable to frontend applications.\n\nFrom the same series\n\n\n  What‚Äôs a BFF\n  Handling API failures in a gateway\n  What‚Äôs an error, and handling connexion to multiple APIs\n  Using a circuit breaker\n\n"
} ,
  
  {
    "title"    : "Prescaling pods in Kubernetes, we open source our solution",
    "category" : "",
    "tags"     : " k8s, kubernetes, pods, prometheus, scaling, hpa, resiliency, go, prescaling, opensource",
    "url"      : "/2022/09/01/kubernetes-prescaling-we-open-source-our-solution.html",
    "date"     : "September 1, 2022",
    "excerpt"  : "Previously we discussed how we manage the load of our Kubernetes clusters and how we can anticipate our needs with prescaling. Today, we are here to share our solution that we have reworked and open sourced! \n\n\nAt Bedrock Streaming, we provide str...",
  "content"  : "Previously we discussed how we manage the load of our Kubernetes clusters and how we can anticipate our needs with prescaling. Today, we are here to share our solution that we have reworked and open sourced! \n\n\nAt Bedrock Streaming, we provide streaming platforms to our customers (6play, Salto, Videoland and many others), we have a good knowledge of the daily load peaks and we know in advance the programs that are likely to generate a lot of traffic. We can therefore rely not only on reactive scaling, which has its limits (cf. prescaling article) but also on prescaling.\n\n&amp;gt; Prescaling consists of increasing the number of critical application pods in our clusters in advance in order to be ready to face a sudden traffic peak.\n\nInitially, we developed an in-house solution in Python for a simple reason: it was the language that most people in the team knew. Since we had time to test our solution, we thought it would be great to share it with everyone. But to do so, we had to make some adjustments.\n\nWe rewrote everything in go\n\nMany open source projects we use are written in Golang. In addition, the DevOps/Cloud world is mostly focused on Go. So, we decided to rewrite our prescaling solution in Go in order to make our teams more skilled in this language. The other goal was to make it cloud agnostic. In the Python version, we had an API part that stored prescaling events in a DynamoDB table, which made the solution dependent on AWS. Since prescaling is Kubernetes oriented, we had thought in the first versions in Python to store these events in Custom Resources (CRD) but due to lack of time, we did not implement it. We took advantage of the redesign to implement it and remove the dependency with AWS DynamoDB.\n\nWe also wanted to simplify the project. In the first versions, we had two bricks: one containing the exporter and another the API. We merged the two applications into one monolith. The API is CRUD and can handle CRD events.\n\nHere we go, we open source it\n\nThe great moment has come. Our prescaling solution is now available on GitHub in its alpha version: https://github.com/BedrockStreaming/prescaling-exporter.\n\nThis is the version we currently use in all our clusters. Let‚Äôs quickly see how to implement the solution (you can find more details in the repo README).\n\nThe prescaling-exporter is distributed with helm charts in order to install it in kubernetes cluster.\n\nPrerequisites\n\nThe following bricks must be installed in the k8s cluster:\n\n  Prometheus Stack or Victoria Metrics Stack\n  Prometheus Adapter\n\n\nIt is possible to use another metrics stack but we do not provide an example at this time.\n\nClone the repo and run the following command with Helm3:\n\nhelm install prescaling-exporter ./helm/prescaling-exporter -n prescaling-exporter --create-namespace\n\n\nIt‚Äôs required to add the following configuration to Prometheus adapter:\n\n- &quot;metricsQuery&quot;: &quot;avg(&amp;lt;&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;})&quot;\n    &quot;name&quot;:\n      &quot;as&quot;: &quot;prescale_metric&quot;\n    &quot;resources&quot;:\n      &quot;overrides&quot;:\n        &quot;namespace&quot;:\n          &quot;resource&quot;: &quot;namespace&quot;\n    &quot;seriesQuery&quot;: &quot;prescale_metric&quot;\n\n\nDaily prescaling event\n\nWe have chosen to manage the configuration of daily events directly on the HPA (HorizontalPodAutoscaler) of the applications. Here is how to activate it, through annotations:\n\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: &quot;&quot;\n  annotations:\n    annotations.scaling.exporter.replica.min: &quot;&quot;\n    annotations.scaling.exporter.time.end: &quot;&quot;\n    annotations.scaling.exporter.time.start: &quot;&quot;\nspec:\n  metrics:\n  - type: External\n    external:\n      metricName: &quot;prescaling_metric&quot;\n      metricSelector:\n          matchLabels:\n            deployment: &quot;&quot;\n      targetValue: 10\n\n\nWe are able to control the start and end time of the prescaling and the minimum number of pods we want during this window. Please note that if the number of pods we want for prescaling is less than the current number of pods, the solution will not downscale the application and the HPA will continue to behave as usual.\n\nOne-time events\n\nWe can also record one-off events. For example, at Bedrock Streaming, during an important soccer match, we will record a special event in a Custom Resource Definition. \nOne-time events allow to prescale all applications having annotations on their HPA by multiplying their prescaling minimum replicas (annotations.scaling.exporter.replica.min) by the multiplier of the event in question.\n\nTo record a one-time event, an OpenAPI UI (formerly known as Swagger) is exposed by the prescaling exporter at the url /swagger/index.html. We can also register a new event from here or directly by making an api call to the following address /api/v1/events/.\n\n\n\nWhat‚Äôs next?\n\nWe will continue to improve the solution. For example, we are thinking about removing annotations on HPAs and replacing them with a new dedicated CRD.\n\nAll contributions are welcome, don‚Äôt hesitate to come and exchange with us on GitHub if you want to use the solution, we would be delighted.\n\n\n\nAuthors: J√©r√©my Planckeel, Valentin Chabrier\n"
} ,
  
  {
    "title"    : "BFF&#39;s error definition, and handling connections to multiple API",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, error, timout, retry, slo, guzzle",
    "url"      : "/2022/08/25/backend-errors-connections.html",
    "date"     : "August 25, 2022",
    "excerpt"  : "A quick sidetrack in our series about Bedrock‚Äôs API gateway.\nThis piece defines what are we talking about when we say ‚Äúan error‚Äù, and explains how we handle the numerous connections to services we are calling.\n\nDefinition\n\nIn the previous article,...",
  "content"  : "A quick sidetrack in our series about Bedrock‚Äôs API gateway.\nThis piece defines what are we talking about when we say ‚Äúan error‚Äù, and explains how we handle the numerous connections to services we are calling.\n\nDefinition\n\nIn the previous article, we‚Äôve seen how we handle errors.\nThis was mainly from a business point of view, and how it‚Äôs done in our domain.\n\nBut what is ‚Äúan error‚Äù?\n\nThis term is a bit generic, and the definition will be too: an error is anything unexpected by the application.\n\nIn our context of an API Gateway, we are restricting this to the services we are calling.\n\nThis can be, but not exhaustively, a service not responding because:\n\n  it‚Äôs offline;\n  it‚Äôs taking too much time to answer;\n  it‚Äôs responding with a 5** error (when talking about an API);\n  it‚Äôs giving us an invalid or unexpected content.\n\n\nWhat are the consequences of those errors?\n\nThe first issue is: we won‚Äôt be able to display some part of the application as intended.\nWe‚Äôve talked about this previously already.\n\nThe second error, more insidious, is that it can slow down our BFF terribly.\n\nThe BFF response time is, on average, equals to the slowest service the BFF is calling.\nIf a service that usually responds in 200ms starts slowing down to an average response time of 1s and also times out half the time, it will increase the BFF response time to 1,5s (1s average, and 50% retry).\n\nThat‚Äôs why we must be careful when configuring those timeouts.\nThe BFF exposes a response-time Service Level Objective (SLO), and frontend applications will cut any connection that takes too long.\nLosing some parts of the responses is better than slowing the BFF down to a point where frontend won‚Äôt get any response at all.\n\nHow are we mitigating the errors?\n\nFor any remote service, we configure short timeouts, and retry when we must.\nA short timeout is a timeout that usually match the SLO of the called services, and that will match 99% of our calls.\nWhen the SLO of the called service is higher than ours, we use a shorter timeout and accept that a larger parts of the calls will be cut.\nThe values are tailored according to our usages.\nWe use our monitoring to adapt those values in order to reduce the number of errors, while minimizing the impact on the BFF response time.\nWe are also constantly challenging our colleagues to improve the average response time of their services that we are calling.\n\nThe choice of using retries is based on the information criticality.\nFor example, retrieving the user‚Äôs previous viewing sessions, is important for his/her experience, so we‚Äôre using a retry here.\nOn the opposite, analytics are less important, so we don‚Äôt use any retry there.\n\n    app.http_client_configs.best_effort:\n        retry: 1\n        timeout: 0.6\n        connect_timeout: 0.1\n    app.http_client_configs.fast_fail:\n        retry: 0\n        timeout: 0.6\n        connect_timeout: 0.1\n    app.http_client_configs.long_fail:\n        retry: 0\n        timeout: 1\n        connect_timeout: 0.1\n    app.http_client_configs.reliant:\n        retry: 2\n        timeout: 60\n        connect_timeout: 0.1\n\nAbove, you can see the yaml configuration our Symfony application uses to build its Guzzle clients.\n\nEach configuration can cascade onto the clients, making variants available for our Symfony services.\n\nBelow lies a Symfony configuration example:\n\n  We have an interface BFF\\Domain\\Content\\Repository from the domain for a content repository.\n  The interface is linked to an implementation BFF\\Infra\\HttpContentClient inside the infrastructure.\n  The implementation is built with variants (best_effort and fast_fail) from a factory using the matching Guzzle configurations.\n  Other services use a chosen repository according to their needs and criticality.\n\n\n    # Service definition with its aliases.\n    BFF\\Domain\\Content\\Repository: &#39;@BFF\\Domain\\Content\\Repository.fast_fail&#39;\n    BFF\\Domain\\Content\\Repository.best_effort: &#39;@BFF\\Infra\\HttpContentClient.best_effort&#39;\n    BFF\\Domain\\Content\\Repository.fast_fail: &#39;@BFF\\Infra\\HttpContentClient.fast_fail&#39;\n\n    # Concrete implementations\n    BFF\\Infra\\HttpContentClient.best_effort:\n        class: &#39;BFF\\Infra\\HttpContentClient&#39;\n        factory: [&#39;@BFF\\Infra\\ContentClientFactory&#39;, &#39;create&#39;]\n        bind:\n            $clientConfig: &#39;%app.http_client_configs.best_effort%&#39;\n    BFF\\Infra\\HttpContentClient.fast_fail:\n        class: &#39;BFF\\Infra\\HttpContentClient&#39;\n        factory: [&#39;@BFF\\Infra\\ContentClientFactory&#39;, &#39;create&#39;]\n        bind:\n            $clientConfig: &#39;%app.http_client_configs.fast_fail%&#39;\n\n    # Other services using the Repository\n    BFF\\Domain\\Navigation\\NavBarResolver:\n        $content: &#39;@BFF\\Domain\\Content\\Repository.best_effort&#39;\n\n    BFF\\Domain\\Layout\\BlockResolver:\n        $content: &#39;@BFF\\Domain\\Content\\Repository.fast_fail&#39;\n\n\nThis is an over simplified example as we have more layers and wrappers used for things like caching, monitoring, logging, etc.\n\nConclusion\n\nIn this article, we‚Äôve clarified what an error is, and explained that we cannot generalize the configuration and usage of our APIs. Timeouts and retries, especially, must be tailored depending on the criticality of each call.\n\nThis was a deviation on the road to our next article, where we will talk about monitoring the errors and stopping calls to failing APIs by implementing the circuit-breaker pattern.\n\nFrom the same series\n\n\n  What‚Äôs a BFF\n  Handling API failures in a gateway\n  What‚Äôs an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n"
} ,
  
  {
    "title"    : "Les spikes : quand, comment, pour quoi faire ?",
    "category" : "",
    "tags"     : " spike, methodologie, cytron, tech",
    "url"      : "/how-to-spike",
    "date"     : "August 23, 2022",
    "excerpt"  : "C‚Äôest une histoire bien connue, dans la vie de n‚Äôimporte quel d√©veloppeur : un ticket arrive dans le backlog, d√©crivant une probl√©matique relativement complexe. C‚Äôest parfois une question de technologie inconnue, ou parfois simplement un chantier ...",
  "content"  : "C‚Äôest une histoire bien connue, dans la vie de n‚Äôimporte quel d√©veloppeur : un ticket arrive dans le backlog, d√©crivant une probl√©matique relativement complexe. C‚Äôest parfois une question de technologie inconnue, ou parfois simplement un chantier un peu trapu. Je pense que toutes les √©quipes ont, au moins une fois dans leur vie, fait face √† ce genre de t√¢che impossible : c‚Äôest l‚Äôoccasion des regards d√©sesp√©r√©s, alors qu‚Äôun junior se lamente en disant ¬´ Mais par o√π est-ce qu‚Äôil faut commencer ? ¬ª. Et c‚Äôest l√† qu‚Äôon r√©pond : ¬´ Essaye de faire un spike ¬ª.\n\nFaire un spike ? Quelle excellente id√©e ! Encore faudrait-il savoir ce qu‚Äôest un spike, comment √ßa marche, et √† quoi √ßa sert.\n\nJe vous propose donc ensemble de voir dans cet article : qu‚Äôest-ce qu‚Äôun spike, quand l‚Äôutiliser, et comment consid√©rer qu‚Äôil est r√©ussi ?\n\nspike ‚Äìhelp üìö\n\nSi je devais citer Wikipedia, je dirais qu‚Äôun Spike, c‚Äôest ‚Äúune m√©thode de d√©veloppement de produit, d√©riv√©e de l‚Äôextr√™me programming, et qui cherche √† cr√©er le code le plus simple possible pour obtenir des solutions potentielles‚Äù.\n\nEn gros, le but d‚Äôun spike, c‚Äôest de r√©pondre √† la question ‚ÄúComment on fait ?‚Äù avec un prototype de code r√©alis√© gr√¢ce √† une s√©rie de petites √©tapes simples. Un spike n‚Äôest pas une formule magique qui va vous permettre de r√©aliser la t√¢che impossible que votre client vous a donn√©. En revanche, le spike va vous permettre de savoir si la t√¢che impossible ou compliqu√©e √† premi√®re vue est en fait possible, et si oui, comment.\nIl arrivera √©galement que votre spike vous permette de constater qu‚Äôune t√¢che donn√©e peut √™tre r√©alis√©e de plusieurs mani√®res : que ce soit en passant par des librairies diff√©rentes, avec une impl√©mentation changeante, ou autre chose encore. Dans ces cas, le spike va √©galement vous servir √† essayer ces diff√©rentes possibilit√©s, et √† choisir celle qui est la plus appropri√©e !\n\nLe moyen le plus simple est de proc√©der morceau par morceau. Alors je vous propose qu‚Äôon s‚Äôy mette maintenant, et qu‚Äôon regarde quoi faire !\n\n‚Äúkowalski, analysis !‚Äù üìä\n\nAvant toute chose, il faut savoir exactement ce que vous souhaitez faire. Rien ne sert de mettre la charrue avant les b≈ìufs.\n\nSi ce n‚Äôest pas fait, √©crivez noir sur blanc les lignes exactes qui vont d√©finir votre t√¢che comme finie. Que ce soit connecter votre utilisateur de fa√ßon s√©curis√©e, afficher une vid√©o sans heurt, ou juste avoir une page qui clignote en blanc et bleu, il faut que vous ayez une liste de bullet points, qui d√©finit pr√©cis√©ment ce que vous voulez faire.\n\nVotre objectif final est de r√©aliser tout ce que vous avez sur cette liste : strictement rien de moins, mais aussi strictement rien de plus ! Pas de demande implicite de type ‚ÄúAh mais je voulais aussi que l‚Äôimage soit visible en noir et blanc‚Äù : si ce n‚Äôest pas sur la liste, ce n‚Äôest pas √† faire.\n\nCette liste peut √™tre √©crite selon votre format favori : un cahier des charges, une s√©rie de directives Gherkin, l‚Äôimportant c‚Äôest qu‚Äôelle soit √©crite, claire et pr√©cise. En d‚Äôautres termes, vous d√©finissez ici votre propre cahier des charges.\n\nLe r√©sultat final doit donc √™tre quelque chose dans ce style :\n\nAs a client\nI want to see my product in 3 dimensions\nSo that I can know what it looks like\n\nAs a client\nI want to be able to rotate my product using the arrow keys\nSo that I can check it out entirely\n\nAs a client\nI want to be able to zoom on my product\nSo that I can see even the smallest details\n\n\nUne fois que vous savez quoi faire, on peut vraiment commencer √† mettre la main dans le code !\n\n// TODO : make the code below work üíª\n\nStop. L√¢chez tout.\n\nJe vous vois d√©j√†, votre liste de points en main, √† tenter de la faire rentrer dans votre gros projet √† grands coups de burin, de vous gratter la t√™te √† comprendre pourquoi √ßa ne rentre pas, et qu‚Äôest-ce qui a bien pu casser, cette fois.\n\nUn peu de calme : le but d‚Äôun spike n‚Äôest pas de faire tout fonctionner, pas du tout. Prenez de la distance, et on va y aller en douceur.\n\nPour commencer, isolez une partie de votre projet et de vos points objectifs. Il existe plusieurs moyens de s‚Äôy prendre : cr√©er un nouveau projet, cr√©er une nouvelle page avec seulement quelques composants, d√©charger votre backend‚Ä¶ On veut un environnement le plus propre possible.\nBeaucoup de projets sont vieux, et si mal con√ßus qu‚Äôil aurait fallu les jeter au bout de deux ans. On cherche ici √† se d√©tacher au maximum de cette dette technique.\n\nN‚Äôh√©sitez pas √† utiliser des mocks, des faux appels et r√©sultats au reste de votre application :  en simulant comment se comporte le reste de votre projet sans v√©ritablement y faire appel, vous diminuez au maximum votre marge d‚Äôerreur, et vous assurez que vous contr√¥lez la moindre information qui transite par votre code.\n\nMaintenant seulement, vous pouvez prendre votre clavier, et coder. Regardez comment impl√©menter chacun de ces points dans votre code propre de mani√®re √©pur√©e.\n√áa fonctionne du premier coup ? G√©nial, notez comment vous avez fait ! √áa ne marche pas ? Dommage, mais ce n‚Äôest pas une raison pour Ctrl+Z et recommencer. Notez bien ce qui n‚Äôa pas march√©, avant de retenter ! Si √ßa ne marche toujours pas au bout de 2/3 essais, pas de soucis, n‚Äôh√©sitez pas √† laisser ce point de c√¥t√© et passer √† un autre. Mais √©crivez tout, car cela va vous servir tr√®s bient√¥t !\n\nif(bug == true) { delete(bug); console.log(‚ÄúIt works !‚Äù); } ü§ñ\n\nIl peut cependant arriver que, parfois, tous vos efforts ne m√®nent √† rien. Vous avez d√©j√† pass√© plusieurs jours sur les diff√©rents sujets du spike, et vous n‚Äôavez pas encore identifi√© de solution pour faire fonctionner le tout.\nDans ce cas-l√†, pas de panique ! Il s‚Äôagit √©galement d‚Äôun des objectifs du spike. Apr√®s tout, si vous n‚Äôavez pas pu r√©aliser votre objectif dans un cadre r√©duit, il est bien probable que vous n‚Äôauriez jamais pu le faire fonctionner dans votre projet lui-m√™me.\n\nLes m√™mes points qu‚Äôindiqu√©s ci-dessus continuent de s‚Äôappliquer : notez ce que vous avez tent√© et les soucis rencontr√©s avec chaque impl√©mentation. Puis, continuez le processus d√©taill√© ici : ce n‚Äôest pas parce que votre code n‚Äôas pas fonctionn√© qu‚Äôil ne doit surtout pas √™tre pr√©sent√©. Peut-√™tre un de vos coll√®gues trouvera-t-il la ligne qui vous manque, ou le point-virgule que vous avez oubli√© : mais peut-√™tre aussi qu‚Äôil vous aidera √† comprendre ensemble pourquoi la solution ne fonctionne pas dans votre cadre.\nEt puis, vous pourrez alors vous poser la question : est-ce qu‚Äôil faut bien faire comprendre que la t√¢che demand√©e est irr√©alisable, ou est-ce qu‚Äôil faut pr√©voir un chantier pour r√©ussir √† trouver un moyen de remplir la requ√™te ?\n\nL‚Äôinstant doc üìù\n\nUne fois que vous avez termin√© de coder, il est temps pour vous de poser votre IDE, et de sortir votre outil de documentation favori : Confluence, Jira, que sais-je. \nPuis, √©crivez un compte-rendu de votre aventure. Pr√©sentez l‚Äôorigine de votre spike (Le Pourquoi), ce que vous avez tent√© (Le Comment). Expliquez ce qui a march√© et ce qui n‚Äôa pas march√© : cela vous servira lorsque vous impl√©menterez vraiment la feature !\nEnfin, √©crivez √©galement les √©tapes qu‚Äôil faudrait suivre pour terminer la feature : ajoutez un maximum de d√©tails techniques. Ce sera autant de probl√©matiques en moins pour le pauvre dev qui va r√©cup√©rer les US apr√®s vous.\n\nJe vous sugg√®re donc de faire un plan de ce type :\n\n\n  Probl√®me - Expliquez ici l‚Äô√©tat initial. Qu‚Äôest-ce qui √©tait demand√© ? Pourquoi avoir choisi de faire un spike ? Quel en est l‚Äôobjectif ?\n  Observations  - Indiquez l√† vos r√©flexions et le code que vous avez produit. Expliquez ce que vous avez tent√©, les probl√®mes rencontr√©s et les solutions √©tablies, vos pistes de r√©flexion. N‚Äôh√©sitez surtout pas √† d√©tailler !\n  Actions - Enfin, d√©taillez dans cette derni√®re partie ce qu‚Äôil restera √† faire afin de transformer ce spike en une feature fonctionnelle. Quels bugs corriger ? Quels points n‚Äôont pas encore √©t√© r√©alis√©s, et comment faire pour les r√©aliser ?\n\n\nPour la derni√®re √©tape, je vous conseille de r√©aliser un tableau d‚Äôactions SMART afin de d√©finir au mieux les t√¢ches √† r√©aliser.\nLe principe SMART suppose qu‚Äôune t√¢che doit √™tre compos√©es des cinq caract√©ristiques suivantes afin d‚Äô√™tre pertinente :\n\n  Elle doit √™tre Sp√©cifique, afin que l‚Äôobjectif soit clair et concis (Qu‚Äôest-ce que je dois faire ? Exemple de r√©ponse : ¬´ Il faut que l‚Äôimage d‚Äôun objet soit en 3D ¬ª)\n  Elle doit √™tre Mesurable, pour d√©finir un objectif quantifiable (Quant est-ce que ma t√¢che sera finie ? Exemple de r√©ponse : ¬´ Il faut que je puisse faire tourner l‚Äôimage avec les fl√®ches gauches et droites du clavier  ¬ª))\n  Elle doit √™tre Atteignable, sans demander de d√©crocher les √©toiles (Comment r√©aliser ma t√¢che ? Exemple de r√©ponse : ¬´ Utiliser la m√©thode Get3D de la librairie Easy3D ¬ª))\n  Elle doit √™tre R√©aliste au sujet en cours, donc n√©cessaire √† l‚Äôaccomplissement final (Est-ce qu‚Äôil est pertinent de prendre du temps pour faire √ßa ? Exemple de r√©ponse : ¬´ Afin que notre client puisse voir l‚Äôavant et l‚Äôarri√®re de nos produits ¬ª)\n  Elle doit √™tre d√©finie de fa√ßon Temporelle, afin de ne pas pouvoir s‚Äô√©terniser (Pour quand ma t√¢che doit-elle √™tre r√©alis√©e ? Exemple de r√©ponse : ¬´ A r√©aliser avant que la feature soit consid√©r√©e termin√©e ¬ª)\n\n\nDans le cas o√π une des t√¢ches que vous avez devis√© ne peut pas r√©pondre √† un de ces cinq points, alors il est probable qu‚Äôelle ne soit pas suffisamment pr√©cise : peut-√™tre la t√¢che manque-t-elle de cadre ou de contexte, ou le temps n√©cessaire pour la r√©aliser ne peut que difficilement √™tre justifi√©. Je vous invite alors √† la supprimer, ou √† la fusionner avec une autre jusqu‚Äô√† enfin pouvoir r√©pondre √† ces cinq questions !\n\nBien entendu, n‚Äôh√©sitez pas √† modifier le plan de cette documentation comme vous l‚Äôentendez : vous √™tes celui qui allez l‚Äôutiliser, apr√®s tout !\n\nLa doc est finie ? Il ne reste plus que deux √©tapes, puis on pourra enfin consid√©rer ce spike comme fini !\n\nPresentation_Spike.ppt üé¨\n\nAvant de pouvoir cl√¥turer ce spike, il serait bien d‚Äôavoir des retours ext√©rieurs. Pour √ßa, rien de mieux que de le pr√©senter √† votre √©quipe !\nOrganisez ensemble une r√©union, pas tr√®s longue. Au sein de mon √©quipe, une demi-heure suffit. Il vous faudra peut-√™tre un peu moins ou un peu plus de temps.\n\nUtilisez cette pr√©sentation afin de montrer, √©tape par √©tape, ce que vous avez r√©alis√©. Rappelez tout d‚Äôabord les objectifs du spike, avant d‚Äôexpliquer votre analyse du probl√®me et les objectifs que vous avez identifi√©s. Puis, pr√©sentez les diff√©rentes impl√©mentations que vous avez tent√©es, avant de conclure en montrant votre documentation et en expliquant les t√¢ches qui restent √† accomplir pour r√©aliser la feature objectif.\n\nIl est tr√®s important que vous ne pr√©sentiez pas uniquement le code que vous avez r√©ussi √† faire fonctionner, mais aussi vos tentatives √©chou√©es, et ce pour plusieurs raisons. Tout d‚Äôabord, il est tout √† fait possible qu‚Äôun de vos coll√®gues, en voyant votre pr√©sentation, r√©alise une de vos erreurs et vous l‚Äôindique. Mais surtout, si quelqu‚Äôun d‚Äôautre que vous r√©cup√®re une des t√¢ches restantes, il risque de tenter les m√™mes pistes que vous, et rencontrer les m√™mes probl√©matiques que vous !\n\nUne fois votre pr√©sentation termin√©e, d√©battez avec le reste de votre √©quipe. S‚Äôils sont d‚Äôaccord avec vous sur le plan d‚Äôaction que vous avez √©tabli gr√¢ce √† votre tableau SMART, il vous reste une toute derni√®re √©tape √† accomplir !\n\n‚ÄúHappily ever after‚Ä¶‚Äù üí≠\n\nMaintenant que tous vos co√©quipiers ont pu constater et valider votre travail, il ne vous reste plus qu‚Äô√† acter la mise en place : et pour √ßa, rien de mieux que, aux c√¥t√©s de votre Product Owner (Ou de l‚Äô√©quivalent dans votre √©quipe) de cr√©er des t√¢ches, User Story, post-its, ou quoi que ce soit, pour que les √©tapes restantes soient visibles et accessibles par tous !\n\nN‚Äôh√©sitez pas √† le guider pour ajouter encore une fois des d√©tails techniques dans ces US ou t√¢ches : vous avez r√©alis√© l‚Äôanalyse, il serait dommage de ne pas l‚Äôutiliser, et ce sera autant de temps gagn√© pour votre √©quipe. Tant que vous y √™tes, pensez aussi √† ajouter un lien vers votre documentation, ou vers une vid√©o de votre pr√©sentation‚Ä¶ Plus il y aura de d√©tails, mieux √ßa sera !\n\nIl est √©galement possible, comme indiqu√© plus haut, que la t√¢che qui a entra√Æn√© la r√©alisation de ce spike se d√©couvre √™tre impossible √† impl√©menter. Il s‚Äôagit l√† √©galement d‚Äôun point √† faire avec votre Product Owner, afin de d√©cider ensemble de la proc√©dure √† suivre : peut-√™tre faudra-t-il red√©finir les crit√®res d‚Äôacceptation, ou bien laisser tomber compl√®tement cette id√©e.\n\nreturn 0;\n\nVous avez fini votre spike ! Ce qui √©tait √† l‚Äôorigine une t√¢che complexe, confuse ou impossible √† pr√©voir, est d√©sormais divis√©e en une s√©rie d‚Äô√©tapes, qui sera d√©sormais bien plus ais√©e √† r√©aliser pour votre √©quipe. Alors, satisfait ?\n"
} ,
  
  {
    "title"    : "Handling dependencies failures in an API gateway",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, resiliency",
    "url"      : "/2022/08/12/backend-fallbacks.html",
    "date"     : "August 12, 2022",
    "excerpt"  : "Welcome to our second article about the backend architecture and its api gateway.\nIn the first part, we talked about the BFF and all services it depends on.\nToday we‚Äôre going to take a look at what to do when one of them (or many), fails to respon...",
  "content"  : "Welcome to our second article about the backend architecture and its api gateway.\nIn the first part, we talked about the BFF and all services it depends on.\nToday we‚Äôre going to take a look at what to do when one of them (or many), fails to respond.\n\nService dependencies\n\nAs seen previously, the BFF uses multiple data sources and services to create a full layout.\n\nThose services are used to gather the contents to be displayed in the application:\n\n  getting user personalisation data;\n  advertising and analytics configuration;\n  asking if the user has some authorizations.\n\n\nIf we don‚Äôt want our BFF to become one giant SPOF (1), we need to be resilient to the death (2) of those dependencies, any of them, at any time!\nYou must keep in mind that our top priority is to always be able to answer something readable to the frontend applications.\n\nDDD\n\nFirst thing first, we are using a DDD (3) approach for our modeling.\nThis means that we focus on the business, as described by our Product Owner. We try not to worry about the various implementation of our backend‚Äôs friends and their different services.\n\nA picture is always easier to understand.\n\n\n\nAbove, we can see that when a user ask for a layout A, we are looking to resolve who is A.\nFrom the domain point of view, the page collection is only an interface.\n\nIn the picture below, we see the ‚ÄúPage collection implem (Infra)‚Äù.\nIt‚Äôs a layer implementing the interface defined in the domain. It uses multiple clients that call the services behind.\nIt‚Äôs its responsibility to chose which service to look on for the page.\n\n\n\nDDD is a too large subjects to be perfectly defined in this article. If you want to dig deeper into it, there are multiple great reads, feel free to check them out!\nNow, how does this help us?\n\nHandling failures\n\nFailures handling is done by the middle layer seen in the previous example.\nIts goal is to catch error (4), and convert them to something expected and defined by the interface.\n\nThat said, its responsibility is not to know what the expected answer is. To do that, we use the domain.\n\nLet‚Äôs see with a small code sample.\n\nNote: The following example is not a real use-case, but it‚Äôs representative and simple enough to illustrate how it works.\n\nIn the code below, we see a class that represents the subscribing status of a user, which has two properties:\n\n  hasAccess controls whether the user can read protected contents;\n  isSubscribed is used in analytics, and to show subscription pages.\n\n\n&amp;lt;?php\nfinal class SubscribeStatus\n{\n    private function __construct(\n        public readonly bool $hasAccess,\n        public readonly bool $isSubscribed,\n    ) {\n    }\n\n    public static function createAnonymous(): self\n    {\n        return new self(false, false);\n    }\n\n    public static function createSubscribed(): self\n    {\n        return new self(true, true);\n    }\n}\n\n\nTo create such an object, we use either one of the two static functions, depending on the status we get from the subscriptions API.\nThis is done in the middle layer, but the business is kept in the domain.\n\nTo handle the failure, we add a new named constructor, dedicated to this specific case.\n\n    public static function createUnknown(): self\n    {\n        return new self(true, false);\n    }\n\n\nWhen an error happens and we can‚Äôt retrieve the user subscription status, we now have a fallback option.\nWith this fallback option, the user will:\n\n  be able to access any content, it‚Äôs better to let an anonymous user access a content it should not, that blocking a paying customer;\n  still be reported as not subscribed and will see all available offers.\n\n\nMost of the time, the answer is even simpler than this one.\n\nAnother example would be user‚Äôs viewing statuses. If we can‚Äôt retrieve them, we don‚Äôt display any progress bar.\nUsers won‚Äôt be able to tell if they have seen a content, but they will still be able to navigate the application.\n\nInfrastructure solution, the stale cache\n\nIn some cases, the above solution doesn‚Äôt work.\nFor example, contents information cannot be replaced by default values. If we don‚Äôt know about a video or a program, we cannot guess what it is.\n\nLuckily, we can rely on the stale cache.\nStale cache is an old cache entry which is expired. When the cache finds such entry, it usually ignores it and asks for a new version of the response.\nIn case of failure, we can use the available staled version.\n\n\n\nThe limitation is that a response must have been cached at least once, in order to have a staled version.\n\nWhen there is no stale cache, we don‚Äôt display the content (5).\n\nSo far, we are only using it with http implementation:\n\n  called API must answers with stale-if-error cache directive, it allows for the response to be used while stale when an error happens;\n  called API can answer with stale-while-revalidate cache directive, for better performances;\n  calling API can query with max-stale cache directive, to use stale response see the mdn for more on those headers;\n  on the client side, we are using the Kevinrob/guzzle-cache-middleware to do the job.\n\n\nFor an entry cached for up to 10 minutes (answered with max-age), we allow up to 4 hours of stale cache (with stale-if-error).\nSince we are using a shared cache, we are using max-stale when querying, with a random value up to 1 hour.\nThis makes most requests use the last stale response while one of them ask for a fresher response.\nThose values are chosen according to our platform usages where peak visitor last for about 2 to 3 hours at night.\n\nWe plan to expand its usage to other kinds of cached entries, such as manually saved data, and database queries.\n\nConclusion\n\nIn today‚Äôs post, we have seen how we handle the loss of our dependencies by anticipating their potential failures and preparing default acceptable behaviours.\n\nNext time, we will see how we can spare some traffic on those dependencies when they‚Äôre struggling with traffic.\n\nNotes\n\n  SPOF, as single point of failure since all frontend applications have to rely on the BFF, I cannot resist linking this excellent xkcd.\n  By ‚Äúdeath‚Äù, we mean anything unexpected. It can be a 500 error code, a timeout, a wrong content. We will talk a bit more about this in the next article.\n  DDD, as domain driven design, you can read more about it on Martin FOWLER‚Äôs website.\n  Throwing errors is still allowed, but restricted to domain exceptions, and must be specified in the method‚Äôs declaration in the interface (i.e. via a comment).\n  There will be a dedicated article on partial rendering.\n\n\nFrom the same series\n\n\n  What‚Äôs a BFF\n  Handling API failures in a gateway\n  What‚Äôs an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  üá∫üá∏ Encrypt AWS AMIs: one way to do it wrong\n  üá´üá∑ Bedrock √† la kubecon 2022 (4 articles)\n\n"
} ,
  
  {
    "title"    : "How to ingest 400GB of logs per hour?",
    "category" : "",
    "tags"     : " onprem, cdn, logs, aws, cloud, nginx, vector, lambda, s3, glue, athena",
    "url"      : "/2022/08/08/private-cdn-logs.html",
    "date"     : "August 8, 2022",
    "excerpt"  : "Bedrock Streaming is a company that sells a white labeled streaming and live platform. Our customers are media groups, TV channels, and streaming companies. Our goal is to deliver a state-of-the-art streaming platform to our customers.\n\nTo achieve...",
  "content"  : "Bedrock Streaming is a company that sells a white labeled streaming and live platform. Our customers are media groups, TV channels, and streaming companies. Our goal is to deliver a state-of-the-art streaming platform to our customers.\n\nTo achieve this goal, we have our own Content Delivery Network (CDN), made of several bare metal servers racked in our Data Centers. Those servers run Nginx and are designed to output hundreds of Gbps (several tens of Pb per month) to end-users. We use them to cache video content at our infrastructure‚Äôs edge.\n\nThis increases efficiency of the platform 96 times out of 100, as video traffic doesn‚Äôt have to flow all the way through our infrastructure, and improves user experience as it serves video faster. Also, it diminishes the cost of our Video On Demand (VOD) infrastructure as we need less servers in VOD Stack.\n\nThis in-turn increases end-users (clients of our customers) satisfaction with the service.\n\nWho needs to ingest 400GB of logs per hour anyway?\nEvery time someone watches a video, it generates traffic on our CDN, resulting in a lot of access logs. Without filtering, it averages to 400GB uncompressed logs per hour.\n\nThis is why, at first, we chose to not log 2XX or 3XX HTTP codes. We had too many of them, and we considered them not as worth it as 4XX and 5XX. The 4XX and 5XX can be especially useful for debugging a particular situation or, from a broader perspective, improving the user experience.\n\nThis was the kind of Nginx configuration we had deployed:\nmap $status $loggable {\n    ~^[23]  0;\n    default 1;\n}\naccess_log /path/to/access.log combined if=$loggable;\n\n\nGiving autonomy for all teams on logs\n\nAt the end of 2021, the finance team approached us with a challenge: how to bill our customers based on their end-users CDN usage?\nThis was in fact a need we already anticipated, we tried the nginx module Traffic_accounting, but it did not satisfy us fully. This module calculates and exposes metrics on-the-fly, which is CPU and memory intensive, especially above 50Gbps of traffic per server.\n\nWe also had another objective that wasn‚Äôt addressed with the nginx module. We needed to give autonomy to QA, Video, Data, and Finance teams. We wanted to allow them to use CDN logs when they needed without having to ask for it, and ideally in a practical and unified way.\n\nThe company philosophy states that we are user obsessed and that we do not finger point. We work as a team to offer the best user experience, this is why we make all our logs available to all teams. We didn‚Äôt come around to do it for the CDN as the volume of logs was too much of a constraint.\n\nTechnical Solution\n\nAt Bedrock, we like to keep things simple. We think our CDN main mission is to serve video as efficiently as possible. Our CDN‚Äôs servers can‚Äôt keep PetaBytes of logs on their disks. This is why we chose to output logs to Amazon S3.\n\nThe real benefit to using S3 is that you can easily plug it into Glue and Athena which allows you to request TeraBytes of data easily.\n\n\n\nSending logs to S3: Vector\n\nTo send logs from our CDN servers to Amazon S3 bucket, we had many options, but chose to test two approaches: Fluentd and Vector. Fluentd is the legacy one, and Vector the new rusty one.\n\nAfter a quick evaluation, we decided to go with Vector as it seemed more memory efficient and output more Logs Per Second under heavy load than Fluentd.\n\n\nSource: Who is the winner ‚Äî Comparing Vector, Fluent Bit, Fluentd performance from Ajay Gupta\n\n\nWe have Nginx and Vector installed on the CDN servers. Nginx now outputs all the access logs to a file. Vector reads the file, compresses logs to GZIP format and every 10Mb sends the logs to S3. Nginx may generate at peak 600GB of logs; we only send 10GB.\n\nThose logs are then locally cleaned by Logrotate.\n\nStoring logs: S3\nWe chose to store logs on an S3 bucket. We figured it was the most scalable and time efficient. S3 buckets can grow to PetaBytes easily. It is a few terraform lines away, this is convenient as we handle all our infrastructure with Terraform.\n\nWe configured our bucket to use several lifecycle policies. One to automatically clean logs after 365 days, another to remove incomplete uploads, and another one to immediately remove files with a delete marker. Also, we configured the storage class in intelligent tiering mode to store logs according to their access frequency.\n\nThis will permit us to diminish the cost of our S3 bucket and not have an ever-increasing S3 bill.\n\nPartitioning logs on S3: Lambda stack\n\nOnce logs are stored in S3 bucket, we need to classify and sort them in order to extract valuable intel. At Bedrock, we already use a modified version of a lambda stack, that does just that. Originally designed for Cloudfront, we have been using it also for Fastly and now for our Private CDN. You can find the original version at AWS Sample Github.\n\nWe have 2 different parts in this lambda stack.\n\n\nsource: moveAccessLogs\n\nThe first part is called by S3 Event when a new file is pushed to a specific path. This lambda moves the file to a path assigned per server and per hour. This way, logs are stored for each server, each month, each day and each hour in a separate prefix.\n\n\nsource: transformPartition\n\nThen, another lambda transforms logs into Parquet format. Parquet is an open source format from the Apache Foundation. It is commonly used in big data. It takes up little space and is very effective.\n\nWe chose to use AWS glue in order to create a database of our logs. The columns of the table are based on our log format. We can then request everything we want in Athena.\n\n\n\nWe are now capable of extracting the bytes sent from a particular virtual host and sum it over a month for all CDN servers to bill our customers.\nThose logs are now available for all the teams who may need them to improve their application or to debug an issue they are facing.\n\nConclusion\nWe chose Vector to transport our private CDN logs to an S3 Bucket. Then, we chose to reuse an AWS Stack using Lambda and Glue to extract information from these logs, asynchronously. This stack is used in production for several months on other projects.\nAll the teams that needed to extract value from our CDN logs are now autonomous to do so. We are now able to bill our customers based on their CDN usage.\n"
} ,
  
  {
    "title"    : "Retour sur la conf√©rence MiXiT 2022",
    "category" : "",
    "tags"     : " conference, agile",
    "url"      : "/2022/07/28/retour-sur-mixit-2022.html",
    "date"     : "July 28, 2022",
    "excerpt"  : "\n\nMiXiT est une conf√©rence ‚Äúavec des cr√™pes et du c≈ìur‚Äù qui se d√©roule √† Lyon. Les sujets sont assez vari√©s abordant autant l‚Äôagilit√©, que la programmation, le droit ou encore l‚Äôhistoire de l‚Äôinformatique.\n\nVoici un r√©sum√© des conf√©rences de l‚Äô√©di...",
  "content"  : "\n\nMiXiT est une conf√©rence ‚Äúavec des cr√™pes et du c≈ìur‚Äù qui se d√©roule √† Lyon. Les sujets sont assez vari√©s abordant autant l‚Äôagilit√©, que la programmation, le droit ou encore l‚Äôhistoire de l‚Äôinformatique.\n\nVoici un r√©sum√© des conf√©rences de l‚Äô√©dition 2022 qui nous ont le plus marqu√©es.\n\nHow to build the alert system that France deserves?\n\nGa√´l Musquet nous a d‚Äôabord expliqu√© le r√¥le de Gustave Ferri√©, qu‚Äôil consid√®re comme le premier hacker, qui a install√© des m√¢ts de t√©l√©graphe sans fil en 1902, entre les √©metteurs en Martinique, pour remplacer le c√¢ble t√©l√©graphique, d√©truit lors de la catastrophe de la montagne Pel√©e du 8 mai 1902. Cet homme avait saisi l‚Äôint√©r√™t d‚Äôavoir un syst√®me de communication fiable.\n\nGa√´l Musquet nous explique ce qu‚Äôon est en droit d‚Äôattendre en 2022 d‚Äôun pays moderne, concernant les alertes sur les risques majeurs, qui varient selon notre emplacement (du tsunami √† la rupture de barrage artificiel).\n\nIl nous incite √† lire le DICRIM de notre ville (celui de Lyon) ainsi qu‚Äô√† nous procurer un poste de radio √† piles, car dans l‚Äô√©ventualit√© d‚Äôun moment catastrophique sans Internet et sans satellites, comment ferons-nous pour nous tenir au courant de ce qu‚Äôil faut faire pour rester en vie ?\n\nPage du talk sur le site de MiXiT et voir le replay\n\nMeet NULL the UNKNOWN\n\nDans cette conf√©rence, La√´tiia Avrot entame un rappel de la norme SQL, que PostgresQL impl√©mente au plus pr√®s, sur la valeur de NULL en SQL. Et la valeur UNKNOWN est √©galement abord√©e. Notamment la complexit√© induite par le fait qu‚Äôun champ de type Boolean peut se retrouver avec comme valeurs possibles : True, False, UNKNOWN et NULL. Cela donne un syst√®me √† quadruple valeur. Pour un champ typ√©.\n\nNULL est plus facile √† d√©finir par ce qu‚Äôil n‚Äôest pas qu‚Äôen expliquant ce qu‚Äôil est.\nUne option int√©ressante pour mettre en √©vidence la valeur NULL dans PostgresQL est d‚Äôen d√©finir nous-m√™me une valeur affich√©e.\n\nEnsuite, La√´titia nous propose un Quizz. Sur une base de donn√©es qu‚Äôon conna√Æt, chaque fois la m√™me question est pos√©e sur ‚ÄúCombien de lignes vont √™tre retourn√©es par la requ√™te SQL ?‚Äù\n\nC‚Äôest int√©ressant, car chaque question comporte un degr√© de complexit√© √©lev√© impliquant l‚Äôusage de la valeur NULL, tout en suivant la logique de la norme SQL. Cerise sur le g√¢teau, La√´titia propose en ‚ÄúR√©ponse D‚Äù, le nom d‚Äôune scientifique c√©l√®bre et nous en donne une courte biographie √† chaque question.\n\nLiens\n\n\n  Blog de l‚Äôoratrice, La√´titia Avrot\n  Page du talk sur le site de MiXiT et voir le replay\n\n\nParlez de vous, faites des feedbacks\n\nLe feedback est un outil communicationnel qui permet de formuler un avis sur une situation pass√©e dans le but de g√©rer les situations futures.\n\nOn peut trouver plusieurs formes de feedbacks :\nle feedback est √† destination de la personne, pour l‚Äôaider √† s‚Äôam√©liorer. Elle peut d√©cider de le suivre ou non,\nla demande que l‚Äôon fait √† quelqu‚Äôun est √† notre b√©n√©fice (on demande √† la personne de changer un comportement qui nous g√™ne) en laissant la possibilit√© √† la personne de d√©cider si elle veut ou non r√©pondre favorablement √† cette demande,\nl‚Äôexigence qui est aussi √† notre b√©n√©fice, mais pour laquelle on ne laisse pas le choix (dans le cadre d‚Äôune relation hi√©rarchique)\n\nJulie Quilli√© propose un mod√®le de feedbacks bas√© sur la CNV (Communication Non Violente) et qui peut se r√©sumer de la mani√®re suivante.\n\nFeedback bas√© sur la CNV (Communication Non Violente)\n\n\n  On v√©rifie la disponibilit√© de la personne en lui demandant si elle est d‚Äôaccord pour qu‚Äôon lui fasse des feedbacks et sous quelle forme.\n  On formule le feedback :\n  D√©crire une Observation, les faits (= pas de jugement)\n  Exprimer le Sentiment que cette situation a engendr√©\n  Expliquer le Besoin qui est la source du sentiment ressenti\n  et finir par faire une Demande (= r√©alisable, formul√©e positivement, pr√©cise)\n\n\nUn exemple :\n  Nous avions rendez-vous √† 12h et il est 12h30 = observation, factuel.\n  Je suis tr√®s f√¢ch√© car je m‚Äô√©tais organis√© pour √™tre √† l‚Äôheure = le sentiment\n  C‚Äôest important pour moi de ne pas perdre de temps et de pouvoir rester libre dans mon organisation = le besoin\n  La prochaine fois que tu sais que tu seras en retard, peux-tu stp m‚Äôappeler d√®s que possible pour me le signaler ? De cette mani√®re, je peux me r√©organiser facilement. = la demande\n\n\n  On v√©rifie ce qui a √©t√© re√ßu par la personne. On lui propose de nous reformuler ce qu‚Äôelle en a retenu. Cela permet de v√©rifier que le message que l‚Äôon voulait faire passer a bien √©t√© entendu.\n\n\n2√®me possibilit√© pour faire un feedback : le feedback en 4 temps\n\n\n  On demande √† la personne ce qu‚Äôelle a aim√© dans ce qu‚Äôelle vient de faire\n  \n    On lui demande ensuite ce qu‚Äôelle aurait aim√© faire diff√©remment\n\n    On lui demande si elle veut qu‚Äôon lui donne notre feedback\n  \n  ‚ÄúMoi, j‚Äôai aim√© ‚Ä¶, parce que ‚Ä¶ ‚Äù : on parle de ce que √ßa nous a apport√© (clart√©, motivation, inspiration, soutien, etc.)\n  ‚ÄúEt j‚Äôaurais aim√© ‚Ä¶  de diff√©rent, parce que ‚Ä¶ ‚Äù on parle de ce que √ßa nous apporterait (clart√©, motivation, inspiration, soutien, etc.)\n\n\nEt en bonus : ‚ÄúPeux-tu me dire comment tu re√ßois ce que je te dis ?‚Äù\n\nPage du talk sur le site de MiXiT et voir le replay\n\nArr√™tez l‚Äôauto-sabotage et sortez de la boucle (syst√©mique)\n\nDans cet atelier, Albane Veyron nous explique que nous avons tous des croyances sur nous-m√™mes et sur les autres. Les croyances sont des pens√©es qui sont des v√©rit√©s, pour nous. Elles ont plusieurs origines : l‚Äôenfance, notre cercle social et notre exp√©rience de vie.\n\nLes croyances peuvent √™tre aidantes ou limitantes.\n\nL‚Äôatelier commence par une premi√®re phase qui consiste √† reconna√Ætre une de ses croyances limitantes :\n\n  les g√©n√©ralisations : personne, tout le monde, toujours, tout le temps, jamais, trop, je dois, il faut, pas assez\n  les barri√®res infinies aka les bonnes excuses pour ne pas passer √† l‚Äôaction : j‚Äôaimerais, mais ‚Ä¶ / je pourrais, mais ‚Ä¶\n  les sensations de d√©j√† vu : les blocages et les situations r√©currentes\n\n\nUne fois qu‚Äôon a rep√©r√© une de ses croyances limitantes, on l‚Äô√©crit sur une feuille et on va ensuite d√©composer cette croyance et r√©fl√©chir √† :\n\n  son origine : d‚Äôo√π nous vient cette croyance ? depuis combien de temps fait-elle partie de nous ? nous vient-elle de notre √©ducation ?\n  les b√©n√©fices : quels b√©n√©fices nous apporte cette croyance ? qu‚Äôest-ce qu‚Äôelle nous permet ?\n  les inconv√©nients / les freins : en quoi cette croyance nous g√™ne et quels sont les impacts sur notre vie (pro ou perso) ?\n  les contradictions : a-t-on d√©j√† fait quelque chose ou √©t√© dans une situation qui vient contredire cette croyance ?\n\n\nOn va ensuite venir agr√©menter notre croyance avec tous ces √©l√©ments puis, pour finir, transformer notre croyance limitante en une croyance aidante.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nComment fonctionne un gestionnaire de mots de passe\n\nLes mots de passe sont partout. Ils nous permettent d‚Äôacc√©der √† nos photos, nos comptes bancaires, nos documents de sant√© et bien d‚Äôautres donn√©es sensibles que l‚Äôon ne souhaite pas voir aux mains d‚Äôindividus que l‚Äôon ne conna√Æt pas.\nTout le monde sait que l‚Äôon doit avoir des longs mots de passe mais comment tous les retenir ? C‚Äôest l√† que les gestionnaires de mot de passe entrent en jeu. Mais peut-on leur faire confiance ? Comment √ßa marche au juste ? C‚Äôest √† cette question qu‚Äôa souhait√© r√©pondre Eric Daspet pendant sa conf√©rence.\n\nLe r√¥le d‚Äôun gestionnaire de mots de passe est de permettre √† son utilisateur d‚Äôutiliser qu‚Äôun seul mot de passe pour ensuite laisser l‚Äôoutil g√©n√©rer et m√©moriser tous les autres mots de passe. On a plus qu‚Äô√† retenir un seul mot de passe qui peut donc √™tre long et complexe. L‚Äôexercice de m√©moire sera alors moins compliqu√© que si on en avait plusieurs √† retenir.\n\n√Ä travers son expos√©, on d√©couvre un peu plus tous les proc√©d√©s de cryptographie utilis√©s afin de g√©rer les mots de passe que l‚Äôon va cr√©er ou modifier en utilisant ces outils.\nGr√¢ce √† de nombreux sch√©mas, il explique clairement les diff√©rentes √©tapes de chiffrements utilis√©es que ce soit pour la cr√©ation du mot de passe ma√Ætre, la cr√©ation et le changement des mots de passe, l‚Äôaffichage des mots de passe et m√™me le fonctionnement du partage de mots de passe (lorsque celui-ci existe dans l‚Äôoutil).\n\nOn d√©couvre pendant cette heure que les gestionnaires de mots de passe ne cherchent pas √† r√©inventer la roue en mati√®re de cryptographie mais s‚Äôappuient sur des concepts d√©j√† √©prouv√©s et robustes. On apprend aussi que tout est chiffr√© de bout en bout et que seul celui qui d√©tient le mot de passe ma√Ætre (l‚Äôutilisateur donc, m√™me l‚Äôoutil ne le conna√Æt pas et n‚Äôen a pas besoin) peut interagir avec les mots de passe cr√©√©s. Rassurant, non ? En tout cas, me voil√† maintenant pr√™t √† expliquer autour de moi pourquoi il est grand temps de passer √† un gestionnaire de mot de passe !\n\nPage du talk sur le site de MiXiT et voir le replay\n\nOptimiser votre revue de code avec le rebase interactif\n\nGIT est un outil bien connu des d√©veloppeurs de nos jours, mais d√®s qu‚Äôon s‚Äô√©carte des commandes traditionnelles (checkout, commit et push), on sait bien moins ce que l‚Äôon peut faire d‚Äôautre avec.\n\nSonia Seddiki nous explique ici comment rendre la revue de code, souvent longue et fastidieuse, plus simple et agr√©able pour nos coll√®gues avec quelques astuces qu‚Äôelle a partag√©es avec nous lors d‚Äôun live coding.\nContrairement √† l‚Äôid√©e que j‚Äôen avais, le rebase interactif n‚Äôest pas l√† que pour nettoyer les noms de commit sans aucun sens que j‚Äôavais mis dans la pr√©cipitation mais que c‚Äôest un outil bien plus puissant.\n\nElle nous a ainsi montr√© comment elle utilise cette commande afin d‚Äôorganiser et de donner une chronologie √† son travail rendant ainsi la revue de code plus facile. Elle a ainsi, devant nos yeux, chang√© des fichiers de commits, r√©organis√© l‚Äôordre des commits et tout √ßa sans alt√©rer le code produit.\n\n√âvidemment, c‚Äôest une habitude √† prendre, elle-m√™me le souligne que ce n‚Äôest pas facile d‚Äôexporter cette bonne pratique au sein des √©quipes avec qui elle travaille. Mais la d√©monstration m‚Äôa convaincu, je vais m‚Äôessayer √† cette pratique et qui sait, un jour j‚Äôarriverai peut-√™tre √† mon tour √† convaincre des gens de mon √©quipe √† en faire de m√™me.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nViolence Herm√©neutique - Comment √©viter le malaise\n\nLe MiXiT est aussi un √©v√®nement nous permettant d‚Äôouvrir notre esprit √† des connaissances qui sortent de notre quotidien. Cette conf√©rence anim√©e par Romeu Moura et Sara Dufour en fait partie. Ce talk nous fait d√©couvrir le concept d‚Äôherm√©neutique, d√©fini en d√©but de pr√©sentation comme √©tant ‚ÄúLa connaissance d‚Äôun concept permettant l‚Äôinterpr√©tation‚Äù. Si vous n‚Äôavez rien compris √† cette d√©finition √† ce stade, c‚Äô√©tait √©galement mon cas.\n\nMalgr√© cette introduction confuse, petit √† petit, en allant de plus en plus dans le d√©tail, des sujets apparaissent et donnent sens √† ce concept. On y parle de syst√©misme, de charge mentale, de patriarcat et autres syst√®mes de notre soci√©t√© dont l‚Äôexercice de compr√©hension va plus loin que leur simple mot ou leur d√©finition. L‚Äôherm√©neutique consiste √† comprendre les fondements et rouages d‚Äôun syst√®me, qu‚Äôon y appartienne ou non.\n\nMais notre soci√©t√©, et l‚Äôhumain, tend √† compliquer cet exercice de compr√©hension de concept. C‚Äôest l√† qu‚Äôon arrive √† la notion de violence herm√©neutique, √† savoir tous les m√©canismes conscients et inconscients, syst√©miques ou non, internes ou externes, qui vont venir entraver et contraindre l‚Äôherm√©neutique. De r√©els freins √† la compr√©hension d‚Äôun syst√®me. Ils peuvent prendre plusieurs formes, comme la notion de norme, le fait de nier l‚Äôexistence d‚Äôun syst√®me ou de r√©futer un sujet du simple fait qu‚Äôil soit consid√©r√© tabou. On y retrouve √©galement la d√©formation de mots, et le fameux ‚Äúwokisme‚Äù.\n\nIl s‚Äôagit d‚Äôune conf√©rence passionnante, d√©rangeante et √©clairante que je conseille √† tous. Le d√©but pi√©tine un peu, mais le voyage en vaut la peine.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nDesigner pour le service public\n\nCela peut faire un peu peur dit comme √ßa, mais je suis all√© sceptique √† cette conf√©rence d‚ÄôAnne-Sophie Tranchet. J‚Äô√©tais rattach√© √† une image peu flatteuse des outils du service public, alors que ces derniers ont connu une vraie progression ces derni√®res ann√©es. Anne-Sophie fait partie du programme beta.gouv qui intervient aupr√®s des administrations pour les services num√©riques.\n\nC‚Äôest arm√© des bonnes pratiques de nos m√©tiers que Beta.gouv a la mission de transformer et d‚Äôaccompagner les services publics. On y apprendra le parcours d‚ÄôAnne-Sophie, ce que travailler pour le service public veut dire, ainsi que les projets et challenges qui en d√©coulent. Leur m√©thodologie centr√©e utilisateur leur permet de travailler en it√©ration, et de d√©livrer de la valeur, en incubation d‚Äôabord, puis jusqu‚Äô√† un d√©veloppement national en fonction des retours sur le service.\n\nOn peut citer quelques r√©alisations Beta.gouv comme la plateforme dossierfacile.com, qui facilite la cr√©ation de dossier pour une location, ou 1000 Premiers Jours, qui d√©livre des informations et un accompagnement sur la grossesse et les 2 premi√®res ann√©es de l‚Äôenfant\n\nPage du talk sur le site de MiXiT et voir le replay\n\nNos autres conf√©rences coup de coeur\n\n\n  Ma vie est un ticket de Romain Couturier, une conf√©rence racont√© avec dessins l√©g√®re et qui donne des id√©es pour lutter contre la mauvaise utilisation des outils de ticketing\n  Tout ce que l‚Äôon ne vous pas dit sur l‚ÄôIA de Am√©lie Cordier, une conf√©rence pleine d‚Äôhumour sur ce qu‚Äôest et n‚Äôest pas une IA\n\n"
} ,
  
  {
    "title"    : "Comment appliquer automatiquement des modifications sur une codebase JS ü§ñ",
    "category" : "",
    "tags"     : " javascript, outil, cytron, frontend, react, refactor, js",
    "url"      : "/refactorer-avec-jscodeshift",
    "date"     : "July 26, 2022",
    "excerpt"  : "Dans cet article, je vais vous pr√©senter JSCodeshift, une libraire qui va vous permettre d‚Äôanalyser et appliquer automatiquement des modifications sur du code Javascript ou Typescript.\n\nCas d‚Äô√©cole üë®‚Äçüéì\n\nMaintenir √† jour les d√©pendances de nos proj...",
  "content"  : "Dans cet article, je vais vous pr√©senter JSCodeshift, une libraire qui va vous permettre d‚Äôanalyser et appliquer automatiquement des modifications sur du code Javascript ou Typescript.\n\nCas d‚Äô√©cole üë®‚Äçüéì\n\nMaintenir √† jour les d√©pendances de nos projets JS est l‚Äôune des r√®gles primordiales que nous nous effor√ßons de bien respecter pour ne pas avoir √† jeter nos applications tous les deux ans. üóë\n\nCette t√¢che exige souvent d‚Äôun d√©veloppeur plus de travail que de simplement changer les versions des libraires dans le package.json.\nSi une d√©pendance est utilis√©e dans diff√©rentes parties du code et qu‚Äôun breaking-change est introduit, on peut vite se retrouver avec des centaines de fichiers √† modifier manuellement.\n\n\n\n\n‚ÑπÔ∏è Exemple d&#39;un project Javascript qui ne respecte pas cette r√®gle\n\n\n\nC‚Äôest un probl√®me de ce genre que nous avons rencontr√© lors de la mise √† jour de notre librairie d‚Äôinternationalisation sur notre web app React en JS.\n\nApr√®s mise √† jour, l‚Äôappel √† l‚ÄôAPI de la librairie change de forme¬†:\n//Before\nconst t: (\n    translationKey: string,\n    // All options are passed as parameters\n    data?: object, // Data used for interpolation\n    number?: number, // Amount used for plural form\n    general?: boolean, // Use general plural form\n    renderers?: object // JSX renderers\n) =&amp;gt; string\n\n//After\nconst t: (\n    translationKey: string,\n    // Object containing all options\n    options?: {\n      data?: object, // Data used for interpolation\n      number?: number, // Amount used for plural form\n      general?: boolean, // Use general plural form\n      renderers?: object // JSX renderers\n    }\n) =&amp;gt; string\n\n\nPlus simplement, quelques exemples de transformations¬†:\n// Before\nconst title1 = t(&#39;translationKeyExample&#39;)\nconst title2 = t(labelKey, { someData }, aNumber);\nconst title3 = t(&#39;translationKeyExample&#39;, undefined, 0);\n\n// After\nconst title1 = t(&#39;translationKeyExample&#39;); // Basic usecase with only one argument, nothing changed on this one\nconst title2 = t(labelKey, { data: { someData }, number: aNumber });\nconst title3 = t(&#39;translationKeyExample&#39;, { number: 0 });\n\n\nDans le cas le plus basique sans les arguments optionnels t(‚ÄòtranslationKey‚Äô) nous n‚Äôavons rien √† modifier, mais dans les autres cas, il y a du changement √† faire. üßπ\n\nLes solutions que nous avons √©cart√©es ‚ùå\n\n\n  Avec un Find All, trouver toutes les utilisations de la librairie et modifier les appels probl√©matiques √† la main.\n    \n      Cette solution est la plus simple, mais peut √™tre tr√®s r√©p√©titive, ce qui augmente la probabilit√© de faire une erreur. On aura du mal √† uniquement filtrer les cas sp√©cifiques qui nous int√©ressent.\n    \n  \n  Utiliser des RegExp pour mieux cibler les cas sp√©cifiques\n    \n      Cela nous a permis de faire rapidement une estimation approximative du nombre de cas qu‚Äôil nous faudrait modifier, mais nous avons eu du mal √† cibler correctement tous les appels et la modification se fait toujours √† la main.\n    \n  \n  Cr√©er un fichier de d√©finition TypeScript pour la librairie, et laisser le Language Server Protocol ou son IDE trouver les appels probl√©matiques\n    \n      La solution la plus rapide et la plus fiable pour la partie d√©tection, mais qui demande toujours de faire les modifications √† la main.\n    \n  \n\n\nMais il nous restait encore un Joker pour cette t√¢che. üÉè\n\nJSCodeshift ü™Ñ\n\nCette librairie permet d‚Äôexposer facilement l‚ÄôAbstract Syntax Tree, autrement dit la repr√©sentation du code apr√®s le parsing des fichiers.\nNous pouvons ainsi √©crire des scripts qui nous permettent de parcourir cet arbre, de le modifier facilement, d‚Äôappliquer les modifications et de les formater.\nCes scripts s‚Äôappellent des codemods.\n\nPour en savoir un peu plus sur l‚ÄôAbstract Syntax Tree, je vous conseille de jeter un coup d‚Äô≈ìil √† ASTExplorer qui vous permet de visualiser l‚ÄôAST d‚Äôun fichier facilement pour en comprendre le fonctionnement.\n\nQuelques librairies ont propos√© des codemods lors de leurs grosses mises √† jour, par exemple React avec react-codemod.\n\n\n\n\n‚ÑπÔ∏è Capture d&#39;√©cran du site ASTExplorer\n\n\n\nEn application üí™\n\nmodule.exports = function (file: FileInfo, api: API) {\n  const j = api.jscodeshift;\n\n  // If we don&#39;t find any &quot;Translate&quot; string inside our file, we can assume that it&#39;s safe to skip it\n  const regex = new RegExp(&#39;Translate[(]&#39;, &#39;i&#39;);\n  if (!regex.test(file.source)) {\n    return null;\n  }\n\n  return j(file.source)\n    .find(j.CallExpression, {\n      callee: {\n        type: &#39;Identifier&#39;,\n        name: &#39;t&#39;,\n      },\n    })\n    .filter(filterOutSimpleUsages)\n    .map(mutatePath(j))\n    .toSource();\n};\n\n\nDans la fonction principale du script, j‚Äôai utilis√© une expression r√©guli√®re pour filtrer les fichiers qui ne poss√®dent pas la cha√Æne de caract√®res Translate(.\nCeci permet de gagner un peu de temps sur l‚Äôex√©cution. ‚åõÔ∏è\n\nEnsuite, je cherche dans le fichier une ou plusieurs variables t. Si aucune n‚Äôest pr√©sente, on peut passer au fichier suivant, sinon on continue le raffinage.\n\nOn passe dans un filtre qui va nous permettre d‚Äôenlever les usages de la fonction t avec un seul argument qui ne posent pas de probl√®me.\n\nconst requiredPropertiesKeys = [&#39;data&#39;, &#39;number&#39;, &#39;general&#39;, &#39;renderers&#39;] as const;\n\n// Filter function to ensure that we enter the mutation function only if needed\nconst filterOutSimpleUsages = (p: ASTPath&amp;lt;CallExpression&amp;gt;) =&amp;gt; {\n  const args = p.value.arguments;\n\n  // If we only have the translation key, we don&#39;t need to refactor this usage\n  if (args.length === 1) {\n    return false;\n  }\n\n  // More than 2 arguments is an absolute sign of an old usage\n  // If second argument is not an object, we need to manually fix this case\n  if (args.length &amp;gt; 2 || args[1].type !== &#39;ObjectExpression&#39;) {\n    return true;\n  }\n\n  // If none of the above properties is found in second argument, we can say that this is an old usage\n  return requiredPropertiesKeys.every(\n    (requiredPropertyKey) =&amp;gt;\n      !(args[1] as ObjectExpression).properties.find(\n        // I needed to do some TS trickery to avoid getting warnings everywhere, sorry for that\n        (property) =&amp;gt; ((property as ObjectProperty).key as Identifier).name === requiredPropertyKey,\n      ),\n  );\n};\n\n\nFinalement, on peut passer dans la fonction de mutation, qui va nous permettre de modifier directement le code des fichiers.\n\n// Mutation function, we apply our modification to the AST\nconst mutatePath = (j: JSCodeshift) =&amp;gt; (p: ASTPath&amp;lt;CallExpression&amp;gt;) =&amp;gt; {\n  const objectProperties = requiredPropertiesKeys.reduce((acc, propertyKey, index) =&amp;gt; {\n    const argument = p.value.arguments[index + 1];\n    // If no argument or argument is a spread type, we don&#39;t take it in consideration\n    if (!argument || argument.type === &#39;SpreadElement&#39;) {\n      return acc;\n    }\n\n    // If argument is undefined, we skip it\n    if ((argument as Identifier).name &amp;amp;&amp;amp; (argument as Identifier).name === &#39;undefined&#39;) {\n      return acc;\n    }\n\n    // We create a new object property with an identifier (the object key) and put our argument inside\n    return [...acc, j.objectProperty(j.identifier(propertyKey), argument)];\n  }, [] as ObjectProperty[]);\n\n  // Finally, we keep our translation key in first position and our newly created object in second argument\n  p.value.arguments = [p.value.arguments[0], j.objectExpression(objectProperties)];\n\n  return p;\n};\n\n\nOn r√©cup√®re les arguments d√©j√† existants, on cr√©e un nouvel objet et on y place nos arguments !\n\nR√©sultats ‚ú®\n\n‚è± Pour √† peu pr√®s 2900 fichiers, le script a mis moins de 5,9 secondes √† s‚Äôex√©cuter (Macbook Pro 13‚Äù 2019).\n\nJSCodeshift nous a permis de cibler tr√®s rapidement 99 % des cas probl√©matiques et de les corriger automatiquement.\n\nLe pourcentage restant concerne des cas o√π il √©tait g√©n√©ralement difficile de cibler la fonction t (pass√©e en props √† un autre composant sous un autre nom). Ces quelques cas ont pu √™tre corrig√©s rapidement √† la main et d√©tect√©s gr√¢ce √† nos nombreux tests (heureusement qu‚Äôon a une r√®gle de bonne pratique pour √ßa üòá).\n\ntl;dr &amp;amp; conclusion üèÉ\n\nVous pouvez retrouver la source du codemod ici m√™me.\n\nSi vous √™tes mainteneur d‚Äôune librairie, il peut √™tre tr√®s int√©ressant de livrer des codemods en m√™me temps que les breaking-changes pour faciliter l‚Äôadoption des mises √† jour par exemple !\n\nAvec une prise en main relativement facile pour un r√©sultat tr√®s rapide, nous avons √©t√© tr√®s satisfaits de JSCodeshift et nous n‚Äôh√©siterons pas √† r√©utiliser cette librairie dans le futur. üëä\n\nMerci √† tous pour la lecture de mon premier article et JSCodeshiftez bien. üòò\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #17",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/07/22/bedrock-dev-facts-17.html",
    "date"     : "July 22, 2022",
    "excerpt"  : "L‚Äô√©t√© arrive, les vacances, le repos.\nEt vu les derniers devfacts des √©quipes Bedrock, il semblerait qu‚Äôil soit temps.\n\nLes autres articles de cette s√©rie sont disponibles ici.\n\nC‚Äôest presque fini\n\n\n  C‚Äôest fait √† 80%.\nJ‚Äôai fait tout le code, mais...",
  "content"  : "L‚Äô√©t√© arrive, les vacances, le repos.\nEt vu les derniers devfacts des √©quipes Bedrock, il semblerait qu‚Äôil soit temps.\n\nLes autres articles de cette s√©rie sont disponibles ici.\n\nC‚Äôest presque fini\n\n\n  C‚Äôest fait √† 80%.\nJ‚Äôai fait tout le code, mais il ne marche pas.\n\n\nLa potion magique\n\n\n  Je n‚Äôai plus de cerveau, je vais aller au bar\n\n\nC‚Äôest dommage, ils allaient le faire !\n\n\n  Notre strat√©gie, c‚Äôest d‚Äô√™tre √† la bourre pour que d‚Äôautres fassent le travail\n\n\nL‚Äôenvers du d√©cor du t√©l√©travail\n\n\n  Quand il y a du vent chez moi la connexion est instable\n\n\nAh !\n\n\n  J‚Äôai tir√© toute mon inspiration de manager des Shadocks\n\n\nLes cons√©quences du management √† la Shadock\n\n\n  Je ne vais tout de m√™me pas faire ma sieste pendant des heures de repos\n\n\nAuto-critique du manager Shadock. On est sauv√© !\n\n\n  Tout le monde s‚Äôen fout de ce que je fais\n\n\nQuand tout le monde le fait, mais que personne ne comprend pourquoi\n\nEn parlant du TimeSheet, de mani√®re na√Øve\n\n  ‚ÄúMais il y a vraiment quelqu‚Äôun dans la boite qui fait √ßa correctement ?‚Äù\n\n\nEst-ce que je peux casser √† moiti√© ?\n\n\n  Est-ce qu‚Äôon peut faire un semi BC-break ‚ÅâÔ∏è\n\n\nC‚Äôest pas faux\n\n\n  C‚Äôest connu et assum√©, √ßa fonctionnera pas jusqu‚Äôau moment o√π √ßa fonctionnera\n\n\nQuand tu as d√©laiss√© la review toute la semaine\n\n\n  Allez bon vendREVIEW √† tous\n\n\nü§î\n\n\n  Les tests passent mais √ßa plante\n\n\nLe d√©tecteur √† petit-dej\n\n\n  \n    ‚Äúje cherche X ‚Ä¶‚Äù\n    ‚Äúva voir √† la cafete, il y a quelqu‚Äôun qui a ramen√© de la bouffe‚Äù\n    ‚Äúc‚Äôest un honeypot √† X √ßa !‚Äù\n  \n\n\nC‚Äôest beau le mob programming\n\n\n\nMeurs un autre jour\n\nQuand tu veux vraiment √™tre s√ªr que ton code est ex√©cut√© mais que tu oublies de nettoyer l‚Äôhistorique\n\n\nC‚Äôest pas compliqu√© l‚Äôinformatique\n\n\n  \n    Comment tu as fait √ßa ?\n    J‚Äôai appuy√© sur des touches de mon clavier\n  \n\n\n\n\nGoogle translate n‚Äôest pas toujours ton ami\n\n\n  \n    ‚ÄúTu peux traduire l‚ÄôUS en fran√ßais quand tu la lis ?‚Äù\n    ‚ÄúPas de souci, Alors‚Ä¶ il faut ajouter un message de grille pain d‚Äôerreur lors du ‚Ä¶\n  \n\n\nSi √ßa passait, c‚Äô√©tait beau\n\nUne personne qui parle √† son √©cran sur lequel il y a du code :\n\n  Dis moi que √ßa va marcher, s‚Äôil te plait üôè\n\n\nSur un malentendu, il a √©t√© engag√©\n\n\n  L‚Äôessentiel de mes connaissances, c‚Äôest du bluff\n\n\nEt on fini avec un instant po√©sie\n\n\n  Mes p‚Äôtits chats, demain c‚Äôest d√©mo infra,\nPour l‚Äôinstant il n‚Äôy a pas d‚Äôinscrits,\nN‚Äôh√©sitez pas √† venir pr√©senter votre travail accompli !\nD‚Äôici √† ce prochain rendez-vous,\nDes bisous üòò\n\n"
} ,
  
  {
    "title"    : "Encrypt AWS AMIs: one way to do it wrong",
    "category" : "",
    "tags"     : " cloud, aws",
    "url"      : "/2022/07/08/encrypt-aws-amis.html",
    "date"     : "July 8, 2022",
    "excerpt"  : "At Bedrock, we build our own privately shared AMIs (Amazon Machine Images) for different parts of our stack: kubernetes platform, vod platform, etc. We build those AMIs to optimize kernel parameters,to embed some tools, and more. We have been usin...",
  "content"  : "At Bedrock, we build our own privately shared AMIs (Amazon Machine Images) for different parts of our stack: kubernetes platform, vod platform, etc. We build those AMIs to optimize kernel parameters,to embed some tools, and more. We have been using Packer for a couple of years, and everything has been working just fine.\n\nConcerned about following AWS best-practices, we recently added encryption by default to all new EBS volumes in all our accounts.\n\nWe didn‚Äôt expect it, but this decision impacted our AMI creation process. We thus began to update our Packer workflow to integrate this new constraint. We were telling ourselves that more security was for the best and we didn‚Äôt take enough steps back to analyze drawbacks.\n\nYou will find in this blog post multiple tips that may help you handle your AMIs encryption, but also why you shouldn‚Äôt handle it our way.\n\nBuild an encrypted AMI\n\nTo build our AMI, Packer launches an EC2 in a ‚Äúbuilder‚Äù account, then a snapshot is created and copied in needed regions. To use this AMI, ‚Äúuser‚Äù accounts are listed in the AMI allowed users.\n\nWith account EBS encryption enabled, snapshots are now encrypted. The default behavior is to use the account‚Äôs default KMS Key. Our first ‚Äúeasy‚Äù problem while trying to build new AMI with Packer was the following error message:\n\nError Copying AMI (ami-xxxxxx) to region (xx-xxx-x): InvalidRequest: Snapshot snap-xxxxxxx is encrypted. Creating an unencrypted copy from an encrypted snapshot is not supported.\n\n\nTo avoid that, we enabled AMI encryption with Packer, but it resulted in another error :\n\nError modify AMI attributes: InvalidParameter: Snapshots encrypted with the AWS Managed CMK can&#39;t be shared.\n\n\nAs our AMI has to be shared to other accounts, it was impossible to encrypt our AMI with the account default KMS Key. So we created a dedicated KMS Key for Packer encryption.\n\nAnd it worked! We had our beautiful encrypted AMI, ready to be used in all our accounts.\n\n\n\nHow we build our encrypted AMIs\n\nRun an encrypted AMI\n\nThis is where it gets complex.\n\nWhen we tried to launch an EC2 instance with our newly encrypted AMI, it failed with this error code :\n\nClient.InternalError: Client error on launch\n\n\nIt means that AWS can‚Äôt use this AMI because it is encrypted.\n\nFirst step was to authorize the KMS Key to be used for encryption in user (external) accounts.\n\nThere are two methods to do that, for two different needs.\n\n\n\nPolicy method\n\nTo authorize an external customer managed role (ours), we had to authorize our role in KMS Key dedicated policy to use it, then authorize KMS Key in our role policy to be used. It is some kind of symmetric reference hard to correctly maintain with IaC (Terraform). And we had to do the same for KMS Key replicas in other regions, because they have a dedicated policy.\n\n\n\nPolicy method\n\nOne important thing to know here: some KMS Key permissions aren‚Äôt available for external account sharing. It means that when we try to add the permission kms:* to our role policy (for debug purposes only, we follow least privileges principles), it failed. You can find which permission is accessible in cross account use and which is not here.\n\nGrant method\n\nTo authorize an AWS managed role, like AWSServiceRoleForAutoScaling (to launch our EC2), we also needed to allow it to use our key. It is impossible to add a new policy on an AWS Managed role. So instead of using a policy method like before, we had to create a grant on that role to use our key. We tried to create that grant from the source account (where the key is created), but it didn‚Äôt work. We had to create that grant from the destination account (where AWSServiceRoleForAutoScaling is), using a role in the destination account that is allowed to create a grant‚Ä¶ So we had to allow a role from the destination account to create a grant with Policy method, then use the previous role to allow an AWS Managed Role to use our KMS Key with Grant method. Pretty fun, right?\n\n\n\nGrant method\n\n\n\nOnce all needed roles were allowed, we tried to launch an EC2 with the allowed role attached as an instance role. It failed again, because we needed to also use the AMI KMS Key on root volume of our instance. By default, it was the account KMS Key that was used.\n\nWe attached that key on our root volume, and it worked. We also could launch our EC2 with ASG. It was all good.\n\nBut there was a big security vulnerability: instead of using one KMS key per account to encrypt our EBS volume, we were now using the same KMS key on all our accounts because of our encrypted AMI.\n\nKMS Key rotation\n\nA short word about Key rotation: it can easily be enabled to automatically rotate key materials each year. All new AMIs will be encrypted with new key material and nothing has to be changed to run encrypted AMIs.\nBut in case of a manual rotation: if a key is leaked for example, you will need to recreate a new KMS Key, its replicas, and all permissions and grants seen before.\n\nConclusion\n\nUsing privately shared encrypted AMI caused us multiple problems:\n\n  higher complexity to maintain.\n  lower security in cross-account configuration.\n\n\nFurthermore, we checked all our AMIs to see if they contain sensitive data. It isn‚Äôt the case : all sensitive data is uploaded at startup by Launch Template. We had no interest in continuing to use encrypted AMI, and we would have spared so much time if we had seen that sooner.\n\nThis is why we decided to disable encryption for all new EBS volume on our builder account and stop building encrypted AMI.\n\nDoing all the previous configuration took us several weeks. We are now more aware that doing security just for the beauty of it can be really counterproductive.\n\nIf your AMIs contain sensitive data, a better way to handle encrypted AMI may be to stop creating privately shared AMIs. Instead, copy and encrypt a private AMI in each of your ‚Äúuser‚Äù accounts with a dedicated KMS Key per account. As a result, there will be a larger amount of AMI to handle (one AMI per account per region), KMS Key permissions will still be complex, but security should improved.\n\nLogo used in thumbnail\nDeath by Imogen Oh from NounProject.com\nKey by Baboon designs from NounProject.com\nGears by Aybige from NounProject.com\n"
} ,
  
  {
    "title"    : "Debugging and reviewing your Android dependencies with apktool",
    "category" : "",
    "tags"     : " android, apktool, instrumentation, debugging, productivity",
    "url"      : "/2022/06/20/android-apktool-decompiling.html",
    "date"     : "June 20, 2022",
    "excerpt"  : "If you maintain an Android application, you might be relying on performance monitoring SDKs like Firebase Performance or New Relic, to name a couple. These plugins usually have a light setup process‚Äîjust apply a Gradle plugin, and they provide the...",
  "content"  : "If you maintain an Android application, you might be relying on performance monitoring SDKs like Firebase Performance or New Relic, to name a couple. These plugins usually have a light setup process‚Äîjust apply a Gradle plugin, and they provide the ability to collect statistics about every network call and database query in your app automatically.\n\nThe usual way to achieve this is to rely on a process called instrumentation, which is supported via the Android Gradle Plugin‚Äôs Transform API, or its successor, the Instrumentation API. This feature is very powerful, and potentially dangerous; in our case, a minor patch of one of these SDKs caused a production bug that left one of our core features crippled.\n\nThe visible cause of our bug, from a developer‚Äôs point of view, was that the video player saw the network requests as always being extremely fast, no matter the network quality. Therefore, it assumed the device had access to a very high bandwidth, and tried loading video segments with a very high bit rate. This did not go well for users with slower network speeds.\n\nTo understand what was going on, what went wrong, how to fix it and how to take measures so that it never happens again, we had to do some investigation.\n\nDiving into the Android build process\n\nBefore we get to the topic of instrumentation, we first need to know a little about the Android app build process. Don‚Äôt worry, we won‚Äôt need to dive too deep into the details.\n\nTo put it simply, during the build process, your source files (Kotlin and Java) are compiled to Dalvik bytecode, which is stored in .dex files. These files are then packaged into an APK file, which is basically just a ZIP file with all your code and resources.\n\n\nflowchart LR\n    kt[.kt files] -- kotlinc --&amp;gt; dex[.dex files] --&amp;gt; packaging[[packaging]]\n    java[.java files] -- javac --&amp;gt; dex\n    res[resource files] -- aapt --&amp;gt; resc[compiled resource files] --&amp;gt; packaging --&amp;gt; APK\n    subgraph APK\n    direction TB\n    dex1[.dex] -.- dex2[.dex] -.- dex3[.dex] -.- dex4[.dex]\n    res1[res] -.- res2[res] -.- res3[res] -.- res4[res]\n    signature -.- manifest\nend\n\n\nUnderstanding bytecode instrumentation\n\nNow, let‚Äôs say you want to take an existing application with its untouched source code, and automatically inject calls to your SDK every time a network call is made, to log whether it was successful or not. How would you achieve this?\n\nThe easiest way is to plug yourself into the build, right after the code is compiled into bytecode, and modify the bytecode to your will.\n\n\nflowchart LR\n    kt[.kt files] -- kotlinc --&amp;gt; dex[.dex files] --&amp;gt; transform[[transform]] --&amp;gt; packaging[[packaging]]\n    java[.java files] -- javac --&amp;gt; dex\n    res[resource files] -- aapt --&amp;gt; resc[compiled resource files] --&amp;gt; packaging --&amp;gt; APK\n    classDef transformed fill:#ff0000\n    class transform transformed\n\n    subgraph APK\n    direction TB\n    dex1[.dex] -.- dex2[.dex] -.- dex3[.dex] -.- dex4[.dex]\n    res1[res] -.- res2[res] -.- res3[res] -.- res4[res]\n    signature -.- manifest\n    class dex1,dex2,dex3,dex4 transformed\nend\n\n\nThe Android Gradle Plugin (AGP) offers APIs to do this, so SDK vendors can just develop a Gradle plugin and ta-da! Once you apply it, your app is automatically instrumented.\n\nNote that there are other ways to achieve this without the AGP. Notably, Kotlin now uses an Intermediate Representation (IR), before it gets compiled down to a target-specific format. You can write a Kotlin IR compiler plugin to transform the IR code and add your own hooks in an Android-agnostic way, although this API is still experimental at the time of writing.\n\nReverse-engineering a built APK\n\nNow, this is great. But when you open an APK file, what do you get?\n\nLet‚Äôs unzip one and look inside.\n\n.\n‚îú‚îÄ‚îÄ META-INF\n‚îú‚îÄ‚îÄ assets\n‚îú‚îÄ‚îÄ google\n‚îú‚îÄ‚îÄ okhttp3\n‚îú‚îÄ‚îÄ res\n‚îú‚îÄ‚îÄ AndroidManifest.xml\n‚îú‚îÄ‚îÄ classes.dex\n‚îú‚îÄ‚îÄ classes2.dex\n‚îú‚îÄ‚îÄ classes3.dex\n‚îú‚îÄ‚îÄ classes4.dex\n‚îú‚îÄ‚îÄ firebase-common.properties\n‚îú‚îÄ‚îÄ firebase-crashlytics.properties\n‚îú‚îÄ‚îÄ play-services-base.properties\n‚îú‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ resources.arsc\n\n\nA bunch of noise, and four interesting .dex files. That‚Äôs where the app‚Äôs code is stored, but unfortunately, these files are not human-readable.\n\nTo turn them into low-level but understandable code, some tooling will be necessary. The easiest to use for this task is apktool, which is free and open-source.\n\nLet‚Äôs run apktool on our APK, and see what happens:\n\n\n\n\n\n~/Downloads\n‚ùØ apktool d bedrock-sample-release.apk\nI: Using Apktool 2.6.1 on bedrock-sample-release.apk\nI: Loading resource table...\nI: Decoding AndroidManifest.xml with resources...\nI: Loading resource table from file: /Users/bcandellier/Library/apktool/framework/1.apk\nI: Regular manifest package...\nI: Decoding file-resources...\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nI: Decoding values */* XMLs...\nI: Baksmaling classes.dex...\nI: Baksmaling classes2.dex...\nI: Baksmaling classes3.dex...\nI: Baksmaling classes4.dex...\nI: Copying assets and libs...\nI: Copying unknown files...\nI: Copying original files...\nI: Copying META-INF/services directory\n\n\n\nThere we go! In our case, we can ignore the warnings. apktool created a new directory with a bunch of .smali files, organized by package: one file per class, containing their Dalvik bytecode.\n\n.\n‚îú‚îÄ‚îÄ AndroidManifest.xml\n‚îú‚îÄ‚îÄ res\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ values\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ strings.xml\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îú‚îÄ‚îÄ layout\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout_home.xml\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ smali\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ com\n‚îÇ¬†¬†  ¬†¬† ‚îú‚îÄ‚îÄ bedrockstreaming\n‚îÇ¬†¬†  ¬†¬† ‚îÇ   ‚îú‚îÄ‚îÄ app\n‚îÇ¬†¬†  ¬†¬† ‚îÇ   ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ mobile\n‚îÇ¬†¬†  ¬†¬† ‚îÇ   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ R$anim.smali\n‚îÇ¬†¬†  ¬†¬† ‚îÇ   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ R$layout.smali\n‚îÇ¬†¬†  ¬†¬† ‚îÇ   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ R$string.smali\n‚îÇ¬†¬†  ¬†¬† ‚îÇ   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ R$style.smali\n‚îÇ       ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ¬†¬†  ¬†¬† ‚îî‚îÄ‚îÄ google\n‚îÇ¬†¬†  ¬†¬†     ‚îú‚îÄ‚îÄ android\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ exoplayer2\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AbstractConcatenatedTimeline.smali\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AudioBecomingNoisyManager.smali\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AudioFocusManager$AudioFocusListener$$ExternalSyntheticLambda0.smali\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AudioFocusManager$AudioFocusListener.smali\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AudioFocusManager.smali\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ BasePlayer.smali\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ BaseRenderer.smali\n‚îÇ¬†¬†  ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ BuildConfig.smali\n‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ           ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ smali_classes2\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ com\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ bedrockstreaming\n‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ app\n‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ mobile\n‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ MobileApplication.smali\n‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ ...\n\n\nIf you see files with mangled names and contents, make sure that you run apktool on an APK with R8 obfuscation disabled, or you‚Äôll have a hard time figuring things out.\n\nUnderstanding Dalvik bytecode\n\nNow, if you open one of these files, it will contain code that looks like the snippet below. It will look unfamiliar; that‚Äôs normal.\n\n.method private final getContent()Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    .locals 2\n    .line 119\n\n    iget-object v0, p0, Lcom/bedrockstreaming/example/HomeViewModel;-&amp;gt;state:Landroidx/lifecycle/LiveData;\n\n    invoke-virtual {v0}, Landroidx/lifecycle/LiveData;-&amp;gt;getValue()Ljava/lang/Object;\n\n    move-result-object v0\n\n    instance-of v1, v0, Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    if-eqz v1, :cond_0\n\n    check-cast v0, Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    goto :goto_0\n\n    :cond_0\n\n    const/4 v0, 0x0\n\n    :goto_0\n\n    return-object v0\n  \n.end method\n\n\nIf you‚Äôve ever worked with assembly code before, you might notice similarities in the way the code is written. Each line begins with an instruction, which can take comma-separated parameters. To work out what these instructions and their parameters mean, you will need to refer to the Dalvik bytecode documentation provided by Google.\n\nLet‚Äôs take an example line from the snippet and decode it together. Looking at the table in the documentation, we can see deduce this:\n\n# We&#39;ll decode this line:\ninvoke-virtual {v0}, Landroidx/lifecycle/LiveData;-&amp;gt;getValue()Ljava/lang/Object;\n\ninvoke-virtual                                                                   # We&#39;re calling a virtual method\n               {v0},                                                             # We&#39;re calling the method on the object referenced in register v0\n                     Landroidx/lifecycle/LiveData;                               # The method we&#39;re calling is defined by androidx.lifecycle.LiveData\n                                                  -&amp;gt;getValue()                   # We&#39;re calling a method called getValue()\n                                                              Ljava/lang/Object; # This method returns an Object\n\n\nWith some determination, we can figure out what the snippet does. Here, we‚Äôre defining a getContent() method that tries to cast a LiveData‚Äôs value to State.Content and returns it, or null otherwise.\n\nUsing a decompiled APK as a debugging tool\n\nInspecting suspicious code\n\nBefore doing anything else, we can already start looking at the generated code to identify patterns that could cause issues. Problem is‚Ä¶ there can be a lot of code to look through.\n\nBefore going this deep in the rabbit hole, we already figured our issue was, somehow, related to instrumentation: disabling it fixed this issue; downgrading to the previous release of the SDK also fixed it. This means that if we want to get a clear look at what needs to change to go from a working APK from a broken one, we could just compare an APK instrumented by the previous SDK version with an APK instrumented by the current one!\n\nOf course, we want to do this on the human-readable smali files, not the raw dex files. We can generate a full diff with the help of the diff tool:\n\ndiff -bur normal/ instrumented/\n\n\nIn our case, it also proved useful to compare an APK that has been instrumented with one that hasn‚Äôt, to understand what that instrumentation is meant to achieve. Most of it was to notify the SDK of every HTTP request, along with its result.\n\nAs a simple example, the snippet below shows a class belonging to Picasso. We can see the HTTP calls it makes are being intercepted by the SDK.\n\n--- normal/smali/com/squareup/picasso/NetworkRequestHandler.smali\t2022-01-05 11:09:22.000000000 +0100\n+++ instrumented/smali/com/squareup/picasso/NetworkRequestHandler.smali\t2022-01-05 11:08:34.000000000 +0100\n@@ -128,10 +128,26 @@\n\n     .line 103\n     :cond_4\n+    instance-of v2, v1, Lokhttp3/Request$Builder;\n+\n+    if-nez v2, :cond_5\n+\n     invoke-virtual {v1}, Lokhttp3/Request$Builder;-&amp;gt;build()Lokhttp3/Request;\n\n     move-result-object v2\n\n+    goto :goto_1\n+\n+    :cond_5\n+    move-object v2, v1\n+\n+    check-cast v2, Lokhttp3/Request$Builder;\n+\n+    invoke-static {v2}, Lcom/vendor/instrumentation/okhttp3/OkHttp3Instrumentation;-&amp;gt;build(Lokhttp3/Request$Builder;)Lokhttp3/Request;\n+\n+    move-result-object v2\n+\n+    :goto_1\n     return-object v2\n .end method\n\n\nFinding the source of the issue by iteration\n\nWe haven‚Äôt talked about apktool‚Äôs greatest strength yet: its ability to recompile an APK from the smali sources it has decompiled! This means we can effectively decompile an APK, make modifications to its low-level code, recompile and run it.\n\nThis proved really useful during our investigation. Since we have one directory with our APK in a bad state, and one directory with our APK in a good state, we can process by elimination to point out exactly which single class, when modified, causes our bug.\n\nIn our case, a useful workflow was to start with a suspect‚Äîlet‚Äôs say we think instrumenting the OkHttp classes might have caused the bug.\n\n\n  Copy the OkHttp classes from the ‚Äúbad‚Äù APK, and only those, to our ‚Äúgood‚Äù APK.\n  Recompile and run the app.\n  Does the bug occur?\n    \n      If it does, then that means it is caused by the instrumentation of at least one of the OkHttp classes. We can go through this process again, this time by selecting only a subset of OkHttp‚Äôs classes, and check if the bug still occurs, etc.\n      If it doesn‚Äôt, revert the OkHttp classes and try again with another suspect.\n    \n  \n\n\nThis process can be accelerated with a very simple script, to iterate faster. The recompilation step occurs incrementally, and so only takes a few seconds.\n\n#!/bin/sh\n\n# rebuild-and-run.sh\n# Rebuild, sign and install an APK from its decompiled source.\n# (c) 2022 Bedrock Streaming\n\n# Inputs:\n# DECOMPILED_APK_PATH: path to your previously decompiled APK directory\n# KEYSTORE_PATH: path to your debug keystore\n# KEYSTORE_PASSWORD: your debug keystore password\n\napktool --use-aapt2 b &quot;$DECOMPILED_APK_PATH&quot; \\\n    &amp;amp;&amp;amp; apksigner sign -ks &quot;$KEYSTORE_PATH&quot; --ks-pass &quot;pass:$KEYSTORE_PASSWORD&quot; &quot;$DECOMPILED_APK_PATH/dist/*.apk&quot; \\\n    &amp;amp;&amp;amp; adb install &quot;$DECOMPILED_APK_PATH/dist/*.apk&quot;\n\n\nHere‚Äôs what it looks like in action:\n\n\n\n\n\n~/bytecode-playground\n‚ùØ ./rebuild-and-run.sh\nI: Using Apktool 2.6.1\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether resources has changed...\nI: Building apk file...\nI: Copying unknown files/dir...\nI: Built apk...\nPerforming Incremental Install\nServing...\nSuccess\nInstall command complete in 445 ms\n\n\n\nIn our case, we narrowed down the issue to the instrumentation of a single class: okhttp3.internal.http.CallServerInterceptor: once it was reverted, the bug disappeared.\n\nIn fact, we narrowed it down to a very small patch with which the app runs fine:\n\n .../okhttp3/internal/http/CallServerInterceptor.smali         | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali b/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\nindex c916149f..c26eab15 100644\n--- a/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\n+++ b/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\n@@ -510,7 +510,7 @@\n \n     instance-of v8, v14, Lokhttp3/Response$Builder;\n \n-    if-nez v8, :cond_b\n+    #if-nez v8, :cond_b\n \n     invoke-virtual {v14, v15}, Lokhttp3/Response$Builder;-&amp;gt;body(Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n \n@@ -574,7 +574,7 @@\n \n     instance-of v15, v8, Lokhttp3/Response$Builder;\n \n-    if-nez v15, :cond_e\n+    #if-nez v15, :cond_e\n \n     invoke-virtual {v8, v14}, Lokhttp3/Response$Builder;-&amp;gt;body(Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n \n-- \n\n\nBasically, when the code went through this if statement, our request got wrapped by com.vendor.instrumentation.okhttp3.OkHttp3Instrumentation:\n\ninvoke-static {v8, v14}, Lcom/vendor/instrumentation/okhttp3/OkHttp3Instrumentation;-&amp;gt;body(Lokhttp3/Response$Builder;Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n\n\nAnd what does this method do, you ask? Let‚Äôs take a look at the decompiled source in Android Studio, so that it‚Äôs a bit easier to read:\n\npublic Builder body(ResponseBody body) {\n    try {\n        if (body != null) {\n            BufferedSource source = body.source();\n            Buffer buffer = new Buffer();\n            source.readAll(buffer);\n            return this.impl.body(ResponseBody.create(body.contentType(), buffer.size(), buffer));\n        }\n    } catch (IOException var4) {\n        log.error(&quot;IOException reading from source: &quot;, var4);\n    } catch (IllegalStateException var5) {\n        log.error(&quot;IllegalStateException reading from source: &quot;, var5);\n    }\n\n    return this.impl.body(body);\n}\n\n\nThe body is being read into memory!\n\nsource.readAll(buffer);\n\n\nWhen correlating this discovery with the source code from ExoPlayer, we could verify that, indeed, our player was expecting that the time it takes reading the response body would be the time it took to download the entire video segment. Here‚Äôs what this flow looks like in a functional app:\n\n\nsequenceDiagram\n    participant exo as OkHttpDataSource\n    participant nr as OkHttp3Instrumentation\n    participant okhttp as OkHttpClient\n    participant server as Server Endpoint\n\n    exo-&amp;gt;&amp;gt;nr: body()\n    nr-&amp;gt;&amp;gt;okhttp: body()\n    activate server\n    okhttp-&amp;gt;&amp;gt;server: \n    server-&amp;gt;&amp;gt;okhttp: \n    okhttp-&amp;gt;&amp;gt;nr: ResponseBody (length=0)\n    activate exo\n    nr-&amp;gt;&amp;gt;exo: ResponseBody (length=0)\n    server-&amp;gt;&amp;gt;exo: length=512\n    server-&amp;gt;&amp;gt;exo: length=1024\n    server-&amp;gt;&amp;gt;exo: length=1536\n    server-&amp;gt;&amp;gt;exo: length=2048\n    server-&amp;gt;&amp;gt;exo: length=2560\n    note right of exo: OkHttpDataSource controls the body reads  and can measure the time it took  to read the whole response\n    deactivate server\n    deactivate exo\n\n\nBut with this bug in the SDK, since the HTTP response has been buffered into memory by some SDK, the read was always almost-instantaneous, no matter the speed of the connection. Additionally, it messed with the overall performance since requests were no longer properly streamed by their rightful users.\n\n\nsequenceDiagram\n    participant exo as OkHttpDataSource\n    participant nr as OkHttp3Instrumentation\n    participant okhttp as OkHttpClient\n    participant server as Server Endpoint\n\n    exo-&amp;gt;&amp;gt;nr: body()\n    nr-&amp;gt;&amp;gt;okhttp: body()\n    activate server\n    okhttp-&amp;gt;&amp;gt;server: \n    server-&amp;gt;&amp;gt;okhttp: \n    activate nr\n    okhttp-&amp;gt;&amp;gt;nr: \n    server-&amp;gt;&amp;gt;nr: length=512\n    server-&amp;gt;&amp;gt;nr: length=1024\n    server-&amp;gt;&amp;gt;nr: length=1536\n    server-&amp;gt;&amp;gt;nr: length=2048\n    server-&amp;gt;&amp;gt;nr: length=2560\n    deactivate nr\n    activate exo\n    note right of exo: OkHttpDataSource is only notified  after everything is downloaded\n    nr-&amp;gt;&amp;gt;exo: ResponseBody (length=2560)\n    deactivate server\n    deactivate exo\n\n\nUsing a decompiled APK as a review tool\n\nIt‚Äôs no secret to developers in any software ecosystem that library updates can be a source of problems - security vulnerabilities, bugs, incompatibilities, and so on. It‚Äôs hard to vet them properly, especially in compiled form, like libraries distributed in the Java ecosystem. Things get even harder when arbitrary Gradle plugins start rewriting our own code!\n\nThe tooling needed to decompile an APK is free, fast, and easy to automate. It‚Äôs a really helpful tool to investigate obscure bugs in places your debugger won‚Äôt let you place a breakpoint, and it‚Äôs also really useful to be able to see a human-readable diff between two binaries.\n\nGenerating a diff of the effects of a library upgrade can seem overkill and hard to do in practice, but at least in the case of bug-fix releases with hopefully few changes, it can be very helpful to have an actual report of what changed. It‚Äôs an accepted practice to review the code your team checks in; why not review the code of others, since it ends up in the exact same artifact?\n"
} ,
  
  {
    "title"    : "Bedrock √† la Kubecon 2022, 4√®me partie¬†: chaos, r√©silience, ressenti global et conclusion g√©n√©rale‚Ä¶",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/16/kubecon-2022-part-4.html",
    "date"     : "June 16, 2022",
    "excerpt"  : "Pour terminer cette s√©rie, un ou deux sujets divers que nous n‚Äôavons pas regroup√© dans les trois articles pr√©c√©dents\n(les performances applicatives et la scalabilit√©, \nles performances bas niveau, le syst√®me et le r√©seau,\nla dev XP, l‚Äôoutillage, l...",
  "content"  : "Pour terminer cette s√©rie, un ou deux sujets divers que nous n‚Äôavons pas regroup√© dans les trois articles pr√©c√©dents\n(les performances applicatives et la scalabilit√©, \nles performances bas niveau, le syst√®me et le r√©seau,\nla dev XP, l‚Äôoutillage, la CI/CD et l‚Äôobservabilit√©), \npuis une conclusion globale avec ce que nous avons retenu de cette KubeCon Europe 2022.\n\n\nLa conclusion, @ KubeCon 2022¬†!\n\nChaos Engineering / Chaos Testing pour une meilleur r√©silience aux pannes\n\nC‚Äôest un des sujets sur lesquels nous avons commenc√© √† travailler activement cette ann√©e¬†: casser des choses dans nos clusters, dans notre plateforme, entre nos microservices.\nL‚Äôid√©e sous-jacente est, bien s√ªr, que tout va casser un jour ou l‚Äôautre, donc autant provoquer du chaos nous-m√™me, en environnement contr√¥l√©. Nous identifierons ainsi des points sensibles de notre plateforme et pourrons les corriger, √©vitant ainsi des incidents, parfois majeurs, au mauvais moment.\n\nCe th√®me du Chaos Engineering est r√©guli√®rement abord√© en conf√©rences et nous √©tions contents de voir que nous ne sommes pas les seuls √† nous interroger sur ‚Äúcomment‚Äù en mettre en place.\nNous sommes repartis avec quelques pistes d‚Äôoutils, comme chaos mesh ou Litmus Chaos, que nous allons peut-√™tre prototyper pour les comparer √† chaos-controller que nous avons r√©cemment exp√©riment√©.\n\nAu cours de la conf√©rence, ‚ÄúCase Study: Bringing Chaos Engineering to the Cloud Native Developers‚Äù (vid√©o) par Uma Mukkara, Litmus et Ramiro Berelleza, Okteto, nous avons pu avoir un aper√ßu de l‚Äôoutil de chaos Litmus, sa force semblant r√©sider dans le partage des scripts de chaos au sein la communaut√©.\nPuis, il a √©t√© d√©crit une approche CI des tests de chaos visant √† int√©grer certains tests de chaos dans le flux de d√©veloppement plut√¥t qu‚Äô√† la fin.\n\nEnfin, toujours sur des questions de r√©silience en cas d‚Äôinterruption de service, dans sa conf√©rence ‚ÄúBuilding for the (inevitable) Next Cloud Outage‚Äù (vid√©o), Pavel Nikolov de Section nous a questionn√©s sur la mani√®re d‚Äô√™tre plus robuste √† une catastrophe.\nLa question n‚Äôest pas de savoir si une catastrophe se produira, mais quand elle se produira. C‚Äôest pourquoi il est aussi pr√©f√©rable de disposer d‚Äôun plan de reprise apr√®s sinistre mais surtout de pr√©voir en amont un syst√®me d‚Äôauto-gu√©rison permettant d‚Äô√™tre plus r√©silient aux catastrophes.\nIl nous a ensuite pr√©sent√© un use case sp√©cifique au r√©seau, nous invitant √† pr√©f√©rer au traditionnel ‚ÄúDNS √† la rescousse‚Äù, la mise en place de BGP (Border Gateway Protocol).\n\nQuelques sujets divers\n\n√Ä travers quelques talks, nous avons jet√© des coups d‚Äô≈ìil sur des sujets sur lesquels nous ne travaillons pas r√©ellement au quotidien ‚Äì appelez √ßa de la curiosit√© intellectuelle si vous le voulez ;-)\n\n\n  Nous avons vu un ensemble de design patterns pour le d√©veloppement de controllers Kubernetes (vid√©o), approche qui devient petit √† petit un moyen r√©pandu de r√©pondre √† des probl√©matiques, en codant directement dans Kubernetes.\n  La conf√©rence ‚ÄúA treasure map of hacking (and defending) K8s‚Äù (vid√©o) √©tait tr√®s sympathique, elle montrait √† quel point il peut √™tre ‚Äúfacile‚Äù de prendre le contr√¥le d‚Äôune infrastructure. Une fa√ßon de montrer que patcher est obligatoire¬†!\n  Et dans un registre hors-technique, ‚ÄúComposability is to software as compounding interest is to finance‚Äù (vid√©o) mettait en √©vidence √† quel point, en construisant des outils, puis des projets, puis un √©cosyst√®me, les uns profitent aux autres, on construit donc plus grand et plus gros. Il suffit de voir le landscape CNCF aujourd‚Äôhui par rapport √† 4 ans en arri√®re.\n\n\nConclusion, KubeCon Europe 2022\n\nNous avons commenc√© √† migrer vers Le Cloud, vers AWS et Kubernetes, il y a plus de quatre ans. Notre premi√®re KubeCon √©tait √† Copenhague, en 2018. Que dire, en conclusion de cette conf√©rence annuelle¬†? Comment conclure ces articles¬†?\n\nAujourd‚Äôhui, les grandes id√©es que nous avons retenues de cette KubeCon Europe 2022, en r√©sumant, sont les suivantes¬†:\n\n\n  Les probl√©matiques d‚Äôauto-scaling sont bien cern√©es, les outils sont plut√¥t matures. Comme beaucoup d‚Äôautres entreprises, nous arrivons sur l‚Äô√©tape suivante, qui est de dimensionner en tenant mieux compte des couts et pas uniquement des performances et/ou de la disponibilit√©.\n  La gestion des co√ªts dans Kubernetes n‚Äôest toujours pas simple. √Ä la fois pour les suivre et les r√©partir, mais aussi pour d√©cider du bon compromis entre performances¬†/ disponibilit√©¬†/ souplesse¬†/ autonomie des √©quipes¬†/ couts.\n  Service Mesh¬†: nous n‚Äôavons toujours pas franchi le pas et Istio, qui √©tait un sujet tr√®s √† la mode il y a quatre ans, nous semble d√©sormais presque oubli√©. Aujourd‚Äôhui, Cilium semble √™tre la nouvelle approche qui s‚Äôimpose, et il se pourrait que nous jouions avec ‚Äúpour voir‚Äù prochainement‚Ä¶\n  L‚Äôobservabilit√©, plus vraiment un probl√®me.\n  Le chaos engineering¬†/ chaos testing¬†: toujours une id√©e s√©duisante, mais pas encore r√©ellement industrialis√©e¬†?\n  L‚Äôoutillage autour de la CI/CD, le d√©ploiement progressif, le rollback (possiblement automatis√©) progresse, et √ßa fait plaisir¬†!\n  L‚Äô√©cosyst√®me progresse, m√ªrit, et on parle de sujets de plus haut niveau que quelques ann√©es en arri√®re. Par exemple, nous avons entendu plusieurs fois parler de base de donn√©es magiquement scalable h√©berg√©e dans Kubernetes, alors que l‚Äô√©poque o√π nous √©vitions de stocker quelque √©tat que ce soit dans un cluster ne nous semble pas si lointaine¬†!\n\n\nEt, pour finir, quelques points dont nous n‚Äôavons pas du tout ou tr√®s peu entendu parler¬†:\n\n\n  Nous avons peut-√™tre loup√© des choses en cr√©ant nos programmes, mais nous n‚Äôavons vu aucun talk autour de ‚Äúcomment nous d√©veloppons des applications cloud-native‚Äù. Pourtant, la probl√©matique de l‚Äôenvironnement de d√©veloppement, avec des services manag√©s, des d√©ploiements vers Kubernetes et des plateformes distribu√©es, ne nous parait pas encore r√©gl√©e¬†!\n  L‚Äôapproche ‚ÄúFaaS‚Äù (Function as a Service) nous para√Æt encore moins r√©pandue que quelques ann√©es en arri√®re¬†?\n  Nous n‚Äôavons pas entendu parler une seule fois de F√©d√©ration, alors que le terme revenait encore et encore il y a quatre ans. Nous avons bien fait de ne m√™me pas essayer, on dirait¬†;-)\n\n\n\nRejoignez-nos √©quipes et venez vivre les prochaines conf√©rences avec nous l‚Äôan prochain\n"
} ,
  
  {
    "title"    : "Bedrock √† la Kubecon 2022, 3√®me partie¬†: Dev XP, outillage, CI/CD, observabilit√©‚Ä¶",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/15/kubecon-2022-part-3.html",
    "date"     : "June 15, 2022",
    "excerpt"  : "Pour notre troisi√®me article de cette s√©rie sur ce que nous avons retenu de la KubeCon Europe 2022, apr√®s \nles performances applicatives et la scalabilit√© et \nles performances bas niveau, le syst√®me et le r√©seau, \npassons √† la Developper eXperienc...",
  "content"  : "Pour notre troisi√®me article de cette s√©rie sur ce que nous avons retenu de la KubeCon Europe 2022, apr√®s \nles performances applicatives et la scalabilit√© et \nles performances bas niveau, le syst√®me et le r√©seau, \npassons √† la Developper eXperience, √† l‚Äôoutillage, √† la CI/CD, aux rollback, √† l‚Äôobservabilit√© et aux incidents¬†!\n\n\nUne nouvelle journ√©e commence, @ KubeCon 2022¬†!\n\nLa prod est tomb√©e¬†!\n\nDans notre secteur d‚Äôactivit√©, nous avons tous subis des incidents de production et le retour d‚Äôexp√©rience, qu‚Äôil soit interne ou public, est important et formateur.\nEn effet, m√™me si les incidents de production sont malheureusement in√©luctables dans nos m√©tiers, il est important de les analyser afin de mieux les comprendre et demieux s‚Äôen pr√©munir.\n\nPreuve de l‚Äôimportance de ces sujets¬†: nous avons assist√© √† deux conf√©rences tr√®s int√©ressantes sur ce th√®me dans des salles pleines √† craquer¬†!\nToutes deux portaient sur des incidents de production majeurs suite √† une modification de code qui peut para√Ætre anodine¬†: la premi√®re √©tait donn√©e par Influxdata, la seconde par Skyscanner.\nLes conf√©rences √©taient particuli√®rement joviales et bienveillantes¬†: les r√©actions du public √† certains slides montraient bien que ce genre de situations sentait le v√©cu pour certains¬†!\n\nNous avons tous √† apprendre de ces cas concrets d‚Äôincident, aussi, nous vous conseillons de visionner les vid√©os de ces conf√©rences¬†: skyscanner et influxdata.\nMais si nous devions les r√©sumer¬†: l‚Äôautomatisation de bout en bout demande une grande maturit√©, beaucoup (beaucoup¬†!) de tests et des reviews de qualit√©¬†!\n\nEt comme il est dit dans une des slides¬†:\n\n\n\nDebugger, en production, avec des conteneurs √©ph√©m√®res\n\nNous utilisons r√©guli√®rement la commande kubectl exec pour entrer dans conteneur / pod et y lancer des commandes de d√©bogage ‚Äì parce que certains probl√®mes ne sont pas reproductibles ailleurs qu‚Äôen production, ou parce qu‚Äôil faut comprendre ce qu‚Äôil se passe avant de savoir reproduire en environnement de d√©veloppement.\n\nCela dit, cette approche n‚Äôest pas g√©niale¬†: si nous modifions des choses dans un conteneur, ces modifications persistent.\nAussi, il faut pouvoir installer des outils de d√©bug dans un conteneur (ce qu‚Äôon ne peut pas facilement faire chez Bedrock, o√π nos conteneurs ne s‚Äôex√©cutent pas en root et ont souvent un filesystem read-only), ou les embarquer dans les images (ce qui les grossit consid√©rablement, sans compter l‚Äôaugmentation du risque de failles de s√©curit√©).\n\nPour rem√©dier √† cette probl√©matique, la fonctionnalit√© de conteneurs √©ph√©m√®res (vid√©o) arrive en b√™ta dans Kubernetes 1.23 et √ßa semble absolument g√©nial¬†!\nL‚Äôoutil parfait pour lancer des conteneurs temporaires √† l‚Äôint√©rieur de pods existant et incroyablement puissant pour d√©bugger¬†!\nNous allons pouvoir r√©duire le nombre d‚Äôoutils de debug int√©gr√©s √† nos images et parvenir √† d√©bugger plus ais√©ment des probl√®mes qui ne surviennent qu‚Äôen production¬†!\n\nLes risques de l‚Äôobservabilit√© / Observabilit√© pirat√©e\n\n‚ÄúHow attackers use exposed Prometheus Server to Exploit Kubernetes Clusters‚Äù (vid√©o) par David de Torres et  Miguel Hernandez, ou ‚Äúcomment obtenir l‚Äôempreinte de vos clusters k8s √† travers vos donn√©es de monitoring‚Äù.\n\nSysdig est venu nous rem√©morer que le monitoring, c‚Äôest bien, mais que ne pas exposer ses donn√©es de monitoring, c‚Äôest mieux¬†!\nEn effet, attention aux informations qui sont expos√©es √† l‚Äôext√©rieur, elles pourraient √™tre recueillies par des attaquants externes pour acqu√©rir des connaissances sur votre plateforme (provider cloud, version de l‚ÄôOS utilis√©‚Ä¶) et s‚Äôen servir ensuite pour s‚Äôintroduire dans votre infrastructure (fuite de donn√©es, cryptominage ou ransomware).\n\n√Ä travers un cas d‚Äôutilisation fictif, ils nous ont d√©montr√© la facilit√© de r√©cup√©ration de ces informations et comment elles sont utilis√©es pour monter une attaque.\nEnfin, ils nous ont rappel√© que pour se pr√©munir de ces attaques, il suffit de suivre les recommandations de s√©curit√©¬†! CQFD.\nIl est toujours bon d‚Äôavoir ces piqures de rappel et de toujours bien penser aux donn√©es que l‚Äôon expose vers l‚Äôext√©rieur.\n\nCI/CD, d√©ploiement progressif\n\nChez Bedrock, nous sommes en pleine refonte de notre cha√Æne de CI/CD¬†: nous basculons tous nos projets du bon vieux Jenkins ‚Äútemporaire‚Äù, que nous avions mont√© au d√©but de notre migration vers Le Cloud, vers Github Actions.\nAu passage, nous nous demandons forc√©ment comment nous pourrions am√©liorer nos d√©ploiements et les rendre plus s√©curis√©s, tant pour la sant√© de notre plateforme que pour la paix d‚Äôesprit de nos √©quipes et de nos utilisateurs.\n\nLa conf√©rence ‚ÄúAutomated progressive delivery using gitops and service mesh‚Äù (vid√©o) parlait de d√©ploiement progressif avec Argo CD, pour am√©liorer l‚Äôexcellence op√©rationnelle, r√©duire le MTTR, accro√Ætre l‚Äôautomatisation et la fiabilit√© des processus de d√©ploiement. Bref, des id√©es qui nous parlent¬†!\n\nReste des fonctionnalit√©s, qui nous semblent primordiales avant de se lancer sur un autre outil, qui ne sont pas encore g√©r√©es, h√©las¬†: mirroring de traffic, routing bas√© sur des en-t√™tes (typiquement¬†: pour faire du d√©ploiement progressif √† la maille ‚Äúutilisateur‚Äù et pas √† la maille ‚Äúrequ√™te HTTP‚Äù), d√©tection d‚Äôanomalie et rollback automatis√©‚Ä¶\nUn projet √† suivre, donc, qui pourrait m√ªrir dans les prochains mois.\n\nAu niveau des aspects moins sympathiques¬†: cette approche de d√©ploiement progressif passe par un service mesh (envoy, ici).\nOr nous n‚Äôen avons pas en place et depuis quatre ans n‚Äôavons toujours pas trouv√© les bons arguments pour en introduire dans nos clusters, notamment √† cause de la complexit√© ajout√©e‚Ä¶\n\nUne autre conf√©rence (vid√©o) mentionnait l‚Äôoutil Flagger pour des d√©ploiements Canary.\n\nQuelques autres id√©es √† retenir\n\nNous avons aussi vu quelques autres conf√©rences dont nous avons tir√© quelques id√©es, en plus bref¬†:\n\n\n  Kubernetes 1.23 apporte (en alpha) une nouvelle commande kubectl events, qui retourne ses r√©sultats dans l‚Äôordre chronologique. Ce que l‚Äôactuel kubectl get events ne fait pas et √ßa peut √™tre bien emb√™tant. Vue comme de la culture g√©n√©rale, la conf√©rence ‚ÄúThe soul of a new command: adding ‚Äòevents‚Äô to kubectl‚Äù (vid√©o) racontait comment cette fonctionnalit√© a √©t√© impl√©ment√©e et √©tait fort int√©ressante.\n  Un speaker parlait de la mise en place de Crossplane dans son entreprise (vid√©o). Sujet potentiellement int√©ressant, mais qui ne correspond pas √† notre approche actuelle. Nous avons toutefois retenu quelques points autour de comment il fournit des outils √† ses coll√®gues d√©veloppeurs¬†: documentation, composition de services, management d‚Äôattentes, utilisation de l‚Äô√©cosyst√®me‚Ä¶ Des probl√©matiques auxquelles nous nous sommes confront√©s de nombreuses fois, pour encourager nos √©quipes √† adopter des √©volutions ou de nouveaux outils¬†!\n  Si vous commencez √† mettre en place votre stack de logs, la conf√©rence ‚ÄúShow me your labels and I‚Äôll tell you who you are‚Äù (vid√©o) est faite pour vous. L‚Äôid√©e d‚Äôutiliser les labels assign√©s aux pods pour aller jusqu‚Äô√† filtrer l‚Äôacc√®s aux logs via RBAC, terrible¬†! Aussi, la cr√©ation de flux de logs avec Logging Operator a l‚Äôair fort sympathique. Si ce talk √©tait venu trois ans plus t√¥t, c‚Äôest quelque chose que nous essayerons¬†!\n\n\nConclusion\n\nCes conf√©rences nous ont permis d‚Äôapprofondir les questions que nous nous posons actuellement alors que notre changeons de CI/CD (d√©ploiement progressif, rollback automatis√© ou non‚Ä¶).\n\nPlus globalement, nous sommes contents de voir que l‚Äôoutillage autour de Kubernetes continue √† progresser et que la Developer eXperience est un sujet pris au s√©rieux dans notre communaut√©.\n\n\nRejoignez-nos √©quipes et venez vivre les prochaines conf√©rences avec nous l‚Äôan prochain\n"
} ,
  
  {
    "title"    : "Bedrock √† la Kubecon 2022, 2nde partie : performances, syst√®me et r√©seau",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/14/kubecon-2022-part-2.html",
    "date"     : "June 14, 2022",
    "excerpt"  : "Pour ce second article de synth√®se de la KubeCon Europe 2022, continuons sur le th√®me des performances, peut-√™tre plus bas niveau,\net plongeons aussi dans des outils pouvant √™tre d√©ploy√©s au c≈ìur de nos clusters !\n\n\nCa va commencer, @ KubeCon 2022...",
  "content"  : "Pour ce second article de synth√®se de la KubeCon Europe 2022, continuons sur le th√®me des performances, peut-√™tre plus bas niveau,\net plongeons aussi dans des outils pouvant √™tre d√©ploy√©s au c≈ìur de nos clusters !\n\n\nCa va commencer, @ KubeCon 2022¬†!\n\nL‚Äôautoscaling, autrement\n\nUne des conf√©rences : ‚ÄúAutoscaling Kubernetes Deployments: A (Mostly) Practical Guide‚Äù (vid√©o) pr√©sent√© par NewRelic pr√©sentait le principe d‚Äôautoscaling dans Kubernetes, avec les trois principales ressources associ√©es √† ce concept : ClusterAutoscaler, HorizontalPodAutoscaler et VerticalPodAutoscler.\n\nCette conf√©rence pr√©sentait :\n\n\n  le fonctionnement du scale up/down des pods avec les p√©riodes de stabilisation¬†;\n  le calcul par rapport aux indicateurs utilis√©s¬†;\n  les types de m√©triques utilisables par les HPA et VPA.\n\n\nPas de grande d√©couverte technique pour nous, mais cette conf√©rence nous a surtout permis de confirmer que, chez BedRock, \nnous sommes de plus en plus matures sur la scalabilit√© de nos clusters Kubernetes.\n\nLa conf√©rence donn√©e par AWS (vid√©o) portait sur deux aspects¬†:\n\n\n  Premi√®rement, l‚Äôutilisation d‚Äôinstances Spot (une option √† ne pas n√©gliger si vous souhaitez fortement r√©duire vos co√ªts de compute) et les bonnes pratiques √† mettre en place en utilisant ce type d‚Äôinstances EC2.\n  Le second traitait de la scalabilit√© des n≈ìuds avec ClusterAutoscaler mais pr√©sentait un nouvel outil de provisionnement de n≈ìuds Kubernetes propos√© par AWS¬†: Karpenter.\n\n\nUne des diff√©rences notables par rapport √† cluster-autoscaler est que Karpenter ne fonctionne pas avec des AutoScalingGroup AWS \nmais provisionne directement des instances EC2.\nOutre cette fonctionnalit√©, Karpenter est actuellement √† l‚Äô√©tude chez BedRock, notamment car il permet l‚Äôutilisation de la \ndimension r√©gion, ce qui n‚Äôest pas possible avec cluster-autoscaler et nous pose des probl√®mes avec nos statefullsets dans des ASG multiAZ.\n\nR√©seau, Bande passante et GPU\n\nAutre point abord√© lors de la KubeCon¬†: comment int√©grer la bande passante comme une ressource limitante, de la m√™me fa√ßon que le CPU et la RAM actuellement.\nNous avons pu suivre deux pr√©sentations √† ce sujet¬†: ‚ÄúNetwork-aware Scheduling in Kubernetes‚Äù de Jos√© Santos, Ghent University (video et ‚ÄúBetter Bandwidth Management with eBPF‚Äù de Daniel Borkmann et Christopher M. Luciano, Isovalent (video).\n\nLa premi√®re session proposait un nouveau plugin (repo github) pour permettre l‚Äôorchestration du d√©ploiement de nouveaux pods en fonction de leur charge et co√ªt r√©seau, afin de r√©duire la latence des d√©ploiements.\nUne nouvelle fonctionnalit√© de ce plugin est par ailleurs en d√©veloppement et permettra d‚Äô√©viter de d√©ployer sur un n≈ìud ou la bande passante est d√©j√† satur√©e.\n\nLa seconde pr√©sentation exposait comment eBPF permet de mettre en place de nouveaux pods en prenant en compte la bande passante. Le replay de la conf√©rence est disponible ici, nous vous conseillons son visionnage.\nCette approche pourrait √™tre tr√®s int√©ressante pour Bedrock si nous d√©cidions de migrer notre plateforme VOD sur un cluster Kubernetes¬†: en effet, elle nous permettrait de mieux g√©rer les burst r√©seaux et le throttling de la bande passante qui se produisent sur nos instances.\n\nC√¥t√© GPU, Google, dans son expos√© ‚ÄúImproving GPU Utilization using Kubernetes‚Äù de Maulin Patel et Pradeep Venkatachalam (video), nous a pr√©sent√© deux fa√ßons de partager des ressources GPU dans un cluster kubernetes¬†:\n\n\n  soit en partageant le temps d‚Äôutilisation (timesharing, temporal multiplexing) entre conteneurs sur un m√™me n≈ìud,\n  soit en multi-instance GPU (MIG, spatial multiplexing) permettant de partager les ressources en parall√®le entre conteneur en allouant une partie des c≈ìurs GPU et de sa m√©moire pour chaque conteneur.\n\n\nCette conf√©rence sur l‚Äôutilisation des GPU dans un cluster k8s nous incite √† r√©fl√©chir aux optimisations que nous pourrions faire sur nos plateformes vid√©o et data‚Ä¶\n\nService Mesh¬†: Cilium\n\nAu cours de diverses conf√©rences, nous avons plusieurs fois entendu le nom de ‚ÄúCilium‚Äù associ√© au concept de Service Mesh.\nLa conf√©rence ‚ÄúA guided tour of Cilium Service Mesh‚Äù (vid√©o) nous a permis d‚Äôen apprendre plus sur ce nouveau service qui ne se base plus sur des sidecars, mais sur eBPF.\n\nUn outil peut-√™tre encore un peu jeune, mais clairement prometteur ‚Äì et tr√®s certainement quelque chose que nous allons √©tudier lors d‚Äôun POC dans le courant de l‚Äôann√©e¬†;-)\n\nR√©capitulatif\n\nIl n‚Äôexiste toujours pas d‚Äôoutils magique pour passer √† l‚Äô√©chelle et supporter les pics de charges.\nToutefois, les solutions pr√©sent√©es au cours de cette KubeCon EU 2022 viennent r√©pondre √† des besoins qui sont apparus au fil des ann√©es et dont peu d‚Äôutilisateurs avaient mesur√© l‚Äôimpact au d√©but de leur p√©riple avec Kubernetes.\n\nAussi, eBPF continue √† faire parler de lui et son utilisation semble se r√©pandre.\nL‚Äôid√©e d‚Äôun service mesh plus l√©ger que Istio, par exemple, a l‚Äôair fort int√©ressante¬†!\n\n\nRejoignez-nos √©quipes et venez vivre les prochaines conf√©rences avec nous l‚Äôan prochain\n"
} ,
  
  {
    "title"    : "Bedrock √† la kubecon 2022, 1ere partie : performances applicatives et scalabilit√©",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/13/kubecon-2022-part-1.html",
    "date"     : "June 13, 2022",
    "excerpt"  : "\n\nBEDROCK √† la KubeCon 2022\n\nApr√®s 2018 √† Copenhague et 2019 √† Barcelone, cette ann√©e encore, nous √©tions trois, Coraline, Julien et Pascal, pr√©sents √† la KubeCon CloudNativeCon Europe 2022, √† Valencia !\n\nPlus de quatre ans apr√®s le d√©but de notre...",
  "content"  : "\n\nBEDROCK √† la KubeCon 2022\n\nApr√®s 2018 √† Copenhague et 2019 √† Barcelone, cette ann√©e encore, nous √©tions trois, Coraline, Julien et Pascal, pr√©sents √† la KubeCon CloudNativeCon Europe 2022, √† Valencia !\n\nPlus de quatre ans apr√®s le d√©but de notre migration vers Le Cloud (AWS + Kubernetes) racont√©e dans Le Plan Copenhague, nous visions √† d√©couvrir de nouvelles id√©es, √† confirmer certains de nos choix et √† apprendre des retours d‚Äôexp√©rience de nos pairs. Apr√®s tout, avec une communaut√© aussi large (plus de 7000 participants et participantes cette ann√©e), il serait dommage de rester seuls avec nos id√©es !\n\nSommaire\n\n√Ä trois, nous avons assist√© √† une grosse quarantaine de conf√©rences. Nous avons choisi d‚Äôorganiser nos notes par th√®mes, en quatre articles :\n\n  Un premier, celui-ci, centr√© sur les performances applicatives, sur la scalabilit√© des applications et la gestion des co√ªts.\n  Le second, consacr√© aux performances syst√®me, aux services mesh, aux fonctionnalit√©s au niveau du cluster.\n  Le troisi√®me, pour regrouper ce qui est Dev XP, outillage, CI/CD, rollback, observabilit√©‚Ä¶\n  Et un dernier, pour quelques sujets divers, dont le chaos engineering et la r√©silience, et pour conclure sur ce que nous avons retenu de cette √©dition de la KubeCon publication jeudi.\n\n\nAvec une plateforme de VOD et de replay d√©ploy√©e en marque blanche pour des broadcasters europ√©en majeurs, des millions d‚Äôutilisateurs actifs, des milliers de CPU consomm√©s, des centaines d‚Äôinstances allum√©es et des dizaines de microservices, les performances sont au c≈ìur de nos pr√©occupations.\n\nCet article reprend nos retours sur les nombreuses conf√©rences consacr√©es √† la scalabilit√© lors de cette KubeCon 2022. Cette fonctionnalit√© essentielle de Kubernetes est l‚Äôune des raisons de notre migration sur cette plateforme. En effet, notre activit√© n√©cessite que nous adaptions la taille de nos clusters en fonction du nombre d‚Äôutilisateurs connect√©s.\nNous avons donc assist√© √† la plupart des conf√©rences consacr√©es √† la performance et √† l‚Äôadaptation de celle-ci en fonction de nos besoins.\n\nLe scaling vertical\nLa conf√©rence ‚ÄúHow Lombard Odier Deployed VPA to Increase Resource Usage Efficiency‚Äù (vid√©o) nous pr√©sentait comment fonctionnent les requests et limits.\nUn sujet qui demande du temps pour √™tre efficace afin de ne pas √™tre en oversizing ou au contraire en undersizing.\n\nMais surtout, le conf√©rencier nous a pr√©sent√© son impl√©mentation d‚Äôun composant Kubernetes assez rarement utilis√© : Le VerticalPodAutoscaler. Le VPA √† fait r√©cemment l‚Äôobjet de discussions au sein de nos √©quipes et cette pr√©sentation a confirm√© notre ressenti : cette ressource est int√©ressante pour des cas d‚Äôusages sp√©cifiques, notamment sur des ‚Äúworkloads‚Äù assez consommateurs en RAM et/ou en CPU et ne pouvant pas √™tre d√©coup√©s en multiples pods via un HorizontalPodAutoscaler.\n\nle VPA souffre toujours d‚Äôune limitation : l‚Äôajout de RAM ou CPU √† chaud n‚Äôest pas possible et n√©cessite la re-cr√©ation du pod.\n\n\n\nAm√©liorer la scalabilit√©\nUne autre conf√©rence, donn√©e cette fois-ci par Intel, pr√©sentait un projet r√©cent : Telemetry Aware Scheduler (vid√©o). Cet outil permet d‚Äôam√©liorer les choix du scheduler de Kubernetes en s‚Äôappuyant sur des m√©triques ‚Äúcustoms‚Äù. Le projet est r√©cent et en ALPHA, mais √† surveiller dans l‚Äôavenir.\n\nLors d‚Äôune autre conf√©rence intitul√©e ‚ÄúHow Adobe is optimizing resource usage in K8s‚Äù (vid√©o), Carlos Sanchez a pr√©sent√© un outil interne permettant d‚Äô√©mettre des recommandations bas√©es sur un historique de m√©triques, un peu comme fait VPA, mais au niveau d‚Äôun namespace ou du cluster entier. Il est √©galement revenu sur comment ils parviennent √† √©teindre automatiquement des applications non utilis√©es par les clients pour r√©aliser des √©conomies cons√©quentes.\n\nMais comment configurer les requests, limits et tout √ßa‚Ä¶ sans y passer des mois ?\n\nNotre plateforme est compos√©e de dizaines de services qui interagissent les uns avec les autres et sont soumis √† un trafic qui varie au quotidien, avec des pics parfois impressionnants. Le param√©trage des requests et limits de chaque conteneur, ainsi que d‚Äôautres ressources, comme le nombre de processus php-fpm par conteneur, est un travail de fourmi, o√π nous devons it√©rer quotidiennement pendant une ou deux semaines, en travaillant application par application. Et tout ce travail est √† refaire lorsque les applications ou leurs usages √©voluent‚Ä¶ un vrai casse-t√™te !.\nNous ne sommes pas les seuls √† rencontrer ces probl√©matiques et c‚Äô√©tait le sujet de la conf√©rence ‚ÄúGetting the optimal service efficiency that autoscaler won‚Äôt give you‚Äù (vid√©o), o√π une approche bas√©e sur de l‚ÄôIA (ou, plut√¥t, sur du brute-force) √©tait pr√©sent√©e.\n\nVoici les grandes lignes de la m√©thodologie pr√©sent√©e :\n\n  d√©finition d‚Äôun sc√©nario de load-test (ce qui reste difficile, il faut qu‚Äôil soit repr√©sentatif de la r√©alit√©)\n  D√©finition d‚Äôobjectifs (les temps de r√©ponses attendus, le pourcentage d‚Äôerreurs‚Ä¶ en fait, des SLOs que chacun devrait d√©j√† avoir pour ses services),\n  Lancer en boucle ces scenarios en retouchant request et limits (et configuration JVM) entre chaque it√©ration.\n\n\nSur un cas r√©el, apr√®s la 34·µâ it√©ration (r√©alis√©es en 19 heures), environ 49% d‚Äô√©conomies ont √©t√© r√©alis√©es. Mais surtout, cela a repr√©sent√© un jour de travail gr√¢ce √† cet outillage, au lieu de deux mois √† la main.\n\nLe logiciel utilis√© ne semble pas disponible en open-source, mais l‚Äôapproche ‚Äúautomatiser les it√©rations‚Äù en retouchant les param√®tres est tr√®s int√©ressante et nous saurions la reproduire. Elle nous permettrait de gagner beaucoup de temps, en supprimant beaucoup de t√¢ches fastidieuses aujourd‚Äôhui. Reste √† continuer √† d√©finir des SLOs, puis cr√©er de nouveaux sc√©narios de load-testing repr√©sentatifs ! ;-)\n\nEt les co√ªts d‚Äôh√©bergement, alors ?\n\nNous avons aussi entendu parler plusieurs fois de co√ªts d‚Äôh√©bergement tout au long de cette KubeCon : comme l‚Äôillustrent les travaux de la FinOps Foundation, nous sommes de plus en plus nombreux √† r√©aliser que si nous ne pensons pas √† l‚Äôimpact financier de nos infrastructures √©lastiques, o√π n‚Äôimporte quel membre des √©quipes peut d√©ployer des applications, la facture augmente vite et fort.\n\nLe talk ‚ÄúWhy Kubernetes can‚Äôt get around FinOps - Cost Management best practices‚Äù (vid√©o) √©tait une bonne introduction aux principes de gestion de co√ªts sur Kubernetes. Rien de nouveau pour nous, sur la th√©orie‚Ä¶ m√™me s‚Äôil nous reste encore beaucoup de progr√®s √† r√©aliser pour mieux ma√Ætriser nos frais d‚Äôh√©bergement !\n\nConclusion\nSur ces sujets de scalabilit√©, les conf√©rences auxquelles nous avons assist√© confirment que bon nombre des choix que nous avons fait sont les bons, et que les probl√©matiques qui nous font encore souffrir sont partag√©es par d‚Äôautres membres de la communaut√©.\n\nNous allons prochainement tenter de mettre en place VPA sur un de nos composants majeur, VictoriaMetrics, qui consomme beaucoup de ressources quelques heures par jour et pour lequel un scaling horizontal n‚Äôest pas adapt√©.\n\nNous n‚Äôen avons pas (ou peu) entendu parler pendant cette KubeCon, mais nous √©tudions en ce moment la solution Karpenter pour remplacer cluster-autoscaler, tr√®s utilis√© dans la communaut√©, mais qui ne sait pas r√©ellement tirer profit de sp√©cificit√©s li√©es √† AWS.\n\nEnfin, sur les co√ªts‚Ä¶ OK, il n‚Äôy a pas que chez nous que c‚Äôest compliqu√©. Et c‚Äôest clairement un sujet, dans Kubernetes comme au niveau d‚ÄôAWS, sur lequel nous avons encore du boulot devant nous pour un an ou deux. Nous avons m√™me un poste FinOps ouvert ;-)\n\n\n"
} ,
  
  {
    "title"    : "Bedrock&#39;s backend architecture and its front API Gateway",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front",
    "url"      : "/2022/06/10/backend-bff-intro.html",
    "date"     : "June 10, 2022",
    "excerpt"  : "What is a BFF, and how does it simplify the development of frontend applications?\n\nIntroduction\n\nAhoy there o/\n\nThis article is the first in a series explaining the backend architecture we use at Bedrock.\nThis first piece is dedicated to the BFF A...",
  "content"  : "What is a BFF, and how does it simplify the development of frontend applications?\n\nIntroduction\n\nAhoy there o/\n\nThis article is the first in a series explaining the backend architecture we use at Bedrock.\nThis first piece is dedicated to the BFF API. Without further delay, let‚Äôs jump into the subject!\n\nSo, what‚Äôs a BFF?\n\nFor years, we have been using a microservices pattern (1). Each with their own responsibilities.\n\nBackend and frontend development have long been decoupled.\nEvery frontend applications had to know about all microservices, call them and know what to do with their data.\n\n\n\nThis approach had three main limitations from a frontend point of view:\n\n  It forced Bedrock to duplicate logic in each application.\n  It prevented us from deprecating legacy APIs.\n  New features implied a frontend development and deployment.\n\n\nThere were other downsides, which were mainly derivatives from those listed above.\n\n  As an example, updating an icon into the menu bar required us to deploy all applications. It is not always easy or doable, and cannot be forced onto users without losing some of them.\n\n\nThe BFF tries to answer those limitations!\n\nIt‚Äôs a single API (2) that handles all the frontend applications queries to display contents, navigation, or even start downloads.\nIn addition, this gateway (3) gathers all business logic. This is done in order to avoid repeating the logic in each application.\nThat‚Äôs what we call a Back For Front!\n\nAbstracting the microservices\n\n\n\nThe first main advantage is to abstract the backend complexity for the frontend teams.\nIn the previous model, each application had to know where each data came from, how to parse it, and what to do with it.\n\nIn the new model, we can easily deprecate an API, replace it, change how the data is stored or returned.\nTo do so, we only need the team leading the change, and the team handling the BFF to work together at their own pace.\nThey can decide, depending on the change, how to handle the migration.\nThey might decide to use a new endpoint to be switched at some point, or add a new attribute in the response, etc.\n\nAll those changes will happen without any frontend application noticing it.\n\nSimplification of the data structure\n\nAnother advantage is to simplify the data representation.\n\n\n\nBy taking all this responsibility in a single API, it now translates the data from the APIs to a single unified representation that all applications can use.\n\n\n\nThis representation is maintained by the BFF in a single openapi schema (4). It shares the same concepts between the multiple endpoints of the API.\n\nThe main usage of the BFF is to handle the navigation between the pages of the application.\nIn the pictures above and below, the central block shows the application screen. The application page is split into two parts.\n\nThe top is answered by the navigation endpoint which gives a list of groups and entries.\nEvery entry can have nested groups, and an action.\n\nThe second part is what we call the layout. It‚Äôs a representation of the page, composed of multiple blocks, each with a list of items.\nEach item has a title, a description, an image, and an action (the same type as in the entries).\n\nThis makes the BFF responsible for what to display in the page, and in which order and how to display it.\nHow to display things is described through template strings that tell how to display each block.\n\nIt‚Äôs important to understand that the BFF does not return HTML! It returns a JSON string that needs to be parsed and interpreted by the application.\n\nEvery application still has to care about its design system, what font to use, which iconography.\nThis means that a template Card might not be displayed exactly the same between a computer, a mobile phone or a television; even if the data are the same.\n\n\n\nThere are other usages to the BFF (5), such as handling downloads, and some others to come, but it shares the same concept by answering to the front something to display.\n\nKeeping all logic in one place\n\nThe last main gain with the BFF, is that we‚Äôre able to put all the logic in one place.\nThis allows us to update and change the business rules at any time.\n\nHere are a few examples\n\n\n  When a user tries to navigate the application, if he uses a new device while he has already reached the limit of allowed devices, we can display a layout asking him to delete a device first.\n\n\nThis limit can be removed or changed at any time in all applications.\n\n\n  In France, explicit contents must be filtered out during daytime\n\n\nIf this rule changes, we will do so directly in the BFF, and no application will ever notice it.\n\nConclusion\n\nThe model known as back for front or API Gateway is nothing new and other major services already use it.\nWe‚Äôve been using this model for more than 3 years now. It has undergone some major updates (6) but this is a model we‚Äôre happy with.\n\nWe plan to expand this pattern to handle even more logic inside the BFF in the coming years and keep being frontend application‚Äôs best friend.\n\n\n\nThat‚Äôs all for today‚Äôs post!\n\nIn the next part we will talk about handling the failures of the dependencies the BFF is calling, and what to do to always answer something usable by the applications.\n\nNotes\n\n\n  For more details about microservices, you can read this piece from AWS.\n  There are some other APIs called by our applications, such as the authentication service, but let‚Äôs not get lost into details‚Ä¶\n  There‚Äôs a lot of resources about API Gateway, here is one from nginx.\n  Open API is used to define the communication standards between our BFF and the clients, more explanation on the dedicated website of the organization.\n  In addition to note 1, we are currently moving to the api gateway model, and some behaviors still require the application to call dedicated microservices.\n  ( in French üá´üá∑ ) An old conference from 2020 given by Benoit VIGUIER, previous Team Lead in charge of the BFF, about API gateway and asynchronous development.\n\n\nFrom the same series\n\n\n  What‚Äôs a BFF\n  Handling API failures in a gateway\n  What‚Äôs an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  üá∫üá∏ Announcing BedrockStreaming/pr-size-labeler github action\n  üá´üá∑ Retour sur l‚ÄôAFUP Day Lille 2022\n\n"
} ,
  
  {
    "title"    : "Migration progressive vers Redux Toolkit",
    "category" : "",
    "tags"     : " redux, lyonjs, meetup, react, javascript, conference",
    "url"      : "/2022/06/08/migration-progressive-vers-redux-toolkit.html",
    "date"     : "June 8, 2022",
    "excerpt"  : "Redux est le gestionnaire d‚Äô√©tat global le plus populaire au sein de la communaut√© JS.\nSes cr√©ateurs encouragent d√©sormais l‚Äôutilisation de Redux Toolkit (RTK). Une suite d‚Äôutilitaires facilitant l‚Äôusage de Redux et r√©duisant notamment sa verbosit...",
  "content"  : "Redux est le gestionnaire d‚Äô√©tat global le plus populaire au sein de la communaut√© JS.\nSes cr√©ateurs encouragent d√©sormais l‚Äôutilisation de Redux Toolkit (RTK). Une suite d‚Äôutilitaires facilitant l‚Äôusage de Redux et r√©duisant notamment sa verbosit√©.\nDans cette pr√©sentation, je vous propose un live coding pour migrer pas-√†-pas une application React/Redux vers RTK.\n"
} ,
  
  {
    "title"    : "Comment ne pas jeter son application Frontend tous les deux ans ?",
    "category" : "",
    "tags"     : " conference, js, react, lyonjs, meetup",
    "url"      : "/2022/06/08/comment-ne-pas-jeter-votre-application.html",
    "date"     : "June 8, 2022",
    "excerpt"  : "Bonnes pratiques pour la maintenance d‚Äôune application web\nRefaire son front tous les 2 ans, c‚Äôest devenu une pratique plut√¥t courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la m√™me codebase et √ß...",
  "content"  : "Bonnes pratiques pour la maintenance d‚Äôune application web\nRefaire son front tous les 2 ans, c‚Äôest devenu une pratique plut√¥t courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la m√™me codebase et √ßa depuis plus de 7 ans! En plus, ce n‚Äôest pas une petite application puisqu‚Äôil s‚Äôagit de 6play et de salto.\nVous pourriez vous dire: ‚ÄúOh les pauvres, maintenir une application vieille de presque 10 ans √ßa doit √™tre un enfer !‚Äù\nRassurez-vous, ce n‚Äôest pas le cas ! Nous avons tous travaill√© sur des projets bien moins vieux mais sur lesquels le d√©veloppement de nouvelles fonctionnalit√©s √©tait bien plus p√©nible.\n\nQuel est notre secret ? C‚Äôest ce que vous allez d√©couvrir pendant ce talk !\nAutomatisation des t√¢ches courantes, gestion de la dette, testing et architecture seront des sujets abord√©s.\nCe talk propose des th√©matiques qui ne concernent pas que le frontend !\n\nPlus de d√©tails dans l‚Äôarticle suivant.\n\n"
} ,
  
  {
    "title"    : "üç™ It‚Äôs Cookie Jar Time üç™ #LFT 03/06/22",
    "category" : "",
    "tags"     : " UX, lft, tech",
    "url"      : "/%F0%9F%8D%AA-its-cookie-jar-time-%F0%9F%8D%AA",
    "date"     : "June 3, 2022",
    "excerpt"  : "D√©couvrez ‚ÄúCookie Jar‚Äù, la base de donn√©es de connaissances utilisateurs qui centralise et documente toute l‚ÄôUX research produite chez Bedrock. \nPr√©sent√© par Elise Carenau.\n",
  "content"  : "D√©couvrez ‚ÄúCookie Jar‚Äù, la base de donn√©es de connaissances utilisateurs qui centralise et documente toute l‚ÄôUX research produite chez Bedrock. \nPr√©sent√© par Elise Carenau.\n"
} ,
  
  {
    "title"    : "La pression je ne la subis pas, je la fais #LFT 03/06/22",
    "category" : "",
    "tags"     : " homemade, diy, beer, brewing, lft, partage",
    "url"      : "/la-pression-je-ne-la-subis-pas-je-la-fais",
    "date"     : "June 3, 2022",
    "excerpt"  : "Dans cette pr√©sentation, Mathieu Lopez nous pr√©sente un retour d‚Äôexp√©rience sur le brassage de bi√®re.\nBrasser sa bi√®re, comment √ßa marche ? Quelles sont les √©tapes cl√©s ? \nAu final, c‚Äôest quoi une bi√®re ?\n\n",
  "content"  : "Dans cette pr√©sentation, Mathieu Lopez nous pr√©sente un retour d‚Äôexp√©rience sur le brassage de bi√®re.\nBrasser sa bi√®re, comment √ßa marche ? Quelles sont les √©tapes cl√©s ? \nAu final, c‚Äôest quoi une bi√®re ?\n\n"
} ,
  
  {
    "title"    : "Errances √† Kiilop√§√§ #LFT 03/06/22",
    "category" : "",
    "tags"     : " voyage, lft, partage",
    "url"      : "/errances-a-kiilopaa",
    "date"     : "June 3, 2022",
    "excerpt"  : "Mode d‚Äôemploi et retour d‚Äôexp√©rience d‚Äôun trek polaire hivernal en solitaire pr√©sent√© par Sylvain Guyon.\n",
  "content"  : "Mode d‚Äôemploi et retour d‚Äôexp√©rience d‚Äôun trek polaire hivernal en solitaire pr√©sent√© par Sylvain Guyon.\n"
} ,
  
  {
    "title"    : "Cr√©er un jeu vid√©o en moins d‚Äôune heure sur Unity #LFT 03/06/22",
    "category" : "",
    "tags"     : " diy, livecoding, brewing, lft, tech",
    "url"      : "/creer-un-jeu-video-en-moins-dune-heure-sur-unity",
    "date"     : "June 3, 2022",
    "excerpt"  : "Julie Nginn nous pr√©sente une introduction au moteur de jeu Unity, en livecodant la construction d‚Äôun jeu video.\nPas besoin d‚Äô√™tre d√©veloppeur pour pouvoir cr√©er un jeu vid√©o, ce talk s‚Äôadresse √† tout le monde üôÇ\n",
  "content"  : "Julie Nginn nous pr√©sente une introduction au moteur de jeu Unity, en livecodant la construction d‚Äôun jeu video.\nPas besoin d‚Äô√™tre d√©veloppeur pour pouvoir cr√©er un jeu vid√©o, ce talk s‚Äôadresse √† tout le monde üôÇ\n"
} ,
  
  {
    "title"    : "Connaissez vous Cache¬∞Cache ? #LFT 03/06/22",
    "category" : "",
    "tags"     : " swift, ios, lft, tech",
    "url"      : "/connaissez-vous-cache%C2%B0cache",
    "date"     : "June 3, 2022",
    "excerpt"  : "Petite d√©couverte d‚Äôun pattern de composition en utilisant une librairie de cache comme exemple, pr√©sent√©e par notre expert Sebastien Drode.\n",
  "content"  : "Petite d√©couverte d‚Äôun pattern de composition en utilisant une librairie de cache comme exemple, pr√©sent√©e par notre expert Sebastien Drode.\n"
} ,
  
  {
    "title"    : "Comment faire un trailer vid√©o qui d√©chire avec les technos web ? #LFT 03/06/22",
    "category" : "",
    "tags"     : " video, react, js, remotion, ffmpeg, lft, tech",
    "url"      : "/comment-faire-un-trailer-video-qui-dechire-avec-les-technos-web",
    "date"     : "June 3, 2022",
    "excerpt"  : "Un jour, alors que Micka√´l Alves √©tait fraichement arriv√© √† Bedrock, il a eu le malheur de demander √† Antoine Caron sur quoi il bossait entre midi et deux, qui semblait fort l‚Äôamuser. Quelle erreur du dev Franco-Portugais ! :scream:\nIl ne se douta...",
  "content"  : "Un jour, alors que Micka√´l Alves √©tait fraichement arriv√© √† Bedrock, il a eu le malheur de demander √† Antoine Caron sur quoi il bossait entre midi et deux, qui semblait fort l‚Äôamuser. Quelle erreur du dev Franco-Portugais ! :scream:\nIl ne se doutait pas encore de la folie de son nouveau tech lead¬†: ¬´¬†J‚Äôessaie de g√©n√©rer des vid√©os en MP4 √† partir de composants React, tu veux voir¬†?¬†¬ª\n"
} ,
  
  {
    "title"    : "Amateur de pression #LFT 03/06/22",
    "category" : "",
    "tags"     : " plong√©e, partage, lft, partage",
    "url"      : "/amateur-de-pression",
    "date"     : "June 3, 2022",
    "excerpt"  : "Ivresse des profondeurs, exploration, d√©passement de soi, durant ce talk, Hugo Riffiod nous partage sa passion pour la plong√©e sous-marine.\n",
  "content"  : "Ivresse des profondeurs, exploration, d√©passement de soi, durant ce talk, Hugo Riffiod nous partage sa passion pour la plong√©e sous-marine.\n"
} ,
  
  {
    "title"    : "Announcing BedrockStreaming/pr-size-labeler github action üéâ",
    "category" : "",
    "tags"     : " oss, github, devops",
    "url"      : "/2022/05/31/github-action-pr-size-labeler.html",
    "date"     : "May 31, 2022",
    "excerpt"  : "\n\nSmaller PR for a reduced mental load\n\nFor several years at Bedrock Streaming the technical teams have used the Pull Requests code review for each project. \nBetween collective ownership, quality improvement, regression detection, knowledge sharin...",
  "content"  : "\n\nSmaller PR for a reduced mental load\n\nFor several years at Bedrock Streaming the technical teams have used the Pull Requests code review for each project. \nBetween collective ownership, quality improvement, regression detection, knowledge sharing, learning, there is no question in this article to further legitimize the immense interest to implement this practice in your teams.\n\nThis practice can however lead to some problems, each developer who proposes Pull Requests for review by his colleagues can sometimes propose monstrous diffs.\nSometimes constrained by certain project mechanics or tools, but sometimes also by the ‚ÄúWheelbarrow‚Äù effect.\n\n\n  While I was there, I took the opportunity to modify this too.\n\n\nIt always starts from a good will, however, to make PR that changes several intentions. \nBy creating his wheelbarrow, the developer is adding diff to a pull request that deviates from the original intent.\n\nLimiting the number of intentions of a pull request often simplifies the proofreading of it.\n\n\n  Has anyone ever had the pleasure of reviewing a Pull Request with more than 1000 lines of changes with more than 100 modified files?\n\n\nWe also forget that making a ‚Äúbig‚Äù Pull Request can also generate a mental load on the person or persons assigned to its development. \nWe have to remember the modified files, we are more likely to generate conflicts.\n\n\n  Ok, lets make smaller PR‚Äôs! We promise!\n\n\nYou can‚Äôt improve anything without measuring it\n\nSaying ‚Äúfrom now on we do smaller PR1‚Äù is a pious hope.\nWe have been doing application monitoring for a long time, we know that thanks to these measurements we are able to understand if the evolution is rather positive or not.\nWhy not do it on our PR sizes?\nWhy not implement monitoring on our devs?\n\nThe idea is absolutely not to measure/comparison the performance of our developers. \nIt would not be positive for the engineering manager and the dev to compare the performance of one developer against another. \nWe are all different after all!\n\nThe size of a dev‚Äôs PRs does not reflect his productivity at all, it just allows to evaluate the personal and collective mental load produced.\nThere are other measures we would like to follow, but let‚Äôs start with the size of the PR.\n\nBe warned, the purpose of this metric is not to say ‚ÄúOh! you made an XL size PR that‚Äôs not right‚Äù üò°.\nIt happens from time to time, and it‚Äôs not bad.\nYou should rather look at the distribution of PR sizes of a dev.\n\nLet‚Äôs take the example of a dev named Bob who would have this distribution over the last month:\n\n\nHere we see that Bob is globally making large PRs (taking arbitrary t-shirt sizes), seeing this we can say: As a TechLead, how can I best accompany Bob to make smaller PRs?\n\nNext, let‚Äôs look at Alice‚Äôs profile, which has a more centered distribution:\n\n\nHere, we can say that overall the majority of RPs are of moderate size (in this absolute scale), so the mental load should be lower than for Bob.\nThis remains an interpretation that will require some discussion to be sure.\n\nHow to set it up?\n\nIf you are interested in this measure and like us you use the Github Actions solution for your automation, it will be very easy for you to implement our brand new pr-size-labeler in your projects.\n\nTo do so, you can add a workflow to your Github repository:\n\nname: üè∑ PR size labeler\n\non: \n  pull_request:\n\njobs:\n  pr-labeler:\n    runs-on: ubuntu-latest\n    name: Label the PR size\n    steps:\n      - uses: BedrockStreaming/pr-size-labeler@v1.1.0\n        with:\n          token: $\n          exclude_files: .lock # RegExp of your excluded file pattern\n\n\nThe action will then put Size/S, Size/XL tags on your PRs automatically according to the number of modified files and the number of added or deleted lines.\n\nüßô‚Äç You can change the text of the labels used and even the thresholds for each size as you wish.\nTake a look at Github presentation page of this Github action.\n\nOnce set up, you should also notice the added labels can allow to evaluate the time needed for the review before starting it.\n\n\n  I‚Äôve got 30 minutes to spare, I‚Äôm not going to start reviewing this PR XL.\n\n\nIt‚Äôs now your turn to play!\n\n  \n    \n      Alias for Pull Request¬†&amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Retour sur l&#39;AFUP Day Lille 2022",
    "category" : "",
    "tags"     : " backend, php, conference",
    "url"      : "/2022/05/30/afup-day-lille-2022.html",
    "date"     : "May 30, 2022",
    "excerpt"  : "\n\nCette ann√©e encore, Bedrock participait √† l‚ÄôAFUP Day 2022, nous avons eu la chance de profiter de conf√©rences de qualit√© et aux sujets vari√©s.\n\nPHP 8.1 en d√©tail\n\nDamien Seguy nous a parl√© des nouveaut√©s de PHP 8.1 mais aussi de celles de PHP 8....",
  "content"  : "\n\nCette ann√©e encore, Bedrock participait √† l‚ÄôAFUP Day 2022, nous avons eu la chance de profiter de conf√©rences de qualit√© et aux sujets vari√©s.\n\nPHP 8.1 en d√©tail\n\nDamien Seguy nous a parl√© des nouveaut√©s de PHP 8.1 mais aussi de celles de PHP 8.2 qui arriveront prochainement.\n\nDamien nous a parl√© en vrac :\n\n\n  de l‚Äôajout du format AVIF dans GD\n  l‚Äôajout des fonctions fsync et fdatasync qui permettent de synchroniser les donn√©es d‚Äôun fichier sur l‚ÄôOS. Cela veut dire que le fichier sera bien √©crit sur le disque.\n  l‚Äôarriv√©e de la prise en compte du format ristreto255 avec le libsodium\n  de la fonction array_is_list\n\n\nLe conf√©rencier a insist√© sur le fait de ne pas h√©siter √† monter de version r√©guli√®rement. Il est possible d‚Äôutiliser les polyfills d√©j√† existants (pour la 8.1 et m√™me la 8.2) ou d‚Äôajouter ses propres fonctions, mais de bien faire attention d‚Äôutiliser function_exists pour pr√©parer les migrations.\n\nUne des nouveaut√©s phare de la version 8.1 est l‚Äôajout de la gestion des enums. Une √©num√©ration est un objet et non un type scalaire. Il est donc possible de l‚Äô√©tendre (via des interfaces, ‚Ä¶). Cela veut aussi dire que nous ne pouvons pas les utiliser comme clefs de tableau par exemple. Il est par contre possible d‚Äôutiliser la propri√©t√© $myEnum-&amp;gt;value. Attention, les √©num√©rations et les classes partagent le m√™me espace de nom, nous ne pouvons donc pas avoir une enum et une classe s‚Äôappelant pareil dans le m√™me namespace.\n\nLes Fibers, solution pour rendre les programmes interruptibles, a √©t√© rapidement introduite, mais nous vous conseillons de regarder la conf√©rence sur ce sujet de notre cher et estim√© coll√®gue Benoit Viguier.\n\nDans la derni√®re version, √† ce jour, a √©t√© introduit le type de retour never. Une fonction ou m√©thode qui retourne ce type ne pourra pas faire de return (m√™me vide) ni m√™me un yield. Il sera, par contre, possible d‚Äôappeler die, exit, throw ou suspend.\n\nLes constantes peuvent maintenant √™tre finales, cela bloquera la possibilit√© de surcharger leurs valeurs par h√©ritage.\n\nIl est d√©sormais possible de faire des propri√©t√©s readonly. Cette propri√©t√© devra forc√©ment √™tre typ√©e, et ne pourra pas avoir de valeur par d√©faut. readonly ne peut pas √™tre utilis√© avec static. Si la propri√©t√© est un objet, l‚Äôinstance pourra √™tre modifi√©e (par exemple avec des setters), mais pas remplac√©e.\n\nIl est maintenant possible d‚Äôinstancier des valeurs par d√©faut. Par exemple :\n\nfunction serialize(\n    string \\$data, \n    Formatter \\$formatter = new DefaultFormatter()\n) { ... }\n\n\nCette instanciation est possible dans les arguments de fonction ou de m√©thode, les variables statiques ou encore les constantes globales. Par contre, ce n‚Äôest pas compatible avec les constantes de classes ou les propri√©t√©s de classes (sauf si ces derni√®res sont des propri√©t√©s promues).\n\nDans sa version 8.1, PHP apporte aussi les types d‚Äôintersections. Un exemple pr√©sent√© serait de vouloir une instance de type Traversable ET Countable. Les types scalaires ne sont pas accept√©s, c‚Äôest uniquement avec plusieurs classes.\n\nDe l‚Äôhumain √† l‚Äôordinateur, ou d√©couvrir le sens d‚Äôun texte avec ElasticSearch\n\nMathias ARLAUD nous a ensuite parl√© d‚ÄôElasticSearch et de comment il est possible d&#39;attribuer un score de corr√©lation entre un texte donn√© et une multitude d&#39;autres.\n\n\nIl a d√©cortiqu√© cette √©quation (d√©j√† simplifi√©e) en nous expliquant les m√©canismes en place pour calculer ce score.\nIl nous a parl√© de Term Frequency (la fr√©quence √† laquelle un mot appara√Æt dans un document), d‚ÄôInverse Document Frequency (la pertinence des mots) ainsi que de Coordination Factor (le fait de valoriser un document avec le plus haut pourcentage de mots pr√©sents dans la requ√™te).\nAvec un exemple simple (Les D√©veloppeurs ü•∞ d√©velopper avec,VIM&amp;amp;excl;), il nous a montr√© comment les filtres de caract√®res tels que html_strip, le mapping, les g√©n√©rateurs de tokens (whitespace -&amp;gt; 1 token = 1 mot) ou bien encore les filtres de tokens (phonetic, stopwords) permettent d‚Äôenlever le bruit des phrases humaines pour ne r√©cup√©rer que les informations les plus pertinentes pour calculer ce score de corr√©lation. Apr√®s application de ces diff√©rentes √©tapes, son exemple se transforme en [developp] [aim] [developp] [vim], ce qui permet alors √† ElasticSearch d‚Äô√©liminer des documents non pertinents et qui seraient remont√©s si ces filtres n‚Äôavaient pas √©t√© appliqu√©s.\nPour plus d‚Äôinformations, Mathias a mis √† disposition les slides de son\ntalk.\n\nGuide pratique d&#39;une m√©thodologie UX pour la conception de features\n\nJessica Martel nous a pr√©sent√© une m√©thodologie UX pour la conception de features qu‚Äôelle a mise en place et suivie lors d‚Äôune exp√©rience chez Decitre et maintenant chez Unow.\nElle nous a parl√© de l‚Äôimportance de la constitution d‚Äôune √©quipe projet regroupant tous les acteurs (PO, devs, le p√¥le Design et les √©quipes m√©tier). Diversifier les acteurs permet d&#39;accro√Ætre l‚Äôadh√©sion du projet, d‚Äôapporter diff√©rentes cultures et de cibler le besoin.\n\nS&#39;ensuivent plusieurs √©tapes :\n\n\n  Product concept : √©valuation du besoin, de la criticit√©\n  UX Research et audit : bench global, entretiens\n  User journey : identification des diff√©rentes √©tapes (d√©termin√©es suite au bench et entretiens), mise en place du workflow\n  Specs fonctionnelles et design : cas d‚Äôusages, r√®gles m√©tier, versions, KPI / cr√©ation de wireframes, maquettes UI et prototypes\n\n\nCette m√©thodologie comprend cependant des limites ! Elle prend beaucoup de temps et est soumise au contexte, aux priorisations d‚Äôautres features, au downsizer‚Ä¶\n\nLe Prom√©th√©e moderne : embarquer PHP dans Go\n\nK√©vin Dunglas nous a parl√© de comment embarquer PHP dans du Go. Apr√®s avoir list√© les diff√©rentes SAPI (Module Apache, FPM, ‚Ä¶) et nous avoir expliqu√© bri√®vement le langage Go et sa librairie standard net/http, K√©vin nous a pr√©sent√© FrankenPHP et toute la r√©flexion et les contraintes rencontr√©es pour le cr√©er. Ce projet est un nouveau serveur Web en Go qui est capable d‚Äôappeler l‚Äôinterpr√©teur PHP et donc de faire tourner nos applications Web. Le projet est bien avanc√©, mais pas termin√©. N‚Äôh√©sitez pas √† le contacter si vous voulez essayer avec vos applications, les retours lui seront utiles. En tout cas, chez Bedrock, on va suivre √ßa de pr√®s !\n\nLes subtilit√©s du e-commerce √† la fran√ßaise\n\nApr√®s nous avoir pr√©sent√© les diff√©rentes taxes fran√ßaises et chez quelques-uns de nos voisins europ√©ens, David Buros nous a r√©sent√© les diff√©rents probl√®mes qu‚Äôil a rencontr√©s avec Sylius:\n\n\n  l‚Äôaffichage du prix HT et du prix TTC\n  la gestion des r√©ductions avec ce double affichage\n  la gestion des √©cotaxes\n  le paiement par mandat administratif\n\n\nComment on est pass√© de 1 800 emails √† 70 000 par jour chez Trustt en 1 mois avec RabbitMQ\n\nC√©dric Driaux nous a expliqu√© comment ils ont mis en place RabbitMQ chez Trustt pour g√©rer l‚Äôenvoi de plus de 70 000 mails par jour, afin de remplacer une ancienne solution qui lan√ßait un CRON toutes les 15 minutes dans le but de faire les calculs et la distribution. Il y avait √©galement des appels API √† des outils externes, causant des ralentissements. De plus, certains reliquats de mail n&#39;√©taient pas envoy√©s.\n\nPour r√©pondre √† ce probl√®me, l‚Äôid√©e √©tait de mettre les mails dans une file ou queue en anglais, permettant une mise en attente de l‚Äôenvoi des messages, puis utiliser un consumer pour traiter les messages.\n\nC√©dric a d√©cid√© de choisir et de mettre en place (en √† peine un mois‚ÄØ!) RabbitMQ comme solution √† cette probl√©matique. Cela a permis : une baisse de charge des serveurs, notamment due √† la suppression des CRON, une augmentation des mails envoy√©s, dont ces derniers peuvent d√©sormais tous √™tre trait√©s dans la journ√©e. Les erreurs sont mises de c√¥t√© pour √™tre trait√©es plus tard et ne pas ralentir le processus. Enfin, il est dor√©navant possible pour eux d‚Äôajouter d‚Äôautres mails dans la queue dans la journ√©e.\n\nGr√¢ce √† RabbitMQ, ils ont pu fortement augmenter leurs capacit√©s d‚Äôenvoi de mails tout en soulageant les serveurs.\n\nCecil, mon g√©n√©rateur de site statique\n\nArnaud Ligny nous a pr√©sent√© son projet perso de g√©n√©rateurs de sites statiques en archive phar : Cecil. Il voulait une solution rapide √† prendre en main, intuitive et avec une s√©paration entre le contenu et la mise en forme. Ce side project avait pour but de remettre ses connaissances √† jour en appliquant les bonnes pratiques. L‚Äôapplication est automatis√©e, le paquet .phar est automatiquement g√©n√©r√© par GitHub Action lors de la cr√©ation d‚Äôune release, scrutinizer qui fait des corrections, des previews sont r√©alisables avec netlify.\n\nEt si on √©tendait SQL avec du PHP\n\nAntoine BLUCHET nous a pr√©sent√© plus en d√©tail les projets Doctrine et principalement l‚ÄôORM. Cet outil est extensible, mais a quelques limites. Comment pouvons-nous faire des requ√™tes complexes avec Doctrine ? Peut-on utiliser des Common Table Expression ? La r√©ponse √† ces questions, propos√©e, est ESQL. Cet outil permet de construire des requ√™tes SQL complexes facilement sans se soucier des noms des tables ou des colonnes, car il permet d‚Äôutiliser ces m√©tadonn√©es depuis Doctrine.\n\nPourquoi vous n‚Äôattirerez et ne retiendrez pas les femmes dans vos √©quipes tech.\n\nMarcy Charollois fait un constat sur le monde du travail dans le num√©rique, domin√© largement par les hommes et ne laissant que trop peu de place aux femmes. Marcy commence par introduire la notion d‚Äôhabitus, qui d√©signe un syst√®me de pr√©f√©rences, de style de vie particulier √† chacun, qui influence les pratiques des individus au quotidien. Ces pratiques sont int√©rioris√©es inconsciemment, car l‚Äôindividu s‚Äôadapte et s&#39;int√®gre √† son environnement social. Cela cr√©e un groupe majoritaire qui devient d√©cisionnaire. Se met alors en place un statu quo qui va soit inclure ou exclure et qui est fortement domin√© par la pens√©e masculine. Marcy nous d√©voile que sur 100 % de freins ressentis par les femmes dans la tech, 30% proviennent des biais d&#39;oppression de groupe, une part donc assez cons√©quente.\n\nLe constat est r√©el, les femmes dans la tech vivent mal leur condition de femmes, il faut changer ce sentiment, mais les attitudes face au changement sont vari√©es. 15% de personnes sont r√©fractaires, il sera donc difficile de faire √©voluer les choses avec eux, 15% sont d√©j√† partantes et 70% sont neutres, potentiellement pour ce changement, mais ne savent pas comment le faire.\n\nMarcy nous donne alors des cl√©s qui permettront d‚Äôattirer les femmes dans nos √©quipes en mettant en avant les freins ressentis par celles-ci : une expression du genre, une l√©gitimit√© face au m√©tier exerc√©, des a priori sur la provenance des profils f√©minins qui sont souvent reconvertis et donc potentiellement juniors :\n\n\n  f√©miniser les postes et en particulier sur les offres d‚Äôemploi, une femme est d√©veloppeuse, pas d√©veloppeur.\n  mettre en avant les t√©moignages de femmes qui montent dans votre entreprise pour donner des exemples concrets de ce qu‚Äôelles pourraient trouver en venant chez vous\n  s‚Äôint√©resser r√©ellement √† elles et non pas de voir en vous ce qu‚Äôelles mettent en avant\n\n\nLa conf√©rence continue sur les actions √† mener pour garder les femmes dans nos √©quipes :\n\n\n  parler d‚Äô√©gal √† √©gal pour √©viter la posture sachant(e)/ignorante\n  soyez clair, transparent sur les salaires, les √©volutions de poste\n  mettez en place des moments conviviaux plus port√©s sur des pr√©f√©rences f√©minines\n  minimiser les interruptions pendant les prises de parole\n  √©coutez des besoins sp√©cifiques inh√©rents aux femmes et accommodez-les en offrant des ressources sans juger : parentalit√©, menstruation, assistance psychologique, adaptation √† l‚Äôemploi du temps\n  encouragez les femmes √† prendre la parole, √† devenir un r√¥le mod√®le parce que comp√©tente\n\n\nMarcy termine sa conf√©rence par la pr√©sentation de quelques chiffres sur l‚Äô√©volution de carri√®re des femmes et des enjeux psychosociaux r√©sultant\nde cette √©volution et conclut en montrant les bienfaits de l‚Äôinclusion des femmes au sein des entreprises et en nous donnant quelques noms de femmes c√©l√®bres dans ce combat.\n\nConclusion\n\nEncore une fois, l‚ÄôAFUP a r√©ussi √† faire un √©v√©nement chaleureux, int√©ressant et diversifi√©\n\nNous sommes ravis d‚Äôavoir pu participer √† cette manifestation qui nous a permis de rencontrer les membres de la communaut√© ainsi que de visiter rapidement la ville de Lille et manger des Welsh.\n\n\n"
} ,
  
  {
    "title"    : "You Build it, you run it",
    "category" : "",
    "tags"     : " conference, docker, meetup, devops",
    "url"      : "/2022/05/13/docker-meetup-you-build-it-you-run-it.html",
    "date"     : "May 13, 2022",
    "excerpt"  : "L‚Äôune des grandes √©tapes de l‚Äôautonomie d‚Äôune √©quipe de d√©veloppement dans la m√©thodologie DevOps est de s‚Äôint√©resser √† l‚Äôalerting li√© √† son infrastructure. Comment sommes-nous arriv√© √† proposer aux √©quipes de d√©veloppement de s‚Äôint√©resser et de m...",
  "content"  : "L‚Äôune des grandes √©tapes de l‚Äôautonomie d‚Äôune √©quipe de d√©veloppement dans la m√©thodologie DevOps est de s‚Äôint√©resser √† l‚Äôalerting li√© √† son infrastructure. Comment sommes-nous arriv√© √† proposer aux √©quipes de d√©veloppement de s‚Äôint√©resser et de ma√Ætriser cet alerting ?\n"
} ,
  
  {
    "title"    : "Retour sur l&#39;Android Makers 2022",
    "category" : "",
    "tags"     : " android, mobile, conference, makers",
    "url"      : "/2022/05/09/bedrock-android-makers-22.html",
    "date"     : "May 9, 2022",
    "excerpt"  : "Que c‚Äôest bon de se retrouver !\n\nApr√®s deux ans sans conf√©rence en pr√©sentiel, l‚ÄôAndroid Makers a fait son grand retour les 25 et 26 avril 2022, pour le plus grand bonheur de la communaut√© Android. \nL‚Äô√©quipe de d√©veloppeurs Android de Bedrock (don...",
  "content"  : "Que c‚Äôest bon de se retrouver !\n\nApr√®s deux ans sans conf√©rence en pr√©sentiel, l‚ÄôAndroid Makers a fait son grand retour les 25 et 26 avril 2022, pour le plus grand bonheur de la communaut√© Android. \nL‚Äô√©quipe de d√©veloppeurs Android de Bedrock (dont je fais partie) a partag√© ce bonheur en assistant √† ce rendez-vous incontournable. Jetpack Compose, accessibilit√©, optimisation de build et autres sont autant de sujets en maturation constante : essayons d‚Äôen faire le tour ensemble.\n\n\n\n\n  Que c‚Äôest bon de se retrouver !    \n      Penser         \n          Design System et Jetpack Compose \n          Accessibilit√© \n          Modularisation \n          Support de Chrome OS \n        \n      \n      D√©velopper         \n          Splashscreen Android 12 \n          Tester les coroutines \n        \n      \n      Optimiser         \n          Toujours plus de CI \n          Optimisation du temps de build \n        \n      \n      Extras         \n          Cr√©ation d‚Äôun UI Toolkit avec Romain Guy et Chet Haase \n        \n      \n      En conclusion \n    \n  \n\n\nPenser \n\nDesign System et Jetpack Compose \n\nPlusieurs conf√©rences ont √©voqu√© le sujet du Design et plus particuli√®rement l‚Äôimpl√©mentation d‚Äôun Design System (DS) avec Jetpack Compose. Fran√ßois Blavoet nous a partag√© l‚Äôexp√©rience d‚ÄôInstacart √† ce sujet, en d√©voilant quelques d√©tails d‚Äôimpl√©mentations de leurs API mises √† disposition des features engineers, afin de leur faciliter l‚Äôint√©gration des √©l√©ments du DS.\nParall√®lement, il nous a aussi invit√© √† r√©flechir sur la n√©cessit√© d‚Äôint√©grer Material Design dans notre impl√©mentation du DS. En effet, si le cadre Material peut parfois s‚Äôav√©rer utile, il est quelques fois trop contraignant et pas toujours adapt√© aux besoins sp√©cifiques de nos applications.\n\nAccessibilit√© \n\nVoil√† un sujet qu‚Äôil est important d‚Äô√©voquer, tant il est facile d‚Äôoublier d‚Äôadresser une application √† tous. Cette √©dition de l‚ÄôAndroid Makers a eu la chance d‚Äôaccueillir une tr√®s belle conf√©rence de Fanny Demey et Gerard Paligot sur le sujet de l‚Äôaccessibilit√©. Dans une s√©ance de Live Coding teint√©e d‚Äôun jeu de r√¥le sur le th√®me de l‚Äô√©mission C‚Äôest pas sorcier !, nous avons pu faire le tour de plusieurs points d‚Äôattention afin d‚Äôinclure au mieux nos utilisateurs porteurs de handicaps :\n\n  ne pas donner d‚Äôinformations inutiles via TalkBack, comme les contentDescription des ic√¥nes d√©coratives\n  penser √† la mani√®re dont TalkBack va assembler les informations provenant de plusieurs vues distinctes\n  donner un retour d‚Äôaction sur les clics de boutons et mieux placer ces actions lorsque le mode accessibilit√© est activ√©\n  et bien d‚Äôautres !\n\n\nLe Live Coding a pu √©galement d√©montrer √† quel point Jetpack Compose consid√®re l‚Äôaccessibilit√© comme une fonctionnalit√© cruciale gr√¢ce √† des APIs tr√®s compl√®tes (je pense ici √† Modifier.sementics par exemple).\n\nModularisation \n\nJean-Baptiste Vincey, d√©veloppeur chez Deezer, a partag√© l‚Äôexp√©rience de son √©quipe concernant la modularisation de leur code pour g√©rer le nombre grandissant d‚Äôapplications dans leur catalogue. Bas√© sur la cr√©ation de biblioth√®ques internes, plusieurs strat√©gies ont √©t√© explor√©es avec leurs bons et mauvais c√¥t√©s. Lanc√© dans un chantier similaire, il est important pour Bedrock de voir comment d‚Äôautres acteurs du milieu ont r√©pondu √† ces questions, sans oublier que chaque entreprise a sa propre r√©ponse qui doit s‚Äôadapter √† ses process, son organisation et son produit.\n\nSupport de Chrome OS \n\nFr√©d√©ric Torcheux et Pierre Issartel, lors de leur conf√©rence sur l‚Äôadaptation Chrome OS des applications Android, ont fait un constat int√©ressant : le nombre de Chromebook vendu a explos√© r√©cemment pour d√©passer le nombre de Mac vendu. Sachant qu‚Äôun nombre grandissant de Chromebook a acc√®s au Play Store, il est de plus en plus important d‚Äôadapter ses applications pour cet usage.\nEn vrac : exploiter le potentiel du curseur de la souris, naviguer dans l‚Äôapplication sans jamais quitter le clavier, supporter l‚Äôenvironnement multi-fen√™tr√© et le redimensionnement de celles-ci, autant de points d‚Äôam√©liorations comportant pi√®ges √† √©viter et bonnes pratiques.\n\nD√©velopper \n\nSplashscreen Android 12 \n\nDeux d√©veloppeurs de chez Google nous ont plong√© dans les entrailles du WindowManager d‚ÄôAndroid, ce composant qui est charg√© d‚Äôorchestrer les applications que nous utilisons tous les jours : charger une application, la placer √† l‚Äô√©cran puis la dessiner, la d√©placer et g√©rer son cycle de vie, autant de responsabilit√©s pour un WindowManager complexe √† maitriser.\n√Ä travers cette conf√©rence pointue, Vadim Caen et Pablo Gamito ont rebondi sur le nouveau syst√®me de SplashScreen d‚ÄôAndroid 12 pour nous expliquer quel probl√®me il doit r√©soudre (essentiellement le ressenti de lenteur au lancement d‚Äôune application) et comment en tirer parti. √Ä ce titre, la documentation de Google sur la migration vers le SplashScreen d‚ÄôAndroid 12 est incontournable.\n\nLe nouveau Splashscreen pour Android 12 comporte son lot de challenges, notamment pour tenir compte des animations. Chez Bedrock, les reflexions √† ce sujet ont d√©marr√©, et nous comptons partager un retour d‚Äôexp√©rience sur notre propre migration !\n\nTester les coroutines \n\nM√°rton Braun, aussi d√©veloppeur chez Google, a pr√©sent√© les nouveaut√©s de la biblioth√®que kotlinx-coroutines-test en version 1.6+. Exit runBlockingTest, place au runTest qui permet, gr√¢ce √† son TestCoroutineScheduler, de g√©rer les d√©lais et l‚Äôordre d‚Äôexecution de toutes les coroutines lanc√©es dans un test.\nL‚Äôancienne version des API de test √©tant maintenant d√©pr√©ci√©e, cette nouvelle version est encore experimentale mais vou√©e √† passer en √©tat stable, tant elle parait plus mature que la pr√©c√©dente.\nLes Flow et StateFlow n‚Äôont pas √©t√© oubli√©s puisqu‚Äôils ont aussi leurs sp√©cifit√©s en mati√®re de tests.\n\nOptimiser \n\nToujours plus de CI \n\nChez Bedrock, la CI tient une place particuli√®re dans nos process de release, et il est toujours int√©ressant de voir comment d‚Äôautres entreprises se saisissent de cet outil et am√©liorent leur process.\nApr√®s les rappels toujours pertinent sur l‚Äôimportance de d√©l√©guer le maximum de t√¢ches r√©p√©titives √† nos environnement de CI, Xavier F. Gouchet, d√©veloppeur chez Datadog, a pr√©sent√© divers outils pour y parvenir.\n\nDetekt, un plugin Gradle permet d‚Äôaller encore plus loin qu‚ÄôAndroid Lint en offrant l‚Äôanalyse statique de n‚Äôimporte quel code Kotlin. Son extensibilit√© nous est expos√©e via une API sur le pattern visiteur, redoutablement efficace pour parcourir le PSI (Program Structure Interface) de Kotlin. D‚Äôautres outils peuvent √©galement √™tre efficaces pour parcourir cette interface.\n\nXavier Gouchet pr√©sente √©galement KSP (Kotlin Symbol Processor), le projet sponsoris√© par Google vou√© √† remplacer KAPT, son anc√™tre bas√© sur Java. Combin√© avec Kotlin Poet, cet outil permet d‚Äôautomatiser la g√©n√©ration de code Kotlin √† partir d‚Äôun code source annot√© dans le projet.\n\nPour aider au d√©veloppement autour du PSI Kotlin, Xavier Gouchet recommande l‚Äôexcellent plugin PSIViewer pour IDE Jetbrains.\n\nOptimisation du temps de build \n\nZac Sweers est venu nous pr√©senter la mani√®re dont Slack, entreprise pour laquelle il travaille, optimise les builds Gradle. Les projets se complexifiant avec toujours plus de code et de modules, les temps de build ont tendance a augmenter.\n\nCette conf√©rence a mis en lumi√®re diverses optimisations pour tirer pleinement parti de Gradle et de ses nouvelles fonctionnalit√©s :\n\n  d√©sactiver les fonctionnalit√©s non utilis√©es du plugin Android\n  profiter du cache, y compris sur serveur\n  √©viter d‚Äôutiliser buildSrc pour factoriser du code\n  √©crire son propre plugin de convention gradle\n  avoir un compte Gradle Entreprises pour profiter des Gradle Build Scans afin de d√©terminer quels sont les points de friction du projet, que ce soit pour l‚Äôutilisation du cache, la parall√©lisation, l‚Äôinvalidation des builds, l‚Äôoptimisation des arguments de la JVM‚Ä¶\n  parfois m√™me, acheter du nouveau mat√©riel ! (Apple Silicon)\n\n\nR√©duire le temps de build est un enjeu constant, et participe au confort du d√©veloppeur au quotidien.\n\nExtras \n\nCr√©ation d‚Äôun UI Toolkit avec Romain Guy et Chet Haase \n\nUne conf√©rence tr√®s int√©ressante a vu Romain Guy et Chet Haase nous pr√©senter un projet experimental d‚ÄôUI Toolkit maison, Apex, tr√®s proche de Jetpack Compose dans son API. \nCet exercice original a √©t√© un moyen de faire valoir le concept d‚ÄôEntity component system, un pattern se basant sur la composition pour enrichir les comportements des entit√©s d‚Äôun syst√®me.\n\nLeur pr√©sentation a mis en lumi√®re la philosophie d‚Äôun UI Toolkit mais a aussi et surtout soulign√© la quantit√© de travail √† accomplir pour passer d‚Äôun projet experimental √† un toolkit utilisable en production. Enrichir sa boite √† outils avec le maximum de widgets diff√©rents, permettre une personnalisation maximale aux d√©veloppeurs, rendre le moteur de rendu multi-plateforme, autant de t√¢ches n√©cessaires pour rendre votre Toolkit vraiment utile pour la communaut√© aujourd‚Äôhui.\n\nEn conclusion \n\n\n\nCette √©dition de l‚ÄôAndroid Makers 2022 s‚Äôest conclue sur une conf√©rence humoristique in√©dite de la part de Chet Haase et Romain Guy. Il a √©t√© question de tourner en d√©rision la communaut√© des d√©veloppeurs sur le nombre de nouveaux patterns de d√©veloppement qui sortent r√©guli√®rement et les discussions acharn√©es (et parfois virulentes) autour de ces derniers.\nCela a √©t√© une tr√®s bonne mani√®re de prendre du recul sur notre communaut√© Android, et de se f√©liciter, tout de m√™me, de la recherche constante d‚Äôam√©lioration des pratiques de d√©veloppement au service de la qualit√© de nos produits et de leur accessibilit√©.\n"
} ,
  
  {
    "title"    : "How did we streamline the delivery of our internal conferences aka LFTs?",
    "category" : "",
    "tags"     : " lft, talks, live, stream, obs, streamyard, conference",
    "url"      : "/2022/04/29/how-we-simplify-our-lft-broadcast.html",
    "date"     : "April 29, 2022",
    "excerpt"  : "\n\nSome time ago, we shared with you an article explaining how we managed to capture and broadcast our conferences in the Bedrock auditorium. \nWe must admit, it worked great, but we wanted to make it simpler.\n\nAs a reminder, our internal conference...",
  "content"  : "\n\nSome time ago, we shared with you an article explaining how we managed to capture and broadcast our conferences in the Bedrock auditorium. \nWe must admit, it worked great, but we wanted to make it simpler.\n\nAs a reminder, our internal conferences or meetups are called LFT. \nIf you want to know more about our Last Friday Talk and the motivations that push us to do it, we invite you to read the above-mentioned article.\n\nAfter the success of this broadcasting, the (voluntary) organization team started thinking about how to make it simpler. \nIt was already a challenge in itself!\nIf you read this part of the article, you can see that we already had some ideas for improvements.\n\nImprovement areas\n\nIn the previous version, you could see that a very important quantity of material was necessary (meters of various cables, multi outlets, a camera, etc‚Ä¶)\nA large part of this material was kindly lent by Pascal (and we thank him for that), but we could not decently borrow it for each LFT.\n\nMoreover, this very specific equipment did not allow each Bedrock employee to manage the control room, quickly and with his or her own machine.\nFinally, as you can imagine, setting up and putting away such a large amount of equipment takes a lot of time. \nThere were no less than four of us setting up and tidying up.\n\nIn another topic, let‚Äôs talk about video quality. We used to use Google Live Stream before, but the 720p broadcast, with its very low bitrate and aggressive compression, sometimes made it difficult to follow.\nText and images were often very pixelated.\n\nAlso, we wanted to, easily, handle a hybrid mode to our LFTs.\nBecause of the pandemic, telecommuting and the fact that some of Bedrock‚Äôs employees are located in Paris, we need to broadcast and capture the LFT both in person and remotely.\nBy hybrid mode we also meant that during the same event, we need to host speakers from the location of their choice.\n\nWhat we did?\n\nAfter a few exchanges with other conference organizations, we decided to give Streamyard a chance.\nAfter some quick tests, we bought a license and started the new version of the LFT.\n\nWhat is Streamyard?\n\nStreamyard is a live streaming platform that runs directly in the browser.\n\nIt does not have all the customization and possibilities of OBS, but its huge advantage is its versatility.\nThis solution allows Bedrock employees to manage the LFT from any computer.\n\nHere are some sample images and overviews of the Streamyard UI:\n\n\n\n\n\nThe positive points of Streamyard:\n\n\n  Allows us to have several people in the control room at the same time\n  Streams in 1080p\n  Personalization: manage banners, chat questions, music on hold,‚Ä¶\n  Handles multi-speakers management\n  Re-stream to Youtube / Facebook / Twitter / Linkedin / Video recording\n  A free trial mode allows one to test it before taking out their credit card.\n\n\nNew setup\n\nAs a reminder, here is the organization of our auditorium during the live broadcast of our LFT.\nThe room is large enough to accommodate all Bedrockers who wish to attend in person the presentations of their colleagues. \nThe remote speakers have their conference broadcasted on the two screens of the room.\n\n\n\n\n\n\n\nWhat has changed since we switched to Streamyard is mainly related to the way we capture the image and sound of what is displayed on the screen.\nPreviously, we were using an Elgato HD60S+ device, to capture the HDMI output from the speaker broadcast. \nHowever, we had to try several times to get it to work each time we switched speakers. Not fun at all.\n\nNow, with Streamyard, every time we switch speaker, we just need to:\n\n  share the link of the ‚ÄúStreamyard broadcast‚Äù\n  The speaker joins the stream (and switches off her microphone and webcam)\n  Connect the HDMI cable of the speaker‚Äôs computer\n  The speaker displays her screen via the room‚Äôs projectors\n  The control room operator puts the RF microphone on the speaker(s)\n\n\nFor a remote speaker, it‚Äôs even easier.\nJust pass them the Streamyard link and they can use their webcam and their own microphone.\n\nThis is what our Stream setup now looks like with the cabling:\n\n\n\n\n\nWhat did we achieve?\n\nLFT is also a group of volunteers who give their energy to offer the best event possible, to offer to all Bedrock members a space where they can share their passion, technical subjects and others.\nFrom the proposal of their subject, through rehearsals and during the broadcast, the team is there to help speakers ‚Äì either beginners or confirmed.\n\nThe switch to 720p and then to 1080p has been a real positive point for the quality of the live show, but also for the recording and the replay.\nMore than 200 participants during the last LFT on the day, the organizing team is delighted.\n\nThe simplification of the broadcasting setup since the switch to Streamyard has also made it easier to set up the room.\nSwitching from one speaker to another is less complex and can be done in just a few minutes.\n\nThe youtube lives allow participants to pause and rewind the broadcast.\nThis is really convenient for them.\nAlso, by going through the youtube chat, we can share questions directly on the screen and on the replay.\n\nThe LFT replays are also available on Bedrock‚Äôs Youtube channel in a private way accessible to all employees.\n\n\n\nNext steps\n\nWe don‚Äôt want to stop there.\nFor the next editions, we will try to do even better.\nWe are working on the matter of sharing some talks in public on our Youtube channel, so more people in our communities can learn from them.\nIn order to simplify the setup, we will try to put in place a more fixed table to avoid wiring and moving furniture.\nWe also wish to propose and train our employees to the use of this setup in order to allow us to host meetups and conferences in the best conditions.\n\nNow, it‚Äôs your turn: if your company or user-group does this kind of talks, how do you manage broadcasting and recording?\n"
} ,
  
  {
    "title"    : "‚ö°Ô∏è Vite ‚ö°Ô∏è the Webpack killer",
    "category" : "",
    "tags"     : " conference, js, webpack, vite, devoxx",
    "url"      : "/2022/04/21/vite-the-webpack-killer.html",
    "date"     : "April 21, 2022",
    "excerpt"  : "Toute application web a besoin d‚Äô√™tre packag√©e afin d‚Äô√™tre livr√©e en production. Pour r√©pondre √† cette probl√©matique, de nombreux outils, connus sous le nom de modules bundler, sont apparus, et ces derni√®res ann√©es, c‚Äôest Webpack qui semble s‚Äô√™tre...",
  "content"  : "Toute application web a besoin d‚Äô√™tre packag√©e afin d‚Äô√™tre livr√©e en production. Pour r√©pondre √† cette probl√©matique, de nombreux outils, connus sous le nom de modules bundler, sont apparus, et ces derni√®res ann√©es, c‚Äôest Webpack qui semble s‚Äô√™tre impos√© comme l‚Äôoutil incontournable.\n\nOn ne va pas se le cacher, si vous avez mis les mains dans une configuration webpack, c‚Äôest loin d‚Äô√™tre un outil simple ni rapide.\n\n√áa n‚Äôa pas √©chapp√© √† Evan You, le cr√©ateur de Vue.JS, qui voulait r√©pondre √† ces probl√©matiques avec une nouvelle fa√ßon de proc√©der, avec des id√©es novatrices, reposant sur les derni√®res fonctionnalit√©s des navigateurs : Vite.\n\nQuelles sont ces id√©es novatrices √† la base de Vite ? En quoi concurrence-t-il Webpack ? C‚Äôest ce que nous allons voir dans ce talk et live coding!\n"
} ,
  
  {
    "title"    : "Bedrock √† l&#39;AWS Summit 2022",
    "category" : "",
    "tags"     : " aws, summit, cloud, sysadmin, conference, kubernetes",
    "url"      : "/2022/04/20/aws-summit-2022-notre-retour-dexperience.html",
    "date"     : "April 20, 2022",
    "excerpt"  : "\n\nRetour √† l‚ÄôAWS Summit \n\nDeux ann√©es se sont √©coul√©es depuis le dernier AWS Summit √† Paris, il a fait son retour ce 12 avril !\nCet √©v√®nement, qui a lieu au printemps dans plusieurs pays, est l‚Äôoccasion de rencontrer la communaut√© AWS fran√ßaise, d...",
  "content"  : "\n\nRetour √† l‚ÄôAWS Summit \n\nDeux ann√©es se sont √©coul√©es depuis le dernier AWS Summit √† Paris, il a fait son retour ce 12 avril !\nCet √©v√®nement, qui a lieu au printemps dans plusieurs pays, est l‚Äôoccasion de rencontrer la communaut√© AWS fran√ßaise, d‚Äôassister √† de nombreuses conf√©rences et de b√©n√©ficier de retours d‚Äôexp√©rience d‚Äôautres clients.\nC‚Äô√©tait aussi pour nous, comme en 2019, l‚Äôoccasion de partager les n√¥tres !\n\nDepuis notre migration vers le Cloud, AWS et Kubernetes entre 2018 et 2021 (plus d‚Äôinformations dans Le Plan Copenhague), nous sommes plusieurs centaines √† travailler au quotidien avec AWS.\nCette ann√©e, cinq de nos DevOps, SysOps et D√©veloppeurs ont eu la chance de se rendre √† l‚ÄôAWS Summit.\n\nNous partageons quelques notes que nous avons prises lors de cette journ√©e, sur des sujets qui nous ont marqu√©s et qui vont sans doute nous occuper une partie de l‚Äôann√©e √† venir.\n\nSommaire\n\n\n  Retour √† l‚ÄôAWS Summit\n    \n      Rendez vos √©quipes de Data Science 10x plus productives avec SageMaker\n      Comment le cloud permet √† France T√©l√©visions d‚Äôinnover dans la diffusion de contenu live\n      D√©couvrez comment Treezor utilise AWS comme moteur de sa plateforme de Banking-as-a-service\n      Tracer votre chemin vers le Modern DevOps en utilisant les services AWS d‚Äôapprentissage machine\n      Innover plus rapidement en choisissant le bon service de stockage dans le cloud\n      S√©curiser vos donn√©es et optimiser leurs co√ªts de stockage avec Amazon S3\n      Minimiser vos efforts pour d√©ployer et administrer vos cluster Kubernetes\n      Serverless et √©v√®nement, les nouvelles architectures\n    \n  \n  Nous √©tions aussi intervenants\n    \n      Transformer le load balancing pour optimiser le cache : objectif 50 millions d‚Äôutilisateurs\n      Etes-vous bien architectur√© ?\n      Pr√©parez et donnez votre premier talk\n    \n  \n  Conclusion de l‚Äôarticle\n\n\nRendez vos √©quipes de Data Science 10x plus productives avec SageMaker \n\nConf√©rence pr√©sent√©e par :\n\n\n  Olivier Sutter - AWS Solution Architect\n  Yoann Grondin - IA Team Leader Canal+\n\n\n\n\nCette conf√©rence pr√©sentait le produit Amazon SageMaker, d‚Äôabord dans sa globalit√©, puis appliqu√© au cas des √©quipes de Data Scientist chez Canal+.\n\nSageMaker a √©t√© lanc√© fin 2017 pour cr√©er, entra√Æner et d√©ployer des mod√®les de machine learning. C‚Äôest une solution tout-en-un, avec une interface graphique intuitive et un accent port√© sur l‚Äôautomatisation. Amazon promet √©galement de gagner en performance sur SageMaker via l‚Äôimpl√©mentation de nombreux algorithmes d‚Äôapprentissage supervis√©s ou non-supervis√©s (XGBoost, kNN, PCA‚Ä¶).\n\nDe mani√®re g√©n√©rale, les √©quipes de Canal+ utilisent des solutions d‚Äôapprentissage pour diff√©rents cas d‚Äôusages :\n\n\n  Personnaliser l‚Äôexp√©rience utilisateur et proposer du contenu cibl√©\n  Mieux conna√Ætre, labelliser, classifier leurs contenus vid√©o\n  Anticiper les besoins des abonn√©s ou des prospects\n\n\nIls se sont tourn√©s vers SageMaker pour diminuer le temps pass√© dans les √©tapes de preprocessing, data cleaning et d√©ploiement en production.\n\nChez Bedrock, nous avons aussi rencontr√© ces probl√©matiques, nous avons r√©alis√© des PoC de diff√©rentes solutions (dont SageMaker) et nous avons retenu la plateforme Databricks.\n\nEn effet, Databricks r√©pond √† nos besoins de fine-tuning des param√®tres des clusters Spark et d‚Äôint√©gration avec Terraform (ce qui est important pour nous car nous utilisons exclusivement de l‚ÄôInfra-as-Code). Nous avons √©galement automatis√© le d√©ploiement en production de nos mod√®les d‚Äôapprentissage, de la m√™me mani√®re que SageMaker.\n\nCette conf√©rence nous a confort√© dans l‚Äôapproche et l‚Äôutilisation que nous avons de nos outils actuels, tout en nous confrontant √† d‚Äôautres solutions techniques et d‚Äôautres cas d‚Äôusages au sein de notre industrie.\n\nR√©sum√© par Gabriel FORIEN - DevOps\n\nComment le cloud permet √† France T√©l√©visions d‚Äôinnover dans la diffusion de contenu live \n\nConf√©rence pr√©sent√©e par :\n\n\n  Rapha√´l Goldwaser - AWS Solution Architect\n  Guillaume Postaire - Directeur de la Media Factory, France T√©l√©visions\n  Matthieu Parmentier - Responsable de l‚ÄôAl factory, France T√©l√©visions\n  Nicolas Pierre - Al factory Lead Tech, France T√©l√©visions\n\n\n\n\nNous avons assist√© √† une conf√©rence pr√©sent√©e par les responsables M√©dia et AI de France T√©l√©visions et Rapha√´l Goldwaser Solutions Architect chez AWS. Ils nous ont parl√© de l‚Äô√©volution de leur usage du cloud dans la diffusion de vid√©o en direct et des diff√©rentes √©tapes de la construction d‚Äôun syst√®me de sous-titrage automatique en direct.\n\nPour ce syst√®me de sous-titrage, ils utilisent les services Media &amp;amp; Entertainment fournis par AWS.\n\nLes flux vid√©o sont envoy√©s directement dans le cloud via Elemental MediaConnect pour g√©n√©rer des sous-titres automatiquement en utilisant Media-Cloud AI et Speechmatics. Une fois les fichiers de sous-titres g√©n√©r√©s, ils sont ins√©r√©s et synchronis√©s sur le flux en direct.\n\nCes outils peuvent √©galement √™tre utilis√©s pour analyser des vid√©os afin de contextualiser les publicit√©s affich√©es et/ou choisir le meilleur moment pour les afficher.\n\nChez Bedrock comme chez France T√©l√©visions, nous challengeons r√©guli√®rement les solutions M√©dias propos√©es par AWS pour am√©liorer nos infrastructures et apporter de nouvelles fonctionnalit√©s √† nos produits.\n\nR√©sum√© par Christian VAN DER ZWAARD - SysOps\n\nD√©couvrez comment Treezor utilise AWS comme moteur de sa plateforme de Banking-as-a-service \n\nConf√©rence pr√©sent√©e par :\n\n\n  Armel Negret - AWS Central Sales Representative\n  Nicolas Bordes - Technical Lead and AWS Sponsor, Soci√©t√© G√©n√©rale\n\n\nTreezor, une filiale du groupe Soci√©t√© G√©n√©rale, fournit une plateforme compl√®tement APIs√©e qui permet aux fintechs et plus g√©n√©ralement aux acteurs de la finance d‚Äôacc√©der √† leurs services bancaires. Cette plateforme est h√©berg√©e sur AWS et utilise une stack de services enti√®rement serverless : API Gateway, CloudWatch, Lambda, SNS et SQS entre autres. Les Lambdas sont d√©velopp√©es en PHP gr√¢ce au framework Bref.\n\nA l‚Äôinstar de Treezor, Bedrock poss√®de √©galement de nombreuses Lambda d√©velopp√©es en PHP avec Bref. Ces Lambdas sont majoritairement d√©ploy√©es via le framework serverless et maintenues par le p√¥le backend. Au p√¥le infrastructure, nous essayons d‚Äôutiliser d‚Äôautres langages comme Python ou Go avec lesquels nous sommes plus √† l‚Äôaise et qui sont nativement support√©s par Lambda.\n\nL‚Äôapproche ‚Äúfull serverless‚Äù est int√©ressante car elle permet de s‚Äôabstraire de la gestion de l‚Äôinfrastructure sous-jacente et donc de se concentrer sur des probl√©matiques intrins√®ques au m√©tier.\nEn sus, les services AWS serverless apportent souvent nativement de la haute disponibilit√© ainsi que de l‚Äôauto-scaling, deux probl√©matiques tr√®s importantes pour garantir un service de qualit√© √† nos utilisateurs finaux. C‚Äôest pour ces raisons que Bedrock utilise de nombreux services AWS serverless : Athena, CloudWatch, DynamoDB, Lambda, S3, SNS, SQS, Kinesis, ‚Ä¶\nLe p√¥le infrastructure de Bedrock √©tant relativement ‚Äúpetit‚Äù par rapport au nombre total de d√©veloppeurs (23 devops/sysops pour 250 fullstack en date du 15 avril 2022), l‚Äôutilisation du serverless est un r√©el enjeu business.\nServerless ne r√©pond pas √† tous les besoins non plus, particuli√®rement sur de tr√®s forts pics de charge o√π nous pr√©f√©rons utiliser Kubernetes.\n\nR√©sum√© par Timoth√©e AUFORT - DevOps\n\nTracer votre chemin vers le Modern DevOps en utilisant les services AWS d‚Äôapprentissage machine \n\nConf√©rence pr√©sent√©e par :\n\n\n  Patrick Lampl√© - AWS Principal Specialist SA\n\n\nLa conf√©rence nous proposait d‚Äôen apprendre un peu plus sur les nouveaux produits AWS Code Guru et DevOps Guru.\n\nCode Guru\nse d√©coupe en deux parties :\n\n\n  Reviewer, qui a pour ambition d‚Äôacc√©l√©rer la revue de code ;\n  Profiler, qui peut aider √† optimiser les performances d‚Äôune application.\nA ce jour, ces services ne supportent que les langages Python et Java.\n\n\nDevOps Guru\npermet d‚Äôidentifier les comportements anormaux des applications au runtime.\nPar exemple, si une application utilise une table DynamoDB qui n‚Äôest pas suffisamment provisionn√©e, une alerte va √™tre d√©clench√©e. Cette derni√®re pourrait permettre d‚Äôidentifier un souci de configuration avant m√™me que l‚Äôapplication ne soit d√©ploy√©e en production.\n\nChez Bedrock, les langages utilis√©s √©tant principalement Javascript, PHP et Python, Code Guru ne sera pas une solution ad√©quate dans toutes les situations. Nous avons donc mis en place la solution KICS qui nous permet, √† l‚Äôaide de r√®gles Open Policy Agent, d‚Äôeffectuer automatiquement de nombreuses validations sur le code infrastructure (Terraform, Docker, YAML, ‚Ä¶). KICS est utilis√© au travers de GitHub Actions pour ajouter des commentaires sur les pull requests comme Code Guru est capable de le faire.\n\nUne analyse au runtime effectu√©e par DevOps Guru pourrait permettre de venir compl√©ter la liste de services AWS que nous utilisons d√©j√† et qui v√©rifient la configuration de notre infrastructure comme : Config, Trusted Advisor, CloudWatch,  ‚Ä¶\n\nLes outils du Modern DevOps d‚ÄôAWS pourraient venir en compl√©ment d‚Äôoutils de qualit√© actuellement utilis√©s chez nous. √Ä tester en compl√©ment de KICS pendant une de nos journ√©es R&amp;amp;D (journ√©es organis√©es le dernier vendredi du mois, un mois sur deux).\n\nR√©sum√© par Valentin CHABRIER &amp;amp; Micka√´l VILLERS - DevOps\n\nInnover plus rapidement en choisissant le bon service de stockage dans le cloud \n\nConf√©rence pr√©sent√©e par :\n\n\n  Thomas Barandon - AWS Enterprise Support Manager\n  Laurent Dirson - Directeur des Solutions Business et des Technologies, Nexity\n\n\nThomas Barandon a rappel√© les solutions de stockage d‚ÄôAWS : S3 pour du stockage objet, EBS pour le stockage bloc et EFS/FSx pour le stockage fichier.\nIl a par la suite pr√©sent√© Storage Gateway, qui permet d‚Äôutiliser les services de stockage AWS dans une infrastructure on-prem via un montage NFS/Samba ou iSCSI.\n\nLaurent Dirson de chez Nexity a ensuite partag√© la strat√©gie adopt√©e pour concevoir leur SI comme un service. Tous leurs documents sont d√©sormais stock√©s dans un bucket S3 mis √† disposition des agences via un montage NFS op√©r√© par l‚Äôoutil Storage Gateway. Une politique d‚ÄôObject Lock permet d‚Äôutiliser le mod√®le WORM (write-once-read-many).\n\nChez Bedrock, nous stockons d√©j√† la grande majorit√© de nos donn√©es dans des buckets S3. Nous aimerions aussi b√©n√©ficier des avantages de ce service pour le stockage de nos m√©triques. Mais, comme nous utilisons VictoriaMetrics, ce mode de stockage n‚Äôest pas disponible et ces donn√©es sont stock√©es dans EBS. Peut-√™tre que Storage Gateway nous permettrait d‚Äô√©crire nos m√©triques directement sur un bucket S3 ?\n\nR√©sum√© par Coraline PETIT - SysOps\n\nS√©curiser vos donn√©es et optimiser leurs co√ªts de stockage avec Amazon S3 \n\nConf√©rence pr√©sent√©e par :\n\n\n  Meriem Belhadj - AWS Storage Specialist Solutions Architect\n\n\nPendant cette pr√©sentation, Meriem Belhadj est revenue sur les classes de stockage disponibles sur S3, en mettant une attention particuli√®re √† Glacier Instant Retrieval et √† Intelligent-Tiering. Le second permet d‚Äôappliquer une politique de stockage bas√©e sur la fr√©quence d‚Äôacc√®s aux donn√©es au cours des 30 derniers jours.\nEn effet, pour d√©terminer la ‚Äúbonne‚Äù classe √† utiliser, il faut notamment conna√Ætre la disponibilit√© des donn√©es. Les autres points √† prendre en compte sont la fr√©quence d‚Äôacc√®s, les performances recherch√©es, la taille des objets √† stocker et enfin la dur√©e de r√©tention.\n\nNous appliquons ces pratiques chez Bedrock depuis plusieurs ann√©es. Toutefois, il serait judicieux de mettre en place des r√®gles, type AWS Config, pour s‚Äôassurer que ces recommandations soient bien appliqu√©es sur tous nos buckets S3.\n\nR√©sum√© par Coraline PETIT - SysOps\n\nMinimiser vos efforts pour d√©ployer et administrer vos cluster Kubernetes \n\nConf√©rence pr√©sent√©e par :\n\n\n  Abass Safouatou - AWS Lead Solution Architect\n  S√©bastien Allamand - AWS Solution Architect Specialist Container\n  Patrick Chatain - CTO Contentsquare\n\n\nContentsquare, analyste de l‚Äôexp√©rience num√©rique, est venu nous parler de son utilisation d‚ÄôEKS Blueprint avec AWS CDK (Cloud Development Kit) pour la configuration et le d√©ploiement de leurs infrastructures Kubernetes. Cet outil leur a permis de rapidement migrer leurs infrastructures dans le Cloud.\n\n√Ä l‚Äôoccasion de cette conf√©rence, un d√©but de comparatif a √©t√© amorc√© entre les solutions de passage √† l‚Äô√©chelle automatique : Cluster Autoscaler et Karpenter.\n\nCette analyse a particuli√®rement attir√© notre attention : nous souhaitons migrer nos clusters Kubernetes, actuellement d√©ploy√© par Kops vers des clusters EKS, pour gagner en maintenabilit√© et en rapidit√© de scaling. Karpenter est l‚Äôune des solutions que nous √©tudions dans le cadre de ce projet, afin de tirer partie de cet outil qui a √©t√© d√©velopp√© par AWS et qui semble mieux tirer profit des fonctionnalit√©s sp√©cifiques d‚ÄôAWS que Cluster Autoscaler.\n\nContentsquare a mentionn√© son besoin de d√©velopper un outil de passage √† l‚Äô√©chelle bas√© non pas sur la consommation CPU et m√©moire mais sur des m√©triques custom. C‚Äôest un besoin que nous avons √©galement chez Bedrock, nous avons donc d√©velopp√© un outil pour y r√©pondre, vous trouverez plus de d√©tails dans l‚Äôarticle de blog d√©di√© √† cet outil. Cette remarque nous a confort√© dans notre volont√© de continuer √† open-sourcer les outils que nous d√©veloppons, pour qu‚Äôils b√©n√©ficient √† la communaut√©.\n\nR√©sum√© par Coraline PETIT &amp;amp; Christian VAN DER ZWAARD - SysOps\n\nServerless et √©v√®nement, les nouvelles architectures \n\nMainframe, monolithe, syst√®me distribu√©, microservices, la conception d‚Äôun SI ou d‚Äôun projet est en constante √©volution.\n\nCeci dit, depuis plusieurs ann√©es, beaucoup d‚Äôentreprises font la transition sur des architectures orient√©es √©v√®nements pour limiter les couplages forts entre les microservices.\n√Ä cela s‚Äôajoute l‚Äôessor du tout Serverless : fini le temps o√π on g√©rait nous-m√™mes le dimensionnement de nos serveurs.\n\nCette ann√©e, plusieurs conf√©rences √©taient consacr√©es √† ces sujets.\n\n3 designs patterns pour bien d√©marrer avec Serverless\n\nMatthieu Napoli, Hero AWS Serverless et cr√©ateur de la librairie Bref, a pr√©sent√© trois designs patterns pour bien d√©marrer avec Serverless.\n\nApplication HTTP\n\nIl est maintenant tr√®s simple et peu co√ªteux de cr√©er des applications HTTP en Serverless, en combinant diff√©rents services AWS :\n\n\n  Lambda function et Lambda function URL (nouvelle fonctionnalit√© sortie une semaine avant le Summit) ;\n  CloudFront CDN ;\n  API Gateway ;\n  S3.\n\n\nUn exemple concret :\n\n\n  CloudFront CDN d√©livre les assets JS/CSS/image depuis S3 ;\n  il transmet √©galement les retours d‚ÄôAPI Gateway ;\n  et API Gateway communique avec la/les Lambdas.\n\n\nAjoutons une base de donn√©es DynamoDB ou Aurora et nous voil√† avec une application full Serverless.\n\nFile de messages avec worker\n\nLorsqu‚Äôon met en place ce type de pattern, nous d√©ployons :\n\n\n  un projet qui pousse un message dans une file ‚Äúproducer‚Äù ;\n  une file de messages (SQS/SNS/MQ) ;\n  un second projet qui lit les messages depuis la file ‚Äúconsumer‚Äù.\n\n\nDans cette architecture, le ‚Äúconsumer‚Äù se connecte √† la file de messages, lit les messages et se charge de toute la gestion d‚Äôerreur et de retry.\n\nEn utilisant une Lambda comme ‚Äúconsumer‚Äù, AWS a mis en place une int√©gration sp√©cifique entre les files de messages et les Lambdas. C‚Äôest maintenant la file de messages qui appelle directement la Lambda en lui donnant le message et qui g√®re √©galement le retry : votre code applicatif est d√©charg√© d‚Äôautant de responsabilit√©s sans valeur m√©tier.\n\nCommunication entre microservices\n\nQuoi de plus contraignant que de g√©rer la communication de plusieurs services ? Il faut g√©rer :\n\n\n  les erreurs : que faire si plusieurs microservices partent en timeout ou √©chouent dans un workflow ?\n  le couplage : lors de la cr√©ation d‚Äôun nouveau microservice, il doit √™tre lui aussi appel√© dans les chaines d‚Äôappels ;\n  l‚Äôauthentification entre les diff√©rents services ;\n  et la latence : les appels de services en cascade augmentent la dur√©e totale d‚Äôex√©cution.\n\n\nDe ce constat, Matthieu propose une solution que nous avons d√©j√† mise en place chez Bedrock depuis plusieurs ann√©es : communiquer avec des √©v√®nements.\nPour cela, AWS fournit EventBridge : un service serverless de routage d‚Äô√©v√®nements sans stockage.\nAinsi, si un microservice doit en informer d‚Äôautres, il lui suffit d‚Äôenvoyer un √©v√©nement dans EventBridge. Les autres services n‚Äôauront qu‚Äô√† ‚Äú√©couter‚Äù l‚Äô√©v√©nement.\nSNS, plus ancien, permettait la m√™me approche, mais EventBridge propose de cr√©er des r√®gles de filtrage sur la totalit√© du message d‚Äôun √©v√©nement.\n\nConstruire des applications serverless orient√©es √©v√©nements\n\nNicolas Moutschen, Solution Architect AWS et Guillaume Lannebere de chez Betclic ont fait un retour d‚Äôexp√©rience sur la mise en place de diff√©rents services serverless AWS orient√©s √©v√©nements.\n\nBetclic absorbe √† chaque match/course, une quantit√© √©norme de donn√©es (plusieurs millions d‚Äô√©v√©nements) en quelques minutes.\nPar exemple, lors d‚Äôun match de football, les paris sont effectu√©s √† tout instant : avant le match, √† la mi-temps, dans les derni√®res minutes‚Ä¶\nLeur SI est donc soumis, fr√©quemment, √† de tr√®s forts pics de charge pendant des laps de temps tr√®s courts.\n\nAfin d‚Äô√©viter de provisionner √©norm√©ment de machines pour se mettre √† l‚Äô√©chelle, Betclic √† fait le choix du full serverless. Les applications de paris et de paiement communiquent par des messages d‚Äô√©v√©nements envoy√©s dans le service AWS SNS : les Lambdas re√ßoivent les messages et les traitent avec une mise √† l‚Äô√©chelle quasi imm√©diate en fonction du trafic.\n\nR√©sum√© par Fabien LALANNE - D√©veloppeur\n\nNous √©tions aussi intervenants \n\nNous aimons tout particuli√®rement apprendre en lisant des articles √©crits par d‚Äôautres membres de notre communaut√© ou en assistant √† des conf√©rences pr√©sent√©es par d‚Äôautres clients. Il est donc normal et important pour nous, de partager aussi notre exp√©rience, ce que nous faisons r√©guli√®rement, y compris sur ce blog.\n\nCette ann√©e, nous avons eu la chance d‚Äôintervenir et de partager avec notre communaut√© lors de trois conf√©rences. Merci √† AWS pour la confiance qui nous a √©t√© accord√©e !\n\nTransformer le load balancing pour optimiser le cache : objectif 50 millions d‚Äôutilisateurs \n\nVincent Gallissot @vgallissot, Lead Cloud Architect, a expliqu√© comment Bedrock a am√©lior√© le Load Balancing chez AWS, pour optimiser le cache de sa diffusion de vid√©os, avec comme objectif 50 million d‚Äôutilisateurs :\n\nD√©marrage des conf√©rences √† l‚Äô#AWSSummit. Et voici un REX int√©ressant pour partager l‚Äôune des probl√©matiques int√©ressantes pic.twitter.com/kuKGZZyE2A&amp;mdash; Akram BLOUZA (@akram_Blouza) April 12, 2022\n\n\nGuillaume Marchand, Senior Solutions Architect chez AWS a d√©but√© notre talk en parlant de Load Balancing chez AWS, des diff√©rentes solutions et des bonnes pratiques, ainsi que des exemples d‚Äôarchitectures possibles. J‚Äôai ensuite expliqu√© notre besoin de scaler des serveurs de cache et comment nous avons relev√© ce challenge, en d√©veloppant notamment Haproxy Service Discovery Orchestrator. Ce talk n‚Äôa pas √©t√© enregistr√©, mais les slides sont disponibles sur ce lien.\n\nEtes-vous bien architectur√© ? \n\nPascal Martin @pascal_martin, Principal Engineer, est intervenu pour partager notre retour d‚Äôexp√©rience client pendant une conf√©rence de pr√©sentation du Well-Architected Framework :\n\nPour cette conf√©rence, R√©mi Retureau, Partner Management SA Lead chez AWS, a commenc√© par pr√©senter les pratiques Well-Architected. Un ensemble de recommandations bas√©es sur 10 ans d‚Äôexpertise de Solutions Architects AWS.\nJe suis ensuite intervenu pour partager un retour d‚Äôexp√©rience : comment nous utilisons Well-Architected Framework chez Bedrock pour nous aider √† valider l‚Äôarchitecture de composants de notre plateforme, √† prioriser des √©volutions ou m√™me, √† en identifier de nouvelles.\nEn quelques mots : nous passons une revue Well-Architected une fois par an et, si nous ne nous posons pas explicitement l‚Äôensemble des questions du Framework √† chaque nouveau projet, nous l‚Äôint√©grons de plus en plus √† nos pratiques et habitudes.\nSi vous commencez √† travailler sur AWS, le Well-Architected Framework et ses recommandations, bien que peut-√™tre effrayantes au premier abord, sont un ensemble de bonnes pratiques qui vous aideront √† concevoir et √† construire une plateforme plus solide, plus r√©siliente et moins co√ªteuse.\n\nPr√©parez et donnez votre premier talk \n\nPascal est aussi intervenu, cette fois en tant qu‚ÄôAWS Hero pour guider la pr√©paration de vos talks :\n\nPour cette seconde intervention, j‚Äôai choisi de parler d‚Äôun sujet qui n‚Äôest pas li√© √† AWS.\nJ‚Äôaime assister √† des conf√©rences : je le fais depuis tr√®s longtemps et j‚Äôapprends beaucoup ainsi.\nJe suis aussi toujours tr√®s content de voir d‚Äôautres speakers monter sur sc√®ne et partager leur exp√©rience. Je sais que beaucoup de personnes, dans notre communaut√©, ont des connaissances et des id√©es g√©niales et j‚Äôaimerais qu‚Äôelles les partagent plus souvent !\n\nJe sais toutefois que cet exercice est effrayant et que se lancer sur sc√®ne pour la premi√®re fois est difficile. J‚Äôesp√©rais donc, √† travers ce talk d√©j√† donn√© chez Bedrock lors d‚Äôun Last Friday Talks (une journ√©e de conf√©rences internes le dernier vendredi du mois, un mois sur deux) aider de nouvelles personnes √† se lancer.\nL‚Äôid√©e vous int√©resse mais vous n‚Äôavez pas pu assister √† cette conf√©rence ? Et bien, j‚Äôai aussi √©crit un livre pour vous accompagner : ¬´ Pr√©parez et donnez votre premi√®re conf√©rence (quand ce n‚Äôest pas votre m√©tier) ¬ª\nEt j‚Äôai h√¢te, l‚Äôann√©e prochaine, de vous voir monter sur sc√®ne et partager avec notre communaut√© !\n\nConclusion de l‚Äôarticle \n\nAvec des milliers de participants et participantes, l‚ÄôAWS Summit est toujours une excellente occasion d‚Äô√©changer et d‚Äôapprendre. Nous √©tions √©galement tr√®s heureux de pouvoir, cette ann√©e encore, partager notre exp√©rience lors de trois interventions.\nCet √©v√©nement √©tait aussi le premier pour certains et certaines d‚Äôentre nous, une tr√®s bonne d√©couverte !\n\nComme beaucoup d‚Äôautres speakers et entreprises rencontr√©s mardi, nous recrutons : des SysOps, des DevOps, des d√©veloppeurs et des d√©veloppeuses, une ou un FinOps. Vous voulez nous aider √† construire et √† faire grandir notre plateforme ? Nous avons encore de super projets et challenges, faites-nous signe !\n"
} ,
  
  {
    "title"    : "Comment faire un trailer vid√©o qui d√©chire avec des technos web ?",
    "category" : "",
    "tags"     : " remotion, react, video, js, frontend, conference, lyonjs",
    "url"      : "/2022/04/04/comment-faire-un-trailer-qui-dechire-avec-des-technos-web.html",
    "date"     : "April 4, 2022",
    "excerpt"  : "Avec Antoine Caron on est all√© mettre des paillettes dans les yeux des participants du LyonJS en leur montrant comment cr√©er des vid√©os avec des technos web ! ‚ú®\n\nIl √©tait une fois ‚Ä¶ üìñ\n\nUn jour, alors que j‚Äôarrivais fraichement √† Bedrock, j‚Äôai eu l...",
  "content"  : "Avec Antoine Caron on est all√© mettre des paillettes dans les yeux des participants du LyonJS en leur montrant comment cr√©er des vid√©os avec des technos web ! ‚ú®\n\nIl √©tait une fois ‚Ä¶ üìñ\n\nUn jour, alors que j‚Äôarrivais fraichement √† Bedrock, j‚Äôai eu le malheur de demander √† Antoine Caron ce sur quoi il bossait entre midi et deux et qui semblait fort l‚Äôamuser.\n\nSa r√©ponse : ‚ÄúJ‚Äôessaie de g√©n√©rer des vid√©os en MP4 √† partir de composants React, tu veux voir ?‚Äù\n\nApr√®s des heures √† tester chaque fonctionnalit√© de Remotion, il √©tait temps de pr√©senter √ßa √† la communaut√© Javascript de Lyon lors du meetup n¬∞71 du Lyon JS ! ü¶Å\n\n\n  \n\n\n\nPas le temps de regarder le replay ? ‚è±\n\nPour vous donner une petite id√©e de ce que l‚Äôon a fait, on vous partage un site qui g√©n√®re dynamiquement des trailers vid√©o en fonction d‚Äôun programme et d‚Äôune couleur ! ü§Ø\n\n\n  \n\n\n\n‚ÑπÔ∏è On vous conseille quand m√™me de regarder le replay, m√™me le cr√©ateur de Remotion a aim√© üòâ\n\n\n  A demo of Remotion in French at @LyonJS!Thanks for organizing this awesome talk @Slashgear_ @CruuzAzul üòÉhttps://t.co/xujfC7tR6e&amp;mdash; Remotion (@remotion_dev) April 3, 2022 \n\n\n\nOne more thing‚Ä¶ Petite surprise du chef ! üë®üèª‚Äçüç≥\n\nVoil√† un petit aper√ßu d‚Äôune vid√©o surprise que l‚Äôon a fait gr√¢ce √† Remotion uniquement avec des composants React ! (Si des gens sont n√©s avant 2000, √ßa doit vous rappeler quelque chose üòâ)\n\n\n  \n\n\n\nVous trouvez √ßa incroyable et vous voulez essayer ? N‚Äôh√©sitez pas √† venir nous montrer vos vid√©os, ou directement sur twitter avec @Slashgear_ et @CruuzAzul üéû\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #16",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/03/26/bedrock-dev-facts-16.html",
    "date"     : "March 26, 2022",
    "excerpt"  : "Sur ce d√©but 2022 les √©quipes de Bedrock se sont lach√©es √ßa promet pour le reste de l‚Äôann√©e.\nLes autres articles de cette s√©rie sont disponibles ici.\n\nPromis !\n\n\n\nL‚Äôarbre qui cache la for√™t\n\n\n  Quand c‚Äôest la couleur des noms de fichiers qui te pe...",
  "content"  : "Sur ce d√©but 2022 les √©quipes de Bedrock se sont lach√©es √ßa promet pour le reste de l‚Äôann√©e.\nLes autres articles de cette s√©rie sont disponibles ici.\n\nPromis !\n\n\n\nL‚Äôarbre qui cache la for√™t\n\n\n  Quand c‚Äôest la couleur des noms de fichiers qui te permet de te rep√©rer dans l‚Äôarborescence du projet‚Ä¶\n\n\n\n\nLa boucle a boucl√©\n\nLe d√©bat est et sera √©ternel.\n\n\n  A: La meilleure extension de VSCode c‚Äôest d‚Äôinstaller Webstorm.\n\n  B: Et la meilleure extension de Webstorm c‚Äôest d‚Äôinstaller Phpstorm.\n\n  C: Et la meilleure extension de Phpstorm c‚Äôest d‚Äôinstaller vim.\n\n  D: La meilleure extension de Vim c‚Äôest d‚Äôinstaller Neovim.\n\n  E: La meilleure extension de neovim est le bloc-note.\n\n  F: Mon bloc-note c‚Äôest VSCode.\n\n\nQuand on va √† la p√™che √† l‚Äôexception\n\nException message (1) :\n------------------------\nAttempted to load class &quot;TruiteException&quot; from namespace &quot;Bedrock\\Stores\\Infra\\HttpClient&quot;.\nDid you forget a &quot;use&quot; statement for another namespace?\n\n\n‚ÄúCe n‚Äôest pas un √©chec‚Ä¶ √ßa n‚Äôa pas march√©‚Äù\n\n\n  Y‚Äôa pas de bug, y‚Äôa juste un truc qui fonctionne pas\n\n\nLes bottleneck c‚Äôest la vie\n\n\n  La vie est un bottleneck\n\n\nAtlas, le g√©ant ?\n\n\n  Qui c‚Äôest qui a d√©plac√© la terre ?\n\n\nTranquilou bilou\n\n\n  Le pair programming n‚Äôest pas du programming p√©p√®re\n\n\nTester c‚Äôest ‚Ä¶\n\n\n  A: Faire des tests, c‚Äôest quand m√™me moins chiant √† faire √† deux\n\n  B: Ouais, comme le sexe\n\n\nLabyrinthe\n\n\n  Pourquoi vous vous faites chier √† mettre les secrets dans SOPS. \nVous l‚Äôauriez mis dans votre doc, personne ne les aurait trouv√©.\n\n\nVoil√† qui voil√† !\n\n\n  A: Je crois que la branche master elle est pas prot√©g√©e.\n\n  ‚Ä¶\n\n  Ah bah heureusement que je suis l√† pour tester\n\n  B: Le plus √©tonnant c‚Äôest que ‚ÄòA‚Äô ait le droit de push non ?\n\n  A: Le plus √©tonnant c‚Äôest que j‚Äôai quelque chose √† push non ?\n\n  ‚Ä¶\n\n  On a r√©solu une faille de s√©curit√© sur le projet, gr√¢ce √† mon investigation.\n\n  C: Le mec c‚Äôest inspecteur gadget en fait, on sait pas comment mais il finit par √™tre utile.\n\n\nüïØ\n\n\n  The Domain is a sanctuary\n\n\nQuand il y a trop de viennoiserie chez Bedrock\n\n\n  A: Fait gaffes tu es en ordre croissant, l‚Äôordre pain au chocolat est bien meilleur\n\n\nMais oui c‚Äôest clair !\n\n\n  On complexifie pour faire plus simple !\n\n\nLa PRO-crastination\n\n\n  Lead: C‚Äô√©tait qui le charg√© du monitoring hier ?\n\n  A: C‚Äô√©tait moi mais j‚Äôai rien fait, mais si vous voulez je peux le refaire aujourd‚Äôhui.\n\n  B: Si tu veux je peux m√™me t‚Äôaider √† rien refaire.\n\n\n√áa sent un peu la poussi√®re ici\n\n\n  A: Eh eh, l√† on a effectivement sur-gonfl√© l‚Äôestim ‚Ä¶ (mais le c√¥t√© pas dans l‚Äôinconnu l‚Äôexplique)\n\n  B: Ouais c‚Äô√©tait carr√©ment la peur de toucher √† un projet legacy de chez legacy, qu‚Äôon ne connait pas, pas dans notre scope‚Ä¶\n\n  A: Mais pas de souci majeur pour un d√©veloppeur qui pratiquait Symfony sous Pompidou (le code date de cette √©poque)\n\n\nUn nouveau mot\n\n\n  Hybriditance\n\n\nDouble hit combo\n\n\n  ‚ÄúI want to identify pages where users hit‚Äù, le developper traduit alors ‚ÄúJe veux identifier la page o√π l‚Äôutilisateur se frappe‚Äù\n\n\nUne histoire de dev enrhum√©\n\n\n\nQui s‚Äôappeleriot le CPU\n\n\n  Il fait des soustractions, des additions‚Ä¶ il fait des trucs de fous\n\n\nUn nouveau KPI\n\n\n  Alors mon piffom√®tre du sprint‚Ä¶\n\n\nC‚Äôest simple non ?\n\n\n  I talked to A that said that B said that C said that it is possible to parallelize steps on Jenkins.\n\n  So I asked B how to do that, saying that A said that he said that C said that it was possible to do it!\n\n"
} ,
  
  {
    "title"    : "How AWS Cloudfront is helping us deliver our Web streaming platform? - Part¬†1",
    "category" : "",
    "tags"     : " cloudfront, aws, cdn, node.js, react, javascript, frontend",
    "url"      : "/2022/03/08/cloudfront-web-streaming-platform-part-1.html",
    "date"     : "March 8, 2022",
    "excerpt"  : "A bit of context\n\nThe web is a major platform for the distribution of our customers‚Äô content at Bedrock.\nMillions of users connect every month to watch their live, replay or the series and movies of their choice.\nThe broadcasting of sports events ...",
  "content"  : "A bit of context\n\nThe web is a major platform for the distribution of our customers‚Äô content at Bedrock.\nMillions of users connect every month to watch their live, replay or the series and movies of their choice.\nThe broadcasting of sports events such as the Euro 2020 soccer tournament represents a real technical challenge when it comes to maintaining the stability and performance of such a platform.\n\nThe web application works in SSR (Server Side Rendering) mode: we have NodeJS Express servers returning pre-rendered HTML pages.\nWe made this choice several years ago, for two reasons: SEO and to improve the first display time on slow devices.\nIn addition to the HTML pages, the web platform is also a huge collection of assets that allow the website to function: Javascript bundles, CSS, images, manifests.\n\nToday our customers have users distributed over a large part of the globe.\n\nTo meet these challenges, a CDN is a very good solution.\n\nWhat is a CDN then ?\n\nCDN for Content Delivery Network is a service delivering content to users across the internet (here our HTML pages and assets).\nWherever the user is in the world.\nTo all users, even if they are many.\n\nCloudfront is the CDN service of AWS.\nWith a large number of POPs (Point Of Presence) around the world, it helps us provide a response to each user as close as possible to their location.\nThis allows us to significantly reduce the time to first byte of our responses.\nDifferent price classes allow you to choose the global ‚Äúarea‚Äù in which your application should be available in order to achieve savings.\n\n\n\nBeing in Lyon (France), we sometimes get answers from the POP of Milan (Italia).\nIndeed, Lyon ‚Üî Milan is almost as closer as Lyon ‚Üî Paris.\n\nNote that it is very easy to know which Cloudfront POP answered you.\nEach POP is identified by a three letter code that corresponds to the code of the nearest international airport (here: CDG corresponds to Paris Charles de Gaulle airport).\n\nx-amz-cf-pop: CDG50-C1\n\n\nDelivering content as close to the user as possible is great, it theoretically reduces waiting time but it does not solve the problem of heavy load.\n\nThe best solution for load problems is caching.\n\n\n  You put 1 second of cache-control, and you already won!\n\n  Y. Verry, our Head of Infrastructure and Ops\n\n\nCloudfront service makes it easy to cache responses at the edge servers.\nIf we take the example of sports broadcasting, users arrive in large numbers in a very short period of time.\nCaching (telling Cloudfront to cache a web page) takes a lot of the load off our Node servers because they are not called.\n\nCaching objects in Cloudfront is also about improving response times.\nNo need to wait for our servers, the user receives the cached object directly.\nCloudfront even takes advantage of this to apply more powerful compression algorithms like Brotli on these cached objects.\nThese compressions, performed directly by the CDN, allow you to drastically reduce the size of your objects on the network.\nReducing objects size make our applications load even faster for our users.\n\nHere is our Cache hit ratio in production on 6play.fr website.\n\n\n\nCloudfront also allows us to do ‚ÄúEdge computing‚Äù: run code directly in Amazon edges and POPs instead of doing it in our applications.\n\nLambda at edge (on regional edges servers), Cloudfront function (function that runs on POP servers), Web Application Firewall, here are some very cool features that will allow you to do usual manipulations on your requests/responses.\n\nFinally, by using regional Pop, hundreds of end server edges do not contact your origin (your application) when the cache is invalidated or exceeded.\nYou can even activate the Origin Shield feature that allows you to further limit the load on your origins.\n\n\n\nGood per-level cache management even allowed us to completely invalidate the cache of a Cloudfront distribution a few minutes before the start of an event without generating huge traffic on our servers.\n\n\n\nAnd that‚Äôs it for this first article, in the next part (and normally the last one) you will discover how we have implemented some patterns on our sites.\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  More efficient Load Balancing and Caching at AWS, using Consistent Hashing and HAProxy\n  Scaling Bedrock video delivery to 50 million users\n\n"
} ,
  
  {
    "title"    : "Streaming recommendations at Bedrock",
    "category" : "",
    "tags"     : " recommender systems, machine learning, data, data science",
    "url"      : "/2022/02/27/streaming-recommendation.html",
    "date"     : "February 27, 2022",
    "excerpt"  : "Personalised recommendations are everywhere. No exception for the streaming world. To improve user experience, recommender systems with machine learning are uplifting.\nAt Bedrock, until recently, there was no recommendation shaped this way.\n\nBut w...",
  "content"  : "Personalised recommendations are everywhere. No exception for the streaming world. To improve user experience, recommender systems with machine learning are uplifting.\nAt Bedrock, until recently, there was no recommendation shaped this way.\n\nBut we are writing a new story.\n\nA quick win solution\n\nWe wanted to find a way to get a solution that would be quick to integrate.\n\nWe chose to use Amazon Personalize. This service aims to construct recommender systems with machine learning. The promise is to Create real-time personalized user experiences faster at scale. Perfect! It was exactly what we were looking for.\n\nRapidly, we encountered an obstacle. You can‚Äôt deploy Personalize with Terraform. Yet, Terraform is the tool we use to manage our infrastructure.\n\nHow to deploy Amazon Personalize?\n\nAs Personalize is supposed to be a temporary solution in our stack, for once, we accepted not using Terraform. We developed a Python script to interact with Personalize. Apache Airflow schedules and monitors the script.\n\nPersonalize is a black box. You can‚Äôt have access to explanations about the generated models. But, with Personalize, you have different ways to evaluate your recommendations.\n\nWith recommender systems, using the offline metrics to judge your model is not enough. It‚Äôs better than nothing! But to check that a recommender system works, you need to evaluate it online with real users.\n\nWe have millions of users. Releasing a recommender system to all our users is definitely not the best idea ever.\n\nHow to release a recommender system?\n\nFirst of all, check offline metrics. They‚Äôre still a valuable hint. Then, analyse the recommendations with people from the editorialist team.\n\nNote that this kind of analysis is very subjective.\n\nFinally, deliver the functionality to a small portion of your users.\n\nWe configured an AB test that gives recommendations to 5% of users. With dashboards, we study the impacts.\n\nPerfect! We have a way to check the success of a full broadcast.\n\nBut how to be sure that the new product will support the load? We have millions of users. It means that 5% of users still represent a lot of people.\n\nHow to assess the performance of a recommender system?\n\nLaunch load tests. Today you have a myriad of tools to do that. At Bedrock, we use Artillery.\n\nDuring the load tests, we had a bug. We discovered that by default, the limit of requests per second with Personalize is 500. In our context, that‚Äôs not acceptable.\n\nWe asked Amazon to help us and they changed the option for us.\n\nThe results\n\nIf you‚Äôre a big fan of reality TV shows, you will see that:\n\n\n\nIf you prefer reports, you will see that instead:\n\n\n\nWhat now?\n\nWe‚Äôve deployed an AB test for our first recommender system built with machine learning.\n\nWe don‚Äôt have the results of the AB test yet. But, we‚Äôve noticed that many users interact with the recommendations.\n\nAfter different challenges, we nailed it.\n\nBut, Personalize is expensive and a black box that we can‚Äôt integrate with Terraform easily (we‚Äôll have to develop something for that, at least). It doesn‚Äôt suit our context. That‚Äôs why we‚Äôve started to develop our first models.\n"
} ,
  
  {
    "title"    : "Tonight&#39;s football time, let&#39;s prescale Kubernetes to avoid a crash!",
    "category" : "",
    "tags"     : " kubernetes, scaling, high availability, aws, cloud",
    "url"      : "/2022/02/03/prescaling.html",
    "date"     : "February 3, 2022",
    "excerpt"  : "Are you experiencing peak loads on your Kubernetes-hosted platform? Rest assured, you are not alone.\nAt Bedrock, we have developed a prescaling solution. It allows us to handle sudden and abrupt, but predictable, \ntraffic spikes, like soccer games...",
  "content"  : "Are you experiencing peak loads on your Kubernetes-hosted platform? Rest assured, you are not alone.\nAt Bedrock, we have developed a prescaling solution. It allows us to handle sudden and abrupt, but predictable, \ntraffic spikes, like soccer games.\n\nKubernetes provides HorizontalPodAutoscalers to handle traffic variations. \nWe‚Äôll look at their limitations in the case of meteoric spikes in load and how prescaling helps us deal with the \nsudden arrival of several hundred thousand users.\n\nTable of Contents\n\n\n  Load and traffic vary\n  Beginning of the scaling problems\n  How does reactive scaling work in Kubernetes?\n    \n      An HorizontalPodAutoscaler\n      What scale out looks like in a real case\n      How fast is reactive scaling?\n    \n  \n  Prescaling‚Ä¶ What is this about?\n  Prescaling our applications\n    \n      Enabling and configuring prescaling on an HPA\n      The prescaling exporter\n      How can the HPAs prescale?\n      Prescaling works!\n    \n  \n  What about special, huge, events?\n    \n      The prescaling API\n      Let‚Äôs see how an application scales during a very special event\n    \n  \n  Prescaling external services: another challenge\n\n\nLoad and traffic vary\n\nLoad and traffic have always varied over time on our platform:\n\n\nCPU per instance over time\n\nTo deal with these load variations, several tools help us to automatically adapt our Kubernetes clusters‚Äô capacity:\n\n  HorizontalPodAutoscaler \n(HPA): adds/removes Pods (= capacity) on a workload resource such as Deployment or a \nStatefulSet.\n  Cluster Autoscaler: \nadjusts the size of a Kubernetes cluster by adding/removing nodes.\n  Overprovisioning: starts ‚Äúempty‚Äù \npods (and new ‚Äúuseless‚Äù nodes, as a consequence), so the cluster has available capacity that will be used to start \napplications pods quicker.\n\n\n\n  If you wish to know more about which tools we use and why we use them in our Kubernetes clusters, I advise you to \ncheck out another dedicated blog post named \n‚ÄúThree years running Kubernetes on production at Bedrock‚Äù.\n\n\nUnfortunately, all those tools are not sufficient to deal with heavy and sudden traffic spikes on some special \nevenings such as the final of a football game or a successful show. During this kind of events, users arrive massively, \nall at the same time. On some evenings, some of our applications see their load rise by 5 in 2 minutes, others even \nsee theirs multiplied by 10 in 2 minutes!\n\nThe predictable aspect of those arrivals is very important because it means we are able to prepare our platform \nbeforehand. That‚Äôs why our prescaling solution was born.\n\nBeginning of the scaling problems\n\nOn an ordinary evening, when the multiple Kubernetes scaling tools were kicking in, here is basically what was \nhappening:\n\n\n\nAs the load increased, our capacity was increasing as well and we always had spare capacity. We were able to double our \ninitial capacity every 5 minutes thanks to reactive scaling when load started to rise.\n\nAfter a while and on some special evenings, we began to see this kind of behavior:\n\n\n\nSometimes, load was increasing faster than what reactive scaling could handle, that is to say about x2 in capacity \nevery 5 minutes. The consequence is that we could not serve everyone. For a while, our platform would fail for some \nusers, until autoscaling kicked in or until load stopped rising so fast.\n\nLet‚Äôs see how reactive scaling works to understand how we can leverage it to prepare the platform in advance.\n\nHow does reactive scaling work in Kubernetes?\n\nAn HorizontalPodAutoscaler\n\nFor a Deployment to be autoscaled (reactively, according to varying load), the number of replicas of a Kubernetes \nDeployment is reconfigured by an HorizontalPodAutoscaler:\n\n\n\nIts manifest usually looks like this:\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\n# ‚Ä¶\nspec:\n  # ‚Ä¶\n  minReplicas: 2\n  maxReplicas: 100\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 70\n  - type: Pods\n    pods:\n      metricName: phpfpm_active_process_percentage\n      targetAverageValue: &quot;40&quot;\n  # ‚Ä¶\n\n\nHere:\n\n  minReplicas is the minimum number of Pods that will run (under normal conditions).\n  maxReplicas is the maximum number of Pods that will run.\n  metrics is a list of metrics the HPA will analyze to determine if it must scale or not. If usage gets higher \nthan the target specified for a metric, the HPA will add new Pods (= scale out). If usage gets lower than all targets \nspecified for all metrics, the HPA will remove Pods (= scale in).\n\n\nWhat scale out looks like in a real case\n\nAt Bedrock, we use three types of metrics to scale: Resource, Custom and External. Here is an example of \nan application scaling out on a Custom metric:\n\n\nProcess status (%) over time\n\nIn this graph, around 20:52:30 and 20:55:00, the percentage of active processes rises to go above \nthe target 40 we saw in the YAML example before. This triggers the scale out of the HPA of this application:\n\n\nPod status over time\n\nWe can see that around 20:53:00 (30 seconds after we first go above the target), the number of unavailable pods rises. \nThe scale-out has just started. A few moments after, the pods become available and are able to serve users.\n\nHow fast is reactive scaling?\n\nScaling in Kubernetes with HPA is not instantaneous:\n\n\n\n\n  Metrics are updated every 30 seconds (for CPU or memory) or every 60 seconds (for external and custom metrics) or so.\n  HPA analyses the metrics to know if scale-out is required every 30 seconds or more.\n  We might not have enough EC2 servers running to host the new Pods that are trying to start (we usually have between \n10% and 20% of spare capacity). If starting new EC2 servers is necessary, this takes between 2 and 4 minutes.\n  Pods (as our applications and Docker images are big) usually need between 30 and 60 seconds to start.\n\n\nThis means when a spike in load occurs, we need between 1‚Äô00‚Äô‚Äô and 6‚Äô00‚Äô‚Äô for new Pods to be able to handle that load. \nScaling is clearly not instantaneous.\n\nPrescaling‚Ä¶ What is this about?\n\nMost our applications deployed in Kubernetes use an HorizontalPodAutoscaler. As we‚Äôve seen, this autoscaling \nmechanism is reactive in nature: Pods are added when load (or another metric) gets higher than a target. \nIt can handle load that increases slowly (say, instant +20% or x2 in 5 minutes), but not huge instantaneous spikes \n(say, x10 in less than 5 minutes). It works fine for most of our usual workloads, but cannot absorb spikes we receive \nduring special events like Top Chef or when the France football team plays.\n\nThe only way we can handle a huge and sudden spike in traffic on an application is by pre-provisioning capacity. \nIn Kubernetes, this is done by running more Pods than necessary so that they are ready to handle the additional load. \nRunning additional, mostly superfluous, capacity has a cost‚Ä¶\n\nWe know our applications receive a sudden and brutal traffic spike once a day, between 20:50 and 21:00 Paris time. \nNot taking special events into account, that‚Äôs the only time load increases violently enough for reactive autoscaling \nto be unable to handle it ‚Äì and, most days, it actually does quite fine. So, we could pre-provision more capacity \naround that time (only), and not pay for it the rest of the day‚Ä¶\n\nPrescaling our applications\n\nEnabling and configuring prescaling on an HPA\n\nTo enable and configure prescaling on an HorizontalPodAutoscaler, we add two sets of information:\n\n  Three annotations to define:\n    \n      Time when prescaling starts.\n      Time when prescaling stops.\n      The minimum number of Pods we want between those two times.\n    \n  \n  A new metric to scale on.\n\n\nThe annotations are set in the metadata block:\n# ‚Ä¶\nmetadata:\n  # ‚Ä¶\n  annotations:\n    annotations.scaling.exporter.replica.min: &quot;25&quot;\n    annotations.scaling.exporter.time.start: &quot;19:30:00&quot;\n    annotations.scaling.exporter.time.end: &quot;23:30:00&quot;\nspec:\n  scaleTargetRef:\n    # ‚Ä¶\n    name: &quot;service-6play-images&quot;\n\n\nIn this example, we indicate we want at least 25 pods between 19:30:00 and 23:30:00. As a result, \nthe number of pods of this application will rise up to at least 25 pods during this period of time.\n\nTimes are expressed in the local timezone of the Kubernetes cluster as prescaling is linked to events on the platform. \nThose events are usually linked to events on live TV and we deploy one cluster per TV broadcaster.\n\nIn the metrics block, you need to configure a new External metric:\n# ‚Ä¶\nmetrics:\n # ‚Ä¶\n - type: External\n   external:\n     metricName: &quot;annotation_scaling_min_replica&quot;\n     metricSelector:\n       matchLabels:\n         deployment: &quot;service-6play-images&quot;\n     targetValue: &quot;10&quot;\n\n\nThe label used for deployment must be set to the value of scaleTargetRef.name (= the name of the Deployment the \nHPA reconfigures). The targetValue must always be set to 10.\n\nYou now know how we configure an HPA for prescaling. What you do not know yet is how the HPA annotations are used \nand which application exposes the metrics called annotation_scaling_min_replica. It‚Äôs now time to talk about the \nprescaling exporter.\n\nThe prescaling exporter\n\nThe prescaling exporter is a Prometheus exporter we developed (in Python). \nIt exposes metrics, used to scale Kubernetes applications on a day-to-day basis during a given time range.\n\n\n  Prometheus is one of the tools we use in our monitoring stack at Bedrock. One of its purposes, among others, is to \ncollect metrics from Pods. This article will not present our Prometheus stack in detail.\n\n\nHere is how the prescaling exporter works:\n\n\n\n\n  Every 15 seconds or so, Prometheus scrapes the prescaling exporter pod to get the metrics it exposes.\n  Scrapping triggers the generation of the metrics. Before they are exposed, the exporter first \ncalls the k8s API to list all HPAs in the cluster.\n  Then, it will:\n    \n      Filter those HPA with the required annotations (the annotations we added a bit earlier on an HPA).\n      Calculate the new annotation_scaling_min_replica metric for each HPA with the prescaling annotations.\n      The prescaling exporter can now expose the metrics.\n    \n  \n  And Prometheus can retrieve them.\n\n\nHow does the prescaling exporter calculate the metrics of the HPA subscribed to the prescaling? Well, it depends \non the content of the annotations you configured on the HPA.\n\nHere is how a Prometheus metric of the prescaling exporter looks like:\n\n\n\nIn each metric, you will find several labels. I chose to put only one here to simplify.\nThe metric can take one of three values, depending on the content of the annotations we saw earlier:\n\n  When we are not within the time range of the prescaling annotations of the HPA, it means that we do not \nneed to prescale. As a result, the metric is set to 0.\n  If we are within the time range of the prescaling annotations, it means we are in the prescaling time range. \nFrom there, two possibilities:\n    \n      If ‚ÄúCurrent number of replicas‚Äù &amp;lt; ‚ÄúNumber of minimum replicas in the HPA annotation‚Äù, we need to add replicas so \nthe metric is set to 11.\n      If ‚ÄúCurrent number of replicas‚Äù &amp;gt;= ‚ÄúNumber of minimum replicas in the HPA annotation‚Äù, we have enough replicas so \nthe metric is set to 10.\n    \n  \n\n\n\n  When the metric is set to 10 and we already have enough replicas running, the number of replicas will never go below \nthe minimum chosen in the prescaling annotation annotations.scaling.exporter.replica.min.\n\n\nHere is what it looks like on Grafana for the application service-6play-images during the evening:\n\n\nannotation_scaling_min_replica over time\n\nIn this example:\n\n  Until 19:30, annotation_scaling_min_replica is set to 0.\n  From 19:30 until about 19:35, annotation_scaling_min_replica is set to 11 (scale-out will happen).\n  From 19:35 until 00:00, annotation_scaling_min_replica is set to 10 (scale-out is done, we have enough Pods).\n  From 00:00 until the following evening, annotation_scaling_min_replica is set to 0 again.\n\n\nWe can guess two things from this example:\n\n  The prescaling period for this application was 19:30 until 00:00.\n  It took about 5 minutes to prescale (= add more Pods) the application.\n\n\nHow can the HPAs prescale?\n\nTo understand how the HPAs can prescale, we need to talk about \nthe prometheus adapter. It is an implementation \nof the Kubernetes metrics APIs. We use it to expose custom and external metrics for HPAs to use in order to scale:\n\n\n\n\n  Prometheus adapter collects metrics from prometheus once every minute and exposes them as External and\nCustom metrics.\n  The HPA controller manager fetches metrics provided by metrics-server and prometheus-adapter to scale out or \nscale in Deployment/Replica Set/Stateful Set resources. To do so, it uses k8s aggregated \nAPIs (metrics.k8s.io, custom.metrics.k8s.io and external.metrics.k8s.io). \nMetrics-server provides resource metrics (only CPU and memory). \nPrometheus adapter provides all non-resource metrics (external and custom).\n\n\nFor more information: HPA Kubernetes documentation.\n\nPrescaling works!\n\nAfter prescaling has been deployed into production and teams started to add annotations in their projects, \nthis is what happened:\n\n\nNumber of pods regarding their status over time\n\nAround 19:30, the number of pods for this application goes from 25 to a bit more than 55. It means its HPA was \nscaled out, based on the prescaling metric of that application. Mission accomplished!\n\nWhat about special, huge, events?\n\nSome days, during very special events, ‚Äúnormal‚Äù prescaling was not enough to handle the load that was rising \nway faster than what we usually see on our platform:\n\n\n\nAs you can see, even with prescaling doing its job before the start of the TV program (we can see capacity \nrising at the beginning of the graph), the traffic rises so quickly at 20:55 that we are still unable to scale fast \nenough to serve all users.\n\nFor these special events, we have developed an additional mechanism that allows us to set a multiplication coefficient \nto all prescaling. We use it to say ‚ÄúI want a 5x higher minimum number of Pods than what‚Äôs configured in the annotation \nwe‚Äôve seen before, for all HPAs bearing this annotation‚Äù.\n\nTo deal with those very special events, we added another component in the prescaling stack: the prescaling API.\n\nThe prescaling API\n\nThe prescaling API is a backend application also developed in Python. It was designed to store prescaling \nsettings for future events on the platform in AWS DynamoDB. We chose DynamoDB because it‚Äôs a serverless database \neasily maintainable through Terraform code. By ‚Äúevent‚Äù, understand a football game or another big show such as \nTop Chef. Those settings define when and how we must enlarge the platform to sustain bigger \ntraffic spikes than on standard days (= on normal prescaling evenings).\n\n\n\nWith this API, our prescaling workflow has evolved:\n\n\n  Prometheus scrapes the prescaling exporter pod, same as before.\n  Scrapping triggers the exposition of the Prometheus metrics. Before the metrics are exposed, the exporter first \ncalls the Prescaling API server to get the current special event if there is one.\n  After calling the prescaling API, the exporter calls the k8s API (as before) to list all HPAs in the cluster.\n  Then, it:\n    \n      Filters those HPA with the required annotations.\n      Calculates the new annotation_scaling_min_replica metrics by merging information from the HPA annotations and \nthe special event from the prescaling-api server.\n    \n  \n  The prescaling exporter now exposes the metrics so that Prometheus can retrieve them.\n\n\nLet‚Äôs see how an application scales during a very special event\n\nHere is how the number of pods evolves with a special prescaling event configured:\n\n\nNumber of pods by status over time\n\nFor this specific application, we had around 25 pods during the day and standard prescaling was configured at \n40 pods in the HPA annotations. On normal days, we would have had about 40 pods throughout the evening. \nOn this particular day, we had around 125 pods: a ‚Äùx3‚Äù multiplier was applied, thanks to the prescaling API.\n\nPrescaling external services: another challenge\n\nReactive scaling still answers most of our needs. We are still able to do ‚Äúx2 every 5 minutes‚Äù in Kubernetes. \nPrescaling is great, it can help critical applications to sustain sudden and expected traffic spikes we had \nproblems dealing with before. On top of that, prescaling for special events even allows us to deal with extreme cases.\n\nStill, the applications we prescale often depend on external services: a database, a cache, a search engine‚Ä¶ \nMost of these external services will not prescale as easily as with our prescaling solution. \nSome services, like AWS DynamoDB or AWS Aurora serverless, come with a reactive autoscaling solution, \nbut not all of them. And still, even those autoscaling services have limits‚Ä¶\n\n\n\nVery special thanks to all my Bedrock Streaming colleagues who helped me improve this blog post.\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #15",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/12/20/bedrock-dev-facts-15.html",
    "date"     : "December 20, 2021",
    "excerpt"  : "Le p√®re no√´l üéÖüèª vous apporte en avance une hotte pleine de devfacts !\n\nTempora mori, tempora mundis recorda\n\n\n  L‚Äôautre jour, on allait au ski. C‚Äô√©tait l‚Äôann√©e derni√®re\n\n\nLe pass sanitaire de la prod\n\n\n  Un test de charge c‚Äôest comme un test PCR, ...",
  "content"  : "Le p√®re no√´l üéÖüèª vous apporte en avance une hotte pleine de devfacts !\n\nTempora mori, tempora mundis recorda\n\n\n  L‚Äôautre jour, on allait au ski. C‚Äô√©tait l‚Äôann√©e derni√®re\n\n\nLe pass sanitaire de la prod\n\n\n  Un test de charge c‚Äôest comme un test PCR, c‚Äôest valable 3h\n\n\nUn pair programming un peu trop random\n\n\n  _ ‚ÄúTu peux aller sur un site web random pour v√©rifier un truc ?‚Äù\n\n  _ ‚ÄúP***hub √ßa te va ou √ßa te d√©range ?‚Äù\n\n\nLa pr√©cision\n\n\n  Alors, pour X, tu as un gros tas d‚Äô√©l√©ments, pour Y un tas d‚Äô√©l√©ments, et pour Z, un petit tas d‚Äô√©l√©ments.\n\n\nOn n‚Äôest pas sorti du sable\n\n\n  ‚Äústdout n‚Äôexiste plus‚Äù\n\n\nL‚Äôeffet papillon\n\n\n  ‚ÄúBon tu viens manger ?‚Äù\n\n  ‚ÄúAttends je suis d‚Äôastreinte et il y a des orages √† Paris qui font sauter le live en Croatie‚Äù\n\n\nA new hope\n\nQuand tu envisages de refacto un bout de code, que tu regardes la PR √† l‚Äôorigine de ce code et que tu trouves un commentaire que tu avais laiss√© qui dit:\n\n\n  I really hope this will solve our problems and not introduce new ones.\n\n\nLe retour du first try\n\n\n\nUn bug ? O√π √ßa un bug ?\n\n\n  Comme j‚Äôai pas r√©ussi √† le r√©soudre, on peut le mettre en termin√©\n\n\nAh la boulette !\n\n\n  Est-ce que vous savez si on peut abort un git rebase --abort ?\n\n\nRetour de cong√©s compliqu√©\n\n\n  Possible de m‚Äôunlock mon compte Okta ?\n\n\nTODO\n\n\n\nUn bon conseil\n\n\n  Regardez le c√¥t√© fonctionnel du succ√®s\n\n\nLe re-retour du first try\n\n\n\nTout est planifi√© depuis le d√©but\n\n\n  C‚Äôest pr√©vu mais √ßa fait pas ce que je voulais.\n\n\nAh !\n\n\n  On lui a demand√© ‚Äúqu‚Äôest-ce qu‚Äôon en fait ?‚Äú, ils nous a r√©pondu ‚Äúrien‚Äù\n\n\nMon nom est Paco ü¶ú\n\n\n  il r√©p√®te tout ce que tu dis ‚Ä¶ mais en faux !\n\n\nNe pas sous-estimer le miss-click\n\n\n  Tu peux pas miss-click trois fois de suite c‚Äôest pas possible, m√™me en vacances tu peux pas.\n\n\nOn se la p√®te un peu\n\n\n  \n    \n      √áa montre bien qu‚Äôon est trop fort!\n    \n    \n      Ne confond pas le hasard avec une comp√©tence!\n    \n  \n\n\nDifficulty Driven Design\n\n\n  Sommes-nous convaincus par le DDD ou des cons vaincus par le DDD ?\n\n\nL‚Äôimportance de se relire\n\n\n  After more reflection I said bullshit.\n\n\nVers l‚Äôinfini et l‚Äôau-del√†\n\n\n  L‚Äôengineering manager m‚Äôa confirm√© que l‚Äôon pourrait aller bien au del√†, voire un peu plus‚Ä¶\n\n\nUn env de dev ? Pourquoi faire ?\n\n\n  \n    Tu as test√© en local ?\n    Bien s√ªr que non. AH √ßa ne fonctionne pas en local\n  \n\n\nQuand on vient annoncer une bonne nouvelle\n\n\n  Bonjour ! Comment √ßa allait ?\n\n\nJ‚Äôy crois moyen\n\n\n  Pour une fois √ßa marcherait m√™me mieux sous Windows.\n\n\nPlus simple que simple\n\n\n  Avec les nouvelles fonctionnalit√©s, c‚Äôest compliqu√© de faire au plus simple.\n\n\nSa vision est bas√©e sur le mouvement\n\n\n  Gris c‚Äôest bien parce que c‚Äôest ni vert ni rouge !\n\n\nLa concurrence est rude\n\n\n  \n    (P.O. Mobile) : ‚ÄúC‚Äôest moche‚Äù\n    (Tech Lead Android) : ‚ÄúC‚Äôest Android‚Äù\n    (Dev iOs) : ‚ÄúSi on peut m√™me plus troller‚Äù\n  \n\n\nEddy malou en marque blanche\n\n\n  La customiseration\n\n\nLe crime parfait\n\n\n  Ah oui, y‚Äôa des voyous ici qui mettent des espaces ins√©cables dans les noms de fonctions.\n\n\nOn l‚Äôavait pas vu venir celle-l√†\n\n\n  C‚Äôest hyper compliqu√© de pr√©voir des choses qu‚Äôon ne peut pas pr√©voir.\n\n\nC‚Äôest trop calme\n\n\n  C‚Äôest l‚Äôenc√©phalogramme de l‚Äôhuitre ton dashboard l√†\n\n\nThree magic words\n\n√Ä la caf√©t√©ria:\n\n\n  \n    üëã Il faut qu‚Äôon discute !\n    Si c‚Äôest pour parler boulot c‚Äôest m√™me pas la peine !\n    Incident en prod\n    Ah oui j‚Äôai vu passer une notif, je regarde √ßa de suite !\n  \n\n\nL‚Äôexp√©rience ou l‚Äôage\n\nMontre un raccourci clavier dans un outil de gestion de tickets\n\n  T‚Äôinqui√®te pas, j‚Äôai roul√© ma bosse, je g√®re !\n\n\nLes geeks no-life\n\n\n  \n    tu peux bouger stp pour la lumi√®re\n    pfff, j‚Äôai boug√© il y a pas 5mn\n  \n\n\nNe t‚Äôinqui√®te pas, on va trouver √ßa dans les log\n\n\n\nFatigue du soir\n\n\n  500 c‚Äôest pour succ√®s, c‚Äôest √ßa ?\n\n\nEn rouge et noir !\n\n\n  Quand c‚Äôest rouge c‚Äôest que c‚Äôest OK.\n\n\nDEL\n\nQuand on supprime du code et qu‚Äôon explique comment on l‚Äôa fait dans la PR.\n\n\n  How?\n\n  Del key on the keyboard.\n\n"
} ,
  
  {
    "title"    : "Scaling Bedrock video delivery to 50 million users",
    "category" : "",
    "tags"     : " aws, cloud, sysadmin, HAProxy, video, high availability, Unified Streaming, VOD, OTT, video delivery",
    "url"      : "/2021/12/15/scaling-bedrock-video-delivery-to-50-million-users.html",
    "date"     : "December 15, 2021",
    "excerpt"  : "Here‚Äôs our journey to migrate tens of thousands of videos, accessed by millions of users, to the cloud. How we minimized our costs without losing the biggest benefit of the cloud: scaling.\n\nThe purpose of this article is to show you the evolution ...",
  "content"  : "Here‚Äôs our journey to migrate tens of thousands of videos, accessed by millions of users, to the cloud. How we minimized our costs without losing the biggest benefit of the cloud: scaling.\n\nThe purpose of this article is to show you the evolution of this cloud video delivery platform, from the first draft to the current version.\n\nTable of Contents\n\n\n  How we do streaming\n  Just In Time Packaging\n  Version 1: The quest for self\n    \n      Local cache with Nginx\n      Network Load Balancer: manages TLS and helps with scaling\n      Content Delivery Network: Keep it Simple and Stupid\n      A first conclusion: V2 needs Consistent Hashing\n    \n  \n  Version 2: Let the requests flow\n    \n      HAProxy to make Consistent Hashing\n      EC2 costs are reduced by using only Spot instances\n      Production launch on this V2\n      EC2-other: the financial abyss\n    \n  \n  Version 3: Cost Explorer Driven Development\n    \n      Be multi-AZ without inter-AZ traffic\n      Mono-AZ AutoScalingGroups\n    \n  \n  Optimizations\n    \n      Adapt HAProxy config for EC2 bandwidth throttling\n      Adjust the hash balance factor to correctly trigger scaling\n    \n  \n  Conclusion\n\n\nHow we do streaming \n\nTo stream video, we cut each video file in 6 seconds chunks. The video player loads the associated manifest, which lists these pieces and in which order it must read them. It then downloads the first video chunk, plays it, then loads the second chunk, etc.\n\n\nPictorial explanation of video streaming\n\nA video is composed of several chunks.\nFor example, a 90 minutes movie, with a duration of 6 seconds per chunk, means 90√ó60√∑6=900 video chunks called from the player plus another 900 audio chunks. A total of 1800 different chunks for a single video.\n\nJust In Time Packaging \n\nA client calls a manifest and chunks to play a video.\nDepending on the format (Dash, HLS, Smooth) a client supports, it will request one of three kinds of manifests+chunks.\n\nThe Unified Streaming software handles these calls. Unified Origin (which we call USP) fetches the associated video from a AWS S3 bucket. It relies on a server manifest (.ism file), stored with the video, to respond with the video format the client requested: Dash, HLS, etc.\n\nSo, we store a complete video and its server manifest on S3, and USP provides the client with a client manifest and specific chunks: this is Just In Time Packaging (JITP).\n\nAnother way is to compute all the chunks and manifests in advance and write them to S3: this is offline packaging.\nIn this case, once packaging is done, there is no need to do these calculations anymore: it lightens the architecture and avoids the availability challenges of doing real-time computing.\n\n\nComparing Just-In-Time Packaging with Offline Packaging\n\nStill, this causes a big cost problem. On AWS S3, you pay for data access (GET requests), as well as storage. The more you store, the more you pay and the more you access, the more you pay.\n\ni.e, a 90mn video, played in Dash, is cut into 900 chunks plus a single Dash manifest. The same video in HLS, it‚Äôs 900 different chunks and another manifest: 1802 files written on S3. Add the Smooth Streaming format and you get 2703 files stored on S3, for a single video.\n\nOffline packaging is interesting, but incompatible with our need to manage a large number of equipments and vast catalogs: tens of thousands of program hours per customer.\n\nAnother approach, which uses the best of the two above solutions, is possible: the CMAF (Common Media Application Format) standard.\nThe player is able to chunk the video itself by adding the HTTP header Range: Bytes.\nMany devices, especially connected TVs or old Android versions, are not compatible with CMAF, which is why the rest of this article will focus on Dash/HLS in JITP.\n\nVersion 1: The quest for self \n\nWe were using USP on-prem. We decided to migrate it to the AWS cloud.\n\nThe goal of the V1 was to quickly provide a platform to our video teams to work on and certify the video players. For Bedrock Ops team, it was a first stepping stone building this platform.\n\nLet‚Äôs detail the components of this V1.\n\n\nv1 of our VOD platform\n\n\n  Players send their requests to a CDN.\n  The CDN uses a network load balancer as its origin.\n  The network load balancer forwards requests using a Round Robin algorithm, to multiple AWS EC2 instances we call ‚ÄúUSP Origin‚Äù. These EC2 instances are controlled by an AutoScalingGroup and are dynamically scaled based on their network or CPU usage.\n  EC2s retrieve files from the S3 bucket.\n\n\nLocal cache with Nginx \n\nOn EC2 instances, USP runs as a module of Apache HTTPD.\n\nWhen a player requests a specific video chunk, it sends an HTTP request to HTTPD. The USP module it embeds will:\n\n\n  load the according .ism file from S3 (the server manifest)\n  load the video metadata, stored in the first 65KB and the last 15B of a .mp4 file on S3\n  load the specific chunk from the mp4 container, according to the player‚Äôs information: bitrate, language, etc. (still on S3)\n\n\nFor each video chunk called from a player, the USP module does another call to the S3 bucket, loading the same .ism manifest and the same metadata (first 65K and latest 15B).\nTo avoid these calls and reduce S3 costs by 60%, we added Nginx on these EC2s. It goes between HTTPD and S3, to cache the manifest .ism files and metadata of .mp4 video files.\nWe‚Äôre using LUA in the Nginx vhost, to cache these 65KB and 15B requests made by USP to the S3 bucket.\n\n\nDetails on the composition of a USP origin\n\nWe use Nginx for caching because we have a solid experience with it, under heavy load, on our on-prem edge servers, which each delivers up to 200Gbps of video traffic. We want to capitalize on this expertise and avoid spreading ourselves thin on multiple tools (e.g, Apache Cache module).\n\nI recommend reading the article published by unified streaming, which uses a similar method: caching via httpd directly.\n\nNetwork Load Balancer: manages TLS and helps with scaling \n\nWe‚Äôre using Network Load Balancers to offload TLS. They are cheaper than Application Load Balancers and we don‚Äôt need to interact with the HTTP layer: this is not the role of the load balancer, we prefer to keep a KISS principle.\n\nThe major advantage of NLBs is a single entry point (a CNAME domain name), which distributes the load over n EC2 instances. This is essential for auto-scaling: nothing to configure at the CDN level, the load balancer will distribute the load among all Ready instances, whether there are 2 or 1000.\n\nAWS managed load balancers are also interesting because certificates are auto-renewed. Another advantage is that they are distributed over all the availability zones, which was one of our prerequisites in our multi-AZ strategy.\n\nContent Delivery Network: Keep It Simple and Stupid \n\nWe‚Äôre using Cloudfront CDN with a basic configuration: we respect standards and use Cache-Control header.\n\nWe‚Äôre also using our on-prem Edge servers and other CDNs. Likewise, they all respect the HTTP protocol RFCs and we provide a valid Cache-Control header to be CDN agnostic.\n\nA first conclusion: V2 needs Consistent Hashing \n\nV1 of this platform allowed our video teams to work on new features and new software versions quicker, compared to on-prem. It was also new for Infra teams: we wanted to understand how to scale the platform on AWS to meet our load requirements, making the best use of managed services and auto-scaling, which we did not have on-prem.\n\nWe have identified the problem of version 1 during our tests: the cache is ineffective under heavy loads. The Round Robin algorithm (used by the NLB) is not adequate in front of cache servers because each server will try to cache all the data and will not be specialized to a part of the data. The more requests we have, the more servers we will add and the less each server will have a relevant cache.\n\n\nInefficiency of a Round Robin algorithm in front of cache servers\n\nTo use the cache as much as possible, we need an adapted load balancing method: Consistent Hashing.\n\n\nConsistent Hashing is an ideal method for caches\n\nLast two images are from our recent blog post about doing advanced load balancing at AWS.\n\nWith Consistent Hashing, we can send all requests for the same video to the same cache server. This would optimize the local Nginx cache and reduce S3 costs.\n\nVersion 2: Let the requests flow \n\nV2 will be used in production, with thousands of requests per second: we need it to handle the load, to be reliable and robust.\nAccording to our strong experience with HAProxy, we know that it is able to do Consistent Hashing with Bounded Loads, which is exactly what we need.\n\nWe started by adding HAProxy servers between the load balancer and the USP servers.\n\nHAProxy to make Consistent Hashing \n\nHAProxy is running on EC2 instances, in a dedicated AutoScalingGroup. As with the USP AutoScalingGroup, this one scales with AWS scaling policies: network bandwidth or CPU consumption. We can launch hundreds of HAProxy servers if we need to, and their scaling is independent from the number of USP servers (but they are often linked).\n\n\nv2 of our VOD platform\n\nTo send requests to USP origin, HAProxy needs to know all the healthy EC2 instances running in their AutoScalingGroup.\nWe started by using Consul, to automatically populate our HAProxy backend with these USP servers.\n\nSee the dedicated blog post to know why we preferred to develop a tool dedicated to this task, which we called HAProxy Service Discovery Orchestrator (HSDO).\n\nEC2 costs are reduced by using only Spot instances \n\nIn addition, HSDO is very responsive to movements in the AutoScalingGroup, which allowed us to replace all EC2 On Demand instances with Spot instances.\nAnd by all instances, I mean all USP servers (with cache), as well as HAProxy servers: 70% reduction in server costs.\n\nNote that replacing USP origins with Spot instances has almost no impact on the cache, as we follow the AWS best practices for Spot: use many different instance types, be multi-AZ and use the ‚ÄúCapacity Optimized‚Äù strategy. This way we observe very few reclaims, which translates to a longer cache life.\n\nProduction launch on this V2 \n\nThe production launch of this V2 has confirmed the stability and performance of the platform. We were happy to see that our objectives were met, so we started migrating all our VOD content from on-prem to the cloud, video by video, client by client.\n\n\nNginx cache hit ratio\n\nWith Consistent Hashing, the cache becomes quite efficient and we saved 62% of calls to S3.\n\nIn addition, the cache speeds up the video packaging and we have reduced the overall origin response time. Win-win.\n\nEC2-other: the financial abyss \n\nEC2-other, in this case, means network traffic between Availability Zones.\nThe private network between two data centers (AZs) at AWS is re-billed and accounted for 48% of the bill for our VOD platforms at the time.\n\n\nAWS Cost Explorer, October 2020\n\nWhen HAProxy servers sent/received traffic from USP servers, the latter might not be in the same Availability Zone and the traffic between the two was charged at full price.\n\nIt was necessary to quickly find a solution for these costs which torpedoed the project. We started the creation of version 3 as soon as we had these metrics from version 2, at the beginning of our VOD content migration.\n\nVersion 3: Cost Explorer Driven Development \n\nThe idea is to do as well for half the price.\n\nBe multi-AZ without inter-AZ traffic \n\nWe have updated HSDO so each HAProxy only sends requests to USP origins of the same AZ.\nAnd the Network Load Balancers still send traffic to the HAProxys, on multiple AZs.\n\nRemoving inter-AZ traffic was not that much work and we quickly saw the difference: 45% cost savings.\n\n\nThe costs of this platform, where v3 was deployed in mid-November 2020\n\nWe can see on the picture above that the EC2-other costs (in orange) have disappeared in December.\n\nThe work on version 3 began shortly after the start of our cloud migration.\nWe were still migrating on-prem to the cloud when we put V3 on prod. That‚Äôs why you can see all costs have increased from October to December: we‚Äôve doubled the number of viewers during the period.\n\nMono-AZ AutoScalingGroups \n\nWe have also replaced the multi-AZ AutoScalingGroups by several mono-AZ ones. It gives us a finer scaling that corresponds to the real needs in each AZ. The randomness of the round robin and client requests means that, from time to time, one AZ receives a significantly higher load than another.\n\n\nv3 of our VOD platform\n\nSince the NLBs are using a Round Robin algorithm, each HAProxy can receive traffic for any video. Now that the HAProxy servers of an AZ only send traffic to the USP origins of the same AZ, everything that is cached exists in as many copies as we have configured of AZs.\nIt makes us all the more resilient to an AZ failure.\n\nOptimizations \n\nSince V3, we have not made any major architectural changes. However, some optimizations were necessary.\n\nAdapt HAProxy config for EC2 bandwidth throttling \n\nOn AWS, an EC2 instance has a baseline network capacity and a burst capacity (see UserGuide).\nBaseline capacity is the network bandwidth you can consume all the time.\n\nThe Burst capacity is what you may be able to consume temporarily before being throttled to the baseline capacity.\nIn the EC2 presentation, the value ‚ÄúUp to‚Äù refers to the burst.\n\nLess visible in the EC2 documentation, one can find the baseline capacity for each instance type (which is public knowledge since July 2021).\n\nFor example, c5.large instances have a network bandwidth Up to 10Gbps (burst) but only 0.75Gbps baseline bandwidth.\n\nTroubles start when HAProxy sends a little more traffic to one instance than to the others: USP origin‚Äôs bandwidth may be throttled at some point‚Ä¶ And we will observe poor performance or even service interruptions of this server.\n\n\nA server whose bandwidth is throttled (seen from CloudWatch)\n\nWe added observe layer7 to the default-server in our HAProxy backends, to remove servers returning HTTP error codes (5xx) from its load-balancing.\n\nWe also added the retry and redispatch options, which allow to retry a request sent to an unhealthy server on a healthy server. It‚Äôs not optimal for the cache, but what matters is that a client‚Äôs request is successfully answered.\n\nWe observed that with throttled bandwidth, the connection time from HAProxy to a USP server increases dramatically.\nSo we‚Äôve also reduced timeout connect to 20 milliseconds.\n\nHighlights of our HAProxy configuration:\n\ndefaults\n   timeout connect     20ms\n   retries     2\n    # We do not use &quot;all-retryable-errors&quot; because we don&#39;t want to retry on 500,\n    # which is an USP expected error code when it goes wrongly\n   retry-on     502 503 504 0rtt-rejected conn-failure empty-response response-timeout\n   option     redispatch\n   timeout server     2s\n   default-server     inter 1s fall 1 rise 10 observe layer7\n\n\nNow, if a USP origin throttles on its network bandwidth or if there is a degradation of service, HAProxy will immediately redispatch the request to another server.\n\nWe are working on adding an agent-check, so that the weight of the servers in HAProxy can be directly defined by an USP origin, if it detects that its bandwidth is throttled.\n\nAdjust the hash balance factor to correctly trigger scaling \n\nOur scaling depends on the average server utilization in an AutoScalingGroup. If a few servers are overloaded but the majority is not doing anything, we don&#39;t scale.\n\nBut all contents on our platforms are not equally popular. This affects Consistent Hashing which would result in few servers receiving way more traffic than others. Few servers would be overloaded and the majority would not do much.\n\nHere is an example in a load test:\n\n\nGraph showing few overloaded servers, using classic Consistent Hashing\n\nWe want to benefit from Consistent Hashing while being able to scale on average consumption.\nThis is what Consistent Hashing with Bounded Loads allows: to benefit from Consistent Hashing, while balancing load.\n\nThe Bounded Loads are controlled by the hash-balance-factor option in HAProxy.\nAccording to the doc:\n&amp;lt;factor&amp;gt; is the control for the maximum number of concurrent requests to\n         send to a server, expressed as a percentage of the average number\n         of concurrent requests across all of the active servers.\n\n\nWe played the same load test, once using classic Consistent Hashing, a second time using bounded loads:\n\n\nGraph showing the effects of Bounded Loads over Consistent Hashing\n\nWe did dozens of load tests before finding the best value for our use: 140.\nFor each load test, we looked at the evolution of:\n\n\n  Nginx Cache Hit Ratio\n  Number of requests to S3\n  HAProxy Backend retries and redispatches\n  HAProxy backend 5xx response codes\n  Free disk space on USP origins\n\n\nOur configuration of the Consistent Hashing with Bounded Loads remains simple:\n\nbackend usp-servers-AZ-C\n    balance hdr(X-LB)\n    hash-type consistent sdbm avalanche\n    hash-balance-factor 140\n\n\nThanks to Consistent Hashing with Bounded Loads, our cache is optimized without impacting our autoscaling.\nThere may be contents much more solicited than others, the load will be balanced and our autoscaling will be activated.\n\nConclusion \n\nWe migrated our video delivery to the cloud, moving from static servers to an end-to-end auto-scaling and multi-AZ infrastructure. We are now able to handle very high loads, which we could not do on-premise.\nWe had the opportunity to review our architecture three times, within a few weeks of each other, even though the migration had begun.\n\nThe v3 is not perfect, but it is quite well optimized, reliable and scalable.\n\nWe are thinking about V4 and saving 20% of the costs by removing the NLB. We also identified some possible improvements, adding cache on HAProxy for example, or using HAProxy Agent Check so that the weight of the servers in HAProxy is driven directly by the servers, using the Amazon metrics on network performances. Another promising performance improvement could be to use HAProxy on ARM as Graviton type instances offer significant discounts, it will be worth testing.\n\nIn parallel, we also invest time on CMAF which is for us, the long-term objective.\n\n\n\nSpecial thanks to all my colleagues at Bedrock Streaming and members of Unified-streaming, for re-re-re-and-rereading this blog post. ‚ù§Ô∏è\n"
} ,
  
  {
    "title"    : "More efficient Load Balancing and Caching at AWS, using Consistent Hashing and HAProxy",
    "category" : "",
    "tags"     : " aws, cloud, sysadmin, HAProxy, video, opensource, high availability",
    "url"      : "/2021/11/18/hsdo.html",
    "date"     : "November 18, 2021",
    "excerpt"  : "AWS ALB &amp;amp; NLB currently supports Round-Robin (RR) and Least Outstanding Requests (LOR) balancing algorithms. But what happens when you try to load balance cache servers with these algorithms? How to implement an effective cache in Cloud at sca...",
  "content"  : "AWS ALB &amp;amp; NLB currently supports Round-Robin (RR) and Least Outstanding Requests (LOR) balancing algorithms. But what happens when you try to load balance cache servers with these algorithms? How to implement an effective cache in Cloud at scale?\n\nContext\nAt Bedrock we use both ALB &amp;amp; NLB for different use-cases (like in front of our Kubernetes clusters) in our platforms. For our last VOD platform, we needed to be able to load balance heavy content (videos) to our cache servers. We knew that the balancing algorithm was a key factor for our cache in Cloud at scale, and that ALB &amp;amp; NLB won‚Äôt be sufficient to achieve our goals.\n\nBalancing algorithms\n\nRound-Robin is a widely used balancing algorithms.\n\n\nRound robin or Least Outstanding Requests algorithms\n\nThe load balancer cycles through cache servers sequentially, so each cache server should receive an equal share of requests. Each cache server has to possibly store every requested object (‚ô†Ô∏è, ‚ô•Ô∏è, ‚ô¶Ô∏è, and ‚ô£Ô∏è represent different objects).\n\nWith Least Outstanding Requests, load balancers send requests to the cache server with least awaiting requests. The cache server still has to store every requested object.\n\nConsistent Hashing is a very interesting balancing algorithm for caching purposes.\n\n\nConsistent Hashing algorithm\n\nThis algorithm allows to distribute the load so that all requests for the same object will always go to the same cache server. This way, each cache server has to store half of the objects.\n\nAnd more cache servers means more load balancing between the servers.\n\n\nConsistent Hashing at scale\n\nWhile at scale, Round-Robin or Least Outstanding Requests will look like this:\n\n\n\nRound Robin or Least Outstanding Requests at scale\n\nCache servers are not efficient at scale with these algorithms. There is a higher chance for the cache to miss, because all servers do not store all objects. Every time a cache expires or cache server boots, objects need to be cached again to be hit. You also need more resources, as the same object needs to be cached on all cache server disks.\n\nWith Consistent Hashing, once an object has been cached you have a greater chance for the cache to hit. And you save money by using smaller disks on cache servers and reducing network bandwidth.\n\nHAProxy implementation\n\nConsistent Hashing at AWS is not available with ALB, ELB and NLB. You need to implement it yourself.\n\nTo do this, we chose to use HAProxy.\n\nHAproxy is fast and reliable. We use it often, we know it well, and it can use consistent hashing.\n\nThis is how we architected it.\n\n\nLoad Balanced Cache Architecture\n\nHAProxy servers and Cache servers are deployed with Auto Scaling Groups (ASG). A Target Group is registering HAProxy ASG instances so NLB will load balance between them. Having separated ASG for HAProxy and Cache allows it to have dedicated automatic scaling and management.\n\nWe need to implement something for HAProxy so it could discover and register Cache ASG instances. And there is one thing important to do consistent hashing with HAProxy: a centralized and consistent configuration.\n\nCentralized configuration\n\nAll HAProxy instances need to have the same configuration with the same list of servers, or requests will be split differently depending on HAproxy instances.\n\n\nConsistent Hashing with different list of servers per HAProxy\n\nYou have a higher chance to have a miss on your cache request as a single object may be at different locations depending on the HAproxy instance.\nWith a centralized configuration, you can be sure that each HAProxy instance will request the same cache server for the same object.\n\nConsistent configuration\n\nHAProxy consistent hashing is based on backend server IDs. These IDs match the position of the server in the backend server list. For example, this HAProxy configuration:\n\nbackend cache\n   server CacheA 192.168.0.1:80 #ID = 1\n   server CacheB 192.168.0.2:80 #ID = 2\n   server CacheC 192.168.0.3:80 #ID = 3\n   server CacheD 192.168.0.4:80 #ID = 4\n\n\nwill be seen as the following:\n\n\n\nTo keep consistent hashing efficient, cache servers need to change ID rarely. HAProxy backend server list must be consistent across all HAProxy instances.\n\nIf CacheC is removed, configuration has to be like:\n\nbackend cache\n   server CacheA 192.168.0.1:80          #ID = 1\n   server CacheB 192.168.0.2:80          #ID = 2\n   server CacheC 192.168.0.3:80 disabled #ID = 3\n   server CacheD 192.168.0.4:80          #ID = 4\n\n\nCacheC backend server is now disabled until another Cache server takes its place.\n\n\n\nWith consistent configuration: ‚ô†Ô∏è, ‚ô£Ô∏è and ‚ô•Ô∏è requests are always balanced to the same cache servers, while ‚ô¶Ô∏è requests are balanced to another available cache server.\nWithout consistent configuration: all requests could be rebalanced to other cache servers. This would mean that for each cache server scale up or down, we no longer have the cached objects: we MISS the cache. This would be inefficient: we would lose the advantage of the cache.\n\nSolutions\n\nConsul\n\nAt first, we started to configure HAproxy through Consul. We already used it at BedRock, and an article gave us hope to quickly achieve what we wanted. \nConsul Service Discovery with DNS won‚Äôt provide sorted/consistent DNS records by design. We can‚Äôt have a consistent configuration with it.\n\nAnother way of doing so would be to use consul-template for generating backends and registering servers into an HAProxy configuration file. With this approach, we would reload systemd to add new servers to HAProxy.\n\nBut HAProxy Runtime API is the recommended way to make frequent changes on configuration, service reloads are not considered safe.\n\nAWS EC2 Service Discovery\n\nHAProxy also released a new functionality called AWS EC2 Service Discovery in July 2021. We haven‚Äôt tested it yet, but it lacks the possibility to keep a consistent list of servers between HAProxy instances, which isn‚Äôt good for consistent hashing as discussed before. We opened an issue on HAProxy dedicated Github repository.\n\nAdded to the fact that we were starting to think that Consul was overkill for our needs, we start to implement our own solution.\n\nHAProxy Service Discovery Orchestrator\n\nWhat we wanted to achieve was to use maximum managed service from AWS, meet our standard of stability and resilience, and keep things simple. We choose Python with boto3 to implement our solution as it is one of our team‚Äôs favorite languages.\n\nHAProxy Service Discovery Orchestrator (or HSDO) is open-source.\n\nHSDO is composed of a server and a client.\n\nHSDO Server\n\nHSDO Server runs in standalone. It could be a Lambda, but we were more comfortable with system processes when we designed it.\nIts job is to keep track EC2 instances of one or multiple Cache ASGs and update a list accordingly.\nHSDO server provides a consistent sorted list of instances. Every time a new cache server appears in the ASG, it is added to the list at a given ID that will never change.\nThis list is stored in DynamoDB.\n\n\nDynamoDB Items View\n\nHSDO Client\n\nHSDO client is reading the DynamoDB table to get the cache servers. The client run on the same instance as HAProxy and use the runtime API to update HAProxy config.\n\n\nHAProxy Status Page\n\nBrown lines are disabled servers, while green lines are servers stored in DynamoDB as seen above.\n\n\nHSDO in Load Balanced Cache Architecture Schema\n\nWith this architecture, we achieve a centralized and consistent configuration to make consistent hashing work at scale for cache servers.\n\nConclusion\n\nWe have been using HSDO since September 2020. We are distributing VOD content for Salto and 6play streaming platforms and are able to handle at least 10.000 requests/s. This wasn‚Äôt possible without a few improvements (on platform cost, timeout funnels, ‚Ä¶) and this will be presented in another post, so keep in touch. ;)\n\nSpecial thanks to all Ops team members in BedRock Streaming for re-re-re-and-rereading this blog post.\n"
} ,
  
  {
    "title"    : "Forum PHP 2021 - L&#39;√©dition des retrouvailles",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2021/11/02/forum-php-2021.html",
    "date"     : "November 2, 2021",
    "excerpt"  : "Cette ann√©e encore, Bedrock participait au Forum PHP o√π √©tait propos√© une grande diversit√© de conf√©rences.\nDes sujets techniques et d‚Äôautres, plus g√©n√©riques, √©taient abord√©s : Symfony 6, Git, environnement, sous-repr√©sentation des femmes dans l‚Äôi...",
  "content"  : "Cette ann√©e encore, Bedrock participait au Forum PHP o√π √©tait propos√© une grande diversit√© de conf√©rences.\nDes sujets techniques et d‚Äôautres, plus g√©n√©riques, √©taient abord√©s : Symfony 6, Git, environnement, sous-repr√©sentation des femmes dans l‚Äôinformatique‚Ä¶\nAvec Sofia LESCANO, Benoit VIGUIER sur les planches et une quinzaine de participantes et participants dans le publique, l‚Äôoccasion de rencontrer √† nouveau la communaut√© PHP en chair et en os a √©t√© saisie avec une certaine impatience.\n\nPlusieurs conf√©rences ont retenu notre attention et auront un impact √† court terme sur nos projets :\n\nSuite √† la conf√©rence ‚ÄúLes exceptions : le trou dans la raquette du typage‚Äù de Baptiste LANGLADE, la bonne gestion des exceptions nous semble primordiale. Nos √©quipes sont donc en train de tester le bundle Innmind/Immutable pour mettre en place le pattern Monad (Maybe et Either) afin d‚Äôam√©liorer la gestion d‚Äôabsence de donn√©es √† diff√©rents niveaux de nos outils.\n\nLa conf√©rence ‚ÄúDes tests unitaires pour nos r√®gles de conception‚Äù de Fr√©d√©ric BOUCHERY mettait en lumi√®re l‚Äôimportance de documenter, expliciter et tester les r√®gles de conception d‚Äôun projet. La mise en place d‚ÄôADR (Architectural Decision Records) et des tests unitaires associ√©s est une bonne pratique que nous souhaitons d√©velopper au sein des √©quipes. Nous attendons avec impatience le bundle que Klaxoon devrait bient√¥t open-sourcer: il permettra de tester facilement nos r√®gles de conceptions avec PHPUnit et d‚Äôautomatiser une partie de la revue technique.\n\nAnne-Laure DE BOISSIEU et Am√©lie DEFRANCE ont rappel√© quelques r√®gles fondamentales d‚Äôaccessibilit√© pour nos sites Internet pendant ‚ÄúAccessibilit√© et SEO : et si on relevait le niveau ?‚Äù. Nous esp√©rons d√©sormais am√©liorer l‚Äôaccessibilit√© de notre back-office. Par exemple : retravailler le contraste couleur de certains √©crans ou ajouter du contenu dans nos balises HTML pour faciliter la compr√©hension.\n\nPendant leur conf√©rence ‚ÄúKairoi, et PHP se r√©concilie avec les t√¢ches planifi√©es‚Äù,  Emeric KASBARIAN et J√©r√©my JAMES nous ont expliqu√© que leurs clients ont un m√™me besoin : ‚ÄúD√©clencher une action automatique √† un moment pr√©cis, sans aucune limite dans le temps‚Äù. Par exemple : ‚Äúsupprimer, automatiquement, un panier d‚Äôachat au bout de 15 minutes‚Äù.\nApr√®s de longues recherches, il n‚Äôexiste rien sur le march√© pour r√©pondre √† un tel besoin. Et nous, BedRock, confirmons : nous avons le m√™me besoin et n‚Äôavons rien trouv√© non plus.\nKairoi est donc n√©. C‚Äôest une application serveur Rust de planification de t√¢ches, avec son propre protocole (inspir√© de celui de Redis) qui permet de :\n\n  R√©cup√©rer des √©v√®nements √† planifier\n  Conna√Ætre l‚Äô√©tat d‚Äôun l‚Äô√©v√®nement\n  Le d√©clencher au moment opportun\n    \n      soit sur un protocole AMQP\n      soit sur un shell\n    \n  \n\n\nLe Forum PHP est l‚Äôoccasion de parler de sujets pointus techniquement, mais aussi une occasion d‚Äô√©change et de partage autour de sujets plus transversaux.\nNotre domaine, l‚ÄôIT, comme bien d‚Äôautres, est sensible au sujet de l‚Äô√©cologie. Deux axes de r√©flexion ont √©t√© √©voqu√©s pendant deux conf√©rences.\n\nEn ouverture, Fran√ßois ZANIOTTO nous a partag√© avec entrain sa recherche de mesures fiables des d√©penses √©nerg√©tiques. Elle l‚Äôa men√©e √† d√©velopper  GreenFrame, un outil en cours de construction chez Marmelab, qui a pour but de cibler au plus pr√®s la consommation √©nerg√©tique afin de tendre ‚ÄúVers la sobri√©t√© num√©rique‚Äù.\n\nH√©l√®ne MAITRE-MARCHOIS a s√ª mettre en perspective le r√¥le de chaque d√©veloppeuse et d√©veloppeur en insistant sur le fait que la responsabilit√© du d√©r√®glement climatique n‚Äôest pas forc√©ment l√† o√π on l‚Äôattend. Avec ‚ÄúComment sauver la plan√®te en ne faisant rien‚Äù, elle entend faire prendre conscience que si, la production et consommation de contenu repr√©sentent des p√¥les sur lesquels en tant que tech, nous pouvons agir. Le renouvellement du parc reste une cause pr√©pond√©rante dans l‚Äôimpact √©cologique.\nLe renouvellement acc√©l√©r√© par le foisonnement de nouvelles fonctionnalit√©s, trop souvent inutiles, est une obsolescence programm√©e.\nUtile, Accessible, Durable. Voil√† trois notions simples qui peuvent, pourtant, nous permettre de faire la diff√©rence.\n\nNicolas GREKAS, principal engineer Symfony, nous a parl√© de l‚Äô√©cosyst√®me de ce framework √† travers de sa conf√©rence ‚ÄúSymfony 6 : le choix de l‚Äôinnovation et de la performance‚Äù. Il nous a pr√©sent√© le calendrier de livraisons et de maintenance des diff√©rentes versions, avec la sortie d‚Äôune nouvelle majeure pr√©vue tous les deux ans. Chaque majeure voit la suppression du code d√©pr√©ci√© dans la version pr√©c√©dente. Par exemple, Symfony 6 est un Symfony 5.4 sans ses d√©pr√©ciations. Les derni√®res versions avant une majeure (comme la 4.4 ou 5.4) sont assur√©es d‚Äôavoir un support √† long terme.\nPour les prochaines versions de Symfony, l‚Äôaccent est mis sur la compatibilit√© avec PHP8. Puisque la majorit√© du travail consiste √† remplacer les annotations @return par un typage natif, Nicolas a parl√© de l‚Äôoutil patch-type-declarations qui automatise cette t√¢che.\n\nPour finir cette s√©rie de conf√©rences, nous avons suivi l‚Äôincroyable histoire de WorkAdventure, lors de ‚ÄúWorkAdventure de la gen√®se √† aujourd‚Äôhui : Retour d‚Äôexp√©rience sur 1 an d‚Äôunivers virtuels‚Äù pr√©sent√© par David N√âGRIER) : le r√©sultat d‚Äôun hackathon fait pour pallier l‚Äôennui des confinements, qui est devenu le support d‚Äô√©v√©nements majeurs l‚Äôann√©e derni√®re.\n\n\n\nLes speakers Bedrock\nLors de cette √©dition, deux Bedrockers ont eu l‚Äôopportunit√© de pr√©senter un sujet, l‚Äôoccasion pour nous de demander √† Sofia LESCANO ‚ÄúFaites confiance aux d√©veloppeurs.euses de votre √©quipe : voyez plus loin que les fonctionnalit√©s‚Äù et Benoit VIGUIER ‚ÄúFiber : la porte ouverte sur l‚Äôasynchrone‚Äù comment ils ont v√©cu cet √©v√©nement.\n\nComment vous est venue l‚Äôid√©e de soumettre un sujet de conf√©rence, et comment avez-vous abord√© sa pr√©paration ?\n\nSofia: Cela faisait longtemps que j‚Äôavais ce sujet en t√™te, et l‚Äôid√©e de refaire des √©v√©nements en pr√©sentiel m‚Äôa fait me lancer. Pour moi les tech meetings √©taient une grande d√©couverte et un rituel que j‚Äôappr√©cie vraiment et je voulais partager cela avec la communaut√©. Pour la pr√©paration, j‚Äôai √©t√© accompagn√©e par Matthieu Napoli avec le programme de mentoring de l‚ÄôAFUP et par mes coll√®gues de Bedrock.\n\nBenoit: Le PHP asynchrone est un sujet qui m‚Äôoccupe beaucoup √† Bedrock, j‚Äôai donc suivi attentivement la RFC Fiber. L‚Äôid√©e d‚Äôen faire un sujet de conf√©rence est venue en me rendant compte que, m√™me au sein de nos √©quipes, il n‚Äô√©tait pas √©vident pour tout le monde de comprendre tout ce que cet outil pouvait changer. Et puis, refaire un √©v√©nement en pr√©sentiel me manquait vraiment ! La pr√©paration de ce format court √©tait nouveau pour moi, heureusement j‚Äôai pu faire quelques r√©p√©titions √† Bedrock et √† l‚ÄôAFUP Lyon pour bien ajuster mon timing.\n\nMaintenant que l‚Äô√©v√©nement est derri√®re nous, que retenez-vous de cette exp√©rience ?\n\nSofia: C‚Äô√©tait une tr√®s belle exp√©rience et les √©changes que j‚Äôai pu avoir suite √† ma conf√©rence ont √©t√© tr√®s int√©ressants. C‚Äôest tr√®s enrichissant d‚Äô√©changer avec la communaut√© et de voir que des pratiques similaires ont lieu ailleurs et pouvoir les enrichir dans les deux sens.\n\nBenoit: C‚Äô√©tait un vrai plaisir de pouvoir √©changer avec de vraies personnes, sans √©crans interpos√©s ! C√¥t√© speaker, l‚Äôorganisation √©tait au top et j‚Äôai eu pleins d‚Äô√©changes prometteurs sur le potentiel des Fibers. C√¥t√© conf√©rences, j‚Äôai vu plein de choses int√©ressantes, √ßa donne toujours mati√®re √† r√©fl√©chir, que l‚Äôon partage le point de vue expos√© ou non. Merci encore √† l‚ÄôAFUP pour avoir mis toute cette √©nergie au service d‚Äôun si bel √©v√©nement.\n\nLe forum, particuli√®rement cette √©dition en pr√©sentiel, c‚Äôest retrouver toute une communaut√© qui partage la m√™me passion. Encore merci aux conf√©renci√®res et conf√©renciers, merci aux organisatrices et organisateurs‚Ä¶ Et √† l‚Äôann√©e prochaine !\n"
} ,
  
  {
    "title"    : "Fiber: the open door to asynchronous",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2021/10/21/fiber-the-open-door-to-async.html",
    "date"     : "October 21, 2021",
    "excerpt"  : "Parmi les nouveaut√©s apport√©es par Php 8.1, les Fibers tiennent une place particuli√®re. Il s‚Äôagit certainement d‚Äôune fonctionnalit√© qui aura un impact majeur sur l‚Äô√©cosyst√®me Php, tout en ayant un impact mineur sur le code que vous √©crivez tous le...",
  "content"  : "Parmi les nouveaut√©s apport√©es par Php 8.1, les Fibers tiennent une place particuli√®re. Il s‚Äôagit certainement d‚Äôune fonctionnalit√© qui aura un impact majeur sur l‚Äô√©cosyst√®me Php, tout en ayant un impact mineur sur le code que vous √©crivez tous les jours. Les Fibers sont comme des g√©n√©rateurs am√©lior√©s, des fonctions interruptibles, mais qui peuvent s‚Äôimbriquer de mani√®re transparente avec d‚Äôautres fonctions. Il est donc enfin possible de cr√©er des fonctions similaires √† await et async pour rendre la programmation asynchrone moins intrusive dans notre code et permettre la compatibilit√© avec les frameworks existants. Voici une introduction √† ces nouveaux concepts, ainsi que des exemples concrets de ce que cela permettra dans l‚Äô√©cosyst√®me Php.\n"
} ,
  
  {
    "title"    : "Faites confiance aux d√©veloppeurs.euses de votre √©quipe : voyez plus loin que les fonctionnalit√©s",
    "category" : "",
    "tags"     : " conference, afup, php",
    "url"      : "/2021/10/21/confiance-aux-devs-de-votre-team.html",
    "date"     : "October 21, 2021",
    "excerpt"  : "Deadlines, besoins produit, pression forte et fonctionnalit√©s √† livrer : nos projets ont besoin de nous ! L‚Äôam√©lioration du quotidien se perd dans un second plan, alors qu‚Äôelle a un impact majeur sur l‚Äôaugmentation de notre productivit√© et la qual...",
  "content"  : "Deadlines, besoins produit, pression forte et fonctionnalit√©s √† livrer : nos projets ont besoin de nous ! L‚Äôam√©lioration du quotidien se perd dans un second plan, alors qu‚Äôelle a un impact majeur sur l‚Äôaugmentation de notre productivit√© et la qualit√© et maintenabilit√© de notre code.\n\nConstatant que nous voulions augmenter notre confort de travail, nous avons, depuis plus d‚Äôun an, mis en place des r√©unions techniques bi-hebdomadaires pour prendre le temps de discuter de notre plateforme et nos outils, au-del√† des fonctionnalit√©s. Chaque membre de l‚Äô√©quipe contribue ainsi √† am√©liorer son exp√©rience de travail et notre produit.\n\n√Ä travers notre v√©cu, nos erreurs et des exemples techniques concrets, repensez vous aussi au d√©veloppement de votre produit.\n"
} ,
  
  {
    "title"    : "Increase performance and stability by adding an Egress Controller in a Kubernetes cluster at AWS",
    "category" : "",
    "tags"     : " php, aws, cloud, performance, sysadmin, kubernetes, HAProxy",
    "url"      : "/2021/10/18/increase-performance-and-stability-by-adding-an-egress-controller.html",
    "date"     : "October 18, 2021",
    "excerpt"  : "Introduction\n\nWe recently encountered issues with our PHP applications at scale in our Kubernetes clusters at AWS. We will explain the root cause of these issues, how we fixed them with Egress Controller, and overall improvements. We also added a ...",
  "content"  : "Introduction\n\nWe recently encountered issues with our PHP applications at scale in our Kubernetes clusters at AWS. We will explain the root cause of these issues, how we fixed them with Egress Controller, and overall improvements. We also added a detailed configuration to use HAProxy as Egress Controller.\n\nContext\n\nBedrock is using PHP for almost all of the backend API of our streaming platforms (6Play, RTLMost, Salto, ‚Ä¶). We have deployed our applications in AWS on our kops-managed Kubernetes clusters. Each of our applications is behind a CDN for caching purposes (CloudFront, Fastly). This means every time an application needs to access another API, requests go on the internet to access the latter through CDN.\n\nDuring special events with huge loads on our platforms, we started to see TCP connection errors from our applications to the outside of our VPC.\n\nErrorPortAllocation source\n\nAfter a few investigations, we saw that TCP connection errors were correlated with NAT Gateways ErrorPortAllocation.\n\n\nSome loadtesting on our platform, which you may see as no traffic, huge traffic, then no traffic again\n\nIn AWS, NAT Gateways are endpoints allowing us to go outside our VPC. They have hard limits that can‚Äôt be modified:\n\n  A NAT gateway can support up to 55,000 simultaneous connections [‚Ä¶]. If the destination IP address, the destination port, or the protocol (TCP/UDP/ICMP) changes, you can create an additional 55,000 connections. For more than 55,000 connections, there is an increased chance of connection errors due to port allocation errors. AWS Documentation\n\n\nOur applications always request the same endpoints: other APIs CDN. Destination port, IP or protocol doesn‚Äôt change that much, so we start hitting max connections, resulting in ErrorPortAllocation.\n\nAt the same time, we found a very interesting blog post: Impact of using HTTP connection pooling for PHP applications at scale, which was a very good coincidence.\n\nAs you can read in Wikimedia‚Äôs post, PHP applications aren‚Äôt able to reuse TCP connections, as PHP processes are not sharing information from a request to another. Recreating new connections on the same endpoints is inefficient: adds latency, wastes CPU (TLS negotiation and TCP connection lifecycle) but also overconsumes TCP connections.\n\n\nPHP application calls another API on internet through the NAT gateway\n\nOutgoing requests optimization\n\nEgress Controller\n\nHAproxy is fast and reliable. We use it often and know it well. We already have it as Ingress Controller in our clusters and we know service mesh needs time to be production-ready. So we thought a service mesh might be overkill in our case and we tried to add HAProxy as Kubernetes Egress Controller in our clusters.\n\n\nOutgoing requests go through the Egress Controller, which pools and maintains TCP and TLS connections\n\nWe configured some applications to send a few outgoing requests to Egress Controller. The latter was configured to do TCP re-use and to forward to desired endpoints.\n\nEffects\n\nWith this optimization, we don‚Äôt encounter ErrorPortAllocation anymore. Requests duration are reduced by 20 to 30%, and apps are consuming less CPU. Ressources were spent to instantiate a new TLS connection, which is now handled by Egress Controller.\n\n\nApplication consumes less CPU, because Egress Controller is responsible of TLS and TCP connections to the outside world, which consumes a lot of resources\n\nDetailed configuration\n\nWe generally prefer to use what already exists rather than starting from scratch, so we tried to see if HAProxy Kubernetes Ingress Controller could be used as egress.\n\nHAProxy Ingress Controller loads its frontend domains in Ingress resource, and loads backend servers in the associated Service resource. To inject an external domain as a backend server, we have to use Service ExternalName.\n\n\n\nTo use HAProxy Kubernetes Ingress Controller as an Egress Controller, we will use Ingress Kubernetes resource as Egress to define domains handled by the Controller.\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app1\nspec:\n  type: ExternalName\n  externalName: app1.example.com\n  ports:\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: app1\nannotations:\n    haproxy.org/server-ssl: &quot;true&quot;\n    haproxy.org/backend-config-snippet: |\n      # See this article for the deep reasons of both parameters: https://www.haproxy.com/fr/blog/http-keep-alive-pipelining-multiplexing-and-connection-pooling/\n      # enforce SNI with the Host string instead of the &#39;Host&#39; header, because HAProxy cannot reuse connections with a non-fixed Host SNI value.\n      default-server check-sni app1.example.com sni str(app1.example.com) resolvers mydns resolve-prefer ipv4\n      # make HAProxy reuse connections, because the default safe mode reuses connections only for the same source.ip\n      http-reuse always\n\nspec:\n  rules:\n  - host: app1.example.com\n    http:\n      paths:\n      - backend:\n          serviceName: app1\n          servicePort: 443\n\n\nWhen everything is ready, you will be able to send requests:\n\ncurl -H &quot;host: app1.example.com&quot; https://haproxy-egress.default.svc.cluster.local/health\n\n\nBy default, HAProxy resolves domain names only at bootime. But it can be configured to resolves during runtime by adding a config snippet to Egress Controller configuration:\n\nglobal-config-snippet: |\n  resolvers mydns\n    nameserver local &amp;lt;MY_DNS&amp;gt;:53\n\n\nConclusions\n\nIt seems surprising to reduce requests latency by adding a hop in a network. But it does really work, even if it has some limits.\n\nThe main problem with this approach is the fact that we are effectively creating a Single Point of Failure in our clusters if we choose to send all our egress traffic through it. Instead, we are carefully selecting applications that should use an Egress Controller to refine the configuration little by little. Some applications are tightly tied to external services and would massively gain from this and others would only be less resilient.\n"
} ,
  
  {
    "title"    : "How did we live stream our Last Friday Talks?",
    "category" : "",
    "tags"     : " lft, talks, live, stream, obs",
    "url"      : "/2021/10/14/live-streaming-lft.html",
    "date"     : "October 14, 2021",
    "excerpt"  : "At Bedrock, the last Friday of every other month1 is Last Friday Talk ‚Äì or LFT.\n\nThis event encourages sharing: technical topics, less technical topics, cross-team topics‚Ä¶\nWe also take the opportunity to meet and talk with colleagues we don‚Äôt work...",
  "content"  : "At Bedrock, the last Friday of every other month1 is Last Friday Talk ‚Äì or LFT.\n\nThis event encourages sharing: technical topics, less technical topics, cross-team topics‚Ä¶\nWe also take the opportunity to meet and talk with colleagues we don‚Äôt work with daily. And the post-LFT snack, put aside during COVID but which I hope will come back, is also meant for that!\nFinally, these LFTs are an excellent opportunity for beginner speakers to practice in front of a friendly audience. We also use them to rehearse and validate talks we will give in public later.\n\nDuring the COVID full-remote period, we moved our LFTs to Google Live Stream, where each speaker shared their screen and showed their face through the camera.\nWith the return to the office2 and partial telecommuting, a new problem arose: how can we broadcast the talks given in our auditorium to 150+ remote colleagues? In good quality, to encourage people to attend several talks in a row? While remaining enjoyable and lively on-site?\n\nThe goal: to broadcast live\n\nWe can seat up to 70 people in our amphitheater, where most of our speakers were giving their talks3. And we were aiming for 100 to 200 people remotely, for whom we wanted to broadcast live4.\n\nFor the broadcast, we worked with one of our usual tools: the organizers and speakers broadcast in a Google Meet, and the audience follows the stream via Google Live Stream5. This solution supports a large number of participants, and access is filtered through our SSO, ensuring that only our colleagues have access to the stream.\n\nWe thought we were going to do this in a bit of an ugly way, like when we were all telecommuting: each speaker joins the Google Meet, shares their screen, and speaks into their microphone, often the headset provided by the company. But, still‚Ä¶ We decided to make an effort and try to provide a better experience for our colleagues attending LFT remotely!\n\nSo, how we did it, this time\n\nHow did we capture and stream slides (or even code and video!), audio, and video of each speaker?\n\nFirst step: our amphitheater\n\nLet‚Äôs start from the room where we broadcast: our amphitheater.\n\n\n\nYou can see the room below. This photo is taken6 from the first row of the bleachers, on the right on the picture above:\n\n\n\nSo, the configuration of the room:\n\n\n  a speaker;\n  a lectern to put his or her PC;\n  from the HDMI output of this PC (possibly via a USB-C to HDMI adapter), we connect an HDMI cable that goes to the two projectors in the room;\n  these two projectors project (the same thing) on two screens, on the left and on the right of the speaker.\n\n\nBehind a screen, we placed a table for the capture and broadcast computer7. This way, it is not too far from the speaker8, without being visible from the audience.\n\nEquipment:\n\n\n  speaker‚Äôs laptop, placed on the lectern;\n  USB-C to HDMI adapter;\n  HDMI cable;\n  two projectors (fixed to the ceiling of the room);\n  two screens (fixed to the ceiling of the room).\n\n\nThis setup allows for efficient presentation in the room‚Ä¶ But we haven‚Äôt started working on the live stream yet.\n\nA PC to manage the video and audio\n\nTo broadcast live, the speaker could join the Google Meet and share their screen. That‚Äôs what we were doing during the COVID‚Ä¶ But we can also do much better!\n\nLet‚Äôs install a PC on the table behind the screen. It will broadcast, to the Google Meet, the video generated by OBS‚Äô virtual camera ‚Äì a free and open-source video recording and streaming software.\n\nA second PC, also placed on this table, is used to watch the Google Live Stream watched by all our remote colleagues. This feedback helps us validate that everything is working well, even if we suffer from a few dozen seconds of lag.\n\nEquipment:\n\n\n  a table, not too visible from the room;\n  a PC (laptop) ‚Äì the one used this time had two USB-A ports and one USB-C port, which affects the cables/adapters required later on;\n  its charger and, perhaps, an extension cord;\n  a good quality internet connection;\n  a second PC (laptop) to view the live stream;\n  and a headset to listen to the live stream.\n\n\nVideo capture of the speaker‚Äôs slides / screen\n\nWe want to broadcast to the live stream what the speaker is projecting in the room. To do this, we place an HDMI capture box between her PC and the projector:\n\n\n\nThis capture box has an HDMI input (= the cable coming out of the speaker‚Äôs PC), an HDMI output (= the cable going to the projector) and a USB-C output (= the cable going to the control / broadcast PC).\n\n\n\nOn the control / broadcast PC, the capture box connected via USB is recognized as a webcam. It is then used as a video input device in OBS.\n\nHardware:\n\n\n  additional HDMI cable;\n  capture device + passthrough: Elgato HD60 S+;\n  USB-C to USB-A cable;\n  USB-A extension cable (because the control desk is a bit far).\n\n\nThe video recording of the room\n\nIt would be even cooler if the remote audience could see the speaker! As a matter of fact, part of the message of each talk is transmitted by the speaker‚Äôs gestures.\n\nSo let‚Äôs position a camera in the room, at the level of the audience sitting in the stands, to give the impression the speakers are looking at the camera when they are looking at the audience.\n\n\n\nWe did not have a real camera at hand. So we used an iPhone 11 Pro Max9. It has only a Lightning port‚Ä¶ And, with an adapter, we can connect an HDMI cable. And the Filmic Pro10 application knows how to stream a clean HDMI11 video.\n\nTo capture the HDMI signal from the iPhone to the PC, we used a second capture box, simpler than the previous one: it has an HDMI input (= to connect the iPhone) and a USB output (= connected to the PC):\n\n\n\nFearing the iPhone‚Äôs battery wouldn‚Äôt last all day while filming, we powered it via an external battery12. The Lightning-to-HDMI adapter happens to include a lightning plug for power.\n\nHardware:\n\n\n  iPhone 11 Pro Max (another high-end model less than four or five years old would have done the trick as well);\n  lightning to HDMI adapter, with Lightning input for power supply;\n  10m HDMI cable (to reach the control PC);\n  external battery;\n  HDMI capture box: Elgato Camlink 4K;\n  USB cable (between camlink and PC);\n  USB to lightning cable (between the battery and the adapter connected to the iPhone).\n\n\nThe capture box is recognized as a USB webcam. We use it as a video input device in OBS.\n\nWe first put the camera on the side of the room, to not disturb. But the speakers never looked in its direction, and it was not very nice for the remote audience, which felt less included. So we moved the camera almost in front of the speaker. This way, the remote audience feels more like the person speaking is looking in their direction.\nAlso, the camera that films the speaker also films a screen, including when the speaker wants to point to a part of what they are presenting.\n\nAudio recording of the speaker\n\nOur amphitheater is equipped with microphones and speakers, but we don‚Äôt yet know how to get the sound from this audio system to inject it into a live stream.\n\nWith the resources and time we had, the best we could do for the speakers was a wireless lapel mic:\n\n\n\nWe didn‚Äôt have a wireless mic, but we had something that could act as such, and we ended up with a 3.5‚Äù jack to plug into the microphone port of the control PC. This microphone is then used as an audio source in the Google Meet.\n\nEquipment:\n\n\n  Lapel microphone with TRRS Jack 3.5 port: RODE SmartLav+;\n  TRRS female to TRS male adapter (between the microphone and the transmitter box);\n  wireless transmitter + receiver kit (TRS 3.5 jack input on transmitter, TRS 3.5 jack output on receiver): RODE Wireless Go (1st generation);\n  TRS 3.5 male to male cable (between the receiver box and the PC).\n\n\nSome other points and nice ‚Äúbonuses‚Äù\n\nOf course, we didn‚Äôt stop at these main points, and lots of other little things came into play throughout the day.\n\nRemote speakers?\n\nI wrote above that I wouldn‚Äôt talk about it, and I lied a bit: one of the talks that day was given by a colleague who was working from home.\nHe joined the organizers‚Äô Google Meet, shared his screen, activated his camera, and presented, exactly as many of us have done during more than a year of forced telecommuting because of COVID.\nFor those in the auditorium, one of the organizers plugged in his computer to the two screens in the room and its sound system.\n\nTransitions between sessions\n\nAt the end of each talk, we quickly took over the speaker‚Äôs desk and microphone. We wanted to set up the next person, plug in their PC and position the mic so as not to interfere with them, explain how we were broadcasting and validate the setup and configuration.\nAn organizer was in charge of managing the transition, indicating at what time we would resume. But this transition, in the room, was done without a microphone ‚Äì and therefore, without being broadcast to the live stream.\nFor the live stream, another organizer made the same transition by joining the Google Meet of organizers and speakers. This way, our remote colleagues were not left in the dark and knew when the next talk would resume.\n\nSome useful, or even essential, utilities\n\nOf course, in a large room with few electrical outlets that are not always well placed, you need to bring extension cords and power strips.\nAlso, to avoid someone getting their feet caught in the extension cords or cables (USB, HDMI), bring a roll of duct tape to secure everything to the floor.\nAnd depending on where the lapel microphone is attached (shirt collar, shirt buttonhole‚Ä¶) and where its cable goes (above the shirt in front, under the shirt / under the shirt, on the shoulder and in the back‚Ä¶), more delicate tape13 is very handy.\n\nAt OBS level\n\nOn the control PC, in OBS, we configured several scenes to highlight the speakers or their slides. The images below are screenshots from the live stream, so in 720p maximum and much too compressed‚Ä¶\n\nScreen only\n\nTo display large code examples, videos‚Ä¶\n\n\n\nLarge screen\n\nWith speaker overlay in the bottom right corner: for slides written small, while reflecting an idea of the gestures. And we also had the same thing with the speaker inlay in another corner, for slides with important texts in the bottom right of the screen.\n\n\n\nMosaiq\n\nScreen on the left half of the screen, video of the speaker on the right half: for large written slides and to allow people at a distance to see the speaker clearly. We used this view a lot when the camera was on the side of the room ‚Äì and we didn‚Äôt use it anymore once the camera was in front of the speaker.\n\n\n\nSpeaker only\n\nOnly the speaker‚Äôs video, framed to include one of the two screens. We used this view ‚Äúas if the person watching the stream was sitting in the stands‚Äù for much of the afternoon, when the camera was placed in the middle of the audience.\n\n\n\nFor the practical aspects of managing the live stream\n\nChanging scenes in OBS can be done with the mouse (but it‚Äôs not convenient) or with keyboard shortcuts (but you have to remember the shortcuts and the scenes they correspond to).\n\nBut it‚Äôs much more fun with a Stream Deck: a command pad with big buttons, which you can customize! Yes, it‚Äôs a bit gimmicky and not at all essential, but it‚Äôs also very cool to use ;-)\n\nHardware:\n\n\n  Elgato Stream Deck;\n  since it connects to USB-A: an adapter or a USB hub.\n\n\n\n\nThe control desk, in photo\n\nA lot of unorganized cables‚Ä¶ Here is what the table looked like with the control PC14:\n\n\n\nSome things to improve\n\nOf course, not everything was perfect‚Ä¶ Still, for less than an hour and a half of installation and testing that morning, we were quite happy with the result!\n\nFirst of all, Google Live Stream: the solution, integrated to Google Workspace, is very practical. Including the aspect of ‚Äúlimiting access to our employees‚Äù15. However, the video quality, in 720p too compressed, is not optimal :-/.\n\nOur auditorium has two cameras fixed to the ceiling. Today, they are not yet functional16, but we hope they will soon replace the iPhone ‚Äì and allow us to get a view of the audience for questions.\n\nAlso, having only one microphone is problematic when two speakers are speaking for a talk. It happened once during the day, and we put the lapel mic on the lectern and asked the speakers to move closer to it when it was their turn to speak. The RODE SmartLav+ being omni-directional and of good quality, it was just about right‚Ä¶ Without being optimal. Also, we didn‚Äôt have a microphone for the questions, so the speakers had to repeat them17.\nWe have two wireless microphones in the auditorium. Currently, they are used to amplify the sound in the room and we didn‚Äôt have time in the morning when we were setting up to figure out how to capture their output and integrate it into the live stream. Maybe an improvement for next time, because they seem to be pretty good ;-)\nAlso, we didn‚Äôt have the time18 to talk with our colleagues who were managing the sound of these events before the COVID: somewhere in a closet, we have a physical mixer, which could have been useful! Another area for improvement!\n\nThat said, now that we‚Äôve seen that it is possible, we want to do even better next time, in two months ;-). And, clearly, a prototype in 1h30 of setup, which worked that well all day long and which improved the experience of 150 or more remote colleagues, is a big success which will push us to do better - and we know we will be able to do so!\nAnd then, with time and successive improvements, we may spend less time each time installing and configuring!\nAlso, a more turnkey solution ‚Äì like Streamyard19 ‚Äì might make our lives easier, compared to OBS‚Ä¶\n\nEvery two months, we organize a LFT day at Bedrock: our Last Friday Talks.\nAfter COVID and with the partial return to the office, we wanted our remote colleagues to experience this LFT as best as they could.\nFor this iteration, we did with what we had. It‚Äôs up to us to iterate and do even better next time!\nAnd you, how do you share these kinds of events with your remote colleagues?\n\n  \n    \n      Historically, it was the afternoon of the last Friday of every month. We revised the format in 2019 to make it a full day every other month.¬†&amp;#8617;\n    \n    \n      In France, some companies ‚Äì including ours ‚Äì started returning to the office, full-time or not, in June 2021, following recommendations from the government.¬†&amp;#8617;\n    \n    \n      One of the talks was given by a remote speaker, I will not talk much about it in this article: he joined the Google Meet of the organizers and shared his camera and screen. An organizer then projected the Google Meet into the auditorium for the live audience.¬†&amp;#8617;\n    \n    \n      We went up to 150 people watching the stream simultaneously.¬†&amp;#8617;\n    \n    \n      Google Live Stream. We use this solution because Google Meet alone does not support enough people in a call.¬†&amp;#8617;\n    \n    \n      Photo taken while projecting talks of the Demuxed conference, for those who wanted to see them on a big screen and together‚Ä¶¬†&amp;#8617;\n    \n    \n      In the photo above, we see the legs of the table and high chairs placed around it. Yes, this space also serves as a break room.¬†&amp;#8617;\n    \n    \n      And the organizers can tap them on the shoulder, to tell them to start their talk or ask them to take a break in case of technical problems‚Ä¶¬†&amp;#8617;\n    \n    \n      High-end smartphones have had very good quality cameras for several years. An iPhone 11 is more than enough to film and broadcast a live conference.¬†&amp;#8617;\n    \n    \n      Filmic Pro: a not cheap application, but very good when it comes to filming with an iPhone.¬†&amp;#8617;\n    \n    \n      Clean HDMI: the filmed video, in real time, without the decorations of the application interface.¬†&amp;#8617;\n    \n    \n      External battery: because there are no electrical outlet in this part of the room and we didn‚Äôt want to add more cables and extension cords.¬†&amp;#8617;\n    \n    \n      I use medical tape, which is easy to cut, sticks well and comes off easily too; and is not too aggressive with clothes and skin.¬†&amp;#8617;\n    \n    \n      Yes, it‚Äôs a bit of a mess‚Ä¶ And, no, you can‚Äôt see everything: when we took this picture, we didn‚Äôt think we would post it ^^¬†&amp;#8617;\n    \n    \n      We may broadcast some talks in public in the future‚Ä¶ But it was not a topic for this time and we know that some topics will remain internal no matter what.¬†&amp;#8617;\n    \n    \n      We moved in recently and other more important items were configured first.¬†&amp;#8617;\n    \n    \n      A person in the control room tapped the speakers on the shoulder when they forgot to repeat the questions‚Ä¶¬†&amp;#8617;\n    \n    \n      We should have done it a few days in advance, not the morning itself‚Ä¶¬†&amp;#8617;\n    \n    \n      Which we‚Äôve already used at conferences, both as organizers and speakers.¬†&amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Best practices for Web application maintenance",
    "category" : "",
    "tags"     : " js, react, web, frontend",
    "url"      : "/2021/09/06/web-best-practices.html",
    "date"     : "September 6, 2021",
    "excerpt"  : "\n  How not to throw away your application every two years?\n\n\nFeedback based on best practices applied to the web platform developed at Bedrock Streaming\n\nA bit of context\n\nAt Bedrock Streaming many teams develop and maintain frontend applications ...",
  "content"  : "\n  How not to throw away your application every two years?\n\n\nFeedback based on best practices applied to the web platform developed at Bedrock Streaming\n\nA bit of context\n\nAt Bedrock Streaming many teams develop and maintain frontend applications for our customers and users.\nSome of those applications are not very young.\nIn fact, the application I‚Äôm mainly working on is a website whose developments started in 2014.\nI have already mentioned it in different articles of this blog.\n\n\n\nYou might think: ‚ÄúOh poor people, maintaining an almost 10 year old application must be hell!‚Äù\n\nDon‚Äôt worry, it‚Äôs not the case!\nI have worked on projects that are much less old but where the development of new features was much more painful.\n\nToday the project is technically up to date, we must be on the latest version of React while it had started on a version 0.x.x.\nIn this world of often criticized web technologies where tools and practices are constantly evolving (eg: the many articles on the Javascript Fatigue), to keep a project ‚Äúup to date‚Äù remains a real challenge.\n\n\n\nMoreover, in the context of this project, in almost 10 years, we have had about 100 contributors.\nSome have only stayed a few months/years.\nHow can we keep the maximum knowledge on ‚ÄúHow we do things and how it works?‚Äù in such a moving human context?\n\n\n\nThis is what I would like to demonstrate in this post.\n\nWith the help of my colleagues, I have collected the list of good practices that still allow us to maintain this project today.\nWith Florent Dubost, we have often thought that it would be interesting to publish it.\nWe hope you will find it useful.\n\nSet rules and automate them\n\nA project that stands the test of time is first and foremost a set of knowledge that is stacked one on top of the other.\nIt‚Äôs like the Kapla tower you used to build as a child, trying to get as high as possible.\nA solid base on which we hope to add as much as possible before a potential fall.\n\nFrom the beginning of a project, we have to make important decisions about ‚ÄúHow do we want to do things?\nWe think for example about ‚ÄúWhat format for our files? How do we name this or that thing?‚Äù\nWriting accurate documentation of ‚ÄúHow we do things‚Äù might seem like a good idea.\n\nHowever, documentation is cool, but it tends to get outdated very quickly.\nOur decisions evolve, but documentation does not.\n\n\n  ‚ÄúTimes change but not READMEs.‚Äù\n\n  Olivier Mansour (deputy CTO at Bedrock)\n\n\nAutomating the checking of each of the rules we impose on ourselves (on our codebase or our processes) is much more durable.\nTo make it simple, we avoid as much as possible to say ‚ÄúWe should do things like that‚Äù, and we prefer ‚Äúwe‚Äôll code something that checks it for us‚Äù.\nOn top of that, on the JS side we are really well equipped with tools like Eslint that allow us to implement our own rules.\n\nSo the reflex we try to adopt is the following:\n\n\n  ‚ÄúWe should try to do it like this now!‚Äù\n  ‚ÄúOk that‚Äôs interesting, but how can we make sure we do it like that automatically with our CI (Continuous Integration)?‚Äù\n\n\nContinuous Integration of a project is the perfect solution to not miss anything on every Pull Request we provide.\nReviews are only easier because you don‚Äôt have to worry about all the rules that are already automated.\nIn this model, the review is more for knowledge sharing than for typo copying and other non-compliance with the project conventions.\n\nIn this principle, we must therefore try to banish oral rules.\nThe time of the druids is over, if all the good practices of a project have to be transmitted orally, it will only take longer to guide new developers into your team.\n\n\n\nA project is not set in stone. These rules evolve with time.\nIt is therefore preferable to add rules that have a script that will autofix the whole codebase intelligently.\nMany Eslint rules offer this, and it is a very important selection criteria when choosing new conventions.\n\neslint --fix\n\n\nA very strict rule that will force you to modify your code manually before each push is annoying in the long run and will annoy your teams.\nWhereas a rule (even a very strict one) that can auto-fix itself at commit time will not be seen as annoying.\n\nHow to decide to add new rules ?\n\nThis question may seem thorny, take for example the case of &amp;lt;tab&amp;gt; / &amp;lt;space&amp;gt; in files.\nFor this, we try to avoid the endless debates and follow the trend and rules of the community.\nFor example, our Eslint configuration base is based on Airbnb‚Äôs which seems to have some success in the JS community.\nBut if the rule we want to impose on ourselves is not available in Eslint or other tools, we sometimes prefer not to follow the rule rather than say ‚ÄúWe‚Äôll do it without a checking CI‚Äù.\n\nThe almost exhaustive list ü§û\n\n\n\n\n  The file format is managed by Editorconfig, prettier and Eslint.\nWe have opensourced our own configuration, if it is of any use to you.\n  We use a specific commit name to generate our changelog.\nTo make sure devs follow it, a simple step in our CI checks it.\n  We don‚Äôt want a dev to make our JS bundles very big in production, so we track and measure their size in the CI.\nWe use an in-house tool but we recommend to use the BuildTracker tool.\n  Test coverage is not an indicator for the team, not all lines have the same need for us to be tested.\nSome teams at Bedrock however follow this indicator which at least has the interest to give a trend.\n  Our unit tests obviously run on the CI, these must pass.\n  Our functional tests (End to end: E2E) run on Chrome Headless, they must be green.\n  The logs of our E2E tests are retrieved and parsed to avoid errors or React warnings (the parsing script is however complicated to maintain)\n  Functional tests run in a sandbox where the whole network is proxied.\nWe make sure that our tests do not depend on a non mocked API that could slow down their execution.\n  During the E2E tests we check that no image request has generated a 404.\n  We perform some accessibility checks with Axe during our E2E tests.\n  We check some rules on the CSS with Stylelint and bemlinter (we don‚Äôt use BEM anymore but there is still some style managed in SCSS that we migrate little by little in StyledComponent)\n  The project is a monorepo on which we try to maintain the same dependencies versions for each package.\nWe developed a tool which automates this check: monorepo-dependencies-check\n  We check that our yarn.lock file has not been inadvertently modified or that it has been updated with respect to the modifications of the package.json.\n  Terraform is used to manage our cloud resources, we check that the file format is correct.\n\n\nTest, test, test\n\nI hope that in 2021 it is no longer necessary to explain why automatic testing of your application is essential to make it sustainable.\nIn JS, we are rather well equipped in terms of testing tools today.\nHowever, the eternal question remains:\n\n\n  ‚ÄúWhat do we want to test?‚Äù\n\n\nGlobally if we search on the internet this question, we see that different needs make emerge very different practices and testing tools.\nIt would be very presumptuous to think that there is a good way to automatically test your application.\nThis is why it is preferable to define one or more test strategies that meet defined and limited needs.\n\nOur test strategies are based on two distinct goals:\n\n\n  To automate the verification of the functionalities proposed to the users by putting ourselves in their place.\n  To provide us with efficient solutions to specify the way we implement our technical solutions to allow us to make them evolve more easily.\n\n\nTo do this, we perform two ‚Äútypes of tests‚Äù that I propose to present here.\n\nOur E2E tests\n\nWe call them ‚Äúfunctional tests‚Äù, they are End-to-end (E2E) tests on a very efficient technical stack composed of CucumberJS, WebdriverIO with ChromeHeadless\nThis is a technical stack set up at the beginning of the project (at the time with PhantomJS for the oldest among you)\n\nThis stack allows us to automate the piloting of tests that control a browser.\nThis browser will perform actions that are as close as possible to what our real users can do while checking how the site reacts.\n\nA few years ago, this technical stack was rather complicated to set up, but today it is rather simple to do.\nThe site that hosts this blog post is itself proof of this.\nIt only took me about ten minutes to set up this stack with the WebdriverIo CLI to verify that my blog is working as expected.\n\nI recently published an article presenting the implementation of this stack.\n\nSo here is an example of an E2E test file to give you an idea:\n\nFeature: Playground\n\n  Background: Playground context\n    Given I use &quot;playground&quot; test context\n\n  Scenario: Check if playground is reachable\n    When As user &quot;toto@toto.fr&quot; I visit the &quot;playground&quot; page\n    And I click on &quot;playground trigger&quot;\n    Then I should see a &quot;visible playground&quot;\n    And I should see 4 &quot;playground tab&quot; in &quot;playground&quot;\n\n    When I click on &quot;playground trigger&quot;\n    Then I should not see a &quot;visible playground&quot;\n\n    # ...\n\n\nAnd it looks like this in local with my Chrome browser!\n\n\n\nHere is a diagram that explains how this stack works:\n\n\n\nToday, Bedrock‚Äôs web application has over 800 E2E test cases running on each of our Pull Request and the master branch.\nThey assure us that we are not introducing any functional regression and that‚Äôs just great!\n\nüëç The positives\n\n\n  WebdriverIO also allows us to run these same tests on real devices on a daily basis through the paid SAAS service Browserstack.\nSo we have every day a job that makes sure that our site works correctly on a Chrome last version on Windows 10 and Safari on MacOs.\n  These tests allow us to easily document the functionality of the application using the Gherkin language.\n  They allow us to reproduce cases that are far from nominal.\nIn a TDD logic, they allow us to advance on the development without having to click for hours.\n  These tests allowed us not to break the old version of the site which is still in production for some customers while our efforts are concentrated on the new one.\n  They give us real confidence.\n  Thanks to our library superagent-mock, we can fixturer (plug, mock) all the APIs we depend on and thus even check the error cases.\nAlso, mocking the browser‚Äôs XHR layer allows for a significant improvement in test execution time. üöÄ\n  They give us access to extended uses like:\n    \n      checking accessibility rules\n      check the browser console logs (to avoid introducing errors or React Warning for example)\n      monitoring all network calls of the site through a proxy\n      and so on‚Ä¶\n    \n  \n\n\nüëé The complications\n\n\n  Maintaining this stack is complicated and expensive.\nSince few resources are published on this domain, we sometimes find ourselves digging for days to fix them üòÖ.\nSometimes we feel quite alone in having these worries.\n  It is very easy to code a so-called flaky E2E test (ie: a test that can fail randomly).\nThey make us think that something is broken.\nThey sometimes take us a long time to stabilize.\nIt is still much better to remove a test that will not give you a stable result.\n  Running all the tests takes a lot of time on our continuous integration.\nWe must regularly work on their optimization so that the feedback they provide you is as fast as possible.\nThese important times also cost money, because we have to run these tests on machines.\nFor your information, the infrastructure of the website (just the hosting of our Node servers + static files + CDN) cost much less than our continuous integration.\nThis obviously makes our Ops team smile! üòä\n  The new recruits in our teams have often never done this kind of testing, so there is a struggle phase of learning‚Ä¶\n  Some features are sometimes too complicated to test with our E2E stack (for example, payment paths that depend on third parties).\nSo we sometimes fall back on other techniques with Jest, especially with a less unitary scope.\n\n\nOur ‚Äúunit‚Äù tests\n\nTo complete our functional tests we also have a stack of tests written with Jest.\nWe call these tests unit tests because we have as a principle to try to always test our JS modules independently from the others.\n\nLet‚Äôs not debate here about ‚ÄúAre these real unit tests?‚Äù, there are enough articles on the internet about this topic.\n\nWe use these tests for different reasons that cover needs that our functional tests do not cover:\n\n\n  to help us develop our JS modules with TDD practices.\n  to document and describe how a JS module works.\n  test very/too complicated edge cases with our E2E tests.\n  facilitate the refactoring of our application by showing us the technical impacts of our modifications.\n\n\nWith these tests, we put ourselves at the level of a utility function, a Redux action, a reducer, a React component.\nWe rely mainly on the automock functionality of Jest which allows us to isolate our JS modules when we test.\n\n\n\nThe previous image represents the metaphor that allows us to explain our unit testing strategy to newcomers.\n\n\n  You have to imagine that the application is a wall made of unit bricks (our ecmascript modules), our unit tests must test one by one the bricks in total independence from the others.\nOur functional tests are there to test the cement between the bricks.\n\n\nTo summarize, we could say that our E2E tests test what our application should do, and our unit tests make sure to check how it works.\n\nToday there are more than 6000 unit tests that cover the application and allow to limit regressions.\n\nüëç\n\n\n  Jest is really a great library, fast, complete, well documented.\n  Unit tests help us a lot to understand several years later how it all works.\n  We always manage to unit test our code, and it complements our E2E tests well.\n  The automock feature is really handy for breaking down tests by modules.\n\n\nüëé\n\n\n  Sometimes we found ourselves limited by our E2E test stack and couldn‚Äôt rely solely on unit tests.\nWe were missing something to be able to make sure that the cement between the bricks worked as we wanted it to.\nFor this, a second test stack Jest was set up called ‚Äúintegration test‚Äù where the automock is disabled.\n  The abuse of Snapshot is dangerous for your health.\nThe use of ‚ÄúSnapshot testing‚Äù can save time on the implementation of your tests but can reduce the quality.\nHaving to review a 50 line object in Snapshot is neither easy nor relevant.\n  With the depreciation of EnzymeJS, we are forced to migrate to React Testing Library.\nIt is of course possible to unit test components with this new library.\nUnfortunately, this is not really the spirit and the way to do it.\nReact Testing Library pushes us not to play with shallow rendering.\n\n\nOur principles\n\nWe try to always follow the following rules when asking the question ‚ÄúShould I add tests?‚Äù.\n\n\n  If our Pull Request introduces new user features, we need to integrate E2E test scenarios.\nUnit tests with Jest can complete / replace them accordingly.\n  If our Pull Request aims to fix a bug, it means that we are missing a test case.\nWe must therefore try to add an E2E test or, failing that, a unit test.\n\n\nIt is while writing these lines that I think that these principles could very well be automated. ü§£\n\nThe project stays, the features don‚Äôt\n\n\n  ‚ÄúThe second evolution of a feature is very often its removal.‚Äù\n\n\nAs a matter of principle, we want to make sure that every new feature in the application does not base its activation on simply being in the codebase.\nTypically, the lifecycle of a feature in a project can be as follows (in a Github Flow):\n\n\n  a person implements on a branch\n  the feature is merged on master\n  it is deployed in production\n  lives its feature life (sometimes with bugs and fixes)\n  the feature is not needed anymore\n  a person unravels the code and removes it\n  new deployment\n\n\nTo simplify some steps, we have implemented feature flipping on the project.\n\nHow does it work?\n\nIn our config there is a map key/value that lists all the features of the application associated with their activation status.\n\nconst featureFlipping = {\n  myAwesomeFeature: false,\n  anotherOne: true,\n}\n\n\nIn our code, we have implemented conditional treatments that say ‚ÄúIf this feature is activated then‚Ä¶‚Äù.\nThis can change the rendering of a component, change the implementation of a Redux action or disable a route in our react-router.\n\nBut what‚Äôs the point?\n\n\n  We can develop new evolutions progressively by hiding them behind a configuration key.\nWe deliver features in production without activating them.\n  In a test environment, we can overload this config to test features that are not yet activated in production.\n  In the case of a white label site, we can propose these features to our customers as possible options.\n  Before deleting code of a feature, we deactivate it and clean it up without risk.\n  Thanks to an in-house tool called Applaunch, this feature flipping config can be overloaded on time in a GUI without deployment.\nThis allows us to activate features without putting the code into production.\nIn the event of an incident, we can deactivate features that have been degraded.\n\n\nTo give you a more concrete example, between 2018 and 2020 we completely overhauled the application‚Äôs interface.\nThis graphical evolution was just a featureFlipping key.\nThe graphical redesign was not a reset of the project, we still live with both versions (as long as the switchover of all our customers is not completed).\n\n\n\nA/B testing\n\nThanks to the great work of the backend and data teams, we were even able to extend the use of feature flipping by making this configuration modifiable for sub-groups of users.\n\nThis allows us to deploy new features on a smaller portion of users in order to compare our KPI.\n\nDecision making, technical or product performance improvement, experimentation, the possibilities are numerous and we exploit them more and more.\n\nThe future flipping.\n\n\n  Based on an original idea by Florent Lepretre.\n\n\nWe regularly had the need to activate features at very early hours in the future.\nFor that we had to be connected at a precise time on our computer to modify the configuration on the fly.\n\nTo avoid forgetting to do this, or doing it late, we made sure that a configuration key could be activated from a certain date.\nTo do this, we evolved our Redux selector which indicated if a feature was activated so that it could handle date formats and compare them to the current time.\n\nconst featureFlipping = {\n  myAwesomeFeature: {\n    offDate: &#39;2021-07-12 20:30:00&#39;,\n    onDate: &#39;2021-07-12 19:30:00&#39;,\n  },\n}\n\n\n\n  Many coffees ‚òïÔ∏è at 9am have been saved by future flipping.\n\n\nMonitor, Measure, Alert\n\nTo maintain a project as long as Bedrock‚Äôs web application, testing, documentation and rigor are not enough.\nYou also need visibility on what works in production.\n\n\n  ‚ÄúHow do you know that the application you have in production right now is working as expected?‚Äù\n\n\nWe assume that no functionality works until it is monitored.\nToday, monitoring in Bedrock on the frontend side takes the form of different tools and different stacks.\nWe rely on NewRelic, Statsd, a ELK stack and even Youbora for the video streaming part.\n\nTo give you an example, each time a user starts a browsing session we send an anonymous monitoring Hit to increment a counter in Statsd.\nWe then have to define a dashboard that displays the evolution of this number in a graph.\nIf we observe a too important variation, it can allow us to detect an incident.\n\n\n\nMonitoring also offers us solutions to understand and analyze a bug that occurred in the past.\nUnderstanding an incident, explaining it, finding its root cause are the possibilities that are open to you if you monitor your application.\nMonitoring can also allow you to better communicate with your customers about the impact of an incident and also to estimate the number of impacted users.\n\nWith the multiplication of our customers, monitoring our platforms well is not enough.\nToo much data, too many dashboards to monitor, it becomes very easy to miss something.\nSo we started to complement our metrics monitoring with automatic alerting.\nOnce we have enough confidence in the metrics, we can easily set up alerts that will warn us if there is an inconsistent value.\n\nHowever, we try to always trigger alerts only when it is actionable.\nIn other words, if an alert sounds, we have something to do.\nSounding alerts that do not require immediate human action generates noise and wastes time.\n\n\n\nLimit, monitor and update your dependencies\n\nWhat goes out of date faster than your shadow in a web project based on Javascript technologies are your dependencies.\nThe ecosystem evolves rapidly and your dependencies can quickly become unmaintained, out of fashion or completely overhauled with big breaking changes.\n\nWe therefore try as much as possible to limit our dependencies and avoid adding them unnecessarily.\nA dependency is often very easy to add but it can become a real headache to remove.\n\nThe graphic component libraries (e.g. React bootstrap, Material Design) are a good example of dependencies that we do not want to introduce.\nThey can make integration easier at first, but they often freeze the version of your component library later on.\nYou don‚Äôt want to freeze the React version in your application for two form components.\n\nMonitoring is also part of our dependency management routines.\nSince the addition of reporting security flaws in an NPM package, it is possible to know if a project has a dependency that contains a known security flaw with a simple command.\nSo we have daily jobs on our projects that run the yarn audit command to force us to apply patches.\n\n\n  Dependency maintenance is greatly facilitated by our E2E test stack which sounds the alarm if the version upgrade generates a regression.\n\n\nToday, except for security flaws, we update our dependencies ‚Äúwhen we have time‚Äù, often at the end of sprint.\nWe are not satisfied with this because some dependencies can be forgotten.\nI personally use tools like yarn outdated and Dependabot on my personal projects to automate the update of my dependencies.\n\nAccepting your technical debt\n\nA project will always accumulate technical debt.\nThis is a fact.\nWhether it is voluntary or involuntary debt, a project that resists the years will inevitably accumulate debt.\nEven more so, if during all these years you keep adding features.\n\nSince 2014, our best practices, our ways of doing things have evolved well.\nSometimes we decided these changes but sometimes we underwent them (an example, the arrival of functional components with React and the Hooks api).\n\nOur project is not completely ‚Äústate of art‚Äù and we assume it.\n\n\n\nWe try to prioritize our refactoring topics on the parts of the application on which we have the most concern, the most pain.\nWe consider that a part of the application that we don‚Äôt like but on which we don‚Äôt need to work (bring evolutions) doesn‚Äôt deserve that we refactor it.\n\nI could name many features of our application that have not evolved functionally for several years.\nBut since we have covered these features with E2E tests since the beginning, we didn‚Äôt really have to touch them.\n\nAs said above, the next evolution of a code feature is sometimes its deactivation.\nSo why spend time rewriting the whole application?\n\n\n  In any case, the code becomes ‚Äúlegacy‚Äù.\n  As long as the features are tested, nothing obliges us to refactor everything permanently so that our entire codebase is state of art.\n  We focus on our pain points, we re-factor what we really need to evolve.\n\n\nTo summarize\n\nThe best practices presented here are obviously subjective and will not be perfectly/directly applicable in your contexts.\nHowever, I am convinced that they can probably help you identify what can make your project go from fun to stale.\nAt Bedrock we have other practices in place that I haven‚Äôt listed here but that will be the occasion for a new article sometime.\n\nFinally, if you want me to go into more detail on some of the chapters presented here, don‚Äôt hesitate to tell me, I could try to dedicate a specific article to it.\n\n"
} ,
  
  {
    "title"    : "Bonnes pratiques pour la maintenance d&#39;une application web",
    "category" : "",
    "tags"     : " js, react, web, frontend",
    "url"      : "/2021/09/01/bonnes-pratiques-web.html",
    "date"     : "September 1, 2021",
    "excerpt"  : "\n  Comment ne pas jeter son application tous les deux ans ?\n\n\nRetour d‚Äôexp√©rience bas√© sur les bonnes pratiques appliqu√©es √† la plateforme web d√©velopp√©e chez Bedrock Streaming\n\nUn peu de contexte\n\nChez Bedrock Streaming de nombreuses √©quipes d√©ve...",
  "content"  : "\n  Comment ne pas jeter son application tous les deux ans ?\n\n\nRetour d‚Äôexp√©rience bas√© sur les bonnes pratiques appliqu√©es √† la plateforme web d√©velopp√©e chez Bedrock Streaming\n\nUn peu de contexte\n\nChez Bedrock Streaming de nombreuses √©quipes d√©veloppent et maintiennent des applications frontend pour nos clients et utilisateurs.\nCertaines ne sont pas toute jeune.\nEn effet, l‚Äôapplication sur laquelle je travaille principalement est un site web dont les d√©veloppements ont commenc√© en 2014.\nJe l‚Äôai d‚Äôailleurs d√©j√† √©voqu√©e dans diff√©rents articles de ce blog.\n\n\n\nVous pourriez vous dire: ‚ÄúOh les pauvres maintenir une application vieille de presque 10 ans √ßa doit √™tre un enfer !‚Äù\n\nRassurez-vous, ce n‚Äôest pas le cas !\nJ‚Äôai travaill√© sur des projets bien moins vieux mais sur lesquels le d√©veloppement de nouvelles fonctionnalit√©s √©tait bien plus p√©nible.\n\nAujourd‚Äôhui le projet reste √† jour techniquement, on doit √™tre sur la derni√®re version de React alors que celui-ci avait commenc√© sur une version 0.x.x.\nDans ce monde des technologies web souvent d√©cri√© (ex: les nombreux articles sur la Javascript Fatigue) dont les outils et les pratiques √©voluent constamment, conserver un projet ‚Äú√† jour‚Äù reste un vrai challenge.\n\n\n\nDe plus, dans le contexte de ce projet, en presque 10 ans, nous avons connu une centaine de contributeurs.\nCertains ne sont rest√©s que quelques mois/ann√©es.\nComment garder au maximum la connaissance sur ‚ÄúComment on fait les choses et comment √ßa marche ?‚Äù dans un contexte humain si mouvant ?\n\n\n\nC‚Äôest ce que je vous propose de vous pr√©senter.\n\nAvec l‚Äôaide de mes coll√®gues, j‚Äôai rassembl√© la liste des bonnes pratiques qui nous permettent encore aujourd‚Äôhui de maintenir ce projet en √©tat.\nAvec Florent Dubost, on s‚Äôest souvent dit qu‚Äôil serait int√©ressant de la publier.\nNous esp√®rons que cela vous sera utile.\n\nS‚Äôimposer des r√®gles et les automatiser\n\nUn projet qui r√©siste au temps c‚Äôest tout d‚Äôabord un ensemble de connaissances qu‚Äôon empile les unes sur les autres.\nC‚Äôest en quelque sorte la tour de Kapla que vous assembliez petit en essayant d‚Äôaller le plus haut possible.\nUne base solide sur laquelle on esp√®re pouvoir ajouter le plus possible avant une potentielle chute.\n\nD√®s le d√©but d‚Äôun projet on est donc amen√© √† prendre des d√©cisions importantes sur ‚ÄúComment on souhaite faire les choses ?‚Äù.\nOn pense par exemple √† ‚ÄúQuel format pour nos fichiers ? Comment on nomme telle ou telle chose ?‚Äù\n√âcrire une documentation pr√©cise de ‚ÄúComment on fait les choses‚Äù pourrait paraitre une bonne id√©e.\n\nCependant la documentation c‚Äôest cool, mais √ßa a tendance √† p√©rimer tr√®s vite.\nNos d√©cisions √©voluent mais pas la documentation.\n\n\n  ‚ÄúLes temps changent mais pas les README.‚Äù\n\n  Olivier Mansour (deputy CTO √† Bedrock)\n\n\nAutomatiser la v√©rification de chacune des r√®gles qu‚Äôon s‚Äôimpose (sur notre codebase ou nos process) est bien plus p√©renne.\nPour faire simple, on √©vite dans la mesure du possible de dire ‚ÄúOn devrait faire les choses comme cela‚Äù, et on pr√©f√®re ‚Äúon va coder un truc qui nous le v√©rifie √† notre place‚Äù.\nEn plus de √ßa, cot√© JS on est vraiment bien √©quip√© avec des outils comme Eslint qui nous permettent d‚Äôimpl√©menter nos propres r√®gles.\n\nLe r√©flexe qu‚Äôon essaie donc d‚Äôadopter est donc le suivant:\n\n\n  ‚ÄúOn devrait essayer de faire comme cela √† pr√©sent !‚Äù\n  ‚ÄúOk c‚Äôest int√©ressant, mais comment peut-on s‚Äôassurer qu‚Äôon fasse comme cela automatiquement avec notre CI (Int√©gration continue) ?‚Äù\n\n\nL‚Äôint√©gration continue d‚Äôun projet est la solution parfaite pour ne rien louper sur chacune des Pull Request que nous proposons.\nLes reviews n‚Äôen sont que plus simples car vous n‚Äôavez plus √† vous soucier de l‚Äôensemble des r√®gles qui sont d√©j√† automatis√©es.\nDans ce mod√®le, la review sert donc plus au partage de connaissance qu‚Äôau flicage de typo et autre non respect des conventions du projet.\n\nDans ce principe, il faut donc essayer de bannir les r√®gles orales.\nLe temps des druides est termin√©, s‚Äôil faut transmettre oralement toutes les bonnes pratiques d‚Äôun projet, l‚Äôaccompagnement de nouveaux d√©veloppeurs dans votre √©quipe n‚Äôen sera que plus long.\n\n\n\nUn projet n‚Äôest pas fig√©. Ces r√®gles √©voluent donc avec le temps.\nOn pr√©f√®rera alors l‚Äôajout de r√®gles qui poss√®dent un script qui autofixera toute la codebase intelligemment.\nDe nombreuses r√®gles Eslint le proposent, et cela est vraiment un crit√®re de s√©lection tr√®s important dans nos choix de nouvelles conventions.\n\neslint --fix\n\n\nUne r√®gle tr√®s stricte qui vous obligera √† modifier votre code manuellement avant chaque push est p√©nible √† la longue et √©nervera vos √©quipes.\nAlors qu‚Äôune r√®gle (m√™me tr√®s stricte) qui peut s‚Äôautofixer automatiquement au moment du commit ne sera pas per√ßue comme g√™nante.\n\nComment d√©cider d‚Äôajouter de nouvelles r√®gles ?\n\nCette question peut paraitre √©pineuse, prenons par exemple le cas des &amp;lt;tab&amp;gt; / &amp;lt;space&amp;gt; dans les fichiers.\nPour cela, on essaie d‚Äô√©viter les d√©bats sempiternels et on se plie √† la tendance et aux r√®gles de la communaut√©.\nPar exemple, notre base de configuration Eslint) est bas√©e sur celle d‚ÄôAirbnb qui semble avoir un certain succ√®s dans la communaut√© JS.\nMais si la r√®gle qu‚Äôon souhaite s‚Äôimposer n‚Äôest pas disponible dans Eslint ou d‚Äôautres outils, il nous arrive de pr√©f√©rer ne pas suivre la r√®gle plut√¥t que de se dire ‚ÄúOn le fait sans CI qui v√©rifie‚Äù.\n\nLa liste presque exhaustive ü§û\n\n\n\n\n  Le format des fichiers est suivi g√©r√© par Editorconfig, prettier et Eslint.\nNous avons opensourc√© notre propre configuration, si jamais celle-ci peut vous √™tre utile.\n  Nous utilisons un nommage de commit bien sp√©cifique pour g√©n√©rer nos changelog.\nPour s‚Äôassurer que les devs le respectent, une simple √©tape de notre CI le v√©rifie.\n  On ne souhaite pas qu‚Äôun dev fasse grossir √©norm√©ment nos bundles JS en production, c‚Äôest pourquoi nous suivons et mesurons leur taille dans la CI.\nOn utilise un outil maison mais on peut vous recommander l‚Äôoutil BuildTracker.\n  La couverture de tests n‚Äôest pas un indicateur pour l‚Äô√©quipe, toutes les lignes n‚Äôont pas la m√™me n√©cessit√© pour nous d‚Äô√™tre test√©es.\nCertaines √©quipes √† Bedrock suivent cependant cet indicateur qui a au moins l‚Äôint√©r√™t de donner une tendance.\n  Nos tests unitaires tournent bien √©videmment sur la CI, ceux-ci doivent passer.\n  Nos tests fonctionnels (End to end: E2E) tournent sur Chrome Headless, ils doivent √™tre au vert.\n  Les logs de nos tests E2E sont r√©cup√©r√©s et pars√©s afin d‚Äô√©viter l‚Äôintroduction d‚Äôerreur ou de React warning (Le script de parsing est cependant compliqu√© √† maintenir)\n  Les tests fonctionnels fonctionnent dans une sandbox o√π tout le r√©seau est proxyfi√©.\nNous surveillons que nos tests ne d√©pendent pas d‚Äôune API non mock√©e qui pourrait ralentir leur ex√©cution.\n  Durant les tests E2E nous v√©rifions qu‚Äôaucune requ√™te d‚Äôimage n‚Äôa g√©n√©r√© une 404.\n  On r√©alise quelques v√©rifications d‚Äôaccessibilit√© avec Axe durant nos tests E2E.\n  On v√©rifie quelques r√®gles sur le CSS avec Stylelint et bemlinter (on n‚Äôutilise plus BEM aujourd‚Äôhui mais il reste encore un peu de style g√©r√© en SCSS qu‚Äôon migre petit √† petit en StyledComponent)\n  Le projet est un monorepo sur lequel nous essayons de maintenir les m√™mes versions de d√©pendances pour chaque package.\nPour cela nous avons d√©velopp√© un outil qui permet de faire cette v√©rification monorepo-dependencies-check\n  On v√©rifie que notre fichier yarn.lock n‚Äôa pas √©t√© modifi√© par inadvertance ou bien qu‚Äôil a √©t√© mis √† jour par rapport aux modifications du package.json.\n  Terraform est utilis√© pour la gestion de nos ressources cloud, nous v√©rifions que le format des fichiers est correct.\n\n\nTester, tester, tester\n\nJ‚Äôesp√®re qu‚Äôen 2021 il n‚Äôest plus n√©cessaire d‚Äôexpliquer pourquoi tester automatiquement son application est indispensable pour la rendre p√©renne.\nEn JS on est plut√¥t bien √©quip√© en terme d‚Äôoutils pour tester aujourd‚Äôhui.\nIl reste cependant l‚Äô√©ternelle question:\n\n\n  ‚ÄúQu‚Äôest-ce qu‚Äôon veut tester ?‚Äù\n\n\nGlobalement si on recherche sur internet cette question, on voit que des besoins diff√©rents font √©merger des pratiques et des outils de testing bien diff√©rents.\nCe serait tr√®s pr√©somptueux de penser qu‚Äôil y a une bonne mani√®re de tester automatiquement son application.\nC‚Äôest pourquoi il est pr√©f√©rable de d√©finir une ou plusieurs strat√©gies de test qui r√©pondent √† des besoins d√©finis et limit√©s.\n\nNos strat√©gies de tests reposent sur deux volont√©s bien distinctes:\n\n\n  Automatiser la v√©rification des fonctionnalit√©s propos√©es aux utilisateurs en se mettant √† sa place.\n  Nous fournir des solutions efficaces pour specifier la mani√®re dont nous impl√©mentons nos solutions techniques pour nous permettre de les faire √©voluer plus facilement.\n\n\nPour cela, nous r√©alisons deux ‚Äútypes de tests‚Äù que je propose de vous pr√©senter ici.\n\nNos tests E2E\n\nOn les appelle ‚Äútests fonctionels‚Äù, ce sont des tests End-to-end (E2E) sur une stack technique tr√®s efficace compos√©e de CucumberJS, WebdriverIO avec ChromeHeadless\nIl s‚Äôagit d‚Äôune stack technique mise en place au d√©but du projet (√† l‚Äô√©poque avec PhantomJS pour les plus anciens d‚Äôentre-vous)\n\nCette stack nous permet d‚Äôautomatiser le pilotage de tests qui contr√¥lent un navigateur.\nCe navigateur va r√©aliser des actions qui se rapprochent le plus de celles que nos vrais utilisateurs peuvent faire tout en v√©rifiant comment le site r√©agit.\n\nIl y a quelques ann√©es, cette stack technique √©tait plut√¥t compliqu√©e √† mettre en place, mais aujourd‚Äôhui il est plut√¥t simple de le faire.\nLe site qui h√©berge cet article de blog en est lui-m√™me la preuve.\nIl ne m‚Äôa fallu qu‚Äôune dizaine de minutes pour mettre en place cette stack avec le WebdriverIo CLI pour v√©rifier que mon blog fonctionne comme pr√©vu.\n\nJ‚Äôai d‚Äôailleurs r√©cemment publi√© un article pr√©sentant la mise en place de cette stack.\n\nVoici donc un exemple de fichier de test E2E pour vous donner une id√©e:\n\nFeature: Playground\n\n  Background: Playground context\n    Given I use &quot;playground&quot; test context\n\n  Scenario: Check if playground is reachable\n    When As user &quot;toto@toto.fr&quot; I visit the &quot;playground&quot; page\n    And I click on &quot;playground trigger&quot;\n    Then I should see a &quot;visible playground&quot;\n    And I should see 4 &quot;playground tab&quot; in &quot;playground&quot;\n\n    When I click on &quot;playground trigger&quot;\n    Then I should not see a &quot;visible playground&quot;\n\n    # ...\n\n\nEt √ßa donne √ßa en local avec mon navigateur Chrome !\n\n\n\nVoil√† un sch√©ma qui explique comment cette stack fonctionne:\n\n\n\nAujourd‚Äôhui, l‚Äôapplication web de Bedrock poss√®de plus de 800 sc√©narios de tests E2E qui tournent sur chacune de nos Pull Request et sur la branche master.\nIls nous assurent que nous n‚Äôintroduisons pas de r√©gression fonctionnelle et c‚Äôest juste g√©nial !\n\nüëç Les points positifs\n\n\n  WebdriverIO nous permet √©galement de lancer de mani√®re journali√®re ces m√™mes tests sur des vrais devices en passant par le service payant SAAS Browserstack.\nOn a donc tous les jours un job qui s‚Äôassure que notre site fonctionne correctement sur un Chrome derni√®re version sur Windows 10 et Safari sur MacOs.\n  Ces tests nous permettent de facilement documenter les fonctionnalit√©s de l‚Äôapplication gr√¢ce au langage Gherkin.\n  Ils nous permettent de reproduire des cas qui sont loin d‚Äô√™tre nominaux.\nDans une logique TDD, ils permettent d‚Äôavancer sur le d√©veloppement sans avoir √† cliquer pendant des heures.\n  Ces tests nous ont permis de ne pas casser l‚Äôancienne version du site qui est toujours en production pour quelques clients alors que nos efforts se concentrent sur la nouvelle.\n  Ils nous apportent une vraie confiance.\n  Gr√¢ce notre librairie superagent-mock, nous pouvons fixturer (bouchonner, mocker) toutes les API dont on d√©pend et ainsi m√™me v√©rifier les cas d‚Äôerreurs.\nDe plus, mocker la couche XHR du navigateur permet une am√©lioration significative du temps d‚Äôex√©cution des tests. üöÄ\n  Ils nous donne acc√®s √† des usages √©tendus comme :\n    \n      v√©rification de r√®gles d‚Äôaccessibilit√©\n      check les logs de la console navigateur (pour ne pas introduire d‚Äôerreur ou de React Warning par exemple)\n      surveiller tous les appels r√©seaux du site gr√¢ce √† un proxy\n      et j‚Äôen passe‚Ä¶\n    \n  \n\n\nüëé Les complications\n\n\n  Maintenir cette stack est compliqu√© et co√ªteux.\n√âtant donn√© que peu de ressources sont publi√©es sur ce domaine, on se retrouve parfois √† devoir creuser pendant plusieurs jours pour les r√©parer üòÖ.\nIl nous arrive de nous sentir parfois bien seul √† avoir ces soucis.\n  Il est tr√®s facile de coder un test E2E dit flaky (ie: un test qui peut √©chouer al√©atoirement).\nIls nous font croire que quelque chose est cass√©.\nIls nous prennent parfois du temps √† les stabiliser.\nIl reste cependant bien meilleur de supprimer un test qui ne vous donnera pas un r√©sultat stable.\n  Faire tourner tous les tests prend un temps important sur notre int√©gration continue.\nIl faut r√©guli√®rement travailler sur leur optimisation pour que le feedback qu‚Äôils vous apportent soit le plus rapide possible.\nCes temps importants coutent √©galement de l‚Äôargent, il faut en effet bien faire tourner ces tests sur des machines.\nPour information, l‚Äôinfrastructure du site web (√† lui seul, juste l‚Äôh√©bergement de nos servers Node + fichiers statiques + CDN) coutent bien moins cher que notre int√©gration continue.\nCela fait bien √©videmment sourire nos Ops ! üòä\n  Les nouvelles recrues de nos √©quipes n‚Äôont souvent jamais r√©alis√© ce genre de tests, il y a donc une phase de gal√®re d‚Äôapprentissage..\n  Certaines fonctionnalit√©s sont parfois trop compliqu√©es √† tester avec notre stack E2E (par exemple, les parcours de paiement qui d√©pendent de tiers).\nIl nous arrive alors de nous rabattre sur d‚Äôautres techniques avec Jest notamment en ayant un scope moins unitaire.\n\n\nNos tests ‚Äúunitaires‚Äù\n\nPour compl√©ter nos tests fonctionnels nous avons √©galement une stack de tests √©crits avec Jest.\nOn qualifie ces tests d‚Äôunitaires car nous avons comme principe d‚Äôessayer de toujours tester nos modules JS en ind√©pendance des autres.\n\nNe d√©battons pas ici sur ‚ÄúEst-ce que ce sont des vrais tests unitaires ?‚Äù, suffisamment d‚Äôarticles sur internet traitent de ce sujet.\n\nOn utilise ces tests pour diff√©rentes raisons qui couvrent des besoins que nos tests fonctionnels ne couvrent pas:\n\n\n  nous aider √† d√©velopper nos modules JS avec des pratiques TDD.\n  documenter et d√©crire comment fonctionne un module JS.\n  tester des cas limites tr√®s/trop compliqu√©s √† tester avec nos tests E2E.\n  faciliter le refactoring de notre application en nous montrant les impacts techniques de nos modifications.\n\n\nAvec ces tests, on se met au niveau d‚Äôune fonction utilitaire, d‚Äôune action Redux, d‚Äôun reducer, d‚Äôun composant React.\nOn se base essentiellement sur la fonctionnalit√© d‚Äôautomock de Jest qui nous propose d‚Äôisoler nos modules JS lorsqu‚Äôon teste.\n\n\n\nL‚Äôimage pr√©c√©dente repr√©sente la m√©taphore qui nous permet d‚Äôexpliquer notre strat√©gie de tests unitaires aux nouveaux arrivant.\n\n\n  ‚ÄúIl faut s‚Äôimaginer que l‚Äôapplication est un mur compos√© de briques unitaires (nos modules ecmascript), nos tests unitaires doivent tester une √† une les briques en ind√©pendance totale des autres.\nNos tests fonctionnels sont l√† pour tester le ciment entre les briques.‚Äù\n\n\nPour r√©sumer, on pourrait dire que nos tests E2E testent ce que notre application doit faire, et nos tests unitaires s‚Äôassurent eux de v√©rifier comment √ßa marche.\n\nAujourd‚Äôhui ce sont plus de 6000 tests unitaires qui couvrent l‚Äôapplication et permettent de limiter les r√©gressions.\n\nüëç\n\n\n  Jest est vraiment une librairie g√©niale, rapide, compl√®te, bien document√©e.\n  Les tests unitaires nous aident beaucoup √† comprendre plusieurs ann√©es apr√®s comment tout cela fonctionne.\n  On arrive toujours √† tester unitairement notre code, et cela compl√®te bien nos tests E2E.\n  L‚Äôautomock est vraiment pratique pour le d√©coupage de tests par modules.\n\n\nüëé\n\n\n  Parfois, nous nous sommes trouv√©s limit√©s par notre stack de tests E2E et nous ne pouvions pas uniquement nous baser sur les tests unitaires.\nIl nous manquait quelque chose pour pouvoir s‚Äôassurer que le ciment entre les briques fonctionnait comme on le souhaitait.\nPour cela, il a √©t√© mis en place une deuxi√®me stack de tests Jest nomm√© ‚Äútest d‚Äôint√©gration‚Äù ou l‚Äôautomock est d√©sactiv√©.\n  L‚Äôabus de Snapshot est dangereux pour la sant√©.\nL‚Äôusage du ‚ÄúSnapshot testing‚Äù peut faire gagner du temps sur l‚Äôimpl√©mentation de vos tests mais peuvent en r√©duire la qualit√©.\nAvoir √† review un object de 50 lignes en Snapshot est ni facile, ni pertinent.\n  Avec la d√©pr√©ciation d‚ÄôEnzymeJS, nous sommes contraints de migrer sur React Testing Library.\nIl est bien √©videmment possible de tester unitairement des composants avec cette nouvelle librairie.\nMalheureusement, ce n‚Äôest pas vraiment l‚Äôesprit et la fa√ßon de faire.\nReact Testing Library nous pousse √† ne pas jouer avec le shallow rendering.\n\n\nNos principes\n\nNous essayons de toujours respecter les r√®gles suivantes lorsqu‚Äôon se pose la question ‚ÄúDois-je ajouter des tests ?‚Äù.\n\n\n  Si notre Pull Request introduit des nouvelles fonctionnalit√©s utilisateurs, il faut int√©grer des scenarios de test E2E.\nDes tests unitaires avec Jest peuvent les compl√©ter / remplacer en fonction.\n  Si notre Pull Request a pour but de corriger un bug, cela signifie qu‚Äôil nous manque un cas de test.\nOn doit donc essayer de rajouter un test E2E ou √† d√©faut un test unitaire.\n\n\nC‚Äôest en √©crivant ces lignes que je me dis que ces principes pourraient tr√®s bien faire l‚Äôobjet d‚Äôune automatisation. ü§£\n\nLe projet reste, les fonctionnalit√©s non\n\n\n  ‚ÄúLa seconde √©volution d‚Äôune fonctionnalit√© est tr√®s souvent sa suppression.‚Äù\n\n\nPar principe, nous souhaitons faire en sorte que chaque nouvelle fonctionnalit√© de l‚Äôapplication ne base pas son activation sur le simple fait d‚Äô√™tre dans la codebase.\nClassiquement, le cycle de vie d‚Äôune ‚Äúfeature‚Äù dans un projet peut √™tre le suivant (dans un Github Flow):\n\n\n  une personne impl√©mente sur une branche\n  la fonctionnalit√© est merg√©e sur master\n  elle est d√©ploy√©e en production\n  vis sa vie de fonctionnalit√© (avec parfois des bugs et des correctifs)\n  la fonctionnalit√© n‚Äôest plus n√©cessaire\n  une personne d√©tricote le code et l‚Äôenl√®ve\n  nouveau d√©ploiement\n\n\nPour simplifier certaines √©tapes, il a √©t√© mis en place du feature flipping sur le projet.\n\nComment √ßa marche ?\n\nDans notre config il y a une map cl√©/valeur qui liste toutes les fonctionnalit√©s de l‚Äôapplication associ√©es √† leur statut d‚Äôactivation.\n\nconst featureFlipping = {\n  myAwesomeFeature: false,\n  anotherOne: true,\n}\n\n\nDans notre code, nous avons donc impl√©ment√© des traitements conditionnels qui disent ‚ÄúSi cette feature est activ√©e alors‚Ä¶‚Äù.\nCela peut changer le rendu d‚Äôun composant, changer l‚Äôimpl√©mentation d‚Äôune action Redux ou bien d√©sactiver une route de notre react-router.\n\nMais √† quoi √ßa sert ?\n\n\n  On peut d√©velopper des nouvelles √©volutions progressivement en les cachant derri√®re une cl√© de configuration.\nOn livre des fonctionnalit√©s en production sans les activer.\n  En environnement de test, on peut surcharger cette config pour tester des features qui ne sont pas encore activ√©es en production.\n  Dans le cas d‚Äôun site en marque blanche, on peut proposer ces fonctionnalit√©s √† nos clients comme des options possibles.\n  Avant de supprimer le code d‚Äôune feature, on la d√©sactive puis on fait le m√©nage sans risque.\n  Gr√¢ce √† un outil maison nomm√© l‚ÄôApplaunch, cette config de feature flipping est surchargeable dans une interface graphique √† chaud sans d√©ploiement.\nCela nous permet d‚Äôactiver des fonctionnalit√©s sans faire de mise en production du code.\nEn cas d‚Äôincident, on peut d√©sactiver des fonctionnalit√©s qui sont d√©grad√©es.\n\n\nPour vous donner un exemple plus concret, entre 2018 et 2020 nous avons compl√®tement refondu l‚Äôinterface de l‚Äôapplication.\nCette √©volution graphique n‚Äô√©tait qu‚Äôune cl√© de featureFlipping.\nLa refonte graphique n‚Äôa donc pas √©t√© la remise √† z√©ro du projet, on continue encore aujourd‚Äôhui de vivre avec les deux versions (tant que la bascule de tous nos clients n‚Äôest pas termin√©e).\n\n\n\nL‚ÄôA/B testing\n\nGr√¢ce au super travail des √©quipes backend et data, on a pu m√™me √©tendre l‚Äôusage du feature flipping en rendant cette configuration modifiable pour des sous groupes d‚Äôutilisateurs.\n\nCela permet de d√©ployer des nouvelles fonctionnalit√©s sur une portion plus r√©duite des utilisateurs afin de comparer nos KPI.\n\nPrise de d√©cision, am√©lioration des performances techniques ou produit, exp√©rimentations, les possibilit√©s sont nombreuses et nous les exploitons de plus en plus.\n\nLe futur flipping\n\n\n  Sur une id√©e originale de Florent Lepretre.\n\n\nNous avions r√©guli√®rement le besoin d‚Äôactiver des feature √† des heures tr√®s trop matinales dans le futur.\nPour cela nous devions √™tre connect√© √† une heure pr√©cise sur notre poste pour modifier la configuration √† chaud.\n\nAfin d‚Äô√©viter d‚Äôoublier de le faire, ou de le faire en retard, nous avons fait en sorte qu‚Äôune cl√© de configuration puisse √™tre activ√©e √† partir d‚Äôune certaine date.\nPour cela, nous avons fait √©voluer notre selector redux qui indiquait si une feature √©tait activ√©e pour qu‚Äôil puisse g√©rer des formats de date et les comparer √† l‚Äôheure courante.\n\nconst featureFlipping = {\n  myAwesomeFeature: {\n    offDate: &#39;2021-07-12 20:30:00&#39;,\n    onDate: &#39;2021-07-12 19:30:00&#39;,\n  },\n}\n\n\n\n  De nombreux caf√©s ‚òïÔ∏è √† 9h ont √©t√© sauv√©s gr√¢ce au futur flipping\n\n\nMonitorer, Mesurer, Alerter\n\nPour maintenir un projet aussi longtemps que l‚Äôapplication web de bedrock, des tests, de la documentation et de la rigueur ne suffisent pas.\nIl faut √©galement de la visibilit√© sur ce qui marche en production.\n\n\n  ‚ÄúComment sais-tu que l‚Äôapplication que tu as en production en ce moment m√™me fonctionne comme pr√©vu ?‚Äù\n\n\nOn part du principe qu‚Äôaucune fonctionnalit√© ne marche tant qu‚Äôelle n‚Äôest pas monitor√©e.\nAujourd‚Äôhui le monitoring √† Bedrock cot√© Frontend se mat√©rialise par diff√©rents outils et diff√©rentes stacks.\nJe pourrais vous citer NewRelic, un Statsd, une stack ELK ou bien encore Youbora pour la vid√©o.\n\nPour vous donner un exemple, √† chaque fois qu‚Äôun utilisateur commence une session de navigation on envoie un Hit de monitoring anonyme pour incr√©menter un compteur dans Statsd.\nOn a alors plus qu‚Äô√† d√©finir un dashboard qui affiche dans un graphique l‚Äô√©volution de ce nombre.\nSi on observe une variation trop importante, cela peut nous permettre de d√©tecter un incident.\n\n\n\nLe monitoring nous offre aussi des solutions pour comprendre et analyser un bug qui s‚Äôest produit dans le pass√©.\nComprendre un incident, l‚Äôexpliquer, en trouver sa root cause sont les possibilit√©s qui s‚Äôoffrent √† vous si vous monitorez votre application.\nLe monitoring peut √©galement permettre de mieux communiquer avec les clients sur les impacts d‚Äôun incident et √©galement d‚Äôestimer le nombre d‚Äôutilisateurs impact√©s.\n\nAvec la multiplication de nos clients, bien monitorer nos plateformes n‚Äôest plus suffisant.\nTrop de donn√©es, trop de dashboards √† surveiller, il devient tr√®s facile de louper quelque chose.\nNous avons donc commenc√© √† compl√©ter notre suivi des mesures par de l‚Äôalerting automatique.\nUne fois que les mesures nous apportent suffisamment de confiance, on peut facilement mettre en place des alertes qui vont nous pr√©venir en cas de valeur incoh√©rente.\n\nNous essayons cependant de toujours d√©clencher des alertes uniquement quand celle-ci est actionnable.\nDans d‚Äôautres termes, si une alerte sonne, nous avons quelque chose √† faire.\nFaire sonner des alertes qui ne n√©cessitent aucune action imm√©diate humaine g√©n√®rent du bruit et de la perte de temps.\n\n\n\nLimiter, surveiller et mettre √† jour ses d√©pendances\n\nCe qui p√©rime plus vite que votre ombre dans un projet web bas√© sur des technologies javascript, ce sont vos d√©pendances.\nL‚Äô√©cosyst√®me √©volue rapidement et vos d√©pendances peuvent vite se retrouver non maintenues, plus √† la mode ou bien compl√®tement refondues avec de gros breaking changes.\n\nOn essaye donc dans la mesure du possible de limiter nos d√©pendances et d‚Äô√©viter d‚Äôen ajouter inutilement.\nUne d√©pendance, c‚Äôest souvent tr√®s facile √† ajouter mais elle peut devenir un vrai casse-t√™te √† enlever.\n\nLes librairies de composants graphiques (exemple React bootstrap, Material Design) sont un bel exemple de d√©pendance que nous tenons √† ne pas introduire.\nElles peuvent faciliter l‚Äôint√©gration dans un premier temps mais celles-ci bloquent souvent la version de votre librairie de composant par la suite.\nVous ne voulez pas figer la version de React dans votre application pour deux composants de formulaires.\n\nLa surveillance fait aussi partie de nos routines de gestion de nos d√©pendances.\nDepuis l‚Äôajout du signalement de failles de s√©curit√© dans un package NPM, il est possible de savoir si un projet int√®gre une d√©pendance qui contient une faille de s√©curit√© connue par une simple commande.\nNous avons donc des jobs journaliers sur nos projets qui lancent la commande yarn audit afin de nous forcer √† appliquer les correctifs.\n\n\n  La maintenance de d√©pendances est grandement facilit√© par notre stack de tests E2E qui sonnent direcement si la mont√©e de version g√©n√®re une regression.\n\n\nAujourd‚Äôhui, hors failles de s√©curit√©, nous mettons √† jour nos d√©pendances ‚Äúquand on a le temps‚Äù, souvent en fin de sprint.\nCela ne nous satisfait pas car certaines d√©pendances peuvent se retrouver oubli√©es.\nJ‚Äôai personnellement l‚Äôhabitude d‚Äôutiliser des outils comme yarn outdated et Dependabot sur mes projets personels pour automatiser la mise √† jour de mes d√©pendances.\n\nAccepter sa dette technique\n\nUn projet accumulera toujours de la dette technique.\nC‚Äôest un fait.\nQue ce soit de la dette volontaire ou involontaire, un projet qui r√©siste aux ann√©es va forc√©ment accumuler de la dette.\nD‚Äôautant plus, si pendant toutes ces ann√©es vous continuez d‚Äôajouter des fonctionnalit√©s.\n\nDepuis 2014, nos bonnes pratiques, nos fa√ßons de faire ont bien √©volu√©.\nParfois nous avons d√©cid√© ces changements mais parfois nous les avons subi (un exemple, l‚Äôarriv√©e des composants fonctionnels avec React et l‚Äôapi des Hooks).\n\nNotre projet n‚Äôest pas compl√®tement ‚Äústate of art‚Äù et on l‚Äôassume.\n\n\n\nNous essayons de prioriser nos sujets de refactoring sur les parties de l‚Äôapplication sur lequel on a le plus de souci, le plus de peine.\nOn consid√®re qu‚Äôune partie de l‚Äôapplication qui ne nous pla√Æt pas mais sur laquelle on n‚Äôa pas besoin de travailler (apporter des √©volutions) ne m√©rite pas qu‚Äôon la refactorise.\n\nJe pourrais vous citer de nombreuses fonctionnalit√©s de notre application qui n‚Äôont pas √©volu√© fonctionnellement depuis plusieurs ann√©es.\nMais comme nous avons couvert ces fonctionnalit√©s de tests E2E depuis le d√©but, nous n‚Äôavons pas vraiment eu √† y retoucher.\n\nComme dit plus haut, la prochaine √©volution d‚Äôune feature de code est parfois sa d√©sactivation.\nAlors pourquoi passer son temps √† r√©-√©crire toute l‚Äôapplication ?\n\n\n  Le code devient dans tous les cas du ‚Äúlegacy‚Äù.\n  Tant que les fonctionnalit√©s sont test√©es, rien ne nous oblige √† tout refactorer en permanence pour que toute notre codebase soit state of art.\n  On se focalise sur nos pain points, on re-factorise ce qu‚Äôon a vraiment besoin de faire √©voluer.\n\n\nPour r√©sumer\n\nLes bonnes pratiques pr√©sent√©es ici restent bien √©videmment subjectives et ne s‚Äôappliqueront pas parfaitement/directement dans vos contextes.\nJe suis cependant convaincu qu‚Äôelles peuvent probablement vous aider √† identifier ce qui peut faire passer votre projet de fun √† p√©rim√©.\n√Ä Bedrock nous avons mis en place d‚Äôautres pratiques que je n‚Äôai pas list√©es ici mais ce sera l‚Äôoccasion de faire un nouvel article un jour.\n\nEnfin, si vous souhaitez que je revienne plus en d√©tail sur certains chapitres pr√©sent√©s ici, n‚Äôh√©sitez pas √† me le dire, je pourrais essayer d‚Äôy d√©dier un article sp√©cifique.\n\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #14",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/07/16/bedrock-dev-facts-14.html",
    "date"     : "July 16, 2021",
    "excerpt"  : "Apportez vos devfacts √† la plage cet √©t√©!\n\nLes phases de test\n\n  Pour une fois qu‚Äôon est pay√© √† faire des 500\n\n\nLe panier de crabes\n\n  _ On attend le GoNoGo\n\n  _ On ne pourrait pas attendre le RustNoRust ce serait plus performant\n\n\n1‚Ä¶ 2‚Ä¶ 3‚Ä¶ extinc...",
  "content"  : "Apportez vos devfacts √† la plage cet √©t√©!\n\nLes phases de test\n\n  Pour une fois qu‚Äôon est pay√© √† faire des 500\n\n\nLe panier de crabes\n\n  _ On attend le GoNoGo\n\n  _ On ne pourrait pas attendre le RustNoRust ce serait plus performant\n\n\n1‚Ä¶ 2‚Ä¶ 3‚Ä¶ extincteurs\n\n  Parlant √† un h√©bergeur cloud\n\n  ‚ÄúTu sais la petite boite fran√ßaise en trois lettres qui a eu chaud ce matin?‚Äù\n\n\n√âchec et mat\n\n  We‚Äôve been hacked by our incompetence\n\n\nLe vrai du faux\n\n  C‚Äôest pas le bon true\n\n\nDarladidadada\nQuand tu fais du m√©nage dans ton compte AWS de tests et que tu te dis qu&amp;#39;un coll√®gue doit avoir l&amp;#39;oreille musicale üé∂ pic.twitter.com/yIINlboQBy&amp;mdash; Pascal MARTIN (@pascal_martin) March 12, 2021\n\nReview in progress\n\n\nIPV6 c‚Äôest pas encore hyper clair\n\n  Oui mais c‚Äôest une grosse IP\n\n\nLa confiance\n\n  Normalement, si tout va bien, √ßa marche!\n\n\nAlors cette mise en production ?\n\n  Elle s‚Äôest bien pass√©e mais il a fallu revert\n\n\nEt oui mon petit\n\n  L‚ÄôInternet, c‚Äô√©tait nous\n\n\nUne API c‚Äôest pas si mal en fait\n\n  Quand je vois l‚ÄôUX de la console AWS, je me demande s‚Äôil n‚Äôy a pas une v√©ritable volont√© √† nous pousser √† faire de l‚Äôinfrastructure as code\n\n\nDe la part d‚Äôun dev backend Php üò±\n\n  J‚Äôaime mettre du javascript partout\n\n\nLe retour du come-back de l‚Äôautomatisation manuelle\n\n  On pourra faire un rattrapage √† moiti√© manuel, mais quand m√™me automatique\n\n\nDans les m√©andres des fonctionnalit√©s de Airtable\n\n  Bon, on va faire simple, on va √©crire du JS\n\n\nUn mercredi √† 15h55\n\n  Cette phrase est trop complexe pour un lundi matin\n\n\n√Ä bicyclette üö≤\n\n  Le client il s‚Äôen fout que tu mettes des petites roues sur ton v√©lo, il veut juste que t‚Äôavances sans te p√©ter la gueule\n\n\nEntendu dans une r√©union √† distance\n\n  Rassurez-vous c‚Äôest pas moi qui ronfle c‚Äôest mon chat\n\n\nEn parlant de choix techniques\n\n  Attention le choix pris aujourd‚Äôhui fera jurisprudence sur la suite\n\n\nC‚Äôest g√©r√© !\n\n  Je crois que l‚Äôon g√®re cette erreur, m√™me si on ne la comprend pas\n\n\n1 + 1 = 11\n\n  30 est un nombre √©norme\n\n\nEntendu juste apr√®s une grosse MEP\n\n  Oh! Il faut que je mette en place un mail de vacances.\n\n\nDes frites üçü\n\n  Mes crit√®res de succ√®s, c‚Äôest les grosses patates\n\n\nPO, pas un m√©tier facile\n\n  Le dev: ‚ÄúEst-ce que les PO peuvent le faire ?‚Äù\n\n  Le PO: ‚ÄúOn ne sait rien faire nous‚Äù\n\n\nBougez pas\n\n  Je d√©ploie le deployer\n\n\nUne l√©gende raconte qu‚Äôil faut d√©ployer deux fois le deployer\n\nUne petite √©quipe\n\n  On √©tait 80 stagiaires sur le projet\n\n\nüé∞\n\n  La magie du chmod -R 777. Je te dirais pas que j‚Äôen ai rajout√© dans mes 3 derni√®res PR pour fixer de la CI\n\n\nIterm 2 secret features\nMa recherche Google:\n\n  Iterm2 big sur blinking\n\n\nGitlab issue reponse :\n\n  Just turn on Prefs &amp;gt; Advanced &amp;gt; Work around Big Sur bug where a white line flashes at the top of the screen in full screen mode.\n\n\nLes quadricolors\n\n  ‚Äú_ Pourquoi avoir choisi du vert pour la doc ?‚Äù\n\n  ‚Äú_ √âcoute je suis daltonien.‚Äù\n\n\nPourquoi douter ?\n\n  ‚Äú_ Je ne sais pas comment valider fonctionnellement mon d√©veloppement.‚Äù\n\n  ‚Äú_ Ben, pareil que tu l‚Äôas test√©.‚Äù\n\n  ‚Äú_ Je ne l‚Äôai pas test√©.‚Äù\n\n  ‚Äú_ üòÖ‚Äù\n\n\nTimber !!!!\n\n  Dans la vie comme dans git tu peux pas supprimer la branch sur laquelle t‚Äôes.\n\n\nC‚Äôest du propre\n\n  La refacto, c‚Äôest comme le m√©nage, il faut en faire r√©guli√®rement.\n\n\nLes conflits de canard\n\n  C‚Äôest git checkout verdun ton rebase\n\n\nReconversion\n\n  De toute mani√®re avec le r√©chauffement climatique, je vais tout plaquer et monter un business de vente de djellaba.\n\n\nIncident sur la ligne B\n\n  Parfois les r√©unions c‚Äôest comme les transports, tu es coinc√© dedans et √ßa n‚Äôavance pas\n\n"
} ,
  
  {
    "title"    : "No-code, ou le d√©veloppement d‚Äôapplications ouvert √† d‚Äôautres m√©tiers !",
    "category" : "",
    "tags"     : " conference, nocode, lowcode, afup",
    "url"      : "/2021/06/11/no-code-developpement-applications-ouvert-autres-metiers-pascal-martin.html",
    "date"     : "June 11, 2021",
    "excerpt"  : "Construire une application sans coder¬†? C‚Äôest une id√©e que j‚Äôentends depuis le d√©but de mes √©tudes‚Ä¶ Et c‚Äôest la promesse de no-code¬†!\nD‚Äôailleurs, pendant que des discours d√©clarent que nos enfants doivent apprendre √† coder √† l‚Äô√©cole, nous √©crivons...",
  "content"  : "Construire une application sans coder¬†? C‚Äôest une id√©e que j‚Äôentends depuis le d√©but de mes √©tudes‚Ä¶ Et c‚Äôest la promesse de no-code¬†!\nD‚Äôailleurs, pendant que des discours d√©clarent que nos enfants doivent apprendre √† coder √† l‚Äô√©cole, nous √©crivons nous-m√™me d√©j√† des applications no-code¬†! N‚Äôavez-vous pas lanc√© Excel r√©cemment¬†?\n\nCes derni√®res ann√©es, l‚Äôapproche no-code a √©volu√© et devient petit √† petit un concept viable.\nDes entreprises, startups ou mastodontes, se lancent sur ce march√© et publient des outils et solutions qui aident √† rivaliser avec certaines des applications que nous aurions pu d√©velopper nous-m√™me‚Ä¶\nJe ne parle bien s√ªr pas (encore?) de supprimer nos m√©tiers‚Ä¶ Mais est-ce que no-code ou low-code ne permettraient pas √† d‚Äôautres profils que les n√¥tres d‚Äôavancer plus rapidement sur leurs projets¬†?\n\n√Ä travers cette introduction, vous d√©couvrirez un pan de l‚Äôapproche no-code et j‚Äôesp√®re vous montrer que le d√©veloppement d‚Äôapplications n‚Äôest plus r√©serv√© qu‚Äôaux d√©veloppeurs‚Ä¶ Et que nos langages pr√©f√©r√©s ne sont plus la r√©ponse √† toutes les questions¬†!\n"
} ,
  
  {
    "title"    : "The organisational challenge of building a Data team: lessons learnt",
    "category" : "Data",
    "tags"     : " Data, Data Science, Data Engineering, Agile, BigData, Organization",
    "url"      : "/data/2021/05/19/organisational-challenge-building-data-team.html",
    "date"     : "May 19, 2021",
    "excerpt"  : "Disclaimer: this is a post I‚Äôve been meaning to share ever since the Dailymotion team published this post, so it‚Äôs going way back into our history but updated to our current status. It‚Äôs also an attempt to bring a counterpart to the blog posts of ...",
  "content"  : "Disclaimer: this is a post I‚Äôve been meaning to share ever since the Dailymotion team published this post, so it‚Äôs going way back into our history but updated to our current status. It‚Äôs also an attempt to bring a counterpart to the blog posts of many web leaders that have written about what worked but rarely detail the difficulties they met and the lessons they learnt.\nHere‚Äôs a very personal (therefore biased) feedback of our story and the lessons we learnt down the road.\n\nAt Bedrock we‚Äôre building the best streaming platform in Europe to help our customers be local streaming champions.\nWithin Bedrock (and before that within M6 where our team was born), we embraced Data in 2016. From a top management perspective the move was straight forward once the company had understood the strategic importance of Data and the major use cases we could address: staff a team and deliver massive value (since Data was definitely the new oil back in those days). But for the teams we built (now north of 40 people), it was another story.\n\nFirst iteration: the Dream Team\n\n\n\nThe first step M6 took was to hire an amazing, PhD qualified, Business focused, Data Scientist whose mission was to build a Data Science team and explore the company‚Äôs data to find valuable use cases. A few months later, we added an experienced Product owner (me) to help set the vision, the roadmap and lead our Data initiatives. And some months later we were joined by a rock star Data/Machine learning Engineer who was to create an engineering team and the data platform we could build upon.\n\nOur collective mission was very ambitious but also very unclear. Basically, we were in charge of using Data to have a major impact on the company‚Äôs business. Nobody in the management had a very clear idea of what that meant and how to get there, that was up to us.\nEach one of us had many ideas on how to make that happen, and quite different approaches of the way to get there. The caricatural (but quite realistic) outline was:\n\n  Our lead Data Scientist wanted data, tools and manpower to explore the data, find awesome machine learning use cases that he would build with the DS team he had staffed, onboard the business, have them buy in and go to production (instantly, in his view the step to production was the tiny part).\n  Our lead Data Engineer wanted to start with small use cases, build a production infrastructure and enrich the science step by step later down the road.\n  I wanted to align with our stakeholders and management to build a shared vision, prioritize ideas and then organize the team work to iterate and increase value on a regular basis.\n\n\n\n\nThis misalignment led to an early split between the Data Scientists who did a lot of exploration + POC‚Äôs and the Data Engineers who worked on an industrial production platform. The projects were of different nature and everything was pretty smooth except for some frustration on my side because I was playing little to no role in the Data Science part of things. Overall, everybody was happy, and we started to deliver value‚Ä¶ until our first real business (beyond tooling) cases entered the roadmap.\n\nThe first business case where we needed to leverage both engineering and scientific skills was A/B testing. After having scanned the market solutions to find a nice solution we chose to build our own because we wanted deep integration into all our frontends (including mobile apps and set-top boxes) and backends, a lot of freedom to build KPI‚Äôs upon all of our Data Lake and a few other things. \nTo build the complete toolbox, we had to create a backend serving layer that would return the correct feature flags for the A/B user groups, calculate KPI‚Äôs with a large combination of filters, ensure that the calculation pipeline was run every day and display the results in a decent dashboard. From a project management perspective, it was a headache because the KPI part needed to bring statistical expertise to production, combining statistical excellence and production grade reliability. In our first iteration, a Data Scientist did a POC, but it was no way near production standards. On a separate track, the Data Engineers did a totally separate POC that delivered a decent framework, the KPI‚Äôs were calculated incorrectly. Our third iteration (that took 6 months to make happen) was a 2-day session of peer programming between a Data Scientist and a Data Engineer that (at last!) delivered something real: a POC that actually calculated the right metrics in a way that could be industrialized.\n\nThe second business case was just as messed up. Our Data Science team headed off solo to build a POC of a program recommendation engine (we stream videos that belong to TV programs). Within a couple of months, they built something that seemed nice, but didn‚Äôt scale when run for our millions of users. They then went into optimising their code, rewriting and optimising again without any Data Engineer help for 4 more months until the algorithm was ready for an A/B test against our long standing business rules. And the POC lost the test. After some rounds of tuning, we stopped the initiative altogether because nobody believed in any potential success.\n\nWe repeated this type of scenario a few times, building up more and more frustration within the 2 teams and the stakeholders. We tried to break it down and make the collaboration work quite a few times, but 2 points couldn‚Äôt be resolved:\n\n  The Data Scientists didn‚Äôt want to embrace any product/project management, agile or not (though I believe the underlying issue was more about control in a ‚Äúus‚Äù versus ‚Äúthem‚Äù mindset).\n  The Data Engineering team refused to put any Python code (the Data Scientists language of choice) into production and imposed that everything would be Java or Scala, versionned on git, unit tested and monitored.\nThis looks like very usual trolls, but they kept us stuck for more than a year.\n\n\nAt the end, our organization literally cracked up. The tensions became conflicts, some people left the team and our top management had to dive in to pacify and rebuild something that would work.\nAlthough we had delivered value in several places, we were clearly under effective.\n\nMy personal take away on our collective difficulties boils down to 2 things:\n\n  Starting off with a bunch of rock stars made collaboration impossible because each one wanted to lead in a very personal way, without much compromise.\n  Letting things slip towards a comfortable separation of 2 expertise teams with 2 very different organisations and agenda‚Äôs instead of insisting on aligning them led to a form of cold war with no collaboration at all.\n\n\n\n\nSecond iteration: pluridisciplinary teams\n\nThe conclusion of the management‚Äôs deep dive was that we needed to split the historic teams in a more official and long term way.\nThe Data Scientists would form the Data Lab in which they would do research and produce POC‚Äôs and whitepapers + staff their own engineers if they needed any. And we would create the Data Factory that would be focused on delivery with the Data Engineers and some new Data Scientists that would work on the team‚Äôs backlog with the Data Engineers on a daily basis.\n\n\n\nFrom the Data Lab perspective, that relieved most of the stress because they were now officially free to set a very scientific agenda for themselves. At the start it was cool, but over time (this was 3 years back), their disconnection from ‚Äúproduction‚Äù put them very far away from the real world. The internal stakeholders turned away from them because their target was to make real things. It ultimately led them to having small business impact and a poor dynamic, ultimately reducing the team down.\nFrom the Data Factory perspective, the new organisation fixed it! Over the past couple of years, we have been building the vision and roadmap within the team with no distinction between the Data Science and Data Engineering roadmap. Our work with the stakeholders feels like we‚Äôre now walking on our 2 feet, the solutions we imagine for their challenges mix plain data engineering solutions with algorithms and statistics seamlessly, and we‚Äôre definitely delivering more value. And of course, there is no debate on what goes to production, everything the team does will end up live! At the time I‚Äôm writing, the Data Factory has scaled from ~10 people to more than 40 members.\n\n\n\nOrganization is one thing, collaboration is another\n\nOur approach to overcome our challenges was totally around changing the organization, but I truly believe the solution was also within the evolution of our mindset.\nWhen we officially split the Data Science and Data Engineering teams, we also increased staffing bringing new people into the game. During that round of staffing, we focused very much on soft skills and mindset, considering that authentic team players would ultimately bring us more value than very strong individuals. Somehow, we moved away from building the dream team and aimed to create a real team. Our idea was that we weren‚Äôt doing rocket science, we were working on a bunch of features that needed to incorporate some data science properly in our pipelines.\nOnce the newcomers joined the team, we put a lot of attention into the way people work together, the team spirit they build, the collective dynamic. That went through team building events, defining team values, helping each team member know and understand the other individuals around, creating collaboration rituals, maximizing pair programming between Data Scientists and Data Engineers, etc. And more than anything, entering a more agile test and learn approach, including within the organisation and ways to collaborate.\nI truly believe that this was the key to our success and it‚Äôs my major learning today: build a team.\n\nPutting the pieces together\n\nIn the Dailymotion team post I quoted in the introduction, there‚Äôs a quite precise and documented blueprint of the organisation about who does what between Data Scientists, Data Analysts and Data Engineers. We‚Äôve never been that documented and structured for now, we‚Äôve given each team a lot of freedom about who does what in the process depending on individual capabilities and what the team likes best.\nOne thing we did do is to sort out the language &amp;amp; tooling trolls. At first, anything that went to production was written in Scala, deployed and reviewed via our Git &amp;amp; continuous deployment pipeline, unit tested and monitored. So if a Data Scientist wanted to work on the production code it was following those rules (that has made some of our Data Scientists need to learn Scala and other things). This is now evolving, thanks to strong local collaboration and we now deliver more and more Python to production :)\n\nConclusion\n\nOver the past 4 years, our data team has scaled from 2 to more than 40 people. In the early days we underestimated the fact that putting excellent individuals together wouldn‚Äôt just work. We learnt the hard way that there was specific attention to be put into the organisation and the way we build teams and successful collaboration patterns between people with different domains of expertise and backgrounds. In that, our conclusion is totally similar to the learnings of the Dailymotion team.\nNow we have fixed the mindset and collaboration part of things, we feel nothing can go wrong, even if we changed our organization pattern.\nIf you want to go further, we spoke about this and some other challenges we faced with Morgiane Meglouli in a conference, the slides (in French) are available here\n\nLast but not least, Bedrock is scaling fast to help build streaming champions around Europe. If you‚Äôd like to join us, check out our open positions\n\n"
} ,
  
  {
    "title"    : "Comment nous r√©duisons l‚Äôaugmentation de nos co√ªts AWS",
    "category" : "",
    "tags"     : " conference, aws, costs",
    "url"      : "/2021/03/30/comment-nous-reduisons-augmentation-couts-aws-pascal-martin.html",
    "date"     : "March 30, 2021",
    "excerpt"  : "Malgr√© les promesses du Cloud, votre facture AWS vous fait peur¬†? Je vous comprends¬†!\n\nVenez d√©couvrir comment, chez Bedrock, nous suivons et cat√©gorisons les co√ªts d‚Äôinfrastructure de notre plateforme de VOD et de Replay. J‚Äôenchainerai avec un re...",
  "content"  : "Malgr√© les promesses du Cloud, votre facture AWS vous fait peur¬†? Je vous comprends¬†!\n\nVenez d√©couvrir comment, chez Bedrock, nous suivons et cat√©gorisons les co√ªts d‚Äôinfrastructure de notre plateforme de VOD et de Replay. J‚Äôenchainerai avec un retour d‚Äôexp√©rience bas√© sur trois ans de r√©ponses √† la question que nous nous posons tous : ¬´¬†comment r√©duire nos co√ªts AWS¬†?¬†¬ª\n\nDiviser par deux une facture DynamoDB¬†? Exploiter des instances spot √† -70%¬†? Effectuer moins d‚Äôappels d‚ÄôAPIs¬†? R√©duire les transferts inter-AZ, massifs lorsque nous manipulons des vid√©os¬†? Ce ne sont que quelques-unes des pistes que nous avons envisag√©es‚Ä¶\n"
} ,
  
  {
    "title"    : "Migration de 6play vers Le Cloud, retour d‚Äôexp√©rience.",
    "category" : "",
    "tags"     : " conference, cloud, migration, cloudsud",
    "url"      : "/2021/03/11/migration-6play-vers-le-cloud-retour-experience-pascal-martin.html",
    "date"     : "March 11, 2021",
    "excerpt"  : "En 2018, nous avons entam√© la migration de la plateforme 6play vers Le Cloud.\n√Ä pr√©sent, nous pilotons notre infrastructure AWS avec Terraform, utilisons des services manag√©s et d√©ployons nos applications sous Kubernetes.\n\nPendant cette conf√©rence...",
  "content"  : "En 2018, nous avons entam√© la migration de la plateforme 6play vers Le Cloud.\n√Ä pr√©sent, nous pilotons notre infrastructure AWS avec Terraform, utilisons des services manag√©s et d√©ployons nos applications sous Kubernetes.\n\nPendant cette conf√©rence, vous d√©couvrirez comment nous avons r√©alis√© cette migration. Vous trouverez des r√©ponses aux questions que vous vous posez si vous envisagez de revoir votre h√©bergement.\nComment avons-nous transform√© notre infrastructure¬†? Quels impacts sur nos projets¬†? Comment nous sommes-nous organis√©s¬†? Quels choix avons-nous effectu√©s tout au long du processus¬†? Qu‚Äôavons-nous appris, qu‚Äôavons-nous fait √©voluer¬†? Comment nos √©quipes se r√©partissent-elles les t√¢ches¬†? Avons-nous d√ª adapter nos applications PHP¬†? Quelles difficult√©s avons-nous rencontr√©es¬†?\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #13",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/03/02/bedrock-dev-facts-13.html",
    "date"     : "March 2, 2021",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nDes branchements surprenants\n\n  Ah tu test ta branche en prod ? Tu peux y aller, si tu touches pas au staging pas de soucis\n\n\nAngular conventional commit\n\nTrouv√© dans l‚Äôhistorique\n\n\n\nTime is relative\n\n\n ...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nDes branchements surprenants\n\n  Ah tu test ta branche en prod ? Tu peux y aller, si tu touches pas au staging pas de soucis\n\n\nAngular conventional commit\n\nTrouv√© dans l‚Äôhistorique\n\n\n\nTime is relative\n\n\n  Non mais c‚Äôest normal, ce sont des mois de 5 minutes\n\n\nLe confinement √† la campagne, c‚Äôest parfois compliqu√©\n\n\n  D√©dicace a tous ceux qui comme moi, vont passer une bonne semaine √† batailler avec internet ! (au moins, j‚Äôai assez de d√©bit pour un mp3).\n320 Kbps\n\n\nApple üí∞\n\n\n  Quand ca parle d‚Äôargent, Apple ne se rate pas g√©n√©ralement\n\n\nLe courage\n\n\n  Une MEP le vendredi avec la CI KO, vous √™tes mes h√©ros üòç\n\n\nNote √† moi-m√™me\n\n\n  TODO: test if my code is the problem\n\n\nAucun abus\n\n\n  Bon, je crois que je vais m‚Äôauto-d√©clarer meilleur d√©veloppeur de l‚Äôann√©e!\n\n\nUne semaine qui commence bien\n\n\n  3 push master, lundi matin, 11h30. La semaine va √™tre bonne\n\n\nSoit gentil, pas m√©chant, c‚Äôest pas gentil d‚Äô√™tre m√©chant\n\n\n  A: ‚ÄúJe comprends pas l‚ÄôAPI elle repond rien‚Äù\n\n\n\n  B: ‚ÄúT‚Äôas essay√© de demander gentiment ?‚Äù\n\n\n{\n    &quot;error&quot;: {\n        &quot;status&quot;: 400,\n        &quot;message&quot;: &quot;Invalid parameters &#39;MAIS_TU_VAS_ME_REPONDRE_UN_TRUC_COHERENT_BORDEL_DE_MERDE_!?!?!?!?!?!?!?!?!!&#39; for route &#39;xxxxxxxxxxxxx&#39;&quot;\n    }\n}\n\n\nDes r√©ponses magiques\n\n\n  ‚Äúso this case is never expected, but obviously it happens‚Äù Ah üòÖ, nous voil√† rassur√©\n\n\nAlerte g√©n√©rale\n\n\n  Quand on cr√©e une alerte pour pouvoir la mettre en silence juste apr√®s üëå\n\n\nLe consentement\n\n\n  A: ‚ÄúJ‚Äôai insist√© et c‚Äôest pass√©‚Äù\n\n\n\n  B: ‚ÄúTu as insist√© mais est-ce que tu as pens√© au consentement de GraphQL et de postgre ?‚Äù\n\n\nü¶•, moi paresseux ?\n\n\n  M√™me ne rien faire c‚Äôest d√©j√† pas mal de taf\n\n\n? Question ?\n\n\n\nüçó Mnom Mnom\n\n\n  A: ‚ÄúAh mince j‚Äôai pas mis mon texte en escalope‚Äù\n\n\n\n  B: ‚Äú?‚Äù\n\n\n\n  A: ‚ÄúEntre quote‚Äù\n\n\nLa documentation √©ternelle\n\n\n  Les temps changent mais pas les readme\n\n\nLes petits g√©nies\n\n\n  Nan mais attendez vous √† plein de commentaires disant que c‚Äôest cod√© avec les pieds, et que Jean-Kevin, 16 ans, aurait mieux fait en 2j pendant son week-end chez sa mamie Germaine, pendant qu‚Äôil faisait une pause dans le d√©veloppement du nouveau Bitcoin.\n\n\nBlackhole sun\n\nEn parlant de lignes supprim√©es.\n\n\n\nTechnique de sioux\n\n\n  Nan mais r√©√©crit les status 404 en 200 et √ßa marchera !\n\n\nOn peut revert une fois, mais pas quinze !\n\n\n\nD√©finition AWS\n\n\n  AZ = Ama Zones\n\n\nHeureusement pour nous\n\n\n  Les cas sans erreurs fonctionnent tr√®s bien\n\n\nManager === üêê ?\n\n\n  La formatrice : je vous donne un mot et vous me donnez le premier mot qui vous passe par la t√™te.\n\n\n\n  La formatrice : ‚ÄúChef‚Äù ?\n\n\n\n  Un participant : Ch√®vre\n\n\nIl n‚Äôy a pas de ‚Äúmais‚Äù !\n\n\n  Je ne veux pas remettre en cause toute l‚Äôarchitecture MAIS ‚Ä¶\n\n\nüêí Singe ?\n\n\n  Quand j‚Äôentends ‚ÄúGo/NoGo‚Äù, mon cerveau comprend ‚ÄúBonobo‚Äù\n\n\nAvec du savon !\n\n\n  A: ‚ÄúIls font du SOAP!‚Äù\n\n\n\n  A: ‚ÄúC‚Äôest pour √™tre bien propre‚Äù\n\n\nL‚Äôautomatisation DIY\n\n\n  C‚Äôest automatiquement fait √† la main\n\n\nü¶• le retour\n\n\n  La paresse n‚Äôest pas un d√©faut, c‚Äôest une optimisation\n\n\nAstuce de pro!\n\n\n  Si vos coll√®gues et votre conscience vous emb√™tent quand vous dites ¬´ je vais tester ce changement en prod ¬ª, dites √† la place ¬´ je vais valider ce changement en prod ¬ª. Vous verrez, √ßa ira tout de suite mieux !\n\n"
} ,
  
  {
    "title"    : "Machine Learning en production",
    "category" : "",
    "tags"     : " machine learning, Lyon Data Science, conference",
    "url"      : "/2021/01/21/machine-learning-en-production.html",
    "date"     : "January 21, 2021",
    "excerpt"  : "Une fois pass√©e la phase de prototype, comment va-t-on en production quand on fait du machine learning ?\nComment s‚Äôassure-t-on que tout va bien une fois en production ?\nD√©ploiement, tests, monitoring, etc. Il y a beaucoup de choses √† penser. Sur c...",
  "content"  : "Une fois pass√©e la phase de prototype, comment va-t-on en production quand on fait du machine learning ?\nComment s‚Äôassure-t-on que tout va bien une fois en production ?\nD√©ploiement, tests, monitoring, etc. Il y a beaucoup de choses √† penser. Sur ce long sujet, je vous propose ici une petite introduction bas√©e sur mes exp√©riences.\n"
} ,
  
  {
    "title"    : "Three years running Kubernetes on production at Bedrock",
    "category" : "",
    "tags"     : " Infrastructure, Cloud, Kubernetes, Kops, AWS, HAProxy",
    "url"      : "/2020/12/08/Three-years-running-kubernetes-on-production-at-Bedrock.html",
    "date"     : "December 8, 2020",
    "excerpt"  : "We migrated our first application to a Kubernetes cluster at AWS in 2018 (a colleague even wrote a book about it).\nThree years later, we manage a dozen clusters, to which we have added a lot of tools and we have a much better grasp of certain subt...",
  "content"  : "We migrated our first application to a Kubernetes cluster at AWS in 2018 (a colleague even wrote a book about it).\nThree years later, we manage a dozen clusters, to which we have added a lot of tools and we have a much better grasp of certain subtleties.\nEach cluster reaches, depending on the load, hundreds of nodes and thousands of pods.\n\nTable of Contents\n\n\n  Base\n    \n      Kops and templates\n      Tools we use\n      Keep tools up to date on all clusters\n    \n  \n  Resiliency\n    \n      DNS\n      Lots of AutoScalingGroups\n      Dedicated AutoScalingGroups by app\n      QOS Guaranteed Daemonsets\n    \n  \n  Scalability\n    \n      Cluster Autoscaler\n        \n          Expander Priority\n        \n      \n      Overprovisioning\n      PriorityClass\n      Low HPA targets\n      Long downscale durations\n    \n  \n  Observability\n    \n      Metrics\n      Logs\n      Alerting\n    \n  \n  Costs\n    \n      Spot instances\n        \n          Inter accounts reclaims\n          On-demand fallback\n          Draino and node-problem-detector\n          Spot Tips\n        \n      \n      Kube-downscaler\n      HAProxy Ingress Controller\n    \n  \n\n\n\n\nBase\n\nKops and templates\n\nEKS didn‚Äôt exist when we started to work on Kubernetes on AWS. So we use Kops which, by the way, works very well.\nKops creates, updates and deletes our clusters, but also associates resources on our AWS accounts: DNS zone + entries, AutoScalingGroups, SecurityGroups, etc.\nOur rolling updates and rolling upgrades are 100% handled by kops which never failed us.\n\nBecause we have several clusters, we use kops toolbox template instead of having a single YAML file per cluster. We have mutualized resources definitions, like AutoScalingGroups, DNS options or namespaces list, inside common files and use a dedicated template file per cluster, referencing mutualized configs through variables.\n\nFor example, the EC2 instance types will be defined as snippets:\n\n¬± cat snippets/spot_4x_32Gb_machine_type.yaml:\n- c5.4xlarge\n- c5d.4xlarge\n- c5n.4xlarge\n\nAnd used inside a generic template file:\n\n¬± cat templates/3_spot-nodes.yaml.tpl\n‚Ä¶\n  mixedInstancesPolicy:\n    instances:\n    { { if eq $index &quot;4x_32Gb&quot; } }\n    { { include &quot;spot_4x_32Gb_machine_type.yaml&quot; . | indent 4 } }\n    { { end } }\n‚Ä¶\n\nFinally, if the cluster requires an ASG with instances size 4x with 32GB RAM on Spot instances:\n\n¬± cat vars/prod-customer.k8s.foo.bar.yaml\n‚Ä¶\nspot_nodes:\n  4x_32Gb:\n    az:\n      - eu-west-3a\n      - eu-west-3b\n      - eu-west-3c\n    min: 1\n    max: 100\n\nA bash script orchestrates all this. It generates manifest files, creates/updates clusters and checks everything is operating normally.\n\nAll of the above lives as files in a git repository, ensuring we‚Äôre doing only Infrastructure as Code.\n\nWe never make any Infrastructure modification outside of code.\n\nTools we use\n\nWe add some tools to a raw Kubernetes cluster:\n\n\n  aws-iam-authenticator\n  cluster-autoscaler\n  cloudwatch-exporter-in-cluster\n  cni-metrics-helper\n  draino\n  elasticsearch-cerebro\n  fluentd\n  haproxy-ingress-controller\n  iam-role-for-serviceaccount\n  k8s-spot-termination-handler\n  kube-downscaler\n  logstash\n  loki\n  metrics-server\n  node-problem-detector\n  overprovisioning\n  prometheus\n  prometheus-dnsmasq-exporter\n  prometheus-pushgateway\n  statsd-exporter\n  statsd-proxy\n  victoria-metrics-cluster\n\n\nSome of those tools stand for compatibility reasons after our cloud migration, so our developers can still use our ELK stack, or a statsd format to generate metrics.\n\nWe need all these tools to have a production-ready cluster, so we can provide scaling, resilience, observability, security with controlled costs. This list isn‚Äôt even exhaustive.\n\nIt evolved a lot over the last two years and will surely evolve a lot in the near future, as both Kubernetes and AWS are moving playgrounds.\n\nKeep tools up to date on all clusters\n\nWe use a Jenkins job for that.\n\nWe deploy k8s-tools the same way we deploy our apis in the cluster: with bash scripts and a helm chart, dedicated per application.\n\n¬± tree app/loki/.cloud/       \napp/loki/.cloud/\n‚îú‚îÄ‚îÄ charts\n‚îÇ   ‚îú‚îÄ‚îÄ Chart.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ templates/\n‚îÇ   ‚îú‚îÄ‚îÄ values.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ values.customerX.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ values.customerY.yaml\n‚îî‚îÄ‚îÄ jenkins\n    ‚îú‚îÄ‚îÄ builder.sh\n    ‚îî‚îÄ‚îÄ deployer.sh\n\nA Jenkins job runs the builder.sh, then the deployer.sh script for every k8s-tool.\nbuilder.sh is run when we need to build our own Docker images.\ndeployer.sh handles the Helm Chart deployment subtleties.\nAll apps are first deployed on all our staging clusters, then on prod.\n\nConsistency is maintained over all our clusters through this Jenkins job.\n\nResiliency\n\nDNS\n\nLike everyone who‚Äôs using Kubernetes on production, at some point, we faced an outage due to DNS. It was either UDP failing because of a kernel race condition, or musl (Alpine Linux‚Äôs replacement of glibc) not correctly handling domain or search, or also the default ndots 5 dnsConfig, or even KubeDNS not handling peak loads properly.\n\nAs of today:\n\n\n  We are using a local DNS cache on each worker node, with dnsmasq,\n  We use Fully Qualified Domain Names (trailing dot on curl calls) as much as possible,\n  We‚Äôve defined dnsConfig preferences for all our applications,\n  We use CoreDNS with autoscaling as a replacement for KubeDNS,\n  We forbid as much as possible musl/Alpine\n\n\nExample of a dns configuration in prod:\n\n  dnsConfig:\n    options:\n    - name: use-vc\n    - name: single-request\n    - name: single-request-reopen\n    - name: ndots\n      value: &quot;1&quot;\n  dnsPolicy: ClusterFirst\n\n\ndnsPolicy: ClusterFirst makes sure we‚Äôre using the node‚Äôs loopback interface, so pods will send their DNS requests to dnsmasq installed locally on each node.\nDnsmasq forwards DNS queries to CoreDNS for cluster.local. sub-domains and to the VPC‚Äôs DNS server for the rest.\n\nLots of AutoScalingGroups\n\nWe had a dozen AutoScalingGroups per cluster.\nThis was both for resiliency and because we use Spot instances.\nWith Spot instance reclaims, we needed to have a lot of instance types and family types: m5.4xlarge, c5.4xlarge, m5n.8xlarge, etc.\nThis is an autoscaler recommendation to split AutoScalingGroups so that each ASG has the same amount of RAM and number of CPU cores when using mixed instances policies.\nAs a result, we had ASGs like:\n\n\n  spot_4x_32Gb\n  spot_4x_64Gb\n  spot_4x_128Gb\n\n\nLots of AutoScalingGroups doesn‚Äôt work well\n\nAZ rebalancing doesn‚Äôt work anymore when using more than one ASG. It becomes totally unpredictable and uncontrollable. It is even a total nightmare with a dozen ASGs.\n\nYou can see the difference of outgoing traffic between our 3 NAT Gateway over 4 hours time range :\n\nThe blue NAT gateway is used way more than the two others between 19h00 and 22h00. The green NAT gateway is used half as much as the other two during peak usage times.\nThis is because AZ-rebalacing has resulted in twice as many instances in one AZ than in the others.\n\nAlso, Kubernetes‚Äôs cluster-autoscaler isn‚Äôt really compatible with many AutoScalingGroups. We‚Äôll cover how it works later in this post (Scalability/ExpanderPriority), but keep in mind that each application should run on no more than a maximum of 4 ASGs. This is due to the failover mechanism of cluster-autoscaler that doesn‚Äôt detect ASGs errors like InsufficientInstanceCapacity, which considerably increases the scale-up time. We are particularly concerned because we need to scale quickly and intensely.\n\nWe‚Äôve rolled-back on the ASG number. We now have a maximum of 4 ASGs per application group (see next section: Resiliency/DedicatedAutoScalingGroups), with 2 being Spot and 2 on-demand fallbacks.\nFor this reason, we no longer respect the recommendation to split AutoScalingGroups so that each ASG has the same amount of RAM and number of CPU cores, in order to reduce ASGs number.\n\nRunning PHP, the CPU is our bottleneck, not RAM. So we made the choice to have mixed ASG with the same number of CPUs, but not the same amount of RAM.\nThis means that our ASG spot-nodes-8x is composed of m5.8xlarge as well as r5.8xlarge\n\nDedicated AutoScalingGroups by app\n\nWe started to dedicate AutoScalingGroups for some applications when Prometheus was eating all the memory of a node, ending up in OOM errors. Because Prometheus replays its WAL at startup and consumes a lot of memory doing so, adding a Limit over the memory was of no use. It was OOMKill during the WAL process, restarted, OOMKilled again, etc. . Therefore, we isolated Prometheus on nodes having a lot of memory so it could use up all of it.\n\nThen, one of our main API experienced a huge load, 60% IDLE CPU to 0% in a few seconds. Because of the brutality of such a peak, active pods started to consume all CPU available on nodes, depriving other pods. Getting rid of CPU limits is a recommendation that comes with drawbacks that we measured and chose to follow the recommendation to ensure performance. As a result, the entire cluster went down, lacking for available CPU. Airbnb shared the same experience: they removed CPU limits because of throttling, but the noisy neighbors forced them to re-introduce limits.\n\nWe tried to isolate this API on its own nodes, as such peaks can repeat in the future, because it‚Äôs uncacheable and userfacing. We added Taints on dedicated nodes and Tolerations on the selected API.\n\nSince then, we had to deploy a dedicated overprovisioning on those nodes as the overprovisioning pods didn‚Äôt have this Toleration. It turned out we‚Äôre also able to adapt the overprovisioning specifically for this API, which wasn‚Äôt the base idea, but it has proven to be very effective due to the API‚Äôs nature. We talk more about overprovisioning‚Äôs conf a little later on (Scalability/Overprovisioning).\n\nNow, we‚Äôre setting CPU limits, at least for all applications not using dedicated nodes and also because we‚Äôve updated our kernels to the patched version. We follow their CPU usage through Prometheus alerting, with:\n\n- labels:\n    severity: notice\n    cluster_name: &quot;{ { $externalLabels.cluster_name } }&quot;\n  annotations:\n    alertmessage: &#39;{ { $labels.namespace } }/{ { $labels.pod } }/{ { $labels.container } } : { { printf &quot;%0.0f&quot; $value } }%&#39;\n    description: Container using more CPU than expected.\n      It will soon be throttled, which has a negative impact on performances.\n    summary: &quot;{ { $externalLabels.cluster_name } } - Notice - K8S - Container using 90% CPU Limit&quot;\n  alert: Notice - K8S - Container getting close to its CPU Limit\n  expr: |\n    (\n      sum(rate(container_cpu_usage_seconds_total{job=&quot;kubelet&quot;, container!=&quot;POD&quot;, container!=&quot;&quot;}[1m])) by (container, namespace, pod)\n    / sum(kube_pod_container_resource_limits_cpu_cores{job=&quot;kube-state-metrics&quot;, container!=&quot;POD&quot;, container!=&quot;&quot;}) by (container, namespace, pod)\n    ) * 100 &amp;gt; 90\n\n\nWe don‚Äôt currently have alerting on Throttling, only a Grafana graph using the metric:\n\nsum by (pod) (rate(container_cpu_cfs_throttled_seconds_total{job=&quot;kubelet&quot;, image!=&quot;&quot;,container!=&quot;POD&quot;}[1m]))\n\nAfter Prometheus, we later isolated Victoria Metrics and Grafana Loki on their own ASGs.\nWe‚Äôre also isolating ‚Äúadmin‚Äù tools, like CoreDNS, cluster-autoscaler, HAProxy Ingress Controller, on dedicated ‚Äúadmin nodes‚Äù group. That way, admin tools can‚Äôt mess with applications pods and vice versa.\n\n\nDevelopers only deploy to Worker nodes. An application‚Äôs pods can only be scheduled on 4 ASGs, including 2 on-demand backups.\n\nOur admin nodes are on-demand. Having an ASG of few nodes all Spot is a risk we didn‚Äôt want to take regarding the criticality of those pods.\n\nQOS Guaranteed Daemonsets\n\nAll our Daemonsets have Requests and Limits set at the same value.\nWe‚Äôve found out that a lot of Daemonsets don‚Äôt define those values by default.\nEnforcing QOS Guaranteed Daemonsets:\n\n\n  ensures our daemonsets request all the resources they need, which is also important for the k8s scheduler to be more effective\n  daemonsets bad behaviours can be contained through Limits, and will not mess up with pods\n  it‚Äôs a good indicator of the overhead we add on each node and helps us choose our EC2 instance types better (E.g: 2x.large instances are too small)\n  it‚Äôs a reminder that a server with 16 CPUs has in fact only 80% of them usable by application pods\n\n\nScalability\n\nCluster Autoscaler\n\nWe automatically scale our EC2 Instances with cluster-autoscaler.\n\n\n\nAs mentioned before, we have several AutoScalingGroups per cluster.\nWe use the service discovery feature of cluster-autoscaler to find all ASGs to work with and to control them automatically.\nThis is done in two steps:\n\n\n  We add 2 tags on ASGs that the cluster-autoscaler should manage\n\n\nk8s.io/cluster-autoscaler/enabled: &quot;true&quot;\nk8s.io/cluster-autoscaler/{ { $cluster.name } }: &quot;true&quot;\n\n\n\n  Then, inside the Chart, we add those two labels to the node-group-auto-discovery parameter:\n\n\ncommand:\n- ./cluster-autoscaler\n- --cloud-provider=aws\n- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/{ { index .Values.nodes .Values.env &quot;clusterName&quot; } }\n‚Ä¶\n\n\nExpander Priority\n\nWe use cluster-autoscaler with the expander: priority.\nASGs will be chosen as:\n\n\n  spot-nodes-.*\n  on-demand-.*\n\n\nCluster-autoscaler will randomly add an EC2 instance in an ASG in the first group: spot-nodes-*. If a new instance hasn‚Äôt joined the cluster after the fallback timeout (--max-node-provision-time), it will try another ASG in the same group. It will try all the ASGs in this group before moving on to the next group: on-demand-*.\n\nWith a dozen ASGs, most of them being Spot, we‚Äôve already waited for 45 minutes to actually be able to successfully add an EC2 instance.\n\nLaunching an EC2 instance sometimes fails with InsufficientInstanceCapacity, especially for Spot instances. With the autoscaler recommendation to split ASGs by the same amount of CPU/RAM, there were just too many ASGs to try before falling back on-demand. We‚Äôve reduced the cluster-autoscaler fallback timeout to 5 minutes and still are facing many scaling problems at Paris, where it seems there are not many Spot instances available.\n\n\n\nExpander priority allows us to have resilience through an automatic fall back to on-demand when there is no more Spot.\nWe have already faced, multiple times, a fallback to on-demand instances even with a dozen different instance types. InsufficientInstanceCapacity errors are not a myth. Even on-demand instances can be in InsufficientInstanceCapacity, which we hope to never face with expander priority, 10+ Spot instance types, 10+ on-demand instance types and low --max-node-provision-time.\n\nOverprovisioning\n\nWe have overprovisioning pods inside the cluster.\nThe objective is to trigger a node scale-up before a legitimate pod actually needs resources. Doing so, the pod doesn‚Äôt wait minutes to be scheduled, but a few seconds. This need for speed is linked to our business and sometimes the television audience bringing us many viewers very quickly.\n\nThis works using overprovisioning pods which request resources without doing anything (docker image: k8s.gcr.io/pause). Those pods are also using a low PriorityClass (-10), lower than our apps.\n\nThis trick is the whole magic of this overprovisioning: we request space that can be reclaimed anytime and very quickly. When an app needs it, the Scheduler will free up this space by expelling overprovisioning pods (of lower priority) because the cluster doesn‚Äôt have enough free space. The expelled pods then change their state to Pending with the Reason: Unschedulable because we just filled the cluster with higher priority pods from the app. Presence of Pending Pods with Unschedulable reason trigger the cluster-autoscaler to add nodes.\n\nWe follow the efficiency of this overprovisioning with these Prometheus expressions:\n\n\n  kube_deployment_status_replicas_unavailable: we know which pods are waiting to be scheduled,\n  sum(kube_node_status_condition{condition=&quot;Ready&quot;,status=&quot;false&quot;}): we know if there are UnReady nodes, like when nodes are scaling-up and new nodes don‚Äôt have their daemonsets Ready.\n\n\nBecause we have some nice load peaks on our applications, we are using the ladder mode of the overprovisioning. That ensures that we always have a minimum amount of overprovisioning running in the cluster, so we‚Äôre able to handle huge loads at any time. Also, we ensure that we don‚Äôt waste too much resources when heavily loaded, so we don‚Äôt reserve 200 nodes in a cluster of 1000 nodes for example.\n\nThe configmap looks like:\n\ndata:\n  ladder: &#39;{&quot;coresToReplicas&quot;:[[16,4],[100,10],[200,20]]}&#39;\n\n\nWe chose to have big overprovisioning pods, bigger than any other pod in the cluster, to ensure that expelling one of the overprovisioning pods is enough to schedule any Pending pod.\n\nPriorityClass\n\nWe sacrifice some applications when overprovisioning is not enough.\n\nThe overprovisioning magic is based on PriorityClass objects.\nWe‚Äôre using the same logic for our other applications, using PriorityClass.\nWe have 3 of them which concern applications:\n\n\n  low: -5\n  default: 0\n  high: 5\n\n\nCritical applications are using the ‚Äúhigh‚Äù PriorityClass.\nMost applications are using the ‚Äúdefault‚Äù one, so they don‚Äôt even have to explicitly use it.\nWorkers doing asynchronous tasks can be cut off for several tens of minutes without any business impact. These are the ones with the ‚Äúlow‚Äù PriorityClass we sacrifice when needed.\n\nHere is an example, during a heavy load :\n\nHundreds of unavailable pods for 10 minutes.\n\nIf we filter out ‚Äúlow‚Äù PriorityClass pods in the graph above, there‚Äôs only one application having unavailable pods:\n\nNew pods for this application stayed in the Unavailable state for 15 seconds.\n\nLow HPA targets\n\nKubernetes takes time to scale-up pods.\n\nWithout overprovisioning, we‚Äôve measured that we wait up to 4 minutes when there‚Äôs no available node where pods can be scheduled.\nThen, with overprovisioning, we mostly wait for 45seconds, between the moment the HorizontalPodAutoscaler changes the Replicas of a Deployment and for those pods to be ready and receive traffic.\n\nWe can‚Äôt wait so long during our peaks, so we generally define HPA targets at 60%, 70% or 80% of Requests. That gives us more time to handle the load while new pods are being scheduled.\n\nOn the following graphs, we can see two nice peaks at 20h52 and 21h02:\n\nAbove, in green, the number of consumed CPUs for one specific application: +55% in one minute.\n\nBelow, in blue, new pods are created in response to the peak.\n\n\nThis is obviously not a good way of managing resources, as we waste them as soon as the load balances.\nThis waste effect is amplified with the load: the more pods we have, the more we waste.\n\nYou can see it in this graph that shows the number of CPU reserved but not consumed:\n\n\nWe consume more CPU during peaks and therefore, we use more efficiently the reservations that do not have time to move, because we do not yet have scale-up.\nAs soon as the new pods added in response to the peak are Ready, 40% of CPU are wasted again.\n\nWe don‚Äôt have a viable solution to solve this.\n\nWe‚Äôre thinking about reducing scale-up duration to 10 seconds, so we won‚Äôt need these additional resources while we launch new pods. This is a challenge as the scaling mechanism is composed of several tools (metrics-server update frequency, autoscaler controller loop frequency, pod autoscaler initial readiness delay, probe launch times, etc.) and changing only one of them can have catastrophic behavior on the cluster stability. This huge subject will need its own dedicated blogpost‚Ä¶\n\nLong downscale durations\n\nRecently, we have increased the HPA‚Äôs downscale durations from 5 to 30 minutes.\n\nIt‚Äôs done through Kops spec:\n  kubeControllerManager:\n    horizontalPodAutoscalerDownscaleStabilization: 30m0s\n\n\nWhen an application fails, its traffic decreases. When front A fails, traffic on backend B decreases too. When front A comes back, both A and B will have a big peak load.\n\nThe default five minutes delay for scaling down pods is too short for us. Increasing this delay makes the return to life of front A transparent on the number of pods of the whole platform, at least for the first 30 minutes of shutdown.\n\nWe‚Äôve seen blog posts where people turn off autoscaling for those very situations.\nFailure is not an extreme case. Failure is expected. Autoscaling strategies must adapt to it.\n\nYou can see that one waits 30 minutes after an upscale, before downscaling:\n\n\nDocumentation specifies that: ‚Äúthis duration specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed.‚Äù\n\nWe can observe on the graph above that it‚Äôs rather: ‚Äúthis duration specifies how long the autoscaler has to wait to perform a downscale after the last upscale‚Äù.\n\nObservability\n\nMetrics\n\nWe scrape application and system metrics via Prometheus.\nWe‚Äôre using Victoria Metrics as long term storage. We found it really easy to deploy and it needs really few time to administer on a daily basis, unlike Prometheus.\n\nDetails:\n\n\n  Prometheus scrapes metrics of pods having:\n\n\nannotations:\n  prometheus.io/path: /metrics\n  prometheus.io/port: &quot;8080&quot;\n  prometheus.io/scrape: &quot;true&quot;\n\n\n\n  Then, inside prometheus jsonnet files, we define a remoteWrite pointing to VictoriaMetrics:\n\n\nremote_write:\n- url: https://victoria-metrics-cluster-vminsert.monitoring.svc.cluster.local.:8480/insert/001/prometheus\n  remote_timeout: 30s\n  write_relabel_configs:\n  - separator: ;\n    regex: prometheus_replica\n    replacement: $1\n    action: labeldrop\n  queue_config:\n    capacity: 50000\n    max_shards: 30\n    min_shards: 1\n    max_samples_per_send: 10000\n    batch_send_deadline: 5s\n    min_backoff: 30ms\n    max_backoff: 100ms\n‚Ä¶\n\n\nWe have 2 Prometheus pods per cluster, each on separate nodes.\nEach Prometheus scrapes all metrics in the cluster, for resilience.\nThey have a really low retention (few hours, because of the WAL replay issue) and are deployed on Spot instances.\n\nWe have 2 Victoria Metrics pods per cluster (cluster version), each on separate nodes, separated of Prometheus pods through a podAntiAffinity\naffinity:\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 100\n      podAffinityTerm:\n        labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - prometheus\n        topologyKey: &quot;kubernetes.io/hostname&quot;\n\n\nEach Victoria Metrics pod receives all metrics in duplicate, from the two prometheus pods.\nWe use the command-line flag dedup.minScrapeInterval: 15s to deduplicate metrics.\n\nWe‚Äôre thinking about totally removing Prometheus from the mix, using only Victoria Metrics Agent to scrape metrics.\n\nLogs\n\nWe collect stderr and stdout of all our containers.\nWe use fluentd for that, as a DaemonSet, which uses the node‚Äôs /var/log/containers directory.\nWe use Grafana Loki as an interface to filter those logs.\n\nOur developers catch most of their logs and send them directly to Elasticsearch. Fluentd and Loki are used only for uncatched errors and have little traffic.\n\nFluentd uses around 200MB of memory per node and so we look at replacing it by promtail which uses only 40MB in our case.\n\n\nWe‚Äôre happy with Loki, because we have few logs to parse. We‚Äôve tested to get our Ingress Controller access logs sent to Loki and it was a nightmare. Too many entries to parse.\n\nThere‚Äôs a default limit of 1000 log entries when querying, which we raised but then Grafana became very slow. Very very slow. 3000 log entries is the best fit for us.\n\nAlerting\n\nWe mostly use alerts defined in the official prometheus-operator repo.\n\nWe also added some alerts of our own. E.g: an alert when our Ingress Controller can‚Äôt connect to a pod:\n- labels:\n    severity: critical\n    cluster_name: &quot;{ { $externalLabels.cluster_name } }&quot;\n  annotations:\n    alertmessage: &#39;{ { $labels.proxy } } : { { printf &quot;%.2f&quot; $value } } requests in error per second&#39;\n    description: &#39;HAProxy pods cannot send requests to this application. Connection errors may happen when one or more pods are failing or there&#39;&#39;s no more healthy pods : Application is crashed !!&#39;\n    summary: &quot;{ { $externalLabels.cluster_name } } - Critical - K8S - HAProxy IC - Backend connection errors&quot;\n  alert: Critical -K8S - HAProxy IC - Backend connection errors\n  expr: |\n    sum(rate(haproxy_backend_connection_errors_total[1m])) by (proxy) &amp;gt; 0\n  for: 1m\n\n\nPrometheus generates alerts that it sends to 2 redundant AlertManager instances, in a separate account that centralises alerts from all our clusters.\nWe have several possibilities then:\n\n\n  Send alerts on Slack dedicated channels\n  Send alerts to PagerDuty for the on-call teams\n\n\nOur developers are managing their own alerts (Kubernetes CRD: PrometheusRule) that are following a different path regarding labels defined. They have their own alerts sent in their own channels.\n\nCosts\n\nSpot instances\n\nWe‚Äôre running 100% of our application workloads on Spot instances.\n\nIt was easy at first: implement spot-termination-handler and voil√†.\nIndeed, but that was only the first step.\n\nInter accounts reclaims\n\nWe created AWS accounts for salto.fr platform, for which we did a lot of load tests with on-demand servers.\nThat‚Äôs when we reclaimed our own instances on our other accounts.\n\n\n\nYour accounts are not ‚Äúlinked‚Äù to each other in terms of Spot reclaims. Having resources in the same region with different accounts creates a relationship itself that we had never though about.\nIn this case, launching on-demand instances on one account triggered reclaims on our other accounts in the same region.\n\nOn-demand fallback\n\nWe didn‚Äôt have on-demand fallback for a year and it went well.\nThere was enough spot capacity and there was no need for fallback. Therefore, we didn‚Äôt prioritize automated on-demand fallbacks.\n\nThen, all our instance types (+10) went InsufficientInstanceCapacity at the same time.\nWe could only work around with a manual ASG we have from our first days on Kubernetes at AWS, on which we could launch on-demand instances as a last-resort fallback.\n\nNow, we‚Äôre using cluster-autoscaler with the expander: priority to automatically fallback on lower priority ASGs (see above Scalability/Cluster-autoscaler).\n\nIt takes us around 10mn to start a node when all our instances are InsufficientInstanceCapacity.\nThere are other mechanisms that directly detect InsufficientInstanceCapacity on an ASG, so we wouldn‚Äôt have to wait 5mn before moving on to the next one. We‚Äôre thinking about implementing them, but they‚Äôre not really compatible with cluster-autoscaler right now.\n\nAs of today, we have two ASGs per application group, as Spot, and also two ASGs as on-demand automatic fallback.\n\nDraino and node-problem-detector\n\nThe problem came when downscaling : cluster-autoscaler removes the least used node, no matter if it‚Äôs a Spot or an on-demand instance.\n\nWe found ourselves with a lot of on-demand nodes after load peaks and they stayed on. And they cost a lot more than Spot instances.\n\nWe were already using node-problem-detector, so we added draino, to detect if an instance is on-demand and try to remove it when it is. Draino waits for 2h after the node is launched before trying to remove it.\n\nSince then, we use on-demand only when there‚Äôs no Spot left and only for a few hours.\n\nWe can see on this graph, that we added automated on-demand fallback and we never stopped having on-demand instances, until we added draino:\n\n\nSpot Tips\n\n\n  You need to be in an ‚Äúold‚Äù AWS region to have a large number of Spot available. I.E. consider eu-west-1 instead of eu-west-3, even if it adds latency,\n  Use the maximum number of instance types possible. A dozen is barely enough. I.E. use all instance family letters regardless of what they‚Äôre optimised for (compute, memory) as long as your workload can use it,\n  Use CapacityOptimized spot allocation strategy, to limit reclaims to the strict necessary,\n  Do not use Spot on a single AZ (this advice is not limited to spot),\n  Prepare yourself to large reclaims, dozens at a time,\n  Configure and test your on-demand fallback\n\n\nKube-downscaler\n\nOpen-source project from Zalando which allows us to scale down Kubernetes deployments after work hours: nights and week-ends.\n\nWe use it on all our staging clusters. We save 60% of EC2 instances.\n\nHAProxy Ingress Controller\n\nThe whole traffic of a cluster goes through a single ALB.\n\nWe load-balance traffic to the correct pod through HAProxy, which uses Ingress rules to update its configuration.\nWe explained the way HAProxy Ingress controller lives inside the cluster during a talk at the HAProxy Conf in 2019.\n\nReducing the number of managed load balancers at AWS isn‚Äôt the only benefit of HAProxy: we have tons of metrics in a single Grafana dashboard. Requests number, errors, retries, response times, connect times, bad health checks, etc.\n\nWhat‚Äôs next\n\nA lot has been done to have scalability, resilience, observability and reasoned costs over the last 3 years.\n\nUsing Kubernetes in production is not that simple.\nIt is necessary to be well equipped, to understand finely the workings of the kubernetes mechanics to find the balance that suits us. Avoid falling into the trap of adopting whatever tool everyone is talking about if you don‚Äôt need it. There‚Äôs a lot of hype around kubernetes and the cloud, it can be dangerous.\n\nThe next step will be for us to increase resilience as much as possible, while at the same time reducing costs.\nPerhaps it will be by speeding up the start-up of pods. Maybe it won‚Äôt work. Maybe we can make some modifications to cluster-autoscaler to make it more compatible with aws events (like InsufficiantInstanceCapacity). But we will certainly work around the costs.\n\n\n\nThanks to all the reviewers, for their good advice and their time ‚ù§Ô∏è\n"
} ,
  
  {
    "title"    : "PHP, c‚Äôest vous ! Et vous pouvez contribuer !",
    "category" : "",
    "tags"     : " conference, php, open-source, afup",
    "url"      : "/2020/11/23/php-cest-vous-et-vous-pouvez-contribuer-pascal-martin.html",
    "date"     : "November 23, 2020",
    "excerpt"  : "Quand nous posons la question ¬´¬†qui contribue √† PHP¬†?¬†¬ª lors des √©v√®nements que nous organisons ou auxquels nous participons, nous n‚Äôobtenons que tr√®s peu de r√©ponses. Est-ce parce que peu d‚Äôentre nous savent ou aiment coder en C¬†? Pourtant, parti...",
  "content"  : "Quand nous posons la question ¬´¬†qui contribue √† PHP¬†?¬†¬ª lors des √©v√®nements que nous organisons ou auxquels nous participons, nous n‚Äôobtenons que tr√®s peu de r√©ponses. Est-ce parce que peu d‚Äôentre nous savent ou aiment coder en C¬†? Pourtant, participer et contribuer ne se limite pas √† des lignes de code, loin de l√†¬†!\n\nAvez-vous trouv√© l‚Äô√©diteur en ligne de la documentation de PHP¬†? Avez-vous soumis des patchs √† composer et symfony ou m√™me √† magento et wordpress, qui sont open-source et attendent vos contributions¬†? Avez-vous test√© les versions alpha de PHP¬†8¬†? Voyez-vous comment organiser un Ap√©ro PHP ou un Meetup¬†? Ou m√™me un AFUP Day ou le Forum PHP¬†? Savez-vous que l‚ÄôAFUP a besoin de vous¬†? Que c‚Äôest une association qui est l√† pour vous aider et ce qu‚Äôelle peut vous apporter¬†?\n\nAvec mille et une fa√ßons de contribuer, venez faire un tour d‚Äôhorizon de modes de contribution que vous n‚Äôaviez peut-√™tre pas encore envisag√©s, ou dont vous vous √©tiez dit qu‚Äôils n‚Äô√©taient pas pour vous. Vous verrez que, vous aussi, vous pouvez contribuer √† PHP¬†;-)\n"
} ,
  
  {
    "title"    : "L&#39;open source, ce n&#39;est pas que pour le web",
    "category" : "",
    "tags"     : " conference, afup, open-source",
    "url"      : "/2020/10/23/l'open-source-ce-n-est-pas-que-pour-le-web.html",
    "date"     : "October 23, 2020",
    "excerpt"  : "Une conf√©rence sur l‚Äôopen source hors des solutions informatiques uniquement, lors du forum PHP 2020 qui marquait les 20 ans de l‚ÄôAFUP.\n\nConnaissez-vous l‚Äôopen hardware ? Savez-vous ce que la NASA partage sur Github ? Vous avez certainement d√©j√† √©...",
  "content"  : "Une conf√©rence sur l‚Äôopen source hors des solutions informatiques uniquement, lors du forum PHP 2020 qui marquait les 20 ans de l‚ÄôAFUP.\n\nConnaissez-vous l‚Äôopen hardware ? Savez-vous ce que la NASA partage sur Github ? Vous avez certainement d√©j√† √©cout√©, ou produit de la musique open source, savez-vous qu‚Äôil existe des m√©dicaments open source ? R√©pliquer une information, et la partager devient rapide et √©mancipateur, le monde se lib√®re un peu plus.\n\nApr√®s une petite plong√©e dans les principes de partage de l‚Äôopen source, nous ferons un tour d‚Äôhorizon des initiatives open source dans d‚Äôautres domaines que l‚Äôinformatique, pour en apprendre un peu plus sur la culture du libre, les rapports de force qui y conduisent, et revenir aux bases du partage.\n"
} ,
  
  {
    "title"    : "La scalabilit√© d‚Äôune √©quipe / d‚Äôun p√¥le technique",
    "category" : "",
    "tags"     : " conference, forumPHP, PHP, Symfony",
    "url"      : "/2020/10/22/la-scalabilite-d-une-equipe-d-un-pole-technique.html",
    "date"     : "October 22, 2020",
    "excerpt"  : "Vous √™tes dans l‚Äô√©quipe technique d‚Äôune entreprise, compos√©e de quelques d√©veloppeurs, dans 1 ou 2 √©quipes, et votre entreprise grandit, et il faut augmenter la capacit√© de production, et donc la taille de l‚Äô√©quipe technique. Sauf que comme 9 femm...",
  "content"  : "Vous √™tes dans l‚Äô√©quipe technique d‚Äôune entreprise, compos√©e de quelques d√©veloppeurs, dans 1 ou 2 √©quipes, et votre entreprise grandit, et il faut augmenter la capacit√© de production, et donc la taille de l‚Äô√©quipe technique. Sauf que comme 9 femmes ne font pas un b√©b√© un 1 mois, 4 √©quipes de 6 personnes ne produisent pas automatiquement 2 fois plus que 2 √©quipes de 6 d√©veloppeurs.\n\nJe me propose de vous faire un retour d‚Äôexp√©rience sur comment nous avons abord√© la scalabilit√© du p√¥le technique de Bedrock, pour passer de 10 √©quipes r√©parties en 3 verticaux techniques, √† plus de 30 √©quipes dans 5 verticaux techniques, en essayant de conserver une coh√©sion technique et fonctionnelle, et d‚Äôoptimiser les flux de d√©veloppements.\n\n"
} ,
  
  {
    "title"    : "DevOps ? Je n&#39;ai jamais voulu faire √ßa, et pourtant ‚Ä¶",
    "category" : "",
    "tags"     : " conference, afup, php, devops",
    "url"      : "/2020/06/24/devops-je-n-ai-jamais-voulu-faire.html",
    "date"     : "June 24, 2020",
    "excerpt"  : "D√©veloppeuse junior : premi√®re semaine. Mes coll√®gues m‚Äôont forc√©e √† d√©ployer ma premi√®re feature sur 6play ! Malgr√© un petit frisson, tout s‚Äôest bien pass√©, gr√¢ce aux outils et bonnes pratiques qui nous guident.\n\nCe n‚Äô√©tait que le d√©but ! Depuis,...",
  "content"  : "D√©veloppeuse junior : premi√®re semaine. Mes coll√®gues m‚Äôont forc√©e √† d√©ployer ma premi√®re feature sur 6play ! Malgr√© un petit frisson, tout s‚Äôest bien pass√©, gr√¢ce aux outils et bonnes pratiques qui nous guident.\n\nCe n‚Äô√©tait que le d√©but ! Depuis, je g√®re l‚Äôinfrastructure de mon projet. Je choisis mes bases de donn√©es, caches, m√©canismes de stockage, ressources‚Ä¶ En prenant en compte leur co√ªt, les modes de backups ou les comp√©tences dans nos √©quipes. Et je suis libre d‚Äôexp√©rimenter avec n‚Äôimporte quel service que je voudrais tester.\n\nEn un an, je suis pass√©e de ‚Äúsimple d√©veloppeuse‚Äù √† quelqu‚Äôun qui a conscience de sa plateforme, qui monitore son code et est responsable de sa production. Comment ai-je v√©cu cette transition ? Comment ai-je grandi en tant que d√©veloppeuse ?\n\nVous aussi, profitez de votre nouvelle libert√© : devenez DevOps !\n"
} ,
  
  {
    "title"    : "6play_API-v2-Final(1).doc",
    "category" : "",
    "tags"     : " conference, php, afup, api",
    "url"      : "/2020/06/24/6play_API-v2-Final(1).html",
    "date"     : "June 24, 2020",
    "excerpt"  : "Votre API est confront√©e √† des contraintes techniques mais elle doit surtout r√©pondre √† vos probl√©matiques m√©tier qui ne cessent d‚Äô√©voluer. Nous avons souvent v√©cu cette situation pour 6play (service de Replay du Groupe M6), et il nous a fallu plu...",
  "content"  : "Votre API est confront√©e √† des contraintes techniques mais elle doit surtout r√©pondre √† vos probl√©matiques m√©tier qui ne cessent d‚Äô√©voluer. Nous avons souvent v√©cu cette situation pour 6play (service de Replay du Groupe M6), et il nous a fallu plusieurs g√©n√©rations d‚ÄôAPI avant d‚Äôarriver √† une version adapt√©e √† nos besoins. Micro-services, Rest/GraphQL, Developer eXperience‚Ä¶ Un r√©cit et des conseils pragmatiques pour concevoir et maintenir votre API.\n"
} ,
  
  {
    "title"    : "React/Redux: pitfalls and best practices",
    "category" : "",
    "tags"     : " js, react, redux, frontend",
    "url"      : "/2020/04/27/react-redux-pitfalls-and-best-pratices.html",
    "date"     : "April 27, 2020",
    "excerpt"  : "After 2 years using React with Redux for the video platform 6play, I was able to identify good practices and pitfalls to avoid at all costs.\nThe Bedrock team kept the technical stack of the project up to date to take advantage of the new features ...",
  "content"  : "After 2 years using React with Redux for the video platform 6play, I was able to identify good practices and pitfalls to avoid at all costs.\nThe Bedrock team kept the technical stack of the project up to date to take advantage of the new features of react, react-redux and redux.\n\nSo here are my tips for maintaining and using React and Redux in your application without going mad.\n\nThis article is not an introduction to React or Redux. I recommend this documentation if you want to see how to implement it in your applications.\n\nYou could also take a look at Redux offical style guide in which you could find some of those tips and others.\nNote that if you use the Redux Toolkit, some of the tips/practices presented in this article are already integrated directly into the API.\n\nAvoid having only one reducer\n\nThe reducer is the function that is in charge of building a new state at each action.\nOne might be tempted to manipulate only one reducer.\nIn the case of a small application, this is not a problem.\nFor applications expressing a complex and evolving business, it is better to opt in for the combineReducers solution.\n\nThis feature of redux allows to manipulate not one but several reducers which act respectively on the state.\n\n\n  When and how to split its application?\n\n\nWhat we recommend at Bedrock is a functional splitting of the application.\nIn my approach, we would tend to represent the business of the application more than the technical stuff implied.\nSome very good articles explain it notably through the use of DDD principles.\n\nIn Bedrock, we use a folder named modules which groups together the different folders associated with the feature of your application.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.reducer.js\n    product/\n      __tests__/\n        product.reducer.spec.js\n      components/\n      product.reducer.js\n    account/\n      __tests__/\n      account.reducer.spec.js\n      components/\n      account.reducer.js\n  store.js\n  index.js\n\n\nSo in store.js all you need to do is combine your different reducers.\n\nimport { createStore, combineReducers } from &#39;redux&#39;\nimport { user } from &#39;./modules/user/user.reducer.js&#39;\nimport { product } from &#39;./modules/user/product.reducer.js&#39;\nimport { account } from &#39;./modules/user/account.reducer.js&#39;\n\nexport const store = createStore(combineReducers({ user, product, account }))\n\n\nBy following this principle, you will:\n\n\n  keep reducers readable because they have a limited scope\n  structure and define the functionalities of your application\n  facilitate the testing\n\n\nHistorically, this segmentation has allowed us to remove complete application areas without having impacts on the entire codebase, just by deleting the module folder associated with the feature.\n\nProxy access to the state\n\nNow that your reducers have been placed in the functional module, you need to allow your components to access the state via selector.\nA selector is a function that has the state as a parameter, and retrieves its information.\nThis can also allow you to select only the props needed for the component by decoupling from the state structure.\n\nexport const getUserName = ({ user: { lastName } }) =&amp;gt; lastName\n\n\nYou can also pass parameters to a selector by wrapping it with a function.\n\nexport const getProduct = productId =&amp;gt; ({ product: { list } }) =&amp;gt;\n  list.find(product =&amp;gt; product.id === productId)\n\n\nThis will allow you to use them in your components using the useSelector hook.\n\nconst MyComponent = () =&amp;gt; {\n  const product = useSelector(getProduct(12))\n  return &amp;lt;div&amp;gt;{product.name}&amp;lt;/div&amp;gt;\n}\n\n\nIt is specified in the react-redux doc that the selector is called for each render of the component.\nIf the selector function reference does not change, a cached version of the object can be returned directly.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.reducer.js\n      user.selectors.js &amp;lt;--- This is where all module selectors are exported\n\n\nPrefix the name of your actions\n\n\n  I really advise you to define naming rules for your actions and if possible check them with an eslint rule.\n\n\nActions are in uppercase letters separated by ‚Äò_‚Äô.\nHere an example with this action: SET_USERS.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.actions.js &amp;lt;--- This is where all module action creators are exported\n      user.reducer.js\n      user.selectors.js\n\n\nAction names are prefixed by the name of the module in which it is located.\nThis gives a full name: user/SET_USERS.\nA big advantage of this naming rule is that you can easily filter the action in redux-devtools.\n\n\n\nAlways test your reducers\n\nThe reducers are the holders of your application‚Äôs business.\nThey manipulate the state of your application.\n\nThis code is therefore sensitive.\n\n‚û°Ô∏è A modification can have a lot of impact on your application.\n\nThis code is rich in business rules\n\n‚û°Ô∏è You must be confident that these are correctly implemented.\n\nThe good news is that this code is relatively easy to test.\nA reducer is a single function that takes 2 parameters.\nThis function will return a new state depending on the type of action and its parameters.\n\nThis is the standard structure for testing reducers with Jest:\n\ndescribe(&#39;ReducerName&#39;, () =&amp;gt; {\n  beforeEach(() =&amp;gt; {\n    // Init a new state\n  })\n  describe(&#39;ACTION&#39;, () =&amp;gt; {\n    // Group tests by action type\n    it(&#39;should test action with some params&#39;, () =&amp;gt; {})\n    it(&#39;should test action with other params&#39;, () =&amp;gt; {})\n  })\n  describe(&#39;SECOND_ACTION&#39;, () =&amp;gt; {\n    it(&#39;should test action with some params&#39;, () =&amp;gt; {})\n  })\n})\n\n\nI also recommend that you use the deep-freeze package on your state to ensure that all actions return new references.\n\nUltimately, testing your reducers will allow you to easily refactor the internal structure of their state without the risk of introducing regressions.\n\nKeep the immutability and readability of your reducers\n\nA reducer is a function that must return a new version of the state containing its new values while keeping the same references of the objects that have not changed.\nThis allows you to take full advantage of Structural sharing and avoid exploding your memory usage.\nThe use of the spread operator is thus more than recommended.\n\nHowever, in the case where the state has a complicated and deep structure, it can be verbose to change the state without destroying the references that should not change.\n\nFor example, here we want to override the Rhone.Villeurbanne.postal value of the state while keeping the objects that don‚Äôt change.\n\nconst state = {\n  Rhone: {\n    Lyon: {\n      postal: &#39;69000&#39; ,\n    },\n    Villeurbanne: {\n      postal: &#39;&#39;,\n    },\n  },\n  Is√®re: {\n    Grenoble: {\n      postal: &#39;39000&#39;,\n    },\n  },\n}\n\n// When you want to change nested state value and use immutability\nconst newState = {\n  ...state,\n  Rhone: {\n    ...state.Lyon,\n    Villeurbanne: {\n      postal: &#39;69100&#39;,\n    },\n  },\n}\n\n\nTo avoid this, a member of the Bedrock team released a package that allows to set nested attribute while ensuring immutability: immutable-set\nThis package is much easier to use than tools like immutable.js because it does not use Object prototype.\n\nimport set from &#39;immutable-set&#39;\n\nconst newState = set(state, `Rhone.Villeurbanne.postal`, &#39;69100&#39;)\n\n\nDo not use the default case\n\nThe implementation of a redux reducer very often consists of a switch where each case corresponds to an action.\nA switch must always define the default case if you follow so basic eslint rules.\n\nLet‚Äôs imagine the following reducer:\n\nconst initialState = {\n  value: &#39;bar&#39;,\n  index: 0,\n}\n\nfunction reducer(initialState, action) {\n  switch (action.type) {\n    case &#39;FOO&#39;:\n      return {\n        value: &#39;foo&#39;,\n      }\n    default:\n      return {\n        value: &#39;bar&#39;,\n      }\n  }\n}\n\n\nWe can naively say that this reducer manages two different actions. It‚Äôs okay.\nIf we isolate this reducer there are only two types of action that can change this state; the FOO action and any other action.\n\nHowever, if you have followed the advice to cut out your reducers, you don‚Äôt have only one reducer acting on your blind.\n\nThat‚Äôs where the previous reducer is a problem.\nIndeed, any other action will change this state to a default state.\nA dispatch action will pass through each of the reducers associated with this one.\nAn action at the other end of your application could affect this state without being expressed in the code.\nThis should be avoided.\n\n\n\nIf you want to modify the state with an action from another module, you can do so by adding a case on that action.\n\nfunction reducer(state = initialState, action) {\n  switch (action.type) {\n    case &#39;FOO&#39;:\n      return {\n        value: &#39;foo&#39;,\n      }\n    case &#39;otherModule/BAR&#39;:\n      return {\n        value: &#39;bar&#39;,\n      }\n    default:\n      return state\n  }\n}\n\n\nUse custom middlewares\n\nI‚Äôve often seen action behaviors being copied and pasted, from action to action.\nWhen you‚Äôre a developer, ‚Äúcopy-paste‚Äù is never the right way.\n\nThe most common example is handling HTTP calls during an action that uses redux-thunk.\n\nexport const foo = () =&amp;gt;\n  fetch(&#39;https://example.com/api/foo&#39;)\n    .then(data =&amp;gt; ({ type: &#39;FOO&#39;, data }))\n    .catch(error =&amp;gt; {\n      // Do something\n    })\n\nexport const bar = () =&amp;gt;\n  fetch(&#39;https://example.com/api/bar&#39;)\n    .then(data =&amp;gt; ({ type: &#39;BAR&#39;, data }))\n    .catch(error =&amp;gt; {\n      // Do something\n    })\n\n\nThese two actions are basically the same thing, we could very well make a factory that would do the code in common.\n\nBasically the meta action we want to represent here when it is dispatched:\n\nFetch something\n-- return action with the result\n-- in case or error, do something\n\n\nWe could very well define a middleware that would take care of this behavior.\n\nconst http = store =&amp;gt; next =&amp;gt; async action =&amp;gt; {\n  if (action.http) {\n    try {\n      action.result = await fetch(action.http)\n    } catch (error) {\n      // Do something\n    }\n  }\n  return next(action)\n}\n\n// in redux store init\nconst exampleApp = combineReducers(reducers)\nconst store = createStore(exampleApp, applyMiddleware(http))\n\n\nThus the two preceding actions could be written much more simpler:\n\nexport const foo = () =&amp;gt; ({ type: &#39;FOO&#39;, http: &#39;https://example.com/api/foo&#39; })\n\nexport const bar = () =&amp;gt; ({ type: &#39;BAR&#39;, http: &#39;https://example.com/api/bar&#39; })\n\n\nThe big advantages of using middleware in a complex application:\n\n\n  avoids code duplication\n  allows you to define common behaviors between your actions\n  standardize redux meta action types\n\n\nAvoid redux related rerender\n\nThe trick when using redux is to trigger component re-render when you connect them to the state.\nEven if rerenders are not always a problem, re-render caused by the use of redux really has to be prevented.\nJust beware of the following traps.\n\nDo not create a reference in the selector\n\nLet‚Äôs imagine the next selector:\n\nconst getUserById = userId =&amp;gt; state =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId) || {}\n\n\nThe developer here wanted to ensure that its selector is null safe and always returns an object.\nThis is something we see quite often.\n\nEach time this selector will be called for a user not present in the state, it will return a new object, a new reference.\n\n\n  With useSelector, returning a new object every time will always force a re-render by default.\nDoc of react-redux\n\n\nHowever in the case of an object, as in the example above (or an array), the reference of this default value is new each time the selector is executed.\nSimilarly for the default values in destructuring, you should never do this :\n\nconst getUsers = () =&amp;gt; ({ users: [] }) =&amp;gt; users\n\n\nWhat to do then?\nWhenever possible, the default values should be stored in the reducer.\nOtherwise, the default value must be extracted into a constant so that the reference remains the same.\n\nconst defaultUser = {}\n\nconst getUserById = userId =&amp;gt; state =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId) || defaultUser\n\n\nThe same goes for the selector usage that returns a new ref at each call.\nThe use of the filter function returns a new array each time a new reference even if the filter conditions have not changed.\n\nTo continue, it is important that useSelector does not return a function.\nBasically you should never do this:\n\nconst getUserById = state =&amp;gt; userId =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId)\nconst uider = useSelector(getUserById)(userId)\n\n\nA selector should not return a view (a copy) of the state but directly what it contains.\nBy respecting this principle, your components will rerender only if an action modifies the state.\nUtilities such as reselect can be used to implement selectors with a memory system.\n\nDo not transform your data in the components\n\nSometimes the data contained in the state is not in the correct display format.\nWe would quickly tend to generate it in the component directly.\n\nconst MyComponent = () =&amp;gt; {\n  const user = useSelector(getUser)\n\n  return (\n    &amp;lt;div&amp;gt;\n      &amp;lt;h1&amp;gt;{user.name}&amp;lt;/h1&amp;gt;\n      &amp;lt;img src={`https://profil-pic.com/${user.id}`} /&amp;gt;\n    &amp;lt;/div&amp;gt;\n  )\n}\n\n\nHere, the url of the image is dynamically computed in the component, and thus at each render.\nWe prefer to modify our reducers in order to include a profileUrl attribute so that this information is directly accessible.\n\nswitch (action.type) {\n  case `user/SET_USER`:\n    return {\n      ...state,\n      user: {\n        ...action.user,\n        profileUrl: `https://profil-pic.com/${action.user.id}`,\n      },\n    }\n}\n\n\nThis information is then calculated once per action and not every time it is rendered.\n\nDon‚Äôt use useReducer for your business data\n\nSince the arrival of hooks, we have many more tools provided directly by React to manage the state of our components.\nThe useReducer hook allows to set a state that can be modified through actions.\nWe‚Äôre really very very close to a redux state that we can associate to a component, it‚Äôs great.\n\nHowever, if you use redux in your application, it seems quite strange to have to use useReducer.\nYou already have everything you need to manipulate a complex state.\n\nMoreover, by using redux instead of the useReducer hook you can take advantage of really efficient devtools and middlewares.\n\n\n\nUseful resources\n\n\n  Use react with redux doc\n  redux flow animated by Dan Abramov\n\n  redux documentation about middlewares\n  immutable-set\n\n\nThanks to the reviewers: \n@flepretre, \n@mfrachet, \n@fdubost,\n@ncuillery,\n@renaudAmsellem\n"
} ,
  
  {
    "title"    : "How to boost the speed of your webpack build?",
    "category" : "",
    "tags"     : " js, webpack",
    "url"      : "/2020/03/05/hunting-webpack-performances.html",
    "date"     : "March 5, 2020",
    "excerpt"  : "How did I cut in half my project‚Äôs webpack build time ?\n\nWho never complained about the infinite duration of a webpack build on a project ?\nI‚Äôm currently working on a big web application coded in React/Redux with server side rendering.\nThe applica...",
  "content"  : "How did I cut in half my project‚Äôs webpack build time ?\n\nWho never complained about the infinite duration of a webpack build on a project ?\nI‚Äôm currently working on a big web application coded in React/Redux with server side rendering.\nThe application exists since 2015 and it has evolved a lot since then\n\n\n\nTLDR;\n\n\n  Never, ever, ever, ever work on performance improvements or optimization without monitoring!\n\n\nIf you want to optimize the duration of a job, you have to monitor precisely the duration of it and all its sub-steps.\nBy doing that, you can really focus on the most expensive task.\nThis will save you from wasting time on optimizations that will have little impact on the system as a whole.\nUse existing monitoring tools! Create them if they don‚Äôt exist!\n\nWhat was the problem with webpack ?\n\nFor several weeks/months my colleagues had been complaining about the duration of our yarn build command. \nThe purpose of this command is to build the distributable package of our application in a production target with webpack.\n\nI even heard:\n\n  ‚ÄúThis command, I don‚Äôt run it locally anymore, it takes too much time.‚Äù\n  ‚ÄúMy computer starts ventilating heavily every time I run this command. There‚Äôs nothing else I can do!‚Äù\n\n\nDepending on the machine on which the build was launched, it took between 5 and 12 minutes.\nIt is not possible to have a build that takes so long.\nwebpack is not a slow bundler. \nIt is our use of webpack that makes it slow.\n\nFocus error, a morning lost\n\nSince this command launches a webpack build in production mode, I figured out that the culprit was webpack config itself.\nGiven that I‚Äôve dug deep into webpack, I thought it would be interesting to focus on this performance concern.\nI have indeed open sourced a set of workshop to learn how to use webpack from scratch (https://webpack-workshop.netlify.com).\nSo at the end of January I took one day to improve the situation.\n\nI had my own idea of the task that would take the most. So I tried to improve it, spending my entire morning on it. \nI just managed to gain 17 seconds.\n\nI‚Äôm not going to lie, I was very disappointed with what I achieved.\n\nThe concern in my strategy was however obvious. \nI started off with a preconceived idea ‚ÄúThis is definitely the stage that takes the longest.‚Äù\n\nNothing was objective in my analysis.\nTo improve the performance of an application it is necessary to focus on objective facts.\n\nSuccessful afternoon\n\nWhen I came back from my lunch break, I was motivated to win more than those poor 17 seconds.\nThen I remembered the Pareto principle.\n\n\n  The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes.\nWikipedia\n\n\nThere is probably one step that takes up most of the webpack build time.\nPareto principle adapted to webpack could mean ‚Äú80% of the build time is caused by 20% of the config‚Äù\n\nLet‚Äôs find the culprit ! üéâ\n\nI had to determine the build time of each loader, of each plugin.\nI was very lucky, the webpack community has already proposed a plugin that allows to measure everything.\nAnd it is very easy to install. ‚ô•Ô∏è\n\nSpeed Measure Plugin\n\nHere are the results I got:\n\nSMP  ‚è±  \nGeneral output time took 4 mins, 5.68 secs\n\n SMP  ‚è±  Plugins\nIgnorePlugin took 57.73 secs\nTerserPlugin took 39.022 secs\nExtractCssChunksPlugin took 3.13 secs\nOptimizeCssAssetsWebpackPlugin took 1.6 secs\nManifestPlugin took 1.55 secs\nWebpackPwaManifest took 0.326 secs\nContextReplacementPlugin took 0.129 secs\nHashedModuleIdsPlugin took 0.127 secs\nGenerateSW took 0.059 secs\nDefinePlugin took 0.047 secs\nEnvironmentPlugin took 0.04 secs\nLoadablePlugin took 0.033 secs\nObject took 0.024 secs\n\n SMP  ‚è±  Loaders\nbabel-loader, and \nrev-replace-loader took 2 mins, 11.99 secs\n  module count = 2222\nmodules with no loaders took 1 min, 57.86 secs\n  module count = 2071\nextract-css-chunks-webpack-plugin, and \ncss-loader, and \npostcss-loader, and \nsass-loader took 1 min, 43.74 secs\n  module count = 95\ncss-loader, and \npostcss-loader, and \nsass-loader took 1 min, 43.61 secs\n  module count = 95\nfile-loader, and \nrev-replace-loader took 4.86 secs\n  module count = 43\nfile-loader took 2.67 secs\n  module count = 32\nraw-loader took 0.446 secs\n  module count = 1\n@bedrock/package-json-loader took 0.005 secs\n  module count = 1\nscript-loader took 0.003 secs\n  module count = 1\n\n\nAs expected, it‚Äôs not great! \nBut at least I‚Äôm starting to get who the culprits are.\nWe can see that for 2222 Javascript modules takes up 2mins but for only 95 Sass files 1min43 ü§£.\n\n\n\nDamn node-sass\n\nOnce the migration from node-sass to sass (new Sass re-implementation) and the update of sass-loader, I was shocked!\nIt took me about 10 minutes because there were few breaking changes and I gained more than 1min30 on the build time.\n\nsass-loader made big improvements on performances, you should definitely make sure you use the last version.\n\nI lost a morning on gaining 17 seconds and I spent 10 minutes to win 1min30.ü§£\n\nIgnorePlugin, TerserPlugin\n\n\n  \n    TerserPlugin is used to uglify the javascript code in order to reduce its size and readability. It‚Äôs a relatively long process, but 39 seconds is too much.\nJust by updating the version of TerserPlugin to use the one integrated in Webpack, I managed to reduce by 20 seconds the build time.\n  \n  \n    IgnorePlugin is a core plugin that was used a lot in our application to avoid loading certain scripts in order to reduce the weight of the site.\nIt was necessary, but today with Webpack we can use much better than that. Dynamic Import, ContextReplacement, there are plenty of solutions. As a general rule, we should avoid compiling files and then not using them.\n  \n\n\nRecommendations from the community\n\nTo improve the build perfs webpack provides a web page listing the actions to take to hunt what takes time.\nI strongly advise to have a look at it.\n\nhttps://webpack.js.org/guides/build-performance/\n\nFinal Result\n\n    SMP  ‚è±  \n    General output time took 2 mins, 18.27 secs\n\n\n\n\nBased on precise and concrete measures, I was able to drastically improve the webpack build of my application.\nNo more computers suffering just to compile a bit of JS and SASS.\nI could have lost whole days on futile modifications if I had not measured precisely what penalized the build.\n\n‚ÑπÔ∏è\n\n  Use Speed Measure Plugin to debug webpack build time\n  Track your build time evolution to detect big evolution before merge\n  Follow webpack performances recommandations\n  Look at webpack 5 new caching strategies\n  Keep your webpack config up to date\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #12",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2020/02/08/m6web-dev-facts-12.html",
    "date"     : "February 8, 2020",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nEn parlant de slack\n\n  stalker ou bosser il faut choisir\n\n\nEn parlant d‚Äôornithorynque:\n\n  Non mais l‚Äôaustralie, c‚Äôest le staging du monde\n\n\nLe dresseur\n\n  Mon charisme l√©gendaire a encore frapp√©‚Ä¶ les VPN...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nEn parlant de slack\n\n  stalker ou bosser il faut choisir\n\n\nEn parlant d‚Äôornithorynque:\n\n  Non mais l‚Äôaustralie, c‚Äôest le staging du monde\n\n\nLe dresseur\n\n  Mon charisme l√©gendaire a encore frapp√©‚Ä¶ les VPNs filent au pas avec moi‚Ä¶\n\n\nCloudix\n\n  Les gaulois qui avaient peur que le ciel/cloud leur tombe sur la t√™te, ils √©taient visionnaires en fait!\n\n\nLe meilleur support\nDans un bar de Lyon\n\n  Une inconnue: ‚Äú je peux pas me connecter √† l‚Äôamour est dans le pr√© ‚Ä¶‚Äù\n\n\n\n  Un dev d‚ÄôM6web: ‚Äúc‚Äôest quoi ta box ?‚Äù\n\n\nDes choix forts\n\n  Si on fait ce choix, c‚Äôest comme tuer un chaton\n\n\nObjets trouv√©s\n\n\nManger ou d√©cider il faut choisir\n\n  Une r√©union sans nourriture devrait √™tre un email.\n\n\nQuand un sysadmin craque une allumette √† la data\n\n  Scala de toute mani√®re, ce n‚Äôest qu‚Äôun wrapper Java\n\n\nC‚Äô√©tait pr√©vu\n\n  ‚ÄúQuand on passera en prod le bug‚Äù\n\n\nChandeleur\n\n  J‚Äôai de la p√¢te, j‚Äôai du sucre mais je sais pas faire des grep\n\n\nLa fatigue\n\n  D√©finition de fatigue =&amp;gt;  ‚ÄúAh des weekend comme √ßa je n‚Äôen veux plus‚Äù ‚Ä¶ Dit-il un vendredi\n\n\nSSO POWA\n\n  Vivement qu‚Äôon passe slack sous le SSO, comme √ßa les gens ne viendront plus nous dire sur slack qu‚Äôils n‚Äôont pas acc√®s au SSO\n\n\nAquarium\n\n\nToutou\n\n  J‚Äôai fait un fichier waf.tf dans le module waf, ca fait waf waf\n\n\nConfluence\n\n  Mais effectivement √† ce niveau c‚Äôest aussi facile d‚Äô√©crire des .txt qu‚Äôon se file par cl√© usb que d‚Äôutiliser Confluence.\n\n\nQuickwin\n\n  C‚Äôest un quickwin jusqu‚Äô√† ce qu‚Äôon fasse F5\n\n\nJavascript\n\n  Un dev PHP: ‚ÄúC‚Äôest du JS mais √ßa se comprend presque‚Äù\n\n\nLa pers√©v√©rance\n\n\nLes co√ªts\n\n  C‚Äôest le type de projet o√π il y a plus de jour-homme que d‚Äôutilisateurs\n\n\nJe l‚Äôai pas touch√©√©√©o\n\n  Un sysadmin: ‚ÄúC‚Äôest quoi le foot?‚Äù\n\n\n\n  Un autre: ‚ÄúC‚Äôest le truc qui fait tomber ton infra quand on en diffuse.‚Äù\n\n\nLa concurrence et la mauvaise foi\n\n  Du coup tous les trucs inutiles de netflix on les fait, mais tous les trucs utiles on les fait pas\n\n\nOul√†!\n\n  C‚Äôest fou ce qu‚Äôon peut faire aujourd‚Äôhui avec des spreadsheets‚Ä¶. Je crois bien que c‚Äôest la derni√®re fois que j‚Äôai cod√©\n\n\nUn certain K.D\n"
} ,
  
  {
    "title"    : "Migration de 6PLAY - l&#39;amour est dans le cloud",
    "category" : "",
    "tags"     : " conference, video",
    "url"      : "/2020/02/07/pascal-martin-laduckconf.html",
    "date"     : "February 7, 2020",
    "excerpt"  : "Pascal Martin a eu le plaisir d‚Äô√™tre invit√© par Octo pour un REX sur la migration de notre architecture dans le cloud.\n\nNous vous invitons √† d√©couvrir sa conf√©rence en vid√©o.\n\nSi vous voulez en savoir encore plus, Pascal a √©crit √©galement un livre...",
  "content"  : "Pascal Martin a eu le plaisir d‚Äô√™tre invit√© par Octo pour un REX sur la migration de notre architecture dans le cloud.\n\nNous vous invitons √† d√©couvrir sa conf√©rence en vid√©o.\n\nSi vous voulez en savoir encore plus, Pascal a √©crit √©galement un livre sur le sujet.\n"
} ,
  
  {
    "title"    : "ScalaIO Lyon 2019",
    "category" : "",
    "tags"     : " scalaio, scala, lyon, 2019",
    "url"      : "/2019/11/25/retour-scalaio.html",
    "date"     : "November 25, 2019",
    "excerpt"  : "Nous √©tions √† la ScalaIO 2019 organis√©e √† Lyon ! \nNous avons assist√© √† de tr√®s bonnes conf√©rences. Voici quelques mots sur les interventions qui nous ont le plus marqu√©es cette ann√©e.\n\nA live-coding introduction to Mill: finally a build tool we ca...",
  "content"  : "Nous √©tions √† la ScalaIO 2019 organis√©e √† Lyon ! \nNous avons assist√© √† de tr√®s bonnes conf√©rences. Voici quelques mots sur les interventions qui nous ont le plus marqu√©es cette ann√©e.\n\nA live-coding introduction to Mill: finally a build tool we can all understand!\n\nMill est outil de build qui permet de coder en scala les diff√©rentes √©tapes de notre build. La puissance de cet outil vient principalement du fait qu‚Äôon √©crit du code. On peut donc effectuer des op√©rations tr√®s complexes lors du build. Les fonctionnalit√©s principales sont :\n\n  La possibilit√© de builder avec diff√©rentes versions de scala une m√™me appli\n  La possibilit√© de ne builder que le code qui a √©t√© modifi√© (gr√¢ce √† l‚Äôutilisation de zinc)\n  La possibilit√© de lancer le build automatiquement √† la modification d‚Äôun fichier (option watch)\n\n\nContext Buddy: the tool that knows your code better than you\n\nContext Buddy est un plugin pour votre IDE (Intellij) qui permet de mieux parcourir l‚Äôhistorique des modifications de votre code. ContextBuddy vous permet de savoir gr√¢ce √† la coloration syntaxique exactement quel √©l√©ment de la ligne a √©t√© modifi√©. De plus, comme il se base sur les donn√©es du compilateur, il est capable de voir si une m√™me classe utilise une nouvelle version de la lib voire m√™me une nouvelle lib.\n\nRailway Oriented Programming - Une approche fonctionnelle pour la gestion d‚Äôerreurs\n\nCette conf√©rence bas√©e sur celle de Scott Wlaschin explique tr√®s bien la composition de fonction et l‚Äôint√©r√™t dans le cas de la gestion des erreurs. En effet, le code devient plus lisible et plus facilement maintenable.\n\nMetals - your next IDE?\n\nMetals est un Language Server Protocole pour Scala, ce qui permet de l‚Äôutiliser en t√¢che de fond pour ‚Äún‚Äôimporte quel‚Äù √©diteur de texte ou IDE. \nConcr√®tement, Metals n‚Äôapporte pas aujourd‚Äôhui 100% des fonctionnalit√©s d‚ÄôIdea, mais les manques sont vraiment minimes. En revanche, tout ce qui est impl√©ment√© semble √™tre plus efficace que sur Idea.\nLes principaux avantages que j‚Äôai retenu :\n\n  Presque tout comme Idea mais beaucoup plus l√©ger et rapide (pour certaines t√¢ches)\n  Fonctionne avec Maven, SBT, Fury, Gradle, Mill\n  Gloop permet le GoToDefinition et plein de choses sympas, plus rapides que l‚Äôindexation intelliJ\n  Fonctionne partiellement pour Java (juste le n√©cessaire)\n  Compilation incr√©mentale avec Zinc\n  En plein d√©veloppement, de super features dans les mois √† venir\n  Tout ce que j‚Äôoublie ;-)\n\n\nRunning Amok: Igniting a Documentation Revolution\n\nL‚Äôid√©e de Jon Pretty (@propensive) est de d√©corr√©ler la documentation du code (diff√©rent repo git) et de pouvoir r√©tro-documenter (mettre √† jour la doc des version ant√©rieures).\nAmok permet de relier chaque fichier de documentation √† un commit, √† partir duquel cette documentation est valide, permettant ainsi de sortir une release du document m√™me si la documentation n‚Äôest pas enti√®rement termin√©e (probl√®me r√©current en open-source).\n\nRefined, des Types sur mesure\n\nPermet de mettre des conditions sur un type. On peut ajouter des pr√©dicats au type (notamment des regex sur les Strings, des range de valeurs pour les Int, etc) et ainsi r√©duire les valeurs possibles. Validation au compile-time quand possible, et pour le runtime des erreurs tr√®s explicites sont jet√©es.\nPour les avantages, voir T(ype)DD, principalement la s√©curit√© apport√©e et plus besoin de tester ce qui est inclu dans les pr√©dicats, Refined le fait pour nous.\n\nApache Spark et le machine learning : r√™ves et r√©alit√©s\n\nA travers des exemples bas√©s sur une population de citrouilles (c‚Äô√©tait Halloween ;-)), Nastasia Saby (@saby_nastasia) nous a fait une pr√©sentation de Spark ML en prenant en exemple KMeans, un des algorithmes de clustering disponibles. Elle en a montr√© les limites et pr√©sent√© KMedoids, n‚Äôexistant pas dans Spark ML et plus complexe mais convergeant mieux. Elle a termin√© sur la n√©cessit√© de mettre en balance l‚Äôutilisation d‚Äôoutils communautaires test√©s et reconnus mais parfois limit√©s, versus d√©velopper ses propres librairies parfaitement adapt√©es √† ses besoins, au risque de se confronter √† des probl√®mes que d‚Äôautres ont d√©j√† r√©gl√©s.\n\nLes slides.\n"
} ,
  
  {
    "title"    : "Machine learning sans magie et sans s&#39;arracher les cheveux",
    "category" : "",
    "tags"     : " machine learning, blendwebmix, conference",
    "url"      : "/2019/11/14/machine-learning-sans-magie-et-sans-sarracher-les-cheveux.html",
    "date"     : "November 14, 2019",
    "excerpt"  : "Comprendre le machine learning en prenant l‚Äôexemple d‚Äôun barbecue.\n",
  "content"  : "Comprendre le machine learning en prenant l‚Äôexemple d‚Äôun barbecue.\n"
} ,
  
  {
    "title"    : "Forum PHP Paris 2019",
    "category" : "",
    "tags"     : " forumphp, php, afup, 2019",
    "url"      : "/2019/11/04/retour-forum-php-2019.html",
    "date"     : "November 4, 2019",
    "excerpt"  : "Comme tous les ans, nous √©tions au Forum PHP 2019 organis√© par l‚ÄôAFUP ! \nNous avons assist√© √† de tr√®s bonnes conf√©rences et √©chang√© avec beaucoup d‚Äôentre vous. Voici quelques mots sur les interventions qui nous ont le plus marqu√©es cette ann√©e.\n\n‚Äú...",
  "content"  : "Comme tous les ans, nous √©tions au Forum PHP 2019 organis√© par l‚ÄôAFUP ! \nNous avons assist√© √† de tr√®s bonnes conf√©rences et √©chang√© avec beaucoup d‚Äôentre vous. Voici quelques mots sur les interventions qui nous ont le plus marqu√©es cette ann√©e.\n\n‚ÄúPHP Pragmatic Development‚Äù et ‚ÄúL‚Äôarchitecture progressive‚Äù\n\nNous sommes plusieurs √† avoir trouv√© cette √©dition du Forum tr√®s pragmatique. Les deux conf√©rences de Frederic BOUCHERY et de Matthieu NAPOLI y sont sans doute pour quelque chose !\n\nComme Matthieu l‚Äôa rappel√©, pas besoin d‚Äôune architecture parfaite pour cr√©er un produit qui marche. Au contraire, √† nous d√©veloppeurs de savoir choisir les bonnes solutions pour r√©pondre √† un besoin. Et faire simple peut apporter infiniment plus de valeur que mettre en place une architecture parfois trop complexe !\n\nFrederic a soulign√© qu‚Äô√™tre pragmatique c‚Äô√©tait savoir √©couter son exp√©rience. Peut-√™tre m√™me savoir dialoguer entre d√©veloppeurs seniors et d√©butants, pour que l‚Äôexp√©rience des uns limite les erreurs des autres ?\n\nPHP 8 et Just In Time Compilation\n\nLe passage de PHP 5 √† PHP 7 a apport√© des gains √©normes en terme de performances, et nous sommes tous impatients de voir si PHP 8 nous r√©servera les m√™mes surprises.\n\nLe JIT est une bonne piste, en permettant de compiler le PHP directement en langage machine, pour se passer de l‚Äôex√©cution sur la machine virtuelle de PHP. Benoit JACQUEMONT a tr√®s bien d√©taill√© l‚Äôhistoire du JIT dans l‚Äô√©cosyst√®me PHP, son objectif et son fonctionnement. M√™me si les tests qu‚Äôil a effectu√© ne montrent pas de gains perceptibles, le sujet √©tait tr√®s int√©ressant.\n\n√Ä retenir : l‚Äôoptimisation du CPU pour PHP n‚Äôa pas beaucoup d‚Äôint√©r√™t si votre application passe son temps √† attendre des I/O.\n\nAggressive PHP quality assurance in 2019\n\nMarco PIVETTA est tr√®s actif dans la communaut√© PHP, notamment pour l‚ÄôORM Doctrine. Il nous a pr√©sent√© les outils qu‚Äôil consid√®re comme indispensables pour assurer la qualit√© et la robustesse d‚Äôun projet, mais aussi l‚Äôordre d‚Äôimportance pour les mettre en place selon lui.\n\nSi nous √©tions d√©j√† convaincus par l‚Äôimportance de l‚Äôanalyse statique, nous avons √©t√© intrigu√©s par la place qu‚Äôil accordait √† tous ces outils bas√©s sur les annotations PHP. Par exemple, il n‚Äôh√©site pas √† laisser publiques les propri√©t√©s de ses classes immutables, sans m√©thode get ni set, et d√©l√©guer √† la CI la responsabilit√© de v√©rifier que toutes les instances des classes avec l‚Äôannotation @psalm-immutable ne soient jamais modifi√©es‚Ä¶ D√©routant, mais √† m√©diter.\n\nMercure, et PHP s‚Äôenamoure enfin du temps r√©el\n\nPouvoir pousser, en temps r√©el, des informations depuis du code PHP server-side vers des centaines de milliers de clients, sans allumer des dizaines de serveurs ? C‚Äôest la promesse du projet Mercure, que K√©vin DUNGLAS est venu nous pr√©senter !\n\nNous avions entendu parler de ce projet sans jamais encore prendre le temps de le tester ni d‚Äôy penser plus en profondeur‚Ä¶ Apr√®s cette conf√©rence, un POC s‚Äôimpose ;-)\n\nTout pour se pr√©parer √† PHP 7.4\n\nLa prochaine version de PHP, la 7.4, devrait √™tre publi√©e en fin d‚Äôann√©e. Comme tous les ans, elle apportera un petit lot de nouveaut√©s que Damien SEGUY nous a pr√©sent√©es.\n\nNous avons h√¢te de pouvoir exploiter certaines d‚Äôentre elles. En particulier, le pre-loading, qui pourrait am√©liorer encore notre tenue √† la charge lors de nos pics de trafic quotidiens !\n\n‚ÄúEn vrac‚Äù\n\nLa derni√®re conf√©rence du premier jour de ce Forum, par Marie-C√©cile GODWIN et Thomas DI LUCCIO, visait √† nous ouvrir les yeux : en tant que designer, concepteurs ou d√©veloppeurs d‚Äôapplications et d‚Äôoutils num√©riques, nous devons penser au futur ; les ressources de notre plan√®te ne sont pas infinies.\n\nCelle du second jour √©tait plus l√©g√®re : Roland LEHOUCQ nous a parl√© de physique, en tirant ses exemples et anecdotes de Star Wars. Qu‚Äôest-ce que la Force ? Quelle puissance est capable d‚Äôexploiter Palpatine ? Ou combien de gigawatts extrait un sabre-laser ? Une tr√®s bonne cl√¥ture pour ce Forum !\n\n\n\nNous avons aussi pr√©sent√© deux conf√©rences, autour de sujets que nous pratiquons au quotidien chez M6 Distribution¬†:\n\n\n  Pascal MARTIN a donn√© quelques pistes pour am√©liorer la r√©silience d‚Äôapplications, en insistant sur le fait que nos plateformes, de plus en plus complexes, ne sont jamais op√©rationnelles : elle se trouvent en permanence dans un √©tat de service partiellement d√©grad√©.\n  Benoit VIGUIER a continu√© dans la lanc√©e de sa conf√©rence de l‚Äôann√©e derni√®re, en pr√©sentant cette fois-ci un retour d‚Äôexp√©rience apr√®s un an d‚Äôutilisation de PHP asynchrone en production. Spoiler alert : PHP r√©pond tr√®s bien au besoin et les g√©n√©rateurs sont le bien ! √Ä noter aussi son intervention aux traditionnels Lightning Talks, o√π il nous a pr√©sent√© une id√©e un peu folle : faire des interfaces graphiques avec Php.\n\n\nL‚ÄôAFUP Day 2020 Lyon est d√©j√† en train de s‚Äôorganiser ! Nous y serons sans doute en nombre et esp√©rons vous y rencontrer √† nouveau !\n\n\n"
} ,
  
  {
    "title"    : "Une application r√©siliente, dans un monde partiellement d√©grad√©",
    "category" : "",
    "tags"     : " conference, architecture, resilience, afup, cloud",
    "url"      : "/2019/10/25/une-application-resiliente-dans-un-monde-partiellement-degrade-pascal-martin.html",
    "date"     : "October 25, 2019",
    "excerpt"  : "Dans un monde en perp√©tuelle √©volution, pouvons-nous toujours atteindre ¬´¬†four-nines¬†¬ª de disponibilit√©¬†?\nCloud et Kubernetes. APIs et Microservices‚Ä¶ Nos architectures s‚Äôenrichissent et se complexifient. Au prix d‚Äôune certaine fragilit√©¬†?\n\nNous co...",
  "content"  : "Dans un monde en perp√©tuelle √©volution, pouvons-nous toujours atteindre ¬´¬†four-nines¬†¬ª de disponibilit√©¬†?\nCloud et Kubernetes. APIs et Microservices‚Ä¶ Nos architectures s‚Äôenrichissent et se complexifient. Au prix d‚Äôune certaine fragilit√©¬†?\n\nNous commencerons par d√©finir SLA, SLO et SLI et rappeler la signification de ces X-nines.\nNous montrerons ensuite comment, dans un contexte en permanence partiellement d√©grad√©, nos assemblages de services distribu√©s nuisent √† la fiabilit√© de nos plateformes.\n\nEn profitant de l‚Äôexp√©rience acquise sur 6play, nous verrons quelques pistes pour am√©liorer la r√©silience de nos applications, pour qu‚Äôelles r√©pondent √† nouveau aux besoins de notre public. Nous prononcerons peut-√™tre m√™me le terme de ¬´¬†Chaos Engineering¬†¬ª¬†;-)\n"
} ,
  
  {
    "title"    : "One year of asynchronous PHP in production",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2019/10/25/one-year-of-asynchronous-code-in-production.html",
    "date"     : "October 25, 2019",
    "excerpt"  : "Oui, il est tout √† fait possible de faire de la programmation asynchrone en PHP et il existe des librairies matures pour le mettre en place dans vos projets. Oui, √ßa peut am√©liorer consid√©rablement la performance de vos applications, mais si c‚Äô√©ta...",
  "content"  : "Oui, il est tout √† fait possible de faire de la programmation asynchrone en PHP et il existe des librairies matures pour le mettre en place dans vos projets. Oui, √ßa peut am√©liorer consid√©rablement la performance de vos applications, mais si c‚Äô√©tait aussi simple tout le monde le ferait d√©j√†. Cela fait plus d‚Äôan que les √©quipes de 6play ont franchit le pas sur certains projets et les applications asynchrones tiennent toutes leurs promesses en production, mais la mise en place a soulev√© beaucoup de questions. √Ä quels crit√®res se fier pour rendre une application asynchrone? Comment former les √©quipes sur ces nouveaux paradigmes? Comment adapter les outils existants et comment g√©rer ce nouveau type de charge sur les serveurs? Voici notre retour d‚Äôexp√©rience sur le PHP asynchrone, du d√©veloppement √† la production, en passant par la vie de tous les jours.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #11",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2019/09/02/m6web-dev-facts-11.html",
    "date"     : "September 2, 2019",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nLa persuasion\n\n  L‚Äôimportant, c‚Äôest d‚Äô√™tre convaincu qu‚Äôt‚Äôes convaincu\n\n\nParfois d√©bug c‚Äôest compliquer\n\n  Comment je teste l‚Äôalerte kidnapping en local ?\n\n\n\n  Bah tu vas dans un parc et tu enl√®ves un en...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nLa persuasion\n\n  L‚Äôimportant, c‚Äôest d‚Äô√™tre convaincu qu‚Äôt‚Äôes convaincu\n\n\nParfois d√©bug c‚Äôest compliquer\n\n  Comment je teste l‚Äôalerte kidnapping en local ?\n\n\n\n  Bah tu vas dans un parc et tu enl√®ves un enfant, d‚Äôici deux trois heures tu devrais voir l‚Äôalerte.\n\n\n√ätre √† sec\n\n  Vous pourriez tirer vers nous! On a plus de fl√®ches de nerf ?\n\n\nSeal of approval (lu sur slack)\n\n  Moi, Mr XXXX, Lead Dev de mon √©tat, sain de corps et d‚Äôesprit, atteste de la validation et du bien fond√© de cette requ√™te, m√™me si c‚Äôest un vendredi\n\n\nPr√©cision\n\n  Si il faut que ce soit pr√©cis, il faut pr√©ciser\n\n\nFrench please\n\n  _ Est ce qu‚Äôon peut parler en fran√ßais maintenant ici ?\n_ Yes.\n\n\nLa difficult√©\n\n  Il y a 2 choses de compliqu√©es en d√©veloppement : nommer les choses et invalider du cache.\nFigure toi que je suis en train de nommer une PR qui invalide du cache\n\n\nLa qualit√© avant tout\n\nif (value instanceof Collection) {\n    return ((Collection) value).isEmpty() ? false : true;\n} else if (value instanceof String) {\n    return StringUtils.isNotBlank((String) value) ? true : false;\n}\n\n\nBoire ou coder\n\n  _ J‚Äôai fait un bateau √† la ganane pour la r√©tro !\n_ Il y a du rhum dans la recette :rolling_on_the_floor_laughing: ?\n_ M√™me pas !!\n\n\nLe CDD\n\n  Conf√©rence de presse Driven Development\n\n\nMais oui c‚Äôest clair\nswitch($categoryId) {\n  case &#39;16&#39;:\n    return 38;\n  case &#39;18&#39;:\n    return 40;\n  case &#39;26&#39;:\n    return 42;\n  case &#39;28&#39;:\n    return 36;\n}\n\n\nLe nommage c‚Äôest important\n\n$catId = &#39;turlututu&#39;;\n$programId = &#39;chapeaupointu&#39;\n\n\nLa magie nuagique\n\n\n  Le cloud est un √©tat d‚Äôesprit, pas un endroit o√π on d√©ploie\n\n\nLes index commencent √† 0\n\n\n  La reproductivit√© est l‚Äô√©tape N¬∞1 du debuggage\n\n\nOn fait quoi demain ?\n\n\n  Rappelle moi d‚Äôacheter un agenda stp !\n\n\nRIP\n\n// Remplissage d&#39;un tableau de donn√©es pour un template de job (pas Steve, il est mort)\n\n\nMme Irma\n\n  Montre moi ton diff, je te dirai qui tu es!\n\n\nLes projets √† succ√®s\n\n  Il parait qu‚Äôun projet legacy c‚Äôest un projet qui a r√©ussi‚Ä¶ bin on a certains projets qui ont vachement bien r√©ussi !\n\n\nL‚Äôapp√©tit des croissants\n\n  Pourriez-vous p√©ter la prod plus souvent ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #10",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2019/08/08/m6web-dev-facts-10.html",
    "date"     : "August 8, 2019",
    "excerpt"  : "√áa faisait un tr√®s tr√®s long moment ! Voici le retour des devfacts !\n\nL‚Äôerreur est humaine\n\n  Il faut mieux recevoir une erreur 500, que 500 erreurs\n\n\nL‚Äôodeur du code\n\n  Le code de merde, tu sais quand c‚Äôest le tien, c‚Äôest comme les pets, pas la p...",
  "content"  : "√áa faisait un tr√®s tr√®s long moment ! Voici le retour des devfacts !\n\nL‚Äôerreur est humaine\n\n  Il faut mieux recevoir une erreur 500, que 500 erreurs\n\n\nL‚Äôodeur du code\n\n  Le code de merde, tu sais quand c‚Äôest le tien, c‚Äôest comme les pets, pas la peine de git blame.\n\n\nRestons zen !\n\n  _ Ah mais en fait !!! Je comprends pourquoi tu viens t√¥t le matin! Tu viens parce que c‚Äôest calme !!!!\n\n\n\n  _ Tais toi Mehdi !!\n\n\nPr√©voir l‚Äôimpr√©visible\n\n  C‚Äôest la premi√®re √©tape de l‚Äô√©tape suivante.\n\n\nCe matin au chiffrage\n\n  J‚Äôai pens√© 5 mais j‚Äôai mis 3‚Ä¶\n\n\nSchroedinger app\n\n  _ √áa marche ?\n\n\n\n  _ Je sais pas mais c‚Äôest en prod !\n\n\nL‚Äôeffet de serre\n\n  _ Il fait froid dans le bureau\n\n\n\n  _ Bah d√©marre 2-3 docker sur ton Mac, √ßa devrait r√©soudre le probl√®me\n\n\nLes priorit√©s\n\n  _ Alleeeez, s‚Äôil te plaiiiiiiiit !\n\n\n\n  _ Les incidents d‚Äôabord, les d√©tails graphiques apr√®s !\n\n\nPetite nouveaut√©, voici quelques m√®mes maison !\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "React Europe 2019",
    "category" : "",
    "tags"     : " React, JS, 6play, Conference, 2019",
    "url"      : "/2019/05/27/react-europe-2019.html",
    "date"     : "May 27, 2019",
    "excerpt"  : "The M6 Distribution‚Äôs (M6 Web‚Äôs new name!) front team hasn‚Äôt posted for a long time. We took part as listeners of the 5th European React Conference in Paris on May 24th and 25th. It‚Äôs an opportunity to talk about what are the last moves in the Rea...",
  "content"  : "The M6 Distribution‚Äôs (M6 Web‚Äôs new name!) front team hasn‚Äôt posted for a long time. We took part as listeners of the 5th European React Conference in Paris on May 24th and 25th. It‚Äôs an opportunity to talk about what are the last moves in the React community and at M6.\n\nAs usual, we were waiting for lot of announcements in this conference and a lot of new tools or new libraries. There has been no big declaration, no surprise. But many subjects were interesting and several talks confirmed the way we have taken over the last few years.\n\nHooks, more hooks and suspense\n\nDuring the keynote, Jared Palmer put the emphasis on hooks through an example to simplify the use of GraphQL queries. At M6 Distribution, we use hooks since the beginning of the year and many are in production. That has changed the way we write components. We already have used functionnal components before, but using hooks simplify the readability and the evolutivity of old class component. The bad point is testing‚Ä¶ Because we use Enzyme, testing hooks is painful for now. Until Enzyme fully supports hooks, we have implemented custom mocks.\n\nJared Palmer also showed us the interest of React Suspense to manage a main loading state in an app instead of many spinners that don‚Äôt offer a good user experience. We can‚Äôt use Suspense for our app because of the SSR. As it is recommended by the React team, we use Loadable Components instead. Jared  announced that a new asynchronous server renderer is in progress and could be released in 2019 supporting Suspense. Suspense will also include data fetching.\n\nOthers features or refactoring will come in the future, but there is very limited information:\n\n  React Fire\n  React Fusion (?)\n  React Native Fabric\n  React Flare\n\n\nDo the harlem shake\n\nSeveral conferences have addressed the theme of animations with React. This subject is very often complicated to solve. Finding a high-performance solution that is compatible with as many browsers as possible can be a real challenge.\n\nFor more than a year now, we have been migrating from Sass to styled-components. Despite some resentment, it should be noted that this new tool makes it very easy to create/compose our new visual components. It also helps us a lot with its management of the theme.\n\nBetween the react-spring library and the tips of Joshua Comeau, we now have many ways to integrate the new animations that designers can invent.\n\nTo summarize their comments, there is no single best way to make animations in all situations. Whether with canvas, SVG, web API, CSS, 2D sprite, you should try to do it in several ways to compare performance and rendering in order to choose the best option for your specific case.\n\nMake tests but make good tests\n\nLisa Gagarina gave us some tips to better testing the code. Even If we have been using ESLint and Prettier for a long time in our JS projects, we have chosen not to implement static typing for the moment. Indeed, we think that it is a huge step, it has a big impact on the code and it can make the onboarding of new developpers more complex. For now, we just use the React proptypes well.\n\nLisa also advices to better use Jest snapshots. They are often too many, too big, not clear, hard to review. And we have experienced it too. A best pratice can be to reduce snapshots size and inline it in the test file. The readability of a test is indeed very important and it is not recommended to refer to other files than the test file (this is valid for snapshots but also for fixtures). There are some ESLint rules to ensure this: jest/no-large-snapshots, jest/prefer-inline-snapshots. snapshot-diff can also helps by making a diff between the nominal case and the tested case of a component rendering.\n\nA difficult thing is to keep test agnostic to implementation details. We should consider the code as a black-box and only test the user interaction of the components, otherwise any refactoring of code will be painful and might discourage developers from writing tests.\n\nE2E tests are also complex to setup, write and debug but are absolutely necessary. For the back office app, unlike our front app where we use a custom stack, we choose Cypress a complete E2E framework that has saved us a lot of time.\n\nLisa concluded her very interesting lightning talk by saying that there is no such thing as a one-size-fits-all approach, the way of testing has to be adapted to the team and the project. For example on our 6play project, we have more than 3000 unit tests performed by Jest in less than 4 minutes and 450 E2E scenarios that save your life every day!\n\nIf you are interested in this subject, take a look at this article about JS testing best practices.\n\nFinally a little GraphQL in our projects\n\nOne of the main themes of the conference was also GraphQL. \nThis data exchange paradigm now seems to have its place among many users.\nFor more than a year now, we have been using GraphQL on our back office and we are already seeing a lot of benefits:\n\n\n  a front/back exchange contract materialized in a schema,\n  easy consumption thanks to queries,\n  cache management provided by Apollo (also providing a collection of great tools for GraphQL).\n\n\nKenny already talked about it in 2015 but now we use it!\n\nDevelopment worflow: the expected journey\n\nOne of the most interesting conferences in my opinion was about the need to optimize the development workflow. This is an element that is too often ignored but is very important in the life of a project. Paul Amstrong presents us here an analysis of the workflow of his team in charge of the development of the Twitter Lite application. He also presented some conclusions and solutions he implemented. In a standard development workflow there are 3 points that allow a significant margin of progress:\n\n  increased developer confidence and delivery speed,\n  automate the PR process to maximum,\n  detect errors as early as possible.\n\n\nThese words echo our own questions on the subject and these conclusions confirm our decisions. \nIt is important to have the shortest and most automated workflow possible between the developer and the user without sacrificing the developer‚Äôs experience because it goes hand in hand with all the other aspects of a project.\n\nAt M6 Distribution, we use most of the tools presented, but two in particular caught our attention.\n\nThe first, React Component Benchmark, is of particular interest to us because in the past we had started to investigate the subject and used tools that are depreciated today.\n\nThe second, Build Tracker, which allows us to test the evolution of bundle size, will allow us to replace an equivalent tool developed internally while providing a more detailed analysis in order to work more accurately on these issues.\n\nHere is an example of a message posted by our in-house build tracker on a Pull Request.\n\n\n\n\n\nA11y is our new challenge\n\nSeveral very inspiring talks on accessibility seem to show that this issue finally appears to be considered by web actors. In particular Facebook, which has distinguished itself by showing its assistant to detect accessibility errors in the development of its new version. However, it is regrettable that this toolkit is not accessible to the community because it could be a great help to avoid putting people from being in a situation of handicap.\n\nIt is clear that our front web app is not yet very accessible but we are working to correct this error, especially on the new screens we have been integrating for several months.\n\nOur expectations on Yarn finally fulfilled\n\nWe were waiting for it at M6, the new yarn release named berry offers almost everything we were missing in our favorite package manager.\nIndeed, by using yarn in monorepo mode for our front projects, we were confronted with several problems that had to be overcome with in-house tools. Example here with our monorepo-dependencies-check tool which is now becoming obsolete thanks to Constraints.\n\nWe are also delighted with the functionality of Zero install. But what we expected the most was about the workspaces. We will finally have a management of the publication of these.\n\nTake a look at this repository for more information.\n\nSome projects that interest us a lot: Next.js &amp;amp; Code Sandbox\n\nEven if these two tools are not used for the development of our applications, the new features of Next.js and Code Sandbox are clearly very interesting.\n\nAs for Next.js, our front web project has its own configuration of server side rendering (Florent explains it in ‚ÄòLast night isomorphic JS saved our life!‚Äô). However, NextJS is a great project that we use for many of our side projects. AMP support, client-only pages and API endpoints are clearly welcome.\n\nAs for Code Sandbox, Ives van Hoorne told us his personal story and that of his project. In addition to the great tool that is CodeSandbox, we have seen that the use of WebAssembly seems to have solved a lot of performance and implementation problems. For example, he cites the coloring of the code based on TextMate only available in C could not have been ported to the browser without going through WebAssembly.\n"
} ,
  
  {
    "title"    : "AFUP Day Lyon 2019",
    "category" : "",
    "tags"     : " php, afup, 2019",
    "url"      : "/2019/05/23/afup-day.html",
    "date"     : "May 23, 2019",
    "excerpt"  : "TechM6WEB √©tait tr√®s fier de sponsoriser le premier AFUP DAY √† Lyon.\n\nPour une premi√®re, c‚Äô√©tait tr√®s r√©ussie. Un programme au top et vari√©, qui mettait en avant de nombreuses probl√©matiques techs et soci√©tales.\n\n\n(photo : Benjamin L√©v√™que)\n\nNos c...",
  "content"  : "TechM6WEB √©tait tr√®s fier de sponsoriser le premier AFUP DAY √† Lyon.\n\nPour une premi√®re, c‚Äô√©tait tr√®s r√©ussie. Un programme au top et vari√©, qui mettait en avant de nombreuses probl√©matiques techs et soci√©tales.\n\n\n(photo : Benjamin L√©v√™que)\n\nNos conf√©rences pr√©f√©r√©es :\n\n  OVH.com from 1999 to 2019, car c‚Äôest toujours int√©ressant les REX de projets importants en toute humilit√©,\n  L‚Äôarchitecture progressive, bien qu‚Äôelle ait soulev√©e pas mal de trolls en interne,\n  Les merveilles m√©connues du SQL, car en vrai ‚Äúyou know no SQL‚Äù :)\n\n\nEt bien sur la table ronde des CTO pendant laquelle Olivier Mansour est intervenu.\n\nMerci l‚ÄôAFUP pour cet √©v√®nement pr√®s de chez nous !\n"
} ,
  
  {
    "title"    : "Migrating production applications from on-premise to the cloud with no downtime",
    "category" : "",
    "tags"     : " Cloud, AWS, Kubernetes, Kops, HAProxy, GOReplay",
    "url"      : "/2019/03/11/Migrating-production-apps-from-on-premise-to-the-cloud-with-no-downtime.html",
    "date"     : "March 11, 2019",
    "excerpt"  : "We are migrating all our on-premise applications to AWS cloud.\nMost of them are/will be migrated to Kops-managed Kubernetes clusters, and some are/will be migrated as lambdas.\nTo secure this migration, we are using HAProxy in front of both on-prem...",
  "content"  : "We are migrating all our on-premise applications to AWS cloud.\nMost of them are/will be migrated to Kops-managed Kubernetes clusters, and some are/will be migrated as lambdas.\nTo secure this migration, we are using HAProxy in front of both on-prem and on-AWS deployments (first only sending 1% of each application‚Äôs requests to AWS, then 5%, then 25% and so on).\nDisclaimer: This article describes a feedback from production environment. We have changed the name of applications mentioned here, but everything else is true within the limits of our knowledge.\n\nYou can find the content of this blogpost (and more) in a 25mn talk at the HAProxy conf given in Amsterdam in 2019 :\n\n\nThe first migrated application\n\nIt‚Äôs an API written in PHP. It has no external dependency (database, redis‚Ä¶), except for another API, called over HTTP.\nWe have migrated this application to the cloud like we‚Äôve done with other applications since.\n\nThe first step was to deploy this application to a kubernetes cluster and expose it over an ELB.\nThen, we wanted to send real-user requests to that app. We wanted to see how it would behave with real production traffic.\nBut we didn‚Äôt want to send 100% of our users over there at once: we‚Äôd first rather check everything works fine with just 1% of our users.\n\nHAProxy\n\nWe‚Äôve been using HAProxy for several years now.\nBecause of its features, like advanced backend monitoring or its enormous number of metrics, it‚Äôs the perfect tool to help us on this migration.\n\nAt first, we didn‚Äôt know how the app, the Horizontal Pod Autoscaler, Liveness probes etc. would react with real live production requests.\nSo, we decided to migrate only 1% of production HTTP requests to our Kubernetes cluster. The other 99% of HTTP requests would remain on premise, where the application works for sure.\n\nHere‚Äôs a part of the associated HAProxy configuration:\n\nbackend application-01\n    http-response add-header X-Backend-Server %s\n    balance roundrobin\n    http-request set-header Host application-01.6play.fr\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-01.6play.fr\n    server aws-prod-Kubernetes-application-01 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-01.6play.fr ca-file ca-certificates.crt inter 1s fall 1 rise 2 resolvers m6dns observe layer7 weight 25\n    server onprem-prod-front-application-01 onprem-application-01.6play.fr:80 check resolvers m6dns weight 75\n\nSome explanations on key elements of this configuration:\n\n\n  http-response add-header: adds an HTTP header to the response, with the server chosen by HAProxy to handle the request. We added this for debugging purposes: to know who handled our request when requesting the service\n  balance roundrobin: because we‚Äôre using a stateless application\n  check .. inter 1s fall 1 rise 2 in the server directive: healthchecks have 1 second of timeout, only 1 bad healthcheck is enough to mark the server as unhealthy and we need 2 good healthchecks in a row to mark it as healthy again\n  observe layer7: It will simulate a bad healthcheck for each application server error (e.g: 500, 502, 503, etc.) making this server fall in such a situation\n  weight 25: We use weights from 0 to 100 (you can go up to 256), so that corresponds to traffic percentages in our case\n\n\nWith the above configuration, as soon as there is an error, HAProxy won‚Äôt send traffic to our AWS/Kubernetes application anymore; consequently, it will have a minimum impact on endusers.\n\nTests\n\nWe first tested this with staging proxies, with a temporary domain name and a 50-50% loadbalancer, to ensure the load-balancing worked fine.\nWe tried to kill the deployment on Kubernetes to check 100% of requests came back on-prem. We tested killing random pods to see if we had some user impact. We tested to slow down the application so it would be slower than 1s to respond. We also tested to slow down only one of two pods running the application.\nIt was all OK, so we were confident to go to production.\n\nMigration steps\n\nBefore its migration, the application infrastructure looked like this:\n\n\nWe inserted HAProxy servers in that schema, so the traffic passes through them before being sent to the caches.\nThat way, HAProxy controls where traffic is sent.\nTo make those migrations as transparent as possible, we first configured HAProxy to send traffic to on-prem servers only.\nIn the same time, developers have deployed all mandatory resources (RDS, DynamoDB, elasticache, etc.) with Terraform and verified the application works fine. The application itself could have changed: either the code or kubernetes manifests.\nWhen ready, both ops and devs gave their approval to send traffic to the cloud.\n\nWe started by load-balancing 1% to the AWS ELB with HAProxy:\n\n\nWe compared everything we could:\n\n  2xx, 3xx, 4xx and 5xx percentages\n  connect and response times\n  failed healthchecks and healthchecks return codes\n  backend retries and bad responses\n\n\nWe were amazed: only 3ms in average difference between on-prem and our Kubernetes cluster in AWS cloud.\nAnd no error. Everything worked as expected. It was almost suspicious.\n\n\nThis graph shows the average connect times from HAProxy.\n\nWe‚Äôre using Paris as AWS zone and our datacenters are located in Paris too, so that explains the few milliseconds to go back and forth from HAProxy (on prem) to the cloud. In fact, this one to two millisecond between our on-prem servers and AWS is one of the reasons adding HAProxy in the mix was possible.\n\n\nThis graph shows the average response times from the application.\n\nWe also had some PHP configurations to update to be ISO prod (OPCache, APCu, etc.). Why? Well, at first, we created a quick‚Äôn dirty (working, but not optimized) Docker image for our application and it went straight to production, before our sysadmins could take a better look at it.\n\n\nThis graph shows the number of 2xx HTTP codes with 25% of traffic sent to AWS.\n\nOnce those PHP optimisations were fixed, we had only 2ms difference between our on-premise and our Kubernetes on AWS. It‚Äôs low enough to allow us to test this setup a bit longer without any visible user impact.\n\nDeploying more and more\n\n1% on our kubernetes cluster was great, but it was not enough to see perfs issues.\nSo we raised HAProxy‚Äôs load balancing from 1%/99% to 10%/90%.\nStill not enough. We raised it to 25%/75%.\nWe checked application‚Äôs pods CPU usage, it was really low. Too low to even trigger the Horizontal Pod Autoscaler based on cpu usage. We couldn‚Äôt validate our pods Requests and Limits for that Kubernetes deployment therefore.\n\nSo far, it was enough for us to validate things we could check were working fine. The application cache was efficient, we had stable performances and no surprise on traffic peaks.\n\nOn this first application migration, we decided to stay at 25% of traffic sent to the cloud for the moment, to observe.\nWe did it because HAProxy would have saved us if something bad happened.\n\nIt did behave well: nothing happened for 26 days.\n\nSome errors encountered\n\nUnknown nodes\n\nOn the 27th day, we noticed two of our three nodes were in Unknown state. As our Replicas specified we wanted several pods, Kubernetes started new pods. But as we only had one Ready node left (3 worker nodes in the cluster, including two in Unknown state), all pods for our application were now on that single node. Not great for reliability.\nWe found that the CoreOS image we used was doing automatic updates that restarted nodes regularly. On those two nodes (and on the third one a few days later!), the restart did not go well and Kubelet wasn‚Äôt starting at bootime. After investigation, it appears we changed the KOPS STATE STORE bucket. Nodes started before that change were impacted as Kubelet couldn‚Äôt find its configuration. Starting new nodes and killing the old ones solved this issue.\nThat allowed us to identify two problems: we didn‚Äôt restrict when and how automatic updates were started; and we didn‚Äôt have enough cluster monitoring.\n\nFrom that, we knew we had to monitor:\n\n  Nodes in a state different from Ready for a certain time\n  If the number of Ready nodes is at least equal to the Auto Scaling Group minimum\n  If the number of Ready nodes is at most 90% the Auto Scaling Group maximum\n\n\nDifferences between cluster-autoscaler and ASG values\n\nWhile trying to solve the problem above, we also found that pods were living around 5mns before being destroyed and created again. After some investigation, we found that min and max EC2 instances were not configured to the same values between the AWS Auto Scaling Group and the cluster-autoscaler pod.\nSome day, we modified the Kops configuration for those worker nodes and the configuration was well applied to the ASG, but was not applied to the cluster-autoscaler. As a result, we had an ASG minimum of 4 worker nodes, but a cluster-autoscaler minimum of 3.\nAt some point in the history, the cluster-autoscaler defined that the current amount of worker nodes (four) was too high compared to the real need, so it tried to reallocate pods to free up a node. It have done that by draining pods from a node. At the same time, the cluster-autoscaler tried to change the ASG‚Äôs desired value to 3 nodes. Because the minimum nodes configured for the ASG was 4, the latter denied the request. Kubernetes scheduler chose to reschedule those recently-killed pods on available nodes, starting by the one with no running pods, AKA: the one that just drained them all.\nThis last phase started again every 5 minutes, making sure that our pods did not survive longer than that.\n\nApplication latencies undetected by health checks\n\nOur application was really slow on Kubernetes/AWS (due to a network misconfiguration) but HAProxy did not disable it. We specified a 1s timeout as shown in the example above, but this is only for healthchecks. Our global server timeout is upper than 1s. Because our application calls another webservice, those calls were timeouting. HAProxy was not aware of that, because the application‚Äôs /HealthCheck health page doesn‚Äôt check external webservices and thus, were not impacted by those external webservices timeouts. This is an application choice that we can encounter on-premise too, with the exact same behavior. For that reason, we decided to change nothing for now (and we‚Äôll discuss this with the devs teams to see if there‚Äôs something we can do).\nWe don‚Äôt check external webservices in our /HealthCheck page on purpose, because that page is also tested by kubernetes for livenessProbe. Kubernetes restarts a pod when it is not healthy anymore but when it comes to an external service that is failing, restarting the current pod is non-sens. Kubernetes will restart pods again and again even if the application itself can‚Äôt to anything about it! The livenessProbe should test only what the pod does. The Amadeus team talked about that at the KubeCon EU 2018 while presenting Kubervisor.\n\nPedal to the metal\n\nWe were stabilized again.\nSo we raised HAProxy load-balancing to 50% on our application in the cloud.\nAfter seven days without any error, we pushed it to 75%.\nAfter another seven days, we passed the on-prem server as a backup in HAProxy, making the application in kubernetes receiving 100% traffic.\n\n\n\nWe stayed with that configuration for 2 months.\nThat gave us plenty of time to adapt pods Requests and Limits.\nThat is really important for us, because we use HorizontalPodAutoscaler resources with CPU metrics to scale most of our APIs. Here you can find slides deep diving one of our applications that autoscales in prod with kubernetes.\nWe had several events during those 2 months that helped us optimize Requests and Limits for that app. For example, we had holiday traffic, a football match and some special primetime sessions.\nWe also improved our knowledge of both Kubernetes and AWS during this time (I.e: What happens when we rolling restart worker nodes?). Finally, we have configured our Prometheus servers with effective and non-noisy alerts.\n\nAfter weeks of optimizations, we migrated this app‚Äôs DNS directly on ELB without HAProxy.\nEverything works perfectly as expected since that day.\n\nNext applications to migrate\n\nWe‚Äôve done a lot of work for our first migration. We‚Äôve capitalized that time for the next projects to finally be able to migrate them in few days.\nThe workflow stays unchanged:\n\n\n  Deploy the application into a kubernetes cluster\n  Add HAProxy servers in front of both on-prem and in-cloud instances\n  Load balance from 1% to 100% traffic to the in-cloud instance\n  Configure accurate Requests and Limits\n  Create efficient alerting\n  Point DNS to ELB\n\n\nMigrate an application path by path\n\nSome of our applications needed to be partially rewritten to be cloud native.\nOnly specific paths were affected by this rewrite.\n\nSo we decided to use HAProxy to migrate those applications, path by path.\nWe also used GOReplay to replicate production traffic for each path, to be sure we didn‚Äôt messed up things before sending end-users traffic.\n\n\nThis schema shows how HAProxy were routing traffic according to specific paths.\n\nThe workflow is almost the same as above, with few changes:\n\n\n  Deploy the application into a kubernetes cluster\n  Add HAProxy servers in front of both on-prem and in-cloud instances\n  Use HAProxy map_reg to route traffic, depending of the requested URL\n  Define path routing preferences in the map file created in step 3 (see example below)\n  Configure and test each path:\n    \n      Let developers rewrite paths, I.E: /HealthCheck\n      Replicate production traffic with GOReplay, to specific paths, including /HealthCheck, from on-prem to the application in the Kubernetes cluster in AWS\n      Stabilize the application: either code optimisations or Kubernetes Requests and Limits adaptations\n      Add this newly created path /HealthCheck on the HAProxy‚Äôs routing map file\n      Repeat for each new path\n    \n  \n  Create a specific HAProxy Backend section for each route to load balance traffic differently for each route\n  Increase traffic load balancing up to 100% to the cloud\n  Create efficient alerting\n  Point the DNS to the ELB\n\n\nTraffic replication with GOReplay\n\nWe use a lot GOReplay.\nNot only because it‚Äôs light and easy to work with, but because we can do whatever we want with it to replicate traffic. It can rewrite headers, catch only a specific domain or a specific url. It‚Äôs the perfect tool to complete our migration workflow.\n\nHere is a script we used in the step 5.b of the workflow above:\n\n#!/bin/bash\n\nreplicate_traffic() {\n    if [[ -z $1 ]]\n    then\n        local REPLICATION_PERCENTAGE=5%\n    else\n        local REPLICATION_PERCENTAGE=$1\n    fi\n\n    if [[ -z $2 ]]\n    then\n        local TIMEOUT=45s\n    else\n        local TIMEOUT=$2\n    fi\n\n    echo &quot;Replicating traffic at ${REPLICATION_PERCENTAGE} for $TIMEOUT&quot;\n\n    ./gor -exit-after $TIMEOUT \\\n    -input-raw :8080 \\\n    -http-disallow-url /v2/critical/sensible_datas/payments/ \\\n    -http-allow-url /v2 \\\n    -input-raw-bpf-filter &quot;dst host 127.0.0.72&quot; \\\n    -output-http &quot;https://application-02.6play.fr/|${REPLICATION_PERCENTAGE}&quot; \\\n    -http-original-host \\\n    2&amp;gt;/dev/null\n}\n\n# The above allows a normal ramp-up of the traffic.\n# That means application replicas can be low and increase naturally without an insane peak\nreplicate_traffic 1% 45s\nreplicate_traffic 2% 45s\nreplicate_traffic 5% 45s\nreplicate_traffic 10% 45s\nreplicate_traffic 20% 60s\nreplicate_traffic 40% 60s\nreplicate_traffic 60% 60s\nreplicate_traffic 80% 60s\nreplicate_traffic 100% 7h\n\nWe‚Äôre using this script and not directly the gor command, to do a slow ramp-up of traffic to the application in Kubernetes.\nOtherwise, since the application is not stressed before traffic is replicated, replicating 100% of traffic all of a sudden would not be representative of real user behavior. It would led to unwanted alerts that would disappear in minutes with auto-scaling, but that would have rang anyway. So we chose to avoid that noise by doing a slow ramp-up to make traffic replication more real.\n\nWe could follow the replication with HAProxy dashboard, like the following graph:\n\n\nHAProxy configuration\n\nTo achieve a path-by-path migration of an application, we used this HAProxy configuration:\n\nfrontend application-02\n    ...\n    # Defined with a &quot;map&quot; style, from file /etc/haproxy/domain2backend.map\n    # CF https://blog.haproxy.com/2015/01/26/web-application-name-to-backend-mapping-in-haproxy/\n    use_backend %[base,map_reg(/etc/haproxy/domain2backend.map,bk_default)]\n\nbackend application-02-on-prem\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns\n\nbackend application-02-on-cloud\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns\n\nbackend application-02-mixed\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns weight 75\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns inter 3s fall 1 rise 2 observe layer7 weight 25\n\nbackend application-02-mixed-critical\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns weight 99\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns inter 3s fall 1 rise 2 observe layer7 weight 1\n\nAnd here‚Äôs the associated map file:\n\n#HOST Header                                                            #HAP backend_name\napplication-02\\.6play\\.fr\\/v2\\/critical\\/(\\w{1,45})\\/payments\\/         application-02-mixed-critical\napplication-02\\.6play\\.fr\\/v2\\/customers\\/                              application-02-mixed\napplication-02\\.6play\\.fr\\/v2\\/                                         application-02-on-cloud\napplication-02\\.6play\\.fr\\/v1\\/                                         application-02-on-prem\n\n# Catch ALL for application-02.6play.fr\napplication-02\\.6play\\.fr\\/                                             application-02-on-prem\n\n\nSome examples of traffic routing made by HAProxy with the configuration above:\n\n\n  application-02.6play.fr/v2/critical/sensible_datas/payments/\n    \n      sent on the specific application-02-mixed-critical backend,\n      with 1% traffic sent to the cloud\n    \n  \n  application-02.6play.fr/v2/customers/\n    \n      sent on application-02-mixed,\n      load balanced at 25% on the cloud\n    \n  \n  application-02.6play.fr/v2/\n    \n      sent on application-02-on-cloud,\n      only on the AWS ELB: cloud only\n    \n  \n  application-02.6play.fr/v1/\n    \n      sent on application-02-on-prem,\n      only on-premise\n    \n  \n  application-02.6play.fr/admin/\n    \n      sent on-premise only,\n      this is the default\n    \n  \n\n\nWith HAProxy map files and the according backend sections, we‚Äôre able to migrate path by path any application from on-premise to our kubernetes cluster in the cloud.\nWith gor on top of it, it‚Äôs even easier to allow developers develop a specific path while another is being migrated, and all that, with no downtime.\n\nNext steps\n\nWe‚Äôve done most of our cloud migration with workflows explained in this blogpost.\nThanks to HAProxy, most of our applications could be migrated at the same time with no impact from on migration to another.\n\nThere are still some applications to migrate though and one of them is a tough one. This application is heavily using Cassandra database. There is no Cassandra managed in AWS, so we are completely rewriting the application to adapt it to DynamoDB and also to face upcoming business needs.\nThe challenge is to keep existing pathUrl of the application, working. In other words: the new version have to give same functionalities, keeping the same URLs, but with totally different data management under the hood.\nGOReplay is a wonderful asset to help us in this task.\n\nIf you found this useful and you‚Äôd like more production return of experiences like this one, please let us know. We plan to write more in the coming weeks.\n"
} ,
  
  {
    "title"    : "7 conseils pour d√©marrer avec Spark",
    "category" : "",
    "tags"     : " spark, scalaio, conference",
    "url"      : "/2019/01/14/7-conseils-pour-demarrer-avec-spark.html",
    "date"     : "January 14, 2019",
    "excerpt"  : "Je suis entr√©e dans le monde de la data avec Spark. \nIl y a eu des moments clairement plus ou moins compliqu√©s. \nAu d√©but, c‚Äô√©tait le Far West : un monde inconnu et dangereux (il ne fallait pas casser la production). \nAvec ce retour d‚Äôexp√©rience, ...",
  "content"  : "Je suis entr√©e dans le monde de la data avec Spark. \nIl y a eu des moments clairement plus ou moins compliqu√©s. \nAu d√©but, c‚Äô√©tait le Far West : un monde inconnu et dangereux (il ne fallait pas casser la production). \nAvec ce retour d‚Äôexp√©rience, je vous propose de vous dire ce que j‚Äôaurais aim√© qu‚Äôon me dise avant de me lancer. \nJe promets aussi de vous parler de ce que bien heureusement mes camarades plus exp√©riment√©s m‚Äôont aussi donn√© comme astuces pour m‚Äôaider dans ce grand saut. \nCe sera donc une entr√©e en mati√®re dans le monde de Spark au travers de 7 conseils qui m‚Äôont √©t√© ou m‚Äôauraient √©t√© bien pratiques pour d√©marrer.\n"
} ,
  
  {
    "title"    : "Le Plan Copenhague : notre migration vers Le Cloud, retour d‚Äôexp√©rience",
    "category" : "",
    "tags"     : " cloud, kubernetes, aws, terraform, livre",
    "url"      : "/2018/12/20/le-plan-copenhague.html",
    "date"     : "December 20, 2018",
    "excerpt"  : "Nous avons commenc√© √† migrer notre plateforme 6play vers Le Cloud il y a un an.\n\nDepuis, nous avons d√©couvert Kubernetes et Helm, AWS et ses services manag√©s, Terraform, Prometheus et une multitude d‚Äôautres outils. Nous avons fait des choix, r√©pon...",
  "content"  : "Nous avons commenc√© √† migrer notre plateforme 6play vers Le Cloud il y a un an.\n\nDepuis, nous avons d√©couvert Kubernetes et Helm, AWS et ses services manag√©s, Terraform, Prometheus et une multitude d‚Äôautres outils. Nous avons fait des choix, r√©pondu √† de nombreuses questions, rencontr√© et franchi des obstacles. Nous avons mis en place des bases solides ou, parfois, pris des raccourcis pour avancer plus vite.\n\nVous aimeriez d√©couvrir comment nous migrons une plateforme comme 6play vers Le Cloud, comment nous exploitons Kubernetes ou les services manag√©s d‚ÄôAWS‚Äâ? Vous vous demandez comment nous optimiserons les co√ªts ? Vous migrez peut-√™tre vous aussi votre h√©bergement, et comparer votre exp√©rience √† la n√¥tre vous aiderait √† avancer‚Äâ?\n\nPascal, DevOps qui accompagne cette migration vers Le Cloud, a commenc√© √† r√©diger un retour d‚Äôexp√©rience autour de ce projet. Les sept premiers chapitres viennent d‚Äô√™tre publi√©s : vous pouvez d√®s maintenant commencer √† lire ¬´‚ÄâLe Plan Copenhague‚Äâ¬ª !\n\nPendant ces cent premi√®res pages, vous d√©couvrirez notre projet, les premi√®res bases que nous avons construites et la mise en place de notre environnement chez AWS et sous Kubernetes. Nous irons jusqu‚Äô√† pr√©senter comment nous avons migr√© notre premi√®re application vers Le Cloud en toute s√©curit√©. La r√©daction des chapitres suivants va s‚Äô√©taler sur une bonne partie de 2019. N‚Äôh√©sitez pas √† vous inscrire pour √™tre pr√©venu lors de leur publication ;-)\n\nBonne lecture !\n"
} ,
  
  {
    "title"    : "Forum PHP Paris 2018",
    "category" : "",
    "tags"     : " forumphp, php, afup, 2018",
    "url"      : "/2018/11/12/retour-forum-php-2018.html",
    "date"     : "November 12, 2018",
    "excerpt"  : "Comme tous les ans, nous √©tions au Forum PHP 2018 organis√© par l‚ÄôAFUP ! Encore une fois, nous avons pu assister √† plusieurs conf√©rences et √©changer avec grand nombre d‚Äôentre vous. Voici quelques mots sur celles qui nous ont le plus marqu√©.\n\n‚ÄúBoost...",
  "content"  : "Comme tous les ans, nous √©tions au Forum PHP 2018 organis√© par l‚ÄôAFUP ! Encore une fois, nous avons pu assister √† plusieurs conf√©rences et √©changer avec grand nombre d‚Äôentre vous. Voici quelques mots sur celles qui nous ont le plus marqu√©.\n\n‚ÄúBoostez vos applications avec HTTP/2‚Äù\n\nNous connaissons et utilisons tous HTTP au quotidien. Mais que se cache-t-il derri√®re ce protocole ?\nK√©vin Dunglas nous √† pr√©sent√© les nouvelles fonctionnalit√©s apport√©es par HTTP/2.\nIl nous a d‚Äôabord fait un rappel sur les concepts HTTP et sur l‚Äô√©volution de ce protocole con√ßu pour √©changer des donn√©es documentaires.\nApr√®s plus de 20 ans en version 1, HTTP √©volue et passe en version 2 !\n\nHTTP/2 √©tait initialement propuls√© par Google. Premi√®re bonne nouvelle, pas de changement n√©cessaire du c√¥t√© de nos applications PHP. La version 2 introduit de nouvelles fonctionnalit√©s int√©ressantes :\n\n\n  Priorisation des requ√™tes\n  Passage au binaire (optimisation de la taille des messages)\n  Notifications push\n  Et bien d‚Äôautre\n\n\nK√©vin nous a aussi pr√©sent√© le protocole Mercure. Il permet de faire des notifications push server side vers diff√©rents clients. Mercure semble simplifier grandement les √©changes push client / serveur. Toutes les parties dialoguent au travers d‚Äôun hub (r√¥le primaire de Mercure) et se synchronisent entre elles. Une petite d√©mo √† confirm√© l‚Äôeffet ‚Äúwhaou‚Äù de cette nouvelle solution.\n\n‚ÄúBeyond the design patterns and principles - writing good OO code‚Äù et ‚ÄúHow I started to love what they call Design Patterns‚Äù\n\nCe Forum a aussi √©t√© l‚Äôoccasion de voir (ou revoir) quelques principes fondamentaux autour des Design Patterns, avec Matthias Noback et Samuel Roze.\n\nIls ont pr√©sent√© des exemples concrets, des mises en application de certains Design Patterns incontournables, toujours dans l‚Äôoptique de d√©coupler notre logique m√©tier, de mieux r√©utiliser notre code et en am√©liorer la maintenabilit√©. Le Domain Driven Design a donc logiquement √©t√© mis √† l‚Äôhonneur, ainsi que le typage fort pour donner du sens au code et favoriser sa bonne utilisation. Pour √©viter la dette technique, il faut ‚Äúacheter en avance la capacit√© de changer‚Äù.\n\n\n\n‚ÄúWe got rid of management‚Äù\n\nMichelle Sanver nous a pr√©sent√© l‚Äôholacratie : un syst√®me d‚Äôorganisation bas√© sur l‚Äôintelligence collective, utilis√© chez Liip. Elle a d√©marr√© la conf√©rence en expliquant les d√©fauts de la hi√©rarchie pyramidale (la photo de la slide parle d‚Äôelle-m√™me ;-)) :\n\n\n\nEn holacratie, la soci√©t√© s‚Äôorganise en cercles et sous cercles de responsabilit√© et chaque collaborateur se voit donner des r√¥les et les moyens de l‚Äôassumer. Les d√©cisions sont prises collectivement et le pouvoir n‚Äôest pas centralis√© dans les mains de quelques personnes. Tout est bas√© sur la transparence, en particulier la r√©mun√©ration et les budgets. Le but principal est d‚Äôassurer des conditions de travail bienveillantes et que chacun se sente en s√©curit√© pour exprimer au mieux ses talents et r√©duire les tensions. Notamment, toutes les r√©unions sont facultatives :-D\n\nLiip a d√©velopp√© un outil communautaire en SAS (www.holaspirit.com) qui simplifie l‚Äôorganisation en g√©rant les cercles, les prises de d√©cisions et la remont√©e des propositions d‚Äôam√©liorations (ou ‚Äútensions‚Äù). Cette outil para√Æt indispensable, car central dans l‚Äôorganisation.\n\nOn voit tr√®s bien comment une holacratie peut se mettre en place dans une nouvelle organisation ou de taille r√©duite mais la conf√©renci√®re n‚Äôaborde pas trop cet aspect pour les entreprises de taille cons√©quente.\n\n‚ÄúVous n‚Äôavez pas besoin de √ßa‚Äù\n\nLe d√©but de la conf√©rence √©tait volontairement critique sur l‚Äôutilisation des nouvelles technos puis peu √† peu se dirigeait sur du bon sens dans le choix des technos avec de bonnes raisons (et non pas car elles sont ‚Äú√† la mode‚Äù).\n\nPour cela (Charles Desneuf) nous a cit√© les avantages et inconv√©nients de diff√©rents outils (micro-services, GraphQL, SinglePageApp, Microservices, GraphQL,‚Ä¶)\n\nLe but √©tait vraiment de pousser √† la r√©flexion dans ce type de choix en prenant bien en compte le contexte (utilisateurs, √©quipe, qualit√©s attendues,‚Ä¶) et de ne pas apporter de la complexit√© inutilement (optimisation/abstraction pr√©matur√©e, mod√©lisation inadapt√©,..).\n\nUne conf√©rence bien dirig√©e en concluant par :\n‚ÄúVous n‚Äôavez (peut-√™tre) pas besoin de √ßa (maintenant).‚Äù\n\n‚ÄúCessons les estimations‚Äù\n\nFr√©d√©ric Legu√©dois nous a livr√© une conf√©rence proche du one man show sur les estimations et les deadlines. Il nous a rappel√© un point important que l‚Äôon oublie parfois : Les estimations ne sont QUE des estimations, on ne peut pas les prendre comme des engagements de la part de celui qui les fait. Ses exemples humoristiques sur les diff√©rentes fa√ßons dont sont faites les estimations provoquaient fr√©quemment l‚Äôhilarit√© de l‚Äôaudience qui r√©pondait par des applaudissements g√©n√©reux.\n\nM√™me si le trait √©tait forc√©ment grossi pour le ¬´ spectacle ¬ª on a pass√© un tr√®s bon moment et √ßa fait r√©fl√©chir, notamment sur le fait que le changement est normal.\n\n‚ÄúEn vrac‚Äù\n\nQuelques autres conf√©rences que vous pouvez visionner sur le site de l‚ÄôAFUP :\n\n\n  ¬´ Serverless et PHP ¬ª : Matthieu Napoli nous a montr√© comment d√©ployer des fonctions Lambda chez AWS en PHP ‚Äì et pourquoi.\n  ¬´ D√©veloppeurs de jeux vid√©o: les rois de la combine ¬ª : Laurent Victorino nous a compl√®tement enfum√© avec sa pr√©sentation interactive !\n  ¬´ Voyage au centre du cerveau humain, ou comment manipuler les donn√©es binaires ¬ª : un retour d‚Äôexp√©rience enrichissant de Thomas Jarrand, parlant d‚ÄôIRM, de binaire, ou encore de voxels.\n\n\nNous avons aussi pr√©sent√© trois conf√©rences, autour de sujets que nous pratiquons au quotidien chez M6 Web :\n\n\n  Benoit Viguier a parl√© de programmation asynchrone avec les g√©n√©rateurs de PHP : une fonctionnalit√© extr√™mement puissante, mais encore trop peu connue. Il a d‚Äôailleurs annonc√© la sortie de la biblioth√®que Tornado. (vid√©o de la conf√©rence)\n  Guillaume Bouyge nous a racont√© l‚Äôhistoire de la migration √† l‚Äôinternational de la plate-forme 6play : comment, en partant d‚Äôun produit d√©velopp√© pour M6, nous sommes arriv√©s √† un produit en marque-blanche vendue √† d‚Äôautres clients d‚Äôautres pays. (vid√©o de la conf√©rence)\n  Et Pascal Martin a pr√©sent√© Kubernetes, l‚Äôoutil que nous utilisons pour piloter des conteneurs Docker dans Le Cloud. Il a encha√Æn√© avec plus de d√©tails sur le processus que nous suivons pour migrer nos projets vers cet h√©bergement ; vous pourrez en apprendre plus en lisant Le Plan Copenhague. (vid√©o de la conf√©rence)\n\n\nVivement l‚ÄôAFUP Day Lyon 2019, o√π nous aurons sans doute le plaisir de vous rencontrer √† nouveau ?\n"
} ,
  
  {
    "title"    : "Generators for Asynchronous Programming: User Manual",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2018/10/26/generators-for-async-programming-user-manual.html",
    "date"     : "October 26, 2018",
    "excerpt"  : "Les g√©n√©rateurs sont souvent r√©duits √† une simplification des it√©rateurs, mais ils sont surtout tr√®s pratiques et performants pour executer des traitements asynchrones. Nous aborderons le fonctionnement d‚Äôun programme asynchrone, le r√¥le des promi...",
  "content"  : "Les g√©n√©rateurs sont souvent r√©duits √† une simplification des it√©rateurs, mais ils sont surtout tr√®s pratiques et performants pour executer des traitements asynchrones. Nous aborderons le fonctionnement d‚Äôun programme asynchrone, le r√¥le des promises, et approfondirons l‚Äôutilisation des g√©n√©rateurs pour simplifier l‚Äô√©criture de notre code. Enfin nous d√©taillerons des cas pratiques ¬´ pr√™ts √† l‚Äôemploi ¬ª pour tout type d‚Äôapplication, avec un retour d‚Äôexp√©rience sur ce qui a √©t√© mis en place chez M6Web.\n"
} ,
  
  {
    "title"    : "Docker en prod ? Oui, avec Kubernetes !",
    "category" : "",
    "tags"     : " conference, php, open-source, afup, docker, kubernetes",
    "url"      : "/2018/10/26/docker-en-prod-oui-avec-kubernetes-pascal-martin.html",
    "date"     : "October 26, 2018",
    "excerpt"  : "Kubernetes. √Ä en croire certains articles, c‚Äôest une solution miracle. D√©veloppeurs, vous avez peut-√™tre entendu ce mot¬†?\nC‚Äôest l‚Äôoutil qui vous permettra de d√©ployer du Docker en production¬†! Parce qu‚Äôautant utiliser Docker en dev c‚Äôest facile, a...",
  "content"  : "Kubernetes. √Ä en croire certains articles, c‚Äôest une solution miracle. D√©veloppeurs, vous avez peut-√™tre entendu ce mot¬†?\nC‚Äôest l‚Äôoutil qui vous permettra de d√©ployer du Docker en production¬†! Parce qu‚Äôautant utiliser Docker en dev c‚Äôest facile, autant en prod‚Ä¶\n\nMais qu‚Äôest-ce que Kubernetes¬†? Quelles possibilit√©s si int√©ressantes nous fournit cet orchestrateur de conteneurs¬†?\nPods, nodes, deployments, services, ou auto-scaling et health checks¬†: autant de primitives et de fonctionnalit√©s que vous allez d√©couvrir et adorer, y compris en tant que d√©veloppeurs¬†!\n\nApr√®s avoir pr√©sent√© ces bases, je vous proposerai un retour d‚Äôexp√©rience sur la migration vers Kubernetes que nous sommes en train d‚Äôeffectuer pour 6play.fr. Comment d√©veloppeurs et sysadmins se r√©partissent-ils les t√¢ches¬†? Avons-nous d√ª adapter nos applications PHP¬†? Quelles difficult√©s avons-nous rencontr√©es, quels compromis avons-nous accept√©s et quelle route nous reste-t-il √† parcourir¬†?\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, global review",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/24/kubecon-2018-copenhaguen-global-review.html",
    "date"     : "May 24, 2018",
    "excerpt"  : "After those three days at KubeCon, we‚Äôve seen and heard a lot of interesting ideas. You can read about our first day here, about our second day there, and about our third day there. If we had to do a short recap, here are the points we would list....",
  "content"  : "After those three days at KubeCon, we‚Äôve seen and heard a lot of interesting ideas. You can read about our first day here, about our second day there, and about our third day there. If we had to do a short recap, here are the points we would list.\n\nFirst of all ‚ÄúCloud native‚Äù seems to be the buzzword of the year. Not just cloud anymore, but cloud native! What does it mean? Instead of just deploying your application to the cloud, it should fully use the cloud.\n\nThen, Kubernetes. It is a mature solution in itself. There doesn‚Äôt seem to be any doubt left about that. Even if this is the case with Kubernetes, the majority of the ecosystem around it is not mature yet, and that‚Äôs a bit of a problem. We saw a lot of tools and some that were presented during talks are still WIP and some demos completely failed. If you‚Äôre surfing the Kubernetes wave, please be careful with the tools you choose, and don‚Äôt loose yourself adopting a fancy/non-working tool that will bring your infrastructure down. Continue to master what you do without being trapped by the hype brought by certain solutions.\n\nDeployment, CI and CD. Well, not so much. There are a few projects out there and several different approaches (kubectl apply, a bit of Jenkins around it, deployments from inside the cluster, several black boxes like CodeFresh‚Ä¶), but not one thing that everyone is doing/using. We are currently looking at Jenkins-X and hope we‚Äôll be able to build our CI/CD stack using it.\n\nFor monitoring, use Prometheus. It‚Äôs pretty much what everyone is using.\n\nService mesh. One of the big things this year, everybody is talking about it, Istio seems to be taking the lead. It‚Äôs still moving fast, though and not enough people are truly mastering this in a production setup.\n\nDevelopment environment. Well, ‚ÄúLOL‚Äù would do it maybe. This is clearly not a priority yet. Some teams and projects (like Telepresence) have started working on this, but there is still road ahead.\n\nGitOps. Everybody is going this way. Versioning, of course. But also using Git events to pilot things.\n\nThings are beginning to move on the security side of things. Companies are starting to notice there is work to be done, startups are appearing with different services.\n\nAnd, finally, multi-clusters. We felt a few people are using multi-cluster, but it‚Äôs often done by hand. That doesn‚Äôt seem mature at all. Maybe a subject we‚Äôll hear more about in the future?\n\nIn any case, we had a really great time during this KubeCon in Copenhagen, we saw many interesting talks and discussed with lots of people!\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 3",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/23/kubecon-2018-copenhaguen-day-3.html",
    "date"     : "May 23, 2018",
    "excerpt"  : "Back to KubeCon 2018 in Copenhagen, for the third and last day! You can read about our first day here and about our second day there.\n\nKeynotes\n\nCloud Native ML on Kubernetes - David Aronchick, Product Manager, Cloud AI and Co-Founder of Kubeflow,...",
  "content"  : "Back to KubeCon 2018 in Copenhagen, for the third and last day! You can read about our first day here and about our second day there.\n\nKeynotes\n\nCloud Native ML on Kubernetes - David Aronchick, Product Manager, Cloud AI and Co-Founder of Kubeflow, Google &amp;amp; Vishnu Kannan, Sr. Software Engineer, Google\n\nTrying to run Machine Learning on Kubernetes? Working with Jupyter and Tensorflow? Kuebeflow could be of interest to you.\n\nRunning with Scissors - Liz Rice, Technology Evangelist, Aqua Security\n\nEven if we never fall, running with scissors could be dangerous. We wouldn‚Äôt run with scissors, would we? Then, why do we keep running containers with privileges they should not need? And why do we mount more directories than needed as volumes?\n\n\n\nChaos Engineering WG Deep Dive ‚Äì Sylvain Hellegouarch, ChaosIQ\n\nOne of the goals of Chaos Engineering being to break stuff, the talk started by a word about mindset: one must love supporting the team, and not try to save the day herself. Also, one must nurture empathy (including for the system), be assertive but not arrogant, not blame / be snarky (many of us are not great on that point).\n\nChaos Engineering follows a continuous loop. Start with a steady state (a baseline that comes from objective observation, which means you need business metrics and to collect data), formulate an hypothesis (not necessarily business oriented), and define an experimental method (the things we vary to prove / disprove the hypothesis). Warning: don‚Äôt vary too many things at once.\n\nThe talk finished with a few words about the Chaos Toolkit, which aims to be simpler than the famous Chaos Monkey.\n\nIstio - The Weather Company‚Äôs Journey - Nick Nellis &amp;amp; Fabio Oliveira, IBM\n\nSeveral istio talks weren‚Äôt enough for me, I wanted more. I didn‚Äôt learn much more on this new one, except those tips:\n\n  You can define route specific retries with Istio\n  They use vistio to visualize istio traffic. That tool seems based on Netflix‚Äôs Vizceral. Unfortunately, I couldn‚Äôt find any GH repo nor blog talking about vistio.\n  If you want to implement Istio: start small\n\n\nAre You Ready to Be Edgy? ‚Äî Bringing Cloud-Native Applications to the Edge of the Network - Megan O‚ÄôKeefe &amp;amp; Steve Louie, Cisco\n\nThere are a few common problems in a cloud computing setting. Many devices (with IoT for instance) can mean a bottleneck on the network side of things, the cloud being far away can cause latency problems. Edge computing, moving apps (or parts of apps) from a centralized cloud, could help with those problems. Think AR/VR for latency, Video for high bandwidth, facial recognition for temporary/secure data.\n\nThe talk introduced the idea of deploying Kubernetes in edge locations ‚Äì like in cell towers ‚Äì and pointing users to the closest location. Those Kubernetes clusters would only host APIs or applications with specific needs, while the main parts would remain in a centralized cloud, which means we would have to develop edge-ready applications, keeping in mind problems such as network splits, data synchronization, deployment, ‚Ä¶\n\nIntegrating Prometheus and InfluxDB - Paul Dix, InfluxData\n\nI was a lot surprised in this talk as we were like 30 in a room for 300.\nI guess people don‚Äôt need to keep metrics more than 15days with the Prometheus engine, or maybe people are already using InfluxDB as a guaranteed long term storage.\nBecause we are in the second case, we are testing influxDB with Prometheus, so this talk came at the right time.\nI didn‚Äôt learn much on InfluxDB + Prometheus, nor on federated queries that comes with HA.\n\nPaul then questioned why not use a single query language for all query engines? That would be more practical and maintainable. The question is still open, even if Paul proposes IFQL to rule them all. The main idea is for all query engines to coordinate and stop creating a new language on each new engine.\n\nA Hackers Guide to Kubernetes and the Cloud - Rory McCune, NCC Group PLC\n\nFirst, a word about threat models. Pretty much everyone will see random Internet hackers looking for easy preys. Some of us will be specifically targeted by attackers. Only a few will be targeted by nation states. You should think about your threat model, which may or may not be the same as your neighbour‚Äôs.\n\nThen, time to think about your attack surface. Attackers will find the weakest point, which is not always where your might think. What about the cloud around your Kubernetes cluster? Github is a great way of getting accesses (many commit their credentials and/or do not remove them from history ‚Äì bots are crawling this!). Developers‚Äô laptop are generally full of interesting data, and are not necessarily protected enough.\n\nOn Kubernetes, external attackers will try to access the API server and etcd, the kubelets, or maybe inject malicious containers. You should turn off the insecure port, control access to kubelet and etcd, retrict the use of service tokens, restrict privileged containers, enable authentication and authorization on the API server, set pod security and network policies, and do regular upgrades. Also, don‚Äôt forget about cloud permissions.\n\nCloudbursting with Kubernetes - Irfan Ur Rehman &amp;amp; Quinton Hoole, Huawei Technologies\n\nThat might not be everyone‚Äôs problem, but still interesting to hear about. If you have multiple cloud providers with different pricings, you might want to optimize your costs by using the most expensive only on load peaks. That is exactly what they did, using Kubernetes clusters federation and specific annotations. We won‚Äôt go over this approach because we‚Äôll stick with one cloud provider, but that may be interesting for some people.\n\nOperating a Global-Scale FaaS on Top of Kubernetes - Chad Arimura &amp;amp; Matt Stephenson, Oracle\n\nThis was about the Fn project. A couples of problems related to multitenancy: network isolation on Kubernetes, noisy neighbors (I/O being the bottleneck). Helm has a few limits, worked around with shell scripts.\n\nInside Kubernetes Resource Management (QoS) ‚Äì Mechanics and Lessons from the Field - Michael Gasch, VMware\n\nResource management goes through cgroups. With containers, we see all the CPU/RAM, but this doesn‚Äôt mean we‚Äôll be able to use them all: we may have to share with other containers. Works with requests. For now, cpu and memory are stable resources, but others (hugepages, ephemeral storage, device plugins) are in beta. You should align Kubernetes‚Äô QoS with the underlying infrastructure, enable quotas in the cluster, and protect critical system pods.\n\nI have to admit I didn‚Äôt take much notes during this talk, but noted the slides contain a lot of informations ‚Äì for more, go read them ;-)\n\nObserving and Troubleshooting your Microservices with Istio - Isaiah Snell-feikema, IBM &amp;amp; Douglas Reid, Google\n\nI promise, this is the last Istio conf I went to.\nIn case that wasn‚Äôt obvious, Istio is becoming the default service mesh, like Prometheus is for metrics. I couldn‚Äôt work much on it so I wanted to learn the most possible from it during this KubeCon. I can say this talk was one of the best for Istio discovery and even advanced skill. I won‚Äôt be able to summarize everything, so here are few tips I kept from it:\n\n  Envoy‚Äôs /stats route gives a lot of infos of servers\n  Istio system logs gives also traffic spikes\n  Istio‚Äôs access logs can be uploaded to fluentd/elk\n  Canary testing / blue-green deployments can be done via RouteRule CRD.\n\n\nIf you are considering using Istio, you must see the slides\n\nVitess and a Kubernetes Operator - Sugu Sougoumarane, YouTube\n\nI heard about Vitess for the first time during this KubeCon, even though it‚Äôs an old project. It‚Äôs a middleware for MySQL, sitting between the database and our applications. It helps scale it through sharding ‚Äì the goal being to answer the common pain points for databases: scalability, cloud and making DBAs happy. It‚Äôs also one of the CNCF project I noted a few days ago I should take a closer look at.\n\nFinal words?\n\nThe weather was nice and our plane was only on Saturday, so we finished the day with a walk in the City.\n\n\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 2",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/22/kubecon-2018-copenhaguen-day-2.html",
    "date"     : "May 22, 2018",
    "excerpt"  : "After an interesting first day at KubeCon 2018, we are back for the second day!\n\nAutoscale your Kubernetes Workload with Prometheus - Frederic Branczyk, CoreOS\n\nThe goal of autoscaling, ultimately, is to fullfill SLO of SLA through SLI (yeah, you ...",
  "content"  : "After an interesting first day at KubeCon 2018, we are back for the second day!\n\nAutoscale your Kubernetes Workload with Prometheus - Frederic Branczyk, CoreOS\n\nThe goal of autoscaling, ultimately, is to fullfill SLO of SLA through SLI (yeah, you may have to think for a while after reading this ^^). Demand must be measured by metrics, which must themselves be collected, stored and made queryable. Autoscaling can be horizontal (increase replicas when necessary, the focus of this talk) or vertical (increase resources request/limits when necessary).\n\nAutoscaling on Kubernetes used to rely on Heapster. But Heapster‚Äôs API is unspecified, it doesn‚Äôt work with Prometheus, and vendor implementations are often unmaintained. Starting with 1.8, Kubernetes provides a resource and custom metrics API. It is a specification (not an implementation), the implementations are developed and maintained by vendors, and each metric returns a single value. Kubernetes‚Äô HPA (HorizontalPodAutoscaler) uses these metrics to scale up/down.\n\nCore metrics are CPU and RAM, by pod/container/node, and there is a canonical implementation called metrics-server. The custom metrics API has the same semantics (a single value is returned) but no canonical implementation is provided (take a look at DirectXMan12/k8s-prometheus-adapter for an implementation). Each metric is related to a Kubernetes object (Pod, Service, Deployment, ‚Ä¶). Finally, there is also an external metrics API (currently in alpha stage) for things not related to a Kubernetes object (like the queue length for a queuing service provided by a Cloud provider ‚Äì SQS on AWS, for example).\n\nPod Anomaly Detection and Eviction using Prometheus Metrics - David Benque &amp;amp; Cedric Lamoriniere, Amadeus\n\nThis talk started with a reminder: stability is hard, especially for a distributed system. And using load-balancers doesn‚Äôt help at all, on the contrary. Solutions include proximity-based load-balancing (sharding by Availability Zone, which is not something Kubernetes does natively, but for which Istio can help with its pilot-agent proxy as it is AZ-aware). Using healthchecks (liveness kills the container when it fails, readiness only removes it temporarily from service discovery when it fails) is a good idea, but you should keep probes simple (complexity = bugs) and you shouldn‚Äôt check external dependencies. Also, don‚Äôt forget about circuit breakers and retries. But note all this suffers from limitations, as it‚Äôs only based on technical signals and depends on local decisions (pods/containers, service mesh/proxy).\n\nThis conference was about a tool called Kubervisor, which aims to detect mis-behaving Pods and remove them from the cluster (switching a label on the corresponding service). The decisions are based on metrics (using PromQL for Prometheus metrics) that can include business metrics, and not only technical ones nor data limited to a specific pod.\n\nI didn‚Äôt write much about this, I was too busy watching the demo, which was really interesting. In the end, I wrote down I should (and I probably will) take a better look at this in a few months, when we have deployed a few more pieces of software to Kubernetes.\n\nChallenges to Writing Cloud Native Applications - Vallery Lancey, Checkfront\n\nBuilding and deploying an application to the cloud has advantages: automatic scaling, load-balancing, replication, infrastructure provisioning and teardown is done for us, ‚Ä¶ It also has challenges.\n\nStorage of persistent data is one of these challenges (simple replication is rarely a good idea, working with unreplicated shards is a common pattern, using multiple volatile copies is a good strategy, but the best approach is runtime data replication ‚Äì which requires a large setup and implies non-negligible maintenance costs). Services coupling (database + services interacting with it) and internal API calls (source of delays) are also common sources of troubles, even if the second can be mitigated with simpler API actions, endpoints for specific actions (not CRUD!), batch endpoints and caching. Testing is a huge concern, especially with microservices, and working with discrete compoments helps, as we can run one service and mock the others. Finally, local development is not a solved problem yet (in any case, you should remove the ‚Äúbuild an image‚Äù step).\n\nGitOps for Istio - Manage Istio Config like Code - Varun Talwar, Stealth Startup &amp;amp; Alexis Richardson, WeaveWorks\n\nDuring this talk, WeaveWorks team talked about how Istio config can be managed like code through git based workflows.\nThey evoked using Terraform to describe cloud state.\nAs of gitops principles, devs shouldn‚Äôt use kubectl to interact with clusters. Additionally, they should push code, not containers. GitHub events must lead deployments, not humans.\nAs part of this automation, deployments must auto-rollout when things break. They either fail or succeed cleanly.\nOne can use operator patterns to help integrating those concepts. The WeaveWorks team also talked about flux to manage environments states.\nWeave Flux brings a lot more annotations for Istio, making automated releases deployments, etc.\n\nOPA: The Cloud Native Policy Engine - Torin Sandall, Styra\n\nPolicy enforcement is a fundamental problem for an organisation, and policy decisions should be decoupled from policy enforcement. Open Policy Agent is an open-source general-purpose policy engine. It uses a high-level declarative language, can be used to implement RBAC and has integrations with Istio or Terraform. This is not my current priority, but it could be worth taking a look at OPA if you need to add policy enforcement to your application.\n\nKubernetes Multi-Cluster Operations without Federation - Rob Szumski, CoreOS\n\nA lot of people are using multiple Kubernetes clusters. For example, Zalando uses 80. It can become a mess to manage all those clusters with their specific components like secrets, controllers, configmaps, etc.\nTo solve this problem, a new cli has been created: kubefed.\nBut Rob explains that this new tool doesn‚Äôt solve all the problems. I.E: You‚Äôll have to give access to all-clusters to people and not only few clusters (that breaks isolation), the Federation API must be run by a top-root user (accessing everything), etc.\n\nCoreOs brought the concept of k8s operators.\nRob explains why that solves problems and why you should use that instead of Federation.\n\nClearly I wasn‚Äôt convinced at all by this presentation. Besides, we are working in the same building and problems brought by Rob (modifications by hundred of devs/SRE split across the world) do not concern us at the moment.\n\nBuilding a Kubernetes Scheduler using Custom Metrics - Mateo Burillo, Sysdig\n\nThere are so many possibilities when scheduling pods. The scheduler first applies filters (resource requests, volumes, selectors/taints), then ranks (including the default behavior of spreading pods of the same service), then goes to applying hard constraints (taints, node selector), and soft contraints (prefer no schedule, node affinity, pod affinity, weight) and pod priority and does taint-based evictions. To understand what actually goes on and prevent complex situations, you should only add important constraints.\n\nIn some cases, you might want to build a custom scheduler, using custom metrics (when the default scheduler is not good enough for you and/or you have very specific needs). An example, based on sysdig metrics: draios/kubernetes-scheduler. And more informations in this blog-post. Remember creating a scheduler is not an easy task and many things can go wrong (think about concurrency and race conditions).\n\nIn the end, the idea of implementing a custom scheduler might be interesting, but a bit scary: messing things up could mean no pod getting scheduled, which is not a nice scenario. I‚Äôm not sure I currently see a situation in which I‚Äôd go this way‚Ä¶\n\nClusters as Cattle: How to Seamlessly Migrate Apps across Kubernetes Clusters - Andy Goldstein, Heptio\n\nAs many people, Andy has a lot of clusters. To re-route traffic between clusters, he uses Envoy.\nTo maintain consistent configurations, he uses Ansible to provision everything.\nSo far, I don‚Äôt really see the point of having a lot of clusters and even less of migrating a single app between clusters, but that can be interesting for people who like that trend.\n\nParty, Tivoli Gardens\n\nFor the evening, we went to an all-attendees party at Tivoli Gardens, an amusement park and pleasure garden right in the middle of Copenhagen. We walked around for a bit, before settling for a beer and a few snacks, talking with other French Kubenetes fans.\n\n\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 1",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/21/kubecon-2018-copenhaguen-day-1.html",
    "date"     : "May 21, 2018",
    "excerpt"  : "At the very beginning of May, we (Pascal and Vincent) went to KubeCon 2018. It was a three-days long event, with more than 300 conferences. 4300 people met at Bella Center, a huge conference place close to Copenhagen in Denmark. Here are some of o...",
  "content"  : "At the very beginning of May, we (Pascal and Vincent) went to KubeCon 2018. It was a three-days long event, with more than 300 conferences. 4300 people met at Bella Center, a huge conference place close to Copenhagen in Denmark. Here are some of our notes about some talks we saw.\n\nKeynotes\n\nCNCF Project Update - Liz Rice, Technology Evangelist, Aqua Security; Sugu Sougoumarane, CTO, PlanetScale Data; Colin Sullivan, Product Manager, Synadia Communications, Inc. &amp;amp; Andrew Jessup, Co-founder, Scytale Inc.\n\nDuring this keynote, I realized the CNCF is helping a lot more projects than I thought it was: it‚Äôs not just Kubernetes. I will take a closer look to some of them in the future ‚Äì they are all listed on l.cncf.io.\n\nCERN Experiences with Multi-Cloud Federated Kubernetes - Ricardo Rocha, Staff Member, CERN &amp;amp; Clenimar Filemon, Software Engineer, Federal University of Campina Grande\n\nOK, so, sometimes, it actually is rocket-science (or pretty close to it). It‚Äôs nice seeing Kubernetes used for science and research, on a federation of around 400 clusters!\n\n\n\nWhats Up With All The Different Container Runtimes? - Ricardo Aravena, Branch Metrics\n\nOverview of the different containers runtimes, starting with OpenVZ in 2006 (still maintained, but the last 2.7 version is not as stable as the previous one, and it doesn‚Äôt support Kubernetes), and LXC (Kubernetes support is low-priority WIP and LXC uses a specific image format) in 2011. Docker (initially based on LXC) arrived in 2013 and things have gone insane since then. With libcontainer and rkt in 2014, OCI in 2015 and CRI in 2016. Today, Kubernetes supports several runtimes.\n\nrkt could be interesting from a security point of view (supports TPM and VMs). With Kubernetes 1.10, the default runtime is runc. crun is the most interesting runtime for performances, but is WIP and isn‚Äôt currently used much. Kata, released in 2018 has the best security (it runs containers in VMs) and is OCI compliant, but is slower and more heavyweight. Other very specific runtimes include nvidia, railcar, pouch, lmctfy, systemd-nspawn‚Ä¶\n\nBasically, today and for most workloads, you should go with the standard docker/containerd runtime. There is a convergence towards OCI, it‚Äôs the default for Kubernetes and docker is going to adopt the CRI plugin.\n\nIntroduction to Istio Configuration - Joy Zhang, Google\n\nAn introduction to the Istio Service Mesh. All Istio components are CRDs. CRDs are becoming a standard when it comes to kubernetes customizations, here requests proxying.\n\nThis talk described Istio components, notably:\n\n\n  Mesh config - global Istio config\n  Service config - Istio operators config\n  Consumer config - overrides config model\n  Galley - Istio config per cluster\n\n\nFYI: you need as much Galleys as you have clusters + environments.\n\nContinuous Delivery Meets Custom Kubernetes Controller: A Declarative Configuration Approach to CI/CD - Suneeta Mall &amp;amp; Simon Cochrane, Nearmap\n\nKubernetes is great but, CI/CD is not really its job ‚Äì and CI/CD depends a lot on the company you work at and its culture. Here, they started deploying their applications with kubectl and YAML files, and even if CI usually doesn‚Äôt cause much troubles, CD is not that easy. And using a managed CI (CircleCI, shippable, AWS Codepipeline‚Ä¶) means exposing Kubernetes‚Äôs control plane on the Internet, which is not great. Also, the Gitops approach with its git is the source of truth mindset is OK, but committing version numbers again and again pollutes the history with a lot of noise ‚Äì when this history could be kept in the cluster itself.\n\nSo, they went with some kind of CD lite: a service running in the cluster, which monitors the registry and deploys the new version of an image when it sees it. This means the cluster doesn‚Äôt need to access git and the CI chain doesn‚Äôt need to access the cluster, making the configuration simpler.\n\nThe tool they developed for this is nearmap/cvmanager. It seems relatively easy to install and configure. And I like the idea of having a gap between Git/CI and CD. I may have to test this, especially to see what it can do when it comes to canary or blue/green deployments, but this is one of the things I saw at KubeCon I will discuss with my colleagues!\n\nPractical and Useful Latency Analysis using Istio and OpenCensus - Varun Talwar, Stealth Startup &amp;amp; Morgan McLean, Google\n\nOpenCensus (distributed tracing metrics system) + Istio is the combo provided for this talk to let devs debug the most of their apps.\n\nOpenCensus is a tracing tool, like the CNCF‚Äôs project Opentracing. It can be implemented in many languages, starting with those we use: PHP and javascript.\n\nOpenCensus can trace RPC and http APIs calls.\n\nYou can install a dedicated dashboard that gives a lot of metrics out of the box (like 90th percentiles, etc.) and allows customizable ones.\n\nMixer (Istio tool) makes the aggregation between those metrics from what is gathered by OpenCensus.\n\nThis definitely need to be tested, but not sure it is worth migrating from OpenTracing to OpenCensus.\n\nHabitat Builder: Cloud Native Application Automation - Elliott Davis &amp;amp; Fletcher Nichol, Chef.io (Habitat.sh)\n\nI had never heard of Habitat before, so I was kind of curious what this was about. This idea of platform-independent build artifacts (with exporters for docker, Kubernetes, Helm) could be interesting for some teams, but it‚Äôs not a need I currently have. The automated deployments might be interesting though, but we are already looking at other tools.\n\nKubernetes and Taxes: Lessons Learned at the Norwegian Tax Administration - A Production Case Study - Bjarte S. Karlsen, The Norwegian Tax Administration\n\nReally nice production case study from the Norwegian Tax Administration about their k8s platform. They are currently using Rancher 2.0 + OpenShift + CodeFresh on top of k8s. All their Docker images are alpine based. They develop Java applications.\n\nOne idea kept my attention is the tagging of their containers:\n\n\n  Pushing docker image tag 1.2.3 also pushes tag 1.2 and tag 1.\n  So going to v2 and rolling back to v1 effectively rolls out 2.0.0 to 1.2.3 without knowing the exact subversions.\n  You are assured that the tag of the major version always points to the latest subversion.\n\n\nIt‚Äôs not clear to me how to implement that. Maybe a codefresh hack. But still, I found the approach interesting.\n\nAnother lesson for those who want to migrate from on-premise to the cloud is to keep things that work on-premise and simply migrate them on cloud as it is. That will ease the migration. You can rethink all afterward if needed, but the first step is to migrate, not rebuild from scratch plus migrating.\n\nAs they use CodeFresh for CI/CD, it‚Äôs easy for them to automate their pipelines. That‚Äôs another lesson: automate everything. To automate, you need to standardize.\n\nAnother lesson is to use what is rock-solid. We all see a lot of tools and startups around the cloud nebula. A lot of them won‚Äôt last and some will, like Kubernetes. This is the tip: use what will last. Don‚Äôt build your whole infrastructure on something unstable or with poor pro/community support.\n\nThe final point I kept from this really good rex, is to create a predictable infrastructure. You cannot guess what will happen. You have to know and use the right tools/annotations to make it behave the way you want, predictably and repeatedly.\n\nHow to Export Prometheus Metrics from Just About Anything - Matt Layher, DigitalOcean\n\nThis presentation was about a few good practices to follow when it comes to exposing Prometheus metrics from a Go application. Basically, you should use the Go client library, be really careful about concurrency, build reusable packages, write unit-tests, use promtool check-metrics, and read and follow the Prometheus metrics best practices.\n\nContinuously Deliver your Kubernetes Infrastructure - Mikkel Larsen, Zalando SE\n\nAnother really good rex from Zalando from their utilization of k8s in prod and lessons learned.\n\nThey talked of Stups, a Zalando toolset around AWS. That definitely needs to be tested.\n\nFrom their experience of managing a k8s cluster on AWS EC2s, they gave us few tips:\n\n\n  Always upgrade to the latest k8s version\n  Manage the smallest possible number of clusters\n  Automate all the things. The only manual step should be merging PRs. This is a base GitOps principle.\n  Define an AWS HA control plane setup behind ELBs. That can be debated but this is a good first step.\n  All cluster config files must be git versioned (another GitOps principle). An upgrade is then only a git branch merge at some point.\n\n\nSome of the points above can be achieved via a CD tool. I remember they use Jenkins for that, but not 100% sure. Alongside this CD tool, there should be a CI tool (or one tool for both).\n\nThey gave us some points on CI tests too:\n\n\n  Run e2e conformance tests for k8s config files\n  Run statefulSet tests\n  Run any additional homemade tests\n\n\nFor those who are using AWS, keep in mind the following: volumes cannot be mounted across several AZ.\n\nKeep yourself away from unavailability by always setting minAvailable.\n\nFinal tip: If you go for a self-managed k8s cluster (not EKS, GKE, etc.), check that nodes are up and running before continuing upgrade.\n\nI really enjoyed this rex that was full of good prod-ready advices.\n\nI recommend you take a look at the slides\n\nSeamless Development Environments on Kubernetes using Telepresence - Ara Pulido, Bitnami\n\nKubernetes is a great production environment, but it feels like development environment is kind of an afterthought: even for a simple application, if you want to develop (locally?), things are not easy. People are currently using two distinct ways: using docker compose to replicate the production environment (but compose doesn‚Äôt do everything: rbac, job, ingress‚Ä¶ and having to maintain everything twice is not fun), or build/push/deploy-to-k8s and wait many seconds everytime one wants to F5 on a page, which is unberably slow (I wouldn‚Äôt ask my developer colleagues to do this for even half a day!).\n\nThe solution proposed during this conference is Telepresence. It allows a developer to swap out a pod from a cluster and inject her own pod, running locally, at its place. Some sort of VPN is established between her computer and the cluster, which means the pod running locally behaves just like if it was still in the cluster (including DNS, service discovery, access to non-Kubernetes managed services and all).\n\nThere are still limitations and constraints (if two developers want to work on the same service, they‚Äôll each need their own namespace in the cluster, as two people cannot swap out the same pod), but plans for this project are interesting and I will definitely take a closer look at it in a couple of months, when I start thinking more about our development stack!\n\nWe went to Datawire‚Äôs booth and saw a nice demo. And also learnt about other tools, such as Forge and Ambassador that can duplicate production requests to a local pod. We found that this feature is ultra dope!\n\nPerformance and Scale @ Istio Service Mesh - Fawad Khaliq, VMware Inc, Laurent Demailly, Google &amp;amp; Surya V Duggirala, IBM\n\nReturn of Istio devs on project‚Äôs recent updates: closed PRs, enhancements, etc.\n\nThat was not really what I was looking for so I went to some bootcamps to say hi, especially the HAProxy bootcamp one that is always a good moment. Special thanks to Baptiste for his time and the awesome talk we had!\nFor the record: HAProxy has it‚Äôs own Ingress Controller\n\nFrom Data Centers to Cloud Native - Dave Zolotusky &amp;amp; James Wen, Spotify\n\nThis last conference of the first day was about Spotify‚Äôs migration from their on-premise datacenters to the cloud. For many years, they were doing everything on-prem (including a 3000 nodes Hadoop cluster ‚Äì the largest in Europe, at the time), often developing their own proprietary software (like custom monitoring, proprietary messaging framework, custom Java service framework, custom container orchestrator, ‚Ä¶ Some have been open-sourced). The first step for them has been to get out of their own datacenters, moving everything to another datacenter (but still using their proprietary stuff). It took them three years and a half, trying to make this migration as seamless as possible for the development teams.\n\nNext step is to become cloud native, especially moving to Kubernetes. They did this in several steps, starting small by sending production traffic to one service deployed to one cluster for one hour (allowed them to validate DNS, logging, service discovery, metrics system, networking). Then, three services on one cluster (permissions, namespaces, quotas for each namespace, developers documentation). After that, services on a volunteer basis (clusters, scripted clusters creation, secrets, deployment tooling based on a wrapper around kubectl, CI integration =&amp;gt; a lot of learning for a lot of people). Then, two high-traffic services, including a service receiving 1.5 million requests per second (horizontal auto-scaling, network setup, confidence, reference for other projects). And, finally, self-service migration, with teams migrating when they want, following the docs, and ops not always knowing what‚Äôs running in the cluster (reliability, alerts, on-call, disaster recovery, backups, sustainable deploy). Everything going pretty much fine by now, it‚Äôs time to investigate on a few odd things and specific needs, with a temporary ops team assembled to help.\n\nThe most important idea here is you don‚Äôt have to do everything right from the start. For example, they waited quite a long time before setting up a sustainable deployment method, which might seem odd to many of us. But it allowed them to move forward and validate a lot of things one after the other. That‚Äôs something I will keep in mind: if it worked for them (4000 employees, including 500 techies), it could work for many other companies!\n\nJenkins X: Easy CI/CD for Kubernetes - James Strachan, CloudBees\n\nThis might be one of the hottest project of this early 2018.\n\nWe already saw this project that was created in February, and we are using it for testing purposes. We hope to use it in production very soon.\n\nFor those of you who don‚Äôt know Jenkins-X:\n\n\n  It‚Äôs piloted by jx, a command line tool (Mac/Linux)\n  It drives a Jenkins instance + Docker Registry + Nexus + Chartmuseum + Monocular\n  It allows you to manage your app‚Äôs deployments via Jenkins blueocean‚Äôs pipelines with k8s endpoints\n  That means Jenkins will be able to run CI tests, Continuously Deploy your project to preview, staging, prod and so on with Skaffold/Helm to k8s\n  Jenkins will run pipelines from the Jenkinsfile in the repo to do that CI/CD part\n  In the provided pipelines given with jx import, you will use provided docker images that embed jx cli and other tools to manage the deployments of your app.\n  That allows you to promote your app between stages, build your docker image, etc. in your pipeline steps.\n  Those deployments are based on Helm Charts in the repo.\n  Jenkins-x follows GitOps strategy, that means anything useful is stored in each app‚Äôs repo: it is versioned and git events will trigger pipelines.\n\n\nJenkins-X brings this CI/CD part that was missing for k8s users. Gitlab + gitlab-ci were already doing that for some years now, but nothing was that fancy for GH users.\n\nWe are very excited about Jenkins-X, as it answers a lot of problematics and brings in gitops as core concept. We‚Äôre actively adopting it and we hope to give feedback asap.\n\nKeynote: Anatomy of a Production Kubernetes Outage - Oliver Beattie, Head of Engineering, Monzo Bank\n\nThis keynote was about  major outage at Monzo. You can read more about it in the post-mortem they posted after it happened. Basically, even when you are careful, an outage can still happen: several causes combined with a very specific bug happening with specific versions in a specific case and voil√†. Nice talk, and nice to hear a bank being so open!\n\nKeynote: Prometheus 2.0 ‚Äì The Next Scale of Cloud Native Monitoring - Fabian Reinartz, Software Engineer, Google\n\nPrometheus is the monitoring stack everyone seems to be using now. This keynote presented how much faster Prometheus 2.x is, compared to Prometheus 1.x. Having never used the 1.x versions, I have to admit I never suffered from it. It is still nice noting 2.x scales much better (requires less RAM/CPU and its performances don‚Äôt degrade much with a huge number of metrics).\n\nWelcome reception\n\nThis first day ended with a nice buffet at Bella Center, next to the sponsor booths. As we each one went to see different talks, it allowed us to chat about what we saw and heard, even if we didn‚Äôt stick around too long, after such a long day ‚Äì especially knowing there would be two more just after!\n\n\n"
} ,
  
  {
    "title"    : "PHP Tour Montpellier 2018",
    "category" : "",
    "tags"     : " phptour, php, afup, 2018",
    "url"      : "/2018/05/17/retour-php-tour-2018.html",
    "date"     : "May 17, 2018",
    "excerpt"  : "Cette ann√©e encore, M6Web a sponsoris√© le PHP Tour, organis√© cette ann√©e par l‚ÄôAFUP √† Montpellier.\nNous √©tions donc nombreux pour assister √† l‚Äôensemble des conf√©rences. Comme d‚Äôhabitude avec l‚ÄôAFUP, les conf√©rences √©taient de bonne qualit√©, et il ...",
  "content"  : "Cette ann√©e encore, M6Web a sponsoris√© le PHP Tour, organis√© cette ann√©e par l‚ÄôAFUP √† Montpellier.\nNous √©tions donc nombreux pour assister √† l‚Äôensemble des conf√©rences. Comme d‚Äôhabitude avec l‚ÄôAFUP, les conf√©rences √©taient de bonne qualit√©, et il y en avait pour tous : d√©butants comme utilisateurs avanc√©s.\n\nPour la premi√®re fois, les conf√©rences √©taient donn√©es dans un cin√©ma Gaumont. Un tr√®s bon choix en termes de configuration : visibilit√©, confort, son et lumi√®re !\n\nEn attendant la mise en ligne des vid√©os, nous remercions les conf√©renci√®res et les conf√©renciers pour leurs pr√©sentations. Vous trouverez ci-dessous quelques mots sur les conf√©rences que nous avons particuli√®rement appr√©ci√©es.\n\n‚ÄúTirer le maximum du moteur PHP7‚Äù\n\nConf√©rence donn√©e par Nicolas Grekas.\n\nL‚Äôapproche de Nicolas √©tait tr√®s int√©ressante et nous √† permis de mieux comprendre le fonctionnement interne de Symfony, en lien avec les optimisations apport√©es par le nouveau moteur de PHP7.\n\nLes exemples cit√©s nous ont permis de voir qu‚Äôavec quelques ‚Äútips‚Äù, il est possible de ‚Äúbypasser‚Äù des √©tapes co√ªteuses lors de l‚Äôex√©cution de notre code.\n\n‚Äú100% asynchrone - 0% callback en PHP‚Äù\n\nUne pr√©sentation de Joel Wurtz.\n\nCette conf√©rence nous a permis d‚Äôaborder un sujet assez peu connu dans l‚Äôunivers PHP : l‚Äôasynchrone.\n\nD√®s qu‚Äôun projet commence √† √™tre complexe, il est souvent possible de r√©aliser des t√¢ches en parall√®le, non bloquantes, permettant d‚Äôoptimiser les temps de r√©ponse.\n\nPour r√©pondre √† ce besoin, Joel nous a pr√©sent√© le concept de l‚Äôasynchrone : l&#39;event loop.\nVia cette boucle, Joel nous a expliqu√© comment les √©v√®nements sont ‚Äúdispatch√©s‚Äù au travers de g√©n√©rateurs.\n\nPour aller plus loin, Joel nous a aussi parl√© des outils existants qui impl√©mentent cette logique d‚Äôevent loop : AMP.\n\nEnfin pour terminer, pour √™tre 0% callback, Joel nous a pr√©sent√© Fiber. Cette extension impl√©mente la RFC Fiber actuellement en cours d‚Äôhomologation.\n\nNous vous recommandons de creuser ce sujet, qui selons nous, ouvre de belles perspectives dans l‚Äôunivers PHP !\n\n‚ÄúBienvenue dans la matrice !‚Äù\n\nCette conf√©rence √©tait anim√©e par Benoit Jacquemont.\n\nAujourd‚Äôhui encore, les d√©veloppeurs ont trop peu de connaissance sur ce qu‚Äôil se passe √† bas niveau sur nos serveurs.\n\nCette conf√©rence, qui pr√©sentait notamment strace (pour suivre les appels syst√®me) et ltrace (pour suivre les appels aux fonctions de biblioth√®ques), √©tait donc particuli√®rement rafra√Æchissante. La d√©mo ‚Äúcomment voir les requ√™tes et r√©ponse en HTTPS, en clair‚Äù, √©tait compl√®tement bluffante !\n\n‚ÄúSans documentation, la fonctionnalit√© n‚Äôexiste pas !‚Äù\n\nCe talk √©tait propos√© par Sarah Ha√Øm-Lubczanski.\n\nTout le monde, dans sa vie de d√©veloppeur, a √©t√© confront√© au probl√®me suivant : √©crire la documentation des fonctionnalit√©s d√©velopp√©es. \nC‚Äôest un challenge auquel nous nous sommes nous-m√™me confront√©s lorsque nous avons travaill√©, l‚Äôann√©e derni√®re, sur l‚Äôinternationalisation de notre plate-forme, puisque nous avons d√ª documenter nos API, d√©sormais appel√©es par des coll√®gues bas√©s dans d‚Äôautres pays.\n\nSarah nous a montr√© comment faire face √† cette barri√®re souvent per√ßue comme insurmontable par bon nombre d‚Äôentre nous.\nElle nous a pour cela donn√© les cl√©s et les bonnes pratiques pour cr√©er, maintenir et r√©diger une documentation coh√©rente.\n\nOn retiendra aussi la pr√©sentation des diff√©rents outils open source de gestion de documentation.\n\n‚ÄúA la d√©couverte du Workflow‚Äù\n\nConf√©rence anim√©e par Gregoire Pineau.\n\nCette conf√©rence a retenu notre attention. Particuli√®rement bien faite, elle r√©sume les fonctionnalit√©s de ce nouveau composant de Symfony, en partant d‚Äôun workflow simple jusqu‚Äôau r√©seau de P√©tri. Gr√©goire donnait des exemples d‚Äôutilisations concr√®tes.\n\nDifficile √† r√©sumer, je vous invite √† consulter la documentation Symfony sur ce composant.\n\nMais encore ?\n\nDe plus, trois conf√©rences ont attir√© notre attention de par leur valeur p√©dagogique. Elles √©taient √† nos yeux particuli√®rement  int√©ressantes pour des d√©butants ou des personnes ne connaissant pas encore le fonctionnement de certains processus suivis par notre communaut√©.\n\nDans l‚Äôordre, vous trouverez :\n\n\n  IT figures par Sara Golemon, qui revient sur ce qu‚Äôest le FIG, organisme important qui r√©git aujourd‚Äôhui une partie de l‚Äôorganisation de la communaut√© PHP, et sur ce que sont les PSRs.\n  Nommer les choses ? Oui : avec le DNS par Julien Pauli. Cette conf√©rence revient sur les bases du fonctionnement du DNS et son utilit√©.\n  Et, pour finir : Caching with PSRs par Hannes Van De Vreken. Derni√®re des conf√©rences ‚Äú√† voir une fois‚Äù, celle-ci revient sur ce qu‚Äôest le cache en g√©n√©ral, pourquoi on en utilise. Puis s‚Äôint√©resse au cache applicatif via les PSRs.\n\n\nLe dernier PHP Tour\n\nCe PHP Tour √©tait le dernier, puisque l‚ÄôAFUP proposera √† partir de 2019 un nouveau format pour les √©v√©nements en r√©gion : l‚ÄôAFUP Day. Nous aurons grand plaisir √† vous y rencontrer √† nouveau, √† Lyon cette fois-ci !\n\nEncore un grand merci √† l‚ÄôAFUP !\n\nEnfin, retrouvez toute l‚Äôactualit√© de l‚Äô√©v√©nement sur #phptour.\n"
} ,
  
  {
    "title"    : "How a fullscreen video mode ended up implementing React Native Portals?",
    "category" : "6play",
    "tags"     : " React, ReactNative, mobile",
    "url"      : "/6play/2018/04/15/how-a-fullscreen-video-mode-ended-up-implementing-react-native-portals.html",
    "date"     : "April 15, 2018",
    "excerpt"  : "This story introduces a declarative native side portal implementation module called rn-reparentable.\n\nMy teammate (Laetitia BONANNI) and I are working on a React Native module embedded in the 6Play application that aims to provide best moments of ...",
  "content"  : "This story introduces a declarative native side portal implementation module called rn-reparentable.\n\nMy teammate (Laetitia BONANNI) and I are working on a React Native module embedded in the 6Play application that aims to provide best moments of different TV shows from the M6 channel.\n\nThe module, called Refresh, is a list of videos that are playing while the user is scrolling. It also provides a ‚Äútheater mode‚Äù which is a way to create an immersive user experience by obscuring the cards that are not focused:\n\n\n\nAs any other video application, it provides a fullscreen experience to the user by rotating the device.\n\nAt the time I‚Äôm writing this article, creating such a thing using React Native is a pain. Here‚Äôs the story why.\n\nCreating a fullscreen, the web developer way\n\nWhile being a web developer, we usually work with positioning to display something over the rest (like a popup for example).\n\nDealing with React Native and its style APIs (which really looks like the web one), we thought that it would be super-easy to simulate the exact same behaviour.\n\nThat‚Äôs why our main idea was to manage the fullscreen mode by adding a style that takes the screen size and an absolute position. Thus, the video would have followed the device edges while rotating:\n\n\n\nReact Native styles are not the same as the web ones\n\nThe idea of creating an almost equivalent API as the web one is really good for the learning curve of React Native. It is a real asset when you want to create simple user interfaces.\n\nBut there is a drawback. This approach makes us want to get the exact same result as we would have on the web.\n\nIn our case, the use of absolute positioning was sadly not working. In fact, it is written in the React Native documentation:\n\nPosition\n\nposition in React Native is similar to regular CSS, but everything is set to relative by default, so absolute positioning is always just relative to the parent.\n\nIf you want to position a child using specific numbers of logical pixels relative to its parent, set the child to have absolute position.\n\nIf you want to position a child relative to something that is not its parent, just don‚Äôt use styles for that. Use the component tree.\n\nSee https://github.com/facebook/yoga for more details on how position differs between React Native and CSS.\n\n\nThe fact that an element is always positioned relatively to its parent has been a problem for us since our player component is part of the list.\n\nOverflow and android are not friends\n\nAnd even if we would have found a way (we could have cheated by calculating negative values, relative to the parent, in order to stick to the edges of the device), we would have met other problems such as the fact the equivalent of overflow prop doesn‚Äôt work on Android.\n\nThere are actually multiple opened issues concerning this problem. Grabbou gave a shot on this one #7229:\n\n\n\nLet‚Äôs make a fullscreen, the native way\n\nHopefully, we are working with native developers, from both platforms. We have shared a lot of information and finally have found a solution.\n\nThis time, while rotating the device, we would have hidden everything around the VideoPlayer component. No more headers, no more footers, nothing except the player. Then, we would have set the player size so that it matches the device size:\n\n\n\nHere‚Äôs the result we have got:\n\n\n\nWhat is happening on here?\n\nThere are multiple interesting things here, at ~200 cards down:\n\n\n  \n    Special visual effects are appearing (gray and blue background color)\n  \n  \n    The list scrolls too high and then refocuses\n  \n  \n    The video restarts\n  \n\n\nExplanations\n\nThe first thing to know is that we only keep 5 players alive (2 above, the focused one, and 2 below) and otherwise we display images. It‚Äôs important because of memory. Without this limitation, the application would have thrown some OutOfMemory errors (we met this kind of problems with Bitmap objects).\n\nThe second thing to notice is that we are always playing the video that is the most centered on the screen.\n\nThe last thing to know is that we actually have multiple rendering cycles to hide the different components around the VideoPlayer.\n\nFor now, with that information, let‚Äôs imagine the following scenario:\n\n\n  \n    Scroll ~200 cards down\n  \n  \n    The most centered video is now playing\n  \n  \n    Rotate the device\n  \n  \n    It resizes all the images / VideoPlayer to match the device size\n  \n  \n    It removes ~200 headers + ~200 footers\n  \n\n\nDuring the 5th step, the list is scrolling up, because it has earned some space with the headers and footers disappearing. This creates the strange behaviour of ‚Äúyo-yo‚Äù list scrolling. Moreover, when the list is scrolling, the application finds a new ‚Äúmost centered card‚Äù, and creates the associated player. If the previous player is not part of the 5 new conserved ones, it‚Äôs destroyed. Thus, the further we scroll in the list, the worse it becomes.\n\nThe combination of the 4th and 5th step creates the actual gray / blue screen in background.\n\nFor now, we have a quasi-functional solution. It‚Äôs not really user friendly but we have something close to work. The key point here is that improving the functional solution (avoid the ‚Äúyo-yo‚Äù effect) would also give a better user experience.\n\nSo, how can we avoid this ‚Äúyo-yo‚Äù behaviour ?\n\nPortal to the rescue\n\nRecently, we heard about React portals. It seems that it could have saved us from this specific situation. The idea is quite simple, we would have teleported the player from its current location to somewhere higher in the component tree, like the React Native documentation encourages us to, without triggering special state based rendering-cycles (aka: Headers + Footers removals):\n\n\n\nThe problem is that React Native doesn‚Äôt support them natively: portals are part of ReactDOM, not React itself. We can‚Äôt use it in our application.\n\nWe‚Äôve found and experienced some great open source alternatives on the JavaScript side such as react-gateway and we even managed to create our own one for this specific case.\n\nThe problem is that React would have created a new instance of the VideoPlayer each time we would have moved it, instead of keeping the old one. It means that we would have created 2 VideoPlayer, and lost both context.\n\nEach time we rotate the device, the video will restart from the beginning.\n\nWhat can we do with portal ? On the native side?\n\nThe portal idea is quite interesting: we need to find a way to create a portal-like behaviour with React Native, but on the native side, so that we won‚Äôt lose the VideoPlayer native context.\n\nSince we had the chance to be at the React Native Europe, we have learnt the way React Native is managing views thanks to Emile Sjolander.\n\nTo demonstrate this idea, let‚Äôs take an example :\n\n\n\nThis is a simple application which provides two  components and displays some content. On the right, we can see the native tree view. The cursor shows the two native views that need to permute. The idea is to make First taking place of Second and vice versa.\n\nIt‚Äôs possible, using React Native, to use the module responsible of view management: UIManager (available directly from react-native module):\n\n componentDidMount() {\n    setTimeout(() =&amp;gt; {\n      // Permute child at indice 0 and 1 of parent tag 6\n      UIManager.manageChildren(6, [0], [1], [], [], []);\n    }, 3000);\n  }\n\n\nThis will end up making something like:\n\n\n\nIt seems that creating a portal-like behaviour is possible using ReactNative.\n\nThe main reason we didn‚Äôt choose this solution is the fact that we didn‚Äôt find a way to get the UIView native identifier from the JavaScript side (I‚Äôm not talking about nativeID or testID props, but the unique identifier of the view set on the native side).\n\nHere‚Äôs a tweet from me concerning unique identifier\n\nNative implementation of ‚Äúportals‚Äù\n\nWe finally decided to implement a React Native native component called  that is able to move View children from a parent view to another one using a declarative API.\n\nUsing this approach, we gain more control over what we would like to do leveraging native side power.\n\nReparentable owns two props :\n\n\n  \n    name that represents the destination of the teleportation\n  \n  \n    target that represents the name of the target\n  \n\n\n&amp;lt;View style={styles.container}&amp;gt;\n  &amp;lt;Reparentable name=&quot;1&quot; target=&quot;&quot;&amp;gt;\n    &amp;lt;Text&amp;gt;First&amp;lt;/Text&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n\n  &amp;lt;Reparentable name=&quot;2&quot; target={this.state.shouldGo ? &quot;1&quot; : &quot;goNowhere&quot;}&amp;gt;\n    &amp;lt;Text&amp;gt;Second&amp;lt;/Text&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n&amp;lt;/View&amp;gt;\n\n\nOn this gist, &amp;lt;Reparentable name=‚Äù2‚Äù ‚Ä¶/&amp;gt; will take place of &amp;lt;Reparentable name=‚Äù1‚Äù ‚Ä¶/&amp;gt; when the state shouldGo will change.\n\nWhat does it mean?\n\nIn our context, it means that when the state isFullscreen is true, we are able to move the player from its current view to the higher one:\n\n&amp;lt;View style={styles.container}&amp;gt;\n  &amp;lt;Reparentable name=&quot;fullscreenView&quot; target=&quot;&quot;&amp;gt;\n    &amp;lt;FullScreenContainer /&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n\n  &amp;lt;Reparentable\n    name=&quot;videoPlayerId&quot;\n    target={this.state.isFullscreen ? &quot;fullscreenView&quot; : &quot;&quot;}\n  &amp;gt;\n    &amp;lt;VideoPlayer /&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n&amp;lt;/View&amp;gt;\n\n\nHere‚Äôs the result we‚Äôve got:\n\n\n\nComparing both variants\n\n\n\nIt took us time to get this result, but we finally have something that meets our needs.\n\nLink to the library : https://github.com/mfrachet/rn-reparentable\n\nThanks for reading,\n"
} ,
  
  {
    "title"    : "The 6play platform goes international",
    "category" : "6play",
    "tags"     : " 6play, intl",
    "url"      : "/6play/2018/03/26/6play-goes-international.html",
    "date"     : "March 26, 2018",
    "excerpt"  : "Within less than a year m6web and techm6web managed to launch websites, android apps and ios apps for three RTL TV channels in Europe, all based on the 6play technology.\n\nThese deployments include:\n\n\n  broadcasted and themathic channels,\n  live st...",
  "content"  : "Within less than a year m6web and techm6web managed to launch websites, android apps and ios apps for three RTL TV channels in Europe, all based on the 6play technology.\n\nThese deployments include:\n\n\n  broadcasted and themathic channels,\n  live streaming, video catchup encoding and playout,\n  video resuming,\n  local adservers (for videos and display), DMP system and CDN,\n  contribution via our backoffice and automated through our API,\n  translations system,\n  and almost all the features of the 6play platform.\n\n\nRTL Play (Belgium)\n\n\n\nhttps://www.rtlplay.be/\n\nRTL Play (Croatia)\n\n\n\nhttps://play.rtl.hr/\n\nRTL Most (Hungary)\n\n\n\nhttps://www.rtlmost.hu/\n"
} ,
  
  {
    "title"    : "Useful (or not) M6Web OSS stuff",
    "category" : "OSS",
    "tags"     : " OSS, Open source, php, js",
    "url"      : "/oss/2018/03/20/useful-or-not-usefull-m6web-stuff.html",
    "date"     : "March 20, 2018",
    "excerpt"  : "At M6Web we do love open source and we are trying to be good open-source dev citizens! Here is a short presentation on our most interesting contributions:\n\n\n\nYou can find all our open source contributions on their dedicated page on our tech blog. ...",
  "content"  : "At M6Web we do love open source and we are trying to be good open-source dev citizens! Here is a short presentation on our most interesting contributions:\n\n\n\nYou can find all our open source contributions on their dedicated page on our tech blog. Enjoy !\n"
} ,
  
  {
    "title"    : "Atteindre les √©toiles avec PHP et Symfony",
    "category" : "",
    "tags"     : " conference, confoo, PHP, Symfony",
    "url"      : "/2018/03/07/atteindre-les-etoiles-avec-php-et-symfony.html",
    "date"     : "March 7, 2018",
    "excerpt"  : "√Ä l‚Äôautomne 2014, M6 d√©cide d‚Äôadapter le programme Rising Star en France, un concours de chant en direct, mais dont le jury est le public, qui vote en direct depuis son application mobile.\nA travers cette conf√©rence, je me propose de vous pr√©sente...",
  "content"  : "√Ä l‚Äôautomne 2014, M6 d√©cide d‚Äôadapter le programme Rising Star en France, un concours de chant en direct, mais dont le jury est le public, qui vote en direct depuis son application mobile.\nA travers cette conf√©rence, je me propose de vous pr√©senter l‚Äôarchitecture mise en place pour √™tre capable de traiter plusieurs dizaines de millions de votes dans un d√©lais de quelques secondes, tout en se synchronisant avec une √©mission de t√©l√© en direct.\n"
} ,
  
  {
    "title"    : "Migration to Spark 2.2",
    "category" : "",
    "tags"     : " Data, Hadoop, BigData, Airflow, Hive, Spark, Java",
    "url"      : "/2017/12/13/spark-2.html",
    "date"     : "December 13, 2017",
    "excerpt"  : "To value our data in order to understand better our service and improve it, we use Spark. You can find more information in a recent article about our datalake. We recently migrated our biggest project from Spark 1.5 to Spark 2.2 and wanted to shar...",
  "content"  : "To value our data in order to understand better our service and improve it, we use Spark. You can find more information in a recent article about our datalake. We recently migrated our biggest project from Spark 1.5 to Spark 2.2 and wanted to share that story.\n\nSpark 2 has been released a year ago (July 26, 2016). Maybe we are a bit late, but better late than never.\n\nWe are working with an official version from Cloudera with Spark 1.6 as the default version.\n\nOur project runs everyday to get data from different sources and send them to different destinations.\n\nIt is built with Java and Spark 1.5, but we encountered several problems with those technologies. First of all, the Java + Spark community is smaller than the ones for Python or Scala. Secondly, the Spark 1.5 community is also smaller than the one of version 2.2.\n\nThat sometimes made information hard to find.\n\nBut most of all, we did not succeed to integrate new components that work with more recent versions.\n\nWe wanted to migrate for bugs fixes in general and in a performance purpose too.\n\nI) Workflow\n\n\n\na) Spark 1.5 to Spark 1.6\n\nFirst, we had decided to migrate to 1.6 to do a progressive migration. But we bumped into a bug with a UDF. We had difficulties fixing it, and it was resolved in 2.2.\n\nWe finally decided to migrate directly to 2.2.\n\nb) First validation with unit tests\n\nWe did the migration and ran our unit tests to see and fix the problems.\n\nc) Functional tests\n\nThen, we ran our jobs with some data sets. The idea was to check the differences with Spark 1.5.\n\nWe wanted to be sure that our unit parts were working together.\n\nd) Double run\n\nThen, we set out for a double run. It means that we had our jobs running both with Spark 1.5 and Spark 2.2 and we compared the outputs each day.\n\nWe used Airflow to deal with that. If you know Airflow, you will understand that we added a new DAG to run our project with Spark 2.2.\n\nThe idea was to see the potential differences between the two on a daily basis.\n\nAt the end, we merged our branch into master.\n\n2) Changes to migrate to 2.2\n\nThere are different changes from Spark 1.5 to 1.6 to 2.2. You will find them described in the documentation.\n\nThe idea here is to focus on the problems we met, the noticeable changes for us and how we dealt with them.\n\na) Dataset\n\nOf course, the main change is that ‚ÄúdataFrame‚Äù does not exist anymore. You must replace it by ‚ÄúDataset&amp;lt;Row&amp;gt;‚Äù.\n\nActually, ‚ÄúDataFrame‚Äù and ‚ÄúDataset‚Äù were unified with Spark 2.0. In reality, for untyped API like Python, ‚ÄúDataFrame‚Äù still exists. But, we work with Java.\n\n\n\nUsing ‚ÄúDataset&amp;lt;T&amp;gt;‚Äù is a way to apply a schema at the compilation. If there is a problem, you will get a logical exception. Before, with ‚ÄúDataFrame‚Äù, you could only have runtime exceptions.\n\nAs a first step, we replaced ‚ÄúDataFrame‚Äù by ‚ÄúDataset&amp;lt;Row&amp;gt;‚Äù\n\nb) SparkSession\n\nA second major difference is ‚ÄúSparkSession‚Äù. It is the new entry to Spark. \nThere is no need anymore to create a ‚ÄúSparkConf‚Äù, a ‚ÄúSparkContext‚Äù and a ‚ÄúSQLContext‚Äù. It is possible to get all of it just with a ‚ÄúSparkSession‚Äù.\n\nBut, it is important to understand that if you just want to migrate your code in a first step to get it work with Spark 2, it is not a need to use ‚ÄúSparkSession‚Äù. ‚ÄúSparkConf‚Äù, ‚ÄúSparkContext‚Äù and ‚ÄúSQLContext‚Äù still work.\n\nThat is what we decided to do.\n\nc) Iterable to Iterator\n\nThe return type ‚ÄúIterable‚Äù is incompatible with ‚ÄúPairFlatMapFunction‚Äù. We had to replace ‚ÄúIterable&amp;lt;&amp;gt;‚Äù with ‚ÄúIterator&amp;lt;&amp;gt;‚Äù.\n\nWe replaced code like that:\n\npublic Iterable&amp;lt;String&amp;gt; call(String s) throws Exception {\n    ...\n    return list;\n}\n\n\nby something like that:\n\npublic Iterator&amp;lt;String&amp;gt; call(String s) throws Exception {\n    ...\n    return list.iterator();\n}\n\n\nd) Creating a UDF using hiveContext is not possible anymore the same way\nBefore, you could do something like that :\nhiveContext.sql(&quot;CREATE TEMPORARY FUNCTION function AS ...&quot;)\n\n\nBut now, you have to enable hive support first. You must do it with the SparkSession:\n\nSparkSession spark = SparkSession\n    .builder()\n    .appName(&quot;Java Spark Hive Example&quot;)\n    .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)\n    .enableHiveSupport()\n    .getOrCreate();\n\n\nIf ‚ÄúenableHiveSupport‚Äù is not enabled, there is an error like this :\n\njava.lang.UnsupportedOperationException: Use sqlContext.udf.register(...) instead.\n\n\nWe decided not to use ‚ÄúSparkSession‚Äù in a first step and to follow the error instructions.\n\nWe replaced our direct call to Hive by a UDF registration.\n\ne) Deprecations\n\nWe noticed some deprecations like HiveContext or Accumulators for instance. But we decided not to deal with them for the moment.\n\nd) Performance\nWe have made some gains in performance. \nBefore, running our jobs lasted around three hours. Now, it lasts around two and a half hours.\n\nWe hope we will make some other gains by migrating to the Spark 2.2 philosophy (‚ÄúSparkSession‚Äù, etc).\n\nConclusion\nAs there are many backward compatibilities with Spark 2, it is not so difficult to make a first migration to make your project work. Nonetheless, it could be long to validate. It depends on your tests stategy too.\n\nOur next step now will be to integrate the new philosophy of Spark 2.2 to get the best of the new version.\n\nNastasia Saby (Zenika consultant)\n"
} ,
  
  {
    "title"    : "Forum PHP AFUP 2017",
    "category" : "",
    "tags"     : " afup, php",
    "url"      : "/2017/11/07/forum-php-2017.html",
    "date"     : "November 7, 2017",
    "excerpt"  : "M6Web √©tait sponsor de cette √©dition du Forum PHP organis√©e par l‚ÄôAFUP et une grande partie de l‚Äô√©quipe backend avait fait le d√©placement. \nCe forum √©tait vraiment in√©dit de par sa taille sans pr√©c√©dent : plus de 650 participants ! Il a √©t√© aussi ...",
  "content"  : "M6Web √©tait sponsor de cette √©dition du Forum PHP organis√©e par l‚ÄôAFUP et une grande partie de l‚Äô√©quipe backend avait fait le d√©placement. \nCe forum √©tait vraiment in√©dit de par sa taille sans pr√©c√©dent : plus de 650 participants ! Il a √©t√© aussi pour l‚Äô√©quipe l‚Äôoccasion de voir des pr√©sentations de grande qualit√© et tr√®s inspirantes. (sans compter celle de nos coll√®gues Fabien et Nastasia sur l‚ÄôAB testing).\n\n\n\nDe tr√®s nombreux retours exhaustifs sont disponibles sur le web et je pense que les vid√©os seront rapidement en ligne sur la page listant tous les talks organis√©s par l‚ÄôAFUP. On peut noter, comme √† chaque Forum, les tendances qui se d√©gagent de l‚Äôensemble des talks et suite aux discussions endiabl√©es qui suivent les pr√©sentations :\n\n  DDD commence √† √™tre pr√©sent dans tous les talks type m√©thodo,\n  GraphQL fait parler de lui, et c‚Äôest tant mieux,\n  des reality checks sur les modes de ces derni√®res ann√©es (comme les micro services, la qualit√© au sens large), mais une maturation et un recul sur des pratiques modernes qui font plaisir √† voir,\n  des consid√©rations tr√®s int√©ressantes sur la gestion du code source (refactoring, clean code, nommer les choses ;) ‚Ä¶).\n\n\nEnfin, j‚Äôai vraiment (en tant que lyonnais) appr√©ci√© la r√©gionalisation de l‚ÄôAFUP, avec une multitude d‚Äôantennes locales cr√©ees ces derni√®res ann√©es.\n\nBravo l‚Äô @afup pour la r√©gionalisation des badges au #ForumPHP ! pic.twitter.com/ekewUkCoKS&amp;mdash; Olivier Mansour (@omansour) 26 octobre 2017\n\n\nUn excellent cru que M6Web √©tait ravi de soutenir ! Et rendez-vous au PHP Tour !\n"
} ,
  
  {
    "title"    : "Genesis of M6&#39;s Datalake",
    "category" : "Data",
    "tags"     : " Data, Hadoop, BigData, Airflow, Hive, Spark, DMP",
    "url"      : "/data/2017/10/23/genesis-of-m6-datalake.html",
    "date"     : "October 23, 2017",
    "excerpt"  : "At M6 we work hard to leverage Data to help our ad sales team, our CRM teams and our product innovation process.\nOver the past 2 years, we have gone from using a market DMP to creating our second Hadoop platform. We now feel that our stack is stab...",
  "content"  : "At M6 we work hard to leverage Data to help our ad sales team, our CRM teams and our product innovation process.\nOver the past 2 years, we have gone from using a market DMP to creating our second Hadoop platform. We now feel that our stack is stable, reliable and scalable so it feels like the right time to share our experience with the community.\n\n\n\nStep 1: embracing the DMP\n\nOur first use case was to monetize data through targeted publicity.\nWe decided to start by installing a DMP (Data Management Platform) because it was a very fast solution to deliver our major needs, in particular :\n\n\n  Collect data from all our services and combine it with our user\nknowledge =&amp;gt; DMPs offer that off the shelf\n  Create segments for audience targeting =&amp;gt; The segmentation approach offered by DMPs was well adapted to the ads market\n  Activate our Data, both in house via our adservers and in the outer market =&amp;gt; DMPs generally offer simple integration with most adservers, and a very straight forward third party integration\n\n\nThe match seemed quite obvious and there‚Äôs a good reason for that: DMPs are designed for this use case above all others.\n\nWe chose Krux (now Salesforce) and deployed it over our ~30 sites and applications. Installing Krux on our network and plugging it to our video and display adservers ended up taking a few months and a decent effort. Convincing all our teams that the increase in ad revenue would make it worth the development time and the negative impact on webperf wasn‚Äôt trivial, but got through thanks to our top management sponsoring. Once on the job, the deployment was quite smooth on the web and mobile apps, but validating the quality of the ingested data turned out to be an endless project.\n\nAt the end of the day, Krux‚Äôs DMP did the job. In November 2015 we launched Smart6tem, our Data platform &amp;amp; advertisement offer based on segments (announcement here, articles here here or here). This move had a very positive effect on our advertisement market, and allowed to start making Data mean something at M6.\nTo give some detail of our use of the DMP, it turned out building our own segments was very successful, but we didn‚Äôt use any 3rd party interconnection because we didn‚Äôt find any valuable Data to buy and didn‚Äôt want to reduce the value of our own Data by sharing it out.\n\n\n\nKrux‚Äôs segment builder\n\nOnce the Advertisement use case was out in the market, we moved our efforts towards leveraging the DMP for our CRM teams. The rationale was simple: targeted emails are more efficient than newsletters. We were hoping to reduce the email pressure on our users while increasing the performance both for revenue and traffic.\n\nStep 2: first round testing Hadoop\n\nHaving a DMP is both a great accomplishment and frustrating.\nIt‚Äôs great because you can start to combine the use of your service with the user profiles to produce segments and activate use cases to address them.\nBut for CRM, the workflows to plug segments into our emailing systems weren‚Äôt native and we needed to build some custom workflows. No rocket science, but when we first received a 2 Billion line file for the user/segment map that we needed to filter and convert into another format, our developers went grumpy.\nWe also got frustrated very fast because we wanted to start to extract some unpreceded analytics insights combining our user knowledge (our major service, 6play had just switched to fully logged-in users) with usage stats or with external sources like our adserver logs. Advanced analytics was clearly not the field of Krux.\nLast but not least came some limitations (either due to the design of Krux or the pricing):\n\n\n  We could only work on 3 months of history if we wanted to keep the price reasonable (on 6play we have a lot of TV shows that run for 3 months per year, and segmenting the users who watched the show last year is important).\n  It‚Äôs based on cookies + device ids on mobile (it‚Äôs the best solution for most use cases, but if your users are logged in, it introduces quite a lot of risk to make mistakes).\n  We never managed to convince our users that the amount of cookies or users inside segments was correct. Every single study we made on this point led to doubt, and our DMP support team never came up with serious answers.\n\n\nAt this point, Hadoop came in as an evidence, so we created our first cluster.\nThe process of creating this proof of concept cluster was pretty much a black box for us since we charged a partner with the job. We ended up with the following setup, all hosted by AWS :\n\n\n  2 name nodes with 16 VCPUs and 30G RAM each\n  4 data nodes adding up to 64 vcpu‚Äôs and 120G RAM\n  Cloudera Enterprise with Hive, Impala, Hue, Python, R and a kinky crontab\n  Tableau Desktop + Tableau Server\n\n\nNothing crazy but that brought us into the world of Hadoop, and that was a major move. We also staffed our first Data Scientist to start to explore our Data and imagine use cases.\n\nOur first steps in Hadoop were hesitant, but within a few months, we had created our first Data Lake, our targeted CRM was live and we had produced a few dozen dashboards providing unpreceded insights throughout the company. From the business perspective, it was a success.\nFor the people who got their hands on a Data Lake for the first time the experience was ground breaking. For the first time, we could connect information from half a dozen different tools seamlessly.\nAn example: finding how many ads were seen by women from 25 to 49 years old during the NCIS TV show.\nBefore the Data Lake, this would have been impossible. The closest we could get would take the following process :\n\n\n  Extract the amount of ads viewed on NCIS from our adserver stats to a text file\n  Extract the NCIS traffic from our video consumption tracking tool (in Cassandra) to a text file\n  Extract our users Database with age and gender (in a third party tool named Gigya) to a text file\n  Load all this up into an Excel spreadsheet\n  Write a bunch of Excel formulas to produce the percentage of the traffic on NCIS that‚Äôs generated by women between 25 and 49 years old\n  Apply that percentage to the adserver stats\n\n\nAs you can see, combining information between our ecosystems involved some very manual processes and could only lead to approximations, so basically we never did them.\n\nWith our Data in Hadoop, all this turns out to be a simple SQL query in Hue (a PhpMyAdmin style interface for Hadoop):\n\nSELECT COUNT(*) FROM adserver_logs A\n   JOIN users U ON A.uid = U.uid\n   JOIN programs P on A.pid = P.id\nWHERE A.type = &#39;impression&#39;\n   AND U.age &amp;gt;= &#39;25&#39;\n   AND U.age &amp;lt;= &#39;49&#39;\n   AND U.gender = &#39;F&#39;\n   AND P.name = &#39;NCIS&#39;\n\n\nHadoop and our Data Lake, we could just jump over the barriers between tools and ecosystems within seconds. Combined with the ability to code in various languages, we could instantly start to industrialize such insights and start going further.\n\nWe convinced our top management very fast about the value of having our own Hadoop cluster, and since it was very (VERY) expensive, we decided to internalize it.\n\nStep 3: building our internal Hadoop cluster\n\nSo there we were with a quite simple roadmap: replace our v1 Hadoop cluster to reduce costs and improve performance as much as possible. We managed to divide the price by 3 while multiplying the resources by 8.\n\nThe first step on this road was to staff a tech team to design and create our platform. That ended up being very tricky and finally took us 10 months to complete.\n\nOnce the team was staffed, we got onto the job. We had 5 steps :\n\n\n  Choose the hosting platform (4 months)\n  Choose the hardware\n  Choose the software stack (2 months, done in parallel)\n  Set up the cluster (2 months)\n  Migrate all our projects to the new platform (3 months)\n  Check to be sure everything was done (1 month)\n\n\na) Hosting platform\n\nThis stage of the project was a very religious one. Many people at M6 had a very strong desire to go towards cloud and managed services, others were totally in favor of Hadoop and have full in house control over the platform. The major options were:\n\n\n  AWS\n    \n      Amazon EMR + S3\n      Amazon EC2 + S3\n    \n  \n  Google Cloud Platform\n    \n      Managed services (Dataflow, BigQuery, Compute engine, Pub/Sub‚Ä¶) + Cloud storage\n      Dataproc + Cloud storage\n    \n  \n  On premise\n    \n      Add servers to our 6play platform at Equinix\n      Work with our hosting subsidiary, Odiso\n    \n  \n\n\nWe spent 3 months talking to the different vendors and considering options.\n\nThe first decision we took was to use Hadoop instead of managed services.\nThe AWS and Google sales teams were very convincing, but we finally declined for 2 main reasons:\n\n\n  People in our company were starting to learn how to use Hadoop, changing the stack would have forced everyone to re-learn what they were just starting to dominate. Not very efficient while building up expertise.\n  Using proprietary solutions like Big Query involves a strong locking risk. If we developped all our projets to leverage a specific platform, changing providers in a few years would involve a lot of reworking on all our code base.\n\n\nThe next step was to choose between the 3 hosting options. On a side note, we compared the price for x4 and x10 resources compared to our v1 platform.\nAt the end of the process we wrote up an evaluation grid. Here is the summary version.\n\n\n\nThe decision was there, we went for a fully on premise stack with Odiso.\nTo detail some of that evaluation, here‚Äôs a few insights on what it came down to.\n\n\n  AWS is cool, but ultra expensive. I mean it‚Äôs 10 times more than our on premise option! We would have gone full AWS if the price was reasonable. The possibility to pop clusters up and down is very interesting and reduces costs, but our v1 platform was using our 4 EC2 Data Nodes at ~80% 24/7, so we could never go down to 0 servers.\n  Google feels better on the service side of things, but it involved taking chances because the commercial product is young and support + community experience seemed weak.\n  On premise was clearly much cheaper, and felt more secure for our low experience on Hadoop since we‚Äôre used to managing servers and our team had managed serious Hadoop before.\n\n\nb) Hardware\n\nGoing on premise means buying physical servers and building them.\nOur goal here was to massively upgrade our current platform to scale with the company‚Äôs usage of Big Data. Since the price was very reasonable, we settled down to x8 on CPU, RAM and storage compared to our initial Hadoop cluster. Here‚Äôs the stack we bought:\n\n4 KVM servers:\n\n\n  DELL PowerEdge R630\n  OS Disks: 2x 400GB ssd, RAID 1\n  Data disks: 8 * 2To\n  RAM: 256Go (8*32G)\n  CPU: 2x12 Cores (3.0Ghz)\n\n\n15 Data Nodes:\n\n\n  DELL PowerEdge R630\n  OS: 2x 400GB ssd, RAID 1\n  Data disks: 8 * 2To\n  RAM: 384Go (24*16G)\n  CPU: 2x12 Cores (3.0Ghz)\n\n\nBuilding and racking the servers was quite straight forward, there‚Äôs nothing special about Hadoop in this process except the high quality network connectivity.\n\nc) Software stack\n\nDesigning the software stack was very straight forward.\nWe had the desire to stay as close as possible to the stack our users were getting used to, and it was pretty much a standard Cloudera stack. That suited us very well because our first priority was to avoid any regression, both for the projects (during this period, they had massively multiplied as we‚Äôll detail in the migration part below) and for the users.\nAnother early choice was to use virtual machines with Proxmox and not dive into the Kubernetes + Docker adventure. Although that was tempting and will probably be an option in future, we considered mastering the Hadoop stack was enough on our plate for the moment, we needed to reduce risk.\n\nHere‚Äôs the stack we chose:\n\n\n  Puppet\n  Centos 7\n  Proxmox\n  Cloudera Hadoop 5.11 (free version)\n  Hadoop 2.6\n  Hive 1.1\n  Spark 1.6 and 2.1 (we had 1.6 before but our Data Scientists really wanted to use new features)\n  Supervisord\n  MariaDB\n  LDAP\n  Ansible\n  OpenVPN\n  Python 2.7 and 3.6 with Anaconda\n  Java 8\n  R 3.3\n  Scala\n  Airflow 1.8 (this is out of the Cloudera stack, an important and epic part of our toolkit that we‚Äôll surely talk about in more detail in a future post)\n  Sqoop\n  Hue 3 with Hive on Spark as default\n  Tableau Desktop + Tableau Online\n  Jupyter\n\n\nd) Install Hadoop and all our tools\n\nOne of the fun parts of our design process was to choose a name for our new cluster. We called it Cerebro (in reference to X-Men and the global view of Professor Xavier), and created a logo :)\n\n\n\nSetting this stack up felt very simple from my perspective, but that‚Äôs surely because our awesome team overcame the issues silently.\nOn the timeline, the biggest part of the setup was receiving the physical servers. That took about 3 months because some parts (SSD disks) were out of stock for a long time.\nWe received a first part of the Data Nodes a couple of months before the rest of the servers, so we decided to start building the cluster with temporary Name Nodes and services, and migrate them after.\n\nWe deployed Cloudera Hadoop via KVM servers (managed with Puppet) and the Cloudera Manager. Very straightforward.\nWe used Ansible to install our stack, manage all our configuration files and user access.\n\ne) Migrate our projects and Data\n\nMigration was a project in the project.\nBetween the day we decided to build our internal platform and the day we delivered, 20 months had gone by. During all that time, Big Data had been going through high pace growth inside M6. We scaled from ~1 to ~25 users, from 0 to ~200 Dashboards and ~60 projects. All of this relying on our ‚ÄúProof Of Concept‚Äù platform created with a partner.\nTo be honest, it was an utter mess in any Software Engineer‚Äôs eye. Imagine: no version control, a unique user hosting all the projects and executing 6000 crontab lines each day. No job optimisation whatsoever. Moreover, most of our users had no developpement process knowledge, so they didn‚Äôt see any problem with all this and weren‚Äôt all in favour of any change. The context was challenging.\n\nThe first step of our migration project was to bring all this back into a ‚Äúmigratable‚Äù state. To do that, we went through the following steps :\n\n\n  Put all the code base in Git\n  Create a code deployment process\n  Split the production jobs down to a 1 user per project approach, both for code execution and data storage\n  Make all paths to data relative\n  Switch from crontab to Airflow\n  Add backups on S3\n\n\nWe reached this milestone after 4 months of a large rework of all our projects by all our teams. The collective investment in this process was a real team success.\n\nThe second step was to rebuild all the projects and databases on the new platform.\nThanks to our new backup system that copied all our Data to S3, rebuilding databases was easy. Basically it took creating a script to restore the backup in the new platform, and we could start checking integrity by querying the datasets. Rebuilding projects was a similar process, we just had to deploy each project and it was ready to test. Everything went fast and easy, proving that all the preparation moves we made were very valuable.\n\nThe third step was to double run all our projects so we could be sure everything worked on the new platform while not breaking production.\nThere‚Äôs a tricky part to this because a fair amount of our projects include an output towards external servers (either other teams within M6 or 3rd parties). For this we had to add an ‚Äúonly run on‚Äù logic. That lead us to create a unified configuration and a library for exports.\nWe also had to distinguish all our code execution monitoring so we could keep an eye on what each workflow was producing, both in production and on the new platform. For this we added the platform name to all our Graphite nodes and updated all our dashboards to filter by platform.\nWith those 2 moves, most projects managed to run ‚Äúout of the box‚Äù. Some needed some refactoring, mostly for parts that had been forgotten in the first step.\n\nThe fourth step was validating that our double run was working well.\nThe theory of this validation was quite elaborate. For each table or output job we would count the number of lines in each partition produced, run checksums, dive into the details of the monitoring, and run manual tests.\nIn practice, that part cracked up quite fast because our v1 platform was being totally outscaled and therefore all our users really didn‚Äôt want to look back. We checked that the backups were good with file sizes and line counts, and for the rest we relied on our monitoring to be sure that the jobs runned and produced the same output volumes. For the most critical production jobs we went into some detailed manual checking, but we took the jump very fast.\n\nThe fifth and last step was migrating all our Tableau Dashboards to Tableau online.\nWe needed all our ingestion and treatment jobs to be up and running before we could migrate our 200+ Dashboards. Once that was done, most dashboards took nothing more that being opened in Tableau Desktop and published to Tableau Online. The only exceptions were the bunch of users who had missed some tables out in step 1. Those had to run through the whole process at fast speed‚Ä¶ Not very pleasant for them.\n\nSo there we are, we now have our 2 feet in our second Hadoop platform. Now we‚Äôre looking forwards, both on how we make this platform evolve to empower our future use cases, and to raise our innovation pace for Big Data to count much more within M6.\nBy all means stay posted, we‚Äôll update you on some of the awesome projects we‚Äôve been working on!\n\nTake away\n\n\n  Deciding to create an internal Hadoop platform took time and a few previous steps for our organisation to start to understand what Big Data was about and the way to go around it.\n  Choosing our hosting solution was hard and very conviction driven.\n  On premise hosting is cheaper than cloud solutions, but obviously less flexible.\n  No surprise for tech people and it‚Äôs valid way beyond Big Data, migrating projects developed without any engineering good practices was hard and risky work.\n\n\n"
} ,
  
  {
    "title"    : "Elasticsearch: la grande migration",
    "category" : "",
    "tags"     : " Elasticsearch, Php",
    "url"      : "/2017/06/01/migration-elasticsearch.html",
    "date"     : "June 1, 2017",
    "excerpt"  : "Pour assurer la scalabilit√© des performances de l‚ÄôAPI 6play, les donn√©es suivent tout un workflow pour √™tre d√©normalis√©es et stock√©es dans Elasticsearch.\nMi-2016, nous avons identifi√© des dysfonctionnements majeurs sur nos serveurs, entrainant par...",
  "content"  : "Pour assurer la scalabilit√© des performances de l‚ÄôAPI 6play, les donn√©es suivent tout un workflow pour √™tre d√©normalis√©es et stock√©es dans Elasticsearch.\nMi-2016, nous avons identifi√© des dysfonctionnements majeurs sur nos serveurs, entrainant parfois des interruptions de service.\nSuite √† quelques mesures d‚Äôurgences pour stabiliser l‚Äôexistant, nous avons entrepris de mettre √† jour notre version d‚ÄôElasticsearch pour b√©n√©ficier des derni√®res am√©liorations.\nNous √©tions alors sur la version 1.7, et souhaitions passer en version 2.0.\nApr√®s plusieurs mois d‚Äôefforts pour effectuer cette migration sans interruption de service ni gel technique, nous voici en version‚Ä¶ 5.2!\nVoici le r√©cit de cette grande migration, et ce que l‚Äôon a appris tout au long de ce p√©riple.\n\n\n\nLa th√©orie\n\nIl n‚Äôy a pas de m√©thode magique pour changer de cluster sans coupure, la strat√©gie adopt√©e est assez classique:\n\n\n  dupliquer les √©critures sur le nouveau cluster\n  basculer les lectures sur le nouveau cluster\n  arr√™ter les √©critures sur l‚Äôancien cluster\n\n\nIl n‚Äôest pas n√©cessaire d‚Äôencha√Æner toutes les √©tapes dans la m√™me journ√©e, cela pr√©sente donc l‚Äôavantage de pouvoir √©taler les d√©ploiements dans le temps en fonction des disponibilit√©s,\nainsi que de surveiller attentivement le monitoring pendant quelques jours pour v√©rifier que l‚Äôinfrastructure supporte bien les changements apport√©s.\n\n√âcritures en Y\n\nNous utilisons des workers Php pour d√©tecter les changements dans notre BDD,\nsuite √† quoi un message est publi√© dans une file d‚Äôattente pour √™tre trait√© par un autre worker qui se chargera de synchroniser les entit√©s entre MySQL et Elasticsearch.\nIl √©tait primordial que le cluster de production ne soit pas impact√© par les √©ventuelles erreurs rencontr√©es sur le nouveau cluster.\nUne de nos premi√®res intentions √©tait de publier le message de mise √† jour dans une deuxi√®me file d‚Äôattente, consomm√©e par des workers d√©di√©s eux aussi au nouveau cluster.\n\n\nLe gros inconv√©nient est que cela impliquait de doubler toutes les lectures sur la BDD, toutes les requ√™tes devaient √™tre execut√©es une fois par cluster,\ncela risquait donc d‚Äôimpacter d‚Äôautres services.\nNous sommes donc partis sur une solution purement logicielle, puisque pour chaque entit√© mise √† jour ce sont les workers qui les envoient sur chaque cluster.\n\n\nIl est par contre n√©cessaire de g√©rer correctement les erreurs, que faire si une erreur intervient sur un cluster mais pas l‚Äôautre?\nSi le nouveau cluster devient instable et que l‚Äôon renvoie les messages syst√©matiquement dans la file d‚Äôattente, on risque d‚Äôaccentuer inutilement la charge en √©criture sur le cluster stable en production.\nNotre compromis est de d√©finir comme master le cluster de production (celui o√π les donn√©es sont lues), et seules ses erreurs provoquent la g√©n√©ration d‚Äôun nouveau message.\nLes erreurs sur le cluster slave sont monitor√©es, mais ne g√©n√®rent pas de nouveaux messages dans la file d‚Äôattente.\nEffectivement, puisque chaque soir nous resynchronisons toutes les donn√©es entre MySql et Elasticsearch, on peut se permettre d‚Äôavoir des donn√©es moins fraiches sur le cluster slave le temps d‚Äôune journ√©e.\n\nInitialement, nous pensions que ce syst√®me master/slave serait temporaire, mais tr√®s rapidement nous avons p√©rennis√© ces d√©veloppements, cela nous permettait de tester facilement diff√©rents clusters,\nou encore de v√©rifier que les donn√©es √©taient bien index√©es de la m√™me mani√®re, faire des rollbacks en urgence‚Ä¶\n\nUne derni√®re difficult√© √©tait de faire cohabiter deux version diff√©rentes du Sdk Elasticsearch Php dans le m√™me projet.\nIl y a effectivement une incompatibilit√© entre les versions 2.* et 5.*, et nous n‚Äôavons pas eu d‚Äôautre choix que de cloner la librairie concern√©e et de changer tous les namespaces pour √©viter les conflits de noms.\nMalgr√© tout, ce ne sont pas les √©critures dans Elasticsearch qui nous ont pos√© le plus de probl√®mes.\n\nMigration des requ√™tes\n\nIl y a eu de nombreux changements apport√©s entre la version 1.7 et 5.0 et souvent pour le mieux.\nLes diff√©rentes √©volutions de syntaxes ont g√©n√©ralement vite √©t√© faites, car nous avions pris soin d‚Äôencapsuler la construction des requ√™tes via quelques fonctions helper (une sorte de query builder).\nIl nous a donc suffit de changer ces quelques fonctions pour traduire les anciennes requ√™tes vers la nouvelle syntaxe.\n\nUne erreur classique que nous faisions en 1.7 √©tait de se contenter d‚Äôun mapping par d√©faut, qui avait le m√©rite de fonctionner sans efforts avec nos requ√™tes et nos donn√©es.\nLors du passage √† la version 5.0, Elasticsearch a commenc√© √† refuser certaines de nos requ√™tes car elles ne pouvaient pas √™tre performantes.\nIl fallait choisir, soit activer explicitement des options de mapping en faisant un compromis sur les performances g√©n√©rales,\nsoit affiner le mapping pour qu‚Äôil soit plus adapt√© √† la nature de nos requ√™tes.\nIl s‚Äôagit d‚Äôun bon exemple de Leaky Abstraction,\non a beau utiliser des outils pour s‚Äôabstraire de la fa√ßon dont sont stock√©es les donn√©es, nous sommes toujours oblig√©s de comprendre ce qu‚Äôil se passe √† l‚Äôint√©rieur pour en tirer les meilleures performances.\nHeureusement pour nous notre mapping √©tait assez trivial √† changer, car nous n‚Äôutilisons pas Elasticsearch pour faire de la recherche full-text\nmais seulement pour des recherches exactes sur des identifiants, des codes‚Ä¶ Il nous a g√©n√©ralement suffit d‚Äôutiliser le nouveau type keyword\npour que nos requ√™tes puissent √™tre accept√©es.\n\nPour s‚Äôassurer du bon fonctionnement des APIs suite √† ces nombreux changements, nous avons investi du temps pour √©crire des tests fonctionnels de bout en bout,\npour v√©rifier que les r√©sultats restaient inchang√©s malgr√© le changement de version de cluster.\nBien s√ªr, m√™me si en local nos tests √©taient au vert, des erreurs pouvaient appara√Ætre en production.\nLe sc√©nario √©tait alors simple, faire un rollback, ajouter les tests correspondants aux nouvelles erreurs detect√©es, les faire passer en local, puis recommencer !\n\nCe qui nous a peut-√™tre le plus √©prouv√© dans cette migration, c‚Äôest une regression introduite dans la version 5.2\nqui avait pour cons√©quence de changer certains de nos tableaux vides en valeur null.\nIl nous a fallut quasiment repasser sur chaque requ√™te pour retransformer ces valeurs en quelque chose de coh√©rent. \nQuand on avait de la chance, ce bug faisait √©chouer nos tests, mais il est malheureusement arriv√© que ce soient les parseurs json des applications 6play de production qui en fassent les frais, avec diff√©rents plantages √† la cl√©‚Ä¶\n\nConclusions\n\nCette migration fut longue et parfois douloureuse, heureusement les r√©sultats sont maintenant au rendez-vous!\n\n\nDe plus, cela nous a donn√© l‚Äôoccasion d‚Äôinvestir un peu de temps pour am√©liorer nos tests fonctionnels, et pour d√©velopper un syst√®me robuste pour la r√©plication des donn√©es sur plusieurs clusters Elasticsearch.\nOn peut n√©anmoins se poser des questions sur la strat√©gie tr√®s offensive de changement de version de la part de Elasticsearch,\nautant de releases avec autant de changements en si peu de temps, il faut √™tre capable de suivre!\nPendant que nous finalisions notre production sur la version 5.2, la 5.3 a eu le temps de sortir, et la 6.0 est apparue en beta.\nNous allons essayer de profiter un peu de ce nouveau cluster avant de poursuivre vers une nouvelle grande migration :)\n"
} ,
  
  {
    "title"    : "Last night isomorphic JS saved our life!",
    "category" : "",
    "tags"     : " SPA, SSR, isomorphic, javascript, node.js, high availability",
    "url"      : "/2017/05/17/spa-mode-isomorphism-js.html",
    "date"     : "May 17, 2017",
    "excerpt"  : "For more than a year and a half, we use Node.js and React together to make the best app possible for our users. These 2 technologies are complementary to write only once code executed on the server and the client side: that‚Äôs the isomorphic way! T...",
  "content"  : "For more than a year and a half, we use Node.js and React together to make the best app possible for our users. These 2 technologies are complementary to write only once code executed on the server and the client side: that‚Äôs the isomorphic way! This approach helped us to develop a reliable app with a fast first render and SEO friendly.\n\nSSR caching\n\nHere is the architecture we use for the 6play web app.\n\n\n\nYou can see that Node.js server responses are cached with Varnish. Indeed, React is not efficient with server side rendering because it just has not been designed for that. The React renderToString method blocks the event loop. Consequently the server can not process an acceptable responses rate for a service like 6play that can reach a lot of requests per second without cache. Particularly when the European Football Championship final bring together France and Portugal and is live or when the last episode of ¬´ Les Marseillais ¬ª, one of the teenagers favorite programs, has just been released on the platform. So caching server responses, with a quite low caching time, is required for our application health!\n\nSPA mode\n\nIsomorphism enables search engines to parse our website without executing any line of JavaScript, only using the server side rendering. We thought that the opposite could be useful too. Imagine our Node servers are down, for various reasons. Our Varnish servers continue to deliver the application pages but only during the cache time. After, the user would get an error‚Ä¶ or not!\n\nIn this case, we would switch to a Nginx server that simply delivers a blank page with the client JavaScript code. The server was responsible for the app state initialization before, the user browser has to do so now. Then it can render the page: our application becomes a simple SPA. And this is almost imperceptible for the user, the first render is just a little longer. This way secures the availability of our service.\n\n\n\nThe Varnish servers check the status of the Node ones via a specific route. When every instance is down, they route all requests to a static HTML file on the Nginx server.\n\nReally useful ?\n\nYes! It is not used until it is! A few months ago we went through a memory leak. Consequences? After some time, we saw an increase in CPU usage, then the servers fell down and SPA mode was enabled. We didn‚Äôt notice the memory leak immediately because we often deploy new versions of our app and it resets the memory. When we detected the problem, it was too late to rollback because the incriminated version was probably weeks or months old.\n\nYou certainly know how difficult it is to find the code responsible for a memory leak in Node.js. It is often not a matter of hours but of days or weeks. With our SPA mode, we could debug our code with serenity. When the Node servers were down, the SPA mode took the reins. Then we simply restart the server to restore the nominal state when we were alerted (sometimes immediately, sometimes several hours after because it happened in the night). This situation went on some weeks. And we finally fixed the memory leak. No user has been affected. For us, this SPA mode is a significant safety for the high availability of our app.\n"
} ,
  
  {
    "title"    : "Symfony Live Paris 2017",
    "category" : "",
    "tags"     : " symfony, symfony live",
    "url"      : "/2017/04/03/sf-live-2017.html",
    "date"     : "April 3, 2017",
    "excerpt"  : "In March we attended Symfony Live Paris 2017, and it was very interesting.\nHere are some special feedbacks about some of our favorite talks.\n\n\nVarnish tags and invalidation\nSpeaker : J√©r√©my Deruss√©\nDescription\nJ√©r√©my presented us how to host appli...",
  "content"  : "In March we attended Symfony Live Paris 2017, and it was very interesting.\nHere are some special feedbacks about some of our favorite talks.\n\n\nVarnish tags and invalidation\nSpeaker : J√©r√©my Deruss√©\nDescription\nJ√©r√©my presented us how to host applications on an production environment on a Raspberry PI using varnish cache.\nToday, the internet traffic and the number of unique visitors increase every days.\nPeople imagines that we need strong hardware servers to be able to respond to this traffic.\n\nThe interesting part of this presentation was the way of how to better use the cache in front of our applications.\nTo be able to minimize the number of requests to the server by managing the cache duration\nand being able to invalidate it on demand when a resource expire.\n\nHow to tag and segment the cache\nBy tagging every resources by a unique id (entityName_id by example), you‚Äôll be able to invalidate later the cache linked to it.\nYou can simplify and automate this tagging by using event dispatcher and listen all entities creations / modifications. With\nthis solution you also centralize your code logic.\n\nCache tagging also helps you with caching / invalidating many-to-many relationships between content items.\n\nHow to invalidate the cache\nJ√©r√©my showed us a very interesting interface, displaying the entity content, automatically refreshed when the backend needs to be called because of a cache invalidation.\n\nOnce a resource is modified and needs to be uncached, your backend needs to call the Varnish to purge the tag linked to\nresources. Once again the FOS HttpCache bundle does it very well.\n\nAdvantages / drawbacks\nCaching everything is fine, but it has limitations :\nWorks well when :\n\n  You have mostly read access\n  You have more cache hit than miss\n  Your application is able to communicate with your cache servers\n\n\nCan be difficult when :\n\n  Operations are not atomic\n  It‚Äôs complexifying / slowing backend writes\n\n\nHosting on a Raspberry PI\nJ√©r√©my finally showed us an impressive demonstration of an application running on Raspberry PI.\nThe configuration :\n\n  Docker\n    \n      MySQL\n      NGINX\n      PHP7-FPM\n      Varnish\n    \n  \n\n\nThe results :\n\n  Number of calls with no caching handling : around 8 / secs\n  Number of calls with varnish cache response : around 900 / secs\n    \n      impressive !\n    \n  \n\n\nResources\nYou can manipulate cache by using the FOSHttpCacheBundle : https://github.com/FriendsOfSymfony/FOSHttpCacheBundle\n\nLink to the speaker presentation : https://www.slideshare.net/JrmyDeruss/grce-aux-tags-varnish-jai-switch-ma-prod-sur-raspberry-pi\n\nEverything a dev should know about Unicode\nSpeaker : Nicolas Grekas\nDescription\n\nNicolas Grekas made a talk about Unicode, from its origin to the latest advanced uses, and more precisely in the PHP ecosystem. He explained how utf8/16 works and the complexity of language management, especially the folding, graphs clusters, modifiers etc.\n\nPHP doesn‚Äôt natively handle the unicode #RIPphp6, so it is important to understand the specificities of utf8 in order to avoid some traps, especially concerning string length calculation, comparisons and insertions in database.\n\nIn order to manage the unicode, you must use the php functions of mbstring, iconv, graph and inttl. The ‚Äúu‚Äù modifier allows the utf8 to be processed correctly for regular expressions. For MySQL, utf8_unicode_ci handles ligatures whereas utf8_general_ci does not handle them - but is therefore faster.\n\nConcerning security and avoiding typosquatting, there is a list of confusable characters for filtering: https://unicode.org/cldr/utility/confusables.jsp?a=6play&amp;amp;r=None\n\nThis presentation was very interesting and very useful. Thank you Nicolas ;-)\n\nResources\nLink to the speaker presentation : https://speakerdeck.com/nicolasgrekas/tout-ce-quun-dev-devrait-savoir-a-propos-dunicode\n\nPerformances optimisation with Php7\nSpeaker: Julien Pauli\nDescription\n\nAs usual, it was a real pleasure to listen Julien Pauli speaking about what‚Äôs under the hood of Php7.\nEverybody knows there is an actual performances gap between version 5 and 7, but this talk gave some clues to understand the technical reasons behind these major improvements.\n\nFirst, Php compiler has been totally rewritten (yes, Php is compiled in opcode and cached in opcache).\nThanks to an AST, the compiler really understands the analyzed code,\nand lot of optimizations can be done at compilation time instead of runtime.\nFor example, all constant expressions (like $a = 1024 * 2048) are now computed once for all.\nOf course, this compilation pass is now longer but it is exactly the reason why opcache\nhas been made, and why you should warm up your opcache during your scripts‚Äô deployment.\nJulien also introduced the parameter opcache.interned-strings-buffer,\nused to configure string interning in Php.\nHe advised to increase its default value (or at least to check relevance),\nbecause even a minimal Symfony application contains a lot of huge DocBlocks (and then huge strings) so the default\nvalue could be underestimated in some cases. Ready to benchmark! :)\n\nWe had also some interesting information about packed arrays optimisations,\nand some string tips (&quot;$a - $b&quot; is better than $a . &#39; - &#39; . $b for example).\nThen he gave a very good overview of all the efforts done on internal Php structures in order to optimize memory access,\nCPU cycles‚Ä¶ an impressive work!\n\nTo finish, we have now an (very) approximated date for Php8 release!! Not before 2020‚Ä¶ be patient :)\n\nResources\n\nLink to the speaker presentation: https://www.slideshare.net/jpauli/symfony-live-2017php7performances\n\nDo not hesitate to visit Julien‚Äôs blog: https://jpauli.github.io/\n\nAnd many more\n\nWe also liked :\n\n\n  Blog posts about Symfony 4 here : https://fabien.potencier.org/\n  Micro-Services Symfony at Meetic: feedback after 2 years of redesign! https://fr.slideshare.net/meeticTech/php-symfony-microservices-migration-meetictech\n  Introduction to CQRS and Event Sourcing: https://www.slideshare.net/samuelroze/introduction-to-cqrs-and-event-sourcing-74061563\n  Web security: and if we continued to break everything? https://github.com/ninsuo/slides\n\n\nYan can find most of the slides here: https://joind.in/event/symfonylive-paris-2017/schedule/list\n"
} ,
  
  {
    "title"    : "Format all the things",
    "category" : "",
    "tags"     : " prettier, javascript, react, lint, eslint, 6play",
    "url"      : "/2017/03/30/prettier.html",
    "date"     : "March 30, 2017",
    "excerpt"  : "Prettier\n\nPrettier is a brand new javascript library. Its simple goal is to reformat your code. For instance, when I write this:\n\n\nand run prettier on it, I‚Äôll ultimately get this:\n\n\nYou can try it for yourself at prettier‚Äôs website\n\nLooks awesome...",
  "content"  : "Prettier\n\nPrettier is a brand new javascript library. Its simple goal is to reformat your code. For instance, when I write this:\n\n\nand run prettier on it, I‚Äôll ultimately get this:\n\n\nYou can try it for yourself at prettier‚Äôs website\n\nLooks awesome doesn‚Äôt it? You don‚Äôt need to be a stylish developer anymore! But let‚Äôs be honest: not all developers are enthusiastic about this new tool. There are two kinds of developers, those who like when a program helps them to format their code, and those who don‚Äôt.\n\nThere are some benefits: using prettier you won‚Äôt waste your time reformatting your code because your destructuring expression overreaches your config‚Äôs character cap. You will no longer have conflicts because your colleagues changed indentation on the code you‚Äôre currently working on.\n\nBut it also comes with some tradeoffs. The first one is your coding style. We generally add line breaks between chained calls on an API (example here). Prettier will make this fit on one line if it can, and your code will lose some clarity. The same thing goes with the parentheses you add in complex boolean operations (example here). You will have to remember operator priorities.\n\nFor a relatively big project like ours, we prefer adding some consistency to our code in order to avoid wasting our time on solving conflicts, at the expense of losing a little bit of readability and style.\n\nHow to configure it?\n\nWe already have eslint setup in our project with some rule tweaks. So for us, Prettier had to interface smoothly with eslint. Fortunately it comes with a bunch of useful plugins.\n\n\n  eslint-config-prettier: this library disables all eslint rules that conflict with the Prettier formatting. Without it we would need to turn off those rules manually.\n  eslint-plugin-prettier: this library includes Prettier proper formatting as an eslint rule. So if our code is not well formatted, eslint will throw errors. This is the most important plugin for us, as it makes the linting fail if the code is not well formatted. Formatting is now mandatory!\n  prettier-eslint (prettier-eslint-cli): this plugin just runs prettier followed by eslint --fix command. The CLI version allows us to use it from the command line.\n\n\nWith these libraries, we can format all our code base and apply Prettier and eslint rules.\n\nMigration\n\nFirst we had to format the whole repository. This results in a pretty big PR, 670 modified files, +11700 -11113 changes‚Ä¶ The implication is obvious: if you choose to use Prettier on your project, it had better be set up from the start.\n\nAnyway, once this huge PR was merged, we had to rebase other PRs. You can see it coming: if your rewrite most of your code and rebase other changes on it, there is nearly no way you can avoid conflicts.\nBut in reality it‚Äôs (almost) easier than it seems. Since all modifications were generated by Prettier, we can simply discard them and regenerate them after the rebase.\n\nSo, the first thing to do was to rebase on the parent of the format commit in order to resolve all conflicts that were not related to the formatting. To put it differently, we are sure that in next rebase conflicts will only be related to Prettier‚Äôs changes.\n\ngit rebase 0404b07~\n# where 0404b07 is the git hash of the format commit parent\n\nAfter that, we rebased our branch on the ‚Äúprettier‚Äù commit, and we asked git to automatically keep the conflicting changes from the branch and to discard those from Prettier.\n\ngit rebase 0404b07 -s recursive -X theirs\n\nThen we just needed to re-run Prettier to reformat the rebased code. After this, branches were well formatted and could get back to their normal life-cycle.\n\nHow does it work on daily basis?\n\nFirst, adding the following scripts in the package.json file enables us to use prettier as a yarn (or npm) command.\n\n\n\nThe first line is used to format files provided as parameters and is used in a git pre-commit hook. The second line was there to format the whole codebase and should not be used anymore. This command takes around 1 minute to execute which is a little too long to be used in the development process. It‚Äôs more interesting to plug Prettier in our IDE and only format modified files.\n\nEven though we now enforce a machine-generated code style, everyone is still free to use their favorite IDE with any formatting and syntax settings they like.\nThose using atom or sublime-text can use plugins for the save action (atom plugin here with ‚ÄúESLint Integration‚Äù checkbox and sublime-text plugin here). Every saved file will be automatically formatted by Prettier. This is clearly the most comfortable solution.\n\nThose used to applying the format action in Webstorm will have to configure an external tool to do it. Here is a good article to help you setup an external tool if you are interested in this solution.\n\nFinally we wrote a pre-commit hook and added it to our documentation. It automatically runs prettier on all added files from our javascript sources. lint-stage does the same, but we don‚Äôt want to force the whole team to use it. It‚Äôs clearly not necessary to run it twice, for those who have a save action which already runs Prettier.\n\nHere‚Äôs an example of our pre-commit hook:\n\ngit diff --name-only HEAD | grep -E &quot;src/.*\\.js.?$&quot; | xargs yarn format\n\nIn conclusion\nPrettier is a new tool to add to your chain. Its role is to format the code for you in a very strict way. Thanks to a bunch of plugins that complement it, it also plays nicely with and applies eslint rules. Like we said, there are few sacrifices to make in terms of clarity, but it allows you to stop taking care of things that add no real value to the code you write. It also helps you to reduce meaningless conflicts and debates on ‚Äúhow we should write this‚Äù. We plan to use it on all our javascript repositories, for greater consistency and good.\n"
} ,
  
  {
    "title"    : "Nouveau socle pour une nouvelle vie",
    "category" : "",
    "tags"     : " conference, confoo, PHP, Symfony",
    "url"      : "/2017/03/10/confoo-2017-nouveau-socle-nouvelle-vie.html",
    "date"     : "March 10, 2017",
    "excerpt"  : "A travers cette conf√©rence, je me propose de vous tracer l‚Äôhistoire de la migration de 6play (syst√®me de t√©l√©vision de rattrapage du groupe M6, premier groupe de t√©l√©vision priv√© fran√ßais) d‚Äôune application monolithique vers un univers de micro-se...",
  "content"  : "A travers cette conf√©rence, je me propose de vous tracer l‚Äôhistoire de la migration de 6play (syst√®me de t√©l√©vision de rattrapage du groupe M6, premier groupe de t√©l√©vision priv√© fran√ßais) d‚Äôune application monolithique vers un univers de micro-service, des avantages en terme de maintenance, d‚Äô√©volution, de mont√©e en charge, mais √©galement des diff√©rents √©cueils rencontr√©s lors de ce changement de paradigme : caching, logging, complexit√© globale.\n"
} ,
  
  {
    "title"    : "L&#39;√©quipe Player de 6play.fr au Paris Video Tech",
    "category" : "",
    "tags"     : " video, ott, react, dash, hls, mse, cmaf, 6play, html5",
    "url"      : "/2017/02/20/retour-de-paris-video-tech.html",
    "date"     : "February 20, 2017",
    "excerpt"  : "\n\nPr√©sentation du Paris Video Tech\nMercredi 1er f√©vrier avait lieu la troisi√®me √©dition du Paris Video Tech, un meetup orient√© autour de tous les sujets techniques de la vid√©o : players HTML5, formats, encodage, distribution, publicit√©, ‚Ä¶\n\nL‚Äô√©quip...",
  "content"  : "\n\nPr√©sentation du Paris Video Tech\nMercredi 1er f√©vrier avait lieu la troisi√®me √©dition du Paris Video Tech, un meetup orient√© autour de tous les sujets techniques de la vid√©o : players HTML5, formats, encodage, distribution, publicit√©, ‚Ä¶\n\nL‚Äô√©quipe Player de M6 Web (Fr√©d√©ric Vieudrin, Nicolas Afresne, Malik Baba A√Øssa et Vincent Valot) pr√©sentait le nouveau player HTML5 de 6play.fr : un player MSE multi-formats, d√©velopp√© enti√®rement en React, le framework JS qu‚Äôon ne pr√©sente plus et qui fait le succ√®s du nouveau 6play.fr depuis 2015.\n\nLa rencontre se d√©roulait dans les locaux de France T√©l√©vision √† Paris et proposait trois talks :\n\n\n  6play : un player MSE en React par l‚Äô√©quipe Player de M6Web\n  CMAF D√©mystifi√© par Cyril Concolato\n  Retour d‚ÄôExp√©rience de Roland Garros 360 par l‚Äô√©quipe innovation de France T√©l√©vision\n\n\n\n\n6play : un player MSE en React\n\nPr√©sentation de 6play.fr\nDans la premi√®re partie, nous avons pr√©sent√© le contexte technique de 6play.fr, autour de React, ainsi que les chiffres cl√©s du site.\n\nApr√®s un rappel de l‚Äôhistorique des players du site de Replay des cha√Ænes du Groupe M6, nous avons pr√©sent√© les enjeux de la refonte de notre pr√©c√©dent player et √©voqu√© nos contraintes.\n\n\n\nArchitecture du player en React / Redux\n\n\n\n\nEn octobre 2015 sortait le nouveau 6play.fr, une Single Page App d√©velopp√©e en React-Redux et Isomorphique. Le succ√®s de cette refonte nous a pouss√© √† √©tudier le refonte du player 6play sur la m√™me stack technique, historiquement en Video.js.\n\nEn compl√©ment de l‚Äôapproche composant propos√©e par React, Redux nous a apport√© la solution √† la gestion de l‚Äô√©tat du player dans le temps. En effet, son fonctionnement par √©v√©nements et actions √©tait parfaitement adapt√© aux √©v√©nements de la balise &amp;lt;video&amp;gt;.\n\nMedia Engines\nInspir√© du syst√®me multi-techs de Video.js, nous avons d√©velopp√© notre propre syst√®me de Bridge pilotant les diff√©rents SDK Video du march√© : hls.js, dash.js, Adobe Primetime Browser TVSDK, et HTML5.\n\nTous les Bridges communiquent ainsi de la m√™me mani√®re avec notre player React au travers des MediaEvents HTML5.\n\nInt√©gration continue, tests, outils et m√©thodes de travail\n\nLes tests automatis√©s font partie int√©grante des d√©veloppements chez M6Web. Tester unitairement nous permet de valider nos classes et m√©thodes, tester fonctionnellement assure le bon fonctionnement du player sur plusieurs navigateurs et √©vite les r√©gressions.\n\nNous utilisons les Webhooks de Github pour executer nos tests, d√©ployer un environnement de recette d√©di√© et nous notifier du statut de la Pull Request dans Slack.\n\nLes slides de notre pr√©sentation\n\nRevoir l‚Äô√©v√©nement en replay\n\n\n\n"
} ,
  
  {
    "title"    : "Get your brownfield React Native app built on demand",
    "category" : "",
    "tags"     : " mobile, github, ci, react-native",
    "url"      : "/2017/01/31/get-brownfield-react-native-app-built-on-demand.html",
    "date"     : "January 31, 2017",
    "excerpt"  : "As you may know, at M6Web we decided to embrace React Native a few months ago.\nIt‚Äôs a really exciting piece of software that adds a lot of value in the mobile development ecosystem.\n\nWe already use it for a side project on a standalone app (not pu...",
  "content"  : "As you may know, at M6Web we decided to embrace React Native a few months ago.\nIt‚Äôs a really exciting piece of software that adds a lot of value in the mobile development ecosystem.\n\nWe already use it for a side project on a standalone app (not public yet, stay tuned!) to record table soccer games, that‚Äôs why, we (mostly @ncuillery üòè) decided to improve the upgrade process for apps made with the embedded generator. See Nicolas‚Äô blog post on it: Easier Upgrades with React Native.\n\nAs a result, we wanted to start using React Native for our most popular app: 6Play (6play iOS, 6play Android).\nSo they would become what Leland Richardson from Airbnb calls ‚Äúbrownfield‚Äù apps.\n6play is the catchup TV platform for the French TV group M6. It offers live-streaming and full episodes for web, mobile and set-top box. Since the apps launched in 2016, there have been over 1.5 billion videos streamed. Our iOS (mostly Swift) and Android native applications, both important parts of the 6play platform, were exclusively developed externally until now.\n\nWe wanted to use React Native to develop this project in-house and to take advantage of the benefits this hybrid technology could bring into our native apps. Here are just some of the benefits we found when using React Native:\n\n\n  JavaScript development for mobile. We have a lot of awesome JavaScript developers internally who develop the 6play website using React. We love React &amp;amp; Redux and want to mutualize this piece of technology we use on most of the frontends of the 6play platform.\n  Hot fixing with CodePush. For our mobile apps, we want to accomplish the same continuous delivery process we have for the website. CodePush helps us to keep the same flexibility by allowing us to make deployments on a weekly or even daily basis.\n  Knowledge sharing. We would like to be closer to the external development of our mobile apps, which was difficult without native knowledge and without any Android or Swift internal developers. React Native allows us to be part of that, we started working closely with the native team, sharing all developments between the two teams and bringing the best of both worlds (native and web) into the same project.\n  Code Sharing. We also want to share major parts of the mobile code base between apps (Android &amp;amp; iOS). Today, the code bases for each app are completely separate and are managed by two separate teams. With React Native, we could have one common code base while being able to implement specificities for a particular platform if needed. We have also imagined some ways to share code with the 6play website.\n\n\nAs we mentioned in a previous blog post, we use Github pull requests extensively in our development process, especially for testing (automatically and manually) each new commits before merging them into the master branch.\n\nIn the past, we tried to use Appetize to preview  our apps in the browser. It was a first shot, but the functionality was quite limited: animations felt janky, some features wouldn‚Äôt work (in-app purchase, video with DRM, ‚Ä¶), user identification was painful. We needed a better solution, and as a result we decided to rethink the way we develop the 6play apps.\n\nFor the second iteration of our development process, we had a few simple requirements:\n\n\n  Test each pull request in conditions as close to the reality as possible,\n  Use the same testing workflow for both iOS and Android apps.\n\n\nThis post outlines our new mobile development process for the 6play apps. We‚Äôll walk through how we manage the environment of a brownfield React Native app, our Git repository structure, our build and release workflows, and how we‚Äôve created a CI environment that mirrors our production environment.\n\nMono Repository / Multi Repositories?\n\nThe first thing we had to do, was to decide how we wanted to organize our Git repositories.\n\nFor this, we looked into how the AirBnb team work with their brownfield app.\n\nWe soon realized we had two options here:\n\nMulti-repositories:\n\n\n  the iOS one\n  the Android one\n  and one for React Native code\n\n\nMono-repository:\n\n\n  One giant repository that has iOS, Android, and React Native folders inside.\n\n\nLet‚Äôs take a look at the pros and cons of both solutions.\n\nThe Mono Repository\n\n‚îú‚îÄ‚îÄ app-6play/\n‚îÇ   ‚îú‚îÄ‚îÄ app-android/\n‚îÇ   ‚îú‚îÄ‚îÄ app-ios/\n‚îÇ   ‚îú‚îÄ‚îÄ react-native-views/\n\n\nAt first glance, this solution seems like the Holy Grail:\n\n\n  (+) Everything is in the same place\n  (+) If a modification needs both native &amp;amp; React Native developments, changes can be contained in a unique pull request.\n  (-) Code, Documentation, Setup, are more difficult at the beginning (For example, how can we keep Git history of each existing repository?).\n  (-) For our workflow, we need to have everyone working the same way, with the same git workflow, and the same review process. Remember that our native team is an external team (in Belgium), the Android &amp;amp; iOS teams are two different teams (located in the same place) and the React Native one is an internal team (in France). If we succeed, we‚Äôll have synchronous development between teams, this is a really positive point, but it may be difficult to reach.\n  (-) Android, iOS and Javascript CI environments are very different (different tools, different needs), so it is really complex to setup.\n\n\nUltimately, the initial cost of setup and maintenance outweighed the benefits of a mono-repository.\n\nThe Multi Repositories\n\n‚îú‚îÄ‚îÄ app-android/\n‚îú‚îÄ‚îÄ app-ios/\n‚îú‚îÄ‚îÄ react-native-views/\n\n\n\n  (+) Each team could have its own Git workflow, branching model, review process,\n  (+) Each platform has its own CI, code conventions,\n  (-) Building the native apps including the React Native bundle is complicated,\n  (-) Three pull requests (one on each repository) are needed if the functionality includes a native bridge and React Native development.\n\n\nNeither approach was perfect. So we decided to choose the safest one, and create multiple repositories. Also, this choice doesn‚Äôt forbid any change of direction toward the mono repository in the future‚Ä¶ The reverse seems much more complicated.\n\nDevelopment workflow\n\nEach native developer is now forced to have the react-native-views to be able to work on the native app.\nYou need to know that the native apps need node_modules dependencies of the React Native project, because they also contain the native part of React Native, and maybe some native code for React Native 3rd party you use.\nSo, we will need to clone the native app and the React Native repository.\n\nFor Android\n\ngit clone app-android\ngit clone react-native-views\n\nSo we will have two sibling folders:\n\n‚îú‚îÄ‚îÄ app-android/\n‚îú‚îÄ‚îÄ react-native-views/\n\n\nWe decided to use symlink to have a cleaner structure (and that will make the CI configuration easy later, see Continuous Integration), so the setup for the Android project will look like this:\n\ncd app-android\nln -s ../react-native-views ./react-native-views\ncd ../react-native-views\nnpm install\n\n‚îú‚îÄ‚îÄ app-android/\n‚îÇ   ‚îú‚îÄ‚îÄ react-native-views -&amp;gt; ../react-native-views\n‚îú‚îÄ‚îÄ react-native-views/\n‚îÇ   ‚îú‚îÄ‚îÄ node_modules/\n‚îÇ   ‚îú‚îÄ‚îÄ package.json\n\n\nFor iOS\n\nSimilar steps to the Android process, but it seems that Xcode has difficulty following package with a symlink ‚Ä¶ so we have to be a little smarter:\n\ngit clone app-ios\ngit clone react-native-views\n\ncd app-ios\nmkdir -p react-native-views/node_modules\ncd ../react-native-views\nln -s ../app-ios/react-native-views/node_modules ./node_modules\nnpm install\n\nWith this method, the node_modules files will be written in the symlink. So those files will be located in the source of the symlink, the app-ios/react-native-views/node_modules directory (This is pretty twisted, we had to admit).\n\n‚îú‚îÄ‚îÄ app-ios/\n‚îÇ   ‚îú‚îÄ‚îÄ react-native-views/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node_modules/\n‚îú‚îÄ‚îÄ react-native-views/\n‚îÇ   ‚îú‚îÄ‚îÄ node_modules -&amp;gt; ../app-ios/react-native-views/node_modules\n‚îÇ   ‚îú‚îÄ‚îÄ package.json\n\n\nReact Native\n\nNow we can choose: JavaScript developers are able to develop on any native app with the React Native packager (npm start in the react-native-views directory) and native developers can develop either with the packager started or with a pre-built React Native bundle (if their developments don‚Äôt concern React Native) by switching a Scheme (iOS) or a flavour (Android).\n\nContinuous integration\n\nThe next step was to find a way to improve the mobile development workflow.\nDuring our research, we found a SAAS tool named buddybuild that‚Äôs able to build the iOS &amp;amp; Android apps on each pull request. The setup for the native apps (before the React Native integration) or the React Native side project was really straightforward. It just magically works!\n\nWith the 3 Git repositories of our brownfield apps, it‚Äôs a bit more complicated than that. For this, buddybuild provides two useful hooks during the CI process. We just have to add a shell file in the repository:\n\n\n  buddybuild_postclone.sh: This is the hook that happens just after the cloning of the current repository by buddybuild\n  buddybuild_prebuild.sh: This hook is called after postclone and after buddybuild gets all dependencies (npm, Pod, Gradle ‚Ä¶), but just before the build starts\n\n\nTo allow our Product Owners to test the app‚Äôs functionality, whether it‚Äôs related to React Native or not, we‚Äôd need:\n\n\n  An iOS build on each pull request on the iOS repository\n  An Android build on each pull request on the Android repository\n  An iOS &amp;amp; Android build on each pull request on the React Native repository\n\n\nTo meet the specific needs of our app development, we required:\n\n\n  For iOS &amp;amp; Android, we need a way to include the React Native code which lies in another repository.\n  For the React Native repository, we need a way to build the iOS &amp;amp; Android apps which lie in other repositories as well, and including the React Native code in it.\n  Our iOS and Android apps up-to-date with both the master branch of the native app, and the master branch of the React Native repository.\n  If a feature needs modifications on both the native code and the React Native code (multiple pull requests, one on each concerned repositories), we want an app synchronized with all repositories.\n\n\nSo let‚Äôs dig in these 4 points.\n\nBuild the iOS &amp;amp; Android apps including the React Native bundle\n\nThe key here is to clone the React Native repository in the postclone buddybuild hook and reproduce the directory structure we have in development mode.\n\nfor iOS\n\nbuddybuild_postclone.sh:\n\ngit clone react-native-views\n\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n# Make Xcode able to access to the node dependencies\nln -s react-native-views/node_modules node_modules\n\nbuddybuild_prebuild.sh:\n\n# export React Native bundle:\nnode_modules/.bin/react-native bundle --platform ios --entry-file index.ios.js --bundle-output ../&amp;lt;appFolder&amp;gt;/main.ios.jsbundle --dev false\n\n‚îú‚îÄ‚îÄ buddybuild workspace/ (app-ios inside)\n‚îÇ   ‚îú‚îÄ‚îÄ react-native-views/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node_modules/\n‚îÇ   ‚îú‚îÄ‚îÄ package.json -&amp;gt; react-native-views/package.json\n‚îÇ   ‚îú‚îÄ‚îÄ node_modules -&amp;gt; react-native-views/node_modules\n\n\nfor Android\n\nbuddybuild_postclone.sh:\n\ngit clone react-native-views\n\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n# When buddybuild will run `npm install`, the node dependencies will be at the right place\nln -s react-native-views/node_modules node_modules\n\nbuddybuild_prebuild.sh:\n\n# export React Native bundle:\nnode_modules/.bin/react-native bundle --platform android --entry-file index.android.js --bundle-output ../&amp;lt;appFolder&amp;gt;/main.android.jsbundle --dev false\n\n‚îú‚îÄ‚îÄ buddybuild workspace/ (app-android inside)\n‚îÇ   ‚îú‚îÄ‚îÄ react-native-views/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node_modules/\n‚îÇ   ‚îú‚îÄ‚îÄ package.json -&amp;gt; react-native-views/package.json\n‚îÇ   ‚îú‚îÄ‚îÄ node_modules -&amp;gt; react-native-views/node_modules\n\n\nThe only thing you have to do in the buddybuild dashboard is to create the app for each platform and activate the build on pull request only (see screenshot below). Buddybuild will automatically trigger an iOS &amp;amp; Android build on each pull request for the native repositories.\n\n\n\nBuild the iOS &amp;amp; Android apps on each pull request from the React Native repository\n\nNow, we‚Äôd like to easily test each react-native-views pull request on both iOS and Android apps.\n\nFor that purpose, we used the buddybuild hook again. Here is the buddybuild_postclone.sh:\n\n# Create a react-native-views folder\nmkdir react-native-views\n# Move everything in it\nmv * react-native-views\n\n# The postclone hook is ran by buddybuild for both iOS and Android builds. We distinguish the platform here, thanks to the env variable BUDDYBUILD_APP_ID (set by buddybuild)..\nif [ &quot;$BUDDYBUILD_APP_ID&quot; = &quot;&amp;lt;buddybuildAndroidAppID&amp;gt;&quot; ]; then\ngit clone app-android\ncd app-android\nelse\ngit clone app-ios\ncd app-ios\nfi\n\n# Move the native app to the root of the workspace\nmv * ..\ncd ..\n\n# Create the future node_modules location folder\nmkdir -p react-native-views/node_modules\n# Create the symbolic link for the app to be able to found the node_modules at the good place\nln -s react-native-views/node_modules node_modules\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n\nFor iOS, you‚Äôll have:\n\n‚îú‚îÄ‚îÄ buddybuild workspace/ (app-ios inside)\n‚îÇ   ‚îú‚îÄ‚îÄ react-native-views/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node_modules/\n‚îÇ   ‚îú‚îÄ‚îÄ package.json -&amp;gt; react-native-views/package.json\n‚îÇ   ‚îú‚îÄ‚îÄ node_modules -&amp;gt; react-native-views/node_modules\n\n\nFor Android, you‚Äôll have:\n\n‚îú‚îÄ‚îÄ buddybuild workspace/ (app-android inside)\n‚îÇ   ‚îú‚îÄ‚îÄ react-native-views/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node_modules/\n‚îÇ   ‚îú‚îÄ‚îÄ package.json -&amp;gt; react-native-views/package.json\n‚îÇ   ‚îú‚îÄ‚îÄ node_modules -&amp;gt; react-native-views/node_modules\n\n\nBy doing that, buddybuild will automatically install the npm dependencies, then launch the same prebuild hook as the native repository to build the React Native bundle.\n\nUsing buddybuild, you can create the app for each platform, and trigger new builds only when pull requests are opened, or when commits are added to existing pull requests. Buddybuild also builds both apps when React Native pull requests are opened as well.\n\nWhen master of React Native change, update the master iOS &amp;amp; Android apps\n\nBuddybuild makes it very easy to trigger a build programmatically via the API. We also use Jenkins for unit tests and lint, so we have a job triggered every time a push is made on the master branch of react-native-views. We have reused this job and append the following:\n\n# Our credentials\nACCESS_TOKEN_BB=&amp;lt;AccessToken&amp;gt;\nAPP_ID_BB_IOS=&amp;lt;buddybuildiOSAppID&amp;gt;\nAPP_ID_BB_ANDROID=&amp;lt;buddybuildAndroidAppID&amp;gt;\n\n# Build iOS\ncurl -X POST -H  &#39;Authorization: Bearer &#39;$ACCESS_TOKEN_BB‚Äù -d &#39;branch=master‚Äô &#39;https://api.buddybuild.com/v1/apps/&#39;$APP_ID_BB_IOS&#39;/build&#39;\n\n# Build Android\ncurl -X POST -H  &#39;Authorization: Bearer &#39;$ACCESS_TOKEN_BB‚Äù -d &#39;branch=master‚Äô &#39;https://api.buddybuild.com/v1/apps/&#39;$APP_ID_BB_ANDROID&#39;/build&#39;\n\nNow, you can activate the master build on the native iOS &amp;amp; Android buddybuild build, and you‚Äôll have those apps up-to-date with the master branch.\n\n\n\nCross platform feature (both native &amp;amp; React Native)\n\nAt this point, this is not enough, because if you develop a feature that needs native and React Native modifications, you will not have the corresponding app before merging everything.\n\nWe have decided here to add a rule: for a ‚Äúcross platform feature‚Äù (like a bridge for a native component for example), we have to define the same name for the branches in each repositories.\n\nA bridge for a native component (the authentication bridge as an example) would have three Git branches with the same name, and three pull requests (one on each repository).\n\nBy following this convention, we only have to checkout that branch when we clone the external repository in our postclone hooks:\n\n{\n  # Detect with the env variable BUDDYBUILD_BRANCH (given by buddybuild) the branch we are on.\n  echo &quot;Git checkout branch: $BUDDYBUILD_BRANCH&quot;\n  git checkout $BUDDYBUILD_BRANCH\n} || {\n  echo &quot;Git default branch: master&quot;\n  git checkout master # if master is the name of your default branch\n}\n\nWe do that branch name checking on the three repositories. This way, the four buddybuild projects (app-ios, app-android, react-native-views-ios, and react-native-views-android) can build native applications with modification on both sides.\n\nConclusion\n\nThanks to React Native and buddybuild, we now have a complete workflow as powerful as we have on the website. Being able to review either React Native or native code, and testing a real app before the code lands on the master branch is a big improvement for code quality and a huge step forward towards more agility.\n\nBig up to Tapptic Team, M6Web React Native team for this work, to the buddybuild support team for the help when needed.\n\nSpecial thanks to Nicolas Cuillery and Alysha for their proofreading!\n"
} ,
  
  {
    "title"    : "Une donn√©e presque parfaite sur 6play",
    "category" : "",
    "tags"     : " lyon, conference, elasticsearch, video",
    "url"      : "/2016/11/24/une-donnee-presque-parfaite.html",
    "date"     : "November 24, 2016",
    "excerpt"  : "Benoit Viguier, prestataire de la soci√©t√© Elao pour M6Web, a fait un retour d‚Äôexp√©rience au Forum PHP de l‚ÄôAFUP sur l‚Äôarchitecture technique mise en place autour de la mise √†  disposition des donn√©es n√©cessaires √† 6play.\n\n\n\nLes slides sont √©galeme...",
  "content"  : "Benoit Viguier, prestataire de la soci√©t√© Elao pour M6Web, a fait un retour d‚Äôexp√©rience au Forum PHP de l‚ÄôAFUP sur l‚Äôarchitecture technique mise en place autour de la mise √†  disposition des donn√©es n√©cessaires √† 6play.\n\n\n\nLes slides sont √©galement disponibles en PDF.\n"
} ,
  
  {
    "title"    : "Enqu√™te exclusive au coeur de la technique de 6play. Les slides.",
    "category" : "",
    "tags"     : " conference",
    "url"      : "/2016/11/03/blendwebmix-6play-conference.html",
    "date"     : "November 3, 2016",
    "excerpt"  : "Voici les slides de la conf√©rence ‚ÄúPlus d‚Äôun milliard de vid√©os vues par an sur 6play - Enqu√™te exclusive au coeur de la technique‚Äù que nous avons donn√© le 2 novembre 2016 lors de la conf√©rence Blend Web Mix √† Lyon.\n\nhttps://docs.google.com/presen...",
  "content"  : "Voici les slides de la conf√©rence ‚ÄúPlus d‚Äôun milliard de vid√©os vues par an sur 6play - Enqu√™te exclusive au coeur de la technique‚Äù que nous avons donn√© le 2 novembre 2016 lors de la conf√©rence Blend Web Mix √† Lyon.\n\nhttps://docs.google.com/presentation/d/1BZGvoiubQsIzVjH9Px22wQyYmkboXZjpiubX2UtkMA4/edit?usp=sharing\n\nNous √©tions ravis d‚Äô√™tre sponsor de cet √©venement. Merci encore √† toute l‚Äôorganisation.\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity New York Conference 2016",
    "category" : "",
    "tags"     : " conference, velocity, webperf, devops",
    "url"      : "/2016/10/12/velocity-nyc-2016.html",
    "date"     : "October 12, 2016",
    "excerpt"  : "Nous √©tions cette ann√©e √† New York, √† quelques blocs de Time Square, pour suivre l‚Äô√©dition New Yorkaise de la Velocity Conference 2016.\nC‚Äôest une conf√©rence que nous appr√©cions particuli√®rement et √† laquelle nous nous rendons quasiment chaque ann√©...",
  "content"  : "Nous √©tions cette ann√©e √† New York, √† quelques blocs de Time Square, pour suivre l‚Äô√©dition New Yorkaise de la Velocity Conference 2016.\nC‚Äôest une conf√©rence que nous appr√©cions particuli√®rement et √† laquelle nous nous rendons quasiment chaque ann√©e, soit dans son √©dition europ√©enne (Berlin, Londres, Barcelone, et Amsterdam cette ann√©e en novembre), soit aux U.S. (pr√©c√©demment Santa Clara, New York cette ann√©e, et San Jos√© l‚Äôann√©e prochaine).\nC‚Äôest l‚Äôoccasion de suivre une conf de tr√®s haute qualit√© compos√©e de 4 ou 5 tracks en parall√®le, d√©di√©e aux probl√©matiques de performance et de scalabilit√©.\nOn remarque que d‚Äôann√©e en ann√©e la conf√©rence s‚Äôest r√©orient√©e autour du mouvement DevOps, alors qu‚Äôelle √©tait pr√©c√©demment beaucoup plus centr√©e sur la WebPerf (desktop et mobile).\n\nLa conf√©rence commence par l‚ÄôIgnite (sorte de mini conf√©rence dans la conf√©rence), bas√©e sur un format court (type Lightning Talk) de 5 minutes pour une pr√©sentation de 5 slides d√©filant automatiquement. On retiendra de cette premi√®re partie des talks int√©ressants exposant les tristes chiffres de la diversit√© dans la tech aux US, mais aussi une conf√©rence tr√®s dr√¥le de @beldhalpern de @ThePracticalDev sur des parodies des livres OReilly (voir le O RLY Cover Generator) :\n\nEnjoying the heck out of @ThePracticalDev at Ignite #velocityconf. Lololol! pic.twitter.com/ZlJWoP4cjh&amp;mdash; Bridget Kromhout (@bridgetkromhout) 20 septembre 2016\n\n\nL‚Äôignite s‚Äôest fini sur le c√©l√®bre Ignite Karaok√© o√π 16 volontaires se sont pr√™t√©s au jeu de cet exercice hilarant mais tellement difficile, consistant √† improviser une conf√©rence sur le sujet de son choix sur 5 slides inconnues de l‚Äôorateur et qui d√©filent automatiquement au bout de quelques secondes üòÉ. Ce qu‚Äôon fait aussi chez M6Web de temps en temps nomm√© Karaok√© Slideshow et que Kenny avait anim√© lors d‚Äôun Forum PHP (voir la vid√©o).\n\n\nCr√©dit : Flickr\n\nNous avons ensuite suivi deux jours de conf√©rences dont les th√®mes majeurs √©taient :\n\n\n  Les Service Workers\n  Les microservices\n  Le monitoring\n  HTTP2\n  La s√©curit√© des apps\n  Les d√©tections d‚Äôanomalie\n  Les ChatOps\n  Le WebMobile, AMP et les PWA\n\n\nChatOps\n\nUn des sujets assez r√©current, notamment dans la mouvance DevOps est l‚Äôutilisation des ChatOps, sujet popularis√© par Github (via Hubot).\nCela consiste g√©n√©ralement en un bot ou une IA pos√©e sur un outil de Chat type Slack, Flowdock ou Hipchat, permettant de simplifier la communication entre diff√©rentes √©quipes et les diff√©rents outils (ticketing, alerting, monitoring, √©tat d‚Äôune machine, etc). Une d√©mo de l‚ÄôIA de Dynatrace √† reconnaissance vocale √† √©t√© faite, montrant comment par la voix, on pouvait recevoir dans l‚Äôoutil de Chat les infos sur les incidents de la veille, cr√©er les tickets de support etc. Voir ici. Un peu gadget, mais rigolo.\n\nL‚Äôun des points √† retenir, c‚Äôest que m√™me si ces outils font partie de la ¬´ culture ¬ª DevOps, ce n‚Äôest pas l‚Äôajout d‚Äôun de ces outils qui fera appara√Ætre cette culture dans votre entreprise si vous ne l‚Äôavez pas.\n\n\n  Tools will not fix a broken culture\n\n\n\nCr√©dit : Flickr\n\nLe WebMobile, AMP, et les PWA\n\nPlusieurs conf√©rences avaient pour but de comparer ce que l‚Äôon pouvait obtenir de nos jours via du WebMobile versus ce que l‚Äôon a sur les apps natives. Le foss√© s‚Äôest √©norm√©ment r√©tr√©ci et les WebApps ont d√©sormais acc√®s √† la plupart des fonctionnalit√©s pr√©sentes c√¥t√© natif :\n\n\n  Notifications\n  Ajout sur le Home Screen de l‚Äôicone\n  Full Screen\n  Orientation\n  Gestion hors ligne\n  ‚Ä¶\n\n\nCe qui nous am√®ne aux Progressive Web Apps : PWA\n\nPete Lepage @petele de chez Google nous a notamment pr√©sent√© des projets open-source de Google pour mettre en place diff√©rentes politiques de cache via les ¬´ serviceWorkers ¬ª (voir https://developers.google.com/web/tools/service-worker-libraries/), ainsi que les futures api : Web Payments, Credential Management ‚Ä¶\n\n\n  Les slides.\n  Exemple de la PWA du Washington Post\n\n\nToujours sur la partie mobile, Malte Ubl (@cramforce), core d√©veloppeur de AMP, nous a pr√©sent√© le futur de ce protocole de Google pour offrir des pages plus rapides pour la consultation de site m√©dia sur mobile.\n\n\n  AMP is a web component library, validator and caching layer for reliably fast web content at scale\n\n\nEn commen√ßant par un bilan d‚ÄôAMP, 3000 PR + 200 contributeurs (au bout d‚Äôun an seulement !), Malte nous a expliqu√© qu‚Äôun site mobile tr√®s optimis√© pouvait logiquement √™tre plus performant qu‚ÄôAMP.\n\nArriveront prochainement sur AMP, le support des formulaires, des optimisations avanc√©es d‚Äôimages via le Google AMP Cache, des Service Workers pour AMP pour ne jamais t√©l√©charger AMP dans le ¬´ chemin critique ¬ª du chargement de la page.\n\nUn petit focus a aussi √©t√© fait sur les PWA et AMP avec amp-install-serviceworker qui est un Service Worker permettant d‚Äôinstaller la PWA apr√®s chargement de AMP, pour faire une upgrade transparente de AMP vers une PWA (Voir une d√©mo ici choumx.github.io/amp-pwa)\n\n\n  AMP : ¬´ Start Fast, Stay Fast ¬ª\n\n\nNous avons aussi vu une conf√©rence sur l‚Äôoptimisation de la consommation des webApps en terme de CPU / temps de r√©ponse, notamment via l‚Äô√©tude des capacit√©s JS de chacun des devices/OS avec le benchmark JetStream Javascript.\n\nOn d√©couvre notamment que l‚ÄôiPhone 7 a des capacit√©s assez impressionnantes, contrairement √† l‚ÄôiPhone 5C, que le mode ¬´ √©conomie d‚Äô√©nergie ¬ª ou encore une bonne insolation rendent les devices beaucoup moins performants. D‚Äôexcellentes slides √† voir ici : hearne.me/2hot\n\nWebPerf\n\nC√¥t√© WebPerf, peu de grosses nouveaut√©s, on retiendra @nparashuram qui nous a montr√© comment automatiser le ‚Äúprofiling‚Äù des ChromeDevTools dans Node.js via ChromeDriver !\n\nPlus d‚Äôinfos ici : https://blog.nparashuram.com/2016/09/rise-of-web-workers-nationjs.html\n\nTammy Everts (@tameverts de Soasta) et Pat Meenan (@patmeenan de Google et cr√©ateur de WebPageTest) nous ont fait un gros retour bas√© sur toutes les m√©triques r√©colt√©es par Soasta mPulse (outils de Real User Monitoring en SAAS) afin de d√©terminer des corr√©lations entre les temps de chargement et d‚Äôautres m√©triques (taux de rebond, conversion, etc.) gr√¢ce √† l‚Äôapplication de concept propre au Machine Learning sur une quantit√© √©norme de data. Toujours int√©ressant.\n\nSlides ici : https://conferences.oreilly.com/velocity/devops-web-performance-ny/public/schedule/detail/51082\n\n\nCr√©dit : Flickr\n\nC√¥t√© Single Page App, le Server Side Rendering est revenu √† plusieurs reprises afin d‚Äôavoir des SPA performantes dont le premier rendu est g√©n√©r√© c√¥t√© serveur, ce que permet nativement React, et d√©sormais Ember et Angular 2. Voir notre article sur l‚Äôisomorphisme.\n\nCot√© HTTP, on retiendra Hooman Beheshti qui nous a fait un retour d‚Äôexp√©rience sur HTTP2. Apr√®s une explication des nouveaut√©s du protocole (binary, single, long-lasting TCP connection, streams encapsulation, frames, bi-directional‚Ä¶), une comparaison avec HTTP 1 nous a √©t√© expos√©e. En conclusion, HTTP2 est complexe et la migration n‚Äôest pas une simple modification de param√®tre. Bien que cette nouvelle version est l√©g√®rement plus rapide, en particulier sur un r√©seau lent (&amp;lt;1Mbps), le protocole supporte tr√®s mal les pertes de paquets ou les fortes congestions √† cause de l‚Äôunique connexion (TCP slow start). La recommandation est de tester sur chaque site et d‚Äôoptimiser ses pages selon la version d‚ÄôHTTP utilis√©e. Une piste serait HTTP2 over UDP.\n\nLes slides\n\nDevOps\n\nDe nombreuses conf√©rences avaient pour objectif d‚Äôaborder les bienfaits du DevOps et plus largement les bonnes pratiques li√©es au mouvement afin de gagner en qualit√© et fiabilit√©.\n\nOn retiendra notamment la conf√©rence de Cornelia Davis (DevOps: Who does what?) explicitant les diff√©rents r√¥les dans un SDLC (Software Development Life-Cycle) et leur r√©partition en √©quipe dans l‚Äôorganisation.\n\nLes r√¥les dans le SDLC :\n\n\n  Architecte : Ent Archi, Biz Analyst, Portfolio Mgmt\n  SCO : Info sec\n  Infra : Srv Build, Cap Plan, Network, Ops\n  Middleware/AppDev : Middleware Eng, SW Arch, SW Dev, Client SW Dev, Svc Govern\n  Data : Data Arch, DBA\n  Biz : Prod Mgmt\n  Ent Apps : DCTM (Documentum) Eng.\n\n\nLa r√©partition en √©quipe propos√©e :\n\nPlatform (unique / transverse) :\n\n\n  Middleware/AppDev : Middleware Eng, Svc Govern\n  Infra : Srv Build, Cap Plan, Network, Ops\n  SCO : Info sec\n  Data : DBA\n\n\nCustomer Facing App (de 1 √† n √©quipes)\n\n\n  Middleware/AppDev : SW Arch, SW Dev, Client SW Dev\n  Data : Data Arch\n  Infra : Cap Plan, Ops\n  Biz : Prod Mgmt\n  Architecte : Biz Analyst\n\n\nEnablement (unique / transverse) :\n\n\n  Architecte : Ent Archi, Portfolio Mgmt\n\n\nDCTM - Documentum (Enterprise Content Management Platform) (unique / transverse) :\n\n\n  Infra : Ops, Cap Plan\n  Ent Apps : DCTM (Documentum) Eng.\n\n\nOn notera notamment la pr√©sence d‚ÄôOps dans les √©quipes Customer Facing App et inversement de Middleware Eng dans l‚Äô√©quipe Platform.\n\nDe m√™me, la pr√©sence d‚Äôarchitectes transverses (enablement) permet de garder une architecture coh√©rente. (Pas de slides disponibles pour cette conf√©rence)\n\n\nCr√©dit : Flickr\n\nMicroservices\n\nLa conf√©rence de @susanthesquark, ax√©e sur les microservices, rappelait quelques bonnes pratiques :\n\n\n  Architecture sans SPOF\n  Ne pas laisser la dette technique s‚Äôaccumuler\n  D√©ploiement continu\n  Travail en √©quipe entre Dev / PM / SRE\n  Monitoring\n  Proc√©dures standard de gestion des incidents\n  Post-mortem pour apprendre de ses erreurs‚Ä¶\n\n\nLes slides\n\nConcernant le monitoring des microservices, la conf√©rence de Reshmi Krishna @reshmi9k s‚Äôint√©ressait √† l‚Äôanalyse de la latence, inh√©rente √† ce type d‚Äôarchitecture. La principale technique propos√©e est celle du suivi d‚Äôune requ√™te de bout en bout, gr√¢ce notamment √† l‚Äôoutil Zipkin. De m√™me, une gestion des timeouts globale (pour chaque requ√™te pour tous les microservices) et dynamique (selon le contexte) permet de ma√Ætriser les probl√®mes en cas de ralentissement d‚Äôun service en particulier.\n\nLes slides\n\nS√©curit√©\n\nConcernant la s√©curit√©, la conf√©rence de Kelly Lum @aloria, passait en revue le minimun vital :\n\n\n  La s√©curit√© doit √™tre pens√©e d√®s la conception\n  Permettre aux utilisateurs de reporter facilement des probl√®mes de s√©curit√© et √™tre √† l‚Äô√©coute des r√©seaux sociaux\n  Toujours remercier les utilisateurs signalant les failles\n  Avoir une √©quipe testant r√©guli√®rement la s√©curit√© (Crack Team).\n  En cas de failles de s√©curit√©, apr√®s correction, toujours analyser les causes et apprendre de ses erreurs.\n\n\nLes slides\n\nConclusion\n\nVous pouvez retrouver la plupart des slides ici.\net voir les vid√©os de certaines conf√©rences ici.\nou ici.\n\nLes photos officielles de la conf sont ici.\n"
} ,
  
  {
    "title"    : "Use the Sensiolabs Security Checker to check potential vulnerabilities on Symfony projects",
    "category" : "",
    "tags"     : " 6tech, lyon, symfony, security, php, jenkins",
    "url"      : "/2016/07/20/sf2-security-checker.html",
    "date"     : "July 20, 2016",
    "excerpt"  : "Numerous vulnerabilities are detected every day. That‚Äôs a good thing and a key benefit of using open source products. At m6web we don‚Äôt want to be exposed to known vulnerabilities, so we use a service provided by Sensiolabs in our continuous integ...",
  "content"  : "Numerous vulnerabilities are detected every day. That‚Äôs a good thing and a key benefit of using open source products. At m6web we don‚Äôt want to be exposed to known vulnerabilities, so we use a service provided by Sensiolabs in our continuous integration tool (Jenkins) to check it.\n\nJust add those lines in your ant build file (and adapt basedir) :\n\n    &amp;lt;!-- =================================================================== --&amp;gt;\n    &amp;lt;!-- Security checker                                                    --&amp;gt;\n    &amp;lt;!-- =================================================================== --&amp;gt;\n    &amp;lt;target name=&quot;sf2-security-checker&quot;&amp;gt;\n     &amp;lt;exec executable=&quot;bash&quot; dir=&quot;${basedir}/sources/bin&quot; failonerror=&quot;true&quot;&amp;gt;\n         &amp;lt;arg value=&quot;-c&quot;/&amp;gt;\n         &amp;lt;arg value=&quot;curl -Os https://get.sensiolabs.org/security-checker.phar&quot; /&amp;gt;\n     &amp;lt;/exec&amp;gt;\n     &amp;lt;exec executable=&quot;php&quot; dir=&quot;${basedir}/sources&quot; failonerror=&quot;true&quot;&amp;gt;\n         &amp;lt;arg line=&quot;${basedir}/sources/bin/security-checker.phar security:check composer.lock&quot; /&amp;gt;\n     &amp;lt;/exec&amp;gt;\n    &amp;lt;/target&amp;gt;\n\n\nAnd automatically check your composer.lock againts vulnerabilities. Your build will fail if something wrong is detected.\n\nFor example, with the recent Guzzle one :\n\n\n\nYou can contribute to the vulnerabilities database and the checker via Github.com.\n"
} ,
  
  {
    "title"    : "Retour d&#39;exp√©rience sur l&#39;utilisation de Cassandra sur 6play en vid√©o",
    "category" : "",
    "tags"     : " 6tech, lyon, conference, cassandra, video",
    "url"      : "/2016/07/04/rex-cassandra.html",
    "date"     : "July 4, 2016",
    "excerpt"  : "\n\nErratum : dans les phases de questions r√©ponses, j‚Äôannonce une phase de test √† 10K RPS (requ√™tes par seconde) ; il s‚Äôagissait de RPM (requ√™tes par minute).\n\nLors du match Suisse vs France, diffus√© sur M6 pendant la coupe d‚ÄôEurope de football, la...",
  "content"  : "\n\nErratum : dans les phases de questions r√©ponses, j‚Äôannonce une phase de test √† 10K RPS (requ√™tes par seconde) ; il s‚Äôagissait de RPM (requ√™tes par minute).\n\nLors du match Suisse vs France, diffus√© sur M6 pendant la coupe d‚ÄôEurope de football, la brique users est mont√©e √† 75K RPM (soit 1200 rps) et 84K pour Islande vs France.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Migrate smoothly your Flux isomorphic app to Redux",
    "category" : "",
    "tags"     : " react, flux, redux, fluxible, isomorphic, javascript",
    "url"      : "/2016/07/04/migrate-smoothly-flux-isomorphic-app-to-redux.html",
    "date"     : "July 4, 2016",
    "excerpt"  : "Flux, history reminders‚Ä¶\n\n¬´ Flux is the application architecture that Facebook uses for building client-side web applications. ¬ª That‚Äôs the definition of Flux on the Facebook website. So Flux is just a pattern, not a framework, that goes well with...",
  "content"  : "Flux, history reminders‚Ä¶\n\n¬´ Flux is the application architecture that Facebook uses for building client-side web applications. ¬ª That‚Äôs the definition of Flux on the Facebook website. So Flux is just a pattern, not a framework, that goes well with React, but not only. The model is focused on user interactions. Its main strength is the unidirectional data flow that enforces developers to be careful and ensures code consistency when application grows up.\n\nSeveral libraries propose tools to implement Flux pattern easily. If no one stood out from the crowd at the beginning, now Redux, created by Dan Abramov, is clearly the one that the community have chosen as you can see below. Most Flux based React start kits you can find are based on Redux.\n\n\n\nAt M6Web, our 6play web application is not designed with Redux, but we use Fluxible. Fluxible is another Flux library, developed by Yahoo. We chose it back in December 2014, when we started the project, because Fluxible was at the time one of the few tool designed for isomorphic applications. Moreover it was already used in production by Yahoo.\n\nWhy do we think Redux is a better choice\n\nEven though Fluxible did get the job done, we are now willing to upgrade our application to Redux. Why?\n\n\n  The popularity of Redux will certainly affect other libraries life and support in the future, maybe Fluxible will be concerned. Fluxible is only supported by a firm and not really by the community.\n  Fluxible has a powerful but complex structure based on contexts and plugins. This can be useful, however when new developers come on the project, this is not always easy to understand. We are always searching to make code simpler for maintainability and we think that Redux is a better alternative than Fluxible on this topic.\n  For a given feature, developers write less code using Redux because the design is very simple, there is no extra boilerplate, the flow is condensed as much as possible. As a consequence, unit tests are easier to write.\n  There are very useful tools about Redux that make Developer eXperience better. For instance, the Redux DevTools allows to time travel live between Flux events. The middleware concept extending capabilities of actions is also interesting.\n  We are beginning to make our development processes converge. Every new React project here starts on Redux, including the Proof Of Concept we made with React Native. Using the same libraries made code sharing easier for us.\n\n\nMigrating a big app from Fluxible to Redux is crazy, isn‚Äôt it?\n\n6play is a very big web application. How to migrate to Redux in a reasonable amount of time and without risk?\n\nWe were quite sure that Redux and Fluxible could work together. The goal would be to migrate gradually to Redux without having to remove Fluxible in one giant step. First of all, because we can‚Äôt mobilise enough resources to do this in a relatively short time. Secondly, we want to avoid a big deploy in production and potentially critical bugs (even though our application is well tested, there are always cases that we can‚Äôt control like memory load for example).\n\nWe tried it‚Ä¶ And we succeeded! And this is quite simple.\n\nFirst, we define the store configuration like other Redux application.\n\n// configureStore.js\n\nimport {createStore, combineReducers, applyMiddleware, compose} from &#39;redux&#39;;\nimport thunk from &#39;redux-thunk&#39;;\nimport {canUseDOM} from &#39;fbjs/lib/ExecutionEnvironment&#39;;\n\nimport myReducer1 from &#39;./modules/myModule1/myModule1.reducer&#39;;\nimport myReducer2 from &#39;./modules/myModule2/myModule2.reducer&#39;;\n\nexport default initialState =&amp;gt; createStore(\n  combineReducers({myReducer1, myReducer2}),\n  initialState,\n  compose(\n    applyMiddleware([thunk]),\n    canUseDOM &amp;amp;&amp;amp; window.devToolsExtension ? window.devToolsExtension() : f =&amp;gt; f\n  )\n);\n\nThen we initialize Redux store in our server file. For isomorphic purposes, we have to serialize stores‚Äô state and give it to the html so that client side can take control of the application with server‚Äôs data. So here, we build data for the client by combining Redux and Fluxible states.\n\n// server.js\n\nimport {provideContext} from &#39;fluxible-addons-react&#39;;\nimport {match, RouterContext} from &#39;react-router&#39;;\nimport configureStore from &#39;./configureStore&#39;;\n\nprocessAppRequest() {\n  // ...\n\n  const fluxibleContext = FluxibleApp.createContext();\n  const reduxStore = configureStore(initialState);\n\n  match({routes: FluxibleApp.getComponent(), location: url}, (error, redirectLocation, routerState) =&amp;gt; {\n    // ...\n\n    // Original Fluxible root element\n    const rootElement = React.createElement(\n      provideContext(RouterContext, customContextTypes),\n      {...routerState, context: fluxibleContext.getComponentContext()}\n    );\n\n    // Now with Redux\n    const markup = ReactDOMServer.renderToString(\n      React.createElement(Provider, {store: reduxStore}, rootElement)\n    );\n\n    // Build state for client\n    const finalState = {\n      ...FluxibleApp.dehydrate(fluxibleContext),\n      reduxStoreState: reduxStore.getState()\n    };\n\n    // Then build the response layout with the markup and the whole state as usual\n    // ...\n  }\n}\n\nOn client side, we do the opposite operation.\n\n// client.js\n\nimport {provideContext} from &#39;fluxible-addons-react&#39;;\nimport {Router, browserHistory} from &#39;react-router&#39;;\nimport configureStore from &#39;./configureStore&#39;;\n\nconst dehydratedState = window[stateVarName];\nconst reduxStore = configureStore(dehydratedState.reduxStoreState);\n\n// Fluxible rehydrate its state\napp.rehydrate(dehydratedState, (error, fluxibleContext) =&amp;gt; {\n  // ...\n\n  // Original Fluxible root element\n  const rootElement = React.createElement(provideContext(Router, customContextTypes), {\n    history: browserHistory,\n    routes: app.getComponent(),\n    context: fluxibleContext.getComponentContext()\n  });\n\n  // Now with Redux\n  ReactDOM.render(\n    React.createElement(Provider, {store: reduxStore}, rootElement),\n    document.getElementById(rootId)\n  );\n});\n\nAnd that‚Äôs it! We can now use Redux in our component as usual, in combination with Fluxible. We can define actions and reducers for new features (instead of using Fluxible) but we can also transform progressively some Fluxible stores and actions into Redux flow, this is very easy. API requests stay in actions but data processing moves to reducers. Then data sorting and filtering logic in Fluxible stores moves to selectors.\n\nComponents connection to stores\n\nTwo files to rule them all\n\nWith Fluxible, we linked components with stores through connectToStore in the same file and exported only the connected component. But we think now it is a bad practice:\n\n\n  splitting data fetching from stores and display logic is interesting for maintainability and code understanding,\n  it is much easier to unit test the component without the connection to store, connectToStores (Fluxible) or connect (Redux) methods are parts of a 3rd-party library, and we don‚Äôt need to test it.\n\n\nFrom now on, components are files named *.component.js and stores connections are in *.connector.js files in the same folder. We can link a component both with Redux and Fluxible stores.\n\n// myComponent.connector.js\n\nimport MyComponent from &#39;./myComponent.component&#39;;\n\n// Stores\nimport connectToStores from &#39;fluxible-addons-react/connectToStores&#39;;\nimport {connect} from &#39;react-redux&#39;;\nimport MyFluxibleStore from &#39;../stores/myFluxible.store&#39;;\n\n// Utils\nimport {getSomeDataFromState} from &#39;../myModule.selectors&#39;;\n\n// Redux\nexport const mapStateToProps = (state, props) =&amp;gt; {\n  return {dataFromRedux: getSomeDataFromState(state, props.myProps2)};\n};\n\n// Fluxible\nexport default connectToStores(\n  connect(mapStateToProps)(MyComponent),\n  [MyFluxibleStore],\n  (context, props) =&amp;gt; ({\n    dataFromFluxible: context.getStore(MyFluxibleStore).getSomeData(props.myProps1)\n  })\n);\n\nWe export mapStateToProps function because in a few cases it contains logic that may be interesting to unit test.\n\nStores connections order\n\nIn this example, the link to Fluxible store is higher in components tree than the Redux one as we can see below.\n\n\n\nIt means that if Redux state changes, the Fluxible wrapper component won‚Äôt be reloaded but in the reverse case, both Fluxible and Redux wrapper components will rerender. In most scenarios, it doesn‚Äôt matter. Connection order of Redux and Fluxible is significant in two situations:\n\n\n  If one connection depends on data stored in the other library state, it has to be lower in components tree.\n\n\n// myComponent.connector.js\n\nimport MyComponent from &#39;./myComponent.component&#39;;\n\n// Stores\nimport connectToStores from &#39;fluxible-addons-react/connectToStores&#39;;\nimport {connect} from &#39;react-redux&#39;;\nimport MyFluxibleStore from &#39;../stores/myFluxible.store&#39;\n\n// Utils\nimport {getSomeDataFromState} from &#39;../myModule.selectors&#39;;\n\n// Fluxible wrapper depends on data from Redux state\nconst MyComponentFluxibleConnector = connectToStores(\nMyComponent,    \n  [MyFluxibleStore],\n  (context, props) =&amp;gt; ({\n    dataFromFluxible: context.getStore(MyFluxibleStore).getSomeDataFromReduxState(props.myPropsFromRedux)\n  })\n);\n\n// Redux\nexport const mapStateToProps = state =&amp;gt; {\n  return {myPropsFromRedux: getSomeDataFromState(state)};\n};\n\nexport default connect(mapStateToProps)(MyComponentFluxibleConnector);\n\n\n  If the higher connection is made on Fluxible stores (like first example of myComponent.connector.js), data passed to props must be immutable otherwise it can cause edge effects. Indeed, Redux wrapper component checks if it has to rerender when props change by comparing their references. So, if we mutate data in Fluxible store when dispatch is handled, references don‚Äôt change and Redux wrapper (and sub-components) will not rerender (unless you tell the connect method that your component isn‚Äôt ‚Äúpure‚Äù).\n\n\nIf we watch carefully to those particular cases, we will succeed in our quest!\n\nIn a nutshell, Redux can easily work in addition to Fluxible (and certainly to other Flux libraries), most likely because of the lightness of its implementation. It is very convenient to upgrade smoothly a big application on Redux! But be aware that it is only a transitory situation, the final goal is to use only Redux. We wrote 50% less code with this upgrade, not bad‚Ä¶ Developers are lazy, don‚Äôt forget this! If you have some feedback on Redux and/or Fluxible, don‚Äôt hesitate to share your experience with us :)\n\n"
} ,
  
  {
    "title"    : "Retour d‚Äôexp√©rience : r√©aliser des Workers en PHP - Fabien de Saint pern au PHP Tour 2016 ",
    "category" : "",
    "tags"     : " 6tech, lyon, conference, video, phptour, php, Symfony",
    "url"      : "/2016/06/23/video-phptour-worker-php.html",
    "date"     : "June 23, 2016",
    "excerpt"  : "Fabien de Saint pern - lead dev de notre team back-end 6play - √©tait au PHP Tour et a fait une pr√©sentation sur la fa√ßon dont nous faisons des workers en PHP.\n\n\n\n",
  "content"  : "Fabien de Saint pern - lead dev de notre team back-end 6play - √©tait au PHP Tour et a fait une pr√©sentation sur la fa√ßon dont nous faisons des workers en PHP.\n\n\n\n"
} ,
  
  {
    "title"    : "Preview your Android &amp; iOS React Native apps on your Github Pull Request",
    "category" : "",
    "tags"     : " reactnative, react, mobile, github, jenkins, fastlane, appetize",
    "url"      : "/2016/06/20/preview-android-ios-react-native-on-github-pull-request.html",
    "date"     : "June 20, 2016",
    "excerpt"  : "We are playing since a few weeks with React Native for a Proof Of Concept and wanted to have the same development workflow for mobile apps, as we have for the web.\n\nHere is the workflow we use for web development:\n\n\n  Branch  : every bugfix or fea...",
  "content"  : "We are playing since a few weeks with React Native for a Proof Of Concept and wanted to have the same development workflow for mobile apps, as we have for the web.\n\nHere is the workflow we use for web development:\n\n\n  Branch  : every bugfix or feature is developed on a new git branch,\n  Pull Request (PR)  : we make PR for each bugfix or feature to propose the modification to the ¬´ master ¬ª git branch,\n  Code Review  : other teammates have to review each PR and add :+1: when they agree with the modification,\n  Test  : a CI system (Jenkins) runs Unit and Integration tests, and Lint on each PR,\n  Preview  : an internal tool (Github Hooker) is called with a Github webhook on each PR to create a staging environment.\n\n\nWhen every step is ok, the PR is merged.\n\nThe ¬´ Branch step ¬ª, ¬´ PR step ¬ª and ¬´ Code Review step ¬ª are mostly related to our CVS (Github Enterprise) and are not a problem.\nThe ¬´ Test step ¬ª is related to React Native. We already use Jest and ESLint, but we have to dig more for Integration test (Appium ?).\n\nThe ¬´ Preview step ¬ª is more interesting. It was not the simplest thing to do on our web project, but this is probably one of the most useful feature we have on our stack.\nHaving a staging environment for all open PR allows devs, PO, PM and scrum masters to play with this exact version of the code (on any browser they want), and really see if the bug is fixed, or if the feature correspond to the PO needs. It allows everyone to iterate and make feedbacks before the code lands on the master branch. It‚Äôs also a good way to be sure your app build didn‚Äôt fail.\n\nSo, what we want is to have on each of our React Native PR, a link to preview iOS and Android version of our app in a web browser, refreshed after every commit on the branch.\n\nThe goal of this blog post is just to show you, that it is something doable and really useful. If you are interested in, here are some more information, that maybe can help you.\n\nThe stack\n\nConcerning the CI, we already use Jenkins, so we will continue to. Beware that for building iOS apps, a CI running on OSX is needed. In our case, we had added a Jenkins slave to our Jenkins pool. If you don‚Äôt have CI system internally, you should take a look at Bitrise or CircleCi because they propose OSX CI systems.\nOur CVS is Github Enterprise, but everything is also possible with Gitlab (or any other CVS).\nWe use Fastlane.tools to automate build and credentials support. (Mostly because it was recommended by some of our iOS developers).\n\nIn order to preview iOS and Android app in a web browser, we use the amazing SAAS service Appetize.io (free for 100min/month).\n\n\n\nHow did we do ?\n\nWe had set up an OSX machine with a fresh Jenkins install, and created a job that triggers a build everytime a push is made on a PR, thanks to the ‚ÄúGithub Pull Request Builder ¬ª Jenkins plugin. There is also a lot of things to configure on this machine (Nodejs, Ruby, xCode ‚Ä¶), and i recommend you to do some builds (iOS and Android) manually to be sure everything is ready.\n\nFastlane is an open-source automation toolset for iOS &amp;amp; Android. It lets you write ¬´ lane ¬ª to automate a lot of things. We set up a unique Fastlane file at the root of our React Native project directory dealing with Android &amp;amp; iOS lanes.\n\nTo suit our needs, we created one lane ¬´ deployAppetize ¬ª for each platform: it performs the corresponding build, uploads it to Appetize.io via their API, and updates the Github PR Statuses during the process.\n\nI‚Äôm not a Ruby programmer, so please, don‚Äôt blame me, and feel free to improve the code below if you want (on this Github Gist).\nThis is neither the state of the art, nor a beautiful open source thing, we just share what we did in case it helps someone :-)\n\nBefore doing anything, you‚Äôll have to set some variables on Fastlane, so go to the Fastfile file in your fastlane folder:\n\n#3rd party lib to do some http calls\nrequire &#39;httparty&#39;\n\nfastlane_version &quot;1.95.0&quot;\ndefault_platform :ios\n\nbefore_all do\n  # put here your token and iOS scheme app\n  ENV[&quot;GITHUB_TOKEN&quot;] = &quot;----&quot;\n  ENV[&quot;APPETIZE_TOKEN&quot;] = &quot;----&quot;\n  ENV[&quot;APP_IOS_SCHEME&quot;] = &quot;----&quot;\n\n  # get the last git commit information\n  ENV[&quot;GIT_COMMIT&quot;] = last_git_commit[:commit_hash]\n\n  # Use ghprbSourceBranch env variable on CI, git_branch lane elsewhere\n  if !ENV[&quot;ghprbSourceBranch&quot;]\n    ENV[&quot;ghprbSourceBranch&quot;] = git_branch\n  end\n\nend\n\nCreate a private lane to make the POST request to your Github statuses API to avoid DRY:\n\n# Update git statuses of your commit.\nprivate_lane :githubStatusUpdate do |options|\n\n  response = HTTParty.post(\n    &quot;https://&amp;lt;yourgithubenterprisedomain.tld&amp;gt;/api/v3/repos/&amp;lt;orga&amp;gt;/&amp;lt;repos&amp;gt;/statuses/#{ENV[&quot;GIT_COMMIT&quot;]}?access_token=#{ENV[&quot;GITHUB_TOKEN&quot;]}&quot;,\n    :body =&amp;gt; {\n      :context =&amp;gt; options[:context],\n      :state =&amp;gt; options[:state],\n      :description =&amp;gt; options[:description],\n      :target_url =&amp;gt; options[:url]\n    }.to_json,\n    :headers =&amp;gt; { &#39;Content-Type&#39; =&amp;gt; &#39;application/json&#39; }\n  )\nend\n\nAppetize allows you to create different apps. We want one app per PR, and update the corresponding app when a new commit is made on a PR. For that, we keep track of the branch name by storing it in the ¬´ notes ¬ª field of the app on Appetize.io.\n\nSo, here‚Äôs a private lane to get back the public key of the corresponding app on Appetize.io, to update the good one if it already exists.\n\n# get the publicKey of the appetizeApp corresponding to your git branch\nprivate_lane :getAppetizePublicKey do |options|\n  publicKey = &quot;&quot;\n\n  response = HTTParty.get(&quot;https://#{ENV[&quot;APPETIZE_TOKEN&quot;]}@api.appetize.io/v1/apps&quot;)\n  json = JSON.parse(response.body)\n\n  # Find branch name in notes\n  json[&quot;data&quot;].each do |value|\n    if value[&quot;note&quot;] == ENV[&quot;ghprbSourceBranch&quot;] &amp;amp;&amp;amp; value[&quot;platform&quot;] == options[:platform]\n      publicKey = value[&quot;publicKey&quot;]\n    end\n  end\n\n  publicKey\nend\n\nNow, we have everything ready to do the deployAppetize lane for iOS :\n\nplatform :ios do\n\n  desc &quot;Deployment iOS lane&quot;\n\n    lane :deployAppetize do\n\n      githubStatusUpdate(\n        context: &#39;Appetize iOS&#39;,\n        state: &#39;pending&#39;,\n        url: &quot;https://appetize.io/dashboard&quot;,\n        description: &#39;iOS build in progress&#39;\n      )\n\n      Dir.chdir &quot;../ios&quot; do\n        tmp_path = &quot;/tmp/fastlane_build&quot;\n\n        #seems not possible to use gym to do the simulator release ?\n        xcodebuild_configs = {\n          configuration: &quot;Release&quot;,\n          sdk: &quot;iphonesimulator&quot;,\n          derivedDataPath: tmp_path,\n          xcargs: &quot;CONFIGURATION_BUILD_DIR=&quot; + tmp_path,\n          scheme: &quot;#{ENV[&quot;APP_IOS_SCHEME&quot;]}&quot;\n        }\n\n        Actions::XcodebuildAction.run(xcodebuild_configs)\n\n        app_path = Dir[File.join(tmp_path, &quot;**&quot;, &quot;*.app&quot;)].last\n\n        zipped_bundle = Actions::ZipAction.run(path: app_path, output_path: File.join(tmp_path, &quot;Result.zip&quot;))\n\n        Actions::AppetizeAction.run(\n          path: zipped_bundle,\n          api_token: &quot;#{ENV[&quot;APPETIZE_TOKEN&quot;]}&quot;,\n          platform: &quot;ios&quot;,\n          note: &quot;#{ENV[&quot;ghprbSourceBranch&quot;]}&quot;,\n          public_key: getAppetizePublicKey({platform: &quot;ios&quot;})\n        )\n\n        FileUtils.rm_rf(tmp_path)\n\n      end\n\n      githubStatusUpdate(\n        context: &#39;Appetize iOS&#39;,\n        state: &#39;success&#39;,\n        url: &quot;#{lane_context[SharedValues::APPETIZE_APP_URL]}&quot;,\n        description: &#39;iOS build succeed&#39;\n      )\n    end\n\n    error do |lane, exception|\n      case lane\n        when /deployAppetize/\n          githubStatusUpdate(\n            context: &#39;Appetize iOS&#39;,\n            state: &#39;failure&#39;,\n            url: &quot;https://appetize.io/dashboard&quot;,\n            description: &#39;iOS build failed&#39;\n          )\n        end\n      end\nend\n\nFor Android, it‚Äôs almost the same things, except we have to do some small business logic to find the apk generated by Gradle, with this private lane :\n\n# find the path of the last apk build\nprivate_lane :getLastAPKPath do\n  apk_search_path = File.join(&#39;../android/&#39;, &#39;app&#39;, &#39;build&#39;, &#39;outputs&#39;, &#39;apk&#39;, &#39;*.apk&#39;)\n  new_apks = Dir[apk_search_path].reject { |path| path =~ /^.*-unaligned.apk$/i}\n  new_apks = new_apks.map { |path| File.expand_path(path)}\n  last_apk_path = new_apks.sort_by(&amp;amp;File.method(:mtime)).last\n\n  last_apk_path\nend\n\nAnd now you should be able to also deploy to Appetize.io on Android :\n\nplatform :android do\n\n  desc &quot;Deployment Android lane&quot;\n\n    lane :deployAppetize do\n\n      githubStatusUpdate(\n        context: &#39;Appetize Android&#39;,\n        state: &#39;pending&#39;,\n        url: &quot;https://appetize.io/dashboard&quot;,\n        description: &#39;Android build in progress&#39;\n      )\n\n      gradle(\n        task: &quot;assemble&quot;,\n        build_type: &quot;Release&quot;,\n        project_dir: &quot;android/&quot;\n      )\n\n      Actions::AppetizeAction.run(\n        path: getLastAPKPath,\n        api_token: &quot;#{ENV[&quot;APPETIZE_TOKEN&quot;]}&quot;,\n        platform: &quot;android&quot;,\n        note: &quot;#{ENV[&quot;ghprbSourceBranch&quot;]}&quot;,\n        public_key: getAppetizePublicKey({platform: &quot;android&quot;})\n      )\n\n      githubStatusUpdate(\n        context: &#39;Appetize Android&#39;,\n        state: &#39;success&#39;,\n        url: &quot;#{lane_context[SharedValues::APPETIZE_APP_URL]}&quot;,\n        description: &#39;Android build succeed&#39;\n      )\n    end\n\n    error do |lane, exception|\n      case lane\n        when /deployAppetize/\n          githubStatusUpdate(\n            context: &#39;Appetize Android&#39;,\n            state: &#39;failure&#39;,\n            url: &quot;https://appetize.io/dashboard&quot;,\n            description: &#39;Android build failed&#39;\n          )\n      end\nend\n\nIt‚Äôs over. You just have to add those commands to your CI to do the job :\n\nnpm install\nFastlane ios deployAppetize\nFastlane android deployAppetize\n\n\nYou have now two new checks on each PR with a link to the iOS or Android instance on Appetize.io.\n\n\n\nThe complete Fastfile on a Github Gist : FastFile\n\nConclusion\n\nAt M6web, we are glad to see the whole React Native promise taking a concrete shape: the developer experience is the same for both mobile &amp;amp; web development, even about tooling. We are continuing to play with it and we‚Äôll certainly keep posting articles here, stay tuned !\n\nP.S.: You could look at the Fabric Blog post on the device grid for Fabric but with Danger commenting on the PR instead of Github Statuses, and iOS only.\n\nP.S.2: You could also look at Reploy.io, which try to improve this workflow with extra features and a more cleaner UX than Appetize.io, but it is ‚Äúalpha‚Äù for now.\n"
} ,
  
  {
    "title"    : "M6web fera un retour d&#39;exp√©rience sur l&#39;usage de Cassandra sur 6play le 14/06/2016",
    "category" : "",
    "tags"     : " 6tech, lyon, conference",
    "url"      : "/2016/05/25/m6web-retourdxp-cassandra.html",
    "date"     : "May 25, 2016",
    "excerpt"  : "Olivier Mansour, responsable R&amp;amp;D, sera pr√©sent au Cassandra Days le 14 Juin √† Paris pour faire un retour d‚Äôexp√©rience sur l‚Äôutilisation de Cassandra sur 6play.\n\n\n\nL‚Äô√©v√®nement est gratuit : https://www.eventbrite.co.uk/e/billets-datastax-day-pa...",
  "content"  : "Olivier Mansour, responsable R&amp;amp;D, sera pr√©sent au Cassandra Days le 14 Juin √† Paris pour faire un retour d‚Äôexp√©rience sur l‚Äôutilisation de Cassandra sur 6play.\n\n\n\nL‚Äô√©v√®nement est gratuit : https://www.eventbrite.co.uk/e/billets-datastax-day-paris-25165891860.\n\n"
} ,
  
  {
    "title"    : "Arr√™tons de perdre du temps √† d√©buguer !",
    "category" : "",
    "tags"     : " afup, php, debug, conference",
    "url"      : "/2016/05/24/arretons-de-perdre-du-temps.html",
    "date"     : "May 24, 2016",
    "excerpt"  : "Arr√™tons de perdre du temps √† d√©buguer ! D√©buguer peut se r√©v√©ler long et fastidieux. \nC‚Äôest du temps perdu qu‚Äôon pourrait passer √† cr√©er de la valeur ajout√©e. \nC‚Äôest d‚Äôune mani√®re ou d‚Äôune autre une perte pour le business. \nAyant commenc√© mon ent...",
  "content"  : "Arr√™tons de perdre du temps √† d√©buguer ! D√©buguer peut se r√©v√©ler long et fastidieux. \nC‚Äôest du temps perdu qu‚Äôon pourrait passer √† cr√©er de la valeur ajout√©e. \nC‚Äôest d‚Äôune mani√®re ou d‚Äôune autre une perte pour le business. \nAyant commenc√© mon entr√©e dans la vie active par une TMA, j‚Äôai compris vite et de mani√®re un peu brutale que √ßa fait pourtant partie de la vie du d√©veloppeur qui devient parfois d√©bugueur. \nQuelles solutions et astuces pouvons-nous mettre en place afin d‚Äô√™tre plus efficace dans cette t√¢che r√©barbative ?\n"
} ,
  
  {
    "title"    : "M6web sera pr√©sent au sfpot de Lille du 16/06/16",
    "category" : "",
    "tags"     : " 6tech, lille, sfpot, conference",
    "url"      : "/2016/05/19/6tech-sfpot-lille.html",
    "date"     : "May 19, 2016",
    "excerpt"  : "Pierre Marichez, Renaud Bougr√© et Nicolas Beze une partie de l‚Äô√©quipe PHP de M6Web Lille, vous feront part d‚Äôun retour d‚Äôexp√©rience sur l‚Äôindustrialisation des d√©veloppements.\nCa parlera jenkins, gitlab, gitlab-ci, outil de gestion de projets, api...",
  "content"  : "Pierre Marichez, Renaud Bougr√© et Nicolas Beze une partie de l‚Äô√©quipe PHP de M6Web Lille, vous feront part d‚Äôun retour d‚Äôexp√©rience sur l‚Äôindustrialisation des d√©veloppements.\nCa parlera jenkins, gitlab, gitlab-ci, outil de gestion de projets, api, sentry, capistrano, user scripts, docker, grafana, slack‚Ä¶\n\nLors de ce sfpot, Kevin Dunglas pr√©sentera le DunglasActionBundle et Alexandre Salom√© et Luc Vieillescazes vous feront un retour sur le sflive 2016.\n\nAlors rendez-vous tous le 16 juin 2016 √† partir de 19h00 au Liberch‚Äôti, 169 Boulevard de la Libert√© √† Lille (M√©tro R√©publique).\n\nPour vous inscrire, √ßa se passe ici\n\n\n\nPour plus d‚Äôinformations sur cet √©v√©nement et les autres sfpot lillois, rendez-vous sur le site des Tilleuls.\n"
} ,
  
  {
    "title"    : "M6web sera pr√©sent au PHPTour Clermont-Ferrand",
    "category" : "",
    "tags"     : " 6play, afup, phptour, conference",
    "url"      : "/2016/05/09/6tech-phptour-clermont.html",
    "date"     : "May 9, 2016",
    "excerpt"  : "Fabien de Saint Pern, un des leads devs sur la plateforme 6play, aura l‚Äôoccasion de pr√©senter une conf√©rence au PHPTour Clermont-Ferrand le 24 Mai. Il fera un retour d‚Äôexp√©rience concret sur nos pratiques autour de la r√©alisation de workers asynch...",
  "content"  : "Fabien de Saint Pern, un des leads devs sur la plateforme 6play, aura l‚Äôoccasion de pr√©senter une conf√©rence au PHPTour Clermont-Ferrand le 24 Mai. Il fera un retour d‚Äôexp√©rience concret sur nos pratiques autour de la r√©alisation de workers asynchrones en PHP (et oui !).\n\nLe PHPTour est un cycle de conf√©rences itin√©rant organis√© par l‚ÄôAFUP r√©unissant toutes les communaut√©s PHP, professionnelles et open-source, d√©di√© au langage et √† son √©cosyst√®me. Ne manquez pas cette conf√©rence ainsi que cet √©v√®nement qui s‚Äôannonce particuli√®rement riche !\n\n"
} ,
  
  {
    "title"    : "La retrospective Agile ‚ÄòGarde √† vous‚Äô",
    "category" : "",
    "tags"     : " agile, scrum",
    "url"      : "/2016/03/29/retro-agile-garde-a-vous.html",
    "date"     : "March 29, 2016",
    "excerpt"  : "Depuis quelques ann√©es les √©quipes d‚ÄôM6Web se sont organis√©es autour des m√©thodes agiles. Scrum, Kanban, Lean, m√©thodes adapt√©es, nous nous effor√ßons de toujours garder en t√™te l‚Äôam√©lioration continue et le fun spirit au coeur du travail de nos √©q...",
  "content"  : "Depuis quelques ann√©es les √©quipes d‚ÄôM6Web se sont organis√©es autour des m√©thodes agiles. Scrum, Kanban, Lean, m√©thodes adapt√©es, nous nous effor√ßons de toujours garder en t√™te l‚Äôam√©lioration continue et le fun spirit au coeur du travail de nos √©quipes.\n\nAu del√† des rituels ‚Äúclassiques‚Äù, l‚Äô√©quipe des scrum master cherche de temps en temps √† th√©matiser et casser les routines en cr√©ant des jeux autour de l‚Äôagilit√©.\n\nNous souhaitons aujourd‚Äôhui au travers de ce blog, partager avec vous ces jeux et surtout vous permettre de les reproduire. Ainsi chaque jeu s‚Äôaccompagnera de r√®gles et d‚Äôun ¬´ kit ¬ª vous permettant d‚Äôimprimer le mat√©riel n√©cessaire au bon d√©roulement.\n\nAu menu de ce premier jeu, nous avions choisi de profiter de l‚Äôarriv√©e de l‚Äô√©mission ‚ÄúGarde √† vous‚Äù sur M6 afin de proposer une r√©trospective pas comme les autres.\n\n\n  Nom : La r√©tro Garde √† vous !\n  Type : R√©trospective d‚Äô√©quipe.\n  Dur√©e : 1 heure.\n\n\nPunchLine :\n\nVotre √©quipe est s√©lectionn√©e pour une retro sp√©ciale. Un d√©fi difficile qui les m√®nera √† d√©passer leur limite. \n√âpreuve physique et mentale, il y en aura pour tous. Mais surtout c‚Äôest en groupe qu‚Äôils r√©ussiront les √©preuves. :)\n\n\n\nObjectifs :\n\n\n  Changer de la r√©tro classique : 2 activit√©s sur les 4 sont des mini-jeux.\n  Cr√©er du team building : Le 1er jeu demande confiance et coh√©sion entre les membres de son √©quipe. Et accessoirement c‚Äôest tr√®s fun !\n  Stimuler les √©quipes entre-elles autour d‚Äôune comp√©tition sympa (nos 5 √©quipes ont fait la m√™me r√©tro lors de la rotation).\n  Garder √† l‚Äôesprit l‚Äôam√©lioration continue au travers des 2 activit√©s post-it.\n\n\nPr√©paration / mat√©riel :\n\n\n  Vous trouverez ici un lien vers le kit de la r√©tro garde √† vous.\n  Cr√©er un d√©cor : Mettez vos √©quipes dans l‚Äôambiance et poussez l‚Äôaspect ¬´ jeux de r√¥le ¬ª. \nExemple : filet √† chat, b√¢che de tente Quechua, palissade, affiche militaire, etc..\n  Costume de l‚Äôanimateur : pantalon/veste militaire (demander autour de vous), cravache, casque avec lunette de ski, chemise beige, etc..\n  Pour l‚Äô√©tape #1 : Post-it, feutres pour √©crire, l‚Äôaffiche paperboard ¬´ Motivation ¬ª.\n  Pour l‚Äô√©tape #2 : 4 bandanas ou serviettes pour bander les yeux + gilets fluos de s√©curit√© + un chronom√®tre + un parcours dans vos bureaux.\n  Pour l‚Äô√©tape #3 : Post-it, feutres pour √©crire, l‚Äôaffiche paperboard ¬´ 4 th√®mes d‚Äôam√©lioration ¬ª.\n  Pour l‚Äô√©tape #4 : 2 Nerfs, 4 canettes vides, 4 peluches (ou autre), 4 balles en mousse, une poubelle, tapis de sol.\n  La feuille des scores.\n\n\n\n\nLe d√©roulement :\n\nVolontairement nous n‚Äôavons donn√© aucun d√©tail √† nos √©quipes sur cette r√©tro. \nQuelques jours avant, nous leur avons envoy√© la vid√©o bande annonce sous forme de ¬´ convocation ¬ª. \nLe jour m√™me, ils ont eu la surprise de voir la salle d√©cor√©e et leur scrum master d√©guis√©.\n\n√âtape.1 - Motivation - 10 minutes :\n\nTexte possible : ¬´ Bonjour √©quipe [nom_√©quipe]. Je suis le sergent ‚ÄúBadass‚Äù, on vous a plac√© chez moi aujourd‚Äôhui pour √©valuer votre trouillom√®tre.\n\n\n  ¬´ Cette r√©tro va se d√©rouler en 4 √©tapes. Comme je suis sympa, je ne vous dis rien. √ßa permettra de voir votre capacit√© d‚Äôadaptation.¬ª\n\n\n\n  ¬´  Sachez que nous aurons 2 √©preuves physiques et 2 √©preuves mentales. Lors des √©preuves physiques, nous noterons vos scores afin de  d√©terminer quelle est la meilleure √©quipe du plateau. ¬ª\n\n\n\n  ¬´  √™tes vous pr√™ts ? ¬ª\n\n\nPaperboard #1 : La motivation\n\n\n  ¬´ On va commencer doucement. Prenez vos post-it et vos crayons.¬ª\n\n\n\n  ¬´  Dites moi ce qui vous motive √† vous levez le matin ? pourquoi vous aimez venir bosser ? ¬ª\n\n\n\n  ¬´  Si un aspect du boulot vous ennuie, vous cloue au lit, dites le √©galement.¬ª\n\n\n\n  ¬´  TimeBox : 2 minutes. ¬ª\n\n\nAu bout des 2 minutes : chacun passe au paperboard coller ses post-it et les expliquer.\n\nObjectif du scrum master :\n\n\n  r√©cup√©rer les aspects positifs de l‚Äôenvironnement, du travail de vos √©quipes : ce sont des bases solides √† avoir en t√™te et √† maintenir dans le groupe.\n  r√©cup√©rer les aspects n√©gatifs : √ßa peut √™tre la cantine, la distance des locaux, etc.. m√™me si certains post-it sont difficilement ‚Äòam√©liorables‚Äô c‚Äôest toujours bien de l‚Äôexprimer.\n\n\n√âtape.2  - En avant, Marche ! - 10 minutes\n\nR√®gle du jeu :\n\n\n  On se met par 2. Si vous √™tes un nombre impair, explication plus bas.\n  L‚Äôune des 2 personnes va avoir les yeux band√©s. On l‚Äô√©quipe d‚Äôun gilet fluo de s√©curit√© afin d‚Äô√©viter de lui rentrer dedans‚Ä¶ :p\n  L‚Äôautre personne devra le guider en utilisant les mots : ¬´ avance / recule / √† droite / √† gauche ¬ª.\n  Il est interdit de toucher son co√©quipier pendant la course.\n  \n    Vous √™tes chronom√©tr√©s. Un classement g√©n√©ral sera fait avec les autres √©quipes pour d√©terminer les plus rapides.\n  \n  Faites mettre le bandana ou la serviette. Prenez les guides et montrez leur le parcours.\n  Faites aligner les paires devant la ligne de d√©part.\n  3, 2, 1, Partez. N‚Äôh√©sitez pas √† les encourager ou √† parler fort afin de les stresser ^^.\n\n\nVersion √† 3 : vous aurez 1 guide et 2 personnes avec les yeux band√©s. Les 2 yeux band√©s sont en file indienne.\n\nLa personne derri√®re pose ses mains sur les √©paules sur la personne de devant. Bonne chance :p\n\nFeuille des scores : Notez le temps de chaque paires.\n\n√âtape.3 - les 4 th√®mes - 20/25 minutes\n\nR√®gles :\n\n\n  Nous avons 4 th√®mes affich√©s au paperboard.\n  Pour chaque th√®me, vous pouvez √©crire au maximum 2 post-it positifs &amp;amp; 2 post-it n√©gatifs\n\n\nNous limitons le nombre de post-it pour une question de temps. Libre √† vous d‚Äôajuster.\n\nLes th√®mes sont :\n\n\n  Communication - Dialogue dans l‚Äô√©quipe, avec le PO, les clients finaux, utilisation des mails, etc‚Ä¶\n  Les outils - De d√©veloppement, m√©thode agile, communication, de d√©ploiement/MEP, de testing, etc..\n  R√©activit√© - Lors d‚Äôune demande PO, d‚Äôun incident de production, phase de cadrage avec PO, etc‚Ä¶\n  Leadership - Pr√©sence de votre Lead-Dev / Responsable R&amp;amp;D, √©coute de vos managers, m√©thode d‚Äôorganisation dans le travail, etc‚Ä¶\n\n\nNous laissons 7 minutes d‚Äô√©criture de post-it (√† ajuster selon vous).\nAu bout des 7 minutes : chacun passe au paperboard coller ses post-it et les expliquer.\n\nObjectif du scrum master :\n\n\n  R√©cup√©rer des axes d‚Äôam√©lioration de l‚Äô√©quipe.\n  Conclure sur les aspects positifs et d√©finir les post-it n√©gatifs sur lesquels on cherche √† agir en 1er.\n  √âviter de d√©finir les actions √† mettre en oeuvre pour les post-it n√©gatifs. Faites le en dehors sinon √ßa prendra trop de temps et cassera la dynamique.\n\n\n√âtape.4 - Duck Hunt - 15 minutes\n\nR√®gles : 3 stands sont propos√©s\n\n\n  Chacun choisit un stand. Les participants peuvent faire un essai rapide si c‚Äôest demand√©.\n  Nous vous laissons ajuster la distance entre les cibles et le joueur.\n  Stand 1 - Le grenadier : 4 grenades (balle en mousse), une poubelle =&amp;gt; lancer les grenades dans la poubelle. 4 essais, 1 r√©ussite = 1 point\n  Stand 2 - le chasseur : 4 canettes, un nerf =&amp;gt; toucher les canettes. 4 essais, 1 r√©ussite = 1 point\n  Stand 3 - le sniper : 4 peluches √©l√©phant PHP, un gros nerf =&amp;gt; mode allong√© dans les bois, 4 essais, 1 r√©ussite = 1 point\n\n\nCeci afin de r√©partir les personnes sur plusieurs stands.\n\nFeuille des scores : Notez le score de chacun.\n\n\n\nConclusion :\n\nAfficher le tableau des scores et f√©liciter tout le monde. \nNext step :\n\n\n  Les r√©sultats des √©quipes seront affich√©s le lendemain / fin de journ√©e / autres (√† vous de voir)\n  Les post-it n√©gatifs de l‚Äô√©tape 1 et 3 seront pris en compte par les scrum master qui travailleront avec les personnes ad√©quates pour continuer √† s‚Äôam√©liorer. Vous pouvez ajouter √† votre DSK (Do, Store, Keep) certaines actions.\n\n\nN‚Äôh√©sitez pas √† nous envoyer vos feedbacks sur ce jeu.\n\nForce et Scrum !\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un d√©veloppeur player vid√©o JavaScript (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/2016/01/26/m6web-lyon-recherche-developpeur-player-video-web-h-f-en-cdi.html",
    "date"     : "January 26, 2016",
    "excerpt"  : "Mise jour : Le poste n‚Äôest √† plus pourvoir. Merci\n\nAu sein de la team Tube (√©quipe Lecteur Vid√©o), en charge entre autre du lecteur de 6play et des lecteurs vid√©os des autres portails Internet d‚ÄôM6 Web (Clubic.com, Deco.fr, ‚Ä¶), vous participez √† l...",
  "content"  : "Mise jour : Le poste n‚Äôest √† plus pourvoir. Merci\n\nAu sein de la team Tube (√©quipe Lecteur Vid√©o), en charge entre autre du lecteur de 6play et des lecteurs vid√©os des autres portails Internet d‚ÄôM6 Web (Clubic.com, Deco.fr, ‚Ä¶), vous participez √† la conception technique et au d√©veloppement de nos lecteurs vid√©os.\n\nVous maitrisez les probl√©matiques et les technologies Web :\n\n\n  ECMAScript 2015 ¬´ ES6 ¬ª\n  VideoJS\n  Gulp / Grunt\n  Les outils de tests (Jasmine, qUnit, Jest, PhantomJS, BrowserStack ‚Ä¶)\n\n\nVous avez une bonne connaissance des probl√©matiques vid√©os :\n\n\n  Les nombreux formats en diffusion continue (streaming) mais aussi en t√©l√©chargement progressif (Progressive Download)\n  Les contraintes d‚Äôencodage vid√©o\n  Les probl√©matiques autour de la s√©curit√© (le chiffrement, les DRMs du march√©, ‚Ä¶)\n\n\nPar ailleurs, vous avez d√©j√† eu √† travailler sur l‚Äôint√©gration de formats publicitaires.\n\nEnfin, une connaissance d‚ÄôActionScript sera appr√©ci√©e.\n\nVous aurez des interactions avec les √©quipes Produit de Paris, ainsi qu‚Äôavec nos autres d√©veloppeurs bas√©s √† Lille.\n\nLe profil recherch√© se caract√©rise par :\n\n\n  Une tr√®s forte sensibilit√© sur les sujets Vid√©o et Qualit√© de Service\n  Un go√ªt prononc√© pour l‚Äôinnovation\n  Une bonne culture du web et du monde du num√©rique\n  Une aptitude √† la prise d‚Äôinitiatives, un grand dynamisme, une curiosit√© et une ouverture d‚Äôesprit\n  Une connaissance (id√©alement valid√©e par une premi√®re exp√©rience) des M√©thodes Agiles (Scrum), et une culture de l‚Äôam√©lioration continue\n\n\nPour postuler : https://www.groupem6.fr/ressources-humaines/offres-emploi/developpeur-frontend-javascript-video-h-f-258357.html\n\n"
} ,
  
  {
    "title"    : "On a test√© fonctionnellement notre app JS",
    "category" : "",
    "tags"     : " tests fonctionnels, javascript, phantomjs, webdriver, Cytron",
    "url"      : "/2016/01/25/tests-fonctionnels-app-js.html",
    "date"     : "January 25, 2016",
    "excerpt"  : "L‚Äôutilit√© des tests fonctionnels pour les applications web n‚Äôest plus √† d√©montrer (comment √ßa, vous ne testez pas encore vos apps ?). Malheureusement, tout ne peut pas √™tre totalement test√© fonctionnellement, ou de fa√ßon ais√©e : je pense par exemp...",
  "content"  : "L‚Äôutilit√© des tests fonctionnels pour les applications web n‚Äôest plus √† d√©montrer (comment √ßa, vous ne testez pas encore vos apps ?). Malheureusement, tout ne peut pas √™tre totalement test√© fonctionnellement, ou de fa√ßon ais√©e : je pense par exemple au player chez nous, un composant strat√©gique mais pauvrement test√© fonctionnellement de par sa nature un peu hybride (m√©lange de flash et de JS). Dans tous les cas, pour ce qui peut l‚Äô√™tre, nous sommes partisans dans l‚Äô√©quipe Cytron d‚Äôuser sans mesure (ou presque !) de cet outil de mani√®re √† √™tre le plus zen possible au moment d‚Äôappuyer sur le bouton ‚Äúdeploy‚Äù.\n\nQuelle stack ?\n\nNotre application est cod√©e en JS isomorphique (ou Universal JS) gr√¢ce √† React et Node.js.\n\nPour les tests fonctionnels, nous utilisons le trio Cucumber.js + WebdriverIO + PhantomJS :\n\n\n  Cucumber.js est l‚Äôoutil qui permet de d√©rouler la suite de tests √©crits dans la syntaxe Gherkin,\n  WebdriverIO permet d‚Äôinterfacer les tests traduits en JS avec un serveur Selenium (dialoguant gr√¢ce au protocole WebDriver Wire et permettant de contr√¥ler un browser),\n  PhantomJS est le browser dans lequel les sc√©narios de tests seront ex√©cut√©s, il embarque son propre serveur Webdriver, Ghostdriver.\n\n\nToutes nos Pull Requests lancent les tests ind√©pendamment via Jenkins dans un environnement ‚Äúdockeris√©‚Äù, donc compl√®tement autonome et isol√©. De fa√ßon √† respecter ce principe jusqu‚Äôau bout et √† ne pas d√©pendre de donn√©es versatiles, nos API sont aussi mock√©es gr√¢ce √† superagent-mock.\n\nSetup\n\nArborescence\nDans notre projet, nous avons un dossier pour les tests fonctionnels organis√©s comme suit :\n\n‚îú‚îÄ‚îê tests\n‚îÇ ‚îú‚îÄ‚îê step_definitions\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ my_feature.steps.js\n‚îÇ ‚îú‚îÄ‚îê screenshots\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ my_scenario.png\n‚îÇ ‚îú‚îÄ‚îê support\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config.json\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ constants.json\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ hooks.js\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ world.js\n‚îÇ ‚îî‚îÄ‚îÄ my_feature.feature\n\nFeatures\nUne feature est un fichier testant une fonctionnalit√© de l‚Äôapplication et regroupant plusieurs sc√©narios de test. Il est √©crit en langage naturel (Gherkin) de fa√ßon √† √™tre lisible par tous.\n\n# tests/support/cookie.feature\nFeature: Scenarios about the cookie banner\n\n  Scenario: See the cookie banner and close it\n    Given My browser storage is empty\n    When I visit the &quot;homepage&quot; page\n    Then I should see the &quot;cookie banner&quot;\n\n    When I click on &quot;Accept cookie&quot;\n    Then I should not see a &quot;cookie banner&quot;\n\n    When I visit the &quot;homepage&quot; page\n    Then I should not see a &quot;cookie banner&quot;\n\nWorld\nLe fichier world.js est le point de d√©part pour Cucumber.js. C‚Äôest ici que nous initialisons WebdriverIO et que nous mettons un place un contexte qui sera disponible pour tous les tests.\n\n// tests/support/world.js\nvar Webdriver = require(&#39;webdriverio&#39;);\nvar config = require(&#39;./config.json&#39;);\nvar assert = require(&#39;assert&#39;);\n\nvar browser = Webdriver.remote({\n  logLevel: config.logLevel || &#39;silent&#39;,\n  host: config.webdriver.host,\n  port: config.webdriver.port,\n  waitforTimeout: config.waitTimeout,\n  desiredCapabilities: {browserName: &#39;phantomjs&#39;}\n});\n\nfunction WorldConstructor() {\n  var world = {\n    browser: browser,\n\n    // Global visit method\n    visit: function (baseUrl, params) {\n      var pathUrl = url.format({\n        pathname: baseUrl,\n        query: params\n      });\n\n      return this.browser.url(pathUrl);\n    },\n\n    // Take screenshot\n    screenshot: function (filename) {\n      return browser.saveScreenshot(path.join(config.screenshot.path, filename));\n    },\n\n    assert: {\n      /**\n       * Assert if element(s) are visible\n       *\n       * @param selector    {String}   Can be query multiple DOM elements\n       * @param failMessage {String}   Fail message if no visible\n       */\n      visible: function (selector, failMessage) {\n        // ...\n      },\n    }\n\n    // ...\n  }\n\n  return world;\n}\n\nmodule.exports = WorldConstructor;\n\nHooks\nCucumber.js permet de d√©clencher des traitements sur certains √©v√®nements cl√©s lors de l‚Äôex√©cution de la suite de tests. Nous utilisons ce syst√®me pour r√©aliser une capture d‚Äô√©cran sur chaque sc√©nario de test en √©chec qui viendra s‚Äôajouter dans le dossier screenshots.\n\n// tests/support/hook.js\nvar config = require(&#39;./config.json&#39;);\nvar sprintf = require(&#39;sprintf-js&#39;).sprintf;\n\nmodule.exports = function () {\n  this.Before(function (scenario) {\n    return this.browser.init().then(function () {\n      return this.browser.setViewportSize({\n        width: config.screenshot.width,\n        height: config.screenshot.height\n      });\n    }.bind(this));\n  });\n\n  this.After(function (scenario) {\n    if (scenario.isFailed()) {\n      return this.screenshot(sprintf(\n        &#39;%s_%d.png&#39;,\n        scenario.getName().toLowerCase().replace(&#39; &#39;, &#39;-&#39;),\n        new Date().getTime()\n      )).then(function () {\n        return this.browser.end();\n      }.bind(this));\n    } else {\n      return this.browser.end();\n    }\n  });\n};\n\nStep definitions\nCe sont les fichiers qui font le lien entre les features (√©crit en langage naturel) et WebdriverIO (initialis√© dans world.js).\n\n// tests/step_definitions/cookie.steps.js\nvar sprintf = require(&#39;sprintf-js&#39;).sprintf;\n\nmodule.exports = function () {\n  /**\n   * Visit a page\n   *\n   * @param page {String}\n   *\n   * @require config routes object\n   */\n  this.When(/^I visit the &quot;([^&quot;]*)&quot; page$/, function (page) {    \n    return this.visit(this.getRoute(page)).then(function () {\n      return this.assert.existing(&#39;#__main&#39;, &#39;React application is not loaded.&#39;);\n    }.bind(this));\n  });\n\n  /**\n   * I click on &quot;label&quot;\n   *\n   * @param label {String}   DOM selector label\n   */\n  this.When(/^I click on &quot;([^&quot;]*)&quot;$/, function (label) {\n    var selector = this.getDOMSelector(label);\n    \n    return this.action.click(selector);\n  });\n\n  /**\n   * Assert element matching the given selector is visible.\n   *\n   * @param label {String}\n   *\n   * @require config DOMSelectors object\n   */\n  this.Then(/^I should see a &quot;([^&quot;]*)&quot;$/, function (label) {\n    var selector = this.getDOMSelector(label);\n    var failMessage = sprintf(&#39;%s is not visible&#39;, label);\n    \n    return this.assert.visible(selector, failMessage);\n  });\n\n  // ...\n}\n\nDesign\nNous n‚Äôavons pas mis en ≈ìuvre le pattern Page Object. Ce n‚Äô√©tait pas un choix d√©lib√©r√© mais le contexte et les enjeux du projet nous ont fait passer √† c√¥t√©, ou ce n‚Äô√©tait peut √™tre simplement pas le moment. Malgr√© tout, nous avons tent√© de rationaliser au mieux l‚Äôorganisation du code. Par exemple, afin de ne pas se retrouver avec des s√©lecteurs CSS √©parpill√©s dans plusieurs fichiers de ‚Äúfeatures‚Äù ou de ‚Äústep definitions‚Äù, nous avons choisi de les regrouper dans un fichier constants.json et d‚Äôutiliser seulement des labels ailleurs. Nous faisons le lien entre le label et le s√©lecteur CSS avec la m√©thode getDOMSelector, visible ci-dessus et d√©finie dans le fichier world.js.\n\nRun\nPour lancer les tests, il faut :\n\n\n  lancer le serveur de l‚Äôapp en local (l‚ÄôURL du serveur est param√©trable dans le fichier de config),\n  lancer un phantomjs en mode webdriver phantomjs --webdriver=5024 o√π 5024 est le port du serveur (√©galement configurable dans config.json),\n  lancer une suite de tests via Cucumberjs, au choix :\n    \n      tous les tests cucumberjs tests/,\n      une feature cucumberjs tests/cookie.feature,\n      un sc√©nario cucumberjs tests/cookie.feature:3 o√π 3 correspond √† la ligne du d√©but du sc√©nario cibl√© dans le fichier cookie.feature.\n    \n  \n\n\nParticularit√© de l‚Äôisomorphisme\n\nDeux chemins sont possibles avec l‚Äôisomorphisme. Soit l‚Äôutilisateur arrive directement sur la page, auquel cas celle-ci sera g√©n√©r√©e sur le serveur, soit il y arrive en naviguant sur l‚Äôapp et c‚Äôest le client qui aura ex√©cut√© le code. Il faut tester ces deux cas car le code concern√© n‚Äôest pas toujours le m√™me (la variable window par exemple n‚Äôest pas accessible c√¥t√© serveur).\n\nIl est bien s√ªr impossible d‚Äô√™tre exhaustif. L‚Äôid√©e est d‚Äôabord de couvrir les cas les plus fr√©quents et les plus critiques pour l‚Äôapplication. Ensuite, il faut s‚Äôastreindre √† ajouter un test √† chaque fois qu‚Äôun bug est d√©tect√© de fa√ßon √† s‚Äôassurer qu‚Äôon ne le rencontrera plus dans le futur.\n\nPhantomJS, la stabilit√© en question‚Ä¶\n\nBas√© sur Webkit, PhantomJS est le plus connu des navigateurs headless, c‚Äôest-√†-dire ex√©cutables sans interface visuelle. D‚Äôautres navigateurs l√©gers et cr√©√©s pour les tests fonctionnels existent comme SlimerJS (bas√© sur Gecko et pas vraiment headless) ou Zombie.js (pas de moteur de rendu). Cependant aucun n‚Äôoffre toutes les fonctionnalit√©s de PhantomJS qui se rapprochent le plus d‚Äôun vrai browser. Il √©mule de fa√ßon transparente tout le rendu graphique avec la possibilit√© de r√©aliser des screenshots par exemple ou de tester la visibilit√© d‚Äôun √©l√©ment du DOM (non opaque, dans le viewport, sur la couche z-index la plus haute‚Ä¶).\n\nN√©anmoins celui-ci n‚Äôint√®gre pas toutes les derni√®res avanc√©es en terme de JS et de CSS. Flexbox n‚Äôest par exemple pas pris en charge ce qui nous a pos√© quelques probl√®mes sur les v√©rifications li√©es √† la visibilit√© des √©l√©ments. Sa version 2.0 qui date de d√©but 2015, malgr√© la bonne volont√© des contributeurs, n‚Äôa toujours pas de build officiel sous Linux, ce qui oblige √† compiler les sources sur sa machine de tests ou √† trouver sur le net un build officieux correspondant √† sa distribution. C‚Äôest ce que nous avons fait via M6Web/phantomjs2. Cependant, l‚Äôoutil est assez instable (builds officiels ou pas) et nous avons rencontr√© beaucoup de crashs al√©atoires ou reproductibles mais incompr√©hensibles (dus par exemple √† l‚Äôajout de quelques lignes de CSS anodines‚Ä¶).\n\nEn local, sur sa machine, PhantomJS est encore moins stable que sur Jenkins. Il semblerait qu‚Äôex√©cution apr√®s ex√©cution, il garde des ‚Äúchoses‚Äù en cache quelque part qui, √† terme, produisent des crashs syst√©matiques de l‚Äôoutil. Nous n‚Äôavons pas r√©ussi √† √©tablir un sc√©nario reproductible qui nous permette de poser une issue sur le projet. N‚Äôh√©sitez pas √† r√©agir en commentaire si vous vous √™tes trouv√© dans un cas similaire.\n\nPour r√©gler temporairement ce probl√®me, nous avons utilis√© l‚Äôimage docker de Gabe Rosenhouse pour le faire tourner dans un environnement ind√©pendant mais ce n‚Äôest pas faciliter la vie des d√©veloppeurs qui veulent juste lancer des tests sans avoir √† mettre en ≈ìuvre une usine √† gaz derri√®re.\n\nEdit: hier, la version 2.1 de PhantomJS a (enfin) √©t√© publi√©e avec un build pour chaque plateforme. Plusieurs de nos soucis pourraient √™tre r√©gl√©s avec cette nouvelle release, √† suivre‚Ä¶\n\nChrome+ChromeDriver, une alternative ?\n\nNous avons alors opt√© pour la solution Chrome+ChromeDriver. ChromeDriver a le r√¥le du serveur Selenium qui permet de faire communiquer WebriverIO avec Chrome. Les avantages de cette stack sont multiples. D‚Äôabord, l‚Äôensemble est beaucoup plus stable, fini les crashs impromptus. Ensuite, le debug des tests en √©chec est bien plus ais√© : on voit en effet la suite se jouer en temps r√©el dans son navigateur, on peut ainsi tout √† fait mettre un point d‚Äôarr√™t et utiliser la console de d√©veloppement. Enfin, on utilise la version de Chrome que l‚Äôon souhaite, donc plus de probl√®me de CSS non support√©s.\n\nAlors pourquoi se cantonner √† n‚Äôutiliser Chrome+ChromeDriver qu‚Äôen local et pas en int√©gration continue sur Jenkins ? Chrome n‚Äôest pas un browser headless et a besoin d‚Äôune interface visuelle qui n‚Äôest pas disponible sur Jenkins. Il existe des solutions pour simuler un affichage graphique avec Xvfb par exemple. Nous avons tent√© de mettre en place une telle stack sur l‚Äôimage docker utilis√©e pour cr√©er notre environnement de test sur Jenkins en se basant sur l‚Äôimage de Rob Cherry. Malheureusement, apr√®s y avoir consacr√© un peu d‚Äô√©nergie, le r√©sultat n‚Äôa pas √©t√© au rendez-vous car :\n\n\n  l‚Äôex√©cution des tests dans Chrome est bien plus lente que sur PhantomJS (2 √† 3 fois plus lent), notre int√©gration continue prenant d√©j√† plus de 10 minutes sur ce projet,\n  il semble difficile d‚Äôobtenir ici aussi une stabilit√© du dispositif, les sessions Webdriver √©taient souvent perdues, sans que nous en trouvions la cause.\n\n\nCes raisons nous ont conduit √† abandonner cette piste.\n\nQuelques tips pour am√©liorer la stabilit√© de ses tests\n\nNous avons continu√© d‚Äôesp√©rer avoir une stack stable pour nos tests fonctionnels. Avec pers√©v√©rance, nous pouvons dire qu‚Äô√† l‚Äôheure actuelle gr√¢ce √† ces quelques tips, nous avons une plateforme de test stable (√† 99%) !\n\nwaitUntil\nC‚Äôest la premi√®re chose √† faire et la plus importante de notre point de vue. On ne sait jamais vraiment quand un √©l√©ment s‚Äôaffichera dans la page car son chargement d√©pend de trop de facteurs non pr√©dictibles (la connexion, l‚Äôutilisation cpu, gpu, m√©moire, etc.). Sur notre projet, nous avons par exemple beaucoup d‚Äôanimations CSS qui retardent le timing d‚Äôapparition des pages et des √©l√©ments du DOM. Notre premi√®re approche a √©t√© de rajouter des sleep un peu de partout dans nos tests. Chose √† ne pas faire. L‚Äôusage des sleep doit √™tre cantonn√© √† des cas tr√®s sp√©cifiques. Pour tout le reste, il faut user et abuser du waitUntil de WebdriverIO, que ce soit pour des actions ou des v√©rifications dans la page, et en adaptant le timeout √† votre projet (certaines de nos animations sont assez longues).\n\nrollover\nUn autre probl√®me que nous avons rencontr√© est la bonne ex√©cution des rollovers. En utilisant la m√©thode moveToObject pour pointer la souris sur un √©l√©ment, il nous arrivait que le comportement ‚Äúhover‚Äù ne soit pas d√©clench√©, mettant en √©chec la suite du test. Nous avons donc chang√© notre mani√®re d‚Äôeffectuer le rollover : on r√©p√®te l‚Äôaction gr√¢ce au waitUntil tant que l‚Äô√©lement devant appara√Ætre au hover n‚Äôest pas visible.\n\nNous n‚Äô√©crivons plus\n\nI rollover the &quot;Header login icon&quot;\n\nmais\n\nI rollover the &quot;Header login icon&quot; to make &quot;Submenu&quot; appear\n\nrerun\n‚ÄúRerun‚Äù est une fonctionnalit√© existante sur d‚Äôautres frameworks de tests fonctionnels tel que Behat et cr√©√©e pour les tests r√©calcitrants encore instables. Elle permet de stocker dans un fichier texte la liste des sc√©narios en √©chec pour les relancer ensuite afin de v√©rifier qu‚Äôils le sont r√©ellement. Nous avons mis en place ce process sur Jenkins, bien qu‚Äôil y ait quelques subtilit√©s qui ne facilitent pas la t√¢che (mais qui devraient √™tre bient√¥t corrig√©es), et nous en sommes satisfaits.\n\nisVisible\nA nos d√©buts, nous avons eu quelques probl√®mes avec la fonction isVisible de WebdriverIO car les √©l√©ments opaques ou en dehors du viewport √©taient consid√©r√©s comme visibles. Nous avons alors choisi d‚Äôutiliser une fonction custom inject√©e via execute. R√©cemment, dans la version 3 de WebdriverIO, la fonction isVisibleWithinViewport a fait son apparition mais nous n‚Äôavons pas encore tent√© de l‚Äôutiliser dans nos tests.\n\nCet article est un retour d‚Äôexp√©rience sur notre usage des tests fonctionnels sur un projet pr√©cis mais il est loin d‚Äôexposer des v√©rit√©s absolues. Si vous avez des remarques ou n‚Äô√™tes pas d‚Äôaccord avec certaines choses, n‚Äôh√©sitez pas √† nous le faire savoir !\n"
} ,
  
  {
    "title"    : "L&#39;envers du d√©cor du nouveau 6play",
    "category" : "",
    "tags"     : " 6play, REST, Symfony, Elasticsearch, Cassandra",
    "url"      : "/2015/11/30/beta-nouveau-6play-backend.html",
    "date"     : "November 30, 2015",
    "excerpt"  : "Il y a quelques semaines, nous vous parlions ici m√™me de la stack technique mise en place pour le nouveau front web de 6play.\n\nAujourd‚Äôhui, nous vous proposons un retour sur ce qui a √©t√© mis en place c√¥t√© backend pour assurer la mise √† disposition...",
  "content"  : "Il y a quelques semaines, nous vous parlions ici m√™me de la stack technique mise en place pour le nouveau front web de 6play.\n\nAujourd‚Äôhui, nous vous proposons un retour sur ce qui a √©t√© mis en place c√¥t√© backend pour assurer la mise √† disposition des donn√©es aux diff√©rents frontaux 6play.\n\nTout d‚Äôabord, il faut commencer par expliquer que l‚Äôunivers 6play ne se r√©sume pas que √† son application web. Il existe aussi une version iOS et Android, mais √©galement une version par Box IPTV (disons une version par FAI).\n\nPas mal de REST ‚Ä¶\n\nC‚Äôest donc tout naturellement que nous sommes partis sur la mise √† disposition d‚Äôune API REST permettant √† ces diff√©rents fronts de consommer simplement les donn√©es.\n\nNotre stack technique habituelle c√¥t√© backend √©tant Symfony2, nous sommes donc partis sur ce framework, ainsi que les habituels bundles :\n\n\n  FOSRestBundle pour la gestion simple des controlleurs REST (validation des param√®tres, routing adapt√©, view au format JSON, gestion des retours d‚Äôerreur)\n  BazingaHateoasBundle pour int√©grer les liens entres les diff√©rents endpoints directement dans les diff√©rentes r√©ponses.\n  NelmioApiDocBundle pour proposer une documentation compl√®te et auto-g√©n√©r√©e depuis le code\n\n\nPour s√©curiser tout √ßa, nous utilisons toujours notre bundle DomainUserBundle permettant de s√©curiser et contextualiser les donn√©es par sous-domaine (voir notre article d√©di√© √† ce bundle).\n\n‚Ä¶ mais pas que\n\nUne fois mise en place la th√©orie brute, nous nous sommes heurt√©s √† la r√©alit√© des choses : face √† un mod√®le de donn√©es complexe, si on reste tr√®s strict face √† la philosophie RESTful, cela peux demander aux clients de r√©aliser un nombre cons√©quent de requ√™tes afin d‚Äôafficher une simple page.\n\nAinsi, nous avons un second applicatif, que nous nommons ‚Äúmiddleware‚Äù qui est un hybride entre une API REST et un catalogue de donn√©es pr√©format√©. Dans cet applicatif, nous r√©alisons les agr√©gations qui permettent de r√©cup√©rer de mani√®re unifi√©e les donn√©es li√©es, permettant aux frontaux de r√©duire leurs appels.\n\nDans ce middleware, nous essayons tout de m√™me de respecter au maximum les verbes HTTP et le format de retour pour que les utilisateurs de ces API obtiennent des r√©ponses coh√©rentes d‚Äôun service sur l‚Äôautre.\n\nDes donn√©es √©lastiques\n\nPour que ce middleware puisse retourner des donn√©es qui sont stock√©es dans plusieurs tables, de mani√®re rapide, tout en g√©rant les contraintes de donn√©es non publi√©es (notre SI contient les anciennes √©missions diffus√©es, mais √©galement celles √† diffuser), nous avons fait le choix d‚Äôutiliser Elasticsearch en le remplissant avec les donn√©es ‚Äúpubliables‚Äù.\n\nNon seulement nous disposons d‚Äôun syst√®me de recherche de donn√©es tr√®s performant, permettant des requ√™tes tr√®s puissantes et tr√®s rapides, dans lequel les donn√©es sont stock√©es de mani√®re optimis√©e pour l‚Äôutilisation (pas de forme normale √† respecter), mais nous nous permettons de n‚Äôy stocker que les donn√©es disponibles publiquement, simplifiant donc grandement les requ√™tes sur ces donn√©es.\n\nWorkerize all the things\n\nPour maintenir les donn√©es √† jour dans cet index Elasticsearch, nous avons mutualis√© sur l‚Äôexp√©rience et le travail que nous avions r√©alis√© pour RisingStar, qui nous a apport√© l‚Äôexp√©rience que des daemons sont beaucoup plus efficaces que des crons. Cette technique nous apporte plusieurs avantages :\n\n\n  Scalabilit√© : il est facilement possible de multiplier les process qui traitent les donn√©es, et donc d‚Äôaugmenter la capacit√© de traitement\n  Rapidit√© : le fait d‚Äôavoir des daemons qui tournent en continue permet de traiter les demandes d√®s leur arriv√©e, et pas lors de la minute suivante. Cela permet aussi de lisser au maximum les traitements sans cr√©er de piles d‚Äôattente inutiles.\n\n\nNous nous sommes donc appuy√©s sur notre DaemonBundle pour mettre en place un double syst√®me d‚Äôindexation :\n\n\n  une fois par jour, l‚Äôindex est compl√©tement reconstruit\n  un daemon tourne en continue pour d√©tecter les modifications en base de donn√©es, et envoyer des messages dans une file RabbitMQ\n  un dernier daemon est d√©di√© au traitement des messages de cette file pour mettre √† jour de mani√®re cibl√©e les donn√©es dans Elasticsearch\n\n\nAinsi, nous assurons une fraicheur des donn√©es quasi-imm√©diate et optimale.\n\nAu cours de ce travail, nous avons construit 2 nouveaux bundle : ElasticsearchBundle et AmqpBundle. L‚Äôun comme l‚Äôautre sont des bundles permettant de faciliter la configuration et l‚Äôutilisation des clients natifs dans Symfony2, en tant que service.\n\nEt la grosse donn√©e ?\n\nSi vous avez essay√© la nouvelle version web de 6play, vous avez certainement remarqu√© que la personnalisation de votre compte est fortement mise en avant. Pour stocker ce fort volume de donn√©es, nous avons fait le choix d‚Äôutiliser Cassandra, pour son approche distribu√©e permettant une forte scalabilit√©, et un ratio rapidit√©/redondance optimal.\n\nComme pour le reste, nous avons l√† aussi cr√©√© un bundle Symfony2 permettant de configurer et manipuler simplement des clients Cassandra en tant que service : CassandraBundle\n\nTout le reste\n\nC√¥t√© monitoring, pour respecter nos bonnes habitudes, nous utilisons toujours Statsd √† outrance, surtout via notre bundle StatsdBundle.\n\nC√¥t√© tests, tous les tests unitaires ont √©t√© √©crits avec atoum.\n\nConclusion\n\nAu cours de ce projet, nous avons eu l‚Äôoccasion de transformer l‚Äôessai de beaucoup de choses que nous avions faites pour RisingStar, de d√©couvrir de nouvelles technos et de mettre en place une architecture moderne et adapt√©e aux nouveaux challenges des fronts.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #9",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2015/11/06/m6web-dev-facts-9.html",
    "date"     : "November 6, 2015",
    "excerpt"  : "C‚Äôest le digital\n\n\n  On a le doigt quelque part ‚Ä¶\n\n\nUne vague histoire de pouce\n\n\n  Sache que sans ton pouce, je ne fais rien. (et non, c‚Äôest pas d√©gueulasse)\n\n\nJe croyais que √ßa rendait sourd plut√¥t\n\n\n  N‚Äôempeche je me trompe toujours quand je li...",
  "content"  : "C‚Äôest le digital\n\n\n  On a le doigt quelque part ‚Ä¶\n\n\nUne vague histoire de pouce\n\n\n  Sache que sans ton pouce, je ne fais rien. (et non, c‚Äôest pas d√©gueulasse)\n\n\nJe croyais que √ßa rendait sourd plut√¥t\n\n\n  N‚Äôempeche je me trompe toujours quand je lis pom.xml hein\n\n\nDyslexie\n\n\n  \n    X : Il parait que Marven n‚Äôarrive pas √† h√©riter de la config de java par d√©faut ¬∞¬∞\n    Y : marven ? connais pas\n    Z : c‚Äôest l‚Äô√©quivalent Java de comrposer :)\n  \n\n\nR√©gime Fifa\n\n\n  Mince, j‚Äôai oubli√© de finir de manger ‚Ä¶\n\n\nLa faille\n\n\n  Le plus long dans cette MEP, √ßa va √™tre de se connecter au Wifi\n\n\nSoif\n\n\n  \n    X : Rappel important : on a 12 litres de ros√© √† boire avant vendredi prochain (et un pack de heinekein).\n    X : si on ne s‚Äôy prend pas en avance √ßa va √™tre la panique\n  \n\n\nLa prod de la dev ?\n\n\n  Est ce qu‚Äôon a la base de donn√©es de dev de prod ?\n\n\nError or not error\n\n\n  \n    X : c‚Äôest parce que tu pars du principe que le message d‚Äôerreur a un rapport avec l‚Äôerreur\n    Y : euh ce qui devrait √™tre le cas non ?\n    X : ouais, mais bon‚Ä¶\n  \n\n\nC‚Äô√©tait un message digital\n\n\n  \n    X : pour les massage d‚Äôerreur\n    Y : Les massages d‚Äôerreur ?\n    Z : humm les massages\n    Y : La premi√®re partie me tente, mais d‚Äôerreur ?\n    X : un laius digital\n    Y : oula ‚Ä¶ massage ‚Ä¶ digital\n  \n\n\nEn Famille\n\n\n  Les mecs ils regardent leur code ! On dirait que c‚Äôest leur photo de famille\n\n\nDeployception\n\n\n  Pour tester le deployer, il faut le deployer en test, mais pour √ßa, il faut deployer le deployer en prod pour pouvoir deployer en test\n\n\nFormalisme\n\n\n  En tant qu‚Äôordinateur ayant acc√®s √† l‚ÄôINTERNET MONDIAL et voulant naviguer sur le site internet de M6 se nommant 6PALY.FR mettre un titre dans la balise &amp;lt;title&amp;gt; ‚Ä¶ merci\n\n\nLes gouts et les couleurs\n\n\n  Moi, je ne fais pas confiance aux technos qui ont moins de 15 ans. Sauf si elles viennent du Br√©sil\n\n\nL‚Äôheure est √† la blague\n\n\n  C‚Äôest l‚Äôheure dammer ‚Ä¶\n\n\nUne v√©rit√© vrai\n\n\n  Quand on dit ‚Äú24h‚Äù g√©n√©ralement c‚Äôest 24h\n\n\nAuto-troll\n\n\n  J‚Äôai cr√©√© un fichier ‚ÄúechoUrl‚Äù qui √©crit dans un fichier ‚Ä¶\n\n\nIncoh√©rence coh√©rente\n\n\n  Comme √ßa, on est coh√©rent dans l‚Äôincoh√©rence !\n\n"
} ,
  
  {
    "title"    : "La b√™ta du nouveau 6play est disponible",
    "category" : "",
    "tags"     : " 6play, react, isomorphic, javascript, flux",
    "url"      : "/2015/10/21/beta-nouveau-6play-react-isomorphic.html",
    "date"     : "October 21, 2015",
    "excerpt"  : "Nous vous parlions en fin d‚Äôann√©e derni√®re sur ce blog, de notre vision de la Single Page App parfaite.\n\nNous avons donc travaill√© depuis le d√©but d‚Äôann√©e √† la mise en place du nouveau 6play sur cette stack technologique :\n\n\n  React(isomorphic/uni...",
  "content"  : "Nous vous parlions en fin d‚Äôann√©e derni√®re sur ce blog, de notre vision de la Single Page App parfaite.\n\nNous avons donc travaill√© depuis le d√©but d‚Äôann√©e √† la mise en place du nouveau 6play sur cette stack technologique :\n\n\n  React(isomorphic/universal) avec du Node.js en backend\n  Fluxible pour la gestion de Flux client et serveur\n  webpack pour la gestion du build js cot√© client\n  React Router pour le routing\n  ES6 avec Babel parce que.\n\n\nAu niveau tests, et parce que nous ne concevons plus de d√©velopper de tels projets sans une approche qualit√© compl√®te :\n\n\n  ESLint pour le respect des conventions de codage\n  Jest pour les tests unitaires\n  Cucumber.js, WebdriverIO et PhantomJS pour les tests fonctionnels\n  superagent-mock (\\o/) pour mocker les requ√™tes HTTP des services externes\n  Jenkins pour l‚Äôint√©gration continue\n  React Hot Loader pour am√©liorer la DX (Developer eXperience)\n\n\nDepuis lundi, vous pouvez d√©sormais tester la b√™ta de ce service vid√©o √† l‚Äôadresse suivante : https://beta.6play.fr\n\n\n\nPour ceux qui veulent en savoir plus sur cette refonte (notamment front-end), une conf√©rence sera tenue par Kenny Dits (@kenny_dee) lors de Blend Web Mix, le 29 octobre √† Lyon √† 16h.\n"
} ,
  
  {
    "title"    : "Simplifiez vous la vie avec les hooks Git",
    "category" : "",
    "tags"     : " git, hooks, workflow, composer, coke",
    "url"      : "/2015/09/09/hooks-git.html",
    "date"     : "September 9, 2015",
    "excerpt"  : "La stack de base de tout projet un minimum s√©rieux √† tendance √† devenir de plus en plus lourde.\n\nChez M6Web, et particuli√®rement dans la team Burton, nous d√©veloppons principalement des projets Symfony2.\nCette stack est donc compos√©e, entre autre ...",
  "content"  : "La stack de base de tout projet un minimum s√©rieux √† tendance √† devenir de plus en plus lourde.\n\nChez M6Web, et particuli√®rement dans la team Burton, nous d√©veloppons principalement des projets Symfony2.\nCette stack est donc compos√©e, entre autre de coke et de composer.\n\nConcernant coke, l‚Äôid√©e est de ne jamais versionner de code qui ne respecte pas les standards de d√©veloppements.\nInterdit les commits ‚Äúfix standards‚Äù ou autre ‚Äúfix coke‚Äù qui alimentent une PR avant d‚Äô√™tre rebas√©s !\n\nConcernant composer, il s‚Äôagit de toujours travailler sur la ‚Äúbonne version‚Äù des d√©pendances. \nCertes, composer (via le composer.lock) permet d‚Äô√™tre s√ªr que toute personne qui lance un composer install aura la m√™me version des d√©pendances.\nToutefois, il faut encore penser √† lancer cette commande, surtout lorsque l‚Äôon r√©cup√®re du code depuis le repository central o√π le composer.lock peut avoir √©volu√©.\n\nPour r√©pondre √† ces besoins, nous nous sommes bas√©s sur l‚Äôexcellent syst√®me de hooks de git et nous avons d√©velopp√© 2 petits scripts.\n\ncheck-coke.sh\n\nCoke est un petit utilitaire qui facilite le lancement de PHP_CodeSniffer sur son projet.\nToutefois, ce dernier est relativement lent, surtout sur les projets compos√©s d‚Äôun grand nombre de fichiers.\nPour optimiser son ex√©cution, le script ‚Äúcheck-code.sh‚Äù se charge de lancer coke uniquement sur les fichiers en cours de modification d‚Äôun point de vue Git.\nAinsi, son ex√©cution est extr√™ment rapide, ce qui permet de l‚Äôex√©cuter en pre-commit pour ne jamais commiter un code ne respectant pas les standards de d√©veloppement\n\ncheck-composer\n\nComme indiqu√© plus haut, le soucis avec composer est qu‚Äôil faut penser √† lancer la commande composer install pour s‚Äôassurer que les d√©pendances sont √† jour.\nNon seulement cette commande prend un certains temps, mais il faut surtout penser √† la lancer, m√™me pour des actions qui semblent anodines, comme changer de branche.\n\nPartant de ce constat, nous avons cr√©√© le script ‚Äúcheck-composer.sh‚Äù, qui v√©rifie s‚Äôil y a une diff√©rence entre la version de d√©part et d‚Äôarriv√©e du fichier composer.lock, et qui lance la commande composer install si n√©cessaire.\n\nN‚Äôh√©sitez pas √† les essayer et nous faire part de vos retours, voir de proposer vos hooks.\nLe but de ce repository partag√© est de nous simplifier la vie en nous permettant de ne plus penser aux outils qui sont autour de notre code, mais de nous concentrer sur ce que nous avons √† faire.\n"
} ,
  
  {
    "title"    : "CR React Europe Conf√©rence 2015 - Day 2",
    "category" : "",
    "tags"     : " javascript, react, reactnative, video, graphql",
    "url"      : "/2015/07/10/cr-react-europe-2015-day-two.html",
    "date"     : "July 10, 2015",
    "excerpt"  : "Apr√®s une premi√®re journ√©e pleine de nouveaut√©s et d‚Äôannonces, voici la suite du compte rendu avec un programme encore tr√®s charg√© pour cette deuxi√®me journ√©e de la React Europe.\n\nImproving Your Workflow With Code Transformation\n\n\n(cr√©dits : Fabie...",
  "content"  : "Apr√®s une premi√®re journ√©e pleine de nouveaut√©s et d‚Äôannonces, voici la suite du compte rendu avec un programme encore tr√®s charg√© pour cette deuxi√®me journ√©e de la React Europe.\n\nImproving Your Workflow With Code Transformation\n\n\n(cr√©dits : Fabien Champigny - React Europe)\n\nNous commen√ßons la journ√©e avec le cr√©ateur du fameux Babel: Sebastian McKenzie.\nBabel est un transpiler JS permettant de transformer le code ES6/7 en code ES5.\nApr√®s un petit historique sur le nom, car cet outil s‚Äôappelait 6to5 avant l‚Äôarriv√©e d‚ÄôES7, pour finalement se renommer Babel :)\nL‚Äôadoption par la communaut√© a ensuite √©t√© assez massive !\n\nS√©bastian nous explique le fonctionnement interne de Babel avec le d√©coupage en 3 sections : Parser / Transformer / Generator\nIls utilisent l‚ÄôAST (abstract syntax tree) pour avoir une ‚Äúdata structure‚Äù du code et pouvoir faire des traitements sur cette structure.\nOn rentre tr√®s (trop ?) en profondeur dans les bas-fond de Babel, afin de partager les diff√©rentes difficult√©s et trucs et astuces pour les transformations que Babel r√©alise.\n\nLe talk se finit sur le futur de Babel, qui sera √† chercher du c√¥t√© de :\n\n\n  Dead code elimination/minification\n  constant folding/static evaluation\n  static analysis / linting\n\n\nSteven Lusher, l‚Äôun des dev Facebook travaillant sur Relay vient de mettre un blog post sur le site Babel concernant l‚Äôutilisation ES6 de React\n\n\n\nThe State of Animation in React\n\n\n(cr√©dits : Fabien Champigny - React Europe)\n\n\n\nCheng Lou fait le point sur les animations en React et nous pr√©sente sa nouvelle approche react-motion.\n\nIl est convaincu qu‚Äôil faut abandonner les ReactCssTransitionGroup au profit des animations bas√©es sur des interpolations ‚Äú√†-la-flash‚Äù.\n\nCSS Transitions\nLes Transitions CSS pr√©sentent plusieurs d√©faut, elles sont difficiles √† controler et elles sont √©troitement li√©es au DOM. En revanche, elles sont plut√¥t performantes, non bloquantes et r√©pondent √† la plupart des usages.\n\nDeclarative Tweens\nLes Declarative Tweens sont une solution alternative interessante qui permettent de composer une animation selon des crit√®res pr√©cis (d√©but, dur√©e, direction, ‚Ä¶). Cette solution pr√©sente aussi l‚Äôavantage de pouvoir cr√©er des animations compos√©es de plusieurs sous-animations et d‚Äôinterrompre leur execution sur demande.\n\nSpring\nCheng Lou a d√©velopp√© une librairie appel√©e &amp;lt;Spring /&amp;gt; qui parlera aux anciens Flasheurs, tout comme lui, puisqu‚Äôelle reprend le principe d‚Äôinterpolation et de courbes. Elle permet de d√©finir une animation sur un composant React, en pr√©cisant sur chacune de ses propri√©t√©s des crit√®res simples de transformation. La librairie se charge d‚Äôinterpoler la structure du composant pour une animation fluide.\n\nd√©mo de Spring\n\n\n\nSimplifying the data layer\n\n\n(cr√©dits : Fabien Champigny - React Europe)\n\nKevin Robinson nous pr√©sente comment Twitter utilise React. L‚Äôapproche pr√©sent√©e fait la part belle au ‚Äúfonctionnel‚Äù, √† l‚Äôimage de leur infrastructure backend.\n\nIl nous d√©taille les m√©canismes mis en place pour l‚Äôacc√®s aux donn√©es, notamment la possibilit√© de g√©rer de mani√®re d√©clarative les d√©pendances aux donn√©es au niveau des composants.\nAu niveau des stores, il prone l‚Äôutilisation de structures immuables en stockant un ‚Äúlog‚Äù d‚Äô√©v√©nement et en utilisant des ‚Äúreducers‚Äù pour en extraire l‚Äô√©tat des donn√©es r√©el.\n\nOn retrouve dans leur approche beaucoup de concepts de la programmation fonctionnel (de la m√™me mani√®re que Redux), mais malheureusement, aucun code n‚Äôest ouvert par Twitter √† ce sujet.\n\nOn reste un peu sur notre faim en ne pouvant pas aller jouer ‚Äúconcr√®tement‚Äù avec leurs outils.\n\nGoing Mobile with React\n\n\n(cr√©dits : Fabien Champigny - React Europe)\n\nJed Watson, cr√©ateur du framework TouchStone JS, un framework JS (bas√© sur React) permettant de faire des applications mobiles hybride (√† base de Webview via Apache Cordova), nous explique comment r√©aliser des applis hybride gr√¢ce √† React.\n\nLe d√©bat ici est plut√¥t de d√©montrer qu‚Äôon peut malgr√© les dires de certains et en connaissant quelques astuces, faire une appli mobile hybride qui ressemblera √† une appli native. Pour nous prouver cela, Jed annonce que l‚Äôappli de la React Europe, dispo sur iOS et Android, et que nous avons tous utilis√© a √©t√© faite avec TouchStone JS !\n\n\n  If you have great developer experience, you are much more likely to get to a great UX\n\n\nJed conseille de ne pas faire de l‚Äôhybride lorsqu‚Äôon :\n\n\n  est Facebook ou Twitter\n  a beaucoup de donn√©es\n  a une utilisation processeur intensive\n  a des animations complexes sur l‚ÄôUI\n  a des interactions complexes\n  a de la gestion m√©moire avanc√©e\n\n\nLes points les plus importants pour qu‚Äôune application hybride fonctionne sont :\n\n\n  React\n  la gestion du Touch\n  le Layout\n  la gestion de la Nav\n\n\n\n  You should not do everything in a webview, but you can\n\n\nLa d√©mo pr√©sente l‚Äôensemble des composants et des transitions disponibles. En plus de React, Touchstone.js utilise cordova, une biblioth√®que d‚ÄôAPIs permettant d‚Äôacc√©der en Javascript aux fonctions natives du mobile, comme l‚Äôacc√©l√©rom√®tre, le GPS ou l‚Äôappareil photo.\n\nLe code de l‚Äôapplication React Europe sera rendu open-source √† la fin de la d√©mo : Sketch &amp;amp; Code de l‚Äôapp React Europe\n\n\n\nReact Router\n\n\n(cr√©dits : Fabien Champigny - React Europe)\n\nBelle pr√©sentation de Michael Jackson (@mjackson), pas aussi spectaculaire qu‚Äôun concert de l‚Äôartiste homonyme, mais assez surprenante et d√©lirante quand m√™me ! Il introduit la librairie qu‚Äôil porte avec Ryan Florence (@ryanflorence) depuis plus d‚Äôun an et qui est majoritairement utilis√©e par les utilisateurs de React pour mettre du routing dans leur application : React Router, ‚ÄúThe‚Äù Router.\n\nMichael commence par nous pr√©senter les bases de la librairie : la d√©finition des routes dans un composant React et le composant Link permettant de g√©n√©rer les liens. Il explique que ce sont des concepts simples et qu‚Äôils permettent √† de nouveaux d√©veloppeurs peu exp√©riment√©s de rentrer facilement dans un projet.\n\nIl fait ensuite l‚Äôanalogie entre les vues et les URLs, affirmant que de bonnes URLs, bien form√©es, augmentent la confiance de l‚Äôutilisateur envers l‚Äôapplication. Une notion importante dans React Router est celle des transitions permettant de changer de vues (et donc d‚ÄôURLs) et de g√©rer le ‚Äúbrowser history‚Äù.\n\nMichael nous annonce une nouveaut√© dans la prochaine version : l‚Äôattribut onEnter sur la d√©finition de route, permettant d‚Äôexecuter une callback avant d‚Äôafficher la page (utile par exemple pour prot√©ger une page par authentification).\n\nIl nous expose sa vision du composant comme une fonction prenant en entr√©e props et state et renvoyant en sortie une UI. Le router n‚Äôest finalement qu‚Äôun composant comme un autre qui re√ßoit en entr√©e l‚ÄôURL. L‚Äôid√©e que ce qui est explicite est bien meilleur que ce qui est ‚Äúmagique‚Äù dans une impl√©mentation lui permet de pr√©senter les changements de l‚ÄôAPI dans les derni√®res versions du React Router avec la r√©cup√©ration des param√®tres de l‚ÄôURL via les props du composant (et plus via une mixin) ou la disparition du composant RouteHandler qui peut simplement √™tre remplac√© par props.children pour utiliser les ‚Äúnested routes‚Äù dans ses composants.\n\nDans les travaux en cours, on retiendra les transitions anim√©es qui permettent √† Michael de faire une d√©mo ‚Äúwahou‚Äù. L‚Äôanimation est, bien entendu, r√©p√©t√©e en sens inverse sur l‚Äôutilisation du back du navigateur. Cette fonctionnalit√© a d‚Äôautant plus d‚Äôimportance que changement d‚ÄôURLs et animations ne sont traditionnellement pas de bons amis et posent souvent probl√®me.\n\nLe ‚Äúdynamic routing‚Äù est la deuxi√®me d√©mo montrant la possibilit√© de contextualiser l‚Äôouverture d‚Äôune URL : sur une page, un contenu peut √™tre ouvert dans une popup, mais en copiant et ouvrant l‚ÄôURL obtenue dans un autre onglet, on a une page avec le m√™me contenu mais une pr√©sentation diff√©rente (plus de popup).\n\nEnfin, le clou du spectacle sera la derni√®re d√©mo qui √©chouera (un dernier commit sur le repo qui aurait provoqu√© une erreur) qui nous aura valu une fabuleuse danse de Ryan Florence sur la sc√®ne (venu en renfort de Michael) ! L‚Äôid√©e initiale √©tait de pr√©senter une fonctionnalit√© assez √©norme permettant de lazy loader les JS de sa SPA en fonction des besoins de chaque route (le ‚Äúgradual loading‚Äù) √©vitant de charger d√®s le d√©part les 3Mo de son bundle webpack alors qu‚Äôon en utilise qu‚Äôune petite partie. Il faudra attendre pour voir cette fonctionnalit√© en action‚Ä¶\n\nCreating a GraphQL Server\n\nApr√®s la conf√©rence de la veille sur GraphQL, Nick Schrock et Dan Schafer nous montrent comment r√©aliser un serveur GraphQL.\n\nGraphQL est une sp√©cification d‚Äô√©change et ne pr√©suppose aucune technologie backend. Comme nous l‚Äôavons d√©j√† vu dans la conf√©rence pr√©c√©dente, l‚Äôid√©e est de faire de GraphQL une couche entre le client et le code backend d√©j√† existant.\n\nFacebook, en ouvrant cette sp√©cification et l‚Äôimpl√©mentation de r√©f√©rence, esp√®re f√©d√©rer une communaut√© autour de cette solution. Si d‚Äôautres personnes r√©alisent des impl√©mentations dans diff√©rents langages, cela permettrait √† tout le monde de capitaliser sur ces techniques et de faciliter la r√©utilisation de code que ce soit c√¥t√© client ou serveur.\n\nLa ‚Äústack‚Äù imagin√©e pour GraphQL est pr√©sent√©e en partant du serveur jusqu‚Äôau client :\n\n\n  GraphQL App Servers\n  Libs (Parse, SQL)\n  Core\n  Spec\n  Common tools (ex: graphicQL, IDE-like tool)\n  Client SDKs (Relay)\n  GraphQL Clients\n\n\nAu del√† de la pr√©sentation th√©orique, on se pose quand m√™me la question de la mise en oeuvre concr√™te au dessus de code existant.\nFacebook utilise maintenant intensivement GraphQL. Par contre, ils n‚Äôutilisent pas l‚Äôimpl√©mentation de r√©f√©rence mais sans doute une impl√©mentation tr√®s imbriqu√©e √† leur backend (et donc difficile √† rendre open-source).\nOn manque malheureusement de retours sur des questions de mise en oeuvre comme le cache ou la gestion des droits par exemple.\nEsp√©rons que des ‚Äúearly-adopters‚Äù puissent nous faire des retours l√†-dessus dans les semaines/mois √† venir.\n\nPour en savoir plus, un bon article sur le sujet : GraphQL overview : Getting start with GraphQL and Node.JS\n\nIsomorphic Flux\n\n\n(cr√©dits : Fabien Champigny - React Europe)\n\nMichael Ridgway (@theridgway) aborde une notion souvent abord√©e ces 2 jours et sur laquelle nous avions fait un article en d√©cembre dernier.\n\nSelon Michael, les avantages du ‚Äúserver rendering‚Äù sont multiples :\n\n\n  le SEO\n  le support des anciens navigateurs\n  le gain de performance per√ßu par l‚Äôutilisateur\n\n\nUn des objectifs de cette d√©marche est de partager le maximum de code entre le serveur et le client.\n\nLa stack propos√©e par Michael est la suivante :\n\n\n  pour la gestion des vues, React √©videmment qui expose une API client et serveur\n  pour le routing, React Router (https://github.com/rackt/react-router)\n  pour le data fetching, superagent (https://github.com/visionmedia/superagent)\n  pour la logique applicative, un pattern l√©ger et c√©l√®bre : Flux\n\n\nPour la mise en oeuvre de Flux c√¥t√© serveur, nous avons d√©j√† vu au cours de ces 2 journ√©es : Redux et React Nexus. Il en existe d‚Äôautres comme marty.js, flummox ou alt. Michael nous propose Fluxible, la librairie d√©velopp√©e par Yahoo.\n\nFluxible cr√©e un contexte pour chaque requ√™te c√¥t√© serveur avec un dispatcher custom optimis√© pour cette op√©ration. L‚Äô√©tat de l‚Äôapplication est transmis du serveur vers le client gr√¢ce √† un m√©canisme de d√©shydratation/r√©hydratation des stores.\n\nMichael pr√©cise que Fluxible force les d√©veloppeurs √† utiliser Flux de mani√®re conforme sans transgresser les pratiques d√©finies par le mod√®le. La librairie fournit des composants de haut niveau permettant une parfaite int√©gration avec React. Enfin, la particularit√© de Fluxible est son syst√®me de plugins permettant de faciliter l‚Äôajout de nouvelles fonctionnalit√©s.\n\nMichael nous montre un exemple de chat isomorphique et la diff√©rence observ√©e au chargement avec une SPA classique. Il pr√©cise ensuite les outils de d√©veloppement qu‚Äôil utilise :\n\n\n  Babel\n  ESLint\n  webpack\n  babel-loader\n  Grunt / Gulp\n  Yeoman Generators\n\n\nMichael termine sa conf en indiquant que plusieurs applications en prod chez Yahoo utilisent la stack pr√©sent√©e et Fluxible mais qu‚Äôil reste encore quelques am√©liorations √† apporter pour les raisons suivantes :\nLes d√©pendances des composants envers les donn√©es ne sont pas facilement connues (rendant le data fetching en amont du rendering c√¥t√© serveur d√©licat). Relay pourrait √™tre une solution.\nLe rendu c√¥t√© serveur de React est relativement lent (mais pourrait √™tre am√©lior√© dans les futures version de React).\nLe Hot Reloading (avec React Hot Loader) ne fonctionne pas avec les stores Fluxible.\n\nConclusion\n\nQue dire apr√®s ces deux jours de conf√©rence ? \nD√©j√† que la communaut√© et l‚Äôengouement autour de React ne cesse de grandir, mais aussi que ca ne ch√¥me pas cot√© Facebook avec Relay, GraphQL, Animated, React Native Android qui ne devraient pas tarder √† pointer le bout de leur nez, avec aussi la mise en place d‚Äôune personne full time sur Jest ! C‚Äôest rassurant sur l‚Äôavenir court/moyen terme de React.\n\nL‚Äôorganisation √©tait vraiment impeccable (mise √† part les soucis de climatisation) avec beaucoup de tr√®s bonnes id√©es, notamment, les bureaux au fond et sur les cot√©s de la salle de conf√©rence pour que les personnes avec LapTop puissent suivre confortablement.\n\nC‚Äôest aussi plut√¥t √©tonnant, pour une conf√©rence en France, d‚Äôavoir vu aussi peu de personnes francophones. Le public √©tant tr√®s majoritairement anglophone. On se dit que React n‚Äôa pas encore compl√®tement pris en France.\n\nCot√© tendance, on voit qu‚Äôau niveau des librairies Flux, Redux parait clairement √™tre celle qui attire tous les buzz. A voir dans le temps si cela suit, mais le talent ind√©niable de son cr√©ateur, combin√© aux bonnes id√©es (reducers, hot reload) donne vraiment envie de s‚Äôy pencher. On regrette aussi toujours le manque de sujets autour des tests.\n\nNous attendons aussi impatiemment React Native Android, pour voir si le buzz et les superbes promesses sont toujours pr√©sentes avec deux environnements cibles, et on esp√®re voir sur nos stores de plus en plus d‚Äôapplis React Native.\n\nGraphQL + Relay parait vraiment √™tre la solution id√©ale pour r√©aliser simplement du data fetching cot√© client (React Web ou React Native), mais l‚Äôabsence de Relay (toujours pas open-sourc√©), combin√©e au manque de retour sur GraphQL pose encore de nombreuses questions.\n\nNous avons donc h√¢te d‚Äô√™tre √† la prochaine React Conf√©rence ou React Europe pour voir la suite de l‚Äô√©volution de React.\n\nVous pouvez retrouvez le compte rendu de la premi√®re journ√©e ici\n"
} ,
  
  {
    "title"    : "CR React Europe Conf√©rence 2015 - Day 1",
    "category" : "",
    "tags"     : " javascript, react, reactnative, video, graphql",
    "url"      : "/2015/07/06/cr-react-europe-2015-day-one.html",
    "date"     : "July 6, 2015",
    "excerpt"  : "Apr√®s la premi√®re conf√©rence officielle sur React, que nous avons d√©j√† couvert en janvier (Jour 1 et Jour 2), nous nous sommes rendus les 2 et 3 juillet √† Paris sous une chaleur infernale pour cette premi√®re √©dition de la React Europe avec l‚Äôenvie...",
  "content"  : "Apr√®s la premi√®re conf√©rence officielle sur React, que nous avons d√©j√† couvert en janvier (Jour 1 et Jour 2), nous nous sommes rendus les 2 et 3 juillet √† Paris sous une chaleur infernale pour cette premi√®re √©dition de la React Europe avec l‚Äôenvie de voir et de mesurer les √©volutions autour de ReactJS.\n\nKeynote\n\nAu d√©part, React c‚Äô√©tait simplement le V de MVC. Maintenant, on parle de ‚ÄúView First‚Äù ou ‚Äú User interface First‚Äù.\n\nChristopher Chedeaux @vjeux, l‚Äôun des core-dev de React, va faire un focus sur 4 axes principaux :\n\n\n  Data\n  Language\n  Packager\n  Targets\n\n\n1) Data\n\nDepuis l‚Äôannonce de Flux ont fleuri beaucoup d‚Äôautres impl√©mentations du pattern, notamment :\n\n\n  Mcfly\n  Barracks\n  Reflux\n  Fluxy\n  Fluxxor\n  Redux\n\n\nD‚Äôapr√®s Christopher, certaines vont mourir dans les prochains mois laissant seulement la place aux impl√©mentations les plus pertinentes (et Redux a fait un buzz sans pareil lors de ces 2 jours, voir plus bas).\n\nL‚Äôimmuabilit√© revient aussi √©norm√©ment en regardant du cot√© de ClojureScript ou ImmutableJS.\n\nCot√© Data fetching, cela commence √† bouger pas mal avec :\n\n  Relay et GraphQL\n  Falcon &amp;amp; JSON Graph\n  Flux over the wire\n  Om Next\n\n\nIl reste encore les cot√©s Persistence et Temps r√©el qui ne sont pas trait√©s dans l‚Äô√©cosyst√®me de React.\n\n2) Languages\n\nLe langage JS a √©normement √©volu√© avec CoffeeScript, jsTransform (utilis√© chez facebook pour la gestion du jsx, ‚Äúinternalization pipeline‚Äù, ‚Ä¶)\n\n\n  ‚Äúthink of js as a compile target‚Äù\n\n\nIl y a eu Traceur et Recast, et d√©sormais Babel qui a tout ecras√© sur son passage. Facebook convertit en ce moment tout son code Front JS √† Babel.\n\nOn retrouve aussi ESLint, un ‚Äúlinter‚Äù de code, et du typage de donn√©es avec TypeScript et Flow.\n\n3) Packager\n\nNous retrouvons Node.js, CommonJS, npm. \nDans le browser : Browserify et Webpack.\nM√™me s‚Äôil y a encore du travail √† faire pour avoir de bonnes performances, et ne pas attendre une compilation via les mises √† jour incr√©mentales, ou React Hot Loader sur lequel nous reviendrons.\n\n4) Targets :\n\nLes cibles de React sont d√©sormais multiples gr√¢ce au Virtual DOM :\n\n\n  DOM\n  SVG\n  Canvas\n  Terminal\n\n\nUn focus est ensuite fait sur React Native, permettant de d√©velopper des apps natives sur iOS et Android tout en faisant du React.\n\n\n  ‚ÄúUX of a native app / DX of a web app‚Äù\n\n\nChristopher insiste sur le terme DX qu‚Äôon ne voit jamais dans des slides tech, signifiant ‚ÄúDevelopper Experience‚Äù. \nIl compare aussi le d√©veloppement de l‚Äôappli Ads de Facebook, r√©alis√© avec React Native sur iOS (7 ing√©nieurs pendant 5 mois), et celui qui a suivi avec React Native Android avec les m√™mes 7 ing√©nieurs durant seulement 3 mois en r√©utilisant 87% du code !\n\nReact Native Android sera open-sourc√© au mois d‚ÄôAo√ªt.\n\n\n  ‚ÄúLearn once : write anywhere‚Äù\n\n\nUn appel est fait pour stopper le ‚Äúbashing‚Äù sur les autres frameworks. C‚Äôest en travaillant main dans la main entre les communaut√©s Ember, Angular et React notamment que le web avancera.\n\n\n\n\n\nInline Styles: themes, media queries, contexts, and when it‚Äôs best to use CSS\n\nStyle are not CSS\n\nMichael Chan @chantastic va nous soumettre une ‚Äúterrible‚Äù id√©e lors de cette conf qui va en faire crier plus d‚Äôun ! ‚ÄúIt‚Äôs time to learn CSS‚Äù est une phrase d‚Äôune autre √©poque, Michael n‚Äôh√©site d‚Äôailleurs pas √† qualifier cette id√©e de bullshit !\n\nCitant Jeremy Ashkenas @jashkenas, cr√©ateur de CoffeeScript et de Backbone.js, il soumet une nouvelle vision : unifier les 3 syntaxes (CSS, HTML et JS) qui permettent de d√©clarer le style d‚Äôune application web car contrairement √† ce qu‚Äôon pense ‚Äúle style n‚Äôest pas le CSS‚Äù.\n\nMichael d√©fend 2 autres axes importants dans React :\n\n\n  les changements de l‚Äô√©tat de l‚Äôapplication (pilot√© en JS via les ‚Äústates‚Äù des composants) sont des changements de l‚ÄôUI,\n  les composants doivent √™tre r√©utilis√©s comme partie enti√®re et ind√©pendante et ne doivent pas √™tre d√©tourn√©s de leur vocation initiale, ‚Äúje pr√©f√®re avoir 1000 composants qui font 1 choses que 100 composants qui font 2 choses‚Äù.\n\n\nStyle over the time\n\nMichael reprend ensuite l‚Äôhistoire des CSS. A l‚Äôorigine, on d√©clarait les styles dans l‚Äôattribut HTML ‚Äústyle‚Äù. Puis, on s‚Äôest rendu compte de cette fa√ßon que le code √©tait dupliqu√©, d‚Äôo√π l‚Äôintroduction et la d√©claration des classes CSS. Le web est devenu s√©mantique avec l‚Äôutilisation des balises &amp;lt;h1&amp;gt;,&amp;lt;p&amp;gt;, &amp;lt;b&amp;gt;, etc. s√©parant la pr√©sentation dans le HTML et le CSS. L‚Äôarriv√©e du web 2.0 a donn√© au JS le moyen d‚Äôint√©rargir avec le HTML pour diriger le comportement de l‚Äôapplication compl√©tant la couche pr√©sentation HTML + CSS.\n\nNot coupled state\n\nAvec le web interactif actuel, l‚Äô√©tat de l‚Äôapplication est noy√© entre ces 3 parties constituantes. Heureusement, React permet d‚Äôorganiser la structure en faisant du ‚Äústate‚Äù la partie centrale de l‚Äôapplication et le ‚Äúmarkup‚Äù, confondu avec le JS, devient l‚Äôinterface. N√©anmoins, l‚Äô√©tat de l‚Äôapplication est toujours coupl√© avec la pr√©sentation et le CSS. Gr√¢ce √† l‚Äôexemple d‚Äôune todolist basique, Michael explique comment extraire le ‚Äústate‚Äù des CSS (repr√©sent√© par la classe ‚Äúis-complete‚Äù) pour l‚Äôint√©grer en inline dans le render du composant React. Le CSS devient uniquement une couche g√©rant l‚Äôapparence de l‚Äôapplication et les composants (donc le JS) g√®re int√©gralement leur √©tat.\n\nNo more CSS\n\nMichael nous montre enfin comment aller plus loin en g√©rant variables de style, pseudo-classes et pseudo-elements en inline dans le composant, et sans trop de difficult√©s. La gestion des hovers et des media queries est beaucoup plus ardue et n‚Äôest clairement pas recommand√©. L‚Äôutilisation d‚Äôune librairie comme Radium (mais il en existe d‚Äôautres) permet de surmonter cet obstacle et d‚Äô√©crire du style inline tr√®s clairement. On aborde quelques conseils pour g√©rer au mieux les couleurs et le layout. Pour voir un exemple illustrant tous les concepts abord√©s par Michael, vous pouvez explorer son projet React Soundplayer.\n\nPour conclure sa conf, Michael cite Sandi Metz @sandimetz, designeuse Ruby, d√©fendant l‚Äôid√©e que l‚Äôobjectif du design est de permettre de (re-)designer plus tard son application et donc de r√©duire les co√ªts du changement. Le composant React est l‚Äôinterface, il se suffit √† lui-m√™me.\n\nLes slides sur SpeakerDeck\n\n\n\nFlux over the Wire\n\nElie Rotenberg @elierotenberg introduit Flux, le pattern cr√©√© par Facebook massivement utilis√© avec React pour g√©rer le cycle de vie des donn√©es √† l‚Äôint√©rieur de son application. Le fondement de Flux est de pouvoir partager les √©tats de l‚Äôapplication (les ‚Äústates‚Äù) de fa√ßon simple et scalable entre l‚Äôensemble de ses composants car tous n‚Äôont pas que des r√©percussions locales.\n\nElie nous montre qu‚Äôon peut voir Flux comme un mod√®le sym√©trique : les composants React sont le miroir des stores (l√† o√π sont stock√©s les states de l‚Äôapplication) et les actions d√©clench√©es par les composants sont le pendant des √©v√®nements de mise √† jour des stores. Le pattern tourne donc autour de 4 m√©thodes ‚Äúsym√©triques‚Äù : onUpdate/dispatch c√¥t√© composant et onDisptach/update c√¥t√© store. La nouveaut√© mise en exergue par Elie est de consid√©rer que le flux entre les composants et les stores peut √™tre impl√©ment√© par n‚Äôimporte quel canal de communication : callbacks/promises par exemple mais aussi streams/EventEmitter et, plus √©tonnant, websockets. Ce dernier canal permettrait de partager l‚Äô√©tat de son application entre plusieurs composants existants sur de multiples clients gr√¢ce aux stores qui persisteraient sur un serveur node distant. Elie donne l‚Äôexemple d‚Äôun chat fonctionnant sur ce principe.\n\nIl pr√©sente ensuite les librairies qu‚Äôil a √©labor√© autour de ses id√©es :\n\n\n  nexus-flux impl√©mentant le pattern Flux de mani√®re ‚Äúclassique‚Äù, notamment autour de l‚ÄôEventEmitter,\n  nexus-flux-socket.io, l‚Äôimpl√©mentation de Flux autour des websockets,\n  react-nexus une surcouche aux pr√©c√©dentes librairies permettant d‚Äô√©couter les stores depuis les composants React en utilisant les decorators ES7,\n  react-nexus-chat, l‚Äôimpl√©mentation du chat donn√© en exemple.\n\n\nUne des forces de sa librairie est la facilit√© √† mettre en oeuvre l‚Äôasynchronisme des actions Flux c√¥t√© serveur.\n\nEnfin, on d√©couvre l‚Äôutilisation r√©el de ces concepts chez Webedia :\n\n\n  Utilisation de PostgreSQL, Redis et Varnish pour la tenue en charge,\n  React Nexus est utilis√© pour la gestion des commentaires et le syst√®me utilisateur de millenium.org,\n  Une refonte compl√®te de jeuxvideo.com est en cours avec React Nexus,\n  Des modules React sont d√©j√† pr√©sents sur d‚Äôautres sites de Webedia.\n\n\nLes slides sur SpeakerDeck\n\n\n\nReact Native: Building Fluid User Experiences\n\nSpencer Ahrens @sahrens2012 de chez Facebook nous pr√©sente une librairie, qui devrait √™tre open sourc√© sous peu pour g√©rer les animations dans React Native iOS : Animated.\n\n \nvar { Animated } = require(‚Äòreact-native‚Äô) \n\nCette librairie devrait marcher directement sur React Native Android et arriver ensuite sur le web.\nL‚Äôimpl√©mentation est 100% JS.\nNous avons suivi un live coding d√©mo sur iOS d‚Äôune application sans animation au d√©part, consistant √† enrichir l‚Äôexp√©rience utilisateur en rajoutant des animations fluides via la librairie Animated.\n\nLe code des exemples et les slides, ainsi qu‚Äôun nouvel exemple sur l‚Äôanimation ‚ÄúTinder‚Äù\n\n\n\nExploring GraphQL + Relay: An Application Framework For React\n\n\n\nLee Byron @leeb a introduit GraphQL, une solution permettant de r√©soudre les probl√©matiques d‚Äôacc√®s aux donn√©es.\nL‚Äôid√©e est de r√©soudre les probl√®mes de l‚Äôapproche RESTful (qui entra√Æne beaucoup d‚Äôaller-retours avec le serveur) et l‚Äôapproche FQL (variante de SQL permettant de limiter les aller-retours, mais tr√®s compliqu√©e √† maintenir).\n\nGraphQL permet au client de d√©finir tr√®s pr√©cis√©ment les donn√©es qu‚Äôil souhaite obtenir via leur relations.\n\nLe principe de base est que la structure de la requ√™te permet de d√©finir le format de la r√©ponse. Ex :\n\nQuery\n{\n  user(id: 4) {\n    id,\n    name,\n    smallPic: profilePic(size: 64),\n    bigPic: profilePic(size: 1024)\n  }\n}\n\nResponse\n{\n  &quot;user&quot;: {\n    &quot;id&quot;: 4,\n    &quot;name&quot;: &quot;Mark&quot;,\n    &quot;smallPic&quot;: &quot;https://cdn.site.io/pic-4-64.jpg&quot;,\n    &quot;bigPic&quot;: &quot;https://cdn.site.io/pic-4-1024.jpg&quot;\n  }\n}\n\nLe tout donne un code tr√®s facile √† lire et √† raisonner. Le serveur expose un sch√©ma des donn√©es disponibles, ce qui permet :\n\n\n  au client de construire sa requ√™te et de la valider\n  de g√©n√©rer du code c√¥t√© client √† partir du sch√©ma\n  une bonne int√©gration dans les IDE (autocompletion)\n  g√©n√©ration d‚Äôune API Doc\n\n\nGraphQL ne s‚Äôoccupe pas du stockage, c‚Äôest uniquement la couche de requ√™tage qui peut √™tre impl√©ment√©e avec votre code actuel.\n\nGraphQL est utilis√© depuis plus de 3 ans chez Facebook et sert √† l‚Äôheure actuelle environ 260 milliards de requ√™tes par jour.\n\nLee Byron a annonc√© lors de sa conf√©rence la diffusion d‚Äôun ‚Äúworking draft‚Äù d‚Äôune RFC GraphQL, ainsi qu‚Äôune impl√©mentation de r√©f√©rence en Javascript.\n\n\n\nSuite √† cette pr√©sentation de GraphQL, Joseph Savona introduit Relay, un framework propos√© par Facebook qui permet de g√©rer c√¥t√© client le data-fetching via GraphQL dans les applications React.\nLe principe de Relay est que chaque composant d√©finit ses propres d√©pendances en utilisant le langage de requ√™te de GraphQL. Les donn√©es sont mises √† disposition dans le composant dans this.props par Relay.\n\nLe d√©veloppeur fait ses composants React naturellement, et Relay s‚Äôoccupe de composer les requ√™tes, permettant ainsi de fournir √† chaque composant les donn√©es pr√©cises dont il a besoin (et pas plus), de mettre √† jour les composants quand les donn√©es changent et de maintenir un store c√¥t√© client (cache) avec toutes les donn√©es.\n\n\n\nDon‚Äôt Rewrite, React!\n\n\n\nRyan Florence @ryanflorence nous propose de profiter de la r√©√©criture de code d‚Äôapplication historique pour introduire de nouvelles technologies et outils.\n\nLe probl√®me avec les r√©√©critures est que l‚Äôon est g√©n√©ralement oblig√© de le faire pour des morceaux assez important de l‚Äôapplication (en partant du haut de l‚Äôarbre fonctionnel de l‚Äôapplication). Cela peut bloquer la correction de bug sur le code historique, emp√™cher de faire quelques √©volutions, obliger √† maintenir des branches ‚Äú√† longue dur√©e de vie‚Äù,‚Ä¶\n\nAu lieu d‚Äôutiliser cette approche, de haut en bas, Ryan nous propose d‚Äôutiliser React en partant du bas de l‚Äôarbre, c‚Äôest √† dire par une fonctionnalit√© unitaire tr√®s limit√©e.\n\nReact se pr√™te parfaitement √† ce type de travail puisque son design permet de l‚Äôutiliser dans un contexte isol√© tr√®s facilement. Petit √† petit, on arrive √† remonter de plus en plus, en r√©√©crivant des fonctionnalit√©s de plus en plus importantes, jusqu‚Äô√† avoir r√©√©crit l‚Äôapplication compl√®te.\n\n\n\nLive React: Hot Reloading with Time Travel\n\n\n\nDan Abramov @dan_abramov nous pr√©sente son workflow React.\nIl est notamment le cr√©ateur de React Hot Loader, et de Redux, l‚Äôune des derni√®res impl√©mentations de Flux jouissant d√©j√† d‚Äôune tr√®s grande popularit√©.\n\nL‚Äôun des messages √† retenir de sa pr√©sentation est l‚Äôimportance de travailler sur ses outils de d√©veloppement afin d‚Äôavoir plus de temps √† passer sur ses applications.\n\nQuelques outils pour acc√©l√©rer le workflow de d√©veloppement :\n\n\n  amok\n  figwheel\n  livereactload\n  React Hot Loader\n  webpack\n\n\nNous faisons ensuite un focus sur son workflow autour de ces principaux outils :\n\n\n  Redux\n  Redux Dev Tools\n  React Hot Loader\n  webpack\n\n\nReact Hot Loader permet de rafra√Æchir son application instantan√©ment √† chaque modification de code, et ce, sans refresh de page, uniquement en rafraichissement les composants ayant chang√© !\nC‚Äôest tr√®s impressionnant en Live d√©mo !\n\nRajouter √† √ßa le Redux Dev Tools qui permet de suivre en temps r√©el les actions lanc√©es, ainsi que l‚Äô√©tat des states, de pouvoir revenir en arri√®re dans les actions ‚Äú√† la git‚Äù, mais aussi d‚Äôavoir un error handler tr√®s quali en live (inspir√© j‚Äôimagine de la gestion d‚Äôerreur de React Native).\n\nL‚Äôid√©e derri√®re Redux (son impl√©mentation du pattern Flux) est de faire un Store immuable. On peut r√©sumer une action √† une fonction prenant en entr√©e un √©tat du store et donnant en sortie un nouvel √©tat du Store (sans toucher au premier). En partant de ce principe, appliquer une s√©rie d‚Äôactions revient simplement √† effectuer une r√©duction (un ‚Äúreduce‚Äù). \nOn applique ici les principes d‚ÄôEvent Sourcing.\nL‚Äôimmuabilit√© permet de stocker les diff√©rents √©tats interm√©diaires du store et donc de naviguer extr√™mement facilement dans les diff√©rentes versions pendant le d√©veloppement.\n\nPlus d‚Äôinfos ici : The evolution of flux\n\n\n  Reducer + Flux = Redux\n\n\n\n\nBack to Text UI\n\nMikhail Davydov @azproduction a eu l‚Äôid√©e folle de cr√©er une interface texte pour le terminal avec les outils web : HTML, CSS, JS et donc React.\nC‚Äôest compl√©tement fou, assez impressionnant, mais on se demande quand m√™me pourquoi ?\n\nVoir les slides\n\n\n\nLightning Talk\n\nPour finir la journ√©e, nous avons eu le droit √† quelques Lightning Talk de qualit√© in√©gale, abordant l‚Äôint√©gration de D3 avec React, de l‚Äôoutil Cosmos permettant de tester dans un browser ses composants React un par un, de React Native Playground , un bel outil pour tester facilement online dans un simulateur des applis ou exemple de code de React Native voir vid√©o du LT, et Turbine une sorte de remplacant de Relay en l‚Äôattendant (voir cet article).\n\nConclusion\n\nExcellente organisation (et on ne dit pas ca seulement pour les bi√®res √† volont√©), un line-up du tonnerre et de belles annonces (React Native Android en Ao√ªt, GraphQL etc).\nC‚Äôest d√©j√† avec plein d‚Äôid√©es et de pistes d‚Äôam√©liorations pour nos projets React que nous sortons de ce premier jour tr√®s complet.\n\nVous pouvez retrouvez le compte rendu de la deuxi√®me journ√©e ici\n"
} ,
  
  {
    "title"    : "On √©tait au PHPTour ! ",
    "category" : "",
    "tags"     : " conference, afup, phptour",
    "url"      : "/2015/06/04/m6web-au-phptour-luxembourg.html",
    "date"     : "June 4, 2015",
    "excerpt"  : "On √©tait au PHP Tour et c‚Äô√©tait bien !\n\n(y avait un gros g√¢teau et des biscuits en forme d‚Äôelephpant)\n\nLe voyage fut un peu √©pique, surtout les quelques kilom√®tres en plus quand le meilleur d‚Äôentre nous a oubli√© son sac √† dos dans une station √† 15...",
  "content"  : "On √©tait au PHP Tour et c‚Äô√©tait bien !\n\n(y avait un gros g√¢teau et des biscuits en forme d‚Äôelephpant)\n\nLe voyage fut un peu √©pique, surtout les quelques kilom√®tres en plus quand le meilleur d‚Äôentre nous a oubli√© son sac √† dos dans une station √† 150 km de l√† :)\n\n\n\nEt on n‚Äôa pas pu battre la team Blablacar et Jolicode au concours de lev√© de coudes - on est forfait les gars !\n\nPlut√¥t qu‚Äôun retour exhaustif (et parce qu‚Äôavec les aqueducs de Mai on cherche un peu le temps), voici quelque chose de plus informel, sur notre ressenti des tendances communautaires (forc√©ment subjectif).\n\nRadio moquette !\n\nIl y a une bonne maturit√© autour des tests et du CI dans la communaut√© PHP. On commence aussi √† voir de plus en plus des pratiques autour du partage de la responsabilit√© du provisionning entre ops et dev (avec Ansible et Vagrant notamment) mais, comme chez M6Web, c‚Äôest tr√®s balbutiant - et chacun a sa fa√ßon de faire. On voit des infras de dev qui passent dans le cloud (variabilisation des co√ªts, flexibilit√©, possibilit√© d‚Äôexp√©rimenter). Les services manag√©s n‚Äôont pas la cote, on reste sur du IAAS, principalement chez AWS.\n\nDes solutions pour faire du PHP async se dessinent. Cela reste √† exp√©rimenter (libevent, ReactPHP, le tradeoff vitesse, consommation CPU √©tant inconnu. C‚Äôest √† creuser, car cela peut sortir √† moindre co√ªt de quelques situations difficiles. L‚Äôint√©gration avec certaines librairies comme Guzzle est tr√®s int√©ressante.\n\nMySQL 5.7 est annonc√© par Oracle avec pleins de features + 2x plus rapide que 5.6 et 3x que 5.5 (query) et encore plus sur le connection time. Ils annoncent une meilleur int√©gration avec FusionIO et ils semblent pousser des solutions de cluster multi-master (via Fabric) alors que c‚Äô√©tait consid√©r√© exp√©rimental avant, c‚Äôest maintenant annonc√© stable.\n\nPHP7 va √™tre important pour le langage. Pour la performance (au moins x2 vitesse, x0.5 m√©moire), les nouvelles fonctionnalit√©s (classes anonymes, scalar type hints, stricts type hints, return type declaration, exceptions on fatals, ‚Ä¶). Presque pas de BC break, on devrait surement chez M6Web faire des tests avec la RC d√®s que possible et migrer rapidement quelques services √† la sortie d‚Äôune stable.\n\nAnother (php) brick in the wall\n\nM6Web √©tait repr√©sent√© par Olivier qui a fait une pr√©sentation sur l‚Äôarchitecture backend du second √©cran.\n\n\n\nN‚Äôh√©sitez pas √† commenter la conf√©rence.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #8",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2015/05/07/m6web-dev-facts-8.html",
    "date"     : "May 7, 2015",
    "excerpt"  : "Une fois n‚Äôest pas coutume, cette nouvelle fourn√©e des Dev Facts est publi√©e un jeudi. Mais c‚Äôest pour vous donner de quoi lire en ce grand week-end !\n\nEt comme √ßa fait longtemps, voici une grosse fourn√©e. Enjoy !\n\nIncident planifi√©\n\n\n  \n    X : n...",
  "content"  : "Une fois n‚Äôest pas coutume, cette nouvelle fourn√©e des Dev Facts est publi√©e un jeudi. Mais c‚Äôest pour vous donner de quoi lire en ce grand week-end !\n\nEt comme √ßa fait longtemps, voici une grosse fourn√©e. Enjoy !\n\nIncident planifi√©\n\n\n  \n    X : normal les erreurs depuis 14h ?\n    Y : c‚Äôest pas la maintenance ?\n    Z : oui, maintenance\n    Z : ils me pr√©viennent au moins\n    Y : un incident planifi√© quoi\n  \n\n\nLa finesse.\n\n\n  Ca va bien rentrer, √† force qu‚Äôon leur en mette partout.\n\n\nUne bi-douille\n\n\n  Il vient de faire une bidouille pour bidouiller\n\n\nLe fullscreen fen√™tr√©\n\n\n  Le fullscreen ne marche pas en plein √©cran !\n\n\nLa kalitay\n\n\n  Ils sont gardien de la qualit√© avec un K majuscule\n\n\nLa m√©thode argile\n\n\n  Nous on fait de l‚Äôagile en V\n\n\nAutomagique\n\n\n  C‚Äôest fait manuellement √† la main\n\n\nLa dure loi du travail\n\n\n  J‚Äôai m√™me pas r√©ussi √† refourger mon travail aux autres !\n\n\nReproduction\n\n\n  Je ne sais pas si √ßa corrige le bug qu‚Äôon ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nNos amis les belges\n\n\n  le script pour les belges, comme c‚Äôest un ‚Äúone shot‚Äù, on peux dire qu‚Äôon va l‚Äôex√©cuter, [accent belge]une fois[/accent belge]\n\n\nProprement sale\n\n\n  C‚Äôest pas forc√©ment plus propre, mais c‚Äôest moins sale\n\n\nPas pareil, mais diff√©rent\n\n\n  \n    X : tu peux v√©rifier que c‚Äôest diff√©rent ?\n    Y : Diff√©rent comment ?\n    X : bah, pas pareil ‚Ä¶\n  \n\n\n√Ä la louche !\n\n\n  C‚Äôest √† peu pr√®s approximatif‚Ä¶\n\n\nOn a pas l‚Äôhabitude\n\n\n  \n    X : Pourquoi tu penses que √ßa va prendre du temps ?\n    Y : Parce qu‚Äôil faut r√©fl√©chir\n  \n\n\nIl faut savoir ce qu‚Äôon veut\n\n\n  C‚Äôest pas pr√©vu pour √™tre utile\n\n\nC‚Äôest le Java bleu\n\n\n  Donc en fait tu regrettes Java car tu sais pas coder\n\n\nLe mythe ‚Ä¶ et la r√©alit√©\n\n\n  \n    X : C‚Äôest bien connu, quand tu installes Linux, il y a 18 mannequins topless qui d√©filent dans le bureau\n    Y : Oui, mais des mannequins pour linuxiens, donc bof quoi\n  \n\n\nROI !\n\n\n  y a autant d‚Äôutilisateurs que de jour homme pour ce projet !\n\n\nMais ca marche chez moi\n\n\n  \n    Y : navigation priv√©e = pas de DRM = erreur 3365\n    Z : pas de DRM ? mais pourquoi ?\n    Z : mais sur les sites X ca marche, je comprends pas\n  \n\n\nPooh\n\nAu sujet d‚Äôune sombre histoire d‚Äôexpression de besoin\n\n\n  Je ne peux pas toujours les aider √† faire leurs besoins‚Ä¶\n\n"
} ,
  
  {
    "title"    : "Mix-IT 2015 - Jour 2",
    "category" : "",
    "tags"     : " mixit, conference, agile, technique",
    "url"      : "/2015/04/17/mixit-2015-jour-2.html",
    "date"     : "April 17, 2015",
    "excerpt"  : "Cet article est le retour du second jour du Mix-IT 2015, le vendredi 17 avril 2015.\nVous pouvez √©galement consulter le retour du jour 1.\n\nAller sur Mars ‚Ä¶ ou presque\n‚Äî Florence Porcel\n\nFlorence est venue nous pr√©senter son r√™ve : devenir une marso...",
  "content"  : "Cet article est le retour du second jour du Mix-IT 2015, le vendredi 17 avril 2015.\nVous pouvez √©galement consulter le retour du jour 1.\n\nAller sur Mars ‚Ä¶ ou presque\n‚Äî Florence Porcel\n\nFlorence est venue nous pr√©senter son r√™ve : devenir une marsonaute, √† savoir une personne qui va aller physiquement sur Mars.\n\nApr√®s avoir pris le temps de faire le point sur tout ce qui a √©t√© fait pour permettre un jour √† l‚Äôhomme de faire le voyage pour mettre le pied sur Mars, Florence nous a expliqu√© les diff√©rentes actions qu‚Äôelle avait entrepris pour agir sur ce cheminement :\n\n\n  Participation √† un projet de simulation de vie sur Mars : elle a fait partie d‚Äôun groupe de personnes qui se sont isol√©es pendant 15 jours dans des conditions de vie semblables √† celles sur Mars (au milieu du d√©sert de l‚ÄôUtah, scaphandre pour sortir, nourriture lyophilis√©e, rationnement);\n  Participation √† un programme de volontaire pour le premier d√©part habit√© vers Mars;\n  ‚Ä¶\n\n\nLe but de tout √ßa ? Suivre son r√™ve.\n\nFlorence a fini sa conf√©rence en nous rappelant que ce n‚Äôest pas notre √©ducation, notre formation ou notre pass√© qui dicte ce que nous pouvons faire de notre vie, mais ce sont nos r√™ves.\nElle nous a rappel√© que, malgr√© sa formation litt√©raire, et son m√©tier de com√©dienne, que son r√™ve d‚Äôaller sur Mars un jour l‚Äôa conduite √† faire ses actions tr√®s concr√®tes, et que c‚Äôest gr√¢ce √† l‚Äôaction un peu utopique de beaucoup de gens qu‚Äôon peux changer le monde.\n\nEn 2 mots, poursuivons nos r√™ves !\n\nReactJS pour les n√©ophytes\n‚Äì Nicolas Cuillery, Matthieu Lux et Florent Lepretre\n\nCet atelier √©tait un peu corporate puisque que Nicolas et Florent sont actuellement en mission chez M6Web.\n\nLors de cet atelier, qui se d√©composait en plusieurs TP permettant de d√©couvrir une √† une les diff√©rentes sp√©cificit√©s de React et du pattern Flux : React et sa notion de composant, le pattern Flux et une de ses variantes, ReFlux, le syst√®me de routing et les tests avec Jest.\n\nBien qu‚Äôil leur ait manqu√© du temps pour finir l‚Äôatelier, ils √©taient tr√®s pr√©sents pour nous aider √† franchir les premiers pas qui permettent d‚Äôentrer dans ce nouveau monde.\n\nEn tout cas, f√©licitations √† tous les 3 pour leur implication pour ce difficile exercice qu‚Äôils ont plut√¥t bien surmont√©.\n\nStartups d‚Äô√©tats\n‚Äî Pierre Pezziardi\n\nPierre, qui dirige un incubateur d‚Äô√©tat, soit un incubateur qui s√©lectionne et h√©berge des petites √©quipes dont le but est de faire √©voluer le syst√®me informatique public, mais √† la sauce d‚Äôune startup : budget r√©duit, √©quipe r√©duite, mode ‚Äúsurvie‚Äù.\n\nIl a eu l‚Äôid√©e de ce syst√®me lorsqu‚Äôil s‚Äôest pos√© la question sur les outils qui ‚Äúmarchent‚Äù et leurs raisons. Pourquoi utilisons nous les outils de Google ou Dropbox ? Parce qu‚Äôils sont simples et efficace ! Ils vont au but, et correspondent √† ce que l‚Äôutilisateur d√©sire.\nEn poussant cette r√©flexion, il en est arriv√© √† la conclusion que tout syst√®me informatique est le reflet de l‚Äôorganisation qui le pilote : plus l‚Äôorganisation est tourn√©e sur sa propre organisation, plus le produit final correspondra √† ce qu‚Äôun grand manager a demand√©, et pourra √™tre d√©cal√© de ce que les utilisateurs attendent.\n\n√Ä l‚Äôinverse, quand une organisation n‚Äôa pas de marge, elle va √† l‚Äôessentiel pour que son produit convienne aux utilisateurs.\n\nVoici quelques exemples de projets qui sortent de cet incubateur d‚Äô√©tat :\n\n\n  data.gouv.fr\n  March√©s Publics Simplifi√©s\n  mes-aides.gouv.fr\n\n\nCoding Dojo et Mob Programming dans les tranch√©es\n‚Äî Bernard Notarianni\n\nCette conf√©rence √©tait un retour d‚Äôexp√©rience expliquant comment une √©quipe de d√©veloppement habitu√©e a travailler avec des releases fixes √† dates r√©guli√®res a tent√© de mettre en place un d√©coupage en sprint pour am√©liorer sa productivit√©.\n\nJe dis volontairement ‚Äúen sprint‚Äù sans parler de Scrum parce qu‚Äôen fait, cette √©quipe s‚Äôauto-organisait de la sorte, mais sur un cahier des charges fixe, un p√©rim√®tre fixe pour chaque release, et pas de feedback avec le produit.\n\nAu final, la conclusion de Bernard a √©t√© sans appel : ils ont essay√©s, l‚Äô√©quipe a fait son travail, a essay√© l‚Äôam√©lioration continue, mais comme le client ne jouait pas le jeu, √ßa c‚Äôest mal pass√©.\n\nFabriquez votre devbox portable avec Docker\n‚Äî Jean-Marc Meessen et Damien Duportal\n\nLors de cette conf√©rence, Jean-Marc et Damien nous ont expliqu√©s comment ils avaient r√©ussi √† utiliser Docker pour r√©aliser une ‚Äúdevbox‚Äù portable.\nAvant cette conf√©rence, je pensais qu‚Äôils allaient nous expliquer comment ils avaient organis√© leurs conteneurs pour que √ßa soit le plus efficace, mais en fait, une ‚Äúdevbox‚Äù est plus un poste de d√©veloppement complet (bureau et IDE compris)\n\nLe r√©sultat est assez impressionnant dans le fait qu‚Äôils ont r√©ussi √† virtualiser une Debian avec son UI via Docker, et qu‚Äôils peuvent le faire tourner sur n‚Äôimporte quel poste.\n\nToutefois, je ne suis pas convaincu par cette approche. √Ä mon sens, Docker permet √† tous de d√©velopper dans l‚Äôenvironnement qui lui convient, tout en ex√©cutant son code dans un environnement qui est le plus proche possible de la production.\n\nIl n‚Äôen reste pas moins que je les f√©licite pour le r√©sultat qu‚Äôils ont obtenu, et je suis encore plus convaincu de la puissance de Docker suite √† cette pr√©sentation.\n\nReading code good\n‚Äî Saron Yitbarek\n\nSaron est venue nous partager sa vision sur le moyen qu‚Äôelle trouve le plus efficace pour apprendre un langage, un framework, une librairie : lire du code. \nDe la m√™me mani√®re, pour progresser, lire le code des autres permet d‚Äôaller au del√† de ce que nous pensons faire. En se confrontant au code des autres, nous apprenons sur les autres mani√®res de r√©soudre un m√™me probl√®me, sur d‚Äôautres approches de code, et nous √©largissons notre connaissance.\n\nSuivre un tutorial, c‚Äôest bien, mais le soucis, c‚Äôest que le code est basique, sur un usage basique.\nLire un code r√©el, c‚Äôest voir un cas d‚Äôutilisation r√©el, c‚Äôest voir comment le d√©veloppeur l‚Äôa r√©solu, voir les techniques qu‚Äôil utilise, ‚Ä¶ et le moyen le plus s√ªr de toujours d√©couvrir et apprendre.\n\nPour aller encore plus loin, il ne faut pas h√©siter √† discuter avec l‚Äôauteur du code que l‚Äôon vient de lire.\n\nL‚Äô√©nergie de Saron et la conviction qu‚Äôelle met dans sa pr√©sentation ont fait de cette conf√©rence un vrai coup de coeur de ma part !\n\nCome to the dark side\n‚Äî St√©phane Bortzmeyer - [Pr√©sentation sur InfoQ]\n\nLors de cette Keynote, St√©phane nous a fait part de son ressenti quand √† l‚Äôimpact de l‚Äôinformatique dans notre vie.\n\nIl n‚Äôy a encore que 20 ans, l‚Äôinformatique √©tait au service de l‚Äôhomme. Elle servait √† am√©liorer son quotidien √† faciliter son travail.\nAujourd‚Äôhui, c‚Äôest l‚Äôinformatique qui dirige nos vies. Si vous √™tes anti Facebook, vous perdez contact avec pas mal de gens. Si vous ne voulez pas d‚Äôordinateur, il y a de plus en plus de d√©marches que vous ne pouvez faire.\n\nEncore plus important, nous avons d√©l√©gu√©s de plus en plus de d√©cisions √† l‚Äôinformatique, sur des aspects qui impactent de plus en plus notre quotidien. Par exemple, lorsque vous faites un paiement, c‚Äôest un algorithme qui va d√©cider si la transaction est autoris√©e ou non, de mani√®re froide et automatique, sans chercher √† comprendre si sa d√©cision peux vous laisser dans une situation compliqu√©e.\n\n√Ä partir de l√†, nous, d√©veloppeurs, avons une grande responsabilit√©. Le code que nous produisons, les algorithmes que nous acceptons d‚Äôimpl√©menter sont ceux qui se retrouvent dans les syst√®mes qui r√©gissent nos vies.\nIl est donc primordial que nous prenions conscience de cette responsabilit√© et que nous nous posions des questions sur ce que nous faisons, quitte √† refuser de le faire si cela va √† l‚Äôencontre de notre √©thique.\n\nJ‚Äôai √©t√© fortement touch√© par cette claque, enfin, cette conf√©rence, car elle nous place devant nos responsabilit√©s, et devant notre devoir de prendre du recul sur ce que nous faisons pour ne pas √™tre un simple robot.\n\nConclusion\n\nComme √† leur habitude, les organisateurs de ce Mix-IT 2015 ont r√©alis√©s un superbe travail.\nUn grand bravo √† eux !\n\nRappel : cet article est d√©coup√© en 2 parties. N‚Äôoubliez pas de consulter le retour des conf√©rences suivies lors du premier jour.\n"
} ,
  
  {
    "title"    : "Mix-IT 2015 - Jour 1",
    "category" : "",
    "tags"     : " mixit, conference, video",
    "url"      : "/2015/04/16/mixit-2015-jour-1.html",
    "date"     : "April 16, 2015",
    "excerpt"  : "Il est tout naturel que M6Web soit pr√©sent √† une conf√©rence qui m√™le sujet technique d‚Äôavant-garde et agilit√©, 2 sujets qui nous sont chers, surtout lorsqu‚Äôelle se d√©roule √† Lyon.\n\nJ‚Äôai donc eu la chance de participer au Mix-IT 2015 qui se tenait ...",
  "content"  : "Il est tout naturel que M6Web soit pr√©sent √† une conf√©rence qui m√™le sujet technique d‚Äôavant-garde et agilit√©, 2 sujets qui nous sont chers, surtout lorsqu‚Äôelle se d√©roule √† Lyon.\n\nJ‚Äôai donc eu la chance de participer au Mix-IT 2015 qui se tenait les 16 et 17 avril derniers au CPE Lyon.\n\nCet article est d√©coup√© en 2 parties. Dans l‚Äôarticle que vous √™tes en train de lire, vous trouverez les retours des conf√©rences que j‚Äôai suivies le premier jour, mais vous pourrez √©galement trouver le retour des conf√©rences suivies lors du second jour.\n\nThe three ages of innovation\n‚Äî Dan North - [Slides]\n\nDan nous a partag√© sa vision de l‚Äôinnovation √† travers l‚Äô√©volution d‚Äôune technologie.\n\nSelon lui, il existe donc 3 √¢ges dans l‚Äô√©volution :\n\n\n  Explore (Maximize discovery)\n\n\nIl s‚Äôagit de la phase initiale, celle de la d√©couverte, de l‚Äôexp√©rimentation.\nDans cette phase, on essaye, on se trompe, on apprend.\n\n\n  Stabilize (Minimize variance)\n\n\nIl s‚Äôagit de la phase o√π on fait le tri sur tout ce qu‚Äôon a test√©, initi√©, et qu‚Äôon essaye de cat√©goriser, stabiliser tout √ßa, voir retirer ce qui n‚Äôest pas une bonne id√©e.\n\nC‚Äôest le moment o√π on est capable de reproduire ces cr√©ations, et donc de les apprendre (repeatability, predictability, teachability).\n\nDans cette phase, nous apprenons √† r√©duire l‚Äôincertitude autour de la mani√®re de r√©aliser un code.\n\n\n  Commoditize (Maximize efficiency)\n\n\nCette phase est celle de l‚Äôindustrialisation, celle o√π on essaye de r√©duire les co√ªts pour augmenter l‚Äôefficacit√©.\n\nEt m√™me si ces 3 phases sont conflictuelles, l‚Äôinnovation existe dans chacune de ces 3 phases, et il faut savoir les respecter et s‚Äôimpliquer dans chacune.\n\nLe pourquoi du pourquoi de l‚Äôagilit√©\n‚Äî C√©dric Bodin - [Screencast de la m√™me conf√©rence, mais √† Nantes]\n\nAu cours de cette pr√©sentation, C√©dric nous a pouss√© √† r√©fl√©chir sur la raison profonde du succ√®s de l‚Äôagilit√©.\n\nNous le savons, l‚Äôagilit√© est une solution efficace contre les plannings qui glissent, les cahiers des charges non tenables, le syndrome de la tour de cristal et autres dysfonctionnement organisationnels.\n\nDepuis 1994, le Chaos Manifesto publie un rapport indiquant le pourcentage de projets qui √©chouent ou r√©ussissent, et il y a 2 fois plus de projets qui √©chouent que de projets qui r√©ussissent. Il est courant que des projets qui s‚Äô√©ternisent, d√©rapent, ‚Ä¶ soient finalement abandonn√©s.\n\nDe m√™me, il est courant que le produit fini ne corresponde pas √† ce que l‚Äôutilisateur attend, √† cause de la distance entre eux et les √©quipes qui d√©veloppent le produit.\n\nMais pourquoi est-ce que nous avons besoin de l‚Äôagilit√© ? Pourquoi en sommes nous arriv√©s √† cette situation o√π tout ces syndromes apparaissent ?\n\nTout simplement parce que nous avons construit l‚ÄôEntreprise d‚Äôaujourd‚Äôhui sur la base du Taylorisme qui permet d‚Äô√™tre le plus rentable possible en pr√©voyant tout les cas pour r√©duire le hasard et donc l‚Äô√©chec. Le socle de base de cette th√©orie est que rien ne change et que tout est pr√©visible.\n\nOr, la soci√©t√© actuelle va plus vite, veut pouvoir √™tre tr√®s r√©active, et notre branche en particulier.\n\n√Ä partir de l√†, le mod√®le qui veux que certaines personnes pensent, pr√©voient, organisent, pour que les techniciens n‚Äôaient ‚Äúque‚Äù √† ex√©cuter est d√©pass√©e. Et cela se voit ! Combien de ‚Äúj‚Äôai fait ce que le cahier des charges demandait‚Äù, combien de discussions st√©rile autour d‚Äôun changement de p√©rim√®tre pour recalculer des d√©lais ?\n\nVoici 2 exemples de phrases qui montrent le d√©calage entre la vision du Taylorisme dans l‚Äôinformatique et la r√©alit√© :\n\n\n  En informatique, la production, c‚Äôest la compilation. Et elle est tellement efficace qu‚Äôon ne la facture plus.\n\n\n\n  Nos m√©tiers de services ne sont pas des m√©tiers de production, mais des m√©tiers de conception.\n\n\n√Ä partir de l√†, il faut consid√©rer le travail du d√©veloppeur comme du c√¥t√© ‚Äúpensant‚Äù et pas du c√¥t√© ‚Äúex√©cutant‚Äù : ne pas essayer de maximiser sa productivit√© en r√©duisant sa r√©flexion. Bien s‚Äôassurer de sa compr√©hension du besoin au lieu de lui lister des t√¢ches √† accomplir sans r√©flexion.\n\nLes principes mis en avant par l‚Äôagilit√© vont en ce sens : rapprocher l‚Äôutilisateur et le d√©veloppeur, laisser l‚Äô√©quipe d√©cider du planning, permettre l‚Äôimpr√©vu.\n\nToutefois, l‚Äôagilit√© n‚Äôest pas un d√©clencheur. Une soci√©t√© qui passe √† l‚Äôagilit√© sans modifier son fonctionnement va √† l‚Äô√©chec. L‚Äôagilit√© n‚Äôest qu‚Äôun moyen de changer.\n\nN‚Äôoublions pas la loi de Conway :\n\n\n  Tout logiciel refl√®te l‚Äôorganisation qui l‚Äôa cr√©√©.\n\n\nSolution focus in team\n‚Äî Vincent Daviet et G√©ry Derbier\n\nCet atelier destin√© plut√¥t aux managers donnait des pistes et des exemples pour r√©ussir √† relancer de la synergie dans des √©quipes qui ont tendance √† se bloquer lors de la mise en place de l‚Äôagilit√©.\n\nLe principal ressort de l‚Äôagilit√© est la communication. Si cette communication est bris√©e, tout est en p√©ril, et c‚Äôest souvent la principale cause d‚Äô√©chec de son adoption.\n\nNous avons r√©alis√© des mises en condition pour voir comment la communication peut √™tre un v√©ritable frein ou un formidable moteur pour avancer. En posant une m√™me question de plusieurs mani√®res, nous avons constat√© qu‚Äôil est tout aussi possible de bloquer un √©change qui √©tait int√©ressant, que d‚Äôaller voir plus loin que les explications qui nous √©taient propos√©es.\n\nSi le TDD est mort, alors pratiquons une autopsie\n‚Äî Thomas Pierrain et Bruno Boucard - [Slides]\n\nDerri√®re ce titre un peu provocateur, Thomas et Bruno voulaient faire une analyse √† date du TDD, pour voir comment il est utilis√©, et pourquoi il a tendance √† √™tre d√©laiss√©.\n\nComme il est un peu facile de dire que les d√©veloppeurs peuvent avoir du mal √† changer leurs habitudes, nous sommes all√©s voir un peu plus loin :\n\nLe d√©veloppeur qui est habitu√© √† √©crire du code, voir s‚Äôil marche, l‚Äô√©diter, voir s‚Äôil marche, ‚Ä¶ fonctionne selon un principe d‚Äôexp√©rimentation. Il ne sait pas tr√®s bien o√π il va, il essaye du code jusqu‚Äô√† ce qu‚Äôil fonctionne. Et une fois qu‚Äôil maitrise le code, il va continuer √† travailler de la m√™me mani√®re, m√™me si le code fonctionne beaucoup plus rapidement qu‚Äôavant.\n\nMais cette mani√®re de travailler reste fortement expos√©e √† 2 limitations :\n\n  perte de vue de l‚Äôobjectif r√©el;\n  sur-architecture.\n\n\nAvec le TDD, le fonctionnement est de d√©crire ce qu‚Äôon veut faire d‚Äôabord (se fixer un objectif) en l‚Äô√©claircissant au maximum, au plus t√¥t.\n\nPour √™tre efficace en TDD, il faut commencer par creuser son sujet et s‚Äôassurer de comprendre ce qu‚Äôon veut, comment, avec quelles limites (les 5 pourquois). Ensuite, il faut le formuler, de pr√©f√©rence √† haute voix pour bien s‚Äôassurer de comprendre ce qu‚Äôon est en train de dire (m√©thode du canard en plastique), puis finalement lister une s√©rie de phrases en ‚Äúmon code devrait‚Äù indiquant le fonctionnement nominal et les cas limites.\n\nUne fois que tout √ßa est respect√©, il est possible de d√©marrer le TDD proprement dit, √† savoir d‚Äô√©crire les tests, de le voir √©chouer, puis de r√©aliser le code qui permet √† ces tests de fonctionner. Ainsi, il est possible de d√©gager son esprit du fonctionnel pour se concentrer sur le technique, jusqu‚Äô√† voir le code r√©ussir √† atteindre le but fix√©.\n\nRappel : cet article est d√©coup√© en 2 parties. N‚Äôoubliez pas de consulter le retour des conf√©rences suivies lors du second jour.\n"
} ,
  
  {
    "title"    : "Introduction √† Immutable.Js, Relay + GraphQL et React Native",
    "category" : "",
    "tags"     : " javascript, react, reactnative, lft, video",
    "url"      : "/2015/04/01/immutablejs-relay-graphql-react-native.html",
    "date"     : "April 1, 2015",
    "excerpt"  : "Voici un petit compte rendu vid√©o, film√© lors de notre Last Friday Talk de Mars, d‚Äôun retour de veille techno suite √† la React Conf√©rence.\n\nLe retour est une introduction sur 3 des sujets qui m‚Äôont paru les plus importants lors de cette conf√©rence...",
  "content"  : "Voici un petit compte rendu vid√©o, film√© lors de notre Last Friday Talk de Mars, d‚Äôun retour de veille techno suite √† la React Conf√©rence.\n\nLe retour est une introduction sur 3 des sujets qui m‚Äôont paru les plus importants lors de cette conf√©rence :\n\n\n  Immutable.Js\n  Relay + GraphQL\n  React Native\n\n\nLes slides :\n\n\n\nPour plus d‚Äôinformations sur la React Conf√©rence, nos CR sont disponibles ici :\n\n\n  Compte rendu React Conf√©rence Jour 1\n  Compte rendu React Conf√©rence Jour 2\n\n\nMalheureusement, la vid√©o n‚Äôest plus disponible‚Ä¶\n"
} ,
  
  {
    "title"    : "Comment a-t-on bouchonn√© les d√©veloppeurs backend ?",
    "category" : "",
    "tags"     : " javascript, superagent, mock, isomorphic, cytron, open-source",
    "url"      : "/comment-a-t-on-bouchonne-les-developpeurs-backend",
    "date"     : "March 30, 2015",
    "excerpt"  : "Chez M6Web, nous travaillons actuellement sur la nouvelle version d‚Äôun site web pour lequel sont d√©di√©es deux teams :\n\n\n  l‚Äô√©quipe backend fournit l‚Äôacc√®s aux donn√©es via des API sous Symfony2,\n  nous, l‚Äô√©quipe frontend, d√©veloppons une applicatio...",
  "content"  : "Chez M6Web, nous travaillons actuellement sur la nouvelle version d‚Äôun site web pour lequel sont d√©di√©es deux teams :\n\n\n  l‚Äô√©quipe backend fournit l‚Äôacc√®s aux donn√©es via des API sous Symfony2,\n  nous, l‚Äô√©quipe frontend, d√©veloppons une application SPA isomorphe utilisant React.JS et le pattern Flux.\n\n\nD√©velopper le front avant les API‚Ä¶\n\nNous avons d√©marr√© le projet au m√™me moment que l‚Äô√©quipe backend, donc sans avoir acc√®s aux API qui nous fournissent les donn√©es n√©cessaires au fonctionnement de l‚Äôapplication. Nous nous sommes alors interrog√© sur la meilleure fa√ßon de d√©velopper notre front sans d√©pendre des API tout en impactant un minimum le code cible.\n\nLe contrat d‚Äôinterface\n\nLe choix technique pour notre SPA a √©t√© guid√© par une r√©flexion pouss√©e sur les app isomorphique. Cette approche, React, Flux et tout l‚Äôenvironnement qui tourne autour nous √©taient alors totalement inconnu. Nous avons eu une phase importante en amont pour poser les bases de l‚Äôarchitecture du site, d√©montrer la faisabilit√© du projet et documenter l‚Äôensemble.\n\nCe petit d√©lai a permis √† l‚Äô√©quipe backend d‚Äô√©tablir des contrats d‚Äôinterface pour les principales routes de l‚ÄôAPI. √Ä partir de ces informations, plus ou moins pr√©cises, nous avons √©tabli des fichiers de fixtures. L‚Äôid√©e √©tait donc de retourner les donn√©es bouchonn√©es pour chaque appel √† une route d‚ÄôAPI non existante.\n\nSuperagent et superagent-mock\n\nPour r√©aliser les requ√™tes aux API, nous utilisons la librairie superagent, un client HTTP javascript facilement extensible. Il est isomorphe, c‚Äôest-√†-dire qu‚Äôil fonctionne aussi bien sur un serveur node.js via npm que c√¥t√© browser dans une application packag√©e via un bundler (webpack, browserify).\n\nNous avons d√©velopp√© superagent-mock, un plugin pour superagent, dont le r√¥le est de simuler les appels HTTP lanc√©s par superagent en retournant des donn√©es de fixtures en fonction de l‚ÄôURL appel√©e.\n\nEn pratique\n\nComme superagent, superagent-mock s‚Äôinstalle via npm et peut √™tre utilis√© sur des applications serveurs ou clientes (via un bundler). Tout d‚Äôabord, il faut rajouter la d√©pendance √† la librairie dans son  package.json.\n\nnpm install superagent-mock --save-dev\n\nIl faut ensuite cr√©er le fichier de configuration. C‚Äôest ici que vous allez d√©cider des routes √† bouchonner. Prenons l‚Äôexemple d‚Äôune route qui n‚Äôexiste pas et qui devra nous retourner la liste des auteurs du blog technique de M6Web : https://tech.bedrockstreaming.com/api/authors.\n\nVoici la structure du fichier de configuration √† mettre en place :\n\n// ./config.js file\nmodule.exports = [\n  {\n    pattern: &#39;https://tech.bedrockstreaming.com/api/authors&#39;,\n    fixtures: &#39;./authors.js&#39;,\n    callback: function (match, data) {\n      return { body : data };\n    }\n];\n\n\n  L‚Äôattribut pattern peut √™tre une expression r√©guli√®re, dans le cas d‚Äôune route qui contiendrait des param√®tres variables (ex : https://tech.bedrockstreaming.com/api/authors/(\\\\d+)).\n  L‚Äôattribut fixtures repr√©sente le lien vers le fichier de fixtures ou une callback.\n  L‚Äôattribut callback est une fonction √† deux arguments. match est le r√©sultat de la r√©solution de l‚Äôexpression r√©guli√®re et data correspond aux donn√©es retourn√©es par les fixtures. match permet d‚Äôutiliser certains param√®tres de l‚Äôappel (ex : l‚Äôid de l‚Äôauteur) pour retourner des donn√©es cibl√©es (ex : l‚Äôauteur dans le fichier de fixtures correspondant √† cette id).\n\n\nEnsuite, il faut cr√©er le fichier de fixtures. C‚Äôest un fichier JS qui exporte une fonction retournant les donn√©es bouchonn√©es.\n\n// ./authors.js file\nmodule.exports = function () {\n  return [\n    {\n      id: 1,\n      name: &quot;John Doe&quot;,\n      description: &quot;unidentified person&quot;\n    },\n    ...\n  ];\n};\n\nPour finir, au d√©but du fichier JS appel√© par node, il suffit de patcher superagent avec le plugin superagent-mock de cette mani√®re :\n\n// ./server.js file\nvar request = require(&#39;superagent&#39;);\nvar config = require(&#39;./config.js&#39;);\nrequire(&#39;superagent-mock&#39;)(request, config);\n\nCes quelques lignes permettent de surcharger certaines m√©thodes de superagent pour lui appliquer la configuration et simuler les requ√™tes bouchonn√©es. Pour comprendre plus en d√©tail le fonctionnement, c‚Äôest par ici.\n\nEt apr√®s ?\n\nAvec cette astuce, vous pouvez d√©velopper votre front sans qu‚Äôaucune API en face ne soit accessible. C‚Äôest tr√®s pratique pour travailler en local, sans acc√®s au net, ou pour rendre les tests fonctionnels de son application compl√®tement ind√©pendants d‚Äôun service tiers externe.\n\nLa partie d√©licate de cette approche intervient lorsque l‚Äôon c√¢ble son application avec la vraie API‚Ä¶ et que l‚Äôon s‚Äôaper√ßoit que le contrat d‚Äôinterface n‚Äôa pas √©t√© respect√© ! Nous avons souvent des corrections √† r√©aliser dans notre code lors de cette √©tape, mais les changements sont g√©n√©ralement mineurs et le gain de temps apport√© par l‚Äôutilisation du bouchon en amont n‚Äôest pas remis en cause. La partie fastidieuse reste de maintenir ses fichiers de fixtures avec l‚Äô√©volution de l‚ÄôAPI, particuli√®rement n√©cessaire si on s‚Äôen sert dans ses tests fonctionnels.\n\nToujours plus\n\nNotre application forge elle-m√™me l‚ÄôURL des images r√©cup√©r√©es via l‚ÄôAPI : elle nous fournit un id et nous reconstituons l‚ÄôURL finale gr√¢ce √† un param√®tre de configuration. Ce n‚Äôest pas REST compliant mais nous avons de bonnes raisons de le faire. Cette g√©n√©ration d‚ÄôURL utilise la librairie sprintf-js. Pour avoir une application compl√®tement ind√©pendante de toute requ√™te externe, nous avons d√ª √©galement bouchonner ces appels sur des images locales. Dans cette optique, nous avons d√©velopp√© sprintf-mock dont le mode de fonctionnement est √©trangement similaire √† celui de superagent-mock.\n\nLes projets superagent-mock et sprintf-mock sont open source. Tr√®s simple d‚Äôutilisation, ils nous permettent de parall√©liser nos d√©veloppements avec l‚Äô√©quipe backend et de rendre autonomes nos tests fonctionnels. Alors n‚Äôattendez plus la finalisation de vos API pour commencer vos d√©veloppements front !\n\n"
} ,
  
  {
    "title"    : "How did we mock the backend developers?",
    "category" : "",
    "tags"     : " javascript, superagent, mock, isomorphic, cytron, open-source",
    "url"      : "/how-did-we-mock-the-backend-developers",
    "date"     : "March 30, 2015",
    "excerpt"  : "At M6Web we are currently working on a new version of a web site, with two separate teams:\n\n  the backend team providing data access through APIs;\n  us, the frontend team, building an isomorphic SPA application using React.JS and the flux pattern....",
  "content"  : "At M6Web we are currently working on a new version of a web site, with two separate teams:\n\n  the backend team providing data access through APIs;\n  us, the frontend team, building an isomorphic SPA application using React.JS and the flux pattern.\n\n\nDevelop the frontend before the APIs\n\nBoth teams started the project at the same time, meaning that at the beginning, we didn‚Äôt have the web services needed for our application. We looked for the best way to develop it without waiting for those web services to become available.\n\nInterface\n\nOur technical choices for the SPA has been guided by a deep thinking about isomorphic applications. This approach, with React, Flux and their surrounding environment, was at the time, totally unknown. Our first important task was to build the foundations of the web site architecture, demonstrate the feasibility of the project and document everything.\n\nThis resulting delay allowed the backend team to specify the output of the API. Based on those informations, we wrote fixtures. The idea was to have data from a nonexistent web service.\n\nSuperagent and superagent-mock\n\nTo request the API we use the superagent library, an easily-extensible Javascript HTTP client. It is isomorphic, so it can be used both on server and client sides.\n\nThen we developed superagent-mock, a superagent plugin dedicated to simulate HTTP requests returning fixtures data.\n\nApplication\n\nLike superagent, superagent-mock can be installed via npm, and be used by server or client side libraries. First, you need to add the library in your package.json.\n\nnpm install superagent-mock --save-dev\n\nThen, create the configuration file, where you will define which data will be mocked. Let‚Äôs take for example a nonexistent API, the authors list on our technical blog: https://tech.bedrockstreaming.com/api/authors.\n\nHere is the file structure we need:\n\n// ./config.js file\nmodule.exports = [\n  {\n    pattern: &#39;https://tech.bedrockstreaming.com/api/authors&#39;,\n    fixtures: &#39;./authors.js&#39;,\n    callback: function (match, data) {\n      return { body : data };\n    }\n];\n\n\n  The pattern attribute should be a regular expression, in case of a route containing variable parameters (ie: https://tech.bedrockstreaming.com/api/authors/(\\\\d+)).\n  The fixtures attribute represents the link to a file or a callback.\n  The callback attribute is a function with two arguments: match is the result of the regular expression and data the fixtures. match allows to use some call parameters (ie: the author id) to return relevant data (ie: the author in the fixture).\n\n\nNext, you have to create the fixture file. This is a JS file exposing a function returning the mocked data.\n\n// ./authors.js file\nmodule.exports = function () {\n  return [\n    {\n      id: 1,\n      name: &quot;John Doe&quot;,\n      description: &quot;unidentified person&quot;\n    },\n    ...\n  ];\n};\n\nFinally, at the top of the file called by node, you have to patch superagent with superagent-mock this way:\n\n// ./server.js file\nvar request = require(&#39;superagent&#39;);\nvar config = require(&#39;./config.js&#39;);\nrequire(&#39;superagent-mock&#39;)(request, config);\n\nThose few lines allow us to overload some superagent methods to apply the configuration of the mocked requests (check the source code).\n\nWhat‚Äôs next\n\nWith this tip, you can develop the frontend without access to any API. It‚Äôs very useful in order to work locally on your computer, without the internet, or to make your functional tests independent of any third party.\n\nHowever it gets tricky when you connect your application with the real API‚Ä¶ and you realize that the interface was not respected. We often have to fix our code at this stage, but the changes are usually minor and time saved by the mock isn‚Äôt questioned. The tedious part is still to maintain fixtures with the API evolution, especially necessary if it‚Äôs used with functional tests.\n\nEven more!\n\nOur app build itself the URLs of images retrieved via the API: it provides us an id and we guess the final URL through a configuration setting. This isn‚Äôt REST compliant but we have good reasons to do this. The URL generation uses the library sprintf-js. To have a completely independent application of any external request, we also had to mock these calls to local images. With this in mind, we have developed sprintf-mock whose operating mode is curiously similar to that of superagent-mock.\n\nProjects superagent-mock and sprintf-mock are open source. Very easy to use, they allow us to parallelize our developments with the backend team and to make our functional tests autonomous. So don‚Äôt wait API completion to start your frontend developments!\n\n"
} ,
  
  {
    "title"    : "CR React Conf√©rence 2015 - Day 2",
    "category" : "",
    "tags"     : " javascript, react, flux, isomorphic, conference",
    "url"      : "/2015/02/10/cr-react-conf-2015-day-two.html",
    "date"     : "February 10, 2015",
    "excerpt"  : "De retour √† Menlo Park pour cette deuxi√®me journ√©e de la React conf√©rence.\n\nKeynote React Native\n\nChristopher Chedeau, @vjeux, revient sur les origines de React Native et les raisons pour lesquelles ils ont d√©cid√© de le cr√©er.\n\nLes 3 piliers d‚Äôune...",
  "content"  : "De retour √† Menlo Park pour cette deuxi√®me journ√©e de la React conf√©rence.\n\nKeynote React Native\n\nChristopher Chedeau, @vjeux, revient sur les origines de React Native et les raisons pour lesquelles ils ont d√©cid√© de le cr√©er.\n\nLes 3 piliers d‚Äôune appli natives qu‚Äôils ont d√ªs traiter pour React Native sont :\n\n\n  Touch Handling : la vraie diff√©rence entre appli native et web\n  Native Components : tout le monde essaye de s‚Äôen rapprocher mais personne n‚Äôy arrive, et il y a d√©j√† beaucoup de tr√®s bons composants natifs\n  Style &amp;amp; Layout : le layout impacte √©norm√©ment la fa√ßon dont on code, que l‚Äôon soit sur le Web, iOS ou Android\n\n\nNous voyons que chaque composant natif a √©t√© recr√©√© comme un composant React : &amp;lt;View&amp;gt;, &amp;lt;Text&amp;gt; ‚Ä¶, et Christopher explique comment un composant React est transform√© en composant natif iOS.\n\nLa transformation du JS en natif se fait via JSCore (le moteur JS dans iOS).\n\nUne d√©monstration nous prouve qu‚Äôon peut utiliser la console Dev Tools de Chrome, pour d√©bugger l‚Äôapplication et voir tout le code ¬´ DOM ¬ª React, comme si nous faisions du web classique.\n\nLa derni√®re partie explique l‚Äôapproche des √©quipes de Facebook sur la mani√®re de faire du CSS. Christopher avait d√©j√† fait hurler pas mal de personnes lors de sa conf√©rence ‚Äúfaire du CSS en JS‚Äù (React CSS in JS). Il d√©clare le style en javascript dans une variable styles, et utilise l‚Äôattribut style : &amp;lt;Text style={styles.movieYear}&amp;gt; qui inlinera le CSS.\n\nC‚Äôest assez d√©stabilisant mais aussi ultra prometteur. Cela permet de r√©soudre quasiment tous les d√©fauts de CSS (Global Namespace, Dependencies, Dead Code Elimination, Minification, Isolation ‚Ä¶)\n\nNous parcourons ensuite les mani√®res de g√©rer du layout nativement dans iOS, que Christophe d√©crit comme ¬´ ultra-compliqu√© ¬ª ! Alors que cot√© web, nous avons le Box Model et Flexbox qui r√©solvent tous ces probl√®mes assez facilement.\n\nLes √©quipes de Facebook ont donc d√©cid√© de re-coder Flexbox et le Box Model en JS avec une approche TDD, de mani√®re √† pouvoir utiliser la plupart des bases de Flexbox dans React Native pour faire du layout facilement sur iOS !\n\nVous pouvez retrouver le r√©sultat ¬´ Css-Layout ¬ª sur le Github de Facebook.\n\nLa d√©monstration continue sur un ¬´ live coding ¬ª montrant le ¬´ live reload ¬ª entre la modification du JS et le rafra√Æchissement instantan√© du Simulator iOS.\n\nNous apprenons aussi que les modules ES6 ou Node comme Underscore, ou le SDK de Parse par exemple, fonctionneront sans probl√®me du moment qu‚Äôils n‚Äôont pas de d√©pendance dans le browser ! \nC‚Äôest encore une fois tr√®s prometteur, et si React Native vous int√©resse, la vid√©o ci-dessous est une excellente introduction.\n\nJ‚Äôai, de mon cot√©, pu jouer quelques heures avec et c‚Äôest effectivement tr√®s sympa, intuitif et tr√®s rapide.\nLa version que nous avons ne contient pas encore tout ce que l‚Äôon voit dans la vid√©o (je n‚Äôai par exemple pas trouv√© le Live Reload ou le Remote Debugging pour l‚Äôinstant), mais cela ne saurait tarder, les √©quipes de Facebook travaillant d‚Äôarrache-pied sur le projet.\n\n\n\nThe complementarity of React and Web Components\n\nAndrew Rota, @AndrewRota, est de Boston, travaille pour Wayfair.com et explique comment utiliser des Web Components avec React, ou du React dans des Web Components.\nIl nous montre un exemple d‚Äôun player html5 vid√©o avec un shadow dom qui contient tous les contr√¥les du player (de simple input HTML).\n\nLa communaut√© WebComponent a d√©j√† partag√© pas mal de WebComponents :\n\n\n  les x-\n  les core-\n  google-\n  paper-\n  et d‚Äôautres ‚Ä¶\n\n\nPour conclure, Andrew partage les bonnes pratique pour cr√©er un WebComponent :\n\npetit\ntr√®s encapsul√©\naussi stateless que possible\nperformant\n\nPour en savoir plus sur les Web Components : Polymer-Project\n\n\n\nImmutable Data and React\n\nLee Byron, @leeb, encha√Æne sur l‚Äôimmuabilit√© !\nConcept passionnant que nous avons entendu dans presque l‚Äôint√©gralit√© des conf√©rences.\n\n\n  Un objet immuable, en programmation orient√©e objet et fonctionnelle, est un objet dont l‚Äô√©tat ne peut pas √™tre modifi√© apr√®s sa cr√©ation. Ce concept est √† contraster avec celui d‚Äôobjet variable. Source : Wikip√©dia\n\n\nLee Byron est donc le cr√©ateur de la librairie Immutable-JS permettant de g√©rer facilement des collections immuable en JS.\n\n\n  Immutable data cannot be changed once created, leading to much simpler application development, no defensive copying, and enabling advanced memoization and change detection techniques with simple logic. Persistent data presents a mutative API which does not update the data in-place, but instead always yields new updated data. Source : immutable-js\n\n\n\n  React is the V in MVC. We don‚Äôt need an M. We already have arrays and objects.\n\n\nD‚Äôailleurs, on parle aussi d‚Äôobjets immuables cot√© Angular 2 : Change Detection in Angular 2\n\n\n\nBeyond the DOM: How Netflix plans to enhance your television experience\n\nL‚Äôune des conf√©rences que j‚Äôattendais le plus, par Jafar Husain, @jhusain, Technical Lead chez Netflix.\nPour plusieurs raisons, d√©j√† parce que Netflix ‚Ä¶ qui en a profit√© pour annoncer la veille que ¬´ Netflix aimait React ¬ª mais aussi parce que Jafar est connu pour pas mal de choses (diff√©rents blog posts ou pr√©sentations), ainsi qu‚Äôun cours interactif sur la programmation fonctionnelle en Javascript sur lequel j‚Äôai pass√© pas mal de temps.\n\nIl nous a donc expliqu√© les plans de Netflix pour am√©liorer l‚Äôexp√©rience T√©l√© sur leurs services, et comment React les a grandement aid√©s √† le faire.\n\nPour conna√Ætre les raisons pour lesquels ils ont choisi React, je vous invite √† lire l‚Äôarticle Netflix like React (Startup Speed et Server Side Rendering \\o/, Runtime Performance, Modularity).\n\nAujourd‚Äôhui, Netflix d√©veloppe majoritairement en Javascript et ont 3 UI en JS, une pour le mobile, une pour le web et une pour les t√©l√©s.\nIls ont vu assez vite que le DOM √©tait tr√®s loin, c‚Äôest pourquoi ils ont cr√©√© et introduit Gibbon (une sorte de Webkit maison plus rapide et adapt√© √† leur besoin sur les t√©l√©viseurs).\nIls ont donc fait √©voluer React (un fork au d√©part) pour permettre de sortir vers quelque chose d‚Äôautres que du DOM afin de correspondre √† leur moteur Gibbon et vont donc continuer en 2015 le d√©ploiement de leur nouvelle UI avec React sur tous les services y compris t√©l√©s.\n\nVous pouvez retrouver le Netflix Open Source Software Center pour d√©couvrir le grand nombre d‚Äôoutils Open source de qualit√© qu‚Äôils d√©livrent.\n\n\n\nScalable Data Visualization\n\nZach Nation travaille chez Dato (anciennement GraphLab) et doit traiter de tr√®s grandes quantit√©s de donn√©es dans ses applications.\n\nIl d√©montre l‚Äôint√©r√™t de React coupl√©e √† d3.js (une librairie de visualisation exceptionnelle) pour repr√©senter √† l‚Äô√©cran des transactions Bitcoin (en parsant un fichier de 21G ! en live).\n\n\n\nRefracting React\n\nTalk par David Nolen, @swannodette, personne tr√®s influente dans la communaut√© JS (son blog). Cr√©ateur de Om, ClojureScript, il nous explique que React doit √™tre vue comme une plateforme (plut√¥t que librairie ou framework). On apprend aussi les concepts √† l‚Äôorigine d‚ÄôOm.\n\n\n\nFlux Panel\n\nBill Fisher (Facebook) a rassembl√© une partie des utilisateurs (voir des contributeurs) √† React pour confronter les diff√©rentes approches sur l‚Äôutilisation de Flux, ainsi que sur la mani√®re de g√©rer de l‚Äôisomorphisme.\n\nOn parle notamment, via Michael Ridgway, @theridgway, de Fluxible, la librairie open source propos√©e par Yahoo (que nous utilisons), qui a annonc√© le jour m√™me la cr√©ation de sa documentation en ligne isomorphique, elle-m√™me open source et utilisant Fluxible.\n\nSpike Brehm est aussi intervenu pour AirBnb, qui fut la premi√®re soci√©t√©, je pense, √† parler d‚Äôisomorphisme.\n\nAndres Suarez a aussi pr√©sent√© la mani√®re de g√©rer l‚Äôisomorphisme chez Soundcloud. Vous pouvez avoir beaucoup plus d‚Äôinfos de sa part dans cette excellente vid√©o.\n\nSi vous √™tes int√©ress√©s par les diff√©rentes approches de Flux, vous pouvez comparez ici les impl√©mentations : Flux Comparison\n\nQuelques questions sont aussi pos√©es √† Jing Chen sur Relay.\n\nL‚Äôapproche √©tait int√©ressante mais le r√©sultat un peu d√©cevant car les sujets et impl√©mentations ne sont que trop peu effleur√©s.\n\n\n\nCodecademy‚Äôs approach to component communication\n\nBonnie Eisenman, @brindelle, travaille pour CodeAcademy.\nCodeAcademy est un service gratuit du secteur √©ducatif permettant d‚Äôapprendre √† coder dans certains langages.\n\nBonnie a partag√© la mani√®re dont son √©quipe a appr√©hend√© React, et notamment la communication entre composant. Ces r√©flexions ont eu lieu avant qu‚Äôils n‚Äôaient connaissance du pattern Flux.\n\nLes slides\n\n\n\nStatic typing with Flow and TypeScript\n\nJames Brantly, de chez AssureSign, commence avec une citation vu le matin (merci le JetLag) sur twitter :\n\nOn the 1st day God created the web. On the 2nd day God wrote jQuery. Then God blacked out, 3 days later awoke &amp;amp; invented React. #reactjsconf&amp;mdash; Matt Huebert (@mhuebert) 29 Janvier 2015\n\n\nIl pr√©sente ensuite TypeScript et Flow, deux outils pour am√©liorer et s√©curiser la production de code.\n\nEn partant d‚Äôune application React d‚Äôexemple, il nous montre comment on int√©gre TypeScript et comment il l‚Äôa ‚Äúhack√©‚Äù pour qu‚Äôil reconnaisse le JSX, puis comment il ajoute Flow.\n\nAu final il recommande d‚Äôutiliser plut√¥t Flow, m√™me si TypeScript est un peu plus mature, et fonctionne lui sous Windows.\n\n\n\nQA with the team\n\nLes deux journ√©es se sont finies sur une session de Questions/R√©ponses avec les √©quipes de React chez Facebook : Tom Occhino, Ben Alpert, Lee Byron, Christopher Chedeau, Sebastian Markb√•ge, Jing Chen, et Dan Schafer.\n\n\n\nConclusion\n\nPour une premi√®re conf√©rence officielle sur React, ce fut une excellente surprise ! On retiendra clairement l‚Äôannonce et les d√©mos de React Native, l‚Äôemballement g√©n√©ral autour de React, ainsi que l‚Äôapprobation global de beaucoup de gros acteurs du web (Yahoo, Mozilla, Netflix, Uber, ‚Ä¶), le succ√®s du pattern Flux (malgr√© le manque de clart√© sur la mani√®re de faire du Data Fetching), les promesses de Relay, les sujets r√©currents autour de l‚Äôimmuabilit√© ‚Ä¶\n\nBref, une vraie belle r√©ussite. Chapeau aux organisateurs (merci @vjeux ;-) ) et √©quipes de Facebook, ainsi qu‚Äôaux speakers pour cette superbe conf√©rence.\n\nIl se passe r√©ellement quelque chose de grand dans la communaut√© Front-End gr√¢ce √† React. Il suffit de voir la vitesse √† laquelle les tickets d‚Äôentr√©e sont partis (m√™me chose au React Meetup parisien de D√©cembre 2014), de voir que tous les frameworks MVC tentent de s‚Äôen inspirer (pr√©-render, SSR, Virtual-Dom ‚Ä¶).\n\nPour finir, je voulais aussi partager le travail d‚Äôune des personnes pr√©sentes lors de cette conf√©rence, ayant une fa√ßon tr√®s particuli√®re de prendre des notes sur chacun des talks : https://chantastic.io/2015-reactjs-conf/\n\np.s: Retrouvez les retours sur la premi√®re journ√©e de la React conf√©rence 2015.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "CR React Conf√©rence 2015 - Day 1",
    "category" : "",
    "tags"     : " javascript, react, flux, isomorphic, conference",
    "url"      : "/2015/02/04/cr-react-conf-2015-day-one.html",
    "date"     : "February 4, 2015",
    "excerpt"  : "Les 28 et 29 janvier 2015, sur le campus de Facebook (√† Menlo Park), avait lieu la premi√®re conf√©rence officielle sur la librairie open-source React (cr√©√©e par les √©quipes de Facebook).\n\n2 jours de conf√©rences riches en talks et en annonces dont v...",
  "content"  : "Les 28 et 29 janvier 2015, sur le campus de Facebook (√† Menlo Park), avait lieu la premi√®re conf√©rence officielle sur la librairie open-source React (cr√©√©e par les √©quipes de Facebook).\n\n2 jours de conf√©rences riches en talks et en annonces dont voici un petit compte-rendu, pour ceux n‚Äôayant pas eu la chance de pouvoir y assister ou de suivre les diff√©rents LT sur Twitter, en commen√ßant par la premi√®re journ√©e.\n\nL‚Äôouverture de la conf√©rence par Tom Occhino\n\nTom Occhino, @tomocchino, a permis de r√©tablir la v√©rit√© sur l‚Äôorigine de React.\nCe sont les √©quipes de Facebook Ads qui sont √† la gen√®se de ce projet.\n\nA l‚Äô√©poque, sur des applis MVC c√¥t√© client, plus les applications et le nombre de d√©veloppeurs grandissaient, plus elles √©taient difficiles √† maintenir et devenaient lentes !\nLe ¬´ Two Way Data Binding ¬ª rendait les mises √† jour en cascade trop compliqu√©es (tout l‚Äô√©cran devait √™tre rafra√Æchi) et le code devenait vraiment non pr√©visible. Mais malgr√© tout, cela ‚Äúmarchait‚Äù ! L‚Äôappli de Chat Facebook fonctionnait aussi de la m√™me mani√®re.\n\nC‚Äôest ainsi que React a √©t√© cr√©√© pour am√©liorer le rendu, l‚Äôorganisation et les performances de ces applications.\n\nInstagram a ensuite rejoint Facebook et les √©quipes ont voulu utiliser React pour refondre le site, mais React √©tait √† l‚Äô√©poque trop coupl√© √† Facebook. \nPete Hunt a donc re-factor√© l‚Äôensemble pour cr√©er le React ‚Äúopen-source‚Äù que l‚Äôon conna√Æt aujourd‚Äôhui.\n\nApr√®s cette introduction sur React, Tom a expliqu√© que l‚Äôun des probl√®mes de React est qu‚Äôil n‚Äôest utilis√© que pour le Web.\nAujourd‚Äôhui, tout le monde tente de cr√©er des composants web ressemblant aux composants natifs, mais √† chaque fois, le r√©sultat est mauvais, l‚Äôenvironnement natif √©tant plus performant que celui d‚Äôun browser. Un exemple donn√© d‚Äôapplication native plut√¥t sexy est celui de ‚ÄúFacebook Paper‚Äù.\n\nIl a ensuite r√©v√©l√© l‚Äôannonce certainement la plus importante de ces 2 jours, l‚Äôarriv√©e prochaine sur Github de ¬´ React Native ¬ª, permettant de d√©velopper en Js via React des composants enti√®rement natifs, avec comme exemple, l‚Äôapplication Facebook Groups pr√©sente sur l‚ÄôApp Store iOs ! \nLa conf√©rence est lanc√©e !\n\nAvec React Native, ils ne veulent pas faire du ‚Äúwrite once, run anywhere ¬ª, mais du ‚Äúlearn once, write anywhere ¬ª de mani√®re √† optimiser les composants et usages suivant les devices.\n\nLe code sera fourni sur un d√©pot priv√© √† tous les participants de la conf√©rence lors de la Keynote de cl√¥ture !\n\nPour finir sa keynote d‚Äôentr√©e, il a voulu lister les frameworks JS qui ont √©t√© influenc√©s par React ces derniers mois : Tous !\n\n\n\nEbay : Tweak your page in real time, without leaving the comfort of your editor\n\nBrenton Simpsons d‚ÄôEbay, @appsforartists, nous a montr√© comment coder en live du React de son mac, avec le rendu affich√© en temps r√©el sans reload sur un ipad.\n\nL‚Äôavantage d‚Äôun iPad √©tant sa taille qui lui permet de repr√©senter 3 √©crans d‚ÄôiPhone 5 sur sa largeur, soit 3 √©tats de son application.\nIl utilise ¬´ WebPack ¬ª et l‚Äôextension pour WebPack ¬´ react-hot-loader ¬ª de Dan Abramov.\n\nEbay a aussi open-sourc√© un framework assez experimental (6 mois d‚Äôanciennet√©) du nom d‚ÄôAmbidex pour g√©rer du server side rendering avec React et Flux : https://github.com/appsforartists/ambidex\n\n\n\nData fetching for React Applications at Facebook\n\nJing Chen, @jingc, et Daniel Schafer, @dlschafer, nous ont pr√©sent√© Relay, une nouvelle approche au pattern Flux orient√© Data Fetching, permettant gr√¢ce √† GraphQL de d√©finir au niveau de son composant les data n√©cessaires.\nRelay se chargeant ensuite de g√©n√©rer les bons appels HTTP gr√¢ce √† GraphQL.\n\nUne approche int√©ressante, mais qui parait tr√®s coupl√©e au fonctionnement de Facebook et soul√®ve pas mal de questions : dois-je modifier toutes mes API pour supporter GraphQL ? Quid de l‚Äôoptimisation du cache cot√© API ? ‚Ä¶\n\nRelay sera open-sourc√© prochainement (ainsi que GraphQL j‚Äôimagine ?).\n\nBeaucoup plus d‚Äôinfos sont disponibles ici : https://gist.github.com/wincent/598fa75e22bdfa44cf47\n\n\n\nCommunicating with channels\n\nJames Long, @jlongster, assez r√©put√© via son blog https://jlongster.com et pour son travail chez Mozilla sur les Dev Tools de Firefox, a pr√©sent√© une mani√®re de communiquer entre composants via des ¬´ channels ¬ª en utilisant la librairie ‚Äú js-scp‚Äù  permettant de coder √† la mani√®re des ¬´ goroutine ¬ª de Go ou des ¬´ core.async ¬ª de Clojurescript.\n\n \n\n\n\nReact-router increases your productivity\n\nMichael Jackson, @mjackson, co-cr√©ateur du routeur le plus populaire de React ‚Äúreact router‚Äù, est venu avec Ryan Florence (l‚Äôautre co-cr√©ateur), nous expliquer les origines du routeur, l‚Äôinspiration tr√®s forte du router d‚ÄôEmber.Js, ainsi que quelques techniques avanc√©es d‚Äôutilisations (transitions, etc). Un excellent speaker et une introduction tr√®s dr√¥le sur les origines de React-Router.\n\n\n  ‚ÄúUrl should be part of your design process‚Äù\n\n\n\n\n\n\nFull Stack Flux\n\nPete Hunt, @floydophone, l‚Äôune des personnes responsable des origines de React et de son ¬´ open-sourcage ¬ª, ancien Lead-Dev d‚ÄôInstagram chez Facebook, a pr√©sent√© un talk un peu particulier expliquant comment on pouvait, cot√© architecture serveur, reproduire le pattern Flux.\n\n\n  ‚Äúshared mutable state is the root of all evil.‚Äù\n\n\n\n\nMaking your app fast with high-performance components\n\nJason Bonta, de l‚Äô√©quipe Facebook Ads, √† l‚Äôorigine de la cr√©ation de React, a cibl√© sa pr√©sentation sur les probl√®mes de performances que r√©sout React.\nCot√© Ads Manager, l‚Äô√©quipe doit faire des interfaces ultra complexes, avec notamment le besoin de pr√©senter un nombre d‚Äô√©l√©ments tr√®s important dans un tableau.\n\nUn composant qui sera annonc√© comme ¬´ open-sourc√© ¬ª durant sa conf√©rence : FixedDataTable\nVous pouvez aussi retrouver une ¬´ review ¬ª du composant ici : https://www.reactbook.org/blog/fixed-data-table-reactjs.html\n\nOnt √©t√© abord√© :\n\n\n  le ReactAddons : PureRenderMixin\n  l‚Äôutilisation du shallowEqual sur le shouldComponentUpdate\n  Ainsi qu‚Äôune bonne pratique pour la r√©alisation des composants, qui est revenue plusieurs fois pendant la conf, consistant √† englober le composant, dans un autre composant de type container ne contenant aucune ¬´ props ¬ª.\n\n\nEn r√©sum√© : \n\n\n\n\nFormat data and strings in any language with FormatJS and react-intl\n\nDerni√®re conf√©rence de la journ√©e par Eric Ferraiuolo, @ericf, sur l‚Äôinternationalisation et la mani√®re de la g√©rer dans React, gr√¢ce √† react-intl (open-sourc√© par Yahoo).\n\nPour ceux qui douteraient encore de la complexit√© de g√©rer plusieurs langues, ainsi que les chiffres et pluralisations, et qui ont cette probl√©matique sur un projet React, cette vid√©o est un must-see.\nFormat.Js a aussi √©t√© cit√© et s‚Äôapparente √† une collection de module Js pour l‚Äôinternationalisation.\n\n\n\nHype\n\nRyan Florence a fini la journ√©e sur un showcase d‚Äôexemple tr√®s int√©ressant. \nIl nous a aussi racont√© son histoire, et comment il est devenu d√©veloppeur : principalement, parce qu‚Äôil voulait toujours r√©pondre ‚Äúoui‚Äù quand on lui demandait si il pouvait faire quelque chose.\n\nBref, une excellente mani√®re de finir la journ√©e de mani√®re fun avec quelques exemples tr√®s int√©ressants, notamment autour des ‚Äúportals‚Äù.\n\nVous pouvez retrouver toutes les d√©mos ici : https://github.com/ryanflorence/reactconf-2015-HYPE\n\nPour ceux qui douteraient encore des performances de React, je vous invite √† regarder les 5-6 premi√®res minutes de la vid√©o.\n\n\n\nConclusion du premier jour\n\nBonne grosse claque sur cette premi√®re journ√©e, notamment avec l‚Äôannonce de React Native. Nous avons eu le droit √† une organisation absolument parfaite (snack, boisson chaude et froide √† volont√©) et des speakers de tr√®s grand talent (ce qui n‚Äôest pas toujours le cas de certaines conf√©rences, surtout aussi cibl√©e que celle-l√†).\n\np.s: Retrouvez les retours sur la deuxi√®me journ√©e de la React conf√©rence 2015.\n"
} ,
  
  {
    "title"    : "App Isomorphic: la Single Page App parfaite ?",
    "category" : "",
    "tags"     : " javascript, webperf, angular, react, flux, isomorphic",
    "url"      : "/2014/12/04/isomorphic-single-page-app-parfaite-react-flux.html",
    "date"     : "December 4, 2014",
    "excerpt"  : "Qu‚Äôest ce qu‚Äôune Single Page App (SPA) ?\n\n\n  ¬´ As rich and responsive as a desktop app but built with HTML5, CSS and Javascript ¬ª\n\n\nLes SPA se r√©pandent de plus en plus, et deviennent un choix ¬´ commun ¬ª lorsque l‚Äôon veut d√©velopper un Front riche...",
  "content"  : "Qu‚Äôest ce qu‚Äôune Single Page App (SPA) ?\n\n\n  ¬´ As rich and responsive as a desktop app but built with HTML5, CSS and Javascript ¬ª\n\n\nLes SPA se r√©pandent de plus en plus, et deviennent un choix ¬´ commun ¬ª lorsque l‚Äôon veut d√©velopper un Front riche (souvent c√¢bl√© sur des API REST) que l‚Äôon souhaite :\n\n\n  testable (unitairement et fonctionnellement)\n  fluide (pas de rechargement d‚Äôurl etc)\n  bien organis√©\n  maintenable et √©volutif\n  ‚Ä¶\n\n\nLes Frameworks type AngularJs et EmberJs tiennent le haut du panier et ont largement fait leurs preuves, mais ils continuent √† √©chouer sur deux sujets pourtant primordiaux dans beaucoup de cas :\n\n\n  La performance (dont le rendu initial)\n  Le r√©f√©rencement\n\n\nLa performance\n\nAujourd‚Äôhui, quand vous chargez une SPA, voici grossi√®rement ce qui se passe cot√© client :\n\n\n  Chargement du fichier HTML\n  Chargement des diff√©rents Assets (Css, image, scripts JS externe comme Angular et Jquery par exemple)\n  Ainsi que de l‚Äôint√©gralit√© du code JS de votre application (sauf si vous lazyloadez)\n  Execution de tout ce petit monde, qui va devoir savoir o√π vous √™tes dans l‚Äôapplication afin de g√©n√©rer le HTML correspondant √† l‚Äô√©tat demand√©.\n\n\nAvoir ces quelques secondes √† attendre avant de se retrouver dans un √©tat fonctionnel est peut √™tre acceptable pour un backoffice. Mais √ßa l‚Äôest beaucoup moins pour un front riche.Et ce temps aura tendance √† augmenter fortement, parall√®lement √† l‚Äôenrichissement de votre application.\n\nSi l‚Äôon se soucie un minimum des aspects de performances Web, c‚Äôest forc√©ment d√©rangeant.\nEt d‚Äôun point de vue plus global, tout le monde sait aujourd‚Äôhui que la performance brute n‚Äôest pas le point fort de ces frameworks.\n\nLe r√©f√©rencement\n\nAutre sujet, qui peut √™tre tr√®s probl√©matique, si le site en question s‚Äôy pr√™te. Ces applications vont fournir comme ¬´ source HTML ¬ª quelque chose de ce style (pour du Angular) :\n\n&amp;lt;!doctype html&amp;gt;\n&amp;lt;html class=&quot;no-js&quot;&amp;gt;\n&amp;lt;head&amp;gt;\n    ...\n&amp;lt;/head&amp;gt;\n&amp;lt;body ng-app=&quot;myApp&quot;&amp;gt;\n    &amp;lt;ng-view&amp;gt;&amp;lt;/ng-view&amp;gt;\n    &amp;lt;script src=&quot;scripts/vendor.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&quot;scripts/main.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n\n\n  Container qui servira √† recevoir le HTML g√©n√©r√© par votre appli JS une fois ex√©cut√©e.\n\n\nDe base, Google (et autres moteurs/crawler) ne verra donc rien, tout votre contenu allant √™tre inject√© via JS dans votre balise ng-view. \nExcept√© le fait qu‚Äôil parait que depuis des mois/ann√©es, Google commence √† r√©ellement crawler du JS ‚Ä¶ Si le site est important, cette supposition ne devrait pas suffire √† vous convaincre, et vous avez raison.\n\nRassurez vous, √† ce stade, des solutions existent pour fournir sp√©cifiquement √† Google une version correspondante aux snapshots HTML g√©n√©r√©s par vos applications.\nCes solutions sont accessibles soit en mode SAAS (payante et h√©b√©rg√©), soit en mode Open-Source √† h√©berger vous m√™me. Je pense notamment √† Prerender.io qui fait plut√¥t bien le job, et vous propose d‚Äôindiquer aux moteurs que vous faites une application de type ¬´ Ajax ¬ª en respectant les recommandations de Google.\n\nPrerender est compos√©e de plusieurs briques :\nUn middleware applicatif (Rails, Node, Varnish, Nginx, etc selon votre infrastructure), qui va intercepter les moteurs et les renvoyer sur votre service de Prerender \nUn service de Prerender qui est une brique Node.js qui va lancer des HeadLess Browser (PhantomJS ou SlimerJs ‚Ä¶) pour executer votre appli JS et renvoyer un snapshot HTML une fois le rendu JS termin√©.\n\nLa solution permet √† priori de faire le boulot, mais cela reste une gymnastique complexe, et beaucoup d‚Äôinterrogations subsistent (pertinence, maintenance, stabilit√©, Page Rank, pond√©ration vs sites classiques ‚Ä¶)\n\nLa lumi√®re au fond du tunnel ?\n\nVous l‚Äôavez donc compris, dans certains cas, les SPA bas√©es sur des frameworks Js posent deux probl√®mes tr√®s g√™nants et difficilement r√©solvables.\nC‚Äôest l√† qu‚Äôentre en piste, une nouvelle fa√ßon de penser les SPA, grace √† une librairie d√©velopp√©e par Facebook : React.JS\n\nReact fait parler de lui car il commence √† √™tre utilis√© massivement par des tr√®s gros acteurs Web, Facebook bien entendu pour ses composants Chat, ou son √©diteur vid√©o, Instagram pour l‚Äôint√©gralit√© du site, Yahoo Mail, Github avec l‚ÄôIDE Atom, Khan Academy, NyTimes, Feed.ly ‚Ä¶\n\nAu premier abord, React n‚Äôest qu‚Äôune librairie qu‚Äôon pourrait comparer √† la partie Vue d‚Äôun Framework MVC (voir aux Directives d‚ÄôAngular), mais il a la particularit√© d‚Äô√™tre bas√© sur un Virtual DOM.\nCe qui parait au d√©part simplement une bonne id√©e pour avoir des performances bien sup√©rieures √† celle d‚Äôun framework MVC bas√© sur le DOM, et √©viter par exemple les Dirty checking du DOM (qui explique en partie le manque de perf d‚ÄôAngular), permet aussi d‚Äôutiliser ces m√™mes composants cot√© serveur !\n\nC‚Äôest ce qu‚Äôon appelle l‚Äôapproche ¬´ Isomorphic ¬ª .\n\nUn composant React n‚Äôest finalement qu‚Äôun module CommonJs et peut donc aussi bien √™tre utilis√© cot√© browser sur le client, que cot√© server dans du Node.Js (ou IO.js devrais-je dire maintenant ?).\nL‚Äôid√©e de l‚Äôisomorphisme est aussi d‚Äô√™tre capable de servir le premier rendu directement par le serveur.\nExemple:\n\n\n  Vous acc√©dez √† votresite.com/votrepage.html\n  Votre serveur Node, construit votre page et sert le rendu HTML g√©n√©r√© par votre appli au client\n  Il sert aussi votre application JS dans un Bundle (g√©n√©r√© via du Gulp ou Grunt par WebPack ou Browserify)\n  Le client re√ßoit un fichier statique et l‚Äôaffiche (sans attendre le moindre JS)\n  Il re√ßoit aussi le bundle Js\n  Une fois affich√©, React sait reprendre la main sur votre appli afin de continuer en mode SPA pour la suite de l‚Äôapplication.\n\n\nEt l√†, vous r√©pondez de mani√®re parfaite aux deux points probl√©matiques.\nGoogle n‚Äôy verra que du feu, et pourra crawler votre site enti√®rement comme si il n‚Äô√©tait compos√© que de fichiers statiques. \nLa performance du premier rendu sera quasi imbattable, car ne n√©cessitant aucun JS !\n\nSur le papier, c‚Äôest juste le r√™ve ultime de tout d√©veloppeur Front-end : tous les avantages d‚Äôune SPA sans les inconv√©nients !\n\nFacebook propose aussi sur son Github, une solution pour ceux ayant d√©j√† un applicatif dans un autre language (ici PHP) : Server side rendering\n\nLa solution parfaite ?\n\nPresque.\nReact n‚Äôest au final que la partie Vue de votre application, il va falloir encore organiser tout √ßa. C‚Äôest ici qu‚Äôentre en compte Flux, un pattern d‚Äôarchitecture unidirectionnel propos√© aussi par Facebook, √† priori plus scalable que ne l‚Äôest le pattern MVC.\n\nMais l√† encore, l‚Äôapproche de Flux est plut√¥t prometteuse, alors quel est le probl√®me ?\n\n\n  Finalement c‚Äôest encore peu mature (d√©j√† React et Flux, mais encore plus l‚Äôapproche Isomorphic)\n  La mont√©e en comp√©tence n‚Äôest pas n√©gligeable\n  Il n‚Äôy a pas vraiment de Framework comparable √† date, et vous allez surement devoir r√©inventer la roue √† certains moments (√† suivre l‚Äôarriv√©e imminente de React Nexus notamment)\n  La documentation est tr√®s faiblarde encore\n  Les ressources tr√®s difficiles √† trouver et de qualit√©s tr√®s diff√©rentes\n  Pas vraiment de starter-kit ou g√©n√©rateur digne de ce nom\n  Le cot√© Isomorphic va aussi engendrer une certaine complexit√© :\n    \n      Est-ce que mon client re√ßoit bien le m√™me √©tat que celui qu‚Äôavait mon serveur au moment du rendu initial\n      Obligation de n‚Äôutiliser que des composants Isomorphic, typiquement un router qui fonctionne aussi bien cot√© client que serveur (React-Router ou Director), m√™me chose pour les requ√™tes HTTP (Superagent par exemple) ‚Ä¶\n    \n  \n\n\nSi malgr√© ces points, vous souhaitez tester cette approche, je vous conseille de regarder du cot√© de Yahoo, qui apr√®s avoir annonc√© la migration de Yahoo Mail de PHP/YUI vers React/Flux Isomorphic a aussi publi√© quelques packages Open-Source tr√®s int√©ressants, pouvant constituer une bonne base de d√©part pour un projet isomorphic :\n\n\n  Fluxible-App\n  Flux-examples\n  ou cet exemple utilisant Fluxible-app : Isomorphic-React\n\n\nSi vous souhaitez plus d‚Äôinfos sur React et Flux, je vous conseille ces deux articles en anglais de @andrewray:\n\n\n  React for stupid people\n  Flux for stupid people\n\n\nOu ce tuto chez nos amis de Jolicode, pour faire un Gifomatic avec React et Flux\n\nD‚Äôautres solutions existent aussi conservant la m√™me approche, mais sur la base d‚Äôautres technos, notamment celle d‚ÄôAirbnb: RendR, permettant d‚Äôutiliser du Backbone cot√© client et serveur.\n\nEt pour finir, si ces sujets vous passionnent tout comme nous, restez √† l‚Äô√©coute ici, d‚Äôautres posts pourraient arriver √† l‚Äôavenir ;-)\n\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - troisi√®me journ√©e",
    "category" : "",
    "tags"     : " conference, velocity, webperf, devops, sysadmin",
    "url"      : "/velocity-europe-2014-day-3",
    "date"     : "December 3, 2014",
    "excerpt"  : "Velocity Barcelone, troisi√®me journ√©e\n\nLe troisi√®me jour √©tant d√©di√© aux tutoriaux, on passe de conf√©rence de 45min √† des ateliers de 1h30.\n\nExtreme Web Performance for Mobile Devices\n\nMaximiliano Firtman nous a dress√© un portrait vraiment exhaust...",
  "content"  : "Velocity Barcelone, troisi√®me journ√©e\n\nLe troisi√®me jour √©tant d√©di√© aux tutoriaux, on passe de conf√©rence de 45min √† des ateliers de 1h30.\n\nExtreme Web Performance for Mobile Devices\n\nMaximiliano Firtman nous a dress√© un portrait vraiment exhaustif du web mobile et de l‚Äô√©tat actuel des navigateurs.\n\n\n\nEn gros c‚Äôest compliqu√©. Le march√© est tr√®s fragment√©, certains constructeurs comme Samsung ajoute du bruit en diffusant massivement un navigateur modifi√©. L‚Äôusage des sites en webview depuis une application native n‚Äôarrange pas les choses (par exemple, l‚Äôapplication Facebook).\n\nApr√®s un rappel sur l‚Äôimportance de la performance, l‚Äôorateur a distill√© de nombreuses pratiques permettant de faire un web mobile plus performant.\n\nOn peut retenir :\n\n\n  Le RWD est un outil, pas une fin en soi,\n  il faut s‚Äôimposer de tester sur du hardware cheap avec une connection faible,\n  ne pas oublier le temps perdu sur le r√©seau (600ms mandatory network overhead),\n  ne pas oublier l‚Äôimpact que le parsing du JS et le rendu CSS est bloquant,\n  utiliser les solutions de stockage cot√© client,\n  de tr√®s nombreux outils de simulation existent, il faut les maitriser.\n\n\nIl propose un site r√©capitulant toutes les informations d√©livr√©es : https://firtman.github.io/velocity/.\n\nSlides :\n\n \n  Extreme Web Performance for Mobile Devices - Velocity Barcelona 2014  from Maximiliano Firtman \n\n\n\nZero Downtime Deployment with Ansible\n\n\n\nSlides\n\nGithub Repo\n\nTutorial int√©ressant conduit par un d√©veloppeur (sur un sujet √† priori plus op√©rationnel) qui d√©montre bien la flexibilit√© et la simplicit√© d‚ÄôAnsible.\n\nApr√®s avoir mener le tutorial √† son terme vous aurez deploy√© deux machines avec du code Java, un load balancer NGINX, et une base de donn√©es PostgreSQL (utilisateur + base).\n\nA contre courant des syst√®mes de gestion de configurations comme SaltStack, Puppet ou Chef, Ansible est bas√© sur le mod√®le push et ne n√©cessite aucun agent, il repose enti√®rement sur SSH. D‚Äôautre part il mixe gestion de configuration et orchestration, ce que qu‚Äôon doit bien souvent faire via des outils tiers comme MCollective.\n\nLa simplicit√© de ce mod√®le en fait sa plus grande force. Ansible est capable de g√©rer dynamiquement les inventaires (de base c‚Äôest une liste statique contenue dans un fichier). Par exemple il est capable d‚Äôinterroger les APIs Amazon, Google Cloud ou RackSpace pour r√©cup√©rer la liste de vos machines, celles de votre Cluster VMWare ou n‚Äôimporte quel script qui sortira une liste en JSON.\n\nAlors que Chef et Puppet offrent une DSL pour d√©crire votre infrastructure sous forme de code, Ansible a opt√© pour une description au format YAML. Sur l‚ÄôAnsible Galaxy vous retrouverez tout les modules disponibles (quelques milliers) comme Nginx, PHP etc‚Ä¶ D√©velopp√©s en Python, il est √©videmment possible de faire soit m√™me ses modules.\n\nLe d√©ploiement avec z√©ro temps de panne peut √™tre impl√©ment√© avec Ansible de la fa√ßon suivante:\n\n\n  r√©cup√©ration de la liste des machines\n  sortie du load balancer d‚Äôune machine (Ansible est compatible GCE et AWS)\n  mise √† jour de la configuration (code et/ou logiciel)\n  p√©riode d‚Äôattente: vous sp√©cifiez si un port TCP doit √™tre disponible, un fichier, etc‚Ä¶\n  it√©ration sur la machine suivante\n\n\nLe nombre de machine trait√©es en parall√®le est bien entendu configurable.\n\n\n\nJe suis Ansible depuis quelques mois d√©j√† et j‚Äôai √©t√© confort√© dans l‚Äôid√©e que c‚Äôest un excellent produit: pas d‚Äôagent, bas√© sur une brique solide qu‚Äôest SSH, et d√©velopp√© en Python :) La gestion de l‚Äôinventaire peut √™tre d√©licate, mais un CMDB comme Collins de Tumblr ou un taggage pr√©cis peuvent r√©soudre l‚Äô√©quation.\n\nAnsible facilite le d√©ploiement d‚Äôinfrastructure immuable, le blue/green, violet et canary deployment de par son mod√®le. C‚Äôest un atout qui en fait √† mon sens le meilleur syst√®me de gestion de configuration aujourd‚Äôhui.\n\nCependant je reste encore un peu dubitatif sur le d√©ploiement et le rollback de code qui ne sont pas encore √† la hauteur de Capistrano. Un aper√ßu du workflow et des sch√©mas de d√©veloppement auraient √©t√© aussi bienvenus.\n\n\n  Ansible Galaxy\n  Ansible Docs\n  How Twitter use Ansible\n  Ansible Tower (Payant)\n  Thoughts on deploying Symfony with Ansible\n\n\n\n\nLinux Containers from Scratch\n\n\n\nSlides\n\nQuelle est la diff√©rence entre le cloud, les containers et un repas gratuit ? Aucun n‚Äôexiste :)\n\nJoshua Hoffman (SoundCloud) est dans le top 5 de mes orateurs pr√©f√©r√©. J‚Äôai beaucoup appr√©ci√© ce tutorial car il fait clairement la part entre virtualisation, containers, LXC et Docker (nom qui ne sera prononc√© qu‚Äô√† la fin lors des questions, pas de buzzword, de hype ni de marketing, merci Joshua).\n\n\n\nLe tutorial vous am√©nera √† cr√©er plusieurs containers portable, du plus simple ou plus complexe, avec les outils de bases du noyau. Vous apprendrez aussi √† vous servir des cgroups, des namespaces process, network, et mount, et serez amen√© √† utiliser des syst√®mes de fichiers unis, ici AUFS.\nJ‚Äôaurais bien aim√© une d√©mo avec le format de QEMU ou btrfs pour ce qui est des syst√®mes de fichiers unis au niveau bloc.\n\nCe tutorial est un must-do pour tout personne d√©sirant s‚Äôinitier aux architectures de containers. Le marketing relativement agressif de Docker ne doit pas faire oublier qu‚Äôil existe d‚Äôautres alternatives, et que Docker est un choix de design bien particulier pas forcement adapter √† tous.\nEx: un container en 3 lignes:\n\n\n\nPour rappel:\n\nLXC/LXD = Ensemble d‚ÄôAPIs et d‚Äôoutils dans l‚Äôespace utilisateur linux exposant les capacit√©s d‚Äôisolation du noyau (cgroups, chroot, namespaces, selinux, iptables etc‚Ä¶), alternative l√©g√®re √† la virtualisation telle qu‚Äôon la conna√Æt (avec Vmware par exemple)\n\nDocker = un des cas d‚Äôusage des containers, application unique, statique, immuable, single app delivery plateform\n\nLXC\n\nDocker F.A.Q\n\n\n\nCoreOps - CoreOS for Sysadmins\n\n\n\nGithub\n\nTutorial tr√®s attendu par beaucoup, Kelsey Hightower (CoreOS Inc.) nous a pr√©sent√© l‚Äô√©cosyst√®me de CoreOs et les probl√®mes qu‚Äôil tente de r√©soudre. Suite √† la demande g√©n√©rale il nous a aussi fait une d√©monstration de Kubernetes, l‚Äôoutil de gestion de containers de Google.\n\nCoreOS est distribution Linux accompagn√©e d‚Äôoutils qui vise √† penser le datacentre comme une seule machine (voir Mesos/Yarn). En d‚Äôautres termes, vous n‚Äôavez que faire de savoir quelle application tourne sur quel serveur. Le datacentre appara√Æt comme une entit√© unique o√π l‚Äôon d√©ploie des applications.\n\nTechniquement, CoreOS est un Linux + systemd + docker + etcd + fleet. CoreOS est bas√© sur Chrome OS, √©pur√© et l√©ger, il b√©n√©ficie du syst√®me d‚Äôupdate en arri√®re plan bien connu de Chrome. On oublie donc le gestionnaire de paquets, les outils de debug (tcpdump etc..) et tout ce qui fait un Linux en mode serveur tel qu‚Äôon le conna√Æt.\n\n\n  gentoo: parfum de distribution Linux (ex: Ubuntu, Debian, Centos)\n  systemd: alternative √† SysV Init, le gestionnaire des d√©mons, le premier programme lanc√© au d√©marrage (PID: 1)\n  docker: Syst√®me de containers l√©gers, ensemble d‚Äôapis et librairies centr√©s sur le d√©ploiement et la gestion d‚Äôapplication isol√©e du kernel.\n  etcd: base de donn√©es cl√©/valeur distribu√©e, utilis√©e pour centraliser la configuration et la d√©couverte de service, fond√©e sur le protocole de consensus Raft.\n  fleet: SysV Init distribu√© (c‚Äôest la glue entre systemd et etcd), votre programme doit au minimum avoir 3 instances ? fleet s‚Äôen assurera !\n\n\n\n\n\n\nLa d√©monstration vous am√®nera √† lancer 1 master et plusieurs machines ‚Äúworkers‚Äù et quelques containers Docker.\n\n\n\nKubernetes est la r√©ponse de Google √† la question des gestionnaires de containers disitribu√©s.\n\nConstitu√© d‚Äôun certain nombre de composants qu‚Äôon ne d√©taillera pas ici, il permet de g√©rer des pods (un ou plusieurs containers qui doivent fonctionner localement sur le m√™me host). Il intervient dans la r√©partition des applications dans le cluster, la distribution et l‚Äôordonnancement des containers Docker.\n\nLiens:\n\n\n  CoreOS Doc\n  Kubernetes\n  Introduction to Kubernetes\n\n\n‚Äì\n\nResponsive and Fast: Iterating Live on a RWD Site\n\nCette conf√©rence est globalement une redite des autres sur l‚Äôoptimisation c√¥t√© front. Colin Bendell d‚ÄôAkamai nous pr√©sente plusieurs outils comme webpagetest, mais aussi des astuces pour tester sur Device depuis chrome. Il nous rappelle qu‚Äôil faut faire attention aux conditions de tests avec certains facteurs comme la connexion. Il faut faire aussi attention √† limiter le nombre d‚Äôimages, de ressources (js, css ‚Ä¶). Un des gros probl√®mes sur un site responsive, est celui des images. Pour √©viter de charger des images trop importantes, il faut utiliser la balise . Cette nouvelle balise n‚Äô√©tant pas disponible sur tous les navigateurs, il nous conseille d‚Äôutiliser un composant Picturefill. En ce qui concerne les CSS, il conseille d&#39;int√©grer directement les css critiques dans le corps de la page et de ne charger, par la suite, que les css correspondants au device que l‚Äôon utilise. Pour conclure, l‚Äôutilisation d‚Äôun CDN avanc√© est hautement recommand√©e gr√¢ce √† des options permettant de diff√©rencier navigateurs / devices.\n\nLiens :\n\n  Slide de la pr√©sentation\n\n\n\n\nBuild a device lab\n\n\n  ‚ÄúQui a un placard avec pleins de devices en vrac qui n‚Äôont ni c√¢ble, ni batterie et dont vous ne connaissez plus le mot de passe ?‚Äù\n\n\nJ‚Äôai lev√© la main ;) .\n\nLara Hogan et Destiny Montague nous ont expliqu√© comment Etsy avait construit un device lab, permettant √† leurs collaborateurs d‚Äôemprunter des appareils mobiles pour tester leurs applications, sites mobiles et newsletters.\n\nL‚Äôid√©e est d‚Äôoutiller puissamment les √©quipes et de leur donner un acc√®s extr√™mement simple √† un parc complet (m√™me un chromebook pixel !) - afin d‚Äôassurer un maximum de tests sur les diff√©rents √©quipements.\n\nBien s√ªr il y a un device lab pour les √©quipes techniques et un autre pour le produit / marketing.\n\nLes sujets suivants ont √©t√© abord√©s :\n\n\n  choix des appareils\n  consommation √©lectrique\n  le setup des devices (√† l‚Äôaide d‚Äôun Mobile Device Management)\n  les tests\n  le r√©seau\n  un retour complet sur l‚Äôexp√©rience utilisateur\n\n\nUn site complet d√©di√© √† leur conf√©rence est disponible : https://larahogan.me/devicelab/.\n\nUne vid√©o de la m√™me conf√©rence √† New York est √©galement en ligne :\n\n\n\n\n\nUne conf√©rence un peu #old car d√©j√† faite, mais toujours d‚Äôactualit√© concernant la probl√©matique. Je suis bluff√© par la capacit√© d‚ÄôEtsy √† mettre en oeuvre des moyens et des comp√©tences sur des sujets qu‚Äôils estiment importants. C‚Äôest s√ªrement en lien avec le succ√®s que la soci√©t√© rencontre actuellement.\n\n\n\nConclusion\n\nUne conf√©rence dense et int√©ressante, qui nous a donn√© l‚Äôopportunit√© de rencontrer pleins de gens int√©ressants et m√™me de visiter (un peu) Barcelone !\n\n\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - seconde journ√©e",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2014-day-2",
    "date"     : "November 24, 2014",
    "excerpt"  : "Velocity Barcelone, seconde journ√©e\n\nDeuxi√®me jour de conf√©rence avec un programme encore plus charg√© et quelques conf√©rences all√©chantes rep√©r√©es au pr√©alable.\n\nMorning Keynotes\n\nUpgrading the Web: Polyfills, Components and the Future of Web Deve...",
  "content"  : "Velocity Barcelone, seconde journ√©e\n\nDeuxi√®me jour de conf√©rence avec un programme encore plus charg√© et quelques conf√©rences all√©chantes rep√©r√©es au pr√©alable.\n\nMorning Keynotes\n\nUpgrading the Web: Polyfills, Components and the Future of Web Development at Scale - Andrew Betts (FT Labs)\n\nL‚Äôorateur fait remarquer que de nombreux syst√®mes existent pour packager et g√©rer les d√©pendances des applications backends, mais rien n‚Äôest disponible pour les composants webs. Il nous a pr√©sent√© le projet Origami qui permet de r√©utiliser massivement des composants HTML.\n\n\n\nSlides : the Future of Web Development at Scale\n\nTroubleshooting Using HTTP Headers - Steve Miller-Jones (Limelight Networks)\n\nSlides : Troubleshooting Using HTTP Headers\n\nUn employ√© de Limelight nous a pr√©sent√© comment l‚Äôajout de headers dans une requ√™te pouvait renvoyer des headers suppl√©mentaires dans la r√©ponse HTTP. Cela peut √™tre utile pour d√©bugguer et analyser un incident.\n  Cette pr√©sentation nous a rappel√©, qu‚Äôen interne, nos gentils ops nous permettent d√©j√† de faire ce genre chose sur nos proxy cache.\n\nMonitoring without Alerts - and Why it Makes Way More Sense than You Might Think - Alois Reitbauer (ruxit.com)\n\nAlois Reitbauer a √©voqu√© la solution Ruxit d√©velopp√©e depuis plus de trois ans. Cette solution consiste √† installer un agent sur vos serveurs qui va automatiquement d√©tecter des anomalies statistiques et corr√©ler cette information avec d‚Äôautres d√©viations dans le but de trouver la root cause d‚Äôun incident.\n\nBeaucoup d‚Äôautres solutions de ce genre existent (et la plupart √©taient dans le salon des sponsors). Nous n‚Äôavons pas √©t√© totalement convaincu de leurs capacit√©s √† d√©tecter des root cause, mais elles sont toutes assez int√©ressantes et matures.\n\n\n\nLowering the Barrier to Programming - Pamela Fox (Khan Academy)\n\nPamela Fox nous a pr√©sent√© l‚Äôinitiative code.org, dont le but est de promouvoir l‚Äôenseignement de l‚Äôinformatique (bon, apparement seulement aux US).\n\nElle a √©galement donn√© quelques conseils si on veut s‚Äôinvestir dans l‚Äôenseignement de l‚Äôinformatique √† destination des plus jeunes. Par exemple cr√©er un code club.\n\n\n\nSlides : Lowering the Barrier to Programming\n\nVelocity at GitHub - Brian Doll (GitHub)\n\nBrian a fait une pr√©sentation tr√®s ‚Ä¶ minimaliste. Il est revenu rapidement sur 7 ans de d√©veloppement √† GitHub et comment ils sont venus √† d√©velopper l‚ÄôEnterprise Edition. Il a √©voqu√© diff√©rents probl√®mes que certains de leurs clients avaient et notamment avec l‚Äôutilisation de GE en environnement cloud.\n\nIl a donc annonc√© le lancement de GitHub Enterprise 2.0 qui fonctionne maintenant sur AWS (et un changement de la grille tarifaire) !\n\n\n\nJ‚Äôai profit√© d‚Äôun instant avec lui pour lui pr√©senter GitHubTeamReviewer (un outil interne open-sourc√©). Il √©tait enchant√© de d√©couvrir ce qui avait √©t√© fait avec l‚ÄôAPI de Github. Il a indiqu√© que l‚Äôentreprise travaillait actuellement sur des vues permettant de pallier aux probl√®mes r√©solus par GitHubTeamReviewer.\n\nHTTP Archive and Google Cloud Dataflow - Ilya Grigorik (Google)\n\nIlya Grigorik a pr√©sent√© https://bigqueri.es/, un outil permettant d‚Äôinterroger HTTP archive. La nouveaut√© est que le body des requ√™tes est maintenant conserv√© et que l‚Äôon peut l‚Äôanalyser. Un engine Javascript a √©t√© int√©gr√© au SQL de bigqueries permettant de faire des requ√™tes tr√®s puissantes.\n\n\n\nPour ceux qui ne voudraient pas se plonger dedans, beaucoup de recherches faites par d‚Äôautres utilisateurs sont disponibles et abondamment discut√©es (exemple).\n\n\n\nWebpagetest-automation 2.0 - Nils Kuhn (iteratec GmbH), Uwe Be√üle (iteratec GmbH)\n\nWebpagetest est un outil formidable mais il est difficile √† automatiser. Les orateurs ont pr√©sent√©s un outil pour le faire, permettant donc de r√©aliser une mesure continuelle de la webperf avec un parcours utilisateur complet - d√©monstration √† l‚Äôappui.\n\nLeur travail est disponible sur GitHub sous licence Apache : https://github.com/IteraSpeed/OpenSpeedMonitor. Un grand merci &amp;lt;3 ! (√† 10 minutes sur la vid√©o).\n\n\n\n\n\n\n\nEtsy‚Äôs Journey to Building a Continuous Integration Infrastructure for Mobile Apps - Nassim Kammah (Etsy)\n\nUne parmi les tr√®s nombreuses conf√©rences Etsy sur la V√©locity (le moment de renouveller les conf√©renciers ?). Nassim Kammah nous a expliqu√© comment Etsy d√©livrait ses applications iOS.\n\nLa livraison des applications sous iOS est au m√™me stade que la diffusion des logiciels via CD-ROMs. Partant de ce constat un syst√®me de build (avec 25 mac-minis derri√®re) a √©t√© mis en place √† chaque commit sur le master. On ne peut pas d√©livrer une version de l‚Äôapplication tous les jours aux clients, mais on peut le faire pour les employ√©s (and eat your own dog food) !\n\nIl y a √©galement un syst√®me de gamification, autour de l‚Äôapplication livr√©e journali√®rement, afin de motiver tout le monde √† trouver des bugs.\n\n\n\nDes tests unitaires sont mis en place, ainsi que des tests fonctionnels avec AppThwack. Il est int√©ressant de constater qu‚Äôils n‚Äôattendent pas, pour les tests fonctionnels, une r√©ussite √† 100% de la suite mais une tendance positive.\n\nLes √©quipes ont √©galement mis en place des testing dojos dans lesquels les ing√©nieurs QA encadrent des salari√©s d‚ÄôEtsy et testent √† fond les applications.\n\nOn peut retrouver tous les √©l√©ments de cette conf√©rence sur le blog technique d‚ÄôEtsy.\n\n\n\nRecycling: Why the Web is Slowing Your Mobile App - Colin Bendell (Akamai)\n\nPourquoi recycler nos contenus pour les applications mobiles ?\n\n\n  acc√©l√©rer le time to market.\n  r√©duire le risque\n\n\nLes APIs encouragent le recyclage.\n\n\n\nColin Blendel nous encourage √† utiliser les m√™mes recettes que pour les navigateurs web et √† en ajouter d‚Äôautres :\n\n\n  g√©rer le pool de connexions en groupant les appels par domaine (quitte √† les passer s√©quentiellement, par exemple, si des cookies sont utilis√©s),\n  surveiller les packet eaters (headers inutiles, Set-Cookies r√©p√©t√©s),\n  setter correctement Content-Type sur des types standard (les exemples de content-type tir√©s des logs d‚ÄôAkamai sont assez dr√¥les, comme par exemple test/binary ^^ !),\n  faire un minimum de redirections,\n  fragmenter son cache au minumum (quitte √† calculer des cl√©s plus consistantes cot√© client),\n  ajouter du cache (Max-Age: 30s c‚Äôest √† peu pr√®s du temps r√©el et √ßa change tout pour un CDN),\n  pr√©fetcher les urls pr√©sentes dans les retours d‚ÄôAPI, car on va surement en avoir besoin imm√©diatement apr√®s,\n  ne pas h√©siter √† mettre CRUD au placard et merger plusieurs appels API en un seul ; il faut trouver une balance efficace pour bien g√©rer la webperf.\n\n\nUne pr√©sentation dense et vraiment int√©ressante !\n\nSlides : Why the Web is Slowing Your Mobile App\n\n\n\nBreaking News at 1000ms\n\nLe Guardian est un journal Anglais pr√©sent sur le web et sur tout type de device. Ils ont r√©cemment fait une refonte de leur site pour passer √† une version Responsive avec pour challenge d‚Äôafficher son contenu en moins d‚Äôune seconde.\n\nLe Guardian c‚Äôest 110 000 utilisateurs, 7000 diff√©rents devices. L‚Äôancien site avait un d√©but de rendu en 8 secondes pour un affichage complet en 12. Avec la nouvelle version le site s‚Äôaffiche en 1 seconde et le chargement complet au bout de 3. Quelles sont les principales optimisations ?\n\nPour commencer, il faut charge le contenu important pour l‚Äôutilisateur en premier, √† savoir le menu, l‚Äôarticle, et le widget d‚Äôarticle populaire. Le reste du contenu sera charg√© dynamiquement en JS.\n\nEn ce qui concerne le css, c‚Äôest la m√™me chose. Les CSS importantes (critiques) qui concernent l‚Äôarticle et le rendu global sont int√©gr√©es inline. Ainsi, nous n‚Äôavons pas de blocage du rendu de la page. Le reste des css est charg√© via Javascript. Avec ce syst√®me, on gagne au moins une demi-seconde sur le d√©but d‚Äôaffichage du contenu.\nPour gagner en fluidit√© pour les prochains affichages, le css est stock√© en localStorage. On gagne ainsi des ressources pour les prochains chargements.\n\nPour les fonts ? C‚Äôest la m√™me chose, elles sont mises en cache dans le localStorage pour supprimer de nouveaux chargements.\n\nEnfin le gros morceau : les images ! Elles sont charg√©es de fa√ßon asynchrone en lazyloading. Cela permet de ne pas bloquer le rendu principal de la page.\n\nEn compl√©ment, ils ont mis en place des outils, notamment pour monitorer dans Github la taille des Assets afin de v√©rifier qu‚Äôil n‚Äôy a pas de grosses variations.\n\nAvec ces optimisations et un syst√®me de Proxy qui va g√©rer les donn√©es mises en localStorage, le site peut m√™me √™tre accessible en mode offline.\n\n\n  Github du Front\n  Slide de la pr√©sentation\n\n\n\n\nOffline-first Web Apps\n\nMatt Andrews nous pr√©sente comment rendre une application web disponible Offline.\n\nPlusieurs contraintes peuvent nous pousser √† avoir besoin d‚Äôune app (signet d‚Äôaccueil) disponible m√™me sans connexion. Que ce soit un article dans le m√©tro ou une carte au milieu de nulle part sans connexion, il y a une r√©elle attente utilisateur.\n\nPremi√®rement, il faut activer AppCache en pr√©cisant qu‚Äôil faut faire un petit Hack pour qu‚Äôil soit vraiment utile (voir slide).\n\nEnsuite l‚Äôutilisation de plusieurs outils nous permet d‚Äôarriver √† nos fins :\n\n  Utilisation de FetchApi : Il permet de remplacer nos appels Ajax avec une fonction succ√®s , d‚Äôerreur et les Promises pour charger le contenu, ou lire le cache en cas d‚Äôabsence de connexion.\n  Cache API : Il permet de choisir des Url a mettre en cache. Ainsi que de forcer le contenu de ces urls dans le code.\n  Service Worker : Il permet d‚Äôintercepter les events de chargement pour ensuite appeler le syst√®me de Cache API.\n\n\nToutes ces optimisations nous permettent d‚Äôacc√©der au site en Offline. Mais ces optimisations nous permettent aussi d‚Äôoptimiser le chargement de nos pages puisqu‚Äôon limite le nombre d‚Äôappels HTTP avec la mise en cache de certaines ressources.\n\nSlide de la pr√©sentation\n\n\n\nLook, Ma, No Image Requests!\n\nPamela Fox nous pr√©sente comment elle a optimis√© les images d‚Äôun site internet.\n\nLa premi√®re astuce est de compresser ces images au maximum. Il existe des outils online comme le site TinyPng qui compresse vos images et vous permet de les t√©l√©charger directement.\n\nDeuxi√®me astuce, mettre les images dans les css en base 64. \nA noter qu‚Äôil existe des outils javascript qui effectuent la conversion dans les css √† l‚Äôaide d‚Äôun petit commentaire en bout de ligne (voir les slides de pr√©sentation).\n\nTroisi√®me solution : Les Fonts ! \nPour remplacer les petites images et surtout pour remplacer les sprites qui ne sont pas forc√©ment adapt√©s, vous pouvez utiliser des Fonts. L‚Äôavantage des fonts est qu‚Äôelles peuvent s‚Äôadapter facilement en taille et en couleur ‚Ä¶ Des outils existent d√©j√† pour les g√©n√©rer : Font Awesome.\n\nAutre astuce, le differ de chargement des images. Pamela nous propose son outils javascript, qui va permettre de vous simplifier les chargements. Il est aussi possible de ne charger que les images pr√©sentes √† l‚Äô√©cran et de charger les suivantes lors du scroll. (lazyload).\n\nPour les vid√©os la m√™me astuce est possible. Puisque les vid√©os sont √† pr√©sent charg√©es dans des iFrame, leur contenu peut √™tre charg√© de fa√ßon diff√©r√©. Attention, il ne faut pas remplir le href par une url blank, sinon on perd en temps de chargement.\n\nSlide de la pr√©sentation\n\n\n\nMicroservices - What an Ops Team Needs to Know\n\nSlides: Microservices - What an Ops Team Needs to Know\n\n\n\nLe buzzword est l√¢ch√©. Le propos n‚Äô√©tait pas ici de troller autour de la notion de micro-services, de l‚Äôimpl√©mentation ou de leur utilisation, mais plut√¥t du changement que cela implique pour les √©quipes d‚Äôexploitation.\n\nSouvent consid√©r√© comme le goulot d‚Äô√©tranglement de la cha√Æne de mise en prod, l‚Äôexploit‚Äô regarde les architectures de micro-services avec circonspection : en plus d‚Äôavoir des d√©pendances entre eux, les composants sont mis √† jour ind√©pendamment et r√©guli√®rement, on peut donc vite tout casser en prod.\nPourtant en fournissant des services de bases et des outils aux √©quipes de d√©veloppement, on peut augmenter leur autonomie et la disponibilit√© des infras.\n\nCela passe par:\n\n\n  automatiser les VMs ou les containers en prod comme en dev\n  un syst√®me de m√©triques ‚ÄúAs a Service‚Äù (similaire √† graphite / statsd)\n  un service de log central (logstash/heka/fluentd)\n  un outil de d√©ploiement (capistrano/deployinator)\n\n\nCes outils et services ainsi fournis vont permettre √† l‚Äôexploitation de se concentrer sur des probl√©matiques plus complexes. En effet les microservices ont besoin d‚Äôoutils de diagnostics plus pouss√©s (on citera au passage Zipkin), d‚Äôalerting et de monitoring sp√©cialis√©s par exemple.\n\n\n\nQui dit droits, dit devoirs, et l√† je paraphraserai notre orateur Michael Brunton-Spall:\n\n\n  Give developers pagers too !\n\n\n\n  Developers should be exposed to the pain they cause\n\n\nCela s‚Äôinscrit totalement dans le mouvement ‚ÄúYou build it, you run it‚Äù, o√π les √©quipes de d√©veloppement sont responsables de leur code depuis la conception jusqu‚Äô√† la maintenance en production.\n\n\n\nIt‚Äôs 3AM, Do You Know Why You Got Paged ?\n\nSlides: It‚Äôs 3AM, Do You Know Why You Got Paged ?\n\nRyan Frantz nous a rappell√© quelques √©l√©ments de bon sens concernant les alertes:\n\n\n  un contexte: quel h√¥te ? serveur ? service ? l‚Äôimpact front / back ?\n  l‚Äôhistorique de l‚Äôalerte et de la m√©trique: √©tat de la m√©trique il y a 5min, 15min, 1 jour, 1 semaine, combien de fois a sonn√© l‚Äôalerte aujourd‚Äôhui ?\n  la raison d‚Äô√™tre du check (r√©dig√©e par le cr√©ateur du check)\n  des couleurs et mise en forme permettant de trouver visuellement l‚Äôinformation le plus rapidement possible (rappel il est 3 heure du matin, et peut √™tre daltonien, pensez donc bien √† vos codes couleurs)\n\n\n\n\n\nSeule une alerte critique doit vous faire lever √† 3H du matin, un volume disque √† 80% plein n‚Äôest pas r√©ellement grave, cependant si son taux de remplissage est pass√© de 1% par heure √† 300% par heure, cela peut devenir probl√©matique.\n\nRyan nous a ensuite pr√©sent√© nagios-herald. Ce plugin nagios permet de multiplexer une alerte dans diff√©rent services (cf sch√©ma) ci dessous.\n\n\n\nPour ma part je pr√©f√®re Sensu qui int√®gre de base ce type de m√©canisme. On peut affecter √† une alerte un groupe de handlers (alerte hipchat + graphite + logstash par exemple)\n\n\n\nCustomizing Chef for Fun and Profit\n\n\n\nSlides: Customizing Chef for Fun and Profit\n\nEn suivant les √©tapes d‚Äôapplication d‚Äôune recette Chef, Jon Cowie a distill√© son savoir sur la personnalisation de Chef.\n\nIl nous a par exemple d√©montr√© qu‚Äôil √©tait tr√®s simple de d√©velopper son propre plugin ohai et ses propres handlers.\n\nJ‚Äôai appr√©ci√© le passage sur la gestion des √©v√©nements Chef, en effet la sortie en ligne de commande n‚Äôest qu‚Äôune des fa√ßons de r√©cup√©rer les logs, les √©v√©nements sont bas√©s sur un syst√®me de pub/sub, on pourrait tr√®s bien imaginer la publication en live stream dans un redis ou autre.\n\nPar ailleurs Jon vient de publier un livre sur le sujet:\nO‚ÄôReilly - Customizing Chef\n\n\n\nMega quiz Velocity\n\nPerry Dyball et Stephen Thair avaient pr√©par√© un quiz interactif avec les participants √† la conf√©rence. Des questions diverses et vari√©es d√©filaient sur le grand √©cran et une application web permettaient √† chacun d‚Äôy r√©pondre. Un moment fun anim√© par deux animateurs survolt√©s.\n Malheuresement il semble que l‚Äôapplication n‚Äôaient pas tenu la charge et personne n‚Äôa pu vot√© apr√®s la seconde question (la prochaine fois ils devraient nous confier le projet :) ), mais un syst√®me de fallback a √©t√© pr√©vu, bas√© sur des feuilles de papier de couleur √† brandir bien haut pour r√©pondre aux questions.\n\n\n  merci le papier ! :)\n\n\n\n\nConclusion\n\nFin des conf√©rences et direction les soir√©es offertes par Facebook (o√π nous avons pu discuter avec Santosh Janardhan, responsable des infrastructures de Facebook ^^ !) et Dyn.\n\nLe r√©sum√© de la premi√®re journ√©e est √©galement disponible.\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - premier jour",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2014-day-1",
    "date"     : "November 19, 2014",
    "excerpt"  : "Velocity Barcelone, premier jour\n\nBaptiste, Fran√ßois et Olivier ont eu la chance de participer √† la V√©locity Conf√©rence Europe 2014 qui avait lieu cette ann√©e √† Barcelone.\n\nVoici le compte rendu des conf√©rences et des moments qui les ont marqu√©s.\n...",
  "content"  : "Velocity Barcelone, premier jour\n\nBaptiste, Fran√ßois et Olivier ont eu la chance de participer √† la V√©locity Conf√©rence Europe 2014 qui avait lieu cette ann√©e √† Barcelone.\n\nVoici le compte rendu des conf√©rences et des moments qui les ont marqu√©s.\n\nMorning Keynotes\n\nLes keynotes du matin semblaient √™tre sc√©naris√©es sur diff√©rents points que les organisateurs de la conf√©rence voulaient mettre en avant.\n\nLife after human error - Steven Shorrock (EUROCONTROL)\n\nSteven Shorrock n‚Äôest pas un homme de l‚ÄôIT, mais travaille autour de la s√©curit√© a√©rienne. Il se d√©finit comme un ergonomiste des syst√®mes. Il a pr√©sent√© comment, autour des erreurs humaines, ‚Äúles mots cr√©aient le monde‚Äù et entrainaient imm√©diatement un jugement social (‚Äún√©gligence‚Äù est √©videment plus connot√© que ‚Äúerreur d‚Äôattention‚Äù). Il peut y avoir des erreurs dans la d√©finition d‚Äôune erreur. Qualifier une erreur demandait une d√©finition pr√©cise de standards et de contextes. \nIl a √©galement conseill√© d‚Äô√©tudier les cas de fonctionnement normaux ; ne pas faire seulement des post-mortem mais des pre et des no mortem.\n\n\n\nUne pr√©sentation int√©ressante sur l‚Äôincident et l‚Äôerreur.\n\n\n\nMaximize the Return of Your Digital Investments - Aaron Rudger (Keynote Systems)\n\nUne pr√©sentation sponsoris√©e bien faite, montrant les difficult√©s de communication entre deux populations (IT et biz en l‚Äôoccurence) et comment un outil performant et agr√©able peut aider √† combler ce gap.\nChez M6Web nous utilisons grafana, et il est vrai que cet outil pourrait largement sortir du p√©rim√®tre de l‚ÄôIT.\n\nSlides : Maximize the Return of Your Digital Investments\n\n\n\nAlways Keep an Eye on Your Website Performance - PerfBar Khalid Lafi (WireFilter)\n\nUne rapide d√©monstration d‚Äôun outil en javascript √† installer sur les postes de vos d√©veloppeurs et permettant d‚Äôafficher des alertes si un site en production (ou ailleurs) d√©passe un certain seuil.\n\nA d√©couvrir : PerfBar\n\n\n\nThe Impatience Economy, Where Velocity Creates Value - Monica Pal (Aerospike Inc.)\n\nIl y a une g√©n√©ration on attendait 10 jours un √©change de courrier postal, aujourd‚Äôhui un adolescent v√©rifie son t√©l√©phone toutes les 10 secondes ! Nous sommes moins attentifs, plus impatients.\nDe ce constat Monica Pal explique comment les backend web doivent s‚Äôadapter et servir de plus en plus d‚Äôinformations contextualis√©es : search, sort, recommand, personalize.\n\nSlides : The Impatience Economy\n\n\n\nRecruiting for Diversity in Tech - Laine Campbell (Pythian)\n\nUn th√®me r√©current de la velocity de cette ann√©e. Laine explique comment l‚Äôascenseur m√©ritocratique est cass√© et que seule une d√©marche volontaire permettra d‚Äôaugmenter la diversit√© dans les entreprises.\n\nSlides : Recruiting for Diversity in Tech\n\n\n\nBetter Performance Through Better Design - Mark Zeman (SpeedCurve)\n\nLa derni√®re keynote √©tait vraiment excellente. Mark Zeman, venu de Nouvelle Z√©lande, a expliqu√© comment le processus cr√©atif pouvait aider √† am√©liorer la performance. Dans ce but il a propos√© de redesign the design process.\n\n  se fixer certains principes/objectifs de performance d√®s le d√©part\n  ajouter les designers dans la feature team et it√©rer via des prototypes\n  de partager le savoir sous forme d‚Äôinformations visuelles (graphique mais aussi sous forme d‚Äôun bookmarklet indiquant quelle partie d‚Äôun site met du temps √† charger)\n\n\n\n\n\n\nJe vous invite vivement √† regarder sa vid√©o :\n\n\n\n\n\nIT Janitor, How to Tidy Up - Mark Barnes (Financial Times)\n\nCe manager au Financial Times a expliqu√© comment le journal a √©t√© touch√© de plein fouet par la r√©volution du web mobile et a d√ª s‚Äôadapter tr√®s rapidement.\n\n\n\nIl a expliqu√© quelle strat√©gie il a adopt√©e pour tuer ou refaire les vieux syst√®mes et comment, en premier lieu, il a vendu le projet √† ses sup√©rieurs.\nIl a tout d‚Äôabord pr√©sent√© le TCO de ce qu‚Äôil a appel√© la version ‚Äùclassic‚Äù de ft.com (la carotte) puis a appuy√© sur la peur de l‚Äôincident et les probl√®mes de s√©curit√© (le b√¢ton ; le journal ayant √©t√© la cible des pirates syriens).\n\nApr√®s une analyse fine du traffic il a ensuite appliqu√© ces strat√©gies :\n\n  tuer directement une application inutile (il y en avait), quitte √† la rallumer si quelqu‚Äôun finalement en √† l‚Äôusage :) (et couper un serveur Solaris avec 1833 jours d‚Äôuptime !)\n  re√©crire l‚Äôapplication et la red√©ployer sur le nouveau syst√®me\n  tuer une application et √©crire plusieurs autres (d√©coupage en micro services)\n\n\nSon cr√©do √©tait ‚Äùtry to make the right thing easier.‚Äù Ainsi les projets bas√©s sur la nouvelle stack disposait out of the box de fonctionnalit√©s de monitoring et de log. Cela a beaucoup motiv√© les √©quipes de d√©veloppements.\n\nAu final la purge du legacy a apport√© :\n\n\n  un gain de 30% de performance\n  un meilleur TTM\n  de substantiels retours sur investissement\n\n\nSlides : IT Janitor - How to Tidy Up\n\n\n\nMansplaining 101: Cisadmin Edition - Marni Cohen (Puppet Labs)\n\nLa conf√©rence la plus geek de la journ√©e. La conf√©renci√®re a ouvert un terminal et a tap√©\n\nbrew install feminism\n\n\n\n\nLa conf√©rence √©tait tr√®s sinc√®re et didactique sur comment mieux int√©grer les femmes dans l‚ÄôIT.\n\nVoici les scripts et les ressources qu‚Äôelle a pr√©sent√©s : https://gitlab.com/marni/mansplaining\n\n\n\nBuilding the FirefoxOS Homescreen - Kevin Grandon (Mozilla)\n\n\n\nSlides : Building the FirefoxOS Homescreen\n\nConf√©rence de pr√©sentation de l‚ÄôOS pour smartphone de Firefox.\n\nLors de cette pr√©sentation, Kevin Grandon Ing√©nieur chez Mozilla nous a pr√©sent√© le nouvel OS, et nous a initi√© √† la programmation sur ce dernier.\nCe nouvel OS est donc bas√© sur des langages simples : HTML / CSS / Javascript.\n\nLe d√©veloppement est donc assez facile √† prendre en main, le d√©bugage aussi car on peux monitorer tout ce qu‚Äôil se passe sur le device de test via un firebug d√©di√©.\n\n\n\nDon‚Äôt Kill Yourself : Mobile Web Performance Tricks that Aren‚Äôt Worth it, and Somme that Are - Lyza Gardner (Cloud Four)\n\nOptimisations pour le web mobile.\n\nLyza Gardner nous a pr√©sent√© sa vision de l‚Äôoptimisation sur web mobile. Elle nous a tout d‚Äôabord fait un compte rendu sur son exp√©rience personnelle. Liza a cherch√© via diff√©rentes analyses (speedIndex‚Ä¶) √† trouver une relation entre temps de chargement, nombres d‚Äôassets etc‚Ä¶ Et la conclusion qu‚Äôelle mettait en avant, c‚Äôest qu‚Äôil n‚Äôy avait pas de recette magique. \nElle a ensuite fait la parall√®le entre le web lors de ces d√©buts qui √©tait limit√© par le d√©bit de nos connexions de l‚Äô√©poque, et le web mobile tel qu‚Äôil est actuellement. Ainsi certaines optimisations de l‚Äô√©poque sont adaptables, et m√™me toujours valables, √† nos probl√©matiques actuelles.\nSelon elle, il ne faut pas optimiser un site pour le mobile, mais l‚Äôoptimiser tout court. Elle propose de se fixer des objectifs, par exemple se fixer une limite de nombre d‚Äôappel asset. Mais surtout d‚Äôoptimiser / limiter les images puisque 62% du trafic d‚Äôun site correspond a ces derni√®res.\n\n\n\nWhat are the Third-party Components Doing to Your Site‚Äôs Performance? - Andy Davies, Simon Hearne (NCC Group)\n\nSlides : Third-party components and site performance?\n\nNous utilisons tous des ¬´ Third-Party ¬ª sur nos sites, mais est-ce une bonne id√©e ?\n\nUn Third-Party est un script que nous chargeons depuis un autre site. Par exemple : Google Analitycs. Il existe diff√©rents type de Third-Party : la publicit√©, les analyseurs de trafic ‚Ä¶ La probl√©matique est que nous ne pouvons pas controller ces outils. Nous n‚Äôavons pas la main sur le temps de chargement, la disponibilit√© de l‚Äôoutils, et cela peut influer sur l‚Äôexp√©rience utilisateur et la qualit√© de nos services.\n\nPour conclure, il faut trouver le bon compromis entre ce que nous apporte le Third-Party et ce qu‚Äôil peut nous co√ªter ‚Ä¶\n\n\n\nGuide to Survive a World Wide Event - Almudena Vivanco, Mateus Bartz (Telef√≥nica\n\nSlides : Survive a World Wide Event\n\nRetour d‚Äôexp√©rience de Movistar TV, une cha√Æne payante multi-support qui a diffus√© la coupe du monde en Espagne, au Br√©sil et en Argentine.\n\nCette soci√©t√© s‚Äôest confront√©e √† une probl√©matique de traffic avec des pics de connexions importants en peu de temps. La soci√©t√© devait diffuser la coupe du monde FIFA 2014 dans plusieurs pays et sur plusieurs devices diff√©rents. Apr√®s des tests en condition r√©elles avant le d√©but de la comp√©tition, ils se sont aper√ßus qu‚Äôils ne pouvaient pas g√©rer le pic de connexion qui arrivait entre 5 minutes avant le coup d‚Äôenvoi et 5 minutes apr√®s, ainsi qu‚Äô√† la reprise du match et d√©but de deuxi√®me mi-temps.\nIl a donc fallu tout refaire √† plusieurs niveaux :\n\n  Cr√©ation d‚Äôun CDN en interne\n  Refonte globale du syst√®me de connexion pour pouvoir supporter les pics.\n  Mise en place de monitoring via Graphite\n  Mise en place de Tests\n\n\nMise en avant de beaucoup de probl√©matiques :\n\n  Multi plateforme\n  D√©ploiement sur plusieurs continents (Am√©rique du Sud, Europe)\n  Rassembler 11 outils de monitoring en un seul.\n\n\n\n\nIs TLS Fast yet ?\n\nSlides : Is TLS Fast yet ?\n\nTL;DR = Oui, il pourrait l‚Äô√™tre !\n\nLe talent d‚ÄôIlya pour les conf√©rences techniques a une fois de plus fait ses preuves. \nTout en d√©taillant l‚Äôutilit√© de TransportLayerSecurity (compression, v√©rification d‚Äôerreurs, authentification, chiffrement‚Ä¶) Ilya nous prouve que dans le meilleur des cas, un RTT suppl√©mentaire est n√©cessaire et l‚Äôimpact CPU tr√®s faible.\n\nOutre l‚Äôutilisation des derni√®res versions du Kernel, d‚ÄôOpenSSL et de votre OS serveur, la performance de TLS passe aussi par la r√©utilisation d‚Äô√©l√©ments n√©goci√©s lors de la premi√®re (et co√ªteuse) poign√©e de main. Cette optimisation se fait cot√© serveur en conservant les ‚Äúsessions identifiers‚Äù cot√© serveur ou cot√© client avec un ‚Äúcookie‚Äù chiffr√©, le ‚Äúsession ticket‚Äù. Il faudra bien entendu ajuster la dur√©e de cache et/ou les timeouts (~ 1 jour).\n\nUne erreur fr√©quemment commise consiste √† ne pas int√©grer le certificat interm√©diaire (peu de CA s‚Äôautorise √† signer votre certificat avec leur CA Root) dans le certificat serveur ce qui a pour cons√©quence de stopper le render, ouvrir une nouvelle connexion tcp et https pour r√©cup√©rer ce dernier chez l‚Äôautorit√© de certification.\n\nL‚ÄôOSCP stappling permet lui d‚Äôinclure directement la r√©ponse OCSP et ainsi √©viter le m√™me probl√®me de blocage du rendu, connexion √† un tiers etc‚Ä¶\n\nL‚Äôutilisation hasardeuse de redirection 301 peut consid√©rablement augmenter le Time To First Byte de votre site, il est donc fortement conseill√© de bien analyser ses cha√Ænes de redirections (ex: https://domain.com =&amp;gt; https://www.domain.com =&amp;gt; https://www.domain.com) et d‚Äôutiliser HSTS. Ce header √©mis par le serveur permettra au navigateur de mettre en cache la d√©cision de redirection vers https.\n\nLe talk s‚Äôest termin√© par un tableau comparatif fort int√©ressant des serveurs HTTP et des CDNs concernant tous ces aspects.\n\nQuelques liens suppl√©mentaires:\n\n\n  https://www.ssllabs.com/ssltest/\n  https://www.feistyduck.com/books/bulletproof-ssl-and-tls/\n\n\n\n\nMonitoring: the math behind bad behavior\n\nSlides : the math behind bad behavior\n\nLa d√©tection d‚Äôanomalies dans les flux continus de donn√©es de type Time Series n‚Äôest pas une chose ais√©e.\n\nSe baser sur un percentile, une moyenne ou une mediane uniquement ne permet pas de capturer les ph√©nom√®nes de saisonnalit√© et d‚Äôanomalies.\n\nTh√©o nous a propos√© une m√©thode de d√©tection de ces derni√®res appel√©e ‚Äúlurching windows‚Äù. Sur des fen√™tres de temps glissantes, on applique la m√©thode CUSUM (Cumulative Sum), qui somme les donn√©es en affectant un poids relatif (en r√©alit√© la probabilit√© que cette valeur existe).\n\nA voir: https://en.wikipedia.org/wiki/CUSUM\n\n\n\nWhat ops can learn from design - Robert Treat (OmniTI)\n\nSlides : What ops can learn from design\n\n‚ÄúUn designer est quelqu‚Äôun qui design‚Äù.\n\nDerri√®re cette lapalissade se cache en r√©alit√© plusieurs concepts importants √† int√©grer pour toutes personnes produisant un code, un service utilis√© par un tiers.\n\nNous sommes tous des designers. Il est donc indispensable de mettre en oeuvre 3 m√©canismes simples pour faciliter l‚Äôutilisation de votre code/service.\n\nLe ‚ÄúFeedback‚Äù: le bon code de retour lors de l‚Äô√©chec d‚Äôun script, un message intelligible et contextualis√© dans un log d‚Äôerreur applicatif, le ‚Äúnatural Mapping‚Äù: -d dans une option en ligne de commande pour indiquer ‚Äìdatabase, et le ‚Äúforce functions‚Äù: sous Unix, kill est par d√©faut non destructif, il faut forcer avec kill -9 pour tuer d√©finitivement un processus, tout comme on vous force √† fermer la porte de votre micro-onde pour le mettre en marche.\n\nConf√©rence int√©ressante qui vous fera sentir moins coupable de ne pas savoir si il fallait pousser ou tirer une porte :)\n\n\n\nStatistical Learning-based Automatic Anomaly Detection @Twitter\n\nArun Kejariwal est maintenant un habitu√© de la Velocity, j‚Äôavais particuli√®rement appr√©ci√© sa pr√©sentation l‚Äôann√©e derni√®re √† Londres sur la d√©tection d‚Äôanomalies chez twitter.\n\nL‚Äôojectif est toujours le m√™me: pr√©dire la capacit√© pour ajouter du mat√©riel en datacenter, d√©tecter des √©v√©nements particulier, distinguer le spam du trafic normal etc‚Ä¶\n\nLeur m√©thode est relativement identique √† ce qui avait √©t√© pr√©sent√© l‚Äôann√©e derni√®re: sur deux semaines de donn√©es on applique un traitement du signal pour d√©composer et filtrer la saisonnalit√©. Il ‚Äúsuffit‚Äù ensuite d‚Äôappliquer une regression ou un ESD sur les r√©sidus pour d√©tecter d‚Äô√©ventuelles anomalies.\n\nChose √† savoir: Twitter va publier un package R contenant ces fonctions et algorithmes, qui seront donc utilisables par le commun des mortels !\n\nConclusion\n\nUne premi√®re journ√©e int√©ressante et intense, sous le soleil de Barcelone !\n\n\n\nLe r√©sum√© de la seconde journ√©e est √©galement disponible.\n"
} ,
  
  {
    "title"    : "Configuration dynamique avec Symfony ExpressionLanguage",
    "category" : "",
    "tags"     : " configuration, symfony, cytron",
    "url"      : "/symfony-expression-language",
    "date"     : "November 17, 2014",
    "excerpt"  : "Gr√¢ce √† notre bundle MonologExtra, nous avons la possibilit√© d‚Äôinclure des informations statiques dans le contexte de nos logs.\nNous souhaiterions maintenant avoir aussi d‚Äôautres informations plus dynamiques comme le nom de l‚Äôutilisateur.\n\nPour ce...",
  "content"  : "Gr√¢ce √† notre bundle MonologExtra, nous avons la possibilit√© d‚Äôinclure des informations statiques dans le contexte de nos logs.\nNous souhaiterions maintenant avoir aussi d‚Äôautres informations plus dynamiques comme le nom de l‚Äôutilisateur.\n\nPour cela, nous avons donc ajout√© la possibilit√© de configurer une expression qui sera √©valu√©e par le composant ExpressionLanguage de Symfony de cette mani√®re :\n\nm6_web_monolog_extra:\n    processors:\n        userProcessor:\n            type: ContextInformation\n            config:\n                env: expr(container.getParameter(&#39;kernel.environment&#39;))\n                user: expr(container.get(&#39;security.context&#39;).getToken() ? container.get(&#39;security.context&#39;).getToken().getUser().getUsername() : &#39;anonymous&#39;)\n\nPour interpr√©ter cette expression, nous avons inject√© dans notre processeur Monolog une instance de ExpressionLanguage ainsi que le container :\n\nservices:\n  m6_web_monolog_extra.expression_language:\n    class: Symfony\\Component\\ExpressionLanguage\\ExpressionLanguage\n    public: false\n  m6_web_monolog_extra.processor.contextInformation:\n    abstract: true\n    class: M6Web\\Bundle\\MonologExtraBundle\\Processor\\ContextInformationProcessor\n    arguments:\n      - @service_container\n      - @m6_web_monolog_extra.expression_language\n    calls:\n      - [ setConfiguration, []]\n\nNous utilisons une d√©finition de service abstraite qui sert de mod√®le pour les services qui sont g√©n√©r√©s √† partir de la configuration s√©mantique g√©r√©e par l‚Äôextension du bundle :\n\n&amp;lt;?php\nforeach ($config[&#39;processors&#39;] as $name =&amp;gt; $processor) {\n    $serviceId = sprintf(&#39;%s.processor.%s&#39;, $alias, is_int($name) ? uniqid() : $name);\n\n    $definition = clone $container-&amp;gt;getDefinition(sprintf(&#39;%s.processor.%s&#39;, $alias, $processor[&#39;type&#39;]));\n    $definition-&amp;gt;setAbstract(false);\n\n    $tagOptions = [];\n    if (array_key_exists(&#39;channel&#39;, $processor)) {\n        $tagOptions[&#39;channel&#39;] = $processor[&#39;channel&#39;];\n    }\n    if (array_key_exists(&#39;handler&#39;, $processor)) {\n        $tagOptions[&#39;handler&#39;] = $processor[&#39;handler&#39;];\n    }\n    $definition-&amp;gt;addtag(&#39;monolog.processor&#39;, $tagOptions);\n\n    if (array_key_exists(&#39;config&#39;, $processor)) {\n        if ($definition-&amp;gt;hasMethodCall(&#39;setConfiguration&#39;)) {\n            $definition-&amp;gt;removeMethodCall(&#39;setConfiguration&#39;);\n            $definition-&amp;gt;addMethodCall(&#39;setConfiguration&#39;, [$processor[&#39;config&#39;]]);\n        } else {\n            throw new InvalidConfigurationException(sprintf(&#39;&quot;%s&quot; processor is not configurable.&#39;, $processor[&#39;type&#39;]));\n        }\n    }\n\n    $container-&amp;gt;setDefinition($serviceId, $definition);\n}\n\nEt l‚Äôexpression est finalement √©valu√©e par le processeur en utilisant le composant quand la valeur est de la forme expr(...), ceci permettant de garder une compatibilit√© ascendante avec les configurations statiques pr√©c√©dentes.\n\n&amp;lt;?php \nprotected function evaluateValue($value)\n{\n    if (preg_match(&#39;/^expr\\((.*)\\)$/&#39;, $value, $matches)) {\n        return $this-&amp;gt;expressionLanguage-&amp;gt;evaluate($matches[1], [&#39;container&#39; =&amp;gt; $this-&amp;gt;container]);\n    }\n    return $value;\n}\n\nAvec la configuration pr√©sent√©e au d√©but, nous r√©cup√©rons ainsi l‚Äôenvironnement et l‚Äôutilisateur connect√© dans le contexte de nos logs.\n\nMonologExtraBundle est disponible en open-source sur le compte GitHub de M6Web.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #7",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2014/11/13/m6web-dev-facts-7.html",
    "date"     : "November 13, 2014",
    "excerpt"  : "√áa faisait un moment ! Voici le retour des devfacts !\n\nReproduction\n\n  Je ne sais pas si √ßa corrige le bug qu‚Äôon ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nAu moins, c‚Äôest efficace\n\n  Quand je fais un ‚Äúecho $id‚Äù, √ßa affich...",
  "content"  : "√áa faisait un moment ! Voici le retour des devfacts !\n\nReproduction\n\n  Je ne sais pas si √ßa corrige le bug qu‚Äôon ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nAu moins, c‚Äôest efficace\n\n  Quand je fais un ‚Äúecho $id‚Äù, √ßa affiche l‚Äôid\n\n\nProprement sale\n\n  C‚Äôest pas forc√©ment plus propre, mais c‚Äôest moins sale\n\n\nA une vache pr√®s !\n\n  C‚Äôest √† peu pr√®s approximatif ‚Ä¶\n\n\nIl faut savoir ce qu‚Äôon veut\n\n  C‚Äôest pas pr√©vu pour √™tre utile\n\n\nC‚Äôest louche.\n\n  Commen√ßons par comprendre pourquoi le code fonctionne\n\n\nToi aussi fais du marketing ‚Ä¶\n\n  On pourrait leverager le ROI du big data avec de l‚Äôanalytics predictif.\n\n\nPour une fois ‚Ä¶.\n\n  Pour une fois c‚Äôest pas un bug ! c‚Äôest un truc qui marche.\n\n\nPooh\n\n  Au sujet d‚Äôune sombre histoire d‚Äôexpression de besoin \n‚ÄúJe ne peux pas toujours les aider √† faire leurs besoins‚Ä¶‚Äù\n\n\nComprends moi !\n\n  ‚ÄúComprends mon incompr√©hension !‚Äù\n\n\nROI !\n\n  Y a autant d‚Äôutilisateurs que de jour homme pour ce projet !\n\n\n"
} ,
  
  {
    "title"    : "Retour sur le forum PHP 2014 organis√© par l&#39;AFUP",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2014/11/12/retour-sur-le-forumphp2014.html",
    "date"     : "November 12, 2014",
    "excerpt"  : "M6Web √©tait pr√©sent en force avec 5 collaborateurs pr√©sent √† l‚Äô√©v√®nement. Voici un retour des conf√©rences qui nous ont le plus marqu√©es.\n\nVers des applications ‚Äú12 factor‚Äù avec Symfony et Docker\n\nCette session avait pour objectif de nous pr√©senter...",
  "content"  : "M6Web √©tait pr√©sent en force avec 5 collaborateurs pr√©sent √† l‚Äô√©v√®nement. Voici un retour des conf√©rences qui nous ont le plus marqu√©es.\n\nVers des applications ‚Äú12 factor‚Äù avec Symfony et Docker\n\nCette session avait pour objectif de nous pr√©senter la m√©thodologie du ‚Äútwelve-factor app‚Äù, √† travers des exemples concrets pour PHP √† l‚Äôaide de Symfony et Docker.\n\n‚ÄúThe twelve-factor app‚Äù est une suite de recommandations, ind√©pendante d‚Äôun langage de programmation particulier et pouvant s‚Äôappliquer √† toutes sortes de logiciels d√©velopp√©s en tant que service.\n\nSans revenir sur l‚Äôensemble de la pr√©sentation, voici un retour sur les 12 facteurs :\n\n\n  Codebase : une app = un repo (ou √©quivalent) servant de source √† tous les d√©ploiements (dev / preprod / recette / prod  etc.). Exemples : git, mercurial etc.\n  Dependencies : d√©claration explicite et compl√®te de l‚Äôarbre de d√©pendances, utilis√© uniform√©ment pour tous les environnements. Exemples : composer, npm etc.\n  Config : s√©paration stricte config/code (Resources, Backing services, Credentials, Hostname etc.). Exemples : parameters.yml pour Symfony 2 ou utilisation de variables d‚Äôenvironnement avec Docker notamment. Utilisation de fig pour l‚Äôorchestration des containers docker.\n  Backing Services : tous les services utilis√©s par l‚Äôapplication sont accessibles par le r√©seau. Il n‚Äôy a pas de distinction entre les ressources locales et distantes car toutes sont accessibles via URL et/ou Credentials. Exemples : MySQL, RabbitMQ, Postfix, Redis, S3 etc.\n  Build, release, run : s√©paration stricte entre\n    \n      ‚Äúbuild stage‚Äù : t√©l√©chargement d‚Äôune version du code et des d√©pendances. Exemples : ‚Äúdocker build‚Äù\n      ‚Äúrelease stage‚Äú : utilise le ‚Äúbuild‚Äù et le combine avec la configuration du d√©ploiement (une version sur un environnement). Exemple : ‚Äúdocker push‚Äù, utilisation de capistrano‚Ä¶\n      ‚Äúrun stage‚Äù : lancement de la ‚Äúrelease‚Äù sur l‚Äôenvironnement cible. Exemple ‚Äúdocker run‚Äù ou ‚Äúfig run‚Äù\n    \n  \n  Processes : chaque composant de l‚Äôapplication est ‚Äòsans √©tat‚Äô et ne doit pas partager directement des donn√©es. Tout doit √™tre partag√© en ‚Äúbacking service‚Äù.\n  Port binding : les services doivent √™tre disponibles en mettant √† disposition un port d‚Äôacc√®s, directement accessible. Cela permet une utilisation ais√©e en environnement de dev mais √©galement de r√©utiliser les services.\n  Concurrency : une application respectant les ‚Äú12 factor‚Äù est facilement scalable, quel que soit son type (web, worker, etc.) car elle repose sur des composants syst√®mes pour son pilotage.\n  Disposability : robustesse par le lancement et l‚Äôarr√™t rapide des services, pour rendre chacune de ses services scalables.\n  Dev/Pro parity : homog√©n√©it√© des environnements dev/prod et gain de temps pour la prise en main d‚Äôun projet. (mais le d√©veloppeur n‚Äôaura pas une vision pr√©cise de la configuration‚Ä¶ boite noire ?)\n  Logs : traitement des logs en tant que flux, utilis√©s par des services. Exemples : ELK, StatsD/Grafana etc.\n  Admin process : Ex√©cuter les t√¢ches de maintenance sur les m√™mes environnements/containers. Exemples : docker exec\n\n\nslides\n\nPersonnellement, j‚Äôai trouv√© cette conf√©rence vraiment riche et instructive. Peut-√™tre un peu plus d‚Äôexemples de configuration fig/docker aurait pu illustrer d‚Äôavantage.\n\nLa mesure ce n‚Äôest pas que pour le devops\n\nLes conf√©renciers ont commenc√© leur pr√©sentation sur un rappel de ce qu‚Äôest le Lean Startup, h√©ritier de la m√©thode Lean mise au point par Toyota. Nous connaissions la d√©marche Lean mais pas du tout son approche sp√©cifique au lancement d‚Äôun produit.\n\nLe concept pourrait se r√©sumer √† : la base du lean startup est de savoir √©couter -ses utilisateurs- car le succ√®s d√©pend d‚Äôun feedback mesurable.\n\nLe processus d‚Äôapplication est tr√®s simple : un cycle construit/mesure/apprend.\n\nS‚Äôen est logiquement suivi une √©num√©ration des mani√®res de mettre en oeuvre le processus en sachant prendre en compte les mesures qui importent (AAA, AARRR), plut√¥t que des ‚Äúmesures de vanit√©‚Äù (followers, nombre de visite,..).\n\nEnfin pour appliquer ces mesures, une pr√©sentation des outils a disposition a √©t√© faite.\n\n\n\nPHP dans les distributions RPM\n\nSlides\n\nCette session avait comme objectif de faire un √©tat de PHP dans les distributions RPM RHEL/Centos/Fedora.\n\nRHEL / Centos :\n\n\n  Objectif de stabilit√© √† 10 ans\n  Stabilit√© binaire et de configuration sur la dur√©e de vie de la distribution\n  RHEL : version payante avec support (contacts avec les ing√©nieurs RedHat, ressources en ligne, cycles de mises √† jour garantis etc.).\n  Centos : m√™me code que RHEL (juste recompil√©) mais uniquement un support communautaire (comme fedora, ubuntu, suse‚Ä¶).\n  RHEL 5 : PHP 5.1 / RHEL 6 : PHP 5.3 / RHEL 7 : PHP 5.4\n  Application des patchs de s√©curit√©s sur les versions anciennes de PHP pendant 10 ans.\n  Possibilit√© d‚Äôutiliser des repos tiers pour choisir une version plus r√©cente sp√©cifique (comme ceux de Remi Collet) - mais pas de support officiel.\n  Distributions plut√¥t destin√©es √† des applicatifs maintenus sur le long terme.\n\n\nFedora 21+ :\n\n\n  3 sous distributions : Workstation / Server / Cloud\n  Derni√®re version de PHP (PHP5.5 pour f20 et PHP5.6 pour f21)\n  Int√©gration continue de PHP dans les cycles de F√©dora. Permet d‚Äô√©viter les r√©gressions.\n\n\nA venir : Software Collections (scl) permet d‚Äôavoir TOUTES les versions de PHP souhait√©es simultan√©ment sur la m√™me installation de Fedora. Vraiment prometteur !\n\nExemple d‚Äôutilisation des SCL en cli :\n\nscl enable php56 -f myscript56.php\nscl enable php56 bash\nscl enable php53 -f myscript53only.php\nscl enable php53 bash\n\nDans une config apache :\n\n&amp;lt;VirtualHost *:80&amp;gt;\n    ServerName php56scl\n    \n    # Redirect to FPM server in php56 SCL\n    &amp;lt;FilesMatch \\.php$&amp;gt;\n    SetHandler &quot;proxy:fcgi://127.0.0.1:9006&quot;\n    &amp;lt;/FilesMatch&amp;gt;\n&amp;lt;/VirtualHost&amp;gt;\n\nFrameworks: A History of Violence\n\n\n\nFrancois Zaninotto nous a offert un vrai show en se mettant dans la peau d‚Äôun homme politique candidat √† la pr√©sidence du parti des d√©veloppeurs. Avec beaucoup d‚Äôhumour il a fait un retour sur l‚Äô√©volution (sa propre √©volution ?) du d√©veloppement web et son futur hypoth√©tique, tout en distillant (son programme) de pr√©cieux conseils pour √™tre un meilleur d√©veloppeur.\n\nSon programme :\n\n\n  Le domaine d‚Äôabord : lier son d√©veloppement m√©tier √† un minimum de tierce partie (pas facile √† faire !),\n  Dites non au full-stack (√ßa se discute !),\n  L‚Äôapplication plurielle : ne pas h√©siter √† m√©langer diff√©rents langages et diff√©rents projets dialoguant via http sur une m√™me application,\n  Repenser le temps : passons aux 32h pour nous permettre de faire de la veille.\n\n\nA la communaut√© PHP nous pourrions proposer une synth√®se (entendu ailleurs) : ‚Äúsoyons plus des d√©veloppeurs web que des d√©veloppeurs PHP, soyons plus des d√©veloppeurs que des d√©veloppeurs web‚Äù.\n\n\n\nRetour d‚Äôexp√©rience ARTE GEIE : d√©veloppement d‚ÄôAPI\n\nUne conf√©rence donn√©e par un de nos confr√®res d‚ÄôARTE sur des probl√©matiques tr√®s actuelles pour nous. Fran√ßois Dume a expliqu√© la strat√©gie de mise en place d‚Äôune API autour de JSON API et des microservices. L‚Äôutilisation de OpenResty et du langage Lua coupl√© √† un serveur oAuth en Symfony2 g√©rant la validation des tokens et le throttling.\n\nIl a ensuite expliqu√© en d√©tail l‚Äôimpl√©mentation de {json:api} dans Symfony2, en mettant en avant de nombreuses contributions open-source.\n\n\n\nUne conf√©rence didactique et claire.\n\n\n\nVDM, DevOps malgr√© moi\n\nMaxime Valette nous expliqu√© comment il a (√† 20 ans √† peine) cr√©e un business incroyable sur Internet et a surtout r√©ussi √† g√©rer une augmentation de 30 √† 40K visiteurs de plus chaque jour avec pratiquement juste sa b* et son c*.\n\n- Comment on fait ? \n- Comme on peut ! \n\n\nDe vrai qualit√© d‚Äôorateur pour Maxime et une conf tr√®s rafraichissante. Une d√©monstration de lean startup par l‚Äôexemple. M√™me si ce choix n‚Äôa pas √©t√© discut√©, PHP √©tait un choix naturel pour lui √† l‚Äô√©poque.\n\n\n\nAn introduction to the Laravel Framework for PHP.\n\nBien que Symfony soit tr√®s largement majoritaire en Europe, Laravel est tr√®s populaire en Am√©rique du Nord, c‚Äôest donc avec curiosit√© que nous avons assist√© √† cette pr√©sentation du framework faite par Dayle Rees, core developer.\n\nUne fois pass√© la tr√®s longue pr√©sentation des livres et autres activit√©s du conf√©rencier, nous avons eu droit √† une pr√©sentation g√©n√©rale du framework qui nous as fortement rappel√© Symfony1 : utilisation de singleton √† outrance, MagicBox (√©quivalent du sfContext), beaucoup de magie pour r√©duire la configuration (nom des contr√¥leurs).\n\nAu final, l‚Äôimpression laiss√©e est mitig√©e : certes, le seuil d‚Äôentr√©e est relativement r√©duit, tout est simple d‚Äôapparence, mais c‚Äô√©tait la m√™me chose pour Symfony1, et l‚Äôexp√©rience nous a montr√© que lorsque l‚Äôon essayait de sortir le framework des sentiers battus, cette simplicit√© devenait un vrai obstacle.\n\nAu final, Laravel est s√ªrement une alternative int√©ressante pour les nostalgiques de Symfony1, puisque le projet est actif et maintenu. Mais, pour les projets que nous d√©veloppons, Symfony2 reste une solution tout √† fait adapt√©e.\n\nLaisse pas trainer ton log !\n\nOlivier Dolbeau nous a fait un retour sur la probl√©matique d‚Äôacc√®s et l‚Äôinterpr√©tation des logs sur les serveurs de production.\n\n\n\nIl nous as donc pr√©sent√© la solution qu‚Äôil utilise, √† savoir la stack ELK, pour ElasticSearch/LogStash/Kibana, qui permet √† chaque serveur d‚Äôenvoyer ses logs vers un serveur central, qui a pour charge de les agr√©ger, et de permettre leur utilisation avanc√©e.\nFini la recherche dans des fichiers textes plats qu‚Äôil faut commencer par comprendre, d√©sormais vos applicatifs peuvent enrichir leurs logs, les envoyer sur un syst√®me d√©di√© √† la gestion des logs disposant de vraies interfaces de recherche et de consultation.\n\nNous avons √©t√© confort√©s dans notre id√©e, puisque nous mettons √©galement en oeuvre cette solution.\n\nTable ronde ‚ÄúEtat des lieux et avenir de PHP‚Äù\n\nPascal Martin a anim√© d‚Äôune main de ma√Ætre une table ronde sur l‚Äôavenir de PHP. Avec Jordi Boggiano, lead developer de Composer, Pierre Joye, core dev de PHP, Julien Pauli, release manager de PHP 5.5 et co-RM de PHP 5.6.\n\nLa communaut√© se pose beaucoup de questions sur le devenir de l‚Äôengine PHP et comment va √©voluer le langage. \nLes d√©bats ont √©t√© intenses et les invit√©s ont pu r√©pondre √† des questions pos√©es via Twitter. Au final peu de conclusions d√©finitives. On peut d√©duire que malgr√© les alternatives propos√©es par HHVM et HippyVM, la communaut√© reste majoritairement sur PHP et est toujours tr√®s friande d‚Äô√©volutions du langage et de sa performance. Les invit√©s de la table ronde ont exhort√©s les participants √† contribuer au code de PHP en nous fournissant pas mal de conseils.\n\n\n\nSlideshow Karaok√©\n\nUne honte ! En plus les slides n‚Äôavaient aucun sens ! :) Bravo √† Mc Kenny pour l‚Äôanimation.\n\n\n\n\n\nUn grand merci √† l‚ÄôAFUP pour ce joli √©v√®nement ! Retrouvez pas mal de ressources partag√©s pendant l‚Äôevent sur eventifier.\n"
} ,
  
  {
    "title"    : "Github Team Reviewer pour gagner la course aux Pull Requests",
    "category" : "",
    "tags"     : " outil, github, pull-requests, cytron, open-source",
    "url"      : "/github-team-reviewer-pull-requests.html",
    "date"     : "November 7, 2014",
    "excerpt"  : "Les PR, c‚Äôest le bien\n\nChez M6Web, nous utilisons Github Enterprise en interne pour nos projets priv√©s et Github pour nos projets open-source. Gr√¢ce √† ces outils, nous avons adopt√© de mani√®re syst√©matique l‚Äôusage des Pull Requests pour faire relir...",
  "content"  : "Les PR, c‚Äôest le bien\n\nChez M6Web, nous utilisons Github Enterprise en interne pour nos projets priv√©s et Github pour nos projets open-source. Gr√¢ce √† ces outils, nous avons adopt√© de mani√®re syst√©matique l‚Äôusage des Pull Requests pour faire relire et valider notre code par nos collaborateurs. La qualit√© de nos d√©veloppements a ainsi √©t√© grandement am√©lior√©e au fil du temps.\n\nOui, mais‚Ä¶\n\nNous utilisons aussi HipChat pour communiquer au quotidien et rapidemment au sein de nos √©quipes. Chaque cr√©ation de Pull Request √©met une notification sur HipChat. Cependant, le nombre de pull requests initi√©es augmente avec le temps et chacun tend √† ignorer peu √† peu les notifications ou y fait moins attention. Les Pull Requests s‚Äôaccumulent sur certains projets et nous n‚Äôavions, jusqu‚Äô√† pr√©sent, pas vraiment de moyen pour lister par √©quipe toutes les PR en cours. Cela nous permettrait d‚Äôavoir une vue globale et d‚Äô√™tre plus r√©actifs et rigoureux.\n\nIl y a bien le nouveau Pull Requests Dashboard de Github avec ses filtres de recherche avanc√©e qui permet de r√©pertorier toutes ses PR ou celles d‚Äôune organisation. Cette mise √† jour n‚Äôest pas encore entr√©e en application dans Github Enterprise. Mais surtout, nous avons plusieurs √©quipes au sein d‚Äôune m√™me organisation et nous voulons pouvoir les g√©rer de mani√®re ind√©pendante : cette fonctionnalit√© ne r√©soud pas notre probl√©matique.\n\nGTR !\n\nNous avons donc d√©velopp√© Github Team Reviewer, un outil ultra simple mais efficace qui permet en un coup d‚Äô≈ìil de voir toutes les PR de ses √©quipes et leur statut, qu‚Äôelles soient sur un Github Entreprise interne ou sur Github. Le projet utilise AngularJS et l‚ÄôAPI fournit par Github. L‚Äôinstallation se fait sur n‚Äôimporte quel serveur web et requiert npm (via Node.js) pour builder l‚Äôapplication gr√¢ce √† Bower et Gulp.js.\n\nL‚Äôapplication propose volontairement un nombre limit√© de param√®tres de configuration √©ditables dans le fichier config/config.json:\n\n\n  l‚Äôintervalle de rafraichissement de la liste des PR,\n  la liste des √©quipes en d√©finissant pour chacune :\n    \n      son nom,\n      les utilisateurs Github concern√©s,\n      les organisations Github concern√©es,\n      l‚Äôurl de l‚ÄôAPI √† interroger (pour Github Enterprise, par d√©faut l‚Äôurl de l‚ÄôAPI public de Github est utilis√©e),\n      un token utilisateur (utile pour augmenter le rate limit de l‚ÄôAPI public).\n    \n  \n\n\nUne select box permet de basculer d‚Äôune √©quipe √† une autre tr√®s facilement.\n\nGithub Team Reviewer est disponible en open-source sur le compte Github de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Providers AngularJS et configuration dynamique",
    "category" : "",
    "tags"     : " configuration, angular, cytron",
    "url"      : "/surcharger-un-provider-angular",
    "date"     : "October 9, 2014",
    "excerpt"  : "Nous avons eu besoin de surcharger un provider AngularJS ‚Äì AnalyticsProvider ‚Äì pour le rendre configurable dynamiquement en fonction d‚Äôun param√®tre de la route. Le service $route n‚Äô√©tant pas disponible dans la phase de configuration d‚ÄôAngularJS, i...",
  "content"  : "Nous avons eu besoin de surcharger un provider AngularJS ‚Äì AnalyticsProvider ‚Äì pour le rendre configurable dynamiquement en fonction d‚Äôun param√®tre de la route. Le service $route n‚Äô√©tant pas disponible dans la phase de configuration d‚ÄôAngularJS, il a fallu ruser‚Ä¶\n\nLe but est donc de changer la m√©thode $get de ce provider afin de lui ajouter notre d√©pendance et ainsi finir notre configuration.\n\nIl existe bien une m√©thode decorator() dans le service d‚Äôinjection de dependance d‚ÄôAngularJS, mais celle-ci ne permet que de d√©corer des services et pas leurs providers.\n\nNous allons donc mettre les mains dans l‚Äô$injector pour r√©cup√©rer et modifier √† la vol√©e le provider :\n\nangular.module(&#39;myModule&#39;)\n  .config(function ($injector) {\n    var AnalyticsProvider = $injector.get(&#39;AnalyticsProvider&#39;);\n    var $get              = AnalyticsProvider.$get;\n    // ...\n  });\n\nMaintenant que nous avons le $get, il faut le modifier pour ajouter notre d√©pendance. Et c‚Äôest assez simple vu qu‚Äôil utilise l‚Äôannotation sous forme de tableau :\n\n// https://github.com/revolunet/angular-google-analytics/blob/e821407fe0436677cb42eafd5b338d767990b723/src/angular-google-analytics.js#L99\nthis.$get = [&#39;$document&#39;, &#39;$rootScope&#39;, &#39;$location&#39;, &#39;$window&#39;, function($document, $rootScope, $location, $window) {\n\nNous devons modifier ce tableau en ajoutant nos d√©pendances et en rempla√ßant la fonction :\n\n// la fonction d&#39;origine est le dernier √©l√©ment du tableau\nvar origFn = $get[$get.length - 1];\n// on la remplace par notre d√©pendance\n$get[$get.length - 1] = &#39;$route&#39;;\n// on ajoute notre nouvelle fonction √† la fin du tableau\n$get[$get.length] = function () {\n    // $route est le dernier argument\n    var $route = arguments[arguments.length - 1];\n    // on fait notre traitement\n    AnalyticsProvider.setAccount($route.current.params.partner ? &#39;partner-account&#39; : &#39;own-account&#39;);\n    // et qui rappelle la fonction originale\n    return origFn.apply(AnalyticsProvider, arguments);\n};\n\nOn peut noter l‚Äôutilisation de l‚Äôobjet arguments qui permet de rester g√©n√©rique et de garder la compatibilit√© en cas de changement des d√©pendances du module surcharg√©.\n\nGr√¢ce √† cette astuce, notre service Analytics est maintenant configur√© dynamiquement selon nos souhaits avant son utilisation.\n"
} ,
  
  {
    "title"    : "Am√©liorer la webperf de son application JS avec GruntJs",
    "category" : "",
    "tags"     : " webperf, angular, grunt, performance",
    "url"      : "/2014/09/30/ameliorer-la-webperf-de-son-application-js-avec-gruntjs.html",
    "date"     : "September 30, 2014",
    "excerpt"  : "L‚Äôun des principaux probl√®mes que nous rencontrons sur nos d√©veloppement chez M6Web est la tenue en charge.\nQuand elles sont li√©es √† des sites √† fort trafic ou √† une √©mission t√©l√© (#effetcapital), nos applications doivent √™tre con√ßues pour support...",
  "content"  : "L‚Äôun des principaux probl√®mes que nous rencontrons sur nos d√©veloppement chez M6Web est la tenue en charge.\nQuand elles sont li√©es √† des sites √† fort trafic ou √† une √©mission t√©l√© (#effetcapital), nos applications doivent √™tre con√ßues pour supporter des pics de charge plus ou moins importants.\n\nC‚Äôest une probl√©matique qu‚Äôon croit souvent li√©e uniquement aux backends (scripts serveurs, bases de donn√©es etc), en oubliant souvent que le front-end est aussi, voir tout autant concern√©.\n\nC‚Äôest notamment le cas pour une ‚ÄúSingle Page Application‚Äù Angular.Js que nous d√©veloppons en ce moment.\n\nL‚Äôobjectif ici est d‚Äôavoir une application qui ex√©cutera le moins de requ√™tes possible pour s‚Äôafficher, et qui ensuite sera quasiment autonome en ne faisant que le minimum de requ√™tes HTTP. Ceci afin de garantir, que lorsque quelqu‚Äôun charge l‚Äôapplication, l‚Äôexp√©rience est quasi parfaite, m√™me si entre temps, le CDN ou l‚Äôh√©bergement conna√Æt une surcharge temporaire.\n\nL‚Äôautre avantage de diminuer le nombre d‚Äôappels HTTP, c‚Äôest aussi de limiter l‚Äôimpact de la latence r√©seau, encore plus imposante dans notre cas, car notre cible est majoritairement mobile.\n\nPour les applications ‚ÄúClient-Side‚Äù, nous utilisons Grunt.Js pour automatiser toutes les t√¢ches de d√©veloppement, build, d√©ploiement ‚Ä¶ (Nul doute que la m√™me chose existe avec Gulp pour les plus Hipsters d‚Äôentre vous). Grunt regorge de plugins en tout genre pour automatiser √©norm√©ment de choses cot√© WebPerf, commen√ßons par le plus √©vident et le plus simple.\n\nP.S : Je passe volontairement l‚Äôinstallation/initialisation de Grunt ainsi que de ses plugins. Le web regorgeant de ressources l√† dessus.\n\nMinification HTML\n\nAfin de gagner quelques octets, nous allons minifier (suppression des espaces, retours charriot, et commentaires HTML) notre code HTML g√©n√©r√©.\nPour ceci, nous utilisons le plugin grunt-contrib-htmlmin.\n\noptions: {\n        collapseWhitespace: true,\n        collapseBooleanAttributes: true,\n        removeCommentsFromCDATA: true,\n        removeOptionalTags: true,\n        removeComments: true\n      }\n\nMinification CSS\n\nM√™me chose au niveau des feuilles de styles avec grunt-contrib-cssmin.\n\nCompression des images\n\nAfin d‚Äô√©viter d‚Äôavoir des images ¬´ brutes ¬ª de taille trop importante, on utilise grunt-contrib-imagemin pour compresser au build nos diff√©rentes images, afin de gagner quelques ko toujours pr√©cieux.\n\nInlining des images d‚Äôinterface\n\nDans notre cas, o√π nous souhaitons r√©duire le nombre de requ√™tes HTTP superflues, nous avons opt√© pour l‚Äôinlining des images dites d‚Äôinterface (boutons d‚Äôactions, picto etc).\n\nNous utilisons aussi le pr√©-compilateur CSS Less, par simplicit√© et pour √©viter le DRY CSS.\nNous avons donc un premier fichier .less qui va contenir toutes les images d‚Äôinterface sous cette forme :\n@facelessImg: url(&#39;images/faceless.jpg‚Äô);\n\nLe plugin Grunt grunt-css-url-embed sera configur√© pour remplacer les urls pr√©sentes dans ce fichier par la version data-uri (=source de l‚Äôimage encod√©e en base64).\nIl est important de se concentrer uniquement sur les images ¬´ d‚Äôinterface ¬ª, car le poids des images sera ici augment√© d‚Äôenviron 30% (√† cause du base64).\n\nDans notre CSS principale, on pourra ensuite mettre cette image en background d‚Äôune classe CSS :\n\n.faceless {\n  background-image: @facelessImg;\n}\n\nEt dans notre code HTML, on pourra placer l‚Äôimage de la mani√®re suivante :\n&amp;lt;span class=&quot;faceless&quot;&amp;gt;&amp;lt;/span&amp;gt;\n\nGr√¢ce √† cet ajout, nous √©conomiserons une requ√™te HTTP pour chacune des images.\n\nVersionning des assets\n\nUne autre bonne pratique est de versionner les assets en production. Cela signifie, donner un nom unique √† chaque fichier statique (JS, CSS, image), ne changeant pas, tant que le fichier en question n‚Äôaura pas subi de modification, dans le but de pouvoir mettre un cache navigateur (Expire) et un cache CDN/Proxy Cache le plus long possible (Cache-control).\nNous passerons de /images/info.jpg √† /images/a21992d7.info.jpg par exemple.\n\nNous utilisons ici le plugin grunt-rev (en combinaison avec grunt-usemin), qui va d‚Äôabord versionner les assets ayant chang√©s, et ensuite, mettre √† jour les r√©f√©rences vers les fichiers en question dans tous vos fichiers HTML, CSS, JS.\n\nConcat√©nation des fichiers JS\n\nDirectement dans le code HTML, toujours avec le plugin grunt-usemin, vous allez pouvoir mettre des commentaires HTML pour d√©finir quels ensembles de fichiers devront √™tre concat√©n√©s.\nLa bonne pratique est d‚Äôavoir un fichier app.js avec son code maison, un fichier vendor.js avec les librairies tierces, et potentiellement un fichier de config.js\nEtant donn√© que dans notre cas, 99% du poids Js est concentr√© dans ‚ÄúVendor‚Äù, nous avons d√©cid√© de concat√©ner l‚Äôensemble dans un seul fichier.\n\n&amp;lt;!-- build:js(.tmp) scripts/risingstar.js --&amp;gt;\n  &amp;lt;script src=&quot;bower_components/jquery/dist/jquery.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;bower_components/angular/angular.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;config.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;app.js&quot;&amp;gt;&amp;lt;/script&amp;gt; \n‚Ä¶.\n&amp;lt;!-- endbuild ‚Äî&amp;gt;\n\nInlining des templates\n\nPour finir, vous aurez peut-√™tre remarqu√©, si vous d√©veloppez des SPA avec Angular, ou un autre framework moderne, un changement de route (ou d‚Äô√©tat) de votre application (ou l‚Äôaffichage d‚Äôune directive) va impliquer des appels XHR pour charger les nouveaux templates √† afficher. La bonne pratique ici √©tant de d√©couper au maximum tous les templates dans des fichiers distincts.\nCela ne pose pas de probl√®me en temps normal, mais dans notre cas, cela ne respecte pas nos ambitions de d√©part.\n\nAngular a la particularit√© de permettre d‚Äôutiliser la balise script pour charger des templates :\n\n&amp;lt;script type=&quot;text/ng-template&quot; id=&quot;views/info.html&quot;&amp;gt;Code HTML du template&amp;lt;/script&amp;gt;\n\nSi votre routeur ou une directive demande un template, avant de v√©rifier si le fichier existe, Angular v√©rifiera si une balise &amp;lt;script type=‚Äôtext/ng-template‚Äô&amp;gt; a √©t√© d√©clar√©e avec l‚Äôidentifiant correspondant au chemin demand√©.\n\nGrunt via le plugin grunt-angular-inline-templates, nous permet d‚Äôautomatiser cette t√¢che au build, afin de regrouper dans le index.html du build, tous les templates dans un script avec l‚Äôid correspondant au chemin du fichier html original. De cette mani√®re, nous n‚Äôavons plus aucun appel HTTP √† faire pendant toute l‚Äôutilisation de l‚Äôapplication.\nAttention toutefois, cela signifie que le poids du fichier HTML original va forc√©ment augmenter.\n\nConclusion\n\nComme vous avez pu le voir, nous avons grandement optimis√© notre application, en utilisant simplement des plugins Grunt √† notre disposition. Nous travaillons donc sur un espace de d√©veloppement respectant toutes les bonnes pratiques (d√©coupages des fichiers JS, CSS, HTML au maximum, code comment√© ‚Ä¶) et toutes les op√©rations d‚Äôoptimisation sont automatiquement effectu√©es au build, fait avant chaque d√©ploiement.\n\nAttention, cela signifie aussi que votre projet en production devient relativement diff√©rent de celui que vous test√© en d√©veloppement. Il devient donc important de mettre en place des tests fonctionnels sur le build de production (avec Protractor par exemple, ou m√™me Behat), et de tester r√©guli√®rement la bonne g√©n√©ration et le bon fonctionnement du build de prod.\n"
} ,
  
  {
    "title"    : "Tests E2E sur son application AngularJS avec Protractor",
    "category" : "",
    "tags"     : " qualite, tests, javascript, angular, protractor, cytron",
    "url"      : "/tests-e2e-application-angularjs-protractor.html",
    "date"     : "September 24, 2014",
    "excerpt"  : "Familier des tests fonctionnels avec Behat et Atoum pour des applications majoritairement PHP, nous l‚Äô√©tions beaucoup moins avec les tests end-to-end pour des applications pures Javascript, qui plus est, sous AngularJS. Les tests end-to-end ou tes...",
  "content"  : "Familier des tests fonctionnels avec Behat et Atoum pour des applications majoritairement PHP, nous l‚Äô√©tions beaucoup moins avec les tests end-to-end pour des applications pures Javascript, qui plus est, sous AngularJS. Les tests end-to-end ou tests e2e ne sont autres que des tests fonctionnels dans la domaine du Javascript. L‚Äôobjectif de cet article est de montrer le cheminement que nous avons emprunt√© pour mettre en place ces tests sur une de nos applications et pour g√©rer les difficult√©s qui en ont d√©coul√©.\n\nLe contexte\n\nIl s‚Äôagit d‚Äôune application web pr√©sentant des √©crans diff√©rents √† l‚Äôutilisateur en fonction des donn√©es contenues dans un fichier distant requ√™t√© √† intervalle r√©gulier court (quelques secondes). L‚Äôutilisateur est invit√© ou non √† agir avec les vues, principalement en appuyant sur des boutons, qui changent l‚Äô√©tat interne de l‚Äôapplication et peut, a posteriori, influer sur les √©crans suivants.\n\nMettre en place Protractor\n\nLa premi√®re √©tape consiste √† installer Protractor, framework de tests e2e d√©di√© √† AngularJS et utilisant Node.js. Si vous utilisez Grunt pour g√©rer les t√¢ches de build de votre projet, il suffit d‚Äôex√©cuter la commande :\n\nnpm install grunt-protractor-runner --save-dev\n\nPuis on cr√©e le fichier de configuration dans le projet :\n\n/* protractor-local.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;\n};\n\nTous les tests e2e de notre application sont √©crits dans des fichiers javascript dont le nom est suffix√© par .e2e.js. Nous avons en effet fait le choix d‚Äôune architecture modulaire se retrouvant dans l‚Äôorganisation des dossiers de notre projet : les fichiers de tests e2e se trouvent dans les m√™mes r√©pertoires que les controllers auxquels ils sont rattach√©s 1.\n\nUn navigateur pour mes tests\n\nPour ex√©cuter ses tests dans les conditions r√©elles de son application, il faut un navigateur. Nous d√©veloppons sur un serveur distant en SSH. Le seul navigateur utilisable est donc un browser headless, le plus connu et utilis√© √©tant PhantomJS. Cependant, combin√© √† Protractor, ce dernier est particuli√®rement instable pour le moment et il n‚Äôest pas recommand√© de l‚Äôutiliser. Nous optons donc pour Chrome (via le plugin chromedriver). N√©cessitant une interface graphique, nous ne pourrons donc pas lancer nos tests sur le serveur de d√©veloppement mais nous devrons le faire en local sur nos machines.\n\n/* protractor-local.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;,\n  maxSessions: 1,\n  multiCapabilities: [\n    { browserName: &#39;chrome&#39; }\n  ]\n};\n\nOn installe les binaires n√©cessaires au lancement de Chrome via Protractor :\n\n./node_modules/grunt-protractor-runner/node_modules/.bin/webdriver-manager update\n\nPuis on ajoute les t√¢ches Grunt :\n\n/* Gruntfile.js */\ngrunt.initConfig({\n  connect: {\n    dist: {\n      options: {\n        port: 9000,\n        hostname: &#39;localhost&#39;,\n        base: &#39;dist&#39;\n      }\n    }\n  },\n  protractor: {\n    local: {\n      options: {\n        configFile: &quot;protractor-local.conf.js&quot;\n      }\n    }\n  }\n});\n\ngrunt.registerTask(&#39;test&#39;, [\n  &#39;build&#39;,\n  &#39;connect:dist&#39;,\n  &#39;protractor:local&#39;\n]);\n\nInt√©gration continue\n\nL‚Äôensemble de nos projets joue automatiquement leurs tests sur un serveur Jenkins commun qui ne dispose pas de navigateurs graphiques. Nous aurions pu mettre en place au sein de notre infrastructure un serveur Selenium pour r√©pondre √† cette probl√®matique. Mais les contraintes du projet ne nous autorisaient pas √† y consacrer le temps n√©cessaire. Nous avons donc opt√© pour une solution tiers plus rapide √† mettre en ≈ìuvre : SauceLabs, plateforme de tests h√©berg√©e dans le ‚Äúcloud‚Äù.\n\nUne fois enregistr√© sur le site, on cr√©e un nouveau fichier de configuration Protractor :\n\n/* protractor-saucelabs.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;,\n  allScriptsTimeout: 30000,\n  jasmineNodeOpts: {\n    defaultTimeoutInterval: 60000\n  },\n  maxSessions: 1,\n  sauceUser: &#39;mySauceUser&#39;,\n  sauceKey: &#39;mySauceKey&#39;,\n  multiCapabilities: [\n    {\n      browserName: &#39;chrome&#39;,\n      platform: &#39;Linux&#39;\n    },\n    {\n      browserName: &#39;firefox&#39;,\n      platform: &#39;Linux&#39;\n    },\n    {\n      browserName: &#39;safari&#39;,\n      platform: &#39;OS X 10.9&#39;\n    },\n    {\n      browserName: &#39;chrome&#39;,\n      platform: &#39;Windows 8.1&#39;\n    }\n  ]\n};\n\nNotons que l‚Äôon peut lancer ses tests sur autant de couples OS/navigateurs que l‚Äôon souhaite en remplissant le tableau multiCapabilities. Le fichier de configuartion Grunt doit √™tre adapt√© pour lancer SauceConnect, l‚Äôinterface entre SauceLabs et l‚Äôapplication, avant le d√©marrage des tests :\n\n/* Gruntfile.js */\ngrunt.initConfig({\n  connect: {\n    dist: {\n      options: {\n        port: 9000,\n        hostname: &#39;localhost&#39;,\n        base: &#39;dist&#39;\n      }\n    }\n  },\n  protractor: {\n    local: {\n      options: {\n        configFile: &#39;protractor-local.conf.js&#39;\n      }\n    },\n    saucelabs: {\n      options: {\n        configFile: &#39;protractor-saucelabs.conf.js&#39;\n      }\n    }\n  },\n  run: {\n    installsc: {\n      options: {\n        wait: true\n      },\n      cmd: &#39;bash&#39;,\n      args: [\n        &#39;-c&#39;,\n        &#39;test -d sc-4.2-linux || (wget https://saucelabs.com/downloads/sc-4.2-linux.tar.gz &amp;amp;&amp;amp; tar xvf sc-4.2-linux.tar.gz)&#39;\n      ]\n    },\n    sauceconnect: {\n      options: {\n        wait: false,\n        quiet: true,\n        ready: /Sauce Connect is up/\n      },\n      cmd: &#39;./sc-4.2-linux/bin/sc&#39;,\n      args: [\n        &#39;-u&#39;,\n        &#39;mySauceUser&#39;,\n        &#39;-k&#39;,\n        &#39;mySauceKey&#39;\n      ]\n    }\n  }\n});\n  \n\ngrunt.registerTask(&#39;test-e2e&#39;, function (target) {\n  var tasks = [\n    &#39;build&#39;,\n    &#39;connect:dist&#39;\n  ];\n\n  if (target === &#39;local&#39;) {\n    tasks.push(&#39;protractor:local&#39;);\n  } else {\n    tasks.push(&#39;run:installsc&#39;);\n    tasks.push(&#39;run:sauceconnect&#39;);\n    tasks.push(&#39;protractor:saucelabs&#39;);\n    tasks.push(&#39;stop:sauceconnect&#39;);\n  }\n\n  grunt.task.run(tasks);\n});\n\nAvec cette configuration, nous lan√ßons les tests en local sur notre machine avec la commande grunt test-e2e:local ou √† distance sur SauceLabs avec grunt test-e2e.\n\nNotre premier test\n\nLe premier test que nous avons √©crit pour valider l‚Äôarchitecture est plut√¥t basique :\n\ndescribe(&#39;Controller: MainCtrl&#39;, function () {\n  it(&#39;should work&#39;, function () {\n    browser.get(browser.baseUrl);\n    expect(true).toBe(true);\n  })\n});\n\nOn remarque que l‚Äô√©criture d‚Äôun test e2e utilise, comme les tests unitaires, la syntaxe du framework Jasmine : un bloc describe regroupe une suite de tests d√©finis dans des blocs it. Les variables de configuration d√©finies dans les fichiers de configuration Protractor sont utilisables via la variable globale browser, variable qui nous permettra d‚Äôentretenir le lien entre nos tests et le code ex√©cut√© dans le navigateur. Pour mieux appr√©hender les √©tapes du processus et les erreurs qui se produisent, il est en effet tr√®s important de bien comprendre la s√©paration entre le code Javascript ex√©cut√© dans Node.js via Protractor, qui correspond au d√©roulement des tests, et le code Javascript de notre application qui lui est ex√©cut√© dans le browser et avec lequel on ne peut interagir depuis les tests que par certaines fonctions du framework (element, executeScript, addMockModule, etc.)2. Ce sont deux univers d‚Äôex√©cution bien distincts.\n\nD√©bugger avec Protractor\n\nLorsque vous lancerez les tests en local, vous remarquerez que Chrome est r√©ellement ex√©cut√© mais vous ne verrez pas grand chose car l‚Äôaffichage est bien trop rapide. Il est possible de mettre des points d‚Äôarr√™t dans ses tests pour y voir plus clair et pour, par exemple, consulter la console Javascript du navigateur. Pour cela, il faut utiliser la fonction browser.debugger() comme point d‚Äôarr√™t et ajouter l‚Äôoption debug dans la configuration Grunt :\n\n/* Gruntfile.js */\nprotractor: {\n  local: {\n    options: {\n      configFile: &#39;protractor-local.conf.js&#39;,\n      debug: true\n    }\n  }\n}\n\nPour passer d‚Äôun point d‚Äôarr√™t √† l‚Äôautre, on saisit c comme continue. Notez que cela ne fonctionnera pas si vous avez plus d‚Äôun navigateur dans le tableau multiCapabilities de votre configuration.\n\nOn peut √©galement ajouter l‚Äôoption --debug √† la commande grunt test-e2e:local pour afficher l‚Äôensemble des requ√™tes lanc√©es par l‚Äôapplication.\n\nMocker sa config\n\nComme souvent dans les projets AngularJS, nous utilisons un module pour d√©finir nos variables de configuration :\n\nangular.module(&quot;config&quot;, [])\n  .constant(&quot;config&quot;, {\n    &#39;ma_variable&#39;: &#39;une_valeur&#39;\n  });\n\nDans les tests e2e, on veut tout tester, en particulier les comportements qui diff√®rent en fonction des valeurs de configuration. Comment faire puisque ce module est charg√© une fois pour toute au lancement de l‚Äôapplication ? Protractor introduit la fonction addMockModule qui permet de bouchonner √† la vol√©e un module Angular.\n\nit(&#39;comportement avec une autre valeur&#39;, function () {\n  browser.addMockModule(&#39;config&#39;, function () {\n  \tangular.module(&#39;config&#39;, []).constant(&#39;config&#39;, {\n    \t\t&#39;ma_variable&#39;: &#39;une_autre_valeur&#39;\n  \t});\n  });\n\n  // mon test\n  \n  browser.removeMockModule(&#39;config&#39;);\n});\n\nMocker le service $http\n\nDans notre application, un fichier externe est requ√™t√© r√©guli√®rement via le service Angular $http. AngularJS fournit d√©j√† un mock complet de ce service nomm√© $httpBackend. Pour y avoir acc√®s, il faut ajouter la d√©pendance angular-mocks en devDependencies dans son fichier bower.json et inclure le fichier bower_components/angular-mocks/angular-mocks.js dans l‚Äôapplication en d√©veloppement. $httpBackend permet de d√©finir quels appels HTTP doivent √™tre intercept√©s et quelles r√©ponses doivent √™tre renvoy√©es.\n\nLa difficult√© dans notre cas r√©side dans le fait de pouvoir simuler le changement d‚Äô√©tat du fichier distant dans un m√™me test pour pouvoir v√©rifier les changements de vue qui en d√©coulent. Il est possible de le faire directement via $httpBackend moyennant quelques acrobaties, mais la librairie HttpBackend simplifie grandement son utilisation pour ce type de tests 3.\n\nvar HttpBackend = require(&#39;httpbackend&#39;);  \nvar backend;\n\ndescribe(&#39;Test workflow&#39;, function() {  \n  beforeEach(function() {\n    backend = new HttpBackend(browser);\n  });\n\n  afterEach(function() {\n    backend.clear();\n  });\n\n  it(&#39;should display result when status is changed to RESULT&#39;, function(done) {\n    backend.whenJSONP(/status.json/).respond({status: &#39;initial&#39;});\n\n    browser.get(&#39;/&#39;);\n\n    var result = element(by.binding(&#39;result&#39;));\n    expect(result.getText()).toEqual(&#39;no result&#39;);\n    \n    backend.whenJSONP(/status.json/).respond({status: &#39;result&#39;, percentage: 70});\n    \n    browser.wait(function () {\n      return browser.getLocationAbsUrl().then(function (currentUrl) {\n        return currentUrl === &#39;https://localhost:9000/#/result&#39;;\n      });\n    }, 5000).then(function () {\n      expect(result.getText()).toEqual(&#39;70 %&#39;);\n      done();\n    });\n  });\n});\n\nOui, mais‚Ä¶\nProtractor nous a √©t√© indispensable pour impl√©menter les tests fonctionnels sur notre application car son int√©gration avec AngularJS offre des possibilit√©s que les autres frameworks de tests fonctionnels n‚Äôont pas. On pense principalement √† la synchronisation qui est mise en ≈ìuvre entre les tests et l‚Äôinitialisation d‚ÄôAngular dans la page (‚Äúwait for angular‚Äù). Cependant, avec le recul que l‚Äôon peut avoir sur notre projet :\n\n\n  il faut l‚Äôavouer, Protractor n‚Äôest pas aussi simple √† mettre en place que Behat par exemple,\n  le debuggage est assez p√©nible car les messages d‚Äôerreur sont souvent peu verbeux et, c‚Äôest l‚Äôinconv√©n√©nient de tester du javascript avec du javascript, on ne sait pas toujours o√π se situe l‚Äôerreur (dans les tests ou dans le code applicatif ?),\n  Protractor est parfois instable avec les webdrivers utilis√©s, ce qui nous oblige √† relancer les tests manuellement,\n  nos tests dans SauceLabs sont (tr√®s) lents, ce qui nous a contraint √† la longue √† r√©duire le nombre de navigateurs test√©s (am√©liorant par la m√™me occasion la stabilit√© des tests).\n\n\n\n  \n    \n      Scalable code organization in AngularJS¬†&amp;#8617;\n    \n    \n      Protractor API¬†&amp;#8617;\n    \n    \n      Angular e2e tests, Mock your backend.¬†&amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Contr√¥lez facilement votre coh√©rence de code sur votre projet Symfony2 avec coke",
    "category" : "",
    "tags"     : " code sniffing, coke, Symfony2",
    "url"      : "/2014/08/05/verifier-la-coherence-du-code-d-un-projet-symfony2-avec-coke.html",
    "date"     : "August 5, 2014",
    "excerpt"  : "Pour qu‚Äôun projet persiste dans le temps, il est important que le style de codage soit le m√™me. Et quand vous vous reposez sur des outils, autant faire en sorte que le style de codage retenu soit proche, si ce n‚Äôest le m√™me, que les briques que vo...",
  "content"  : "Pour qu‚Äôun projet persiste dans le temps, il est important que le style de codage soit le m√™me. Et quand vous vous reposez sur des outils, autant faire en sorte que le style de codage retenu soit proche, si ce n‚Äôest le m√™me, que les briques que vous utilisez. Et dans le cas o√π vous utilisez un framework, c‚Äôest d‚Äôautant plus important.\n\nAvec Symfony2, c‚Äôest d‚Äôautant plus facile que l‚Äôarchitecture des bundles est tr√®s marqu√©e, et qu‚Äôun coding guide est publi√©.\n\n√áa, c‚Äôest pour la th√©orie, mais en pratique, si ce n‚Äôest pas super simple, automatique, une somme de toutes petites erreurs apparaissent et le sentiment d‚Äôabandon s‚Äôinstalle rapidement.\n\nCoke\n\nIl y a un peu plus d‚Äôun an, chez M6Web, nous avons d√©velopp√© coke pour configurer simplement l‚Äôex√©cution de PHP_CodeSniffer.\n\nDepuis quelques mois, il est possible d‚Äôinstaller coke via Composer :\n\n{\n  &quot;require&quot;: {\n    &quot;m6web/coke&quot;: &quot;~1.2&quot;\n  }\n}\n\nL‚Äôavantage de passer par Composer, c‚Äôest que coke va lui-m√™me installer PHP_CodeSniffer en tant que d√©pendance Composer (dans le dossier vendor), permettant de ne pas avoir √† suivre la fastidieuse proc√©dure d‚Äôinstallation via PEAR.\n\nInstaller un coding standard via Composer\n\nLorsque nous voulons utiliser un coding standard qui n‚Äôest pas inclus par d√©faut avec PHP_CodeSniffer, il est possible de l‚Äôinstaller en utilisant Composer\n\nSymfony2-coding-standard\n\nChez M6Web, nous maintenons le standard Symfony2-coding-standard qui permet de valider que le code d‚Äôun projet respecte les coding standard de Symfony2.\n\nPour rendre √† C√©sar ce qui appartient √† C√©sar, nous avons r√©cup√©r√© la base du standard telle que cr√©√© par opensky.\n\nSi nous avons d√©cid√© de le forker, c‚Äôest que la structure ne correspondait pas √† ce qui est n√©cessaire pour une installation de ce standard via Composer\n\nProc√©dure compl√®te, pas-√†-pas\n\nCr√©er le fichier composer.json suivant :\n\n{\n  &quot;require-dev&quot;: {\n    &quot;m6web/coke&quot;                       : &quot;~1.2&quot;,\n    &quot;m6web/symfony2-coding-standard&quot;   : &quot;~1.1&quot;,\n  }\n}\n\nInstaller les d√©pendances Composer :\n\ncomposer install\n\nCr√©er le fichier .coke suivant :\n\n# Standard used by PHP CodeSniffer (required)\nstandard=vendor/m6web/symfony2-coding-standard/Symfony2\n\nIl est d√©sormais possible d‚Äôappeler la commande suivante pour valider le style de codage de votre projet\n\n./vendor/bin/coke\n\nConclusion\n\nAvec cette technique, il est tr√®s simple de valider le style de codage d‚Äôun projet. Du coup, plus d‚Äôexcuse pour ne pas le faire ;)\n\nBonus\n\nL‚Äôid√©al, pour ne jamais commiter un code ne respectant pas les conventions de codage, est d‚Äôutiliser les hooks de commit pour que cette v√©rification soit faite automatiquement.\n\nLa mani√®re la plus simple de le faire est d‚Äôajouter la ligne ./vendor/bin/coke dans le fichier .git/hooks/pre-commit, mais cette m√©thode a le d√©faut de v√©rifier tout le projet, et pas uniquement le code modifi√© et √† commiter.\n\nPour aller plus loin, vous pouvez vous inspirer du script suivant qui ne lance coke que sur les fichiers dans le ‚Äústaging‚Äù de Git (les fichiers √† commiter).\n\n"
} ,
  
  {
    "title"    : "M6Web √©tait pr√©sent au PHPTour Lyon 2014",
    "category" : "",
    "tags"     : " afup, phptour, conference, video",
    "url"      : "/2014/06/25/m6web-etait-au-phptour-lyon-2014.html",
    "date"     : "June 25, 2014",
    "excerpt"  : "Le Lundi 23 et Mardi 24 juin a eu lieu l‚Äô√©v√©nement PHP de l‚Äôann√©e √† Lyon : le PHPTour Lyon.\n√Ä cette occasion, les √©quipes d‚ÄôM6Web ont pr√©sent√© un talk, dont voici les slides et vid√©os :\n\n#Nouveau socle pour une nouvelle vie, chez M6Web (par Kenny ...",
  "content"  : "Le Lundi 23 et Mardi 24 juin a eu lieu l‚Äô√©v√©nement PHP de l‚Äôann√©e √† Lyon : le PHPTour Lyon.\n√Ä cette occasion, les √©quipes d‚ÄôM6Web ont pr√©sent√© un talk, dont voici les slides et vid√©os :\n\n#Nouveau socle pour une nouvelle vie, chez M6Web (par Kenny Dits)\n\nLa seconde conf√©rence de @techM6Web a √©t√© tenue par Kenny Dits (@kenny_dee) : ‚ÄúNouveau socle pour une nouvelle vie, chez M6Web‚Äù.\n\n\n\n\n\nVoir les commentaires sur Joind.in\n\nNous avons aussi retrouv√© une bonne partie des d√©veloppeurs de l‚Äô√©quipe (anciens ou actuels) qui ont jou√© le jeu de la borne photo Pixiway mise √† disposition :\n\n\n\n#Conclusion\n\nL‚ÄôAfup a encore r√©alis√© un boulot consid√©rable cette ann√©e, pour accoucher sans aucun doute, du meilleur PHP Tour jamais fait.\nBravo √† toute la team pour l‚Äôorganisation sans faille, et aux autres speakers pour la qualit√© de leurs talks.\n"
} ,
  
  {
    "title"    : "M6Web sera pr√©sent au PHPTour Lyon 2014",
    "category" : "",
    "tags"     : " afup, phptour",
    "url"      : "/2014/05/15/m6web-sera-present-au-phptour-lyon-2014.html",
    "date"     : "May 15, 2014",
    "excerpt"  : "M6Web sera bien repr√©sent√© au PHPTour 2014 organis√© par l‚ÄôAFUP et est tr√®s heureux de soutenir l‚Äô√©v√®nement en √©tant sponsor Argent.\n\n\n\nVenez nombreux augmenter votre pilosit√© faciale, tel un vrai sysadmin.\n\nFaites le plein d‚Äôanecdotes croustillant...",
  "content"  : "M6Web sera bien repr√©sent√© au PHPTour 2014 organis√© par l‚ÄôAFUP et est tr√®s heureux de soutenir l‚Äô√©v√®nement en √©tant sponsor Argent.\n\n\n\nVenez nombreux augmenter votre pilosit√© faciale, tel un vrai sysadmin.\n\nFaites le plein d‚Äôanecdotes croustillantes et d√©couvrez l‚Äôhistoire de M6Web Lyon avec Kenny Dits.\n"
} ,
  
  {
    "title"    : "Babitch, the story behind our table soccer web application",
    "category" : "",
    "tags"     : " opensource, babyfoot, angularjs, d3js, symfony",
    "url"      : "/2014/04/23/babitch-the-story-behind-our-table-soccer-web-application.html",
    "date"     : "April 23, 2014",
    "excerpt"  : "At M6Web, we love playing foosball!\nWe have one old (incredibly strong) soccer table in our ¬´ fun room ¬ª, and at lunch time, a part of us enjoy playing it.\n\nThe soccer table in enterprise is awesome for a lot of things:\n\n\n  Team building between e...",
  "content"  : "At M6Web, we love playing foosball!\nWe have one old (incredibly strong) soccer table in our ¬´ fun room ¬ª, and at lunch time, a part of us enjoy playing it.\n\nThe soccer table in enterprise is awesome for a lot of things:\n\n\n  Team building between each players,\n  Don‚Äôt think about work (almost) when we are playing,\n  Fun! a lot of!\n  Attract good people ‚Ä¶\n\n\nOur rules are simple : Doubles (4 players) only, and the first team at ten win.\n\nSo, as programmers, we tend to have software ideas for each questions in our life ! and the questions we had with foosball were:\n\n\n  Who is the best player?\n  How many goals did I score ?\n  Who won the most games?\n  ‚Ä¶\n\n\nSo, we begin to talk several months ago about a foosball app, allowing us to record each games, and each goals, to compute lot of stats about our games.\nEveryone had good ideas about it, but someone had many more than us. Even more that it ruined all motivation of the other folks wanting to do work on this webapp, because the first steps to begin the app with all the features we had in mind was too big for all of us ‚Ä¶\n\nSo before having started the project, it was over !\n\nFew months later, an undercover part of the team began the development of a more simple and stupid foosball application : it just allowed to select 4 players, and to register matches by telling who scored and what kind of goal it was (normal or own goal).\nThis was ugly as possible, but it worked ! And it was an awesome start for improving it and giving back all the motivation developpers had lost before !\n\nBabitch was born :)\n\nArchitecture\n\nAt the beggining, there was only one project, with the server API, and the client part.\nThis was bad. It was a good way to start fast, but a bad way to allow each project to evolve on its own side.\nSo we decided to divide the Babitch Project into two parts, Babitch, the server API, and BabitchClient, a client to consume Babitch Api data.\n\nBabitch, the API\n\nThe Babitch API is a simple PHP/MySQL project, based on Symfony2, Doctrine, FosRestBundle, and NelmioApiDocBundle.\nThe documentation generated by the NelmioApiDocBundle is available at /api/doc and allows to view each Route of the API, and to send requests on them with the Sandbox menu.\nThe API is functionnaly tested by Behat.\nA Vagrant file is available if you want to try it easily. (More information in the Readme.md)\n\nBabitch, the Client\n\nThe Babitch Client, is the ‚Äúofficial‚Äù client for the API.\nThe client side doesn‚Äôt require any webserver, it‚Äôs just an Angular.js app, doing REST queries to the Babitch API thanks to Restangular.\n\nWe used Yeoman to bootstrap the project because it helps in many ways:\n\n\n  adds grunt configuration and support for serving, building and testing the project,\n  have generators for controllers, service, etc ‚Ä¶\n\n\nFor development on this client, we are heavily using Grunt, Karma for Unit Testing, and the new Protactor for E2E testing.\n\nThe client is divided into four major parts:\n\nNew Game\n\n\n\nThis is the main feature, it allows to begin a new game, choose 4 players, and assign each goals to the right players.\nThe game is saved only when the last goal is made.\nEach player is represented by his Gravatar for a nicer UI :)\n\nLive\n\n\n\nThe table soccer is not at the same floor than we are, so we are using our monitoring screen to show at lunch time the live state of the table score !\n\nWith a screen on this feature, we could see:\n\n\n  if a game is played right now,\n  who‚Äôs playing,\n  the live score,\n  for each goal, in live, who scored, and on which side :)\n\n\nThe live part use a Faye server, which you can host freely on Heroku (more information on Readme.md). You configure a channel name, and all actions done on the new game view are forwarded to the Faye server, forwarded back to the client listening on the Live view. It just rocks !\n\nStats\n\n\n\nAll of this would be useless if you don‚Äôt have any way to compare your ‚Ä¶ stats to others competitors, right ?\nSo stats section is here for that purpose.\nIt shows you :\n\n\n  the last played match,\n  a sortable table by each stat of each player,\n  data visualization on each type of stats,\n  an individual card by player,\n  a sortable table by each stat of each team.\n\n\nAnd for each player and team, you have access to a lot of stats:\n\n\n  Elo Ranking (According to the Bonzini Usa Player Ranking/Rating System),\n  Percentage of goals per ball played,\n  Percentage of victory/loose per game,\n  Number of games played,\n  Team Goalaverage,\n  ‚Ä¶\n\n\nThere was some long debate about how stats have to be computed : on the server side ? (not really the goal of a REST Api ‚Ä¶), or on the client side ?\nAfter some successfull tries, we decided to compute stats on the client side, in an Angular.Js Service.\nThe service loads the last 300 games, and computes team and player stats fastly.\nWe also use the awesome D3.Js framework for data visualization.\n\nAdmin\n\nProbably the first screen you will need, for a simple way to add, modify, or delete players.\n\nConclusion\n\nWorking on our free (or lunch) time on a side-project like this is awesome!\nIt allows us to use several technologies or tools we don‚Äôt use often, to improve our knowledge on tons of other things, to view the project on the product owner side and to mix teams who don‚Äôt work a lot together.\n\nOne other thing interesting to remember about that project: Keep things as simple and small as possible (according to KISS principles) ! And only when your simple project is done, iterate by adding more and more features.\n\nTry and contribute?\n\nSo, if like us, you love foosball and play at work, give it a try, and give us feedback if you use it :)\n\nAlso, it‚Äôs open-source, so you‚Äôre welcome to contribute on BabitchClient and BabitchService, by posting/reading/commenting issue and PR.\n\nThanks ! :)\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un Lead Developpeur / Architecte web (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/2014/04/11/m6web-lyon-recherche-un-lead-developpeur-architecte-web-h-f-en-cdi.html",
    "date"     : "April 11, 2014",
    "excerpt"  : "\n\nM6Web Lyon recrute, en CDI, un Lead D√©veloppeur LAMP, avec une tr√®s forte expertise sur les technologies PHP 5.5, MySQL, Symfony2, GIT, et capable d‚Äôencadrer une petite √©quipe de d√©veloppement.\n\nNous recherchons quelqu‚Äôun de tr√®s passionn√©, enth...",
  "content"  : "\n\nM6Web Lyon recrute, en CDI, un Lead D√©veloppeur LAMP, avec une tr√®s forte expertise sur les technologies PHP 5.5, MySQL, Symfony2, GIT, et capable d‚Äôencadrer une petite √©quipe de d√©veloppement.\n\nNous recherchons quelqu‚Äôun de tr√®s passionn√©, enthousiaste, et mordu de veille technologique : un missionnaire de l‚Äôopen source, un int√©griste de la qualit√© de code, des tests unitaires et fonctionnels, et un architecte de projets aguerri avec une premi√®re approche en m√©thodologie de d√©veloppement agile, et une exp√©rience de management de d√©veloppeurs.\n\nSi, en plus, vous √™tes un malade de l‚Äôoptimisation back-end et front-end, que des technologies comme Node.js vous √©moustillent, que, malgr√© la qualit√© de MySQL, vous envisagez dans certains cas des solutions NoSQL alternatives (Mongo, Redis‚Ä¶), votre profil nous int√©resse !\n\nVenez apporter vos comp√©tences aux √©quipes techniques de M6Web en travaillant sur des sites √† tr√®s forte charge (6Play, m6.fr, clubic.com, jeuxvideo.fr ‚Ä¶), et partagez-les gr√¢ce √† des conf√©rences internes ou externes et des articles sur notre blog.\n\nSi vous avez les qualit√©s requises et l‚Äôenvie de nous rejoindre, allez sur le lien ci-dessous et faites nous part de votre CV, de votre compte github, et d‚Äôune lettre attrayante pour nous motiver √† vous rencontrer.\n\nSi vous souhaitez postuler ou avoir plus d‚Äôinfos : https://www.groupem6.fr/ressources-humaines/offres-emploi/lead-developpeur-architecte-web-h-f-229879.html\n"
} ,
  
  {
    "title"    : "Conf√©rence au Symfony Live 2014 : Symfony √† la t√©l√©",
    "category" : "",
    "tags"     : " symfony, conference",
    "url"      : "/2014/04/10/SfLive2014-symfony-a-la-tele.html",
    "date"     : "April 10, 2014",
    "excerpt"  : "Invit√© par SensioLabs au Symfony Live 2014, j‚Äôai pu pr√©senter le travail des √©quipes de M6Web, et de nos partenaires, autour de Symfony 2.\n\nVoici les slides de la conf√©rence :\n\n \n\nL‚Äôenregistrement audio (avec les slides) est disponible ici :\n\n\n\nJe...",
  "content"  : "Invit√© par SensioLabs au Symfony Live 2014, j‚Äôai pu pr√©senter le travail des √©quipes de M6Web, et de nos partenaires, autour de Symfony 2.\n\nVoici les slides de la conf√©rence :\n\n \n\nL‚Äôenregistrement audio (avec les slides) est disponible ici :\n\n\n\nJe tiens √† remercier toutes les personnes avec qui j‚Äôai pu √©changer autour des th√©matiques de la conf√©rence. J‚Äôai eu beaucoup de plaisir √† m‚Äôapercevoir que de nombreux conf√©renciers citaient le travail de M6Web pendant leur talk !\n\nRendez vous au phptour √† Lyon pour les prochaines conf√©rences techniques M6Web.\n"
} ,
  
  {
    "title"    : "Utilisation du StatsdBundle avec le composant Console",
    "category" : "",
    "tags"     : " statsd, php, symfony, console, monitoring, cytron",
    "url"      : "/2014/03/04/utilisation-du-statsdbundle-avec-la-console.html",
    "date"     : "March 4, 2014",
    "excerpt"  : "Le StatsdBundle\n\nChez M6Web, nous utilisons StatsD et nous avons cr√©√© un bundle pour cela.\nCe bundle permet d‚Äôajouter facilement des incr√©ments et des timings dans StatsD sur des √©v√©nements Symfony2.\n\nDe la Request √† la console\n\nOr pour des raison...",
  "content"  : "Le StatsdBundle\n\nChez M6Web, nous utilisons StatsD et nous avons cr√©√© un bundle pour cela.\nCe bundle permet d‚Äôajouter facilement des incr√©ments et des timings dans StatsD sur des √©v√©nements Symfony2.\n\nDe la Request √† la console\n\nOr pour des raisons de performances, lors des √©v√©nements Symfony, les incr√©ments et timings sont seulement stock√©s dans une variable et ne sont envoy√©s r√©ellement √† StatsD que pendant le kernel.terminate qui se d√©roule apr√®s l‚Äôenvoi de la r√©ponse HTTP au client.\nCeci pose un probl√®me pour les √©v√©nements lanc√©s depuis une commande Symfony puisque en console, il n‚Äôy pas de Request et donc pas de kernel.terminate.\nNous avons envisag√© d‚Äôutiliser l‚Äô√©v√©nement console.terminate pour palier √† cela, mais cela pose deux probl√®mes :\n\n\n  pour une commande qui est cens√©e tourner ind√©finiment (par exemple un consumer), on ne veut pas attendre la fin de la commande pour envoyer les donn√©es,\n  dans le cas d‚Äôune exception pendant la commande, l‚Äô√©v√©nement console.terminate est lanc√© avant console.exception.\n\n\nLa premi√®re solution √©tait donc d‚Äôappeler manuellement $container-&amp;gt;get(&#39;m6_statsd&#39;)-&amp;gt;send() dans la commande ou dans un ConsoleExceptionListener mais cela nous fait perdre le principal int√©r√™t du StatsdBundle √† savoir le d√©couplage entre la commande et le client StatsD.\n\nLa seconde solution a donc √©t√© de modifier le StatsdBundle et d‚Äôajouter une configuration au niveau de l‚Äô√©v√©nement pour forcer l‚Äôenvoi instantan√© des donn√©es.\n\nAinsi, avec la configuration suivante :\n\nclients:\n    event:\n        console.exception:\n            increment:      mysite.command.&amp;lt;command.name&amp;gt;.exception\n            immediate_send: true\n        m6kernel.exception:\n            increment: mysite.errors.&amp;lt;status_code&amp;gt;\n\nL‚Äôincr√©ment mysite.command.&amp;lt;command.name&amp;gt;.exception sera envoy√© en temps r√©el, alors que les autres comme mysite.errors.&amp;lt;status_code&amp;gt; continueront √† √™tre envoy√©s pendant kernel.terminate.\n"
} ,
  
  {
    "title"    : "Refonte de notre syst√®me de vote",
    "category" : "",
    "tags"     : " api, symfony, redis, monitoring, qualite, cytron",
    "url"      : "/2014/02/18/refonte-de-notre-systeme-de-vote.html",
    "date"     : "February 18, 2014",
    "excerpt"  : "Notre syst√®me de vote est utilis√© d‚Äôune part pour g√©rer l‚Äôensemble des questions et des r√©ponses associ√©es utilis√©es dans nos quizz et d‚Äôautre part pour r√©colter le nombre de votes des internautes lors des jeux concours.\n\nActuellement, le trafic g...",
  "content"  : "Notre syst√®me de vote est utilis√© d‚Äôune part pour g√©rer l‚Äôensemble des questions et des r√©ponses associ√©es utilis√©es dans nos quizz et d‚Äôautre part pour r√©colter le nombre de votes des internautes lors des jeux concours.\n\nActuellement, le trafic g√©n√©r√© par cette fonctionnalit√© varie entre quelques votes par minute la nuit √† quelques dizaines de votes par seconde lors des premi√®res parties de soir√©e.\n\nHistorique\n\nComme souvent, les besoins ont r√©guli√®rement √©volu√© depuis la mise en place initiale du syst√®me en 2009, faisant parfois prendre des chemins tortueux √† l‚Äôimpl√©mentation technique. Au fil des demandes, notre syst√®me a par exemple d√ª stocker ses donn√©es dans nos forums pour une fonctionnalit√© qui a ensuite √©t√© rapidement abandonn√©e.\n\nL‚Äôann√©e 2012 a vu l‚Äôarriv√©e du second √©cran : √† l‚Äôaide de l‚Äôapplication gratuite ad√©quate, les p√©riph√©riques mobiles peuvent d√©sormais se synchroniser avec l‚Äô√©mission en cours de visionnage, en direct ou en diff√©r√©, sur la TV ou sur le web (la synchronisation se fait par la bande son). Cette synchronisation nous permet de pusher instantan√©ment sur les p√©riph√©riques mobiles du contenu adapt√© √† ce que le t√©l√©spectateur regarde : le d√©tail de la recette que le cuisinier pr√©pare dans Top Chef ou un sondage concernant la derni√®re trouvaille linguistique d‚Äôun ch‚Äôtit face √† sa ch‚Äôtite.\n\nLe second √©cran s‚Äôannon√ßait alors comme une source importante de trafic suppl√©mentaire pour notre syst√®me de vote. Effectivement, en plus du trafic historique g√©n√©r√© par les sites web, nous allions aussi recevoir tous les votes provenant des p√©riph√©riques mobiles.\n\nCe nouveau trafic a une saisonnalit√© tr√®s marqu√©e : il est principalement pr√©sent en d√©but de soir√©e et reste tr√®s d√©pendant du programme diffus√© et de la contribution apport√©e.\n\nProbl√©matique\n\nLa principale probl√©matique venait de l‚Äôarchitecture des bases de donn√©es MySQL. √âtant fortement coupl√©es sur l‚Äôensemble de la plateforme, la moindre d√©faillance de l‚Äôune d‚Äôelles, due √† une surcharge sur un sondage, risquait de p√©naliser les internautes de tous nos autres sites (un sondage du second √©cran pouvait donc impacter l‚Äôexp√©rience utilisateur de Clubic).\n\nLe code √©tait aussi fortement coupl√© entre nos diff√©rentes applications : l‚Äôaction PHP d‚Äôun vote √©tait ex√©cut√©e sur la m√™me plateforme que notre BO permettant √† tous nos web services de fonctionner ainsi qu‚Äôaux contributeurs d‚Äôajouter du contenu. Une surchage sur les votes aurait donc pu entrainer des perturbations sur le fonctionnement global du site m6.fr et de ses web services, donc de beaucoup de produits par extension.\n\nPour r√©sumer, l‚Äôimbrication du code et des bases de donn√©es dans l‚Äôusine logiciel ne permettait pas de calibrer le syst√®me de vote pour qu‚Äôil puisse recevoir la charge attendue par le second √©cran.\n\nC‚Äôest donc d√©but 2013 que Kenny Dits m‚Äôa contact√© pour que nous trouvions une solution permettant de d√©coupler le syst√®me de vote tout en faisant √©voluer son architecture interne afin qu‚Äôil puisse facilement s‚Äôadapter √† la charge inconstante du second √©cran.\n\nSolution\n\nNous avons alors con√ßu un nouveau service d√©di√© uniquement √† la gestion des questions, r√©ponses et votes des utilisateurs. Ce nouveau service Polls est autonome, ce qui nous permet de le d√©coupler compl√®tement de notre usine logicielle avec laquelle il communique via une API REST.\n\nConcernant le stockage des donn√©es, nous avons simplement choisi un moteur tr√®s performant qui supporterait la charge sur une seule machine bien calibr√©e. Cela nous √©vitait alors les probl√©matiques complexes de clustering. Mais nous devions tout de m√™me stocker quelques informations relationnelles : il fallait donc avoir acc√®s √† quelques primitives nous permettant d‚Äô√©muler les relations minimum entre nos donn√©es. Redis s‚Äôest donc impos√© comme la solution ad√©quate. Cela reste malgr√© tout une solution th√©oriquement insatisfaisante, car non r√©ellement scalable. Mais en pratique, les tr√®s bonnes performances de Redis permettent de r√©pondre √† (bien plus que) nos attentes.\n\nLe code se trouve, pour sa part, compl√®tement isol√© sur son propre serveur.\nComme ce service est compl√®tement stateless et que notre base de donn√©e est centralis√©e et suffisamment performante, nous pouvons donc facilement ajouter ou supprimer des serveurs web selon la charge attendue : on peut dire qu‚Äôen pratique le service Polls est scalable horizontalement.\n\nLorsque l‚Äôarchitecture mise en place permet de r√©partir la charge sur un nombre variable de machines, le contrat est rempli : ce n‚Äôest plus qu‚Äôune question d‚Äôargent pour supporter n‚Äôimporte quelle charge. Et comme tout le monde le sait : l‚Äôargent n‚Äôest pas un probl√®me, c‚Äôest une solution.\n\nD√©veloppement\n\nLe service Polls a √©t√© d√©velopp√© en PHP avec Symfony et le FOSRestBundle. Nous avons d‚Äôabord suivi certaines r√©f√©rences, puis nous avons ensuite d√©velopp√© un micro ORM maison pour faire persister nos donn√©es dans Redis et enfin nous avons monitor√© tous ce que l‚Äôon pouvait √† l‚Äôaide de notre bundle d√©di√©.\n\nUne attention toute particuli√®re a √©t√© port√©e √† la qualit√© avec des tests unitaires couvrant un maximum de code et des tests fonctionnels couvrant la plupart des cas d‚Äôutilisation des clients. Les nombreuses mises en production journali√®res pendant la phase d‚Äôoptimisation ont ainsi √©t√© grandement facilit√©es, notamment gr√¢ce √† la s√©r√©nit√© apport√©e par l‚Äôint√©gration continue.\n\nMise en production\n\nL‚Äôint√©gration de ce nouveau service Polls a cependant √©t√© bien plus longue que son d√©veloppement. Nous l‚Äôavons d‚Äôabord mis en production en doublon de l‚Äôancien syst√®me : toutes les √©critures √©taient faites sur les deux syst√®mes, mais l‚Äôancien √©tait encore la r√©f√©rence lors de la lecture des r√©sultats par les clients.\n\nPuis apr√®s deux semaines, lorsque nous avons valid√© l‚Äôexacte corr√©lation entre les deux courbes du nombre de votes par minute √† l‚Äôaide de Graphite, nous avons alors chang√© les clients pour qu‚Äôils viennent lire les r√©sultats sur le service Polls.\n\nEncore deux semaines plus tard, lorsque tout √©tait valid√© et que nous avions d√©velopp√© et ex√©cut√© un script d‚Äôimport de l‚Äôhistorique, nous avons d√©branch√© l‚Äôancien syst√®me.\n\nL‚Äôint√©gration a donc √©t√© au moins trois fois plus longue, et donc couteuse, que le d√©veloppement du service en lui-m√™me.\n\nOptimisation\n\nLa premi√®re optimisation est simplement conceptuelle : nous avons concentr√© la criticit√© sur une seule route, celle qui est utilis√©e par chaque client pour voter. Il est ainsi plus simple de mesurer et donc d‚Äôam√©liorer les performances du service Polls. Cette route est critique parce qu‚Äôelle est utilis√©e par tous les clients, qu‚Äôelle ne peut pas √™tre cach√©e et qu‚Äôil faut √©crire des donn√©es en base lors de chaque appel.\n\nIl existait plusieurs pistes d‚Äôoptimisation connues (syst√®me de queue, node.js, etc.) mais dans une optique KISS, nous avons d‚Äôabord opt√© pour l‚Äôutilisation des technologies en place pour ensuite interpr√©ter les r√©sultats r√©cup√©r√©s lors des tests de charge et s‚Äôadapter si besoin.\n\nDans un premier temps, nous avons l√©g√®rement ajust√© notre mod√®le de donn√©es pour limiter le nombre d‚Äôaction √† r√©aliser sur la base de donn√©es : nous avons seulement deux instructions Redis de complexit√© constante O(1) √† r√©aliser pour chaque vote. Puis nous avons utilis√© les transactions pour grouper ces deux instructions et √©viter la latence d‚Äôune connexion suppl√©mentaire vers notre serveur Redis.\n\nNous avons enfin supprim√© la v√©rification de deux contraintes d‚Äôint√©grit√© sans importance. Le code retour en cas d‚Äôerreur est juste un peu moins coh√©rent (400 au lieu de 422) mais cela n‚Äôimpacte ni l‚Äôint√©grit√© des votes ni la s√©curit√© du service.\n\n\n\n\n\nAfin de savoir si nous n‚Äôavions pas compl√®tement pris une mauvaise direction dans notre utilisation de Symfony, nous avons alors fait appel √† Alexandre Salom√©, consultant SensioLabs, pour auditer notre code.\n\nLors de cette journ√©e, durant laquelle nous avons beaucoup appris, nous avons simplement d√©sactiv√© tous les bundles que nous n‚Äôutilisions pas r√©ellement en production : principalement Twig. Cela a occasionn√© une l√©g√®re modification de notre code car le FOSRestBundle n√©cessite Twig pour afficher les erreurs m√™me lorsque celles-ci sont en JSON.\n\nUne fois cette modification apport√©e, nous avons gagn√© les ultimes millisecondes nous permettant de passer sous la barre symbolique des 10ms de temps de r√©ponse sur notre route critique.\n\n\n\nVous remarquerez que nous avons d‚Äôabord d√©ploy√© le syst√®me en production avant de chercher √† l‚Äôoptimiser : nous pouvions ainsi mesurer en temps r√©el l‚Äôimpact de nos d√©veloppements sur une multitude d‚Äôindicateurs dont le temps de r√©ponse.\n\nMise en pratique\n\nLe service Polls a facilement tenu la charge pour la premi√®re √©mission mettant en avant le second √©cran : un √©pisode de Hawa√Ø 5-0 durant lequel les internautes pouvaient choisir le coupable avec un sondage (sur leur t√©l√©phone, tablette ou PC).\n\n\n\nPlus pr√©cis√©ment, nous sommes mont√©s √† 150 requ√™tes par secondes (ce qui est √©videmment bien moins que nos tests de charge), mais nous savons que nous pourrons maintenant nous adapter tr√®s simplement √† une charge beaucoup plus forte en ajoutant des serveurs web. Notamment lors d‚Äô√©missions faisant grandement appel au second √©cran.\n\nDans le pire des cas, si le service Polls devient indisponible, aucune autre partie de notre infrastructure ne sera compromise.\n\nLe√ßons\n\nAu cours du d√©veloppement, de la mise en production et de la maintenance de ce service, j‚Äôai appris plusieurs choses que j‚Äôessaierai de ne pas oublier trop vite :\n\n\n  Yes we can! Il est possible de combler petit √† petit la dette technique, mais uniquement si c‚Äôest la volont√© des d√©cideurs,\n  la s√©r√©nit√© apport√©e par les tests automatis√©s est sans √©gale pour le confort de d√©veloppement,\n  Redis est tr√®s performant.\n\n\nHa ? Attendez ! On me dit dans l‚Äôoreillette que certains doutaient encore qu‚Äôil √©tait possible de faire du code performant avec un framework full stack comme Symfony.\n\nPas moi :-)\n"
} ,
  
  {
    "title"    : "How we use StatsD",
    "category" : "",
    "tags"     : " statsd, graphite, php, nodejs, monitoring",
    "url"      : "/2014/01/28/how-we-use-statsd.html",
    "date"     : "January 28, 2014",
    "excerpt"  : "What we want\n\nAs developers, we (M6Web) want to keep our eyes open on what is going on in production.\n\nOur local CMO (chief monitoring officier ;) ) did a nice presentation about this (in french).\n\nAs someone very wise (Theo Schlossnagle) said: ‚ÄúI...",
  "content"  : "What we want\n\nAs developers, we (M6Web) want to keep our eyes open on what is going on in production.\n\nOur local CMO (chief monitoring officier ;) ) did a nice presentation about this (in french).\n\nAs someone very wise (Theo Schlossnagle) said: ‚ÄúIt‚Äôs not in production unless it‚Äôs monitored‚Äù. Another cool mantra is: ‚ÄúI am wondering what to monitor ? everything dude !‚Äù. Finally ‚Äúif you can not measure it, you can not improve it.‚Äù (Lord Kelvin).\n\nWe ship new apps very often, so we have to industrialise this practice.\n\nWhat is it?\n\nStatsD is a Node.Js daemon allowing you to send metrics (increment values and timers) over UDP. The fire and forget feature of UDP is great for reducing risks of introducing latency or crashes in your application.\n\nStatsD is open sourced by etsy. In our configuration, we use several StatsD deamons and aggregate metrics on Graphite - one point per minute. Many servers allows us to scale, because we don‚Äôt sample the data at all.\n\nOn client side, we use a simple consistent hashing algorithm to dispatch metrics overs StatsD nodes on the same server.\n\nCollecting metrics\n\nFrom raw PHP\n\nWe‚Äôve created a simple PHP lib to dispatch metrics over UDP. Check it out on Github or Packagist.\n\nThe usage is pretty straightforward :\n\n&amp;lt;?php\n// client creation\n$client = new Statsd\\Client(\n                    array(\n                        &#39;serv1&#39; =&amp;gt; array(&#39;address&#39; =&amp;gt; &#39;udp://200.22.143.12&#39;),\n                        &#39;serv2&#39; =&amp;gt; array(&#39;port&#39; =&amp;gt; 8125, &#39;address&#39; =&amp;gt; &#39;udp://200.22.143.12&#39;)\n                    )\n                );\n// usage\n$client-&amp;gt;increment(&#39;a.graphite.node&#39;);\n$client-&amp;gt;timing(&#39;another.graphite.node&#39;, (float) $timing);\n\nFrom Symfony2\n\nAs basic Symfony2 fanboys, we‚Äôve built a bundle on top of the StatsD component.\nIt provides these features:\n\n\n  manage multiple Symfony services with different configurations\n  bind any event to increment nodes and collect timers\n\n\nDuring Symfony 2 execution, metrics are collected and sent only at the kernel shutdown. A nice feature is that you can easily collect basic metrics based on events without touching your code.\n\nFor example, in conjunction with the M6Web\\HttpKernelBundle, just dropping this in config.yml is enough:\n\nm6_statsd:\n    clients:\n        default:\n            servers: [&#39;all&#39;]\n            events:\n              m6.terminate:\n                increment:     request.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;\n                timing:        request.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;\n                custom_timing: { node: memory.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;, method: getMemory }\n              m6kernel.exception:\n                increment: errors.&amp;lt;status_code&amp;gt;.yourapp\n\n\n\nOffering this to the tech team means that I am now pretty sure that almost all new PHP apps pop with those metrics out of the box.\n\nPlease checkout the bundle documentation on github.\n\nFrom anywhere else\n\nFrom Flex, mobile app or JS applications we‚Äôve developed a simple Node.js app, translating an HTTP call to a StatsD UDP one. Like the PHP implementation, this application shards the metrics over multiple servers.\n\nPlease consider sending metrics asynchronously and add a timeout to this HTTP call.\n\nLiving with metrics\n\nAbout 120K metrics are collected on our platform. That‚Äôs a lot.\n\nGraphite dashboards are quite rustic. But surprisingly lots of non-techs people use this tool: SEO experts, advertising managers, contributors, ‚Ä¶\n\n\n\n\n\nFor now we keep using Graphite. We try to keep our dashboards organised and well named.\n\nFor alerting purpose, a tool based on Graphite JSON output has been developed. It sends emails when it reaches some user defined conditions. Honestly, it does the job, but frankly we are still looking for something else, more flexible with more notification systems than emails.\n\nIf you use such a tool, and you‚Äôre happy with it, please let us know in the comments.\n\nFound a typo or bad english langage, just propose a pull request.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #6",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2014/01/20/m6web-dev-facts-6.html",
    "date"     : "January 20, 2014",
    "excerpt"  : "Parce que nous en avons encore une quantit√© incroyable en stock, voici une nouvelle s√©lection des meilleures phrases entendues dans nos bureaux !\n\nTo code or not to code\n\n\n  \n    Je veux faire du code qui marche, et qui sert √† quelque chose !\n    ...",
  "content"  : "Parce que nous en avons encore une quantit√© incroyable en stock, voici une nouvelle s√©lection des meilleures phrases entendues dans nos bureaux !\n\nTo code or not to code\n\n\n  \n    Je veux faire du code qui marche, et qui sert √† quelque chose !\n    C‚Äôest soit l‚Äôun, soit l‚Äôautre, mec ‚Ä¶\n  \n\n\nImage trompeuse\n\nUn d√©v teste une appli sur ipad\n\n\n  \n    Ah mais l‚Äôimage est toute moche\n    C‚Äôest ton reflet que tu vois\n  \n\n\nUne histoire de chocolat\n\n\n  \n    Qui a test√© la Kindle HDX ?\n    Moi j‚Äôai pas trop aim√©\n    Pourquoi ?\n    Je pr√©f√©re le Kindle Surprise\n  \n\n\nc‚Äôest automatique je te dis\n\n\n  Mais je te dis que c‚Äôest automatique ‚Ä¶ √† 90%\n\n\nUne √©vidence √©vidente\n\n\n  C‚Äôest pas toujours simple parce que c‚Äôest compliqu√©\n\n\nTout tout tout\n\n\n  Ca couvre la quasi totalit√© de tout\n\n\nVers l‚Äôinfini et au del√†\n\n\n  Il faut que le forum soit cach√© int√©gralement autant que possible\n\n\nFichier introuvable\n\n\n  Je t‚Äôenvoie par mail l‚Äôurl du fichier\n\n\nPar mail :\n\n\n  file://C:/Users/**/Desktop/Sanstitre-1.html\n\n\nUnknown Notice\n\n\n  Les notices c‚Äôest tabous, on en viendra tous √† bout\n\n\nLa minute de 30 secondes - le retour\n\n\n  Tu as pris six mois d‚Äôann√©e sabatique\n\n"
} ,
  
  {
    "title"    : "Vagrant &amp; Cie, du D√©v √† la Prod avec Julien Bianchi",
    "category" : "",
    "tags"     : " lft, vagrant, video",
    "url"      : "/2014/01/18/vagrant-julien-bianchi.html",
    "date"     : "January 18, 2014",
    "excerpt"  : "Un grand merci √† Julien Bianchi qui, √† notre demande, est venu nous parler un peu de vagrant lors d‚Äôun de nos fameux Last Friday Talk.\n\nSes slides : https://speakerdeck.com/jubianchi/vagrant-and-cie-du-dev-a-la-prod.\n\nMalheureusement, la vid√©o n‚Äôe...",
  "content"  : "Un grand merci √† Julien Bianchi qui, √† notre demande, est venu nous parler un peu de vagrant lors d‚Äôun de nos fameux Last Friday Talk.\n\nSes slides : https://speakerdeck.com/jubianchi/vagrant-and-cie-du-dev-a-la-prod.\n\nMalheureusement, la vid√©o n‚Äôest plus disponible‚Ä¶\n"
} ,
  
  {
    "title"    : "API √† consommer avec mod√©ration",
    "category" : "",
    "tags"     : " outil, api, symfony, doctrine, cytron, open-source",
    "url"      : "/2014/01/08/api-a-consommer-avec-moderation.html",
    "date"     : "January 8, 2014",
    "excerpt"  : "Apr√®s avoir travaill√© pendant plusieurs mois sur la cr√©ation et les tests de nos API avec Symfony, le moment de leur publication est enfin arriv√© !\n\nOr, les clients de nos API sont multiples : il peut s‚Äôagir d‚Äôapplications mobiles, de sites web ma...",
  "content"  : "Apr√®s avoir travaill√© pendant plusieurs mois sur la cr√©ation et les tests de nos API avec Symfony, le moment de leur publication est enfin arriv√© !\n\nOr, les clients de nos API sont multiples : il peut s‚Äôagir d‚Äôapplications mobiles, de sites web mais aussi d‚Äôun back office interne. Chacun de ces clients peut n√©cessiter des ‚Äúvues‚Äù diff√©rentes de l‚ÄôAPI.\n\nEffectivement, alors que le BO devra pouvoir acc√©der √† la totalit√© des ressources disponibles, l‚Äôapplication mobile ne devra avoir acc√®s qu‚Äôaux ressources publi√©es. De la m√™me mani√®re, la gestion du cache ainsi que la disponibilit√© des routes doit pouvoir s‚Äôadapter facilement aux clients qui consomment l‚ÄôAPI.\n\nNous avons opt√© pour l‚Äôutilisation d‚Äôun sous-domaine par client afin de l‚Äôidentifier et ainsi de lui appliquer des configurations particuli√®res. Ex :\n\n\n  https://bo.api.monservice.fr pour le BO,\n  https://mobile.api.monservice.fr pour l‚Äôapplication mobile.\n\n\n####¬†Authentification\n\nNous utilisons le composant s√©curit√© de Symfony, qui permet de cr√©er un utilisateur authentifi√© √† la vol√©e et de charger la configuration sp√©cifique √† celui-ci.\n\nNous avons tout d‚Äôabord besoin de cr√©er une classe User impl√©mentant Symfony\\Component\\Security\\Core\\User\\UserInterface, et contenant les informations de configuration sp√©cifique.\n\nLes diff√©rents Users sont ensuite cr√©√©s √† l‚Äôaide d‚Äôun fournisseur d‚Äôutilisateurs impl√©mentant Symfony\\Component\\Security\\Core\\User\\UserProviderInterface.\nDans notre cas, chaque utilisateur poss√®de son propre fichier de configuration yml. Le fournisseur d‚Äôutilisateur v√©rifie donc que l‚Äôutilisateur demand√© poss√®de un fichier de configuration et instancie un objet User avec cette configuration. Ce UserProvider est d√©fini comme service dans notre bundle et configur√© dans security.yml.\n\nIl faut ensuite cr√©er notre propre fournisseur d‚Äôauthentification pour avoir une authentification par nom de domaine. Pour cela nous avons suivi et adapt√© le cookbook de Symfony. Cette authentification s‚Äôarticule autour de 2 classes : un FirewallListener et un AuthenticationProvider. Pour que notre FirewallListener puisse facilement r√©cup√©rer le client associ√©, nous avons ajout√© un param√®tre au routing Symfony :\n\nhost: {client}.api.monservice.fr\n\nLe FirewallListener utilise donc ce param√®tre du routing comme nom d‚Äôutilisateur et le transmet √† notre AuthenticationProvider. Celui-ci r√©cup√®re le User gr√¢ce au UserProvider et profite de cette phase pour v√©rifier que l‚Äôadresse IP du client est bien autoris√©e dans sa configuration gr√¢ce au FirewallBundle.\n\nEffectivement, nous avons ajout√© un filtrage initial (mais optionnel) sur les IPs pour chaque client, dans le fichiers app/config/users/{username}.yml :\n\nfirewall:\n    user_access:\n        default_state: false\n        lists:\n            m6_prod: true\n            m6_preprod: true\n            m6_dev: true\n            m6_lan: true\n            m6_local: true\n            m6_public: true\n\nPour plus de pr√©cisions, voir la documentation du FirewallBundle.\n\nAutorisation\n\nPour g√©rer les autorisations d‚Äôacc√®s des utilisateurs aux diff√©rentes routes, nous avons cr√©√© un EventListener qui √©coute kernel.request et qui d√©cide de laisser passer la requ√™te ou non en fonction de la configuration de l‚Äôutilisateur.\n\nallow:\n    default: true\n    methods:\n        delete: false\n    resources:\n        exam: false\n    routes:\n        get_articles: false\n\nDans cet exemple, l‚Äôutilisateur a acc√®s par d√©faut √† toutes les routes sauf les m√©thodes DELETE, les routes concernant les exams et la route sp√©cifique get_articles.\n\n####¬†Dur√©e de cache\n\nLes temps de cache sont diff√©rents en fonction de l‚Äôutilisation des donn√©es. Les donn√©es du backoffice ne seront pas cach√©es, tandis que les donn√©es de l‚Äôapplication mobile auront un temps de cache de 300s.\nNous avons l√†-aussi cr√©√© un EventListener qui √©coute cette fois kernel.response et qui modifie les headers de cache de la r√©ponse en fonction de la configuration utilisateur qui peut contenir une dur√©e par d√©faut de cache et des dur√©es de cache par route.\n\nFiltrage automatique avec Doctrine\n\nNous pouvons offrir une ‚Äúvue‚Äù diff√©rente de nos donn√©es √† chaque client en d√©finissant des crit√®res de filtrage pour Doctrine (ex: date de publication, ressource activ√©e, etc.) dans les fichiers de configuration des clients :\n\nentities:\n    article:\n        active: true\n        publication: false\n\nAfin de ne pas modifier le comportement par d√©faut de Doctrine, nous avons ajout√© une m√©thode findWithContext √† nos repositories qui reprend les m√™mes param√®tres que la m√©thode findBy en injectant le SecurityContext. Cette m√©thode permet donc de r√©cup√©rer des entit√©s filtr√©es en fonction des param√®tres d‚Äôun client :\n\n&amp;lt;?php\n$article = $this\n    -&amp;gt;get(&#39;m6_contents.article.manager&#39;)\n    -&amp;gt;getRepository()\n    -&amp;gt;findWithContext($this-&amp;gt;container-&amp;gt;get(&#39;security.context&#39;), [&#39;id&#39; =&amp;gt; $id]);\n\n####¬†Personnalisation avanc√©e\n\nGr√¢ce √† l‚Äôutilisation du Bundle Security de Symfony, toute la configuration sp√©cifique √† un sous-domaine est stock√©e dans l‚Äôutilisateur courant. Et dans Symfony, l‚Äôutilisateur courant est facilement r√©cup√©rable √† partir du service security_context. Il est ainsi possible de personnaliser n‚Äôimporte quelle brique de l‚Äôapplication en y injectant la d√©pendance sur ce service.\n\nDomainUserBundle\n\nAfin d‚Äôimpl√©menter facilement ce fonctionnement sur nos API, nous avons d√©velopp√© un bundle d√©di√©. Il peut donc aussi vous permettre de g√©rer l‚Äôauthentification et la configuration de vos API par nom de domaine.\n\nDomainUserBundle est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Qui a bouchonn√© mon Redis ?",
    "category" : "",
    "tags"     : " qualite, outil, redis, cytron, open-source",
    "url"      : "/redismock-qui-a-bouchonne-mon-redis",
    "date"     : "December 11, 2013",
    "excerpt"  : "Les tests fonctionnels tiennent un r√¥le majeur dans la r√©ussite et la p√©rennit√© d‚Äôun projet web, d‚Äôautant plus s‚Äôil est d√©ploy√© continuellement. Nous nous √©tions donc d√©j√† int√©ress√©s √† cette probl√©matique dans le cas d‚Äôun service proposant une API...",
  "content"  : "Les tests fonctionnels tiennent un r√¥le majeur dans la r√©ussite et la p√©rennit√© d‚Äôun projet web, d‚Äôautant plus s‚Äôil est d√©ploy√© continuellement. Nous nous √©tions donc d√©j√† int√©ress√©s √† cette probl√©matique dans le cas d‚Äôun service proposant une API REST et utilisant MySQL et Doctrine. Mais nous d√©veloppons aussi des services du m√™me type utilisant d‚Äôautres syst√®mes de stockage de donn√©es comme Redis.\n\nAfin de tester fonctionnellement ces services, nous avons d‚Äôabord eu l‚Äôid√©e d‚Äôinstaller une instance Redis sur nos serveurs de tests. Mais nous allions in√©luctablement retomber sur les m√™mes obstacles qu‚Äôavec MySQL :\n\n\n  il n‚Äôest pas toujours possible de monter une instance Redis d√©di√©e aux tests,\n  mais surtout une telle architecture n‚Äôest pas viable dans un syst√®me de tests concurrentiels.\n\n\nLa librairie RedisMock\n\nNous nous sommes alors pench√©s sur la possibilit√© de bouchonner Redis, chose qui parait au premier abord plus ais√©e que de bouchonner Doctrine : Redis propose une API simple et bien document√© (m√™me si abondante). Nous pensions trouver une librairie PHP d√©j√† existante mais nos recherches sont rest√©es vaines.\n\nNous avons donc cr√©√© la librairie RedisMock qui reprend simplement les commandes de l‚ÄôAPI de Redis et simule leur comportement gr√¢ce aux fonctions natives de PHP. √âvidemment, toutes les commandes Redis n‚Äôont pas encore √©t√© impl√©ment√©es, seules celles qui sont utilis√©es dans nos tests sont pr√©sentes. Vous pouvez nous proposer l‚Äôimpl√©mentation de nouvelles fonctions Redis, selon vos besoins, via des Pull Requests sur le projet.\n\nToutes les commandes expos√©es par le mock sont test√©es unitairement via atoum en reprenant pour chaque cas les sp√©cifications √©nonc√©es dans la documentation Redis.\n\nUtiliser RedisMock dans vos tests sur Symfony\n\nTout d‚Äôabord, il faut rajouter la d√©pendance √† la librairie dans le composer.json et mettre √† jour les vendors :\n\n\n\nL‚Äôutilisation du mock reste tr√®s simple dans un projet Symfony. Chez M6Web, nous utilisons notre propre composant Redis, lui m√™me bas√© sur Predis. Afin que le mock puisse compl√®tement se faire passer pour la librairie Redis lors de l‚Äôexecution des tests, nous avons impl√©ment√© une factory qui cr√©e √† la vol√©e un adapteur h√©ritant de la classe √† bouchonner. La m√©thode getAdpaterClass permet de r√©cup√©rer le nom de la classe √† instancier.\n\n\n\nPour simplifier la cr√©ation de l‚Äôadapteur et son injection dans l‚Äôapplication via le fichier config_test.yml, on peut utiliser la m√©thode getAdapter qui instancie directement l‚Äôobjet sans param√®tre. Il nous suffit alors de modifier la d√©finition du service Redis dans l‚Äôenvironnement de test.\n\n\n\nEt voil√†, le tour est jou√© ! Les tests utilisent maintenant le mock √† la place du v√©ritable Redis. Attention cependant, si votre librairie utilise des fonctionnalit√©s non impl√©ment√©es dans RedisMock, vous pourriez faire face √† des comportements al√©atoires ind√©sirables.\n\nRedisMock  est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Composer installation without github.com (nor packagist) dependency - like a boss !",
    "category" : "",
    "tags"     : " satis, composer, aws, s3, github, packagist, cloud",
    "url"      : "/composer-installation-without-github",
    "date"     : "December 2, 2013",
    "excerpt"  : "\n\nFirst a thought about github, composer, packagist : we like / adore / thanks the contributors, for those great services and all the open source people dropping great software on it.\n\nThat said, picture yourself operating an online PHP service, g...",
  "content"  : "\n\nFirst a thought about github, composer, packagist : we like / adore / thanks the contributors, for those great services and all the open source people dropping great software on it.\n\nThat said, picture yourself operating an online PHP service, generating hundreds euros per hour (cool isn‚Äôt it ?).\n\nIf you use Symfony2 and other public packages, like us, you‚Äôre probably deploying your application using composer.\n\n\n\nSuddenly the service is dealing with more and more and more traffic (maybe someone talk about on national tv ‚Ä¶ or Justin Bieber tweet something ‚Ä¶ maybe :) ). No problem, says the system administrator (yes our sysadmins are cool), lets pop more virtual machines and deploy more instance of the service !\n\nAnd then :\n\n\n\nboum ! =&amp;gt;composer install command can‚Äôt download distant packages on api.github.com (website is down, or the network connection or whatever).\n\nGood luck explaining to your boss that you rely on free hosting service to deploy your business critical website !\n\nThis is our situation. So here how we deal with that.\n\nPrinciples.\n\n\n\nWe chose to use Satis - a great tool provided by the Composer team. The main idea is, regulary download packages and their informations on our local servers. We (at M6Web) deployed services on our local infrastructure and on S3 servers in Amazon Web Services.\n\nHow to ? For your local network.\n\nWe set 2 different satis instance. One for our private packages, and another for all the dependencies we use (basically around Symfony2). The first one (satis-private) will build every 5 minutes, the second (satis-public) every half hour.\n\nfor example :\n\n\n  satis-private.yourcompany.com\n  satis-public.youcompany.com\n\n\nSatis for private package configuration (data/satis.json) :\n\n{\n    &quot;name&quot;: ‚Äúsatis-private&quot;,\n    &quot;homepage&quot;: &quot;https://satis-private.yourcompany.com&quot;,\n    &quot;archive&quot;: {\n        &quot;directory&quot;: &quot;dist&quot;,\n        &quot;absolute-directory&quot; : &quot;/srv/data/satis-private/dist&quot;,\n        &quot;format&quot;: &quot;zip&quot;,\n        &quot;skip-dev&quot;: true\n    },\n    &quot;repositories&quot;: [\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/great-bundle&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/great-component&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/awesomelib&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/raoul&quot; },\n‚Ä¶\n    ],\n    &quot;require-all&quot;: true\n}\n\n\nSatis for public package configuration (data/satis.json) :\n\n{\n    &quot;name&quot;: ‚Äúsatis-public&quot;,\n    &quot;homepage&quot;: &quot;https://satis-public.yourcompany.com&quot;,\n    &quot;archive&quot;: {\n        &quot;directory&quot;: &quot;dist&quot;,\n        &quot;format&quot;: &quot;zip&quot;,\n        &quot;skip-dev&quot;: false,\n        &quot;absolute-directory&quot; : &quot;/srv/data/satis-public/dist&quot;\n    },\n    &quot;repositories&quot;: [\n        { &quot;type&quot;: &quot;composer&quot;, &quot;url&quot;: &quot;https://packagist.org&quot; }\n    ],\n    &quot;require&quot;: {\n\n        &quot;m6web/firewall-bundle&quot; : &quot;*&quot;,\n        &quot;m6web/statsd-bundle&quot;   : &quot;*&quot;,\n\n        &quot;doctrine/orm&quot;               : &quot;~2.3&quot;,\n        &quot;doctrine/common&quot;            : &quot;~2.4&quot;,\n        &quot;doctrine/dbal&quot;              : &quot;~2.3&quot;,\n        &quot;doctrine/doctrine-bundle&quot;   : &quot;~1.2&quot;,\n\n        &quot;naderman/composer-aws&quot;      : &quot;~0.2.3&quot;,\n‚Ä¶\n    },\n    &quot;require-dependencies&quot;: true\n}\n\n\nOn the crontab, add this command for each satis instance :\n\nphp -d memory_limit=xx bin/satis build data/satis.json web\n\n\n(increasing memory_limit was mandatory for us with satis-public).\n\nPlease note the require-dependencies directive. It tell satis to digg on sub-dependencies on the required packages. And yes, it can take a while. You will probably hit the Github API rate limit. To increase it, add a Github key on your composer configuration file on the satis servers.\n\n$ cat .composer/config.json\n{\n    &quot;config&quot;: {\n        &quot;github-oauth&quot;: {\n            &quot;github.com&quot;: ‚Äúxxxxxx&quot;\n        }\n    }\n\n}\n\n\nIn your projects, edit the composer.json and replace the repositories entry by\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://satis-private.yourcompany.com&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://satis-public.yourcompany.com&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nRemove your composer.lock and vendors then run composer update on the project.\n&quot;packagist&quot;: false&quot; mean : ‚Äúdo not search missing packages on packagist.com‚Äù. If a package is missing during install, you have to add it in satis-public configuration file then try again.\n\nthat‚Äôs it :)\n\nHow to ? For AWS.\n\nSync our 2 satis servers with an S3 bucket.\n\n\n\nOn satis servers, use s3cmd to keep in sync the S3 bucket. Let‚Äôs say : yourcloud-satis.\n\nAdd some commands after the build script of satis :\n\nphp -d memory_limit=xx bin/satis build data/satis.json web\ncd web\nsed &#39;s#https://satis-private\\.yourcompany\\.com#s3://yourcloud-satis/satis-private#&#39; packages.json &amp;gt; packages-cloud.json\ns3cmd put index.html s3://yourcloud-satis/satis-private/index.html\ns3cmd put packages-cloud.json s3://yourcloud-satis/satis-private/packages.json\ncd /srv/data/satis-private/\ns3cmd sync ./dist s3://6cloud-satis/satis-private/\n\n\n(do the same for satis-public).\n\nupdate your projects\n\nIn your projects, edit the composer.json and replace the repositories entry by\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-private/&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-public/&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nEnable the AWS plugin in EC2 servers\n\nAdd our repositories in ~./composer/composer.json file of the user used to deploy your code.\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-private/&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-public/&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nYou have to install the S3 plugin for composer on your EC2 instance.\n\n$ composer global require &quot;naderman/composer-aws:~0.2.5&quot;\n\n\nIf you don‚Äôt use IAM roles, add the following composer config on your EC2 servers (~/.composer/config.json) :\n\n{\n    &quot;config&quot;: {\n        &quot;amazon-aws&quot;: {\n            &quot;key&quot;:    &quot;KEYYYYYY&quot;,\n            &quot;secret&quot;: &quot;seeeeecret&quot;\n        }\n    }\n}\n\n\ncomposer install --prefer-dist command will now download all the packages files from S3 !\n\nThanks to Pierre and Jeremy for their help.\n\nFound a typo or bad english langage, just propose a pull request.\n"
} ,
  
  {
    "title"    : "JenkinsLight, mettez en lumi√®re vos jobs Jenkins",
    "category" : "",
    "tags"     : " outil, jenkins, ci, cytron, open-source",
    "url"      : "/jenkinslight-mettez-en-lumiere-vos-jobs-jenkins",
    "date"     : "November 20, 2013",
    "excerpt"  : "L‚Äôid√©e de JenkinsLight a germ√© lorsque nous nous sommes fait taper sur les doigts pour la troisi√®me fois (√† juste titre) parce que l‚Äôon avait d√©sactiv√© la publicit√© sur nos sites de cha√Æne lors d‚Äôune mise en production. Or la publicit√© est un poin...",
  "content"  : "L‚Äôid√©e de JenkinsLight a germ√© lorsque nous nous sommes fait taper sur les doigts pour la troisi√®me fois (√† juste titre) parce que l‚Äôon avait d√©sactiv√© la publicit√© sur nos sites de cha√Æne lors d‚Äôune mise en production. Or la publicit√© est un point critique car directement reli√©e au chiffre d‚Äôaffaires. Le pire est que nous testions d√©j√† le bon fonctionnement de la publicit√© en int√©gration continue sur nos serveurs de preprod, avant la mise en production. Mais une configuration l√©g√®rement diff√©rente sur les serveurs de prod rendait le nouveau code instable. Cette situation rend donc impossible la d√©tection de certaines anomalies avant la mise en production‚Ä¶\n\nD‚Äôo√π notre besoin d‚Äôavoir un tableau de bord nous permettant de v√©rifier chaque instant la disponibilit√© des fonctionnalit√©s n√©vralgiques de nos sites en production afin de r√©agir au plus vite en cas de probl√®mes. Et ce, avant m√™me que l‚Äôanomalie ne nous soit remont√©e par les autres secteurs. Nous avions d√©j√† nos tests dans Jenkins que nous avons alors fait pointer vers la prod. Il nous manquait donc juste une sorte de ‚ÄúPanic Board‚Äù sur un √©cran plac√© au sein de nos bureaux nous remontant rapidement le moindre probl√®me sur nos sites en production.\n\nNous avons cr√©√© JenkinsLight qui permet d‚Äôafficher distinctement le statut des jobs d‚Äôune vue Jenkins en quasi temps r√©el. Le projet utilise AngularJS et l‚ÄôAPI de Jenkins pour r√©cup√©rer les informations n√©cessaires. L‚Äôinstallation se fait sur n‚Äôimporte quel serveur web et requiert uniquement Bower pour installer les composants. Afin de permettre √† l‚ÄôAPI d‚Äô√™tre appel√©e en crossdomain (CORS), il est √©galement n√©cessaire d‚Äôinstaller un plugin sp√©cifique sur votre serveur Jenkins.\n\nL‚Äôapplication propose quelques variables de configuration √©ditables dans le fichier ‚Äúapp/scripts/config.js‚Äù permettant de sp√©cifier :\n\n\n  l‚Äôurl du serveur Jenkins,\n  l‚Äôidentification au serveur (si n√©cessaire),\n  la vue Jenkins par d√©fault,\n  les types de jobs affich√©s,\n  une regexp pour exclure certains jobs,\n  le nombre maximum de jobs par ligne sur l‚Äô√©cran,\n  l‚Äôintervalle de rafra√Æchissement (en millisecondes),\n  une image de fond quand il n‚Äôy a aucun job √† afficher.\n\n\nJenkinsLight est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 3",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-3",
    "date"     : "November 19, 2013",
    "excerpt"  : "\n\nDerni√®re journ√©e de cette Velocity Europe, avec en plus du track Performance et Ops, l‚Äôouverture d‚Äôun track Culture.\n\nPour rappel, si vous les avez rat√©s, les CR des journ√©es pr√©c√©dentes sont √† retrouver ici :\n\n\n  CR Velocity Europe 2013 - Day 1...",
  "content"  : "\n\nDerni√®re journ√©e de cette Velocity Europe, avec en plus du track Performance et Ops, l‚Äôouverture d‚Äôun track Culture.\n\nPour rappel, si vous les avez rat√©s, les CR des journ√©es pr√©c√©dentes sont √† retrouver ici :\n\n\n  CR Velocity Europe 2013 - Day 1\n  CR Velocity Europe 2013 - Day 2\n\n\nPour ce ‚ÄúDay 3‚Äù, nous nous retrouvons tous dans la grande salle avec la vid√©o ‚ÄúSlow Motion Water Balloon Fight‚Äù en guise d‚Äôintroduction :\n\n\n\nExtreme Image Optimisation: WebP &amp;amp; JPEG XR\n\nIdo Safruti (Akamai) @safruti\n\n\n\nA ce jour, d‚Äôapr√®s HTTP Archive, 62% du poids des pages Web correspond aux images sur desktop, 65% sur mobile.\n\nNous utilisons toujours des technos vieilles de plus de 15 ans ! Jpg, Png, Gif ‚Ä¶\n\n\n  ‚ÄúDeploying new image formats on the web is HARD (but doable)‚Äù Ilya Grigorik\n\n\nLa conf√©rence traite de deux formats bien plus r√©cents :\n\n\n  WebP (2011)\n  JXR : Jpeg eXtended Range (2009)\n\n\nqui supportent le lossless et lossy, ainsi que la transparence (en lossless et lossy aussi)\n\nOn retrouve un tableau tr√®s int√©ressant sur une comparaison taille entre les diff√©rents formats sur un m√™me niveau de qualit√© :\n\n\n\nAttention, certaines fois (quelques %), l‚Äôimage peut √™tre plus grosse qu‚Äôen jpg. Si vous partez du Jpg pour la compression, comparez les tailles et affichez le Jpg s‚Äôil est plus petit.\n\nLe support de ces formats reste toutefois minime :\n\n\n  WebP : Chrome &amp;gt;= 23, Opera &amp;gt;= 12, ‚Ä¶\n  Jpeg XR : IE =&amp;gt; 10, ‚Ä¶\n\n\nJXR, g√®re notamment le progressive, ce que ne g√®re pas encore date WebP. Plus d‚Äôinfos sur les ‚Äúprogressives Jpeg‚Äù sur le blog de Patrick Meenan (Cr√©ateur de WebPageTest).\n\nUne petite anecdote int√©ressante sur WebP aussi. Facebook avait mis en place WebP mais est revenu en arri√®re car les utilisateurs r√¢laient ! Quand ils enregistraient ou partageait une photo de Chrome (donc en WebP), les utilisateurs IE notamment, ne pouvaient pas la consulter ‚Ä¶\n\nIdo fera un article sur l‚Äôincontournable calendrier de l‚Äôavant sur le sujet : Performance Calendar en d√©cembre 2013 !\n\n\n\nSLOWING DOWN TO GO FASTER: Responsive Web Design And The Problem Of Agility vs Robustness\n\nTom Maslen (BBC News) @tmaslen\n\n\n\nGros retour d‚Äôexp√©rience des √©quipes de BBC News sur leur approche du ‚ÄúResponsive Web Design‚Äù, et sur la mani√®re dont cela a impact√© leurs workflows, ainsi que leur culture.\n\nLe RWD prend du temps, beaucoup plus de temps (3x), sur le Design, le d√©veloppement, le test.\n\nTom parcourt les optimisations ‚Äúclassiques‚Äù, ainsi que la mani√®re dont ils enrichissent l‚Äôexp√©rience : Ils d√©livrent une ‚ÄúCore Experience‚Äù √† tous, et une ‚ÄúEnhanced Experience‚Äù aux navigateurs qui le supportent, utilisent Grunt pour certaines automatisations (pour fournir les bonnes images √† la bonne taille https://github.com/BBC-News/Imager.js/, versionner les assets https://github.com/kswedberg/grunt-version ‚Ä¶).\n\nVous pouvez aussi d√©couvrir Wraith, leur outil de comparaison de screeshot Responsive.\n\nBref, une excellente conf√©rence avec un tr√®s bon speaker (tr√®s dr√¥le sur la fin).\n\n\n  ‚ÄúDon‚Äôt do whoopsies on other people things‚Äù Tom Maslen\n\n\n\n\nAn Introduction to Code Club\n\nJohn Wards (White October)\n\n\n\nCode Club est un projet lead√© par des b√©n√©voles pour cr√©er des clubs de coding dans les √©coles, pour des enfants entre 9 et 11 ans. Plus de 1400 clubs ont d√©j√©t√© cr√©√©s en Angleterre !\n\nLes enfants utilisent le projet Scratch, et qui permet via un langage de programmation assez simple, de programmer des jeux.\n\nLeurs vid√©os de pr√©sentation sont de plus assez fun, notamment celle ci, qui nous √† √©t√© montr√©e, √† 6mn50 dans la vid√©o ci dessous :\n\n\n\nLightning Demo: Automating WebPagetest with wpt-script\n\nJonathan Klein (Etsy) @jonathanklein\n\n\n\nAfin d‚Äôautomatiser la prise de mesure synth√©tique √† l‚Äôaide de WebPageTest (notamment si vous avez install√© une instance priv√©e), les gars d‚ÄôEtsy ont d√©velopp√© un wrapper Php √† l‚ÄôApi de Webpagetest. Le wrapper permet aussi de pousser les r√©sultats dans un Graphite ou un Splunk.\n\nL‚Äôoutil est dispo sur Github : Wpt-Script\n\n\n\nLightning Demo: Introducing a New RUM Resource From SOASTA\n\nBuddy Brewer (SOASTA)\n\n\n\nSOASTA, soci√©t√© connue notamment pour avoir rachet√© LogNormal (outil de R.U.M. l‚Äôann√©e derni√®re), propose aujourd‚Äôhui un outil de R.U.M. nomm√© : mPulse.\n\nIls ont publi√© de nombreuses statistiques sur leur site, sur les performances, suivant le navigateur, la localit√© etc : https://www.soasta.com/summary/\n\n\n\nLightning Demo: Automating The Removal Of Unused CSS\n\nAddy Osmani (Google Chrome) @addyosmani\n\n\n\nL‚Äôun des petits probl√®mes r√©currents du d√©veloppement web est situ√© dans nos fichiers Css. A force d‚Äôajout de fonctionnalit√©s ou de framework (notamment les fameux frameworks Css, Bootstrap &amp;amp; co), on finit par obtenir des fichiers CSS gigantesques, dans lesquels il devient tr√®s compliqu√© de savoir ce qui est utilis√© ou pas sur votre site.\n\nAddy pr√©sente des solutions qu‚Äôon peut retrouver :\n\n\n  pour un nettoyage mono page, dans la DevTools de Chrome (Onglet Audit puis Run puis ‚ÄúRemove Unused Css Rules‚Äù)\n  pour un nettoyage d‚Äôun site complet, via des outils autour de Grunt, notamment Grunt Uncss https://github.com/addyosmani/grunt-uncss fait par Addy en personne, bas√© sur le module Uncss de Giakki\n\n\n\n\nLearning from the Worst of WebPagetest\n\nRick Viscomi (Google)\n\n\n\nRick travaille pour YouTube chez Google, comme WebD√©veloppeur Front-end orient√© performance.\n\nSa passion, se moquer des mauvais r√©sultats sur les historiques du WPT public :-)\n\nC‚Äôest d‚Äôailleurs pour lui, l‚Äôune des bonnes sources pour d√©couvrir les ‚Äúanti-patterns‚Äù de la perf, et les choses √† ne pas faire.\n\nIl pr√©sente une Pull Request en cours sur WebPageTest avec le Multi Variate Testing, permettant de tester tout un site, sur plusieurs localit√©s. Plus d‚Äôinfos sur l‚Äôarticle de son blog sur le sujet : https://jrvis.com/blog/wpt-mvt/\n\nAre today‚Äôs good practices ‚Ä¶ tomorrows performance anti-patterns\n\nAndy Davies @andydavies‚Äã\n\n\n\nAvec l‚Äôarriv√©e d‚ÄôHTTP 2.0, on se demande, si les optimisations WebPerf que nous r√©alisons aujourd‚Äôhui ne seront pas g√™nantes demain : Les dataURI, le JS inline, le domain sharding, les sprites ‚Ä¶\n\nLes r√©ponses ne sont pas aussi simples, et nous ‚Äúd√©veloppeurs‚Äù nous devons de nous poser les questions afin d‚Äôavoir les bonnes r√©ponses avant l‚Äôarriv√©e d‚ÄôHTTP 2.0. Andy √† le m√©rite de lancer le d√©bat, via des protocoles de test pour chacun des cas. en comparant HTTP 1.0 et SPDY.\n\n\n\nProvisioning the Future - Building and Managing High Performance Compute Clusters in the Cloud\n\nMarc Cohen, Mandy Waite (Google)\n\nMarc et Mandy nous ont pr√©sent√© le Google Cloud, alternative AWS. Bas√© sur du KVM hautement modifi√© par les √©quipes de Google, on retrouve grossi√®rement des services identiques (stockage √©lastique persistent, SDN pour le r√©seau, load-balancing, des profils de machines highmem ou highcpu). Toutefois on notera l‚Äôabsence d‚Äôun marketplace pour les images des VMs et seules Debian et Centos sont disponibles. √ânorme avantage par rapport AWS: la facturation la minute au bout de 15min ! Mandy nous a fait la d√©mo du lancement de 1000 Vms en 2min15. Google fournit une api compl√®te et un outil en ligne de commande pour piloter absolument tout: gcutil.\n\nSecurity Monitoring (With Open Source Penetration Testing Tools)\n\nGareth Rushgrove (Government Digital Service)\n\nCombien d‚Äôentre nous testent la s√©curit√© de leur applicatif en continu ? Elle devrait pourtant faire partie de l‚Äôassurance qualit√© du d√©veloppement d‚Äôun logiciel. Gareth propose donc d‚Äôajouter des tests de s√©curit√© via Jenkins et des tests unitaires dans notre pipeline de d√©veloppement. Parmis la liste d‚Äôoutils (rkhunter, naxsi, logstash, fail2ban, auditd, la distrib BackTrack, clamav, Arachni) certains sont ais√©ment int√©grables au workflow. A tester le tr√®s bon OWASP ZAP (et https://www.dvwa.co.uk/ pour se faire la main !).\n\nLes slides :\n\n\n\nBeyond Pretty Charts‚Ä¶. Analytics for the cloud infrastructure\n\nToufic Boubez (Metafor Software) @tboubez\n\nToufic travaille depuis 20 ans dans la gestion des donn√©es des datacenters et la d√©tection d‚Äôanomalies. Comme lors de la pr√©sentation de Twitter il explique qu‚Äôon ne peut pas appliquer ces donn√©es temporelles des m√©thodes statistiques classiques (holt winter forecast, r√©gression lin√©aire, smooth splines), car elles sont non stationnaires (violant ainsi le principe d‚Äôhomog√©n√©it√©) et la plupart du temps elles ne sont pas distribu√©s normalement (principal pr√©-requis). Il nous a donc pr√©sent√© le test de Kolmogorov-Smirnov coupl√© aux techniques de bootstraping qui permet d‚Äôavoir des pr√©dictions assez fiables. Comme les mod√®les ARIMA, il fait partie de la famille des m√©thodes statistiques non param√©triques, qui ne pr√©supposent pas de la distribution des donn√©es)\n\nLes slides :\n\n\n\nAutomated Multi-Platform Golden Image Creation, Unlocking New Potential\n\nMitchell Hashimoto (HashiCorp) @mitchellh\n\nAvoir un environnement stable, clonable depuis la dev vers la prod est le r√™ve conjoint des d√©veloppeurs et sysadmins. L‚Äôutilisation des images pour le d√©ploiement de machines et de code peut √™tre fastidieux, au moindre changement de version il faut instancier l‚Äôimage, faire la modification, recr√©er l‚Äôimage etc‚Ä¶Ce qui n‚Äôest pas forcement adapt√© au cloud computing et la virtualisation. A l‚Äôinverse n‚Äôutiliser que des logiciels de gestion de la configuration (Cfengine, Puppet, Chef) ne certifie pas qu‚Äôun serveur vierge aura le m√™me comportement qu‚Äôun serveur sur lequel on aura pass√© 10.000 modifications. Il m‚Äôarrive r√©guli√®rement d‚Äôavoir des erreurs de run puppet cause d‚Äôune d√©pendance non satisfaite (packages) ou de probl√®mes r√©seau, (le plus souvent on souffre de la lenteur d‚Äôapplication d‚Äôun profil puppet).\n\nDe plus pass√© du VMWare du AWS ou Vagrant demande d‚Äôavoir autant d‚Äôimages que de plateforme ! Ce saint graal du ‚Äúserveur immuable‚Äù et agnostique de la plateforme est possible en utilisant une m√©thode interm√©diaire : Packer permet de cr√©er des images (Aws, Virtualbox, VMWare) partir d‚Äôune source et avec l‚Äôaide d‚Äôun chef/puppet/cfengine. Le workflow propos√© est le suivant: commit dans le repository =&amp;gt; build avec Packer =&amp;gt; CI (Jenkins) =&amp;gt; Image ready !\n\nCela permet de garder la flexibilit√© d‚Äôun Puppet avec l‚Äôidempotence des images et leur facilit√©, rapidit√© de d√©ploiement.\n\nL‚Äôorchestration peut √™tre r√©alis√©e avec Serf, il impl√©mente un protocole Gossip (tout le monde se parle, mais pas en m√™me temps). C‚Äôest un agent install√© sur le serveur qui g√®re des messages et des handlers (scripts personnalis√©s dans le langage de votre choix). Dans l‚Äôexemple de d√©ploiement de multiples load balancer, l‚Äôimage va permettre d‚Äôavoir un syst√®me fonctionnel rapidement, les utilisateurs, les logiciels et c‚Äôest Serf qui r√©cup√©rera la configuration appliquer au load balancer.\n\nOn peut aller plus loin en d√©ployant les code avec Docker, ce qui ajoute une couche d‚Äôabstraction suppl√©mentaire extr√™mement puissante.\n\nDOM to Pixels: Accelerate Your Rendering Performance\n\nPaul Lewis (Google - Team Chrome) @aerotwist\n\nPaul Lewis explique quelques principes mis en oeuvre lors du rendu graphique dans Chrome tel que la gestion des calques qui permet d‚Äôutiliser plus intens√©ment la puissance du GPU mais dont la multiplication peut s‚Äôav√©rer contre productive : la gestion de trop nombreux calques par le CPU contrebalance la performance du rendu par le GPU (√©videmment, sinon cela serait trop simple).\n\nPour bien appr√©hender cette pr√©sentation, il m‚Äôa sembl√© n√©cessaire d‚Äôavoir un petit background dans la programmation graphique : savoir, par exemple, pourquoi une ombre ou un flou sont couteux pour le rendu (cause des calculs entre les diff√©rentes zones de m√©moire contenant les informations graphiques superposer).\n\nPaul pr√©sente ensuite en d√©tail l‚Äôoutil de debug du rendu dans les WebTools : comment enregistrer en temps r√©el les diff√©rentes frames affich√©es par Chrome et visualiser les diff√©rents temps de calculs.\n\nCette pr√©sentation, bien que tr√®s int√©ressante par son contenu fut aussi mise en valeur par l‚Äôinterpr√©tation de Paul Lewis : toujours pr√©cise mais simple, s√©rieuse et fun la fois.\n Ce f√ªt pour moi, la meilleure pr√©sentation (show!) de la V√©locity.\n\nEt n‚Äôoubliez pas :\n\n\n  ‚ÄúTools, not rules‚Äù Paul Lewis\n\n\nConclusion :\n\nEpuis√© par trois jours de conf√©rence d‚Äôune densit√© incroyable, La Velocity a encore ais√©ment tenu toute ses promesses.\n\nNous esp√©rons que ces comptes-rendus vous auront √©t√© utile, autant que les pr√©sentations nous l‚Äôont √©t√©.\n\nN‚Äôh√©sitez pas √† commenter l‚Äôun des CR pour donner votre avis, sur le CR, ou sur certains points couvert par les Talks.\n\nMerci.\n\nVous pouvez retrouver :\n\n\n  quelques vid√©os de la conf√©rence sur Youtube\n  les slides sur le site d‚ÄôOreilly\n  et les photos ici sur Flickr : https://www.flickr.com/photos/oreillyconf/sets/72157637657689424/\n\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 2",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-2",
    "date"     : "November 16, 2013",
    "excerpt"  : "\n\nDe retour √† l‚Äôh√¥tel Hilton de Londres, afin de commencer cette deuxi√®me journ√©e qui s‚Äôannonce tr√®s charg√©e : jusqu‚Äô4 tracks en parall√®le. Performance, Mobile, Ops, et Sponsors.\n\nMaking Government digital services fast\n\nPaul Downey @psd\n\n\n\nPaul e...",
  "content"  : "\n\nDe retour √† l‚Äôh√¥tel Hilton de Londres, afin de commencer cette deuxi√®me journ√©e qui s‚Äôannonce tr√®s charg√©e : jusqu‚Äô4 tracks en parall√®le. Performance, Mobile, Ops, et Sponsors.\n\nMaking Government digital services fast\n\nPaul Downey @psd\n\n\n\nPaul est un ‚ÄúTechnical Architect‚Äù pour le gouvernement anglais. Il nous explique comment ils g√®rent et priorisent les probl√©matiques de performances pour offrir des services internet centr√©s sur les besoins des utilisateurs avant ceux du gouvernement.\n\nAvec notamment une r√©duction drastique du nombre de pages, ce qui leur a permis d‚Äôobtenir plus de visites au final !\n\nLe tout est enti√®rement document√© en ligne, en acc√®s public, et regorge d‚Äôinformations int√©ressante que vous pouvez retrouver sur le Gov.Uk Service Manual.\n\n\n\nStand down your smartphone testing army\n\nMitun zavery (keynote)\n\n\n\nMitun travaille chez Keynote, et fait une d√©monstration de deux de leurs outils :\n\nKeynote DA Free (DA = Device Anywhere)\n\nL‚Äôoutil tr√®s int√©ressant propose un grand nombre d‚Äôappareils mobiles qu‚Äôon peut acqu√©rir pendant 10 minutes, afin de lancer des tests. Le gros int√©r√™t est qu‚Äôon parle ici de vrais appareils, pas de simulateurs.\n\nLe service est disponible sur : https://dafree.keynote.com apr√®s vous √™tres inscrit gratuitement sur cette url https://www.keynotedeviceanywhere.com/da-free-register.html\n\n(Le service ne fonctionne pas sur Chrome mac pour ma part)\n\nC‚Äôest plut√¥t impressionnant techniquement, on lance les applications que l‚Äôon souhaite, rentre du texte, change l‚Äôorientation ‚Ä¶ ! A m√©moriser.\n\nMITE, le deuxi√®me outil qui √† l‚Äôair tr√®s complet permet d‚Äôaller beaucoup plus loin, mais avec des simulateurs cette fois : https://mite.keynote.com/download.php\n\nDommage que l‚Äôon oublie les Macs dans l‚Äôhistoire.\n\n\n\nTesting all the way to production\n\nSam adams (lmax exchange) @LMAX\n\n\n\nPour ceux qui ne seraient pas encore convaincu de l‚Äôint√©r√™t des tests automatis√©s, Lmax exchange (site sur l‚Äôunivers boursier g√®rant des sommes d‚Äôargent assez ph√©nom√©nales) pr√©sente le workflow de d√©veloppement bas√© sur les tests pour d√©ployer du code le plus souvent possible en √©vitant au maximum les r√©gressions.\n\n\n\nMost of the time we measure the performance of others\n\nklaus enzenhofer (compuware) @kenzenhofer\n\n\n\nCourte pr√©sentation de Compuware qui √©dite des solutions de monitoring, et aussi Dynatrace Ajax avec une √©tude de cas assez simple sur la d√©tection d‚Äôanomalies sur un site (142 domaines de 3rd party charg√©s !).\n\nLe blog de Compuware regorge d‚Äôarticle en tout genre sur la Webperf.\n\n\n\nMaking performance personal at Ft labs\n\nAndrew Betts @triblondon\n\n\n\nLes √©quipes du Financial Times sont tr√®s actifs dans le domaine de la webperf, avec notamment l‚Äôoutil FastClick qui permet d‚Äôenlever le delay du touch sur mobile (entre 100 et 300 ms!). Ils developpent aussi une webapp html5 tr√®s riche, et expliquent comment rendre la probl√©matique de performance importante aux yeux du ‚Äúproduit‚Äù. On apprend pas mal d‚Äôastuces pour mesurer la performance, g√©rer le appcache, √©viter les sprites etc ‚Ä¶\n\nVous pouvez retrouvez les slides ici : https://triblondon.github.io/talk-makingperfpersonal/#/\n\n\n\nLightning demo : Global web page performance\n\nJames Smith (Devopsguys) @thedevmgr\n\n\n\nJames est venu pr√©senter lors d‚Äôune lightning d√©mo : worldwidepagetest.com\n\nUn outil permettant de tester partout dans le monde les performances de son site, bas√© sur Webpagetest, et les locations et browsers disponible.\n\nEn plus de l‚Äô√©chec total de la d√©mo (bug/plantage ‚Ä¶ ‚Äúworst demo ever‚Äù d‚Äôapr√®s le speaker lui m√™me), l‚Äôoutil qui parait int√©ressant sur le papier me semble une fausse bonne id√©e et le risque de saturer les instances de WPT mondial √† cause de ce type d‚Äôoutil me parait bien plus g√™nant que les avantages qu‚Äôil apporte.\n\nLightning demo: HTTP Archive, BigQuery, and you!\n\nIlya Grigorik @igrigorik\n\n\n\nL‚Äôimpressionnante quantit√© de donn√©es agr√©g√©es par HTTP Archive est maintenant disponible dans Google BigQuery : toutes les donn√©es statistiques sur les requ√™tes et r√©ponses HTTP de plusieurs centaines de milliers de site diff√©rents sont donc simplement requ√™tables et disponibles la vitesse de la lumi√®re (c‚Äôest une expression √† la mode en ce moment par ici).\n\nUn article d‚ÄôIlya explique la marche suivre pour utiliser les donn√©es de HTTP Archive stock√©es sur BigQueries : https://www.igvita.com/2013/06/20/http-archive-bigquery-web-performance-answers/\n\nNous pouvons ainsi ais√©ment effectuer quelques comparaisons avec nos ‚Äúconcurrents‚Äù et n√©anmoins amis pr√©sents la conf√©rence :-) : https://denisroussel.fr/httparchive-bigquery-french-test.html\n\nIlya pr√©sente aussi le site communautaire BigQueri.es (Powered by Discourse), permettant de partager les r√©ponses des questions statistiques sur les bases pr√©sentes dans BigQuery (notamment celle de HTTP Archive)\n\n\n\nGimme More! Enabling User Growth in a Performant and Efficient Fashion\n\nArun Kejariwal (Twitter) @arun_kejariwal, Winston Lee (Twitter Inc.) @winstl\n\nLa planification de la capacit√© (‚Äúcapacity planning‚Äù) chez Twitter passe par l‚Äôutilisation de mod√®les statistiques et la pr√©diction sur des donn√©es temporelles. C‚Äôest absolument n√©cessaire pour le dimensionnement des plateformes techniques.\n\nL‚Äôutilisation d‚Äôune simple r√©gression lin√©aire capture la tendance globale, mais ne prends pas en compte la saisonnalit√© ni les pics de trafic (positifs ou n√©gatifs). Un mod√®le ‚Äúsmooth splines‚Äù correctement param√©tr√©, de part son design, ne le permet pas non plus. Idem pour le Holt Winters (que vous pouvez tester avec graphite). Ils utilisent donc le mod√®le ARIMA (Autoregressive integrated moving average), qui permet d‚Äôeffectuer des pr√©dictions partir de donn√©es temporelles non stationaires (c‚Äôest dire que la moyenne et la variance change = pics de trafic). Le nettoyage des donn√©es et la v√©rification du mod√®le repr√©sente la majorit√© du travail. Les donn√©es journali√®res permettent de pr√©dire jusqu‚Äô90 jours, et les donn√©es la minute un trimestre. Les pr√©dictions sur 1 mois des m√©triques syst√®mes (cpu, ram) sont consid√©r√©es comme fiables alors que les m√©triques business (nombre d‚Äôutilisateurs, nombres de photos ou vid√©os stock√©es) le sont pour 3 ou 4 mois. Pour les √©v√®nements exceptionnels (superbowl, nouvelle ann√©e) ces pr√©dictions ne sont pas assez fiables, ils se basent donc simplement sur les ann√©es pr√©c√©dentes.\n\nWhen dynamic becomes static: the next step in web caching techniques\n\nWim Godden (Cu.Be Solutions)\n\nLe monsieur effectue d‚Äôabord un r√©capitulatif des pratiques de cache dans le web depuis son commencement : sans cache, avec du cache applicatif, avec du reverse proxy cache et enfin avec beaucoup trop de syst√®mes de cache qui rende l‚Äôarchitecture tr√®s complexe (tiens tiens).\n\nPuis apparaissent les ESI, c‚Äôest en gros du reverse proxy cache par bloc (en r√©alit√©, ils sont parmi nous depuis bien longtemps). Mais une limitation conceptuelle √©vidente borne leur utilisation : les sites sont tr√®s souvent personnalis√©s en fonction de l‚Äôutilisateur (affichage du nom de l‚Äôutilisateur connect√© par exemple). Et du coup, les blocs personnalis√©s, m√™me simples, ne peuvent b√©n√©ficier du reverse proxy cache. Ce qui d√©fie un peu le concept.\n\nPour pallier √† ce probl√®me, Wim et son √©quipe ont d√©velopp√© un langage sp√©cifique dans Nginx (qui est aussi un reverse proxy en plus d‚Äô√™tre un serveur http) permettant au serveur web de g√©rer des variables directement dans le reverse proxy afin que celui-ci les stock dans son propre memcache et puisse y acc√©der pour retourner la page au client sans faire un appel suppl√©mentaire au serveur web : SCL.\n\nAlors oui. C‚Äôest pas forc√©ment l‚Äôid√©al de commencer poser des variables dans le reverse proxy. Mais n‚Äôayons crainte, ce n‚Äôest pas pour tout de suite : la release publique ne devrait pas arriver avant mi-2014 :-)\n\nLes slides\n\nDeveloper-Friendly Web Performance Testing in Continuous Integration\n\nMichael Klepikov (Google, Inc)\n\nInt√©grer les mesures/tests de r√©gressions de performance dans nos outils d‚Äôint√©grations continues et une t√¢che tr√®s compliqu√©. Michael pr√©sente une approche assez maligne consistant utiliser les tests fonctionnels d√©jen place, pour r√©colter les mesures des outils de R.U.M. d√©j√† pr√©sent sur le site (soit parce que les mesures sont pr√©sentes dans l‚Äôurl d‚Äôappel de l‚Äôoutil de R.U.M.), soit en r√©cup√©rant les valeurs dans les DevTools de Chrome.\n\nL‚Äôoutil TSviewDB permet d‚Äôavoir une interface qui agr√®ge plusieurs time-series sur une seule time-series (plus d‚Äôinfos dans le Readme du projet).\n\nPas mal d‚Äôinformations √† creuser dans les slides, comme l‚Äôenvoi de donn√©e directe WebPageTest pour utiliser l‚ÄôUI sur le r√©sultat, ou la fa√ßon de r√©cup√©rer les infos de la DevTools de Chrome en vid√©o\n\nLes slides :\n\n\n\nIntegrating multiple CDN providers at Etsy\n\nMarcus Barczak (Etsy), Laurie Denness (Etsy)\n\nPour des raisons de haute disponibilit√©, de r√©silience et de balance des co√ªts, Etsy a mis en place depuis 2012 un syst√®me qui leur permet d‚Äôutiliser de multiples CDN (parmi eux Akamai, Fastly, et EdgeCast). Leur crit√®res d‚Äô√©valuations sont le hit ratio et la d√©charge de trafic des serveurs origine, le reporting, le pilotage via des APIs, la personalisation et l‚Äôacc√®s aux logs HTTP. Ils ont pour cela d√ª faire du m√©nage dans leur codes et dans leurs headers (cache-control, expires, etag, last-modified)\n\nIls ont commenc√© par les images (1% puis 100% du trafic), et se sont servis des CDNs pour effectuer leur tests A/B. L‚Äôequilibrage de charge entre les CDNs se fait manuellement via une interface web ou via un outil en ligne de commande qu‚Äôils ont mis disposition de la communaut√© (cdncontrol sur leur github). L‚Äôinconv√©nient de cette solution est la multiplication des requetes DNS (puisqu‚Äôils utilisent des CNAME type img1.etsystatic.com =&amp;gt; global-ssl.fastly.net par exemple), la non atomicit√© et le delais des modifications DNS qui engendrent une long tail importante et le debug plus complexe. Les requetes depuis les CDNs vers les origines sont track√©s via un header HTTP et sont monitor√©s dans un graphite surveill√© par un nagios selon un seuil d√©termin√©.\n\n\n  ‚ÄúIf you can do it at the origin, do it !‚Äù.\n\n\n\n\nWhat is the Velocity of an Unladen Swallow? A quest for the Holy Grail.\n\nPerry Dyball (Seatwave Ltd) @perrydyball\n\nRetour d‚ÄôXP tr√®s utile, plein d‚Äôhonn√™tet√© et d‚Äôhumilit√©, de SeatWave (site permettant d‚Äôacheter des tickets concerts/spectacle etc), sur l‚Äôeffet douloureux de la premi√®re pub t√©l√© qu‚Äôils ont achet√© pour promouvoir leur service. On d√©couvre la fa√ßon dont ils ont su optimiser leur application pour supporter les publicit√©s suivantes, gr√¢ce un syst√®me de queuing avec un d√©compte en cas de fortes charges, ainsi que les impacts sur une multitude de m√©triques et le cot√© financier.\n\nLe ph√©nom√®ne est presque un running gag chez nous (ou m√™me sur Twitter), quand votre (ou m√™me l‚Äôun de nos ‚Ä¶) site est pr√©sent√© dans une pub ou Capital, et que votre application et/ou serveur ne supporte pas la charge.\n\nBref, encore un bel exemple de culture d‚Äôentreprise, qui d√©montre que la performance n‚Äôest pas un projet ou une feature ‚Äúone shot‚Äù, mais une culture et une mentalit√© constante.\n\n\n  ‚ÄúPerformance it‚Äôs not just for today, it‚Äôs for every day‚Äù Peter Dyball\n\n\n\n\nGetting 100B metrics to disk\n\nJonathan Thurman (New Relic)\n\nNew relic a pr√©sent√© l‚Äôarchitecture MySQL qui stocke leur 196 milliards de m√©triques journali√®res. Elle est bas√©e sur des shards MySQL, propuls√©e par de puissants serveurs (12 actuellement) √©quip√©s de SSD Intel et de shelf de disques Dell. Les shards sont fait (via leur API shardGuard) par num√©ro de client, et les tables MySQL sont construites sur le mod√®le num√©roClient_year_julianDay_metricResolution. Il y a environ 200.000 tables par databases. Les m√©triques sont r√©gulierement (toutes les heures), purg√©es et aggr√©g√©es, en utilisant le innodb_lazy_drop_table de percona 5.5 (et surtout par delete from ou drop table).\n\nLe code initialement en Ruby est pass√© Java/Jetty. Les inserts se font s√©quentiellement en batch de 5000 et sont buffuris√©s en RAM (ils doivent se faire en moins d‚Äôune minute, r√©solution minimale du produit). Ils pr√©voient d‚Äôutilis√© de multiples instances MySQL par serveurs et de gonfler leur capacit√© hardware (SSD 800G, 96G RAM)\n\n\n\nHigh Velocity Migration\n\nJoshua Hoffman (SoundCloud) @oshu\n\nJoshua nous a cont√© l‚Äôhistoire d‚Äôune startup (fictive mais pas vraiment car c‚Äôest celle de Tumblr), qui a commenc√© en 2006 entre deux amis souhaitant partager des images de parties d‚Äô√©checs et comment en 2012 elle a d√ª, en 6H de maintenance, basculer 1200 serveurs et les donn√©es de 50M d‚Äôutilisateurs. Il nous a d√©taill√© l‚Äô√©volution ann√©e par ann√©e de l‚Äôinfrastructure et du nombre de devs/sysadmins. Les ingr√©dients pour g√©rer une croissance comme celle ci sont selon lui : le provisionning automatique (ipxe, kickstart), la gestion de la configuration (puppet/chef/ansible), le monitoring et l‚Äôalerting et les outils de d√©ploiement de code. Il pr√©cis√© qu‚Äôil faut accepter l‚Äôimperfection de ses outils, ne pas chercher r√©inventer la roue mais plut√¥t utiliser l‚Äôopen source, ne pas h√©siter tuer les projets Zombies (ceux qui durent depuis trop longtemps et qui n‚Äôont pas les fonctionnalit√©s attendues !) et surtout respecter le principe KISS (keep it simple and stupid).\n\nLa migration en 2012 de leur plateforme g√©r√©e par une soci√©t√© tierce vers leur propre infrastructure a commenc√© 120 jours plus t√¥t avec l‚Äôinstallation des serveurs, machines, syst√®mes de d√©ploiements, l‚Äôacquisition de leur num√©ro d‚ÄôAS, et l‚Äôutilisation en front d‚Äôun proxy pour plus tard pouvoir rediriger le trafic de fa√ßon transparente vers la nouvelle infrastructure. Le jour J la fenetre de maintenance du site de 6H √©t√© suffisante pour synchroniser les donn√©es utilisateurs entre les deux infras, tester et mettre en production.\n\n\n\nCode is Evil\n\nDan Rathbone (British Sky Broadcasting)\n\nFace aux probl√®mes de performance du site https://www.skybet.com/, qui doit probablement attirer une quantit√© impressionnante de parieurs tout en devant afficher des donn√©es tr√®s fra√Æches (cela fait partie du business model), Dan a remis plat toute la logique de d√©veloppement du site.\n\nLorsque la performance passe seule au premier plan, il est ainsi possible de renverser le paradigme du d√©veloppement dans son ensemble : alors qu‚Äôen g√©n√©ral, les donn√©es sont stock√©es structur√©es puis extraites pour peupler du code m√©tier puis affich√©es par des templates √©labor√©s, dans ce cas particulier, les donn√©es sont directement stock√©es de mani√®res d√©normalis√©es, directement pr√™tes √™tre affich√©es par des templates simplistes. Le code m√©tier est en amont et sert pr√©-calcul√© les donn√©es qui sont stock√©es en base.\n\nIl est ainsi possible de minimiser drastiquement la quantit√© de code critique. Et cela ouvre beaucoup de portes : peu de code = peu de maintenance, aucun framework n√©cessaire, aucun cache n√©cessaire, etc.\n\nC‚Äô√©tait une pr√©sentation assez pol√©mique mais particuli√®rement int√©ressante (ce qui n‚Äô√©tait pas l‚Äôavis de l‚Äôaudience, semblerait-il) et rafra√Æchissante car elle permet de sortir des cas standards du monde du web. Entre nous, tous ces principes √©taient d√©jen vogue dans le d√©veloppement des jeux vid√©o dans les ann√©es 90 : nous devions constamment contourner la limitation du mat√©riel (les optimisations √©taient au cycle processeur pr√®s).\n\nBreaking 1000ms Mobile Barrier\n\nIlya Grigorik (Google)\n\n\n\nComment arriver afficher sa page web sans d√©passer la barri√®re de 1000 ms ! Un dur challenge dont les √©preuves sont d√©taill√©s par Ilya.\n\n\n\nDes probl√®matiques de latence sur le ‚ÄúTouch‚Äù mobile, sur les communications 3G/4G, du fonctionnement TCP, du critical rendering path au niveau CSS et JS, mod_page_speed et ngx_page_speed, ainsi que des √©volutions venir sur Page Speed Insights, c‚Äôest un panel ultra complet de la WebPerf qui √©t√© couvert sur cette heure ultra dense, mais oh combien indispensable. C‚Äôest donc, comment souvent avec Ilya Grigorik, un must read absolu pour ceux que la Performance Front-End et Mobile, ainsi que la latence, passionne.\n\nLes slides sont ici https://docs.google.com/presentation/d/1wAxB5DPN-rcelwbGO6lCOus_S1rP24LMqA8m1eXEDRo/present#slide=id.p19\n\nLive Sketching !\n\nAvant de conclure, petit hommage Natalia Talkowska , qui, sur chaque conf√©rence, r√©alisait un live sketching d‚Äôune qualit√© incroyable\n\n@Natalka_Design #livesketching is back with @allspaw @souders and @courtneynash opening up #velocityconf, let&amp;#39;s go! pic.twitter.com/FYBQIVk8tr&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @psd at #velocityconf as first #keynote! pic.twitter.com/9zAMXJZNWW&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @keynotesystems at #velocityconf pic.twitter.com/S8XYaNFKzU&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @LMAX at #velocityconf pic.twitter.com/UYLYDTSuPO&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @kenzenhofer at #velocityconf pic.twitter.com/l2ndwj8V1G&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nMust follow: @NatiTal: #livesketching @psd at #velocityconf as first #keynote! pic.twitter.com/W8xTTjC581 #Awesomeness&amp;mdash; Mike Hendrickson (@mikehatora) November 14, 2013\n\n\n#livesketching @triblondon at #velocityconf pic.twitter.com/VTF2gZFEsH&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @thedevmgr at #velocityconf pic.twitter.com/iaaZgRxVwN&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nNot a bad likeness! ‚Äú@NatiTal: #livesketching @triblondon at #velocityconf pic.twitter.com/Pq4NiAPEW7‚Äù&amp;mdash; Andrew Betts (@triblondon) November 14, 2013\n\n\n#livesketching @igrigorik at #velocityconf pic.twitter.com/5rBPelS9an&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @edgecast at #velocityconf last #keynote pic.twitter.com/rZ5Pa1MvhQ&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nConclusion :\n\nC‚Äôest compl√®tement lessiv√© que nous sortons de cette journ√©e, avec une quantit√© d‚Äôid√©es / projets tester incroyable.\n\nVous pouvez retrouver le compte rendu de la premi√®re journ√©e ainsi que de la derni√®re sur notre Blog.\n\nN‚Äôh√©sites pas donner vos retours (positifs ou n√©gatifs en commentaire). Merci :-)\n\n¬© des photos : Flickr officiel O‚ÄôReilly\n\nCR r√©dig√© par Baptiste, Denis Roussel et Kenny Dits\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 1",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-1.html",
    "date"     : "November 14, 2013",
    "excerpt"  : "Introduction :\n\n\n\nNous voici de retour √† Londres pour la troisi√®me √©dition de la V√©locity Europe, qui se d√©roule, pour la deuxi√®me fois √† Londres (la pr√©c√©dente √©tait √† Berlin).\n\nPour rappel, la V√©locity est la conf√©rence autour de la performance ...",
  "content"  : "Introduction :\n\n\n\nNous voici de retour √† Londres pour la troisi√®me √©dition de la V√©locity Europe, qui se d√©roule, pour la deuxi√®me fois √† Londres (la pr√©c√©dente √©tait √† Berlin).\n\nPour rappel, la V√©locity est la conf√©rence autour de la performance web. Qu‚Äôelle soit Front-End, Back-End, D√©v ou Ops. C‚Äôest l‚Äô√©v√©nement de l‚Äôann√©e √† ne pas manquer en Europe, ou aux US (ou Chine) pour les plus chanceux\n\nCette premi√®re journ√©e (ayant eu lieu le 13 novembre 2013) est ax√©e sur le signe des ‚ÄúTutorials‚Äù. De looongues conf√©rences de 90 minutes dont voici le compte rendu √©crit √† 6 mains.\n\nLa conf√©rence ‚Äúclassique‚Äù commence le 14 et se d√©roulera sur deux journ√©es.\n\nGone in 60 frames per second\n\nAddy Osmani (Google Chrome) @addyosmani\n\n\n\nAddy est une figure incontournable du web. Cr√©ateur de TodoMVC, Lead d√©v de Yeoman et travail dans la Google Chrome Team sur les outils √† destination des d√©veloppeurs autour du navigateur.\n\nApr√®s la g√©n√©ration du code html par les serveurs et le transfert de ce code par les r√©seaux, le rendu graphique de la page par le navigateur est le dernier √©v√®nement significatif du chargement de la page lors de la consultation d‚Äôun site par un client.\n\nVoici donc un r√©sum√© des bonnes pratiques permettant d‚Äôobtenir un meilleur framerate (nombre de rafra√Æchissement de la page par seconde) et ainsi une meilleure fluidit√© lors de la navigation :\n\n\n  disposer des images √† la bonne taille pour √©viter les redimenssionnements √† la vol√©e,\n  limiter les handlers sur l‚Äô√©v√©nement onScroll(),\n  limiter tous les √©l√©ments ‚Äòfixed‚Äô car cela force le navigateur √† recalculer constamment la zone affich√©e (ou utiliser l‚Äôastuce translateZ(0)),\n  \n    limiter les directives CSS qui n√©cessites un calcul suppl√©mentaire (lorsque tout est d√©j√† affich√©) :\n  \n  les ombres,\n  les flous,\n  et les d√©grad√©s : (Bootstrap a supprim√© tous les d√©grad√©s sur ses boutons : +100% de rapidit√© l‚Äôaffichage).\n\n\nEnsuite, il reste quelques conseils plus g√©n√©raux :\n\n\n  Il faut se souvenir que les performances des t√©l√©phones ne sont pas celles des PC,\n  un framerate de 60 fps est parfait (c‚Äôest d√ª au mat√©riel), mais un framerate de 30 fps peut aussi √™tre suffisant pour peu qu‚Äôil soit constant,\n\n\nEnfin, comme souvent, tous les outils pour comprendre et am√©liorer le rendu graphique de ses pages web sont disponible dans tous les navigateurs. Dans Chrome, il suffit d‚Äôaller dans la section ‚ÄúFrames‚Äù de l‚Äôonglet ‚ÄúTimeline‚Äù des DevTools.\n\nLes slides sont disponible ici : https://speakerdeck.com/addyosmani/velocityconf-rendering-performance-case-studies\n\nLa pr√©sentation de la conf√©rence par l‚Äôauteur lui-m√™me : https://addyosmani.com/blog/making-a-site-jank-free/\n\n\n\n\n\nVid√©o de la m√™me conf√©rence (donn√©e √† la Smashing Conf 2013)\n\nBring the noise : Making effective use of a quarter million metrics\n\nJon Cowie (Etsy) @jonlives\n\n\n\nJon est ‚ÄúOps Engineer‚Äù chez Etsy (Dont le VP, John Allspaw, co-organise avec Steve Souders, la V√©locity).\n\nQuelques donn√©es sur Etsy :\n\n\n  Ils font du d√©ploiement continu\n  1.5 milliards de pages vues\n  250 contributeurs (tout le monde d√©ploie du code, m√™me les chiens)\n  ils utilisent Deployinator pour d√©ployer leur code avec un unique ‚Äúbouton‚Äù, et schemanator pour les migrations SQL\n  60 d√©ploiements par jour / 8 commit par deploiement\n  ¬º millions de m√©triques !\n\n\n\n  ‚ÄúWe optimize for quick recovery by anticipating problems instead of fearing human error‚Äù John Cowie\n\n\n\n  ‚ÄúCan‚Äôt Fix what you don‚Äôt measure‚Äù W. Edwards Deming\n\n\nLeurs outils pour le monitoring :\n\n\n  \n    Not homemade :\n  \n  Ganglia\n  Graphite\n  Nagios\n  \n    Homemade :\n  \n  StatsD : Simple Daemon for easy stats integration\n  Supergrep : Real time log streamer\n  Skyline : A real time anomaly detection system\n  Oculus : A metric correlation component\n\n\n\n  ‚ÄúNot All things that break throw errors‚Äù Oscar Wilde\n\n\n\n  ‚ÄúIf it moves, graph it ! If it doesn‚Äôt move, graph it anyway‚Äù Jon Cowie\n\n\nLa pr√©sentation s‚Äôaxe ensuite plus particuli√®rement sur la stack ‚ÄúKale‚Äù, qui englobe deux outils que l‚Äôon va d√©tailler : Skyline et Oculus. Voir l‚Äôarticle sur le blog technique de Etsy https://codeascraft.com/2013/06/11/introducing-kale/\n\nL‚Äôobjectif de Skyline, est de d√©tecter les comportements anormaux (gros pics par exemple), avec pour principal challenge, la r√©cup√©ration des donn√©es (via le ‚Äúrelay agent‚Äù de Graphite, ils envoient en continue les donn√©es dans Redis via redis.append() ), le stockage de 250 000 m√©triques (dans Redis) au format MessagePack. Oculus quand lui permet de corr√©ler les m√©triques, en utilisant les donn√©es brutes de l‚Äôapi de Graphite, car il est bien plus efficace de comparer des chiffres, que des images ‚Ä¶\n\nIl n‚Äôy a pas un mais huit algorithmes de d√©tections d‚Äôanomalies qui sont utilis√©s dans un vote √† majorit√©, d√©terminant ainsi si l‚Äôanomalie est av√©r√©e (parmi ceux ci, OLS, Grubb‚Äôs test, l‚Äôhistogramme bining etc‚Ä¶). La d√©tection se fait sur une fen√™tre d‚Äôune heure et une seconde de 24 heures. Skyline souffre encore de quelques faiblesses: l‚Äôabsence de prise en compte de la saisonnalit√©, les pics qui peuvent en cacher d‚Äôautres plus faibles, le postulat pas toujours vrai que les donn√©es sont normalement distribu√©es et les corr√©lations n√©gatives.\n\nIls comparent donc la distance euclidienne (slide 99), en g√©rant aussi le d√©calage temporel (dynamic time warping / DTW) (voir slide 100).\n\nLa partie la plus int√©ressante est la simplification d‚Äôune m√©trique temporelle, en la normalisant sur une courbe √©chelle r√©duite (de 0 √† 25), et en la transformant en une chaine textuelle comportant cinq valeurs :\n\n\n  sharpdecrement\n  decrement\n  flat\n  increment\n  sharpincrement\n\n\nEt ceci en fonction de la valeur en cours par rapport √† la valeur pr√©cedente.\n\nIls poussent toutes ces m√©triques normalis√©es dans Elastic Search dans un champ non tokenis√© en r√©alisant des recherches de phrases afin de corr√©ler les m√©triques ayant le m√™me pattern et en scorant via un plugin cod√© par leurs soins (incluant une version ‚Äúrapide‚Äù du DTW).\n\nUne fois les m√©triques corr√©l√©es affich√©es, il est possible de sauvegarder un snapshot de ces derni√®res et d‚Äôinclure des commentaires dans une ‚Äúcollection‚Äù. Cela permet notamment de construire une base de donn√©es de connaissance sur les incidents ou les comportements anormaux mais explicables.\n\nSkyline est visible par tous dans leur bureaux, sur l‚Äôun des 6 √©crans de dashboards, devant lesquels on peut notamment lire le nombre de requ√™tes HTTP par seconde, le top 10 des pages, les temps de g√©n√©rations et d‚Äôaffichage etc‚Ä¶\n\nLes slides sont disponible ici : https://www.slideshare.net/jonlives/bring-the-noise\n\n\n\nResponsive images Technique and Beyond\n\nYoav Weiss (WL Square) @yoavweiss\n\n\n\nYoav est un sp√©cialiste de la WebPerf et travaille sur les probl√©matiques des images li√©es au Responsive Web Design. Il est aussi Technical Lead au RICG (Responsive images community Group)\n\nLe principal probl√®me des images responsive, c‚Äôest de charger l‚Äôimage correctement dimensionn√©e par rapport √† une page, de mani√®re efficace.\n\n72% des sites RWD servent les m√™mes ressources entre les r√©solutions petites et grandes ‚Ä¶\n\nOn peut √©conomiser 72% en taille d‚Äôimage en compressant correctement (voir https://timkadlec.com/2013/06/why-we-need-responsive-images/).\n\nYoav a d√©velopp√© un outil utilisant PhantomJs, permettant de mesurer la diff√©rence entre les images charg√©es, et celle qui seraient correctement dimensionn√©es : Sizer Soze\n\nOn aborde ensuite les deux cas principaux g√™nant :\n\n\n  Servir une dimension diff√©rentes de l‚Äôimage √† diff√©rents support. (et les Retina uniquement aux devices le supportant)\n  et le ‚ÄúArt direction‚Äù, avoir une image qui correspond au layout\n\n\nAinsi que l‚Äôint√©r√™t du Pre-loader, souvent peu connu. Beaucoup plus d‚Äôinfos sur cet article d‚ÄôAndy Davies (https://andydavies.me/blog/2013/10/22/how-the-browser-pre-loader-makes-pages-load-faster/)\n\nYoav parcours ensuite toutes les techniques des images responsive avec avantages/inconv√©nients et exemple pour chacune, que vous pouvez retrouvez d√®s la slide 57 de la pr√©sentation ci apr√®s : https://yoavweiss.github.io/velocity-eu-13-presentation/#/\n\nL‚Äô√©tude et les retours sont extr√™mement complet, et immanquable, si vous travaillez ou allez travaillez sur le sujet. Il aborde aussi une approche en cours d‚Äô√©tude, qui verra peut √™tre le jour prochainement (Responsive Image Container).\n\nPerformance Analysis of JVM components for non-specialists\n\nBen Evans (JClarity) @kittylyst\n\n\n\nLa performance et la complexit√© des applications qui fonctionnent sur la JVM ont suivi l‚Äô√©volution de la loi de Moore. Malgr√© que nous ayons gagn√© de la puissance et des transistors, notre code s‚Äôest complexifi√© d‚Äôann√©e en ann√©e et d‚Äôautant plus avec le boom d‚ÄôInternet.\n\nLe tuning de la JVM est indispensable pour avoir une application performante et doit se faire de fa√ßon rigoureuse et scientifique, il faut comprendre, mesurer, tester, v√©rifier et r√©p√©ter ce processus jusqu‚Äôce que l‚Äôon consid√®re la performance comme bonne.\n\nBen a ensuite d√©taill√© l‚Äôanatomie de la JVM, les sp√©cificit√©s du langage Java, les ‚Äúmid 90‚Äôs decisions design‚Äù qui ont √©t√© faites, comment est g√©r√© l‚Äôallocation m√©moire, la heap, et le fonctionnement du garbage collector (mark and sweep, stop the world). La dur√©e du ‚Äústop the world‚Äù est ridicule compar√© aux temps de latence r√©seau, ceux engendr√©s par l‚Äôhyperviseur etc‚Ä¶\n\nIl a pr√©sent√© quelques optimisations indispensables selon lui, et a insist√© sur le fait que l‚Äôoptimisation pr√©matur√©e pouvait √™tre la source de bien des soucis cot√© code.\n\nTuning Network Performance to Eleven\n\nIlya Grigorik (Google) @igrigorik\n\n\n\nAKA comment condenser un livre dans un tutorial d‚Äô1H30. Exercice encore plus difficile lorsqu‚Äôil faut r√©sumer le r√©sum√© d‚Äôun livre aussi dense et complet. Ilya en tant que sp√©cialiste de la webperf a examin√© les m√©canismes de la latence et de la bande passante, le fonctionnement du protocole TCP, la gestion de congestion, les probl√®mes structurels de HTTP 1.0 et HTTP 1.1, l‚Äôimpact de TLS (le chiffrement) sur les performances. Il a donn√© ses recommandations pour optimiser TCP et bien utilis√© TLS.\n\n‚Äúbandwidth + latence =~ performance‚Äù\n\n\n  ‚ÄúVideo streaming is bandwidth limited, web browsing is latency limited‚Äù Ilya Grigorik\n\n\nIl a ensuite expliqu√© comment fonctionne le r√©seau radio 2G/3G/4G et les contraintes que ces architectures exercent sur les temps de chargement et la dur√©e de vie des batteries pour les appareils mobiles.\n\nLe tutorial s‚Äôest achev√© sur les d√©fauts de HTTP 1.1 et les nouveaut√©s (nombreuses et sexys) d‚ÄôHTTP 2.0. Ce fut extr√™mement plaisant d‚Äôassister √† cette pr√©sentation, tant Ilya est pointu techniquement, pr√©cis et didactique dans ses d√©monstrations. Le livre est un MUST-READ !\n\nIl est d‚Äôailleurs disponible gratuitement ici : https://chimera.labs.oreilly.com/books/1230000000545\n\nLes slides sont disponible ici\n\nBe Mean to your code with Gauntlt and the Rugged Way\n\nJames wickett (Mentor Graphics) @wickett\n\n\n\nCette pr√©sentation fut le seul et unique vrai ‚ÄúWorkshop‚Äù du jour, dans le sens o√π une machine virtuelle (monter avec Vagrant) √©tait fournie pour r√©aliser l‚Äôatelier au fur et mesure de la pr√©sentation sur sa machine.\n\nGauntlt est un framework autour de la s√©curit√©, qui fournie des hooks pour de nombreux outils d‚Äôattaques (Xss, Sql injection etc ‚Ä¶).\n\nApr√®s une introduction un peu longue autour de la place de la ‚Äús√©curit√©‚Äù aux seins de nos services.\n\nL‚Äôapproche de Gauntlt est bas√©e sur le ‚ÄúRugged Manifesto‚Äù\n\nGauntlt permet donc d‚Äôautomatiser au sein de son syst√®me d‚Äôint√©gration continue, des tests autour de la s√©curit√© de son applicatif et de son infra, bas√©s sur Cucumber, utilisant le langage Gherkin (que certains connaissent peut √™tre mieux dans le monde php via Behat), et interfa√ßant des outils tels que :\n\n\n  Garmr\n  Nmap\n  Arachni\n  Sqlmap\n  ‚Ä¶\n\n\nSi vous voulez tester l‚Äôoutil, qui √† l‚Äôair tr√®s prometteur, vous pouvez suivre ce tutoriel : https://bit.ly/gauntlt-demo-instructions qui vous fourni la Virtual Box, les consignes d‚Äôinstallations, et les exemples ayant √©t√© r√©alis√©s pendant la conf√©rence, ainsi qu‚Äôune application de test en Ruby Railsgoat pour servir de cible √† vos tests.\n\nLes slides sont disponible ici\n\n\n\nHands-on Web Performance Optimization Workshop\n\nAndy Davies (Asteno) @andydavies , Tobias Baldauf (Freelancer) @tbaldauf\n\n\n\nDerni√®re session de la journ√©e, avec Andy et Tobias, sur un workshop ax√© Performance Web.\n\nOn commence par une pr√©sentation g√©n√©ral d‚Äôun outil qu‚Äôon ne devrait plus pr√©senter : WebPageTest, l‚Äôoutil principal pour les probl√©matiques de performances front-end.\n\nAndy aborde ensuite quelques autres outils :\n\n\n  PhantomJs (un headless browser)\n  Simple Website Speed Test\n  et surtout Phantomas, un module PhantomJs pour collecter les m√©triques de Webperf.\n  le wrapper Node.Js pour WebPageTest de Marcel Duran\n  SiteSpeed.io pour monitorer toutes les pages de son site, bas√© notamment sur Yslow\n  HttpArchive, l‚Äôexcellent service de Steve Souders qui tracke le web avec une multitude de stats int√©ressante, que vous pouvez d‚Äôailleurs installer pour une instance priv√©e afin de tracker vos sites : https://bbinto.wordpress.com/2013/03/25/setup-your-own-http-archive-to-track-and-query-your-site-trends/ \\o/\n\n\nLa suite de la conf√©rence consister a analyser en live certains sites dont quelques uns assez hilarant au niveau performance :\n\n\n  Dailymail.co.uk avec ces +de 800 requ√™tes HTTP et 7 mo !\n  Wildbit.com qui consomme un CPU √©norme cause de l‚Äôanimation sur le logo qu‚Äôon ne voit quasiment pas :)\n\n\nLes slides :\n\n\n\nConclusion :\n\nBonne premi√®re journ√©e avec ce format ‚ÄúTutorials‚Äù un peu trop touffu (90 minutes par conf√©rence ‚Ä¶). D√©j√† des tonnes d‚Äôid√©es qui ressortent, on a h√¢te de voir la suite.\n\nRetrouvez les autres CR :\n\n\n  Compte rendu du jour 2 \n  Compte rendu du jour 3 \n\n\n¬© des photos : Flickr officiel O‚ÄôReilly\n\nCR r√©dig√© par Baptiste, Denis Roussel et Kenny Dits\n"
} ,
  
  {
    "title"    : "Tester fonctionnellement une API REST",
    "category" : "",
    "tags"     : " qualite, symfony, atoum, tests fonctionnels",
    "url"      : "/2013/10/tester-fonctionnellement-une-api-rest-symfony-doctrine-atoum",
    "date"     : "October 14, 2013",
    "excerpt"  : "Un des enjeux des tests fonctionnels est de pouvoir √™tre jou√©s dans un environnement compl√®tement ind√©pendant, dissoci√© de l‚Äôenvironnement de production, afin de ne pas √™tre tributaires de donn√©es versatiles qui pourraient impacter leur r√©sultat. ...",
  "content"  : "Un des enjeux des tests fonctionnels est de pouvoir √™tre jou√©s dans un environnement compl√®tement ind√©pendant, dissoci√© de l‚Äôenvironnement de production, afin de ne pas √™tre tributaires de donn√©es versatiles qui pourraient impacter leur r√©sultat. Il faut, cependant, que cet environnement soit techniquement similaire √† celui de production pour que les tests aient une r√©elle validit√© fonctionnelle.\n\nAvec la Team Cytron, nous sommes tomb√©s face √† cette probl√©matique lorsque nous avons voulu tester fonctionnellement un service agnostique de contenu mettant √† disposition une API REST et utilisant Symfony2, MySQL, Doctrine et atoum.\n\nMonter un serveur de donn√©es d√©di√© aux tests\n\nDans le cas d‚Äôune application utilisant MySQL, on pense alors monter un serveur applicatif de test reli√© √† une base de donn√©es de test. Plusieurs probl√®mes peuvent alors d√©couler d‚Äôun tel syst√®me :\n\n\n  il faut √™tre en mesure de pouvoir mettre en ≈ìuvre un serveur MySQL d√©di√© uniquement aux tests,\n  mais surtout cette architecture n‚Äôest pas exploitable pour ex√©cuter des tests de mani√®re concurrentielle (ce qui pose probl√®me pour l‚Äôint√©gration continue). En effet, des collisions appara√Ætraient en base de donn√©es et le r√©sultat des tests ne seraient plus exploitables.\n\n\nMocker Doctrine\n\nNotre seconde r√©action a √©t√© de vouloir mocker Doctrine pour devenir ind√©pendant de MySQL. Lourde t√¢che.\n\nTant bien que mal, nous sommes arriv√©s √† un r√©sultat plut√¥t satisfaisant car notre API r√©alise des op√©rations simples : ajout, modification, suppression et consultation avec un filtrage √©l√©mentaire.\n\nLa premi√®re chose √† faire est de s‚Äôassurer que notre serveur de test n‚Äôacc√®de pas aux donn√©es de production dans MySQL en changeant la configuration Doctrine dans le fichier config_test.yml.\n\n\n\nEnsuite, nous avons cr√©√© une classe abstraite dont h√©ritent toutes nos classes de test, et qui permet d‚Äôinitialiser le mock de Doctrine.\n\nAutant vous dire que le d√©veloppement de cette classe a √©t√© fastidieux car incr√©mental : chaque nouveau besoin de manipulation de donn√©es dans nos tests, il a fallu modifier le mock pour prendre en compte des m√©thodes ou des fonctionnalit√©s de m√©thodes qui n‚Äôavaient pas √©t√© encore mock√©es (comme le filtrage par crit√®res dans la fonction findBy).\n\nLes possibilit√©s de ce mock reste limit√©es. Nous sommes, par exemple, tomb√©s sur le cas o√π deux managers de donn√©es en relation (des recettes et leurs ingr√©dients) d√©pendaient d‚Äôun m√™me EntityManager Doctrine : tel que nous l‚Äôavons d√©velopp√©, le mock ne sait pas g√©rer cette situation et engendre des erreurs √† l‚Äôex√©cution. Il aurait fallu refactoriser le code pour parvenir √† nos fins et passer encore plus de temps sur ce projet‚Ä¶ et nous n‚Äôen avions pas beaucoup !\n\nAutre probl√®me : nous utilisons des fonctionnalit√©s de la librairie Gedmo/DoctrineExtensions pour la gestion automatique des dates de cr√©ation et de modification. √âvidemment, elles ne sont pas op√©rationnelles avec notre mock et nous aurions encore d√ª d√©velopper pour faire passer nos tests.\n\nUtiliser les transactions de Doctrine\n\nIl a donc fallu nous rendre l‚Äô√©vidence : cette solution ne correspondait pas √† nos besoins ! Nous avons alors √©mis l‚Äôhypoth√®se d‚Äôune alternative qui nous permettrait peut-√™tre de nous passer d‚Äôune config sp√©cifique MySQL pour nos tests : l‚Äôutilisation des transactions via Doctrine.\n\nAu d√©but de chaque test, nous aurions ouvert une transaction mais qui n‚Äôaurait jamais √©t√© commit√©e par la suite, √©vitant toute interaction avec la base de donn√©es de production. Mais avec cette solution, dangereuse √† mettre en place et √† maintenir, nous aurions couru le risque de modifier des donn√©es de production.\n\nRemplacer MySQL par un autre SGBD uniquement pour les tests\n\nFinalement, nous sommes partis sur une autre piste, celle qui fait actuellement tourner nos tests fonctionnels sur ce projet. Nous utilisons SQLite dans notre environnement de test la place de MySQL. Ce SGBD est tr√®s l√©ger et simple mettre en ≈ìuvre : pas besoin d‚Äôune installation sur un serveur d√©di√©, il suffit simplement d‚Äôactiver une extension de PHP. SQLite se base sur des fichiers physiques pour g√©rer le stockage des donn√©es. Ainsi, chaque build de test peut avoir ses propres fichiers de BDD dans son r√©pertoire √©vitant toute collision dans le cas de tests concurrentiels.\n\nNous avons donc configur√© Doctrine, pour qu‚Äôil utilise SQLite lors de son ex√©cution en environnement de test en modifiant le config_test.yml\n\n\n\nComme pour le mock de Doctrine, nous avons mis en place une classe abstraite qui permet de g√©rer la r√©initialisation de la base pour chaque test.\n\nNous pouvons donc maintenant tester unitairement et fonctionnellement notre API REST d√©velopp√©e en PHP l‚Äôaide de Symfony2 et Doctrine. Et nous ne nous en privons pas : notre API est couverte par bient√¥t 5.000 assertions.\n\nG√©n√©ration des donn√©es de test\n\nApr√®s avoir trouv√© une solution pour l‚Äôacc√®s √† la structure de donn√©es en environnement de test, nous nous sommes pench√©s sur la question du contenu de ces donn√©es de tests. Pas longtemps.\n\nNotre service REST permettant des op√©rations CRUD, nous partons pour chaque test d‚Äôun contenu vide que nous remplissons √† l‚Äôaide de notre propre service. Cela permet de tester beaucoup plus de cas d‚Äôutilisation. Mais surtout cela permet aussi de tester des cas plus r√©els, plus proches de son utilisation par nos clients.\n"
} ,
  
  {
    "title"    : "Distribuez votre vid√©o partout avec 3 euros en poche et devenez millionaire. Ou presque.",
    "category" : "",
    "tags"     : " lft, video",
    "url"      : "/2013/10/distribuez-votre-video-partout-avec-3-euros-en-poche-et-devenez-millionaire-ou-presque.html",
    "date"     : "October 9, 2013",
    "excerpt"  : "‚ÄúComment gagner des millions, sans sortir de chez vous, en robe de chambre, en distribuant des vid√©os de chats sur les internets, gr√¢ce √† ffmeg, h264, dash, tous pleins de buzz word, justin bieber‚Äù (Merci ! Toute l‚Äô√©quipe SEO).\n\nUne pr√©sentation d...",
  "content"  : "‚ÄúComment gagner des millions, sans sortir de chez vous, en robe de chambre, en distribuant des vid√©os de chats sur les internets, gr√¢ce √† ffmeg, h264, dash, tous pleins de buzz word, justin bieber‚Äù (Merci ! Toute l‚Äô√©quipe SEO).\n\nUne pr√©sentation de Ludovic Bostral, notre ex valeureux responsable R&amp;amp;D en charge - jusqu‚Äôil y a peu de temps - de la fabrication de toutes nos vid√©os et du SI associ√©.\n\nSi vous vous posez des questions ce sujet, n‚Äôh√©sitez pas √† venir lui faire un petit coucou virtuel, ou sur Nantes. Ca marche aussi pour discuter zombie ou nanar. Ou mieux, un nanar avec des zombies !\n\nRetrouvez Ludovic sur son site : https://digibos.com.\n\n"
} ,
  
  {
    "title"    : "Le NoSQL, Focus sur MongoDB par C√©dric Derue (Altran)",
    "category" : "",
    "tags"     : " lft, nosql, mongodb, video",
    "url"      : "/le-nosql-focus-sur-mongodb-par-cedric-derue-altran",
    "date"     : "October 8, 2013",
    "excerpt"  : "Porte-√©tandard des bases de donn√©es NoSQL de type document, MongoDB nous a √©t√© pr√©sent√© cet √©t√© par C√©dric Derue (@cderue) , de la soci√©t√© Altran, lors de nos conf√©rences internes.\n\nDans cette pr√©sentation d‚Äôenviron une heure, il aborde un tour d‚Äô...",
  "content"  : "Porte-√©tandard des bases de donn√©es NoSQL de type document, MongoDB nous a √©t√© pr√©sent√© cet √©t√© par C√©dric Derue (@cderue) , de la soci√©t√© Altran, lors de nos conf√©rences internes.\n\nDans cette pr√©sentation d‚Äôenviron une heure, il aborde un tour d‚Äôhorizon des diff√©rentes cat√©gories de bases de donn√©es NoSQL, pour s‚Äôattacher ensuite sur un focus assez complet de MongoDB, agr√©ment√© de quelques d√©monstrations.\n\nMerci √† Altran et C√©dric pour le partage de cette pr√©sentation.\n\nVous pouvez aussi retrouver d‚Äôautres sessions de nos Last Friday Talk :\n\n\n  Introduction Drupal par Claire Roubey (Clever Age)\n  Redis on Fire\n  La POO Canada Dry\n\n\nMalheureusement, la vid√©o n‚Äôest plus disponible‚Ä¶\n"
} ,
  
  {
    "title"    : "Vigo, le fl√©au des Carpates",
    "category" : "",
    "tags"     : " outil, qualite, javascript, tests fonctionnels",
    "url"      : "/vigo-le-fleau-des-carpates-la-tristesse-de-moldavie",
    "date"     : "August 13, 2013",
    "excerpt"  : "CasperJS permet d‚Äô√©crire des scripts javascript qui vont automatiser des tests fonctionnels de pages web. Il ex√©cute ces tests dans une instance de PhantomJS qui est un navigateur scriptable et sans interface graphique (‚ÄúHeadless‚Äù dit-on dans le m...",
  "content"  : "CasperJS permet d‚Äô√©crire des scripts javascript qui vont automatiser des tests fonctionnels de pages web. Il ex√©cute ces tests dans une instance de PhantomJS qui est un navigateur scriptable et sans interface graphique (‚ÄúHeadless‚Äù dit-on dans le milieu).\n\nAfin de mieux structurer nos tests, de faciliter leur √©criture et de pouvoir les lancer avec une commande unique, nous avons cr√©√© VigoJS, une surcouche pour CasperJS.\n\nFonctionnalit√©s\n\nToutes les fonctionnalit√©s de base de CasperJS sont accessibles. Nous y avons simplement ajout√© un m√©canisme de configuration contenant plusieurs param√®tres de base dont l‚ÄôURL de test par d√©faut, l‚Äôauthentification HTTP √©ventuelle ou encore la taille de l‚Äô√©cran virtuel. Il est √©galement possible de sp√©cifier des environnements (dev, preprod, prod‚Ä¶) pour diff√©rencier les comportements de certains tests. Ainsi, en fonction de l‚Äôenvironnement demand√© dans la ligne de commande, les tests peuvent √™tre jou√©s sur des URL diff√©rentes avec la bonne authentification HTTP.\n\nQuelques fonctions utilitaires sont aussi disponibles pour r√©aliser rapidement certaines v√©rifications r√©currentes et ainsi faciliter le d√©veloppement des tests. On peut, par exemple, rechercher ais√©ment la pr√©sence d‚Äôerreurs ou warnings PHP dans une page. Il est aussi possible de faire un retry lorsqu‚Äôun test a √©chou√© afin d‚Äô√™tre certain que ce n‚Äôest pas une erreur du type ‚ÄúMySql server has gone away‚Äù qui peut se produire de temps en temps sur les serveurs de tests. Par ailleurs, quand un test √©choue, VigoJS exporte une capture d‚Äô√©cran qui s‚Äôav√®re tr√®s pratique pour comprendre ce qu‚Äôil s‚Äôest pass√© !\n\nTous les param√®tres ajout√©s √† la ligne de commande et dans la configuration sont inject√©s et accessibles dans la classe de test. On garde, de cette mani√®re, une certaine flexibilit√©. Cela peut permettre, par exemple, de d√©couper les tests avec de la pagination :\n\n\n\n\n\nAffichage dans le terminal\n\nNous avons aussi am√©lior√© l‚Äôaffichage des r√©sultats des tests. Il est ainsi possible de pr√©ciser pour chaque test : un titre et une description personnalis√©s afin de rendre les comptes-rendus plus compr√©hensible pour les utilisateurs. De m√™me des commentaires utilisateurs peuvent √™tre ajout√©s plus simplement dans le d√©roulement des tests.\n\n\n\nInt√©gration continue\n\nCasperJS g√©n√®re nativement des rapports xUnit. VigoJS int√®gre donc cette fonctionnalit√© pour √™tre utilis√© sur une plateforme d‚Äôint√©gration continue comme Jenkins. Il est aussi possible de modifier le param√®tre classPath dans le fichier xUnit pour am√©liorer la lisibilit√© des r√©sultats :\n\n\n\nLe chemin dans lequel est g√©n√©r√© le rapport est configurable par l‚Äôoption ‚ÄìbuildPath (ou dans la configuration) :\n\n\n\nIl suffit ensuite de configurer le job Jenkins pour qu‚Äôil r√©cup√®re le rapport de test dans ce dossier. Sans oublier de faire un job pour tester les Pull Requests de votre projet.\n\nVigoJS est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Introduction √† la qualit√© logicielle avec Node.js",
    "category" : "",
    "tags"     : " nodejs, javascript, qualite",
    "url"      : "/introduction-qualite-logicielle-avec-node-js",
    "date"     : "August 12, 2013",
    "excerpt"  : "\n\n(Source : https://www.flickr.com/photos/dieselbug2007/414348333/)\n\nChez M6Web, nous avons r√©cemment r√©ecrit un de nos projets Node.js.\nLe monde Node.js √©volue tr√®s rapidement et a fait d‚Äô√©norme progr√®s dans le domaine de la qualit√© logicielle.\nN...",
  "content"  : "\n\n(Source : https://www.flickr.com/photos/dieselbug2007/414348333/)\n\nChez M6Web, nous avons r√©cemment r√©ecrit un de nos projets Node.js.\nLe monde Node.js √©volue tr√®s rapidement et a fait d‚Äô√©norme progr√®s dans le domaine de la qualit√© logicielle.\nNous avons donc decid√© de monter en qualit√© sur nos projets Node.js en utilisant les derniers outils propos√©s par la communaut√©.\n\nPour cela, nous mesurons maintenant diff√©rentes m√©triques sur nos projets Node:\n\n\n  la qualit√© du code (checkstyle)\n  des tests unitaires et fonctionnels\n\n\net tout ceci est lanc√© par notre serveur d‚Äôint√©gration continue: Jenkins.\n\nTests unitaires\n\nPour tout ce qui est ‚Äútests‚Äù, nous avons choisi le tr√®s bon duo :\n\n\n  Mocha\n  Chai\n\n\n\n\nMocha c‚Äôest un ‚Äútest-runner‚Äù javascript qui fonctionne aussi bien sur Node que dans un navigateur web. Plus simplement mocha est l‚Äôoutil qui va contenir nos tests: il va ex√©cuter les tests et afficher les r√©sultats.\n\n\n\nChai est une librairie d‚Äôassertion assez compl√®te, permettant plusieurs syntaxe :\n\n\n  assert.equal(foo, ‚Äòraoul‚Äô);\n  foo.should.equal(‚Äòraoul‚Äô);\n  expect(foo).to.equal(‚Äòbar‚Äô);\n\n\nCes deux outils fonctionnent aussi bien pour tester vos javascripts Node que front.\n\nCe duo permet une √©criture de test simple et tr√®s lisible, dont voici un exemple :\n\n\n\nTests fonctionnels\n\nPour les tests fonctionnels, nous avons choisi d‚Äôutiliser Supertest, un package Node.js qui permet de simplifier l‚Äô√©criture de requ√™te HTTP (une surcouche au package http disponible dans Node.js).\n\nCi-dessous, un exemple de tests fonctionnels :\n\n\n\nCheckstyle\n\nEn javascript, on peut aussi √©crire du code propre et respecter des conventions de codage.\n Afin de v√©rifier que notre code respecte les standards en vigueur, nous utilisons JsHint.\n\nInt√©gration continue\n\nToutes ces m√©triques sont r√©colt√©es gr√¢ce √† Jenkins-CI √† l‚Äôaide du fichier Ant suivant :\n\n\n\n\n\nLe r√©sultat de l‚Äôint√©gration continue dans jenkins.\n\nConclusion\n\nNode.js propose des outils tr√®s performants pour la qualit√© logicielle, et √©crire des tests avec le duo ‚ÄúMocha + Chai‚Äù devient vite quelque chose de simple. Et m√™me les d√©veloppeurs les plus r√©fractaires aux tests devraient appr√©cier.\n\nN‚Äôh√©sitez pas √† commenter cet article et √† indiquez la solution que vous utilisez pour vos projets Node.\n\n"
} ,
  
  {
    "title"    : "Introduction √† Drupal par Claire Roubey (Clever Age)",
    "category" : "",
    "tags"     : " lft, drupal, video",
    "url"      : "/introduction-%C3%A0-drupal-par-claire-roubey-clever-age",
    "date"     : "July 19, 2013",
    "excerpt"  : "Drupal, le CMS tr√®s tr√®s connu mais que nous on connait pas ! A notre demande Clever Age, par l‚Äôinterm√©diaire de Claire Roubey, est venue nous pr√©senter cet outil lors d‚Äôun de nos fameux Last Friday Talk.\n\nMalheureusement, la vid√©o est coup√©e √† en...",
  "content"  : "Drupal, le CMS tr√®s tr√®s connu mais que nous on connait pas ! A notre demande Clever Age, par l‚Äôinterm√©diaire de Claire Roubey, est venue nous pr√©senter cet outil lors d‚Äôun de nos fameux Last Friday Talk.\n\nMalheureusement, la vid√©o est coup√©e √† environ la moiti√© de sa dur√©e (fort dommage car les questions √©taient tr√®s int√©ressantes). Les slides sont toutefois disponibles : https://fr.slideshare.net/claire_/drupal-m6-web310513.\n\nUn √©norme merci Clever Age et Claire !\n\nMalheureusement, la vid√©o n‚Äôest plus disponible‚Ä¶\n"
} ,
  
  {
    "title"    : "L√¢che moi la branch !",
    "category" : "",
    "tags"     : " qualite, jenkins, github",
    "url"      : "/lache-moi-la-branch",
    "date"     : "July 15, 2013",
    "excerpt"  : "Test continu des Pull Requests\n\nMaintenant que nous utilisons GitHub Enterprise chez M6Web, nous avons la joie de pouvoir utiliser les Pull Requests de fa√ßon abusive. Mais leur puissance n‚Äôest maximale que lorsqu‚Äôelles peuvent √™tre test√©es individ...",
  "content"  : "Test continu des Pull Requests\n\nMaintenant que nous utilisons GitHub Enterprise chez M6Web, nous avons la joie de pouvoir utiliser les Pull Requests de fa√ßon abusive. Mais leur puissance n‚Äôest maximale que lorsqu‚Äôelles peuvent √™tre test√©es individuellement avant d‚Äô√™tre merg√©es sur le master.\n\n\n\nPour ce faire, nous avons utilis√© le plugin GitHub Pull Request Builder de Jenkins, qui apr√®s une configuration assez simple, nous a permis de cr√©er un job qui lance automatiquement un build lorsqu‚Äôune Pull Request est modifi√©e. Ce build se positionne sur la branch point√©e par la Pull Request et ex√©cute les tests.\n\n\n\nIl est donc n√©cessaire de cr√©er un job d√©di√© au test des Pull Requests pour chaque projet dont nous souhaitons voir les Pull Request automatiquement test√©es. √áa peut para√Ætre √©vident, mais lorsqu‚Äôon a plus de 200 repositories, c‚Äôest tout de suite moins trivial.\n\nConfiguration du plugin\n\nLe fonctionnement par d√©faut du plugin GitHub Pull Request Builder est assez restrictif. Il n√©cessite qu‚Äôun contributeur ajoute un commentaire sur la Pull Request en demandant un test puis qu‚Äôun admin (parmi une liste √† configurer) r√©ponde avec un deuxi√®me commentaire acceptant de lancer les tests (le tout avec des phrases types configurables). C‚Äôest uniquement ensuite que Jenkins lancera un build.\n\nOr dans notre contexte d‚Äôentreprise, nous souhaitons que l‚Äôautomatisation soit totale, comme dans Travis : chaque modification d‚Äôune Pull Request lance l‚Äôensemble des tests. Pour arriver ce fonctionnement, il suffit de cocher ‚ÄúBuild every pull request automatically without asking (Dangerous!)‚Äù dans la section ‚ÄúAvanc√©e‚Äù des options de lancement de build par ‚ÄúGithub pull requests builder‚Äù.\n\nTest continu du master\n\nNous essayons tant que possible de suivre le workflow de d√©ploiement de GitHub : on d√©veloppe une fonctionnalit√© par branch, on fait une Pull Request sur le master et on ne merge que lorsque tout le monde est d‚Äôaccord et que les tests sont pass√©s. Cela nous permet de garder le master toujours d√©ployable.\n\nNous avons donc, pour chaque projet, un second job qui lance l‚Äôensemble des tests lors de chaque modification du master. Cela n‚Äôarrive normalement que lors du merge des nouvelles fonctionnalit√©s contenues dans les Pull Requests, qui ont d√©j√† √©t√© individuellement test√©es. Nous sommes donc sereins sur l‚Äôint√©gration crois√©e de toutes les nouvelles fonctionnalit√©s sur le master.\n\nD√©ploiement\n\nAvant de d√©ployer √† l‚Äôaide de Capistrano, nous v√©rifions que les tests passent (r√©sultat de l‚Äôint√©gration continue + lancement manuel des tests). Le manque d‚Äôautomatisation concernant ces mises en production fait apparaitre une faille assez large. Pour la r√©sorber, nous pourrions par exemple accepter le d√©ploiement d‚Äôun service, uniquement si ses tests sont pass√©s et si aucun autre n‚Äôest en cours ou en attente. M√™me si cela ajoute une d√©pendance aux serveurs d‚Äôint√©gration continue, cela s√©curise les d√©ploiements.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #5",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-5",
    "date"     : "July 12, 2013",
    "excerpt"  : "On est dredi, et on a encore pas mal de phrases chocs de nos d√©veloppeurs, entendues dans nos locaux partager.\n\nEn voici une nouvelle s√©lection avec les Dev Facts #5\n\nFaille de s√©cu ?\n\n\n  C‚Äôest un peu complexe, c‚Äôest une back-office front !\n\n\nLa b...",
  "content"  : "On est dredi, et on a encore pas mal de phrases chocs de nos d√©veloppeurs, entendues dans nos locaux partager.\n\nEn voici une nouvelle s√©lection avec les Dev Facts #5\n\nFaille de s√©cu ?\n\n\n  C‚Äôest un peu complexe, c‚Äôest une back-office front !\n\n\nLa boucle est boucl√©e\n\n\n  Il finit l√† o√π il s‚Äôest arr√™t√©\n\n\nEnvoi impossible\n\n\n  Mais tu n‚Äô√©tais pas en pi√®ce jointe !\n\n\nC‚Äôest l‚Äôhistoire d‚Äôune fille\n\n\n  C‚Äôest un peu comme ‚ÄúUn gars, une fille‚Äù mais sans le gars\n\n\nLa minute de 30 secondes\n\n\n  J‚Äôai pris une ann√©e sabatique de 6 mois.\n\n\nL‚Äôann√©e des deux mains\n\n\n  Ca d√©pend si c‚Äôest une ann√©e ambidextre !\n\n\nLes dents de la neige\n\n\n  \n    Vous avez entendu, une championne de snowboard est morte.\n    Elle s‚Äôest faite manger par un requin ?\n  \n\n\nLe vrai du faux\n\n\n  Ils ont trouv√© une faille infaillible\n\n\nLe roi de la combine\n\n\n  \n    En France, un truc comme ca, t‚Äôen as pour 280 euros !\n    Et tu l‚Äôas eu a combien ?\n    280 euros.\n  \n\n\nA bient√¥t pour un prochain √©pisode\n\n"
} ,
  
  {
    "title"    : "Benchmarking WebSockets avec NodeJs",
    "category" : "",
    "tags"     : " nodejs, websockets, benchmark, open-source",
    "url"      : "/benchmarking-websockets-avec-nodejs",
    "date"     : "July 5, 2013",
    "excerpt"  : "Nous avons r√©cemment eu √† repenser une application Node.js de timeline temps r√©el, bas√©e sur les WebSockets afin de tenir une charge plus √©lev√©e.\n\nL‚Äôapplication timeline\n\nFonctionnellement, l‚Äôapplication timeline est relativement simple: elle cons...",
  "content"  : "Nous avons r√©cemment eu √† repenser une application Node.js de timeline temps r√©el, bas√©e sur les WebSockets afin de tenir une charge plus √©lev√©e.\n\nL‚Äôapplication timeline\n\nFonctionnellement, l‚Äôapplication timeline est relativement simple: elle consiste √† afficher un flux de message publi√©s par des contributeurs en temps r√©el pour les internautes pr√©sent sur la page. Pour cela l‚Äôapplication se base sur socket.io pour la partie websocket, et supporte √† peu pr√®s 15 000 connexions simultan√©es.\n\nAfin d‚Äôaugmenter la capacit√© de l‚Äôapplication, nous avons d√©cid√© de la rendre scalable horizontalement. C‚Äôest dire, r√©partir la charge sur un nombre X de serveurs communiquant entre eux, par exemple, par le biais de Redis.\n\n\n\nPour cela socket.io propose un store redis qui permet aux diff√©rentes instances de communiquer entre elles. Malheureusement les performances de ce store sont plut√¥t d√©sastreuses car le store que propose socket.io est beaucoup trop verbeux et √©crit absolument tous les √©v√®nements que re√ßoit un serveur sur un seul channel redis. L‚Äôapplication devenait inutilisable autour de 8 000 connexions. Il √©tait donc inenvisageable de l‚Äôutiliser en production.\n\nNous avons donc d√©cid√© rapidement de passer une autre solution que socket.io. Apr√®s pas mal de recherche nous avons fait notre choix sur Faye, une impl√©mentation du protocole de Bayeux, bien document√© et proposant aussi d‚Äôutiliser redis comme ‚Äústore‚Äù. Apr√®s test, cette solution s‚Äôest r√©v√©l√©e bien plus performante que socket.io.\n\nTests de charge\n\nUne des probl√©matiques rapidement rencontr√©e sur ce projet a √©t√© de tester la charge de notre application: comment simuler 15 000 connexions simultan√©es ?\n\nEn faisant le tour des solutions de benchmark de websocket (thor, ‚Ä¶) ,nous n‚Äôavons pas trouv√© la solution qui nous permettait de faire les tests que nous souhaitions. Siege, ab ne le propose pas encore,Gatling, Jmeter, Tsung ont des plugins web-socket mais l‚Äôutilisation et le reporting ne sont pas des plus clair.\n\nLa solution ?\n\nWebsocket-bench\n\nNous avons donc d√©cid√© de d√©velopper notre propre outil de benchmark de websocket (Socket.io ou Faye), au nom tr√®s original : websocket-bench.\n\nCet outil se base sur les clients Node que proposent Faye et Socket.io. Il peut √™tre facilement √©tendu √† l‚Äôaide de ‚Äúgenerator‚Äù (module Node), afin de rajouter la logique de votre application. Par exemple dans le cas de notre application, en se connectant, un client doit envoyer un message au serveur pour valider la connexion.\n\nCi dessous un exemple de g√©n√©rateur qu‚Äôon a pu utiliser lors de nos tests de charge.\n\n\n\nCet outil, lanc√© sur des instances Amazon, nous a permis d‚Äôex√©cuter nos tests de charge.\n\nUn exemple : la commande ci dessous va lancer 25 000 connexions, √† raison de 1000 connexions par seconde en utilisant le generateur ‚Äúgenerator.js‚Äù :\n\n\n\n\n\nNombre de clients connect√©s sur Graphite\n\nAu del√† de 25 000 connexions, l‚Äôinstance Amazon (large) qui lan√ßait les tests ne tenait plus. Une solution pour tester un nombre plus √©lev√©s de connexions serait d‚Äôutiliser plusieurs machine de tests, peut √™tre √† l‚Äôaide de bees with machin guns et ainsi d‚Äôutiliser plusieurs instances pour lancer les tirs de charge.\n\nBonnes pratique de test de charge\n\nLors de votre test de charge (et pour la prod), n‚Äôoubliez pas d‚Äôaugmenter le nombre maximal de descripteurs de fichiers cot√© client ET cot√© injecteur (ulimit -n 256000 par exemple dans la conf de supervisor, et dans le terminal avant de lancer le benchmark).\n\nSurveillez votre conntrack (si firewall iptables), augmentez votre plage locale de port, et si vous √™tes amen√©s √† tester plus de 25K connexions, utilisez plusieurs machines et/ou plusieurs IP sources diff√©rentes.\n\nComment contribuer au projet ?\n\nN‚Äôh√©sitez pas √† remonter d‚Äô√©ventuels bug via les issues ou √† contribuer au projet l‚Äôaide de pull request github (https://github.com/BedrockStreaming/websocket-bench)\n\n"
} ,
  
  {
    "title"    : "Performances web et &quot;Disaster case&quot; sur applications mobile native",
    "category" : "",
    "tags"     : " webperf, mobile",
    "url"      : "/performances-web-disaster-case-applications-mobile-native",
    "date"     : "July 2, 2013",
    "excerpt"  : "La performance Web (ou grossi√®rement temps de chargement) est devenue aujourd‚Äôhui une probl√©matique majeure dans tout d√©veloppement Web.\n\nLes outils pour mesurer / comprendre sont plut√¥t reconnus d√©sormais et arrivent a une certaine maturit√©. Il y...",
  "content"  : "La performance Web (ou grossi√®rement temps de chargement) est devenue aujourd‚Äôhui une probl√©matique majeure dans tout d√©veloppement Web.\n\nLes outils pour mesurer / comprendre sont plut√¥t reconnus d√©sormais et arrivent a une certaine maturit√©. Il y a toutefois encore un cr√©neau plut√¥t peu document√© (√† mon go√ªt) dans le domaine, celui permettant de mesurer les temps de chargement dans des applications mobiles natives (Android / iOs ‚Ä¶)\n\nVoici un retour des m√©thodes que nous utilisons pour mesurer les performances (notamment de chargement) de nos applications natives et g√©n√©rer des Waterfall Charts, mais aussi sur la mise en place de tests ‚Äúdisaster case‚Äù en cas d‚Äôindisponibilit√© de services utilis√©s par l‚Äôapplication.\n\nPour les besoins de ce tutoriel, nous allons prendre comme configuration, un Mac, avec une application native sur un iPhone 4 (reli√© au m√™me r√©seau Wi-Fi que le Mac), ainsi que la version d‚Äôessai du logiciel CharlesProxy install√©. (Mais la configuration et proc√©dure est la m√™me sur un autre OS, ou un autre mobile, et fonctionne aussi pour tester des webapps ou sites mobiles)\n\nCharlesProxy\n\nNous allons donc utiliser le logiciel payant CharlesProxy, qui est un proxy HTTP ou Reverse Proxy permettant de capturer le traffic HTTP de son ordinateur. Il existe une version d‚Äôessai sans limite de 30 jours. Il y a peut √™tre des alternatives libres, mais Charles √©tant plut√¥t une r√©f√©rence, c‚Äôest l‚Äôoutil que nous utilisons.\n\nCommencez donc par aller sur le site et installez CharlesProxy.\n\nUne fois install√©, lancez le, il devrait automatiquement commencer √† capturer le trafic r√©seau.\n\nConnexion Wi-Fi et r√©cup√©ration IP\n\nLa deuxi√®me √©tape consiste √† connecter votre Ordinateur et votre T√©l√©phone sur le m√™me r√©seau Wi-Fi.\n\nR√©cup√©rons ensuite notre adresse Ip via les ‚ÄúPr√©f√©rences Syst√®me‚Äù, section ‚ÄúInternet et sans fil‚Äù et icone ‚ÄúR√©seau‚Äù sur votre configuration Wi-Fi :\n\n\n\nConfiguration du proxy sur son iPhone\n\nPassons ensuite sur le t√©l√©phone, dans vos pr√©f√©rences Wi-Fi.\n\nCliquez ensuite sur la fl√®che bleu √† droite du nom de la connexion sur le param√©trage Wi-Fi de notre iPhone, et descendre tout en bas du param√©trage pour configurer manuellement notre proxy HTTP :\n\nConfigurez le proxy de cette connexion pour passer par le Proxy Charles, avec l‚Äôadresse IP r√©cup√©r√©e plus haut, et le port par d√©faut de Charles 8888.\n\nUne fois la connexion lanc√©e avec le Proxy activ√© et Charles bien lanc√© sur votre Mac, une popup d‚Äôactivation devrait apparaitre :\n\n\n\nAller ensuite sur un site mobile via Safari pour v√©rifier que le trafic est bien captur√© par votre Proxy.\n\nA ce stade, tout est pr√™t pour commencer les mesures.\n\nPrise de mesure avec Charles\n\nPour prendre une mesure avec Charles, allez dans le menu ‚ÄúProxy‚Äù, d√©cochez le ‚ÄúMAC OS X Proxy‚Äù afin de ne pas parasiter vos mesures, et nettoyez l‚Äô√©cran de Charles pour commencer une ‚Äúsession‚Äù propre.\n\n\n\nVous n‚Äôavez ensuite plus qu‚Äôa lancer une application pour mesurer la liste des requ√™tes HTTP n√©cessaire √† son d√©marrage.\n\nDans la partie Structure, un clic sur un domaine vous donnera plus d‚Äôinfos (nombre de requ√™te, et d√©tails de chacune) ‚Ä¶\n\nS√©lectionnez toutes les requ√™tes, puis cliquez sur ‚ÄúChart‚Äù sur la droite, pour obtenir un premier Waterfall (made in Charles)\n\n\n\nG√©n√©ration de Waterfall (plus complet)\n\nToujours sous Charles, avec toutes les structures s√©lectionn√©es, Fichier / Export puis selectionner le format Http Archive (.har)\n\nNous allons ensuite utiliser l‚Äôoutil harviewer, pour visualiser le waterfall sous une forme plus compl√®te que dans Charles.\n\nRendez vous ici (avec Firefox, plut√¥t que Chrome dont le rendu est bugg√© sur cet outil) : https://www.softwareishard.com/har/viewer/\n\nD√©cochez la case ‚ÄúValidate data before processing?‚Äù pour √™tre moins emb√™t√© par des probl√®mes de compatibilit√© surement li√©s √† l‚Äôexport de Charles.\n\n\n\nEnsuite, faites un Drag &amp;amp; Drop de votre fichier .har dans le textarea de HarViewer pour obtenir votre waterfall, tr√®s proche de l‚Äôonglet R√©seau de Firebug ou Network de la console de Chrome.\n\nVous retrouvez donc pour chaque requ√™te tous les √©lements classique, avec d√©tail des r√©ponses, code HTTP de retour, taille etc, et le tout sur une timeline tr√®s pr√©cise.\n\n\n\nThrottling\n\nPour le moment, nous avons donc test√© notre application sur notre connexion Wi-Fi, cas plut√¥t id√©al. Mais comment simuler une connexion 3g par exemple, peut √™tre plus proche de la r√©alit√© des utilisateurs de l‚Äôapplications ?\n\nPour cela, il vous suffit d‚Äôaller dans Charles, puis le menu ‚ÄúProxy‚Äù et ‚ÄúThrottle Settings‚Äù.\n\nLa latence par d√©faut configur√©e est un peu √©lev√©e (600ms), mais vous pouvez la modifier et affiner vos tests pour se rapprocher de conditions plus r√©elles.\n\nEnsuite, toujours dans le menu ‚ÄúProxy‚Äù, activ√© l‚Äôoption ‚ÄúThrottle‚Äù et vous pourrez tester sur une connexion diff√©rente.\n\n\n\nDisaster Case ?\n\nComment savoir comment se comporte votre application si vos Webservices sont injoignables ? ou si l‚Äôun des services tiers que vous utilisez est down ? Comment trouver les SPOF (Single Point Of Failure) de vos apps ?\n\nToujours dans Charles, Allez dans ‚ÄúTools‚Äù, puis ‚ÄúMap Remote‚Äù.\n\nIci, vous allez pouvoir rediriger les domaines de vos choix, vers un domaine de type Blackhole.\n\nC‚Äôest √† dire que le domaine choisi r√©agira comme si votre serveur web √©tait dans un √©tat de mort c√©r√©brale ! Pas celui o√π il rejette la connexion imm√©diatement (trop facile), celui o√π il v√©g√®te sans arriver √† acquitter la r√©ponse (le fameux ‚Äúen attente de https:// ‚Ä¶.‚Äù)\n\nPour ce besoin, nous allons utiliser le Blackhole fourni par Patrick Meenan pour l‚Äôoutil de mesure de performance web : WebPageTest : https://blackhole.webpagetest.org\n\n\n\nVous pouvez ensuite jouer avec les domaines, et regarder comment se comporte votre application dans le cas o√π l‚Äôun d‚Äôentre eux est inaccessible.\n\nSur notre iPhone 4 de test, on remarque d‚Äôailleurs un timeout sur les requ√™tes de 75 secondes ! Imaginez le cas, o√π le d√©veloppement et l‚Äôappel √† ce service est synchrone ? 75 secondes de loading dans votre application avant de passer aux requ√™tes suivantes ‚Ä¶\n\n\n\nVoil√†, vous avez d√©sormais une solution vous permettant de g√©n√©rer des Waterfall Charts pour vos apps natives, et de tester des conditions de mauvaises connexions, ou d‚Äôindisponibilit√© de service.\n\nSi vous avez d‚Äôautres m√©thodes, plus simples ou plus compl√®tes, ou tout autre remarque sur cette article, n‚Äôh√©sitez pas √† le faire dans les commentaires ci-dessous.\n\nMerci.\n\nP.s: pour compl√©ment, n‚Äôh√©sitez pas √† creuser le blogpost de Steve Souders sur les waterfall mobile, qui utilise une m√©thode tr√®s diff√©rente avec tcpdump et pcapperf https://www.stevesouders.com/blog/2013/03/26/mobile-waterfalls/\n\n"
} ,
  
  {
    "title"    : "Coke, pour bien sniffer son code",
    "category" : "",
    "tags"     : " outil, qualite, php, open-source",
    "url"      : "/coke-pour-bien-sniffer-son-code",
    "date"     : "June 27, 2013",
    "excerpt"  : "Afin d‚Äôuniformiser nos d√©veloppements, nous avons d√©cid√© de suivre des conventions de code. Les projets deviennent ainsi plus homog√®nes et la revue de code, comme la maintenance, s‚Äôen trouvent simplifi√©es. Comme la majorit√© de nos services sont en...",
  "content"  : "Afin d‚Äôuniformiser nos d√©veloppements, nous avons d√©cid√© de suivre des conventions de code. Les projets deviennent ainsi plus homog√®nes et la revue de code, comme la maintenance, s‚Äôen trouvent simplifi√©es. Comme la majorit√© de nos services sont en PHP, nous utilisons PHP CodeSniffer.\n\nLe manque\n\nCependant, l‚Äô√©ventail des frameworks utilis√©s en interne (Symfony, ZF, homemade) ne nous permet pas d‚Äôemployer une seule et m√™me convention. De plus, l‚Äôorganisation des projets est assez h√©t√©rog√®ne (ex: les r√©pertoires de test ne se nomment pas tous de la m√™me mani√®re). Nous avions donc besoin de pouvoir configurer sp√©cifiquement PHP CodeSniffer pour chacun de nos projets.\n\nLe deal\n\nA la mani√®re de Travis, nous avons opt√© pour la m√©thode dite ‚Äúdu fichier .truc pos√© √† la racine de chaque projet‚Äù (tm). Nous avons donc d√©velopp√© Coke, un script de sniff, qui lance PHP CodeSniffer avec la configuration contenu dans le fichier ‚Äú.coke‚Äù la racine du projet :\n\n\n\nAinsi, lorsque le fichier est param√©tr√© et que le script coke est correctement install√© sur le syst√®me, il suffit d‚Äôex√©cuter la commande ‚Äúcoke‚Äù depuis la racine du projet sniffer.\n\nLe fix\n\nDans l‚Äôoptique d‚Äôautomatiser le plus possible nos processus, nous avons ins√©r√© la v√©rification des coding styles √† l‚Äôaide de Coke, dans un hook git de pre-commit.\n\nCoke est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Encodage - packaging - DRM - tout sur la vid√©o",
    "category" : "",
    "tags"     : " video, codec, drm, lft",
    "url"      : "/encodage-packaging-drm-tout-sur-la-vid%C3%A9o",
    "date"     : "June 26, 2013",
    "excerpt"  : "\n\nUne nouvelle vid√©o de l‚Äôann√©e derni√®re provenant d‚Äôun Last Friday Talk.\n\nSouvent le monde de vid√©o est source d‚Äôimpr√©cision, cette vid√©o met √† plat l‚Äôensemble des termes qui sont utilis√©s dans le domaine :\n\n\n  encodage, transcodage\n  packaging (...",
  "content"  : "\n\nUne nouvelle vid√©o de l‚Äôann√©e derni√®re provenant d‚Äôun Last Friday Talk.\n\nSouvent le monde de vid√©o est source d‚Äôimpr√©cision, cette vid√©o met √† plat l‚Äôensemble des termes qui sont utilis√©s dans le domaine :\n\n\n  encodage, transcodage\n  packaging (transformation du conteneur vid√©o)\n  DRM\n\n\nMalheureusement, la vid√©o n‚Äôest plus disponible‚Ä¶\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #4",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-4",
    "date"     : "June 14, 2013",
    "excerpt"  : "Parceque c‚Äôest dredi et que ca nous fait toujours rire de partager les phrases chocs entendues dans nos bureaux, voici les Dev Facts #4.\n\nCaptain Obvious\n\n\n  \n    ‚ÄúVous avez oubliez quelque chose ?‚Äù\n    ‚ÄúLe probl√®me, c‚Äôest que quand t‚Äôoublie, t‚Äôy ...",
  "content"  : "Parceque c‚Äôest dredi et que ca nous fait toujours rire de partager les phrases chocs entendues dans nos bureaux, voici les Dev Facts #4.\n\nCaptain Obvious\n\n\n  \n    ‚ÄúVous avez oubliez quelque chose ?‚Äù\n    ‚ÄúLe probl√®me, c‚Äôest que quand t‚Äôoublie, t‚Äôy penses pas‚Äù\n  \n\n\nLa minute de 30 secondes\n\n\n  Si je gagne cette somme, je me prends 6 mois d‚Äôann√©e sabatique\n\n\n$i++\n\n\n  Avec un ticket resto, tu peux manger 2, avec toute ta famille\n\n\n???\n\n\n  On t‚Äôa pas sonn√© les oreillettes\n\n\nla m√©moire\n\n\n  La musique est m√©morable, mais je m‚Äôen souviens plus\n\n\nAnonymous Proxy Land\n\n\n  Pour poster en anonyme, il faut √™tre loggu√©.\n\n\nJ√©sus multipliait les pains\n\n\n  On va d√©dupliquer les clics par quatre\n\n\nAbsent coupable!\n\n\n  ‚ÄúD√®s que je ne suis pas l√†, j‚Äôai toujours tord ‚Ä¶ :(‚Äú\n ‚ÄúBein oui! Les innocents ont toujours tord‚Äù\n\n\nTransgiving\n\n\n  1 mec sur 3 qui regardent des pornos sur Internet sont des femmes\n\n\nFatal error never die\n\n\n  PHP : Fatal error: date() [function.date]: Timezone database is corrupt - this should never happen! in ///**/error.php on line 105\n\n\n"
} ,
  
  {
    "title"    : "Firewall applicatif PHP et bundle Symfony",
    "category" : "",
    "tags"     : " outil, php, symfony, open-source",
    "url"      : "/firewall-applicatif-php-et-bundle-symfony",
    "date"     : "May 30, 2013",
    "excerpt"  : "Nous publions aujourd‚Äôhui notre firewall applicatif sur notre compte GitHub. Il se compose :\n\n\n  d‚Äôun composant PHP (5.4+) g√©rant les IPs (V4 et V6), plages, wildcards, white/black lists, etc.\n  d‚Äôun bundle Symfony permettant d‚Äôutiliser le composa...",
  "content"  : "Nous publions aujourd‚Äôhui notre firewall applicatif sur notre compte GitHub. Il se compose :\n\n\n  d‚Äôun composant PHP (5.4+) g√©rant les IPs (V4 et V6), plages, wildcards, white/black lists, etc.\n  d‚Äôun bundle Symfony permettant d‚Äôutiliser le composant Firewall dans les controllers √† l‚Äôaide des annotations et de retourner une r√©ponse HTTP personnalis√©e.\n\n\nIls utilisent tous les deux Composer et sont disponibles sur Packagist.\n\nQu‚Äôest ce qu‚Äôun Firewall applicatif ?\n\nUn Firewall applicatif permet de restreindre l‚Äôacc√®s de certaines IPs √† certaines parties d‚Äôune application. Vous pouvez par exemple d√©finir la liste des IPs autoris√©es dans la section d‚Äôadministration ou au contraire celles que vous souhaitez bloquer dans un forum.\n\nPourquoi cette impl√©mentation ?\n\nNous souhaitions √©viter de red√©finir l‚Äôensemble des IPs chaque point de restriction. Nous avons donc cherch√© centraliser la configuration. Le FirewallBundle permet de mettre en place des listes hi√©rarchis√©es ainsi que des configurations pr√©d√©finies que nous pouvons r√©utiliser et adapter chaque besoin.\n\nComment contribuer ?\n\nSi notre firewall applicatif r√©pond certaines de vos probl√©matiques, mais que vous souhaitez le voir √©voluer, n‚Äôh√©sitez pas participer son d√©veloppement :\n\n\n  forkez les projets sur GitHub,\n  faites une branche par fonctionnalit√©,\n  proposez-nous vos √©volutions et optimisations via les Pull Requests.\n\n\nVous pouvez √©galement nous remonter les probl√®mes rencontr√©s lors de son utilisation dans les issues du composant ou les issues du bundle.\n\nEnfin, n‚Äôh√©sitez pas utiliser les commentaires de cet article pour nous faire part de vos r√©actions.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #3",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-3",
    "date"     : "May 24, 2013",
    "excerpt"  : "Episode 3 des devfacts ! Parce qu‚Äôon ne s‚Äôen lasse pas.\n\nFort Boyaux\n\n\n  Cassandra t√™te de tigre !\n\n\nla dev m√™me en prod\n\n\n  C‚Äôest un environnement de dev m√™me en prod !\n\n\nPCF\n\n\n  \n    J‚Äôadore le pr√©nom de Staline !\n    Sylvester ?\n  \n\n\nglory and ...",
  "content"  : "Episode 3 des devfacts ! Parce qu‚Äôon ne s‚Äôen lasse pas.\n\nFort Boyaux\n\n\n  Cassandra t√™te de tigre !\n\n\nla dev m√™me en prod\n\n\n  C‚Äôest un environnement de dev m√™me en prod !\n\n\nPCF\n\n\n  \n    J‚Äôadore le pr√©nom de Staline !\n    Sylvester ?\n  \n\n\nglory and consequences\n\n\n  OH PUTAIN j‚Äôai √©t√© RT par un participant de la belle &amp;amp; ses princes ! : jour de gloire :\n\n\nl‚Äôoptimiste\n\n\n  C‚Äôest moins pire que rien\n\n\nle blagueur\n\n\n  T‚Äôas trois poussins sur une table ; comment tu fais pour en avoir plus que deux ? ‚Ä¶ T‚Äôen pousses un.\n\n\nI can haz root access !\n\n\n  \n    Tu peux me donner les acc√®s MySQL ?\n    Ils sont en root.\n    Et ils arrivent quand ?\n  \n\n\nle jimmy cliff\n\n\n  I can see clearly now, the regex‚Äôs gone.\n\n\nla honte\n\n\n  \n    j‚Äôai un peu honte de ce que je fais l√† ‚Ä¶\n    quoi tu fais du javascript ?\n  \n\n\nl‚Äôaveuglement\n\n\n  Tu peux √™tre valide w3c, l‚Äôaveugle, il verra toujours rien !\n\n\nle choix dans la date\n\n\n  Date de sortie (jj/dd/yyyy)\n\n"
} ,
  
  {
    "title"    : "Redis on fire !",
    "category" : "",
    "tags"     : " redis, nosql, lft, video",
    "url"      : "/redis-on-fire",
    "date"     : "May 22, 2013",
    "excerpt"  : "On continue la diffusion de quelques LFT tri√©s sur le volet.\n\nCette fois ci c‚Äôest Kenny Dits qui s‚Äôy colle avec une pr√©sentation de Redis et des cas d‚Äôutilisation de cette technologie.\n\nMalheureusement, la vid√©o n‚Äôest plus disponible‚Ä¶\n",
  "content"  : "On continue la diffusion de quelques LFT tri√©s sur le volet.\n\nCette fois ci c‚Äôest Kenny Dits qui s‚Äôy colle avec une pr√©sentation de Redis et des cas d‚Äôutilisation de cette technologie.\n\nMalheureusement, la vid√©o n‚Äôest plus disponible‚Ä¶\n"
} ,
  
  {
    "title"    : "CR Conf√©rence Agora Cms du 15 mai 2013",
    "category" : "",
    "tags"     : " conference, cms",
    "url"      : "/cr-conference-agora-cms-du-15-mai-2013",
    "date"     : "May 20, 2013",
    "excerpt"  : "\n\nLe 15 mai 2013 avait lieu √† Paris, la premi√®re √©dition de l‚ÄôAgoraCMS, Conf√©rence ax√©e sur les CMS et la gestion de contenu Web.\n\nCette conf√©rence est organis√©e par des acteurs importants du milieu, de chez Microsoft, Epitech, Oxalide, Cap Gemini...",
  "content"  : "\n\nLe 15 mai 2013 avait lieu √† Paris, la premi√®re √©dition de l‚ÄôAgoraCMS, Conf√©rence ax√©e sur les CMS et la gestion de contenu Web.\n\nCette conf√©rence est organis√©e par des acteurs importants du milieu, de chez Microsoft, Epitech, Oxalide, Cap Gemini ‚Ä¶\n\nAu rendez-vous, des sujets sur Drupal, Wordpress, le Responsive, les R√©seaux Sociaux d‚Äôentreprise, et des retours d‚Äôexp√©rience de diff√©rents acteurs fran√ßais sur leurs utilisations de CMS public, ‚Äúhome made‚Äù ou propri√©taire.\n\nLes CMS - √©cosyst√®me - √©tat des lieux et tendance, par Marine Soroko (CoreTechs) et Fr√©d√©ric Bon (Clever Age)\n\nPremi√®re conf√©rence, et bonne introduction en la mati√®re avec une pr√©sentation de la typologie des CMS.\n\nOn nous met en garde sur les ‚Äú√©diteurs de contenu‚Äù notamment, ou un CMS ne doit pas permettre de g√©n√©rer du contenu dans les pages. Un CMS de nos jours, doit permettre de g√©rer un r√©f√©rentiel de contenu, que nos pages doivent pouvoir requ√™ter, sinon nous devenons vite confront√©s a des probl√®mes de r√©-usabilit√©.\n\nLes diff√©rents acteurs du march√© sont pr√©sent√©s via les fameux ‚ÄúQuadrant magic‚Äù qu‚Äôon nous annonce finalement tr√®s loin de la r√©alit√©.\n\nOn finit sur une projection du futur des CMS qui devront r√©pondre aux probl√©matiques suivantes :\n\n\n  multi-canal (support tablette / site tiers etc)\n  multi-source\n  personnalisation\n  int√©gration e-commerce\n  Analyses et statistique\n  interactions et e-services\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nVotre CMS intelligent gr√¢ce l‚Äôanalyse des logs, par J√©rome Renard (Belogik)\n\nJ√©rome Renard, ancien d√©veloppeur EzPublish, a mont√© ces derniers mois une start-up proposant du Log As a Service, avec une solution du nom de Belogik, un peu comparable un Loggly.\n\nLa conf√©rence pr√©sente l‚Äôint√©r√™t d‚Äôanalyser les logs (ici sortant d‚Äôun CMS, mais transposable tout site/service web) :\n\n\n  Incident de production\n  Service de la preuve\n  SEO\n  Performances\n  Gestion applicative\n  S√©curit√©\n  D√©veloppement\n\n\nMais aussi la difficult√© √† traiter des logs de formats diff√©rents, pas forc√©ment disposition, quand vous ne maitrisez pas ou peu l‚Äôh√©bergement.\n\nPour la recherche dans ses logs, on a des solutions comme SolR ou les diff√©rents produits bas√©s sur Lucene comme ElasticSearch (je rajouterais aussi le couple LogStash + Kibana utilisant aussi ElasticSearch)\n\nBref, une excellente pr√©sentation, dans la veine de celles que nous avions pu pr√©senter chez m6web au niveau du Monitoring, un sujet tr√®s compl√©mentaire avec le Logging.\n\nPour plus d‚Äôinformations, belogik.com, ou sur leur compte twitter : @belogikCom.\n\nSinon vous pouvez consulter les slides sur le lien ci-dessous.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nVotre CMS intelligent gr√¢ce l‚Äôanalyse des logs\n\nhttps://jrenard.info/talks/agoracms2013/\n\nTendances du design et nouveaux usages, par Patrick Maruejouls (Think Think)\n\nTr√®s bonne pr√©sentation sur l‚Äôimportance du design dans le premier sens du terme : un outil pouvant et devant permettre de servir les int√©r√™ts strat√©giques d‚Äôune entreprise. Il en d√©coule qu‚Äôil serait bon de placer le design en amont des d√©cisions et non en aval comme c‚Äôest souvent le cas.\n\nDe ses nombreuses exp√©riences, nous pouvons retenir qu‚Äôune des plus importantes tendances pour les ann√©es venir est l‚Äôadaptation du design l‚Äôutilisateur. Ainsi, une grande enseigne de pr√™t-√†-porter masculin a ins√©r√© une puce RFID dans l‚Äô√©tiquette de ses v√™tements pour que l‚Äôambiance des cabines d‚Äôessayage s‚Äôadapte aux v√™tements essay√©s (ex: une petite musique des √Æles se d√©clenche lorsque le client essaye une chemise hawa√Øenne).\n\nDe m√™me, cette tendance d‚Äôadaptation du design l‚Äôutilisateur pourra se voir magnifi√©e par la TV connect√©e et le second √©cran.\n\n\n\n\n\nLe second √©cran chez M6Web\n\nLes meilleurs th√®mes et modules Drupal, par Dorian Marchan (Kernel 42) et Romain Jarraud (Trained People)\n\nPetite introduction Drupal, pr√©sent√© comme un CMS, mais surtout un CMF (Content Management Framework), avec la pr√©sentation de quelques modules tr√®s int√©ressants pour des d√©veloppeurs ou pour r√©aliser son ‚ÄúUsine √† Site‚Äù.\n\nOn retiendra GCC dont une d√©mo tr√®s int√©ressante sera faite, Drupal Commerce, √©tant une distribution de Drupal avec un assemblage de modules et personnalisation pour orienter son drupal vers le e-commerce, mais aussi la pr√©sentation d‚Äôun des th√®mes les plus √©volu√©s de Drupal : Omega, th√®me ultra complet, Responsive avec un back-office assez puissant.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nLes R√©seaux sociaux d‚Äôentreprise, par Edouard Ly (Oxalide) et Marine Soroko (Core Techs)\n\nTerme la mode depuis ces derni√®res ann√©es, les RSE commencent envahir les entreprises.\n Pr√©sentation (sans d√©mo ou screenshot malheureusement), des diff√©rents acteurs du march√© (tr√®s peu d‚Äôacteurs open source d‚Äôailleurs :( ) :\n\n\n  Jive (leader du march√©)\n  BlueKiwi\n  Telligent\n  BuddyPress : extension Wordpress\n  Yammer\n  Elgg\n  Chatter\n  Sharepoint\n  Liferay\n  Drupal commons\n  ‚Ä¶\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nResponsive design : un site mobile en moins d‚Äôune heure, par Raphael Goetter (AlsaCreations)\n\nRaphael Gotter ( @goetter), le cr√©ateur d‚ÄôAlsaCr√©ations, est compl√®tement incontournable pour tout ce qui touche l‚Äôint√©gration HTML, ou le RWD (Responsive Web Design) dans la communaut√© francophone, nous explique les approches √† avoir pour r√©aliser un site RWD.\n Notamment que le Mobile First, para√Æt toujours √™tre la bonne approche, ainsi qu‚Äôun rappel sur le fait que le RWD doit √™tre pens√© et pr√©vu en amont.\n\nRaphael nous d√©montre aussi que son titre, et la r√©alisation d‚Äôun site RWD en moins d‚Äôune heure est infaisable. En prenant l‚Äôexemple du site d‚ÄôAgora CMS, et en nous pr√©sentant l‚Äôapproche pour transformer sa HP en Responsive.\n Plus de 15 jours de boulot au final, et des slides tr√®s int√©ressantes d√©couvrir ci-dessous, remplies d‚Äôastuces, de checklists, et autres notions √† bien comprendre avant de s‚Äôint√©resser et de se lancer dans le RWD :\n\n\n  Comprendre les surfaces d‚Äôaffichage\n  Conna√Ætre les Media Queries css 3\n  le Box-sizing\n  Halte aux d√©bordements\n\n\nJe vous invite √† tester sur un mobile Mobitest.me pour bien comprendre la diff√©rence entre viewport, largeur en pixel r√©elle, ‚Ä¶\n\nPr√©sentation de la propri√©t√© hyphens aussi pour g√©rer les d√©bordements de texte, coupler avec la propri√©t√© word-wrap, le framework / css de base Knacss.com, les tailles de typo en ‚Äúrem‚Äù.\n\nBref, conf√©rence tr√®s riche, dr√¥le et a creuser imp√©rativement pour tout ceux qui travaillent de pr√®s ou de loin sur ces probl√©matiques.\n\nCompte rendu par Raphael lui m√™me : https://blog.goetter.fr/post/50567713227/conference-un-site-responsive-en-une-heure avec le r√©sultat voir ici : https://kiwi.gg/rg/agora/\n\nEt les slides ci-dessous :\n\n\n\n20 minutes: g√©rer le multi-canal, apps, web, devices, par Arnaud Limbourg (20 minutes)\n\nArnaud ( @arnaudlimbourg) a rapidement expliqu√© la strat√©gie mise en place par 20minutes pour permettre le multi-support : centralisation des donn√©es dans un r√©f√©rentiel accessible via une API permettant aux diff√©rents devices de se fournir en donn√©es, chacun leur mani√®re.\n\nIl a ensuite donn√© quelques conseils afin de fournir une bonne API, en pr√©cisant qu‚Äôil √©tait tr√®s difficile d‚Äôen r√©aliser une bonne :\n\n\n  architecture REST en HTTP,\n  faibles temps de r√©ponse,\n  bon monitoring,\n  facilit√© d‚Äôapprentissage et d‚Äôutilisation,\n  concepts simples,\n  documentation.\n\n\nEnfin, Arnaud explique qu‚Äôil n‚Äôy a pas de solution miracle entre l‚Äôinternalisation ou l‚Äôexternalisation des d√©veloppements (l‚Äôapp Android a √©t√© d√©velopp√©e en interne alors que le d√©veloppement de l‚Äôapp iOS a √©t√© externalis√©e).\n De la m√™me mani√®re, le choix entre le HTML5 et le code natif d√©pends des besoins et des ressources.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nRetour d‚Äôexp√©rience : France T√©l√©visions, par L√©o Poiroux (France T√©l√©visions)\n\nNos confr√®res de chez France T√©l√©visions nous pr√©sentent leur retour d‚Äôexp√©rience sur leurs usines site, avec au micro, L√©o ( @Leo_Px).\n\nAu d√©part, sur Spip, il nous explique pourquoi ils ont migr√© (douloureusement au d√©but) sur Drupal, et qu‚Äôest ce que cela leur a apport√©.\n\nUne conf√©rence tr√®s transparente, dr√¥le, agr√©able et int√©ressante.\n\nL‚Äôavenir de leur c√¥t√© tend vers une transformation de l‚Äôusine √† site, vers une ‚Äúusine √† interface‚Äù notamment pour g√©rer les multi-√©crans, et une ouverture de leurs api/services vers de l‚ÄôopenApi, OpenData.\n\nFT c‚Äôest une tr√®s grosse DT (une centaine de personnes), une √©quipe d‚Äôexpert transverse surnomm√©e SWAT (compos√©e d‚ÄôExpert frontend JS/WebPerf, Expert Archi/Varnish, Expert Drupal, et de deux Coach Agile).\n\nD‚Äôautres excellentes id√©es comme leurs Dojo et Safari.\n Les Dojo sont des sessions d‚Äôune heure o√π les d√©veloppeurs se relaient toutes les 5 minutes sur un m√™me code, pour avancer un d√©veloppement..\n Les safaris sont une sorte de ‚ÄúVis ma vie‚Äù avec une journ√©e en immersion dans une autre √©quipe de d√©veloppement par exemple ou dans une √©quipe de journaliste utilisant l‚Äôun des sites qu‚Äôils ont d√©velopp√©s.\n\n\n  Pourquoi Drupal ? \n ‚ÄúLa maison blanche utilise Drupal‚Äù\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nTable Ronde. Quel mod√®le choisir : cr√©er mon propre CMS ? utiliser un CMS existant ? OpenSource / propri√©taire ?\n\nLa journ√©e se termine sur une table ronde compos√©e de diff√©rents responsables techniques :\n\n\n  Olivier Grange Labat ‚Äì @ogrange DT @ Le Monde interactif : Olivier est en charge de la technique de Lemonde.fr. Il a fait le choix de d√©velopper son propre outil de gestion de contenu\n  Damien Cirotteau ‚Äì @cirotix : Damien est CTO chez Rue89 qui utilise et est sp√©cialis√© en Drupal.\n  Olivier Fouqueau ‚Äì DSI de la mairie d‚ÄôAulnay-sous-bois : Olivier est en charge des Syst√®mes d‚Äôinformations et de l‚Äôinnovation la Mairie d‚ÄôAulnay-sous-bois. Il fait le choix du CMS Ametys.\n  Galdric Pons ‚Äì @hebiflux Chef de projet digital @ BNP Paribas : Galdric est Chef de projet digital au p√¥le innovation de BNP Paribas ou il a mis en place une usine site avec WordPress.\n\n\nChacun des participants au cours d‚Äôune interview anim√©e par Cyril Pierre de Geyer ( @cyrilpdg) , va expliquer son choix de CMS, les avantages et les inconv√©nients et donner de pr√©cieux conseils aux personnes dans la m√™me situation.\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nConclusion\n\nUne journ√©e haute en couleur, avec de tr√®s bonnes conf√©rences et des retours d‚ÄôXP toujours aussi int√©ressants.\n On salue une organisation impeccable et le tout pour un prix tr√®s accessible (20‚Ç¨).\n\nCe CR ne rend-compte finalement que d‚Äôune mince partie des conf√©rences (4 conf√©rences en parall√®le pour cause), mais vous pouvez retrouver d‚Äôautres slides sur le site officiel de l‚Äô√©venement.\n\nN‚Äôh√©sitez pas commenter ce CR si vous avez des remarques ;-)\n\nP.s: Merci @nsilberman pour les photos que vous pouvez retrouver en int√©gralit√© ici : https://photos.silberman.fr/Other/AgoraCMS/\n"
} ,
  
  {
    "title"    : "La POO Canada Dry",
    "category" : "",
    "tags"     : " php, poo, lft, video",
    "url"      : "/la-poo-canada-dry",
    "date"     : "May 6, 2013",
    "excerpt"  : "Nous vous avions parl√©, il y‚Äôa quelques mois, de nos conf√©rences interne, les Last Friday Talk.\n\nVoici une premi√®re vid√©o de l‚Äôune de ces sessions sur ‚Äúla POO Canada Dry‚Äù.\n\nLa POO (Programmation Orient√© Objet) ne consiste pas √† mettre du code dans...",
  "content"  : "Nous vous avions parl√©, il y‚Äôa quelques mois, de nos conf√©rences interne, les Last Friday Talk.\n\nVoici une premi√®re vid√©o de l‚Äôune de ces sessions sur ‚Äúla POO Canada Dry‚Äù.\n\nLa POO (Programmation Orient√© Objet) ne consiste pas √† mettre du code dans des classes, elle fait appel √† des concepts vitaux pour le d√©veloppeur moderne. Olivier nous pr√©sente quelques mauvais exemples tir√©s de code legacy et quelques bonnes pratiques pour faire de la POO (mais en fait c‚Äôest surtout du troll).\n\nMalheureusement, la vid√©o n‚Äôest plus disponible‚Ä¶\n"
} ,
  
  {
    "title"    : "CR Real Time Conf√©rence Europe 2013 - Day 2",
    "category" : "",
    "tags"     : " conference, nodejs, realtime",
    "url"      : "/cr-real-time-conference-europe-2013-day-2",
    "date"     : "April 26, 2013",
    "excerpt"  : "\n\nCr√©dit : https://www.flickr.com/photos/andyet-photos/8679275805/in/set-72157633306379029\n\nApr√®s la premi√®re journ√©e, on continue avec la deuxi√®me journ√©e de conf√©rence. Toujours sur le format de 20 minutes pour pr√©senter le sujet.\n\nWOOT, Arial B...",
  "content"  : "\n\nCr√©dit : https://www.flickr.com/photos/andyet-photos/8679275805/in/set-72157633306379029\n\nApr√®s la premi√®re journ√©e, on continue avec la deuxi√®me journ√©e de conf√©rence. Toujours sur le format de 20 minutes pour pr√©senter le sujet.\n\nWOOT, Arial Balkan\n\n\n\nCr√©dit : https://twitter.com/OriPekelman/status/326600103475425281/photo/1\n\n@aral a travaill√© sur une solution d‚Äô√©dition partag√©e de contenu, une solution sans transformation op√©rationnelle (OT sur Wikipedia) : WOOT qui signifie Without Operational Transformation.\n\nLe concept expos√© est de ne pas faire de suppression des caract√®res d‚Äôune cha√Æne, mais plut√¥t de travailler sur la notion de visible / invisible et de position du caract√®re au sein de la cha√Æne. L‚Äôobjectif √©tant de garder la convergence et de pr√©server les intentions des utilisateurs.\n\nIl nous indique diff√©rentes ressources pour approfondir le sujet :\n\n\n  une vid√©o de Google Tech Talk :Issues and Experiences in Designing Real-time Collaborative Editing Systems\n  une librairie JS : ShareJS\n  pour aller plus loin dans la gestion du travail collaboratif, l‚ÄôInria rend disponible diff√©rents travaux de recherches sur le sujet\n\n\nConvention-Driven JSON, Steve Klabnik\n\n\n\nCr√©dit : https://twitter.com/OriPekelman/status/326607298556489728/photo/1\n\n@steveklabnik avait d√©j√† parl√© la veille sur un autre sujet. Aujourd‚Äôhui, il nous expose la probl√©matique de passer des objets (quelque soit le langage) JSON. Bien souvent, on utilise JSON pour la communication entre diff√©rents services (l‚Äôun en PHP et l‚Äôautre en Python par ex, ou deux services en PHP).\n\nPour un site web, on arrive souvent au r√©sultat suivant :\n\nObjet -&amp;gt; Template -&amp;gt; HTML\n\nPour √©viter certains probl√®mes lors des √©changes de donn√©es, il propose par exemple en ruby d‚Äôutiliser active_model_serializers ce qui permet d‚Äôobtenir le r√©sultat suivant :\n\nObjects -&amp;gt; Serializer -&amp;gt; HTML\n\nEn r√©sum√©, il recommande de passer par un outil de serialisation des donn√©es afin de ne pas perdre la structure de l‚Äôobjet et donc d‚Äôavoir une plus grande r√©activit√© entre le client et le serveur.\n\nRealtime vs Real world, Tyler Mac Mullen\n\n@tbmcmullen travaille pour Fastly. Sa soci√©t√© propose des solutions d‚Äôoptimisation au sein des infrastructures de type CDN.\n\n\n\nCr√©dit : https://twitter.com/OriPekelman/status/326613272969228288/photo/1\n\nIl commence sa pr√©sentation en d√©finissant les deux termes :\n\n\n  Realtime = r√©duire la latence\n  Realworld = notion d‚Äôinfrastructure\n\n\nIl indique √©galement l‚Äôimpossibilit√© de construire des infrastructures en temps-r√©el. Seul les CDNs ont la possibilit√© de s‚Äôapprocher du temps-r√©el. La notion de purge est √©galement essentielle.\n\nTyler pr√©sente ensuite 3 possibilit√©s de purge :\n\n\n  Utilisation de Rsyslog : soit via TCP (probl√®me : lenteur), soit via UDP (probl√®me : pas de retour d‚Äôerreur). Un noeud notifie tous les autres.\n  Love triangle : pas de serveur central mais notion de peer-to-peer. Chaque noeud interagit avec 2/3 autres noeuds. Probl√®me : il n‚Äôy a pas d‚Äô√©tat global ni de possibilit√© de scalabilit√© avec ce type d‚Äôinfrastructure\n  Hybride : on met en place des switchs au niveau des noeuds. Les switchs interagissent entre eux, puis redistribuent l‚Äôinformation au niveau de ces serveurs.\n\n\nLa soci√©t√© a d√©j√† fait d‚Äôautres sessions lors d‚Äôautres conf√©rences tel que Velocity qui pourrait fortement int√©ress√© les adminsys ;-)\n\nDiscoRank : optimizing discoverability on SoundCloud, par Am√©lie Anglade\n\n\n\nCr√©dit : https://www.flickr.com/photos/andyet-photos/8680450402/in/set-72157633306379029\n\n\n\n@utstikkar est une fran√ßaise qui travaille pour Soundcloud en tant que MIR Software Engineer.\n\nElle nous a expliqu√© l‚Äô√©volution effectu√©e au sein de leur moteur de recherche : Discorank. Ce syst√®me peut √™tre assimil√© au PageRank de Google.\n\nIls utilisent pour cela : MySQL puis HDFS et enfin tout est re-manipul√© dans ElasticSearch\n\nBuddyCloud - Rethinking Social, par Simon Tennant\n\n\n\nCr√©dit : https://twitter.com/OriPekelman/status/326635400175161344/photo/1\n\nSimon Tennant est CEO de la soci√©t√© BuddyCloud. Il a tent√© de faire passer les informations suivantes :\n\n\n  f√©d√©rer ou mourir\n  travailler sur les protocoles non sur les APIs\n  pour construire du social dans un produit, il recommande de se baser sur l‚Äôopen source, les standards et protocoles ouverts\n\n\nL‚Äôobjectif de sa soci√©t√© est de permettre aux personnes de construire un r√©seau social f√©d√©r√© et bien entendu temps-r√©el.\n\nN‚Äôh√©sitez pas √† fouiller dans leur source sur Github) qui fourmille de fonctionnalit√©s.\n\nRealtime at Microsoft, Pierre Couzy\n\nPierre Couzy travaille depuis plus de 10 ans chez Microsoft. Il nous pr√©sente un projet r√©alis√© par des d√©veloppeurs US : SerialR (Github). Le projet utilise les Websockets.\n\n\n\nCr√©dit : https://www.flickr.com/photos/andyet-photos/8679336549/in/set-72157633306379029\n\nLe projet poss√®de deux d√©pendances principales :\n\n\n  Json.net c√¥t√© serveur\n  Jquery c√¥t√© client\n\n\nIl est noter que la n√©gociation exacte entre le client et le serveur d√©pend du navigateur utilis√©.\n\nA noter que cette pr√©sentation est l‚Äôune des rares qui n‚Äôa pas √©t√© r√©alis√©e avec un MacBook ;-)\n\nLearning from Past Mistakes, a new node http layer, par Tim Caswell\n\n@creationix √©tait un des anciens core dev de NodeJS. Il nous expose les diff√©rents points cause desquels il a quitt√© le projet.\n\n\n\nCr√©dit : https://twitter.com/OriPekelman/status/326670519371980800/photo/1\n\nUn des autres probl√®mes avec NodeJS est que le changement est difficile :\n\n\n  utilis√© en production par diff√©rentes soci√©t√©s\n  difficult√© √† modifier les APIs\n\n\nIl a donc d√©velopp√© Luvit bas√© sur la technologie Lua (l√©g√®re, rapide et permettant les co-routines).\n Cette nouvelle couche HTTP donne la possibilit√© :\n\n\n  de suspendre et de reprendre la fibre actuelle\n  lorsque l‚Äôon a des fibres on peut faire d‚Äôautres choses\n  √©crire sur les objets stream avec .write(item)\n  lire sur les objets stream avec .read()\n  de terminer un stream avec false item\n\n\nVous pouvez retrouver l‚Äôensemble des slides sur Github\n\nHTTP Proxy, par Nuno Job\n\n\n\nCr√©dit : https://www.flickr.com/photos/andyet-photos/8680421388/in/set-72157633306379029\n\n@dscape nous propose un bon article sur le load balancing avec nodejs.\n Pour le speaker nodejs c‚Äôest : net protocols &amp;amp;&amp;amp; libuv &amp;amp;&amp;amp; v8 &amp;amp;&amp;amp; npm\n\nVous pourrez retrouver l‚Äôensemble des slides sur Github\n\nLearning How To Let Go, par Kyle Drake\n\n@kyledrake introduit d‚Äôautres solutions en remplacement de JSON : bas√©s sur des donn√©es en binaire.\n\n\n\nCr√©dit : https://twitter.com/OriPekelman/status/326684373090963456/photo/1\n\nCependant, tout le monde n‚Äôutilise pas correctement les √©changes binaires. Le speaker nous fait un tr√®s bon r√©sum√© de la situation pour effectuer des pushs sur la plateforme d‚ÄôApple (Apple Push Notification Service) et des types de retours effectu√©s par Apple. Ceci r√©sume assez bien la situation.\n\n\n\nCr√©dit : https://twitter.com/noel_olivier/status/326686457257406464\n\nJS ne propose pas d‚ÄôAPI ‚Äúnative‚Äù mais un projet permet de traiter du binaire : binaryjs (du binaire via websockets).\n\nKyle effectue diff√©rents benchs sur la taille des contenus envoy√©s : l‚Äôun en JSON, l‚Äôautre en Binary JSON et le dernier via MessagePack. Bien entendu, ce sont les contenus en binaire qui sont les plus l√©gers, mais il reste voir l‚Äôimpact du t√©l√©chargement du JS associ√© et du traitement c√¥t√© client.\n\nUn sujet donc √† √©tudier qui rappelle AMF pour √©changer des informations au format binaire entre PHP et Flash.\n\nSecuring socket applications, par James Coglan\n\nDans un premier temps, @jcoglan nous indique que la s√©curit√© c‚Äôest difficile et que cela concerne :\n\n\n  l‚Äôauthentification\n  la vie priv√©e\n  les XSS\n  les CSRF\n\n\nPour r√©pondre aux diff√©rentes probl√©matiques, il nous pr√©sente Faye un syst√®me simple de message pub/sub pour le web. Ses slides sont disponibles.\n\nReal-time design, par Jan-Christoph Borchardt\n\n\n\nCr√©dit : https://twitter.com/OriPekelman/status/326711206209527809/photo/1\n\n@jancborchardt n‚Äôest pas un d√©veloppeur, mais il nous rappelle quelques concepts importants :\n\n\n  Plus l‚Äôutilisateur s‚Äôennuie, plus la confusion augmente\n  Sous 0,1 ms, l‚Äôutilisateur consid√®re cela comme du temps r√©el\n  Attention aux transitions\n  Ne pas tuer la fluidit√©\n  ‚ÄúInterruptification‚Äù (exemple flagrant sur l‚Äôimage ci-dessous)\n  Pas de notifications pendant l‚Äôutilisation (ex batterie faible 20% sur un mobile)\n\n\n\n\nEn r√©sum√©, l‚Äôinterface et le design sont importants √©galement pour s‚Äôapprocher d‚Äôune exp√©rience utilisateur temps-r√©el.\n\nFin de la seconde journ√©e\n\nPour les plus courageux, le livestream est √©galement disponible.\n\n\n\nCet √©v√©nement √©tait tr√®s int√©ressant :\n\n\n  par son avance de phase, la majorit√© des pr√©sentations correspondaient des r√©sultats de R&amp;amp;D, voire d‚Äôinnovation.\n  par l‚Äôambiance\n  par le networking que l‚Äôon pouvait y faire\n\n\nOn peut en revanche peut √™tre un peu regretter le nombre de fran√ßais (la fois c√¥t√© speaker et √©galement c√¥t√© public).\n\nPour terminer, un grand merci Julien Genestoux qui a organis√© l‚Äô√©v√©nement.\n\nRendez-vous pour la prochaine √©dition.\n"
} ,
  
  {
    "title"    : "CR Real Time Conf√©rence Europe 2013 - Day 1",
    "category" : "",
    "tags"     : " conference, nodejs, zeromq, rabbitmq, realtime",
    "url"      : "/cr-real-time-conference-europe-2013-day-1",
    "date"     : "April 25, 2013",
    "excerpt"  : "Les 22 et 23 Avril 2013, ont eu lieu, la Real Time Conf√©rence en version Europ√©enne.\n\nPour cette premi√®re √©dition, les festivit√©s se d√©roulaient √† Lyon, √† la Plateforme, une p√©niche pos√©e sur les quais du Rh√¥ne tr√®s sympathique.\n\nPass√© l‚Äôaccueil ‚Äú...",
  "content"  : "Les 22 et 23 Avril 2013, ont eu lieu, la Real Time Conf√©rence en version Europ√©enne.\n\nPour cette premi√®re √©dition, les festivit√©s se d√©roulaient √† Lyon, √† la Plateforme, une p√©niche pos√©e sur les quais du Rh√¥ne tr√®s sympathique.\n\nPass√© l‚Äôaccueil ‚Äúla Titanic‚Äù avec l‚Äôorchestre dans le hall d‚Äôentr√©e, nous descendons au sous-sol pour commencer suivre la premi√®re journ√©e de conf√©rence, qui s‚Äôannonce d√©j√† tr√®s charg√©e.\n\n\n\nLa vue de la salle de conf√©rence (Cr√©dit : https://twitter.com/frescosecco/status/326302218515017729/photo/1 )\n\nWebSuckets, par Arnout Kazemier\n\nPremi√®re conf√©rence autour des Websockets et des bugs ou difficult√©s d‚Äôimpl√©mentation que l‚Äôon peut rencontrer.\n\nOn parle notamment de Firefox qui en prend pour son grade : Si l‚Äôon appuie sur ESC apr√®s que la page soit charg√©e, toutes les connexions sont ferm√©es ‚Ä¶ Firefox ne peut pas se connecter non plus sur une Websocket non s√©curis√©e en HTTPS.\n\nCot√© Safari Mobile, √©crire dans une Websocket ferm√©e plante votre t√©l√©phone, et cela arrive quand on revient sur un onglet qui utilisait des Websocket, ou lorsque l‚Äôon r√©ouvre un safari pr√©c√©demment r√©duit.\n\nBref, en gros, ca d√©motive un petit peu sur l‚Äôutilisation des Websockets !\n\nArnout ( @3rdEden ) d√©conseille aussi l‚Äôutilisation des Websocket sur mobile, et indique de ne les utiliser que quand c‚Äôest vraiment n√©cessaire sur desktop.\n\nQuelques pr√©sentations d‚Äôoutillages :\n\n\n  HA Proxy\n  HTTP-Proxy\n  Nginx-devel\n\n\nVous pouvez retrouver une battle sur les perfs de ces proxys ici : github.com/observing/balancerbattle\n\nOn aborde aussi les probl√©matiques de tirs de charge sur les Websocket avec :\n\n\n  wsbench\n  websocketbenchmark\n\n\nLes deux √©tant, d‚Äôapr√®s Arnout, incomplets ou d√©pass√©s ‚Ä¶\n\nIl a donc d√©velopp√© son propre outil : Thor, ‚Äúsmasher of Websockets‚Äù √† tester de toute urgence : https://github.com/observing/thor\n\nLes frameworks mentionn√©s pour en simplifier l‚Äôimpl√©mentation :\n\n\n  Faye\n  Signalr\n  xsockets\n  sockjs\n  socket.io\n\n\nAttention aussi aux √©l√©ments perturbateurs : firewall, extensions de browsers, antivirus, ou proxy qui peuvent bloquer les ports utilis√©s par les Websockets.\n\nBref, une premi√®re entr√©e en mati√®re tr√®s compl√®te et int√©ressante qui couvre vraiment toute la partie moins glamour des Websockets.\n\nJe vous invite aussi √† consulter son blog si le sujet vous int√©resse : https://blog.3rd-eden.com/\n\n\n\n(Cr√©dit : https://twitter.com/hintjens/status/326243158109347841/photo/1 )\n\n\n\nSocketStream 0.4, par Owen Barnes\n\nVoici l‚Äôun des frameworks pour l‚Äôimpl√©mentation des Websockets, o√π son cr√©ateur ( @socketstream ) nous partag√© ses id√©es de la conception et de l‚Äôutilisation d‚Äôun framework : d√©couplage, simplicit√©, modularit√© etc.\n\nLe framework est ‚ÄúTransport Agnostics‚Äù et peut donc utiliser sockJs, Engine.io, ou Websockets native juste en changeant une simple ligne.\n\nLe FW est bas√© sur Prism, un module de serveur realtime, lui aussi open-sourc√© sur github.com/socketstream/prism.\n\nLa 0.4 pr√©sent√©e est en cours de finalisation, et sera disponible prochainement en version finale sur le github https://github.com/socketstream/\n\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8672407195/in/set-72157633306379029/ )\n\nXSockets, par Magnus Thor\n\nMagnus ( @dathor ), nous pr√©sente son framework Xsockets pour l‚Äôutilisation des Websockets avec une d√©mo ‚Äúlive coding‚Äù peut-√™tre int√©ressante, mais tent√©e ‚Äúonline‚Äù et avec une connexion bien foireuse (comme dans toutes les conf√©rences techniques, non ?) ‚Ä¶\n\nBref, un peu douloureux √† regarder, mais la d√©mo avait l‚Äôair d‚Äôavoir du potentiel : une application web utilisant WebRPC pour partager en mode Peer To Peer la Webcam de l‚Äôutilisateur.\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8673638532/in/set-72157633306379029/ )\n\nZeroMQ as scriptable sockets, par Lourens Naud√© (Bear Metal)\n\nLourens ( @methodmissing) est l‚Äôun des ‚Äúco-maintainer‚Äù de ZeroMq.\n\nIl nous pr√©sente ZeroMq comme une solution de messagerie instantan√©e pour les apps. Ca n‚Äôest pas un serveur, ni un broker, mais une librairie sur la communication et gestion de la concurrence.\n\nOn parcourt ensuite les diff√©rents types de sockets support√©s :\n\n\n  Req / Rep\n  Pub / Sub\n  Push / Pull\n\n\nVoir la pr√©sentation ci dessous :\n\n\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8672695927/in/set-72157633306379029/ )\n\nWebRTC, par Sam Dutton (Google)\n\nSam Dutton, Developper Advocate chez Google ( @sw12 ), qu‚Äôon √† d√©ja vu/entendu par le pass√© √† la V√©locity Conf (voir pr√©c√©dent CR) nous parle de WebRPC.\n\nOn parcourt les diff√©rentes API disponible, le support des navigateurs (Chrome, Firefox Nightly et IE Chrome Frame ‚Ä¶).\n\nOn d√©couvre ensuite de nombreuses d√©mos tr√®s sympa :\n\n\n  Ascii Cam√©ra\n  GetUserMedia\n  Webcam Toy\n  Magic Xylophone\n  Screen Capture (n√©cessite Chrome Canary)\n  ‚Ä¶\n\n\nPour d√©bugger plus facilement, utilisez le chrome://webrtc-internals\n\nLibs, apps et frameworks pour XML RPC :\n\n\n  easyRTC : full stack\n  conversat.io built with SimpleWebRTC\n  PeerJS : API abstraction\n  webRTC.io\n  Sharefest\n\n\nPlus d‚Äôinfos/codes ou d√©mos sur les slides : https://samdutton.net/realtime2013/\n\n\n  ‚ÄúWebRTC and HTML5 could enable the same transformation for real-time communications that the original browser did for information.‚Äù Phil Edholm / Nojitter\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8673907134/in/set-72157633306379029 )\n\nEnemy of the state : An introduction to functional reactive programming with Bacon.Js, par Phiilip Roberts (Float)\n\n\n\nL‚Äôune des pr√©sentations les plus int√©ressantes de la journ√©e par Philip Roberts ( @philip_roberts ), CTO et co-founder de Float avec l‚Äôintroduction Bacon.Js et la ‚ÄúFunctional Reactive Programming‚Äù en Javascript. Une fa√ßon diff√©rente de coder pour √©viter les ‚Äúcallback hell‚Äù notamment.\n\nLe projet r√©pond aussi √† une probl√©matique tr√®s courante des d√©v JS, avec l‚Äôexemple du ‚ÄúCheck Username Availibility‚Äù qui lance une requ√™te Ajax chaque KeyPress et dont l‚Äôordre n‚Äôest pas maitris√©. (partir de la slide 38)\n\nBacon.Js est dispo sur Github : https://github.com/raimohanska/bacon.js\n\nP.s: la visualisation des streams sur ses slides √©tait tr√®s sympa : https://latentflip.com/bacon-examples/\n\nPlus d‚Äôinfos sur les slides : https://latentflip.com/bacon-talk-realtimeconfeu/\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8673684632/in/set-72157633306379029 )\n\nQuick Wins with Redis for your website, par Cathering Jung\n\nCatherine Jung ( @bufferine ) travaille sur des services de paris en ligne. Elle explique les probl√©matiques de temps r√©el qu‚Äôelle doit affronter, et comment Redis lui permet de mieux supporter la charge.\n\nAu final, on parle un peu plus de Scala que de Redis, mais tout retour d‚Äôexp√©rience est toujours bon prendre.\n\nRetrouvez les slides ici :\n\nhttps://docs.google.com/file/d/0By6ZH5wplIR-MzgyOEJCMEkyWmc/edit?usp=sharing \n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8673926016/in/set-72157633306379029/ )\n\nRealtime and Go : Leaving the frameworks behind, par Paddy Foran\n\nPaddy ( @paddyforan ) nous pr√©sente le language Go.\n\nA la question : ‚ÄúWhat is Go ?‚Äù la r√©ponse est :\n\n\n  A better C, from the guys that didn‚Äôt bring you C++\n\n\nhttps://goonaboat.com/\n\nBref, Go c‚Äôest :\n\n\n  Compiled\n  Static typed\n  Fast\n  Elegant\n  Concurrent\n\n\nLes slides sont disponibles ici : https://goonaboat.com/ et le code de la pr√©sentation : https://github.com/paddyforan/goonaboat\n\nPlus d‚Äôinfos sur le langage ici : https://golang.org/ avec un ‚ÄúTour‚Äù qui parait tr√®s bien fait : https://tour.golang.org/#1\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8673939104/in/set-72157633306379029 )\n\nCloud Messaging with Node.Js and RabbitMQ, par Alvaro Videla\n\nAlvaro ( @oldsound ) est le co-auteur de ‚ÄúRabbit Mq In action‚Äù.\n\nIl a pr√©sent√© l‚Äôint√©r√™t d‚Äôutiliser un rabbitMQ dans un projet qui est un fork d‚ÄôInstagram, mais Real Time : CloudStagram, sur une stack ‚ÄúCloud Foundy‚Äù, Rabbit MQ, Redis, MongoDB et SockJS\n\nNotamment le concept de tout g√©rer via √©v√©nement (slide 54 ci-dessous).\n\nBref, pas mal de bonnes id√©es √† retenir et pas mal de projets int√©ressants sur son github : https://github.com/videlalvaro , comme le RabbitMqSimulator pour pr√©senter clairement le fonctionnement des RabbitMQ\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8673976982/in/set-72157633306379029/ )\n\n\n\nOffline first!, par Jan Lehnardt\n\nJan ( @janl ) bosse sur CouchDb. Apache CouchDB est une base de donn√©es de type document bas√©e sur le format JSON et utilisant Javascript (notamment pour les MapReduce).\n\nIl commence sa pr√©sentation par un ‚ÄúYou are all doing it wrong !‚Äù. En r√©expliquant que le r√©seau est toujours rapide, mais que c‚Äôest la latence qui est probl√©matique. (Voir l‚Äôexcellent article de 2010 de @edasfr sur le sujet toujours aussi pertinent ).\n\nIl faut aujourd‚Äôhui travailler Offline First ! (un peu l‚Äô√©quivalent d‚Äôun Mobile First cot√© apps), et prend pas mal d‚Äôexemples de bonne ou mauvaise impl√©mentation (de la gestion hors connexion, du passage dans un tunnel, en se moquant de la mauvaise couverture fran√ßaise dans le TGV).\n\nOn aborde ensuite les :\n\n\n  CouchDB\n  PouchDB : Javascript database that syncs!\n  TouchDB : CouchDB-compatible embeddable database engine for mobile &amp;amp; desktop apps\n\n\net la pr√©sentation du framework Hoodie : https://hood.ie/, bas√© sur le Offline par d√©faut.\n\n\n  ‚ÄúThink of CouchDB as Git for your application data‚Äù Jan Lehnardt\n\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8672884965/in/set-72157633306379029/ )\n\nBuilding Realtime HTML5 apps for Android and Firefox OS, par Anthony Ricaud\n\nPr√©sentation par Anthony Ricaud ( @rik24d ) des fonctionnalit√©s HTML5 impl√©ment√©es par les √©quipes de Mozilla, notamment pour connaitre l‚Äô√©tat de la batterie, l‚Äôorientation, la gestion des apps supportant la s√©lection de photos par exemple‚Ä¶\n\nChaque site peut √™tre une apps, √† condition de mettre les lignes n√©cessaires dans un fichier manifest. Beaucoup de d√©bats aussi autour des syst√®mes ferm√©s de MarketPlace.\n\nPlus d‚Äôinfos dans les slides ci dessous :\n\n\n\n(Cr√©dit : https://www.flickr.com/photos/andyet-photos/8672894401/in/set-72157633306379029 )\n\n\n\nNodeCopter + Hackathon\n\nJe laisse la parole √† Olivier Mansour pour la pr√©sentation du NodeCopter :\n\nRomain Huet ( @romainhuet ) nous a pr√©sent√© et fait une petite d√©monstration du pilotage d‚Äôun AR Drone avec NodeJs.\n\nIssu du projet nodecopter https://nodecopter.com/, un ensemble de librairies Node.js est disponible et rend le pilotage du drone compl√®tement accessible. Mouvements en vol, stream de la cam√©ra, Romain nous a fait une d√©monstration fun et captivante de l‚Äôengin.\n\nEt faire voler un drone dans un bateau ‚Ä¶ on avait jamais vu √ßa !\n\nFin de la 1√®re journ√©e :\n\nUne premi√®re journ√©e tr√®s sympathique, bourr√©e d‚Äôid√©es et d‚Äôoutils en tout genre. L‚Äôorganisation est vraiment au poil, et on repart en voulant refaire le monde techniquement :-)\n\nLe compte rendu de la deuxi√®me journ√©e est ici : https://tech.bedrockstreaming.com/cr-real-time-conference-europe-2013-day-2\n\nP.s : Merci &amp;amp;Yet pour la plupart des photos pr√©sentes ici : https://www.flickr.com/photos/andyet-photos/sets/72157633306379029/\n\nPour les plus motiv√©s, la conf√©rence √©t√© enregistr√©e en vid√©o :\n\n\nVid√©o de la premi√®re journ√©e de la RealTime Conf Europe\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #2",
    "category" : "",
    "tags"     : " humour, devfacts",
    "url"      : "/m6web-dev-facts-2",
    "date"     : "April 9, 2013",
    "excerpt"  : "On continue la s√©rie des D√©v Facts, phrases oh combien cultes prononc√©es par nos chers d√©veloppeurs lors d‚Äôoublis c√©r√©braux :-)\n\nPour ceux qui avaient rat√© la premi√®re s√©rie, c‚Äôest ici : https://tech.bedrockstreaming.com/m6web-dev-facts-1\n\nEnjoy\n\n...",
  "content"  : "On continue la s√©rie des D√©v Facts, phrases oh combien cultes prononc√©es par nos chers d√©veloppeurs lors d‚Äôoublis c√©r√©braux :-)\n\nPour ceux qui avaient rat√© la premi√®re s√©rie, c‚Äôest ici : https://tech.bedrockstreaming.com/m6web-dev-facts-1\n\nEnjoy\n\nCaptain Obvious\n\n\n  Chef de Projet : ‚ÄúVous avez oubli√© quelque chose ?‚Äù\n\n  D√©veloppeur : ‚ÄúLe probl√©me c‚Äôest que quand t‚Äôoublie, t‚Äôy pense pas‚Äù\n\n\nPaye ta culture\n\n\n  C‚Äôest une citation de la bible ‚Ä¶ et de Civilisation IV\n\n\nSeigneur non !\n\n\n  \n    Avant j‚Äôencodais les vid√©os pour l‚Äô√©mission ‚ÄúLe jour du Seigneur‚Äù\n    C‚Äôest quoi ? une √©mission porno ?\n    Ca d√©pend comment t‚Äô√©cris ‚Äúseigneur‚Äù\n  \n\n\nLes cong√©s du fantastique\n\n\n  Le 4, je suis en RPG ‚Ä¶ euh RTT\n\n\nDouble comp√©tence ‚Ä¶\n\n\n  Je viens de recevoir le CV d‚Äôun gars, il a fait une formation ‚ÄúMa√Ætrise en patisserie‚Äù ‚Ä¶ il doit ma√Ætriser les cookies non ? :)\n\n\nIncomparable pour ne rien comparer\n\n\n  \n    Ah bon ? C‚Äôest les TCL qui font le plus souvent gr√®ve en France ? Plus que la SNCF ?\n    Oui, √† titre de comparaison, je crois que c‚Äôest incomparable ‚Ä¶\n  \n\n\nRetour vers le futur\n\n\n  Donc vous savez que je ne suis pas l√† la semaine derni√®re\n\n\nTrouv√© !\n\n\n  Je viens de trouver une d√©couverte !\n\n\nLa preuve par dix\n\n\n  On a doubl√© la bande passante par dix\n\n\nLe flegme illustr√©\n\n\n  De toute facon, y‚Äôa pas de cons√©quence : au pire, on meurt.\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #1",
    "category" : "",
    "tags"     : " humour, devfacts",
    "url"      : "/m6web-dev-facts-1",
    "date"     : "March 21, 2013",
    "excerpt"  : "Depuis de nombreuses ann√©es, toutes les ‚Äúphrases chocs‚Äù dites par les √©quipes techniques de M6Web sont archiv√©es, logg√©es, historis√©es. Pas moyen de sortir une √¢nerie sans qu‚Äôelle soit grav√©e dans le marbre.\n\nDu coup, nous avons d√©cid√© de profiter...",
  "content"  : "Depuis de nombreuses ann√©es, toutes les ‚Äúphrases chocs‚Äù dites par les √©quipes techniques de M6Web sont archiv√©es, logg√©es, historis√©es. Pas moyen de sortir une √¢nerie sans qu‚Äôelle soit grav√©e dans le marbre.\n\nDu coup, nous avons d√©cid√© de profiter de cette p√©riode tr√®s calme sur le blog pour vous faire partager une petite s√©lection en vrac de quelques Dev Facts entendus dans les locaux d‚ÄôM6Web Lyon :\n\ninvalider le cache, nommer les choses ‚Ä¶\n\n\n  \n    On purge le cache infini tous les jours.\n    Non c‚Äôest le cache 7 jours qu‚Äôon purge tous les jours !\n  \n\n\nJ‚Äôajoute -2\n\n\n  J‚Äôai r√©duit de fois 10\n\n\nwork - not - flow\n\n\n  On a des workflows, on a aussi des worknotflows\n\n\nLa bonne affaire\n\n\n  Pour le m√™me prix, t‚Äôas bien moins cher ailleurs !\n\n\nEt pour arr√™ter ? tu cliques sur d√©marrer !\n\n\n  La vid√©o a √©t√© mise en erreur avec succ√®s.\n\n\nBig or what ?\n\n\n  Il y a une librairie Rennes, elle est √©norme! Tu y rentres, c‚Äôest tout petit‚Ä¶ !\n\n\nNiveau CE2\n\n\n  je fais un mail de r√©ponse avec une explication niveau CE2\n\n\nBref\n\n\n  \n    \n      J‚Äôai demand√© Pierre, qui m‚Äôa dit qu‚Äôil ne savait pas mais que si j‚Äôavais l‚Äôinfo, il la voulait bien. Du coup, j‚Äôai demand√© Kenny, qui m‚Äôa dit de demander a Antony‚Ä¶ qui ne savait pas et m‚Äôa dit de demander Pierre\n    \n    \n      Bref, j‚Äôai pos√© une question un admin\n    \n  \n\n\nCDP Junior\n\n\n  Raa mais chui un cdp junior moi je sais rien faire de mes 10 doigts :(\n\n\nLa suite dans un prochain √©pisode ;-)\n\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un Lead Developpeur / Architecte web (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/m6web-lyon-recherche-un-lead-developpeur-architecte-web-h-f-en-cdi",
    "date"     : "February 5, 2013",
    "excerpt"  : "\n\nMise jour : Le poste n‚Äôest √† plus pourvoir. Merci\n\nM6Web Lyon recrute, en CDI, un Lead D√©veloppeur LAMP, avec une tr√®s forte expertise sur les technologies PHP 5.4, MySQL, Symfony2, GIT, et capable d‚Äôencadrer une petite √©quipe de d√©veloppement.\n...",
  "content"  : "\n\nMise jour : Le poste n‚Äôest √† plus pourvoir. Merci\n\nM6Web Lyon recrute, en CDI, un Lead D√©veloppeur LAMP, avec une tr√®s forte expertise sur les technologies PHP 5.4, MySQL, Symfony2, GIT, et capable d‚Äôencadrer une petite √©quipe de d√©veloppement.\n\nNous recherchons quelqu‚Äôun de tr√®s passionn√©, enthousiaste, et mordu de veille technologique : un missionnaire de l‚Äôopen source, un int√©griste de la qualit√© de code, des tests unitaires et fonctionnels, et un architecte de projets aguerri avec une premi√®re approche en m√©thodologie de d√©veloppement agile, et une exp√©rience de management de d√©veloppeurs.\n\nSi, en plus, vous √™tes un malade de l‚Äôoptimisation back-end et front-end, que des technologies comme Node.js vous √©moustillent, que, malgr√© la qualit√© de MySQL, vous envisagez dans certains cas des solutions NoSQL alternatives (Mongo, Redis‚Ä¶), votre profil nous int√©resse !\n\nVenez apporter vos comp√©tences aux √©quipes techniques de M6Web en travaillant sur des sites tr√®s forte charge (m6.fr, clubic.com, jeuxvideo.fr ‚Ä¶), et partagez-les gr√¢ce des conf√©rences internes ou externes et des articles sur notre blog.\n\nSi vous avez les qualit√©s requises et l‚Äôenvie de nous rejoindre, allez sur le lien ci-dessous et faites nous part de votre CV, de votre compte github, et d‚Äôune lettre attrayante pour nous motiver vous rencontrer.\n\nSi vous souhaitez postuler ou avoir plus d‚Äôinfos : https://www.groupem6.fr/ressources-humaines/offres-emploi/lead-developpeur-architecte-web-h-f-229879.html\n\n"
} ,
  
  {
    "title"    : "Organiser des conf√©rences technique en interne",
    "category" : "",
    "tags"     : " conference, culture, lft",
    "url"      : "/organiser-des-conferences-technique-en-interne",
    "date"     : "December 5, 2012",
    "excerpt"  : "\n\nLes ‚ÄúLast Friday Talk‚Äù : Le concept\n\nDepuis 10 mois d√©sormais, chez M6Web, nous organisons chaque ‚Äúdernier vendredi‚Äù du mois, une manifestation que nous avons nomm√©e ‚ÄúLast Friday Talk‚Äù.\n\nLe concept : 2 heures de 13h30 15h30, o√π 4 sessions ‚Äútype ...",
  "content"  : "\n\nLes ‚ÄúLast Friday Talk‚Äù : Le concept\n\nDepuis 10 mois d√©sormais, chez M6Web, nous organisons chaque ‚Äúdernier vendredi‚Äù du mois, une manifestation que nous avons nomm√©e ‚ÄúLast Friday Talk‚Äù.\n\nLe concept : 2 heures de 13h30 15h30, o√π 4 sessions ‚Äútype conf√©rence‚Äù de 25 minutes, suivies de 5 minutes de questions, sont pr√©sent√©es par des personnes de la Direction Technique. La participation (orateur ou public) est bien entendue facultative.\n\nL‚Äôid√©e, est que chacun a quelque chose dire dans le web de nos jours, quelque chose pr√©senter/partager aux autres. Soit un retour d‚Äôexperience sur une techno, un outil, une m√©thodologie, soit m√™me une pr√©sentation de ses d√©veloppements ou projets pass√©s.\n\nDes exemples ?\n\nQuelques exemples de pr√©sentations qui ont √©t√© faites :\n\n\n  Z√©roMq, la biblioth√®que r√©seau\n  Pr√©sentation du langage Python\n  VIM pour les nuls\n  Les m√©thodes agiles\n  Doctrine 2\n  La s√©curit√© SQL\n  Le d√©ploiement chez Facebook\n  La WebPerf avanc√©e\n  Pr√©sentation de CoffeeScript\n  ‚Ä¶\n\n\nL‚Äôinter√™t ?\n\nLes apports pour vos √©quipes sont nombreux :\n\n\n  Toute la direction technique participe et partage une partie de la veille technologique de chacun.\n  Les conf√©renciers am√©liorent leur communication de ‚Äúgroupe‚Äù.\n  Ils deviennent souvent le r√©f√©rent sur le sujet dans l‚Äôentreprise.\n  C‚Äôest du Team Building, et un rendez-vous mensuel avec vos √©quipes.\n  Et cela donne des id√©es √† tous les autres d√©veloppeurs pour de futurs projets (perso ou d‚Äôentreprise bien s√ªr), et attise leur curiosit√©.\n\n\nNous avons donc tous les mois entre 5 et 10 pr√©sentations propos√©es pour n‚Äôen choisir que 4, et une trentaine de participants par session au niveau du public, et nous filmons toutes les conf√©rences, et les archivons sur un site interne.\n\nLes ‚ÄúKaraok√© Slideshow‚Äù pour plus de fun !\n\nPour combler les mois les plus creux, nous avons aussi tent√© une session ‚ÄúKaraok√© Slideshow‚Äù qui fut hilarante (dans l‚Äôesprit des ‚ÄúIgnite Karaok√©‚Äù pour ceux qui connaissent) ! \n Le concept : Entre 5 et 10 volontaires font une pr√©sentation tour tour, sur une s√©rie de 5 slides qu‚Äôils n‚Äôont jamais vus, dont chaque slide d√©file toutes les 15 secondes. Le th√®me est libre et est improvis√© en fonction des slides !\n\nC‚Äôest un bel exercice d‚Äôimprovisation et le fun est garanti :-)\n\nConclusion\n\nAlors si votre entreprise ou votre univers le permet, nous vous conseillons vraiment de tenter l‚Äôaventure. C‚Äôest tr√®s formateur, instructif, et int√©ressant pour tout le monde, et cela apporte une vraie culture de la veille dans votre environnement ;-)\n\n\n\nOlivier Mansour nous pr√©sente la Programmation Orient√© Objet ‚ÄúCanada Dry‚Äù !\n\n"
} ,
  
  {
    "title"    : "M6Web au banquet de la cuisine du web",
    "category" : "",
    "tags"     : " conference, lcdw",
    "url"      : "/m6web-au-banquet-de-la-cuisine-du-web",
    "date"     : "November 23, 2012",
    "excerpt"  : "Une partie de l‚Äô√©quipe de M6Web √©tait pr√©sente au banquet de la cuisine du web avec la fine fleur du web Lyonnais !\n\n\n\n",
  "content"  : "Une partie de l‚Äô√©quipe de M6Web √©tait pr√©sente au banquet de la cuisine du web avec la fine fleur du web Lyonnais !\n\n\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conf√©rence Europe 2012 : Day 3",
    "category" : "",
    "tags"     : " conference, velocity, webperf, mobile, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-3",
    "date"     : "October 9, 2012",
    "excerpt"  : "\n\nTroisi√®me et derni√®re journ√©e la V√©locity Europe.\n\nOn arrive d√©j√† fatigu√© et gav√© d‚Äôinformations et id√©es en tout genre, mais on a h√¢te de d√©marrer cette journ√©e ! :-)\n\n[Mobile] The Performance of Web Vs Apps, par Ben Galbraith et Dion Almaer (W...",
  "content"  : "\n\nTroisi√®me et derni√®re journ√©e la V√©locity Europe.\n\nOn arrive d√©j√† fatigu√© et gav√© d‚Äôinformations et id√©es en tout genre, mais on a h√¢te de d√©marrer cette journ√©e ! :-)\n\n[Mobile] The Performance of Web Vs Apps, par Ben Galbraith et Dion Almaer (Walmart.com)\n\nBen (@bgalbs) et Dion (@dalmaer) nous reprennent dans les grandes lignes, la conf√©rence faite la V√©locity Us (voir le CR + la vid√©o de ce talk : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-3-devops-webperf )\n\nL‚Äôid√©e est de comparer les experiences possibles sur WebApp et Apps Native, avec toujours cette comparaison tr√®s dr√¥le entre le mode de distribution des apps natives ce que cela donnerait si les show tv devraient √™tre distribu√©s de la m√™me mani√®re en prenant l‚Äôexemple de la s√©rie Friends : Hilarant !\n\nVoir la vid√©o ci-dessous vers 17min 30 :\n\n\n\nBen Galbraith et Dion Almaer (Source : https://royal.pingdom.com/2012/10/05/report-from-velocity-europe-day-3/ )\n\n\n\n[WebPerf] Lightning D√©mos, par Steve Souders et Patrick Meenan (Google)\n\nPatrick nous a montr√© les derniers ajouts int√©ressants de WebPageTest.org, avec notamment le ‚ÄúBlock Ads Feature‚Äù, l‚Äôonglet ‚ÄúSPOF‚Äù dans les param√®tres avanc√©s pour tester si nos scripts tiers sont des SPOF sur nos sites (j‚Äôy reviendrai) ‚Ä¶ L‚Äôoutil s‚Äôenrichit progressivement et reste toujours LA r√©f√©rence ultime du domaine !\n\n\n\nSteve Souders de son c√¥t√©, est revenu sur un cas √©tudi√© la veille, √† savoir l‚Äôimpl√©mentation d‚Äôun LazyLoader sur un caroussel, afin de d√©termin√© via Browserscope.js si cela repoussait le OnLoad event, et c‚Äôest le cas !\n\nPetite parenth√®se sur les caroussels, je vous invite lire cet article : Don‚Äôt Use Automatic Image Sliders or Carousels, Ignore the Fad\n\n\n\n[WebPerf] Do All Users Benefit Equally from Web Performance Optimizations? , par Arnaud Becart (ip-label)\n\nTalk sponsoris√© assez int√©ressant, qui √©tudie les donn√©es r√©colt√©es par Ip-label afin de voir si tout le monde profite de la WebPerf de la m√™me mani√®re. La r√©ponse est √©vidente, mais c‚Äôest int√©ressant de rappeler qu‚Äôil faut toujours comparer tests synth√©tiques au r√©el, et qu‚Äôen fonction du navigateur, du terminal, de la puissance de votre machine, ‚Ä¶, des optimisations WebPerf auront un impact diff√©rent, du n√©gatif au tr√®s positif.\n\n\n\n[DevOps] From DevOps to Operation Science, par Christopher Brown (Opscode)\n\nSon twitter : @skeptomai\n\nTalk orient√© ‚ÄúCulture‚Äù int√©ressant par l‚Äôun des cr√©ateurs de EC2. Mon moment d‚Äôabsence de la journ√©e :)\n\n\n\n[WebPerf] Performance and Metrics on lonelyplanet.com, par Mark Jennings et Dave Nolan (Lonely Planet)\n\nRetour d‚Äôexperience des gars de Lonelyplanet (sorte de Routard) tr√®s enrichissant. Notamment sur la fa√ßon de communiquer √† des √©quipes non techniques, les diff√©rentes exp√©rimentations r√©alis√©es, et le changement de culture op√©r√©, l‚Äôutilisation de Graphite avec notamment les Holt-Winters ‚Ä¶\n\n\n  ‚ÄúBeing Right isn‚Äôt Always Enough‚Äù !\n\n\n\n  ‚ÄúGive your metrics a public presence‚Äù\n\n\nLes slides : https://fr.slideshare.net/mbjenn/performance-and-metrics-at-lonely-planet-14589911\n\n\n\nMark Jennings et Dave Nolan (Source : https://twitter.com/smcinnes/status/253805752312033280/photo/1 )\n\n\n\n[WebPerf] Third-Party Scripts and You, par Patrick Meenan (Google)\n\nPatrick Meenan nous parle ici de SPOF (Single Point Of Failure ou Point Individuel De Defaillance en fran√ßais ‚Ä¶) et des 3rd party scripts.\n\nL‚Äôid√©e est de montrer comment suivant l‚Äôint√©gration Javascript de scripts tiers, vous pouvez rendre l‚Äôaffichage de votre site d√©pendant du bon fonctionnement des serveurs du script tiers.\n\nLes navigateurs mettent en g√©n√©ral 20 secondes (45 sous mac et linux) avant de rejeter une connexion sur un script tiers en rade. Vous pouvez voir des vid√©os de l‚Äôeffet que √ßa peut avoir sur vos pages dans les slides.\n\nAfin de les d√©tecter, il existe l‚Äôextension SPOF-O-Matic :\n\nhttps://chrome.google.com/webstore/detail/spof-o-matic/plikhggfbplemddobondkeogomgoodeg\n\nEn surfant, vous saurez rapidement si un SPOF est pr√©sent sur votre site ou non et combien de contenu il bloque, et pourrez g√©n√©rer un WebPageTest comparatif en simulant le plantage du script en question (Redirection sur domaine blackhole.webpagetest.org)\n\nPour r√©gler ces probl√®mes, plusieurs solutions : Script √† charger dynamiquement via Js de mani√®re asynchrone, script avec async et defer, ou au pire, script avant le /body.\n\nLes slides : https://www.slideshare.net/patrickmeenan/velocity-eu-2012-third-party-scripts-and-you\n\n\n\n[Ops] How Draw Something Absorbed 50 Million New Users, in 50 Days, with Zero Downtime, par J Chris Anderson (Couchbase)\n\nNous arrivons √† l‚Äôentr√©e de la salle o√π a lieu cette pr√©sentation, barr√© par des commerciaux CouchBase, nous emp√©chant de rentrer sans prendre le prospectus CouchBase et sans se faire scanner son QRcode pr√©sent sur nos badges ‚Ä¶ √ßa commence mal ‚Ä¶\n\nAu bout de 2 minutes de talk par J Chris Anderson ( @jchris ) , co-fondateur de Couchbase, le malaise est confirm√© : on ne parlera pas ici de Draw Something, mais de Couchbase 2.0 uniquement, le nom Draw Something n‚Äô√©tant l√† que pour app√¢ter du client potentiel, et √ßa marche, la salle est comble ‚Ä¶\n\nDifficile du coup d‚Äô√™tre concentr√© dans cette approche plus que douteuse ‚Ä¶ les questions au final seront aussi assez violentes sur le sujet : ‚Äúpourquoi app√¢ter les gens avec Draw Something, si √ßa n‚Äôest que pour parler de CouchBase ?‚Äù La r√©ponse est √©vasive ‚Ä¶ Nous n‚Äôavons pas eu le droit d‚Äôen parler ‚Ä¶\n\nBref, le produit Couchbase a tout de meme l‚Äôair tr√®s int√©ressant, et plut√¥t costaud, avec de tr√®s chouettes Dashboard de monitoring temps r√©els built-in.\n\nJ‚Äôen sors quand m√™me avec l‚Äôimpression tr√®s d√©sagr√©able de m‚Äô√™tre fait pi√©ger ‚Ä¶\n\nLes slides (non dispo) ressemblait fortement √† cette autre pr√©sentation de J Chris : https://speakerdeck.com/u/jchris/p/nosql-landscape-speed-scale-and-json\n\n\n\n[WebPerf] WebPagetest - Beyond the Basics, par Aaron Peters (Turbobytes), Andy Davies (Asteno)\n\nPas mal de conf√©rences parlaient de WebPageTest, mais celle-ci promettait d‚Äôaller en profondeur. Le cr√©ateur n‚Äôa jamais cach√© son manque de talent pour les interfaces, et WPT regorge de richesses en tout genre cach√©es dans les m√©andres de ses pages :-)\n\nEnormement d‚Äôinformations et tips sont donc pr√©sents dans les slides de cette conf√©rence.\n\nPour rappel : instruction d‚ÄôAndy pour monter une instance priv√©e de WPT : https://andydavies.me/blog/2012/09/18/how-to-create-an-all-in-one-webpagetest-private-instance/\n\nLes slides : https://www.slideshare.net/AndyDavies/web-page-test-beyond-the-basics\n\n\n\nAndy Davies et Aaron Peters (Source : https://royal.pingdom.com/2012/10/05/report-from-velocity-europe-day-3/ )\n\n\n\n[DevOps] What HTTP/2.0 Will* Do For You, par Mark Nottingham (Akamai)\n\nL‚Äôune des conf√©rences les plus int√©ressantes de la V√©locity pour ma part avec notamment l‚Äôannonce que HTTP/2.0 sera bas√© sur SPDY d√©j‚Ä¶\n\nMark Nottingham ( @mnot ), Chair of the IETF HTTPbis Working Group, excellent conf√©rencier, nous explique donc ce que sera HTTP/2.0 :\n\n\n  Aucun changement la s√©mantique HTTP\n  Bas√© sur Speedy\n  Multiplexing (voir slide 22)\n  Header Compression, technique tr√®s int√©ressante, pour √©viter de r√©-envoyer les memes headers pour chaque requ√™te HTTP\n\n\nPas mal de ressources sont disponibles sur son site : https://www.mnot.net/\n\nLes slides sont un mod√®le du genre, simples et efficaces : https://www.slideshare.net/mnot/what-http20-will-do-for-you\n\n\n\n[DevOps] Web &amp;amp; Native Cross-Platform Multiplayer, par Ashraf Samy Hegab (Orange)\n\nComment d√©velopper une exp√©rience de Gaming multi-plateforme : Web, Android, Iphone ? C‚Äôest la question √† laquelle Ashraf a essay√© de r√©pondre, sur cette derni√®re conf√©rence avec un humour et une √©nergie tr√®s communicative.\n\nPas mal de bonnes ide√©s applicables au web traditionnel, sur une stack NodeJs/Mongo/Socket.io, pour faire la majorit√© du travail et communiquer avec les parties natives d‚Äôapp Android et Ios.\n\nNous avons aussi fait une d√©mo live sur le jeu Phone Wars (Disponible sur Appstore et Google Play) d‚Äôune √©xperience Gaming Multi-plateforme.\n\nTr√®s rafra√Æchissant pour finir ces 3 journ√©es marathons !\n\nConclusion :\n\n√áa y‚Äôest, la V√©locity Europe est finie pour cette ann√©e. Les bouch√©es doubles ont √©t√© mises par rapport √† la V√©locity Berlin de l‚Äôann√©e derni√®re, et cette conf√©rence reste vraiment la conf incontournable pour tous ceux que la Webperf, les Devops, et les sites fort traffic int√©ressent !\n\nOn regrette simplement que seule la grande salle ait √©t√© film√©e, que la chasse aux slides soit toujours aussi tordue (trop peu renseign√© sur le site de la V√©locity). Le reste est juste parfait !\n\nA la prochaine, et merci pour vos retours.\n\nP.s: Merci aussi aux √©quipes de Pingdom pour leurs Twitt Live et les chouettes photos prises ( https://royal.pingdom.com/ )\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-2\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-1\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conf√©rence Europe 2012 : Day 2",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-2",
    "date"     : "October 8, 2012",
    "excerpt"  : "\n\nDeuxi√®me journ√©e, avec le d√©but ‚Äúofficiel‚Äù de cette conf√©rence, o√π l‚Äôon nous donne rendez vous dans l‚Äôimmense King‚Äôs Room.\n\nPour bien d√©marrer, on commence avec en quelque sorte l‚Äôhymne de la V√©locity : Speed &amp;amp; Velocity !\n\n\n\n[DevOps] Move Fa...",
  "content"  : "\n\nDeuxi√®me journ√©e, avec le d√©but ‚Äúofficiel‚Äù de cette conf√©rence, o√π l‚Äôon nous donne rendez vous dans l‚Äôimmense King‚Äôs Room.\n\nPour bien d√©marrer, on commence avec en quelque sorte l‚Äôhymne de la V√©locity : Speed &amp;amp; Velocity !\n\n\n\n[DevOps] Move Fast and Ship Things, par Girish Patangay (Facebook)\n\nPremi√®re pr√©sentation de la journ√©e par l‚Äôun des Manager chez Facebook, (maintenant dans les bureaux londoniens), sur la capacit√© de Facebook, et leur volont√©, d‚Äô √©voluer et de d√©ployer rapidement.\n\nIl nous raconte les d√©buts de Facebook, avec peu de serveurs, des changements inf√©rieurs 5Mb, Rsync pour pusher en prod etc ‚Ä¶ puis la migration vers HipHop.\n\nD√©sormais chaque changement n√©cessite de recompiler un gros binaire de 1.2Go, et d‚Äôy envoyer sur plus de 10 000 serveurs, et ce plusieurs fois par jour !\n\nAvec Bittorent, ils envoient 500Mb en moins d‚Äôune minute sur les 10 000 serveurs.\n\nOn a eu le droit une pr√©sentation de GateKeeper, outil interne, permettant de faire du feature flipping g√©olocalis√©. La timeline a par exemple plus d‚Äôune centaine de GateKepper.\n\nAujourd‚Äôhui, Facebook cherche trouver le moyen de scaler de plus de 1000 d√©veloppeurs √† 10000, et d‚Äô√©voluer sur ce syst√®me de ‚ÄúMove Fast‚Äù dans le mobile natif.\n\nPour ceux que √ßa int√©resse, en plus de la vid√©o √ßi dessous, Quora est une mine d‚Äôor d‚Äôinfos sur FB : https://www.quora.com/Girish-Patangay\n\n\n\n(Source https://royal.pingdom.com/2012/10/03/report-from-velocity-europe-day-2/ )\n\n\n\n[WebPerf] Keynote KITE and MITE, par Robert Castley (Keynote)\n\nVient ensuite la conf√©rence de Keynote (Sponsoris√©e), qui nous pr√©sente deux outils int√©ressant :\n\n\n  KITE pour Keynote Internet Testing Environnement : https://kite.keynote.com/\n  MITE pour Mobile Internet Testing Environnement : https://mite.keynote.com/\n\n\nMITE est d‚Äôailleurs utilis√© par le site de Google Howtogomo.com\n\nBref, si vous √™tes sur Windows (‚Ä¶), jetez y un oeil. Plus d‚Äôinfos en vid√©o :\n\n\n\n[WebPerf] Lightning D√©mos\n\nD√©mo 1 : Chrome Dev Tools :\n\nPremi√®re d√©mo de Iliya Grigorik ( @igrigorik) sur les capacit√©s avanc√©es de la chrome Dev Toolbar.\n\nOn peut par exemple faire un clic droit sur l‚Äôonglet Network pour r√©cup√©rer le har (voir le format HTTP Archive) en json, et utiliser d‚Äôautres outils avec ce har, notamment Yslow dont je parlais dans le compte rendu suivant, qui permet d‚Äôajouter les r√©gressions possible WebPerf dans votre CI Jenkins.\n\nOn parle aussi du chrome://tracing , du d√©bugger mobile, que la devtools est une WebApp avec une url propre et scriptable, du Chrome Benchmarking (extension) ‚Ä¶\n\nVoir la pr√©sentation suivante pour plus d‚Äôinfos, bien plus compl√©te : https://www.igvita.com/slides/2012/devtools-tips-and-tricks/\n\n\n\nD√©mo 2 : Box Anemometer :\n\nGavin Towey ( @gtowey), DBA MySql chez Box.com nous pr√©sente une interface pour visualiser et traiter correctement le slow log de Mysql. Bas√© sur Php 5.3, les outils Percona et Bootstrap pour l‚Äôinterface, l‚Äôoutil est un vrai bonheur pour tous ceux qui font un peu d‚Äôoptimisation MySql au quotidien, d√©veloppeur, sysadmin ou dba. Nous l‚Äôavions d√©j√† d√©couvert aux DevOpsDays Mountain View cet √©t√©, et l‚Äôutilisons massivement depuis.\n\nLe projet est sur Github : https://github.com/box/Anemometer\n\nVoir aussi le projet Rain Gauge dans la lign√©e de Anemometer, toujours par Gavin Towey : https://github.com/box/RainGauge\n\nVoici la d√©mo en vid√©o :\n\n\n\n[WebPerf] Emerging Markets / Growth Markets, par Jeff Kim (CDnetworks)\n\nJeff Kim, Chief Operating Officer chez CDnetworks nous a partag√© quelques donn√©es et chiffres int√©ressants sur les march√©s √©mergeants comme l‚ÄôInde, Indon√©sie, les Philippines, le Br√©sil etc\n\nOn apprend que Chrome a une part de march√© de 62% au Br√©sil, 51% en Inde, et Op√©ra de 26% en Russie. Que l‚Äôe-commerce au final n‚Äôa pas vraiment encore d√©marr√© en Inde, Br√©sil et Russie‚Ä¶\n\nApr√®s des √©tudes d‚ÄôEye Tracking View, on remarque aussi entre la population chinoise et am√©ricaine, que sur une page de r√©sultats de recherche, les am√©ricains ne regardent que le coin haut gauche de la page pour se contenter des premiers r√©sultats, alors que les chinois consultent vraiment toute la hauteur de la page, pour regarder tous les r√©sultats.\n\n\n\n[WebPerf] Why page speed isn‚Äôt enough, par Tim Morrow (Betfair)\n\nSon twitter : @timmorrow\n\nAncien de ShopZilla, et d√©ja pr√©sent Berlin l‚Äôann√©e derni√®re, Tim a partag√© son retour d‚Äôexp√©rience sur la refonte de BetFair (tr√®s gros site de pari en ligne). Les gens se plaignaient d‚Äôune mauvaise exp√©rience (alors que les pages refondues √©taient plus rapides), mais n√©cessitaient √† priori beaucoup plus de temps pour parvenir au pari final que dans l‚Äôancienne version. En gros, le temps de chargement de vos pages n‚Äôest pas suffisant, il faut aussi regarder les sc√©narios fonctionnels de vos sites. Ils sont pass√©s sur une navigation typ√©e Ajax pour ne pas rafra√Æchir dans certains cas l‚Äôint√©gralit√© de la page.\n\nVoir les slides et la vid√©o : https://fr.slideshare.net/timmorrow/why-page-speed-isnt-enough-tim-morrow-velocity-europe-2012\n\n\n\n\n[WebPerf] W3C Status on Web Performance, par Alois Reitbauer (Compuware, Dynatrace)\n\nBeau r√©capitulatif du status du ‚ÄúW3C performance working group‚Äù sur la performance Web, avec des rappels sur la NavTiming et les diff√©rents standards, quelques √©changes autour de la NavTimingV2, d‚Äôune Resource time Measurement etc ‚Ä¶\n\nSon twitter : @compuware et @AloisReitbauer\n\nPlus d‚Äôinfos dans la vid√©o ci dessous :\n\n\n\n[WebPerf] 3.5s Dash for attention and other stuff we found about RUM, par Philipp Tellis (Log Normal)\n\nSon twitter : @bluesmoon\n\nLe transcript et d√©tail complet de la pr√©sentation est pr√©sent sur un blogPost tr√®s complet de Philip Tellis, cr√©ateur de Log Normal (rachet√© par SOASTA) :\n\nhttps://www.lognormal.com/blog/2012/10/03/the-3.5s-dash-for-attention/\n\nBeaucoup de chiffres et d‚Äôinfos int√©ressantes autour de la WebPerf, avec notamment cette m√©trique bas√©e sur un Bounce Rate &amp;gt;= 50%\n\nLes slides sont disponibles : https://fr.slideshare.net/bluesmoon/the-35s-dash-for-user-attention-and-other-things-we-found-in-rum\n\n\n\n[Mobile] Escaping the uncanny valley, par Andrew Betts (FT Labs)\n\nSon twitter : @triblondon\n\nConf√©rence tr√®s int√©ressante par Andrew, le directeur d‚ÄôFT Labs (Financial Times), qui a parcouru l‚Äôensemble des travaux que ces √©quipes ont effectu√©s autour des capacit√©s mobile pour faire une webapp HTML5 int√©gr√©e dans une appli native avec la meilleure experience possible.\n\nAu menu, rappel sur la lourdeur de parcours du DOM en JS, les incoh√©rences du SetTimeout (notamment sur IOs), les probl√©matiques de l‚ÄôAppCache ou localStorage. Le fait d‚Äôutiliser au maximum l‚Äôacc√©l√©ration mat√©rielle sur les CSS, l‚Äôoptimisation des paint, les spinners et loading bar, le progressive rendering ‚Ä¶\n\nRappel de la latence sur les ‚Äúclics‚Äù tactiles avec le projet Fastclick, qui enl√®ve les 300ms de d√©lai sur le clic mobile : https://github.com/ftlabs/fastclick/\n\n[Mobile] Make your mobile web apps fly, par Sam Dutton (Google)\n\nSon twitter : @sw12\n\nSam Dutton, Developers Advocate (?) chez Google, nous fait un parcours assez complet des bases d‚Äôoptimisations WebPerf pour le mobile. Un peu de redondance versus d‚Äôautres confs vues plus t√¥t, mais le sujet est bien ma√Ætris√© et bien couvert.\n\nQuelques petits outils int√©ressants, notamment le Multires pour la gestion des images Retina : https://fhtr.org/multires/\n\nA noter aussi une phrase que j‚Äôai beaucoup aim√© sur l‚Äôergnonomie mobile, et la position des boutons de contr√¥le :\n\n\n  ‚ÄúControls should be beneath content: think calculator‚Äù\n\n\nLes slides : https://www.samdutton.com/velocity2012/ \n\n[DevOps] Scaling Instagram, par Mike Krieger (Instagram)\n\nSon twitter : @mikeyk\n\nPetit debrief orient√© Ops de la success story d‚ÄôInstragram. Le rythme de la pr√©sentation √©tait vraiment pushy, donc plut√¥t dur suivre pour nous autre francophones ‚Ä¶\n\nEn plus d‚Äôavoir l‚Äôimpression de voir un demi milliard bouger sur sc√®ne, nous avons quand m√™me appris certaines choses sur la Stack Instagram : EC2, Python Django, Postgres, Gearman, RabbitMQ ‚Ä¶\n\nLa pr√©sentation n‚Äô√©tant pas nouvelle, est disponible ici : https://fr.slideshare.net/iammutex/scaling-instagram\n\n\n\nMike Krieger (Source : https://royal.pingdom.com/2012/10/03/report-from-velocity-europe-day-2/ )\n\n\n\n[WebPerf] Bringing HTML5 into Nativelandia: A Tale of Caution, par Jackson Gabbard (Facebook)\n\nL‚Äôune de conf√©rences que j‚Äôattendais beaucoup, de Jackson Gabbard, Mobile Engineer chez Facebook, qui nous explique le passage du HTML5 au native pour les applications mobile (IOs pour le moment?).\n\nPass√© les stats toujours aussi hallucinantes, il explique tout ce qui n‚Äô√©tait pas convainquant sur l‚Äôancienne webApp HTML5, et que ce qui a vraiment √©chou√©, est le marriage entre la WebView et le natif. Que l‚Äôexp√©rience ‚Äúnative‚Äù est bien plus concluante pour l‚Äôutilisateur par rapport ce qu‚Äôils souhaitent obtenir niveau fluidit√©, performances, efficacit√©, utilisation r√©seau etc ‚Ä¶\n\nIls ont du coup redevelopp√© pas mal de leurs outils pour s‚Äôadapter a ce mode de fonctionnement (un GateKeeper plus light) etc ‚Ä¶\n\nConf√©rence vraiment passionnante, avec un gars plut√¥t tr√®s transparent sur Facebook et les raisons qui ont pouss√©es ce changement.\n\nConclusion :\n\nDu lourd encore une fois, avec √©normement de sujets tr√®s int√©ressants, m√™me si l‚Äôon commence √† tourner un peu en rond autour de la WebPerf Mobile.\n\nA noter aussi qu‚Äôun grand nombre de livre Oreilly ont √©t√© donn√© et d√©dicac√© par Steve Souders et John Allspaw ;-)\n\nLes salles sont d√©j√† bien plus sympa, et l‚Äôorganisation toujours au top ! Vivement demain.\n\nPour finir, voici quelques vid√©os diffus√©es pendant les breaks la V√©locity :\n\n\nHot Wheels World Record: Double Loop Dare at the 2012 X Games Los Angeles\n\n\nJeb Corliss ‚Äú Grinding The Crack‚Äù\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-3\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-1\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conf√©rence Europe 2012 : Day 1",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-1",
    "date"     : "October 4, 2012",
    "excerpt"  : "Nous voici de retour pour une V√©locity Conference, le paradis de la WebPerf et des Devops !\n\nApr√®s l‚Äôexcellente moisson de la V√©locity US de Santa Clara, dont voici nos 3 CR :\n\n\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-da...",
  "content"  : "Nous voici de retour pour une V√©locity Conference, le paradis de la WebPerf et des Devops !\n\nApr√®s l‚Äôexcellente moisson de la V√©locity US de Santa Clara, dont voici nos 3 CR :\n\n\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-2-devops-webperf\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-3-devops-webperf\n\n\nEt apr√®s la session de l‚Äôann√©e derni√®re qui avait lieu √† Berlin, nous nous retrouvons cette fois dans la capitale anglaise Londres, au Hilton h√¥tel.\n\nVoici le compte rendu des conf√©rences de la premi√®re journ√©e, journ√©e un peu √† part plac√©e sous le signe des ‚ÄúTutorials‚Äù (2 octobre 2012)\n\n[OPS] Monitoring and Observability in complex architecture, par Theo Schlossnagle (OmniTI)\n\nPremi√®re conf√©rence de la journ√©e, avec Theo, habitu√© des Velocity, et plut√¥t expert dans les domaines ‚Äúinfra‚Äù et ‚Äúmonitoring‚Äù. Cr√©ateur de Omniti, MessageSystems et Circonus.\n\nSon twitter : @postwait\n\nTheo nous explique comment monitorer et observer des archi complexes avec une pr√©sentation tr√®s bas niveau.\n\nLes outils de collectes de statistiques qu‚Äôil cite :\n\n\n  Metrics.js : https://github.com/mikejihbe/metrics\n  Resmon : https://labs.omniti.com/labs/resmon\n  Folsom : https://github.com/boundary/folsom\n  Metrics : https://metrics.codahale.com/\n  Metrics-net : https://github.com/danielcrenna/metrics-net\n  StatsD : https://github.com/etsy/statsd\n\n\nEt pour le stockage :\n\n\n  Reconnoiter : https://labs.omniti.com/labs/reconnoiter\n  Graphite : https://graphite.wikidot.com/\n  OpenTSDB : https://opentsdb.net/\n  Circonus : https://circonus.com/\n  Librato : https://metrics.librato.com/\n\n\nOnt suivi ensuite des d√©mos tcpdump assez pouss√©es plut√¥t int√©ressante :\n Par exemple, pour voir les nouvelles connexions entrantes (r√©cup√©ration des packets SYN) : tcpdump -nnq -tttt -s384 ‚Äòtcpport 80 and(tcp[13] &amp;amp; (2|16) == 2)‚Äô\n\n\n  d‚Äôexemples sont disponibles dans les slides, avec aussi d‚Äôautres exemples live sur ‚Äústrace‚Äù et ‚Äúdtrace‚Äù.\n\n\nBref, ca commence tr√®s (trop?) fort, surtout pour nous, pas forc√©ment de nature tr√®s ‚ÄúOPS‚Äù !\n\nLes slides sont disponible ici et plut√¥t parlantes pour ceux qui voudraient creuser le sujet : https://www.slideshare.net/postwait/monitoring-and-observability\n\n\n\n\n  ‚ÄúYou cannot correct what you cannot measure‚Äù Theo Schlossnagle\n\n\n\n\nTheo Schlossnagle (Source : https://img.ly/o0Ht )\n\n[DevOps] Escalading Scenario : a deep dive into outage falls, par John Allspaw (Etsy)\n\nOn prend toujours autant de plaisir √©couter John Allspaw, VP d‚ÄôEsty et Co-organisateur de la V√©locity avec Steve Souders, nous parler d‚Äôincident, et de la meilleure mani√®re de les g√©rer.\n\nSon Twitter : @allspaw\n\nBeaucoup de parall√®les sont faits avec des incidents dans l‚Äôaviation, l‚Äôindustrie, voir l‚Äôarm√©e, avec des r√©f√©rences aussi au PostMortem d‚Äôappolo 13 ‚Ä¶ Un joli condens√© de bouquins ‚ÄúMust Read‚Äù d‚Äôapr√®s Allspaw comme : Normal Accidents ou encore Naturalistic Decision Making, nous sont pr√©sent√©s.\n\nDifficile d‚Äôen faire un r√©sum√©, tellement la conf√©rence √©tait bourr√©e d‚Äôinformations, graphiques et anecdotes en tout genre ! Malgr√© tout, dans les ‚Äúchoses‚Äù retenir, en vrac :\n\n\n  Attention au contexte de vos graphiques : un graph qui parait anormal sur une heure, peut s‚Äôav√©rer normal sur une √©chelle de temps d‚Äôune journ√©e par exemple\n  Des bons conseils sur la r√©solution d‚Äôincident en √©quipe, notamment au niveau de la communication avec des exemples de conversation pendant des incidents chez Etsy trop ambigus. Il est important de confirmer les r√©ponses, corriger la communication des autres, afin d‚Äô√©viter tout soucis de compr√©hension\n  Ne pas h√©siter √† demander des ‚Äúpre-mortem‚Äù. Dire √† l‚Äôauteur du projet par exemple, que cela va planter dans plusieurs mois, et lui demander d‚Äôessayer de trouver la ou les raisons qui pourraient amener le projet au plantage.\n  ‚Ä¶\n\n\nJe vous invite encore consulter les slides pour plus d‚Äôinformations et de r√©f√©rences : https://fr.slideshare.net/jallspaw/velocity-eu-2012-escalating-scenarios-outage-handling-pitfalls\n\n\n\n\n\nJohn Allspaw (Source : https://twitter.com/lozzd/status/253074489540239360 )\n\n[WebPerf] Running a WebPerf Dashboard in 90 minutes, par Jeroen Tjepkema\n\nSon twitter : @jeroentjepkema\n\nL‚Äôobjectif de cette conf√©rence √©tait de proposer en 90 minutes, les √©tapes n√©cessaire pour monter un dashboard orient√© WebPerf.\n\nOn re-parcourt du coup un peu tout le classique de la performance web, en pr√©sentant d√©j√† quelques exemples de Dashboard (rien de tr√®s sexy ‚Ä¶, hormis peut-√™tre celui de Nrc.nl orient√© ‚Äúaudience √©ditoriale‚Äù plutot int√©ressant), la pertinence de certains types de ‚Äúgraphs‚Äù comme les ‚Äúheatmap‚Äù et aussi en comparant les diff√©rentes solutions pour mesurer la performance web, avec avantages et inconv√©nients :\n\n\n  Synthetic Monitoring (Gomez, Keynote, IpLabel, Pingdom etc) (slide 104-105)\n  Real User Monitoring (LogNormal dont l‚Äôacquisition par Soasta a aussi √©t√© annonc√© ce jour, Boomerang.js, Torbit, Google Analytics ‚Ä¶) (slide 121-122)\n  Real User Benchmarking (WebPageTest) (slide 134)\n\n\nQuelques id√©es sympa de design sont diss√©min√©es tout au long de cette longue pr√©sentation, on regrette simplement de survoler toujours un peu tous les concepts, mais malgr√©s tout, cela reste l‚Äôune des rares tentatives de faire un dashboard WebPerf accessible √† des ‚Äúnon-techniciens‚Äù. Chapeau pour cela.\n\nUne d√©mo du dashboard est testable ici : https://app.measureworks.nl/secured/dashboard (Login : demo@measureworks.nl , password: performance )\n\nLes slides sont disponibles ici : https://www.slideshare.net/MeasureWorks/measureworks-velocity-conference-europe-2012-a-web-performance-dashboard-final\n\n\n\n\n\nJeroen Tjepkema (Source : https://twitter.com/pingdom/status/253135289327951872 )\n\n[WebPerf] Deep Dive into Performance analysis, par Steve Souders et Patrick Meenan (Google)\n\nLeurs Twitter : @souders et @patmeenan\n\nDernier ‚ÄúTutorials‚Äù du jour avec Steve Souders (Chief performance Officer chez Google), et Patrick Meeman (aussi chez Google d√©sormais, cr√©ateur de Webpagetest).\n\nHistoire de s‚Äôadapter au public Anglais, les deux comp√®res ont d√©cid√©s de s‚Äôattaquer aux sites des √©quipes de Premier League du Foot Anglais :-)\n\nComme imagin√©, ca n‚Äôest pas ‚Äúfolichon‚Äù, et on √©tudiera en profondeur en live les sites de Chelsea et Tottenham, qui chacun enchaine un nombre d‚Äôaberrations plus grandes les une que les autres !\n\nLe tableur utilis√© pendant la pr√©sentation avec les liens vers les tests WebPageTest : goo.gl/YfbRn\n\nQuelques exemples sur le site de Chelsea : https://www.chelseafc.com/\n\nLes tests WPT annoncent un Load Time 21 secondes, 203 requ√™tes HTTP et 3mo4 t√©l√©charg√©s !\n\nLe Waterfall que vous pouvez voir ici, est on ne peut plus parlant, avec une mention sp√©ciale pour la liste des images pr√©sentes sur la HP : https://velocity.webpagetest.org/pageimages.php?test=120925_0_13&amp;amp;run=2&amp;amp;cached=0\n\nJ‚Äôimagine que le probl√®me va vous sauter aux yeux, entre les fonds √©normissimes et images non compr√©ss√©es, les images de chacun des joueurs de l‚Äô√©quipe (oui, le pauvre carroussel en bas de page ‚Ä¶), et le nombre incroyable de logo et ou picto en tout genre, on voit vite comment am√©liorer la page :-)\n\nLe carroussel du haut donne aussi un effet assez comique sur le ‚ÄúFilmStrip View‚Äù o√π l‚Äôont voit vers les 10 secondes, un d√©but d‚Äôimage se charger, pour s‚Äôeffacer car le carroussel passe d√©ja au panel suivant ‚Ä¶ Merci au passage au jCaroussel qui charge b√™tement toutes les images ‚Ä¶\n\nOn remarque aussi un nombre cons√©quent de JS qui retarde grandement le Start Render. Optez autant que possible pour les positionner juste avant le /body, ou les charger en async/defer ou via un chargement asynchrone en Js.\n\nPas mal de petites astuces sont partag√©es par Patrick et Steve, notamment sur l‚Äôutilisation de la courbe de Bande Passante, pour voir les parties pouvant √™tre optimis√©es (celle ou la bande passante n‚Äôest pas utilis√©e fond par exemple), on remarque aussi quelques ajouts r√©cents comme l‚Äôaffichage des √©venements Paint sur la FilmStrip View (Screenshot encadr√© de Orange), ou encore la possibilit√© via clic droit dans la Chrome Dev Tools de vider le cache et les cookies rapidement etc ‚Ä¶\n\nNous avons aussi eu la confirmation que Google prenait le Onload Time comme r√©f√©rence pour ses algorithmes de ranking.\n\nBref, superbe application de tous les concepts WebPerf avec des cas concrets d‚Äô√©tude et une conf√©rence tr√®s interactive avec suffisament d‚Äôastuces pour combler aussi les habitu√©s de WPT.\n\n\n\nPatrick Meenan &amp;amp; Steve Souders (Source : https://twitter.com/simonox/status/253146271156670464 )\n\n[DevOps] Ignite Talk\n\nPour finir cette journ√©e, rendez vous dans l‚Äôimmense salle (dans laquelle aura lieu la majorit√© des conf√©rences suivantes), pour un Ignite Talk combin√© entre la V√©locity Conf√©rence et Strata Conf√©rence qui ont lieu au m√™me moment dans l‚ÄôHilton h√¥tel Londres.\n\n\n\nSalle King‚Äôs Room (Source : https://twitter.com/cmsj/status/253139957093367808/photo/1 )\n\nNous suivrons une s√©rie de 7 ou 8 Ignite Talk dont le concept est de pr√©senter un sujet sur 20 slides d√©filant automatiquement toutes les 15 secondes. C‚Äôest assez d√©cal√©, fun, et l‚Äôexercice parait tr√®s ‚Äúsport‚Äù ‚Ä¶ :)\n\nPas mal de sujets tournaient autour de l‚ÄôOpen Data ou Big Data, les DataViz ‚Ä¶\n\nVoici par exemple un talk sympa sur les ‚ÄúDataviz as interface‚Äù par @makoto_inoue : https://fr.slideshare.net/inouemak/data-viz-asinterfacemakotoinoue\n\n\n\n\n\nMakoto Inoue, de l‚Äô√©nergie, de la danse, et de la bi√®re ! (Source: https://royal.pingdom.com/2012/10/02/velocity-europe-1/ )\n\n[WebPerf] Step by Step Mobile Optimization, par Guy Podjamy (Akamai)\n\nConf√©rence auquelle je n‚Äôai pas pu assist√© : https://fr.slideshare.net/guypod/step-by-step-mobile-optimization\n\n\n\nConclusion :\n\nBonne premi√®re journ√©e avec d√©jpas mal de choses retenir et appliquer quotidiennement !\n\nOn regrette le fait d‚Äôavoir √©t√© dans des petites salles (s√ªrement √† cause de la derni√®re journ√©e de la Strata Conf√©rence), et du coup d‚Äôavoir altern√© le manque de place, avec la chaleur des salles ‚Ä¶ Et je n‚Äôai pas l‚Äôimpression que les sessions du jour √©taient film√©es malheuresement !\n\nC‚Äôest en tout cas un tr√®s bon avant-gout de ce qui nous attend demain ;-)\n\nEnjoy !\n\nP.s : N‚Äôh√©sitez pas nous faire des retours sur ce CR ! :)\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-2\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-3\n\n"
} ,
  
  {
    "title"    : "Tentative d&#39;explication des Fast-Forward sous Git",
    "category" : "",
    "tags"     : " git",
    "url"      : "/tentative-d-explication-des-fast-forward-sous-git",
    "date"     : "July 13, 2012",
    "excerpt"  : "\n\nTous les projets M6Web sont pass√©s r√©cemment sous le syst√®me de gestion de contenu Git.\n\nGit, c‚Äôest super cool ! On peut faire facilement des branches, les ‚Äúmerger‚Äù les unes aux autres et ‚Äúswitcher‚Äù d‚Äôune branche une autre. Pratique donc (dans l...",
  "content"  : "\n\nTous les projets M6Web sont pass√©s r√©cemment sous le syst√®me de gestion de contenu Git.\n\nGit, c‚Äôest super cool ! On peut faire facilement des branches, les ‚Äúmerger‚Äù les unes aux autres et ‚Äúswitcher‚Äù d‚Äôune branche une autre. Pratique donc (dans l‚Äôid√©e) !\n\nIl a √©t√© finalement assez facile de se faire au vocabulaire et au fonctionnement de git. Je ne dis pas que je ne fais pas non plus mes commit sur la bonne branche chaque fois, mais on arrive tout de m√™me assez facilement √† s‚Äôen sortir (‚Äúgit reset ‚Äìhelp‚Äù si vous √™tes dans ce cas) !\n\nLe tr√®s int√©ressant article ‚ÄúA successful Git branching model‚Äù a mis en avant la gestion des fast-forward, cela dit l‚Äôutilit√© m‚Äôest rest√© assez floue et ne couvrait pas l‚Äôensemble de mes questions.\n\nJ‚Äôai donc fouill√© la documentation de Git afin de d√©broussailler les ‚Äúfast-forward‚Äù.\n\nQue fait git lors d‚Äôun merge\n\nGit ‚Äúmerge‚Äù deux branches lorsque :\n\n\n  La commande ‚Äúmerge‚Äù est utilis√©e, par exemple git merge feature-myfeature,\n  la commande pull est utilis√©e, exemple git pull origin master\n\n\nGit ‚Äúfast-forward‚Äù\n\nMettons que Bob fasse une modification sur une branche, il cr√©e un commit Y.\n\nIl fait une autre modification qu‚Äôil commit, il cr√©e alors un commit Z.\n\nAutomatiquement git ‚Äúfast-forward‚Äù, c‚Äôest dire qu‚Äôil fait pointer la branche, qui pointait sur le commit Y, vers le commit Z. Sur les graphiques de git log, les deux commit sont li√©s par un trait continu.\n\n- Y - Z\n\nTant que Bob continue faire des modifications + commit sans toucher au fast-forward, git va automatiquement ‚Äúfast-forwarder‚Äù. On aura donc un encha√Ænement de commit qui sont li√©s par un trait continu.\n\n- Y - Z - AA - AB - AC - ...\n\nGit ne ‚Äúfast-forward‚Äù pas\n\nNous sommes maintenant sur une branche qui en est la r√©vision X.\n\nAlice travaille sur son projet et cr√©e la r√©vision A.\n\nCela dit, Bob travaille aussi sur le projet et cr√©e la r√©vision B.\n\nAlice pousse ses modifications :\n\nLe commit A a pour parent le commit X, qui est le dernier commit connu par la branche, Git peut donc ‚Äúfast-forwarder‚Äù.\n\n- X - A\n\nBob pousse ses modifications :\n\nLe commit de bob ne conna√Æt pas de commit A dans son historique (son commit parent est le commit X).\n\n\n- X - A\n   \\\n    B\n\nSi git ‚Äúfast-forwardait‚Äù ici, il ferait pointer la branche sur le commit B, et perdrait le commit A. Comme on ne souhaite pas perdre les modifications d‚ÄôAlice, il va donc passer en mode ‚Äúno fast-forward‚Äù automatiquement.\n\nGit va donc r√©cup√©rer les modifications de A et les m√©langer (merge) aux modifications de B en cr√©ant un commit C.\n\nLe commit C a pour parent les commit** B** et** A, le pointeur de dernier commit peut donc √™tre plac√© sur **C sans risque de perte d‚Äôhistorique.\n\n- X - A\n   \\   \\\n    B - C\n\nOption* ‚Äìno-ff*\n\nL‚Äôauteur de l‚Äôarticle cit√© pr√©c√©demment conseille d‚Äôutiliser l‚Äôoption ‚Äìno-ff sur les merge.\n\nCette option force git a cr√©er un commit de ‚Äúmerge‚Äù qui aura pour parents notre commit de modification, et le dernier commit connu sur la branche, m√™me si il n‚Äôy a pas eu de modification sur cette derni√®re.\n\nCela permet de revenir facilement √† la version ant√©rieure de la branche, sans avoir √† fouiller dans les nombreux commits ayant pu amener un bug : on revient √† la version initiale avant de passer plus de temps pour corriger le bug.\n\n- M1 -- -- -- -- M2\n    \\            /\n     B1 - B2 - B3\n\nDans l‚Äôexemple ci-dessus, on peut facilement revenir au commit M1, et exclure ainsi toute la branche B. Si on avait ‚Äúfast-forward√©‚Äù, il nous aurait fallut retrouver le commit M1 en regardant tous les commit pr√©c√©dent.\n\nMode auto contre ‚Äìno-ff\n\nForcer le *‚Äìno-ff *ne sera finalement utile que lorsque vous d√©veloppez une fonctionnalit√© pour laquelle vous allez beaucoup ‚Äúcommiter‚Äù, sans que personne d‚Äôautre ne commit entre-temps.\n\nA vous de l‚Äôutiliser de mani√®re intelligente !\n\nSources\n\n\n  nvie.com : A successful Git Branching Model\n  git push ‚Äìhelp\n\n\n"
} ,
  
  {
    "title"    : "Retrouvez l&#39;intervention du CTO de M6 Web, Martin Boronski, √† la table ronde du Forum PHP 2012",
    "category" : "",
    "tags"     : " forumphp, afup, video, php",
    "url"      : "/retrouvez-l-intervention-du-cto-de-m6-web-martin-boronski-a-la-table-ronde-du-forum-php-2012",
    "date"     : "July 11, 2012",
    "excerpt"  : "\n\n",
  "content"  : "\n\n"
} ,
  
  {
    "title"    : "Int√©gration continue avec Jenkins et Atoum",
    "category" : "",
    "tags"     : " php, atoum, jenkins, ci",
    "url"      : "/integration-continue-avec-jenkins-et-atoum",
    "date"     : "July 11, 2012",
    "excerpt"  : "\n\nChez M6 Web nous tentons de cr√©er une approche open-source intra entreprise. L‚Äôobjectif est que certains composants g√©n√©riques adapt√©s notre m√©tier puissent √™tre cr√©es et diffus√©s largement parmis les dizaines de projets g√©r√©s chaque ann√©e. Un p...",
  "content"  : "\n\nChez M6 Web nous tentons de cr√©er une approche open-source intra entreprise. L‚Äôobjectif est que certains composants g√©n√©riques adapt√©s notre m√©tier puissent √™tre cr√©es et diffus√©s largement parmis les dizaines de projets g√©r√©s chaque ann√©e. Un prochain post traitera de cette probl√©matique.\n\nDans cette optique, il faut nous assurer de la qualit√© et la non r√©gr√©ssion de ces composants. Pour cela nous avons mis en place Jenkins afin d‚Äôassurer l‚Äôint√©gration continue de nos tests unitaires. Voici un exemple d‚Äôint√©gration avec Atoum (ce n‚Äôest pas forcement la meilleur m√©thode, n‚Äôh√©sitez pas √† la commenter).\n\nStructure du composant :\n\n\n  ./src contient les classes du composants au format PSR-0\n  ./tests contient les TU Atoum\n  ./build-tools/jenkins contient les fichiers de configuration pour Atoum et Ant\n  ./vendor contient les d√©pendances externes du projet (g√©r√©es avec Composer)\n\n\nVoici le composer.json utilis√©.\n\n\n\nVoici le fichier de configuration de Atoum : build-tools/jenkins/atoum.ci.php et celui de jenkins build-tools/jenkins/build.xml\n\n\n\n(cette configuration inclut l‚Äôensemble des outils d‚Äôanalyse statique que l‚Äôon utilise)\n\nEnfin, voici la configuration faire sur Jenkins (en image).\n\n\n\n\n\n\n\nVia cette conf on obtient le r√©sultat des tests (naturellement) ainsi que la couverture de code des tests avec la coloration des lignes couvertes ou non couvertes dans les classes test√©es.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 3 (DevOps/WebPerf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops, mobile",
    "url"      : "/cr-velocity-conference-2012-day-3-devops-webperf",
    "date"     : "June 28, 2012",
    "excerpt"  : "Derni√®re journ√©e de cette monstrueuse conf√©rence qu‚Äôest la V√©locity Conf√©rence.\n\nOn commence dans la joie et la bonne humeur avec la ‚ÄúSeven Databases Song‚Äù :D\n\n\n\n[Mobile WebPerf] The Performance of Web vs. Apps, par Ben Galbraith (Walmart.com) &amp;am...",
  "content"  : "Derni√®re journ√©e de cette monstrueuse conf√©rence qu‚Äôest la V√©locity Conf√©rence.\n\nOn commence dans la joie et la bonne humeur avec la ‚ÄúSeven Databases Song‚Äù :D\n\n\n\n[Mobile WebPerf] The Performance of Web vs. Apps, par Ben Galbraith (Walmart.com) &amp;amp; Dion Almaer (Walmart.com)\n\n\n\nPetit sujet assez trollesque sur les WebApps vs Apps. Conf√©rence hyper √©n√©rg√©tique et tr√®s dr√¥le ! Notamment le passage √† 12mn dans la vid√©o, o√π l‚Äôon compare le mode de distribution des apps natives √† ce que cela donnerait si les show tv devraient √™tre distribu√©s de la m√™me mani√®re en prenant l‚Äôexemple de la s√©rie Friends : Hilarant !\n\nL‚Äôid√©e int√©ressante sur la fin du talk, concerne le rendu de l‚Äôapplication, qui gr√¢ce Node.js (dispo d√©sormais en v0.8.0 enfin) peut √™tre aussi bien fait cot√© client que serveur suivant le client qui demande. A creuser.\n\n\n\n[WebPerf] Akamai Internet Insights, Stephen Ludin (Akamai)\n\n\n\nPetit talk de Stephen Ludin ‚ÄúChief Architect for Akamai‚Äôs Site Acceleration and Security group‚Äù.\n\nApr√®s une pr√©sentation assez hallucinante en quelques chiffres du traffic et des donn√©es qui passent chez Akamai :\n\nToutes les 60 secondes =&amp;gt; 1 milliard 3 de logs, + de 6200 heures de vid√©os stream√©s ‚Ä¶\n\nIl a aussi partag√© une initiative louable et tr√®s int√©ressante sur un projet de partage des donn√©es r√©colt√©es chez Akamai : https://www.akamai.com/io\n\nOn y observe quelques statistiques (relativement peu date) sur les browsers notamment. On voit d‚Äôailleurs quelque chose d‚Äôassez fun sur les IE8 : chaque weekend, on apercoit une baisse de pr√©sence sur IE8 (qui se retrouve sur d‚Äôautres navigateurs plus r√©cent) ‚Ä¶ Bref, on voit encore que c‚Äôest le monde de l‚Äôentreprise qui ralenti la propagation des navigateurs r√©cents !\n\nSource : \nhttps://www.akamai.com/html/io/io_dataset.html#stat=browser_ver&amp;amp;top=5&amp;amp;type=line&amp;amp;start=20120601&amp;amp;end=20120626&amp;amp;net=n\n\nEt slides ici : \nhttps://assets.en.oreilly.com/1/event/79/Akamai%20Internet%20Insights%20%20Presentation.pptx\n\nLightning Demos, par Marcel Duran (Twitter Inc.), Nat Duca (Google), Lindsey Simon (Twist)\n\n\n\nEnsuite, viennent trois sessions de Lightning Talk : 5 minutes pour pr√©senter un sujet.\n\nOn commence par Marcel Duran, cr√©ateur de Yslow, c√©l√®bre extension WebPerf de Firebug √† l‚Äôorigine, qui fait son petit bonhomme de chemin depuis :\n\n\n  Disponible dans quasiment tous les browsers\n  Ruleset personnalisable (cf C3PO voir plus bas)\n  Une version en ligne de commande (en Node.Js) pour extraire les donn√©es YSlow partir d‚Äôun HAR : https://github.com/marcelduran/yslow/wiki/Command-Line-%28HAR%29\n  Un serveur Node.js que vous pouvez tester ici n√©cessitant aussi un HAR : https://yslow.nodester.com/\n  et le meilleur pour la fin, une version pour Phantom.Js (Projet tr√®s impressionnant d‚ÄôHeadless Browser) qui vous permet de simplement mentionner l‚Äôurl et d‚Äôavoir le r√©sult√¢t en sortie ! Avec en plus la possibilit√© via le format TAP (Test Any Protocol), d‚Äôint√©grer les r√©sultats dans votre Int√©gration Continue pour √©viter les r√©gressions. Juste ultime, tout est expliqu√© sur ce Github : https://github.com/marcelduran/yslow/wiki/PhantomJS\n\n\nJ‚Äôai h√¢te d‚Äôimpl√©menter tout ca chez M6Web :) Une vid√©o √† voir donc absolument :\n\n\n\n\n\nOn continue dans le lourd, avec Nat Duca qui travaille sur le d√©veloppement du navigateur Chrome et qui nous d√©montrer une feature tr√®s bas niveau mais au combien int√©ressante : le chrome://tracing/\n\nCette fonctionnalit√© va vous permettre de profiler les actions du navigateur au plus bas niveau possible. Encore un excellent nouvel ajout au niveau du panel d‚Äôoutillage du browser Chrome destination des d√©veloppeurs. Voir vid√©o ci dessous :\n\n\n\n\n\nEt pour finir cette jolie session de Lightning Talk, Lindsey Simon, nous pr√©sent√© Browserscope : https://www.browserscope.org/\n\nOutil dont la puissance et l‚Äôinter√™t pour tout d√©veloppeurs Front-end Desktop ou Mobile n‚Äôest plus d√©montrer.\n\nSi vous ne connaissez pas, passez 5 minutes de votre temps sur cette vid√©o :\n\n\n\n[WebPerf] Browsers, par Luz Caballero (Opera Software), Tony Gentilcore (Google), Taras Glek (Mozilla Corporation)\n\nPetite d√©ception sur cette classique des V√©locity, o√π les talks ce sont plut√¥t concentr√© sur les nouveaut√©s des browsers mobile de Google et Op√©ra Mini, et o√π le gars de Mozilla n‚Äôa pas jouer le jeu et pr√©f√©r√© parler de la lenteur du SetTimeout Javascript ainsi que de l‚Äôapi LocalStorage ‚Ä¶\n\nSlide Mozilla : \nhttps://people.mozilla.com/~tglek/velocity2012/#/step-1\n\nSlide Op√©ra mini avec notes : \nhttps://www.slideshare.net/gerbille/speed-in-the-opera-mobile-browsers-13476236\n\nConcernant Google, la conf√©rence par Tony Gentilcore (cr√©ateur de FasterFox pour ceux qui ce souviennent) √©tait plus int√©ressante, d√©ja par l‚Äôannonce suivante :\n\n\n  Chrome for Android will be the default browser starting with Jelly Bean\n\n\nTony Gentilcore\n\nIl a aussi parl√© du fonctionnement de WebKit, du Compositor Thread, ainsi que du Chrome Remote Debugging\n\nPour info, Google a peu de temps apr√®s annonc√© la pr√©sence de Chrome sur iOs !\n\n[DevOps] Simple log analysis and trending, par Mike Brittain (Etsy)\n\n\n\nOn retrouve Mike sur un sujet un peu diff√©rent : Comment analyser des logs Apache pour en sortir des graphites. Quelques astuces sur la fonction PHP apache_note() sont mentionn√©es, sur le traitement des logs avec les commandes linux ‚Äúawk‚Äù et ‚Äúsed‚Äù, et l‚Äôutilisation assez √©tonnante de Gnuplot pour grapher : https://www.gnuplot.info/ !\n\nLes slides sont dispos ici : https://www.mikebrittain.com/blog/2012/06/22/velocity-2012/ , et les codes d‚Äôexemples sur Github : https://github.com/mikebrittain/presents\n\nEncore pas mal d‚Äôid√©es piocher ! (Ca commence faire beaucoup d‚Äôid√©es ‚Ä¶)\n\n\n\n[WebPerf] Social Button BFFs, par Stoyan Stefanov (Facebook)\n\nStoyan n‚Äôest plus pr√©senter dans l‚Äôindustrie des performances web. Il est d√©sormais chez Facebook, travailler sur les performances des plugins, dont le ‚ÄúLike‚Äù ! Suivez le sur Twitter, c‚Äôest bourr√© de superbes infos @stoyanstefanov ainsi que son blog : https://www.phpied.com/ !\n\nL‚Äôid√©e du talk est de faire en sorte que les boutons sociaux (et widgets tiers) en g√©n√©ral, deviennent vos BFF ! (Best Friend Forever :D) : https://www.phpied.com/social-button-bffs/\n\nIl explique de quel mani√®re doit-on int√©grer ces widgets sur nos sites, et vous permet de le v√©rifier par l‚Äôextension Chrome qu‚Äôil a d√©velopp√© 3PO#Fail (3PO = 3rd Party Optimization) ou via une extension de RuleSet pour YSlow.\n\nLes slides ici : https://www.slideshare.net/stoyan/social-button-bffs\n\n\n  ‚ÄúFriends don‚Äôt let friends do document.write‚Äù Stoyan Stefanov\n\n\n[WebPerf] 5 Essential Tools for UI Performance, par Nicole Sullivan (Stubbornella)\n\nEncore un excellent talk pour ce dernier jour avec Nicolas Sullivan, Experte et consultante dans l‚Äôoptimisation CSS, sur le fonctionnement tr√®s pr√©cis de la gestion des CSS par vos navigateurs et toutes les optimisations r√©centes qu‚Äôils y ont apport√©es, ainsi qu‚Äôune d√©mo (qui fait toujours son petit effet dans une salle Geek) de Tilt sur Firefox\n\nLes slides ne sont malheuresement pas encore en ligne, mais cela ne devrait tarder sur son Slideshare.\n\nVous pouvez retrouvez l‚Äôid√©e du talk sur l‚Äôinterview ci dessous r√©alis√©e elle aussi lors de la V√©locity.\n\n\n\nConclusion\n\nVoil, c‚Äôest termin√© pour ce compte rendu en 3 actes de ce que j‚Äôai v√©cu et retenu cette V√©locity Conf√©rence 2012. Fran√ßois prendra le relais pour pr√©senter sa vision d‚Äôautres talks, mais orient√©s Ops (Sysadmin).\n\nJ‚Äôesp√®re que ces comptes rendu auront servi √† partager quelques outils, liens ou best practices qui vous donnerons des tonnes d‚Äôid√©es de nouvelles choses √† faire cot√© Web dans votre soci√©t√©. De mon cot√©, comme la V√©locity Berlin l‚Äôann√©e derni√®re, j‚Äôai appris beaucoup et appr√©ci√© une grande partie des conf√©rences. Cette conf reste pour moi (et nous chez M6Web) la plus importante au monde sur les aspects de Performance.\n\nPour finir, je vous remercie pour vos retours (et je vous invite √† continuer m‚Äôen faire un maximum) et lectures. Vous pouvez en connaitre d‚Äôavantage sur les autres talks avec quelques vid√©os gratuite disponible sur \nhttps://www.youtube.com/playlist?list=PL80D9129677893FD8, Ainsi que les slides qui continuent d‚Äôarriver sur\nhttps://velocityconf.com/velocity2012/public/schedule/proceedings\n\nEt pour ceux que ca int√©resse, sachez que Oreilly mettra disposition un pack complet des vid√©os pour g√©n√©ralement un tarif autour des 400$, et qu‚Äôune V√©locity Europe aura lieu Londres les 3 et 4 octobre 2012.\n\nMerci tous !\n\n\n  CR V√©locity Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n  CR V√©locity Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-2-devops-webperf\n\n\n(Cr√©dit photo : https://www.flickr.com/photos/oreillyconf/sets/72157630300659948/)\n\n\nLe Job Board assez hallucinant !\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 2 (DevOps/WebPerf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops, mobile",
    "url"      : "/cr-velocity-conference-2012-day-2-devops-webperf",
    "date"     : "June 28, 2012",
    "excerpt"  : "Compte rendu des tracks DevOps/WebPerf de cette deuxi√®me journ√©e de cette V√©locity Conf√©rence √† Santa Clara (Californie) qui marque l‚Äôouverture ‚Äúofficielle‚Äù de la conf√©rence, la veille √©tant consid√©r√©e comme des conf√©rences bonus orient√©es Tutoria...",
  "content"  : "Compte rendu des tracks DevOps/WebPerf de cette deuxi√®me journ√©e de cette V√©locity Conf√©rence √† Santa Clara (Californie) qui marque l‚Äôouverture ‚Äúofficielle‚Äù de la conf√©rence, la veille √©tant consid√©r√©e comme des conf√©rences bonus orient√©es Tutoriaux.\n\nLa matin√©e offrait un track unique dans une salle gigantesque.\n\n\n\nVid√©o d‚Äôintro √† la V√©locity !\n\n\n\nL‚Äôouverture officielle est donc pr√©sent√©e par Steve Souders (Google) et John Allspaw (Etsy), toujours dans un show l‚Äôam√©ricaine, et m√™me d√©guis√©s. \n\n\nS‚Äôenchaine ensuite un condens√© de session plut√¥t courte par des acteurs tr√®s prestigieux du web.\n\n(Cr√©dit photo : https://instagr.am/p/MV5xAAJLSt/ )\n\n[DevOps] Building for a billion Users, par Jay Parikh (Facebook)\n\nPremi√®re conf√©rence du matin, avec une pr√©sentation du ‚ÄúVP of Infrastructure Engineering at Facebook‚Äù.\n\nOn suis avec attention, une pr√©sentation tr√®s dense de l‚Äôinfrastructure de Facebook, avec quelques chiffres hors normes.\n\nLa philosophie Facebook est pr√©sent√©e en 4 points :\n\n\n  Focus on Impact\n  Move Fast\n  Be Bold\n  Be Open\n\n\nAvec une explication sur les fameux Bootcamp cher Facebook, formation obligatoire auquelle tout le monde participe en rentrant chez Facebook.\n\nUne pr√©sentation tr√®s br√®ve des outils internes utilis√©s et d√©velopp√©s par Facebook : Perflab, GateKeeper (sorte de Feature Flipping), Claspin, Tasks, SevManager ‚Ä¶\n\nUne explication sur les proc√©dures de d√©ploiement chez Facebook et leur gestion du cache, sur leur volont√© de constamment tout refaire, pour toujours √™tre meilleur.\n\n\nEt pour finir, une anecdote assez dr√¥le sur un incident ayant eu lieu chez Facebook, o√π toutes les fonctionnalit√©s ‚Äúnon termin√©es‚Äù se sont un jour retrouv√©es en production.\n\nBref, une conf√©rence int√©ressante, mais tr√®s dense, dont je vous conseille de regarder la vid√©o √ßi dessous.\n\n\n  ‚ÄúFix More, Whine less.‚Äù Jay Parikh\n\n\n\n\n[DevOps] Investigating Anomalies, par John Rauser (Amazon)\n\nBelle surprise de la journ√©e, avec cette conf√©rence sur la gestion d‚Äôincident, qui raconte l‚Äôhistoire de l‚Äô√©pid√©mie de Cholera ayant eu lieu √† Londres en 1854, et comment John Snow, trouver l‚Äôorigine de l‚Äô√©pid√©mie, en se concentrant sur les donn√©es, et non pas seulement sur les chiffres.\n\n\n  ‚ÄúExplaining anomalies often makes your theroy bulletproof‚Äù John Rauser\n\n\nUne deuxi√®me partie √©tait concentr√©e sur le fait d‚Äô√©tudier les extremit√©s sur vos √©chantillons de mani√®re √† trouver ce qui n‚Äôallait pas. Point de vue tr√®s instructif.\n\nLa vid√©o ci dessus est un ‚Äúmust-see‚Äù de la V√©locity.\n\n\n  ‚ÄúLook at the extremes and you‚Äôll find things that are broken‚Äù John Rauser\n\n\n\n\n[DevOps] Building Resilient User Experiences, par Mike Brittain (Etsy)\n\nLe message autour de cette pr√©sentation, est que votre application DOIT s‚Äôadapter aux incidents. Si possible faire en sorte que cela ne soit m√™me pas percu par la plupart de vos internautes. En d√©coupant chacune des fonctionnalit√©s de votre site, vous devez pouvoir ne pas afficher celle qui ne fonctionne pas correctement sans que cela impact vos utilisateurs (Graceful Degradation).\n\nLes slides sont disponible ici : https://www.slideshare.net/mikebrittain/building-resilient-user-experiences-13461063\n\n\n\n[WebPerf] Predicting User Activity to Make the Web Fast, par Arvind Jain (Google), Dominic Hamon (Google)\n\nLa pr√©sentation commence avec un rappel sur ‚ÄúHow Fast is the web today‚Äù. \n En quelques chiffres :\n\n\n  Chrome ~2.3s/5.4s page load time (median/mean)\n  Google Analytics ~2.9s/6.9s page load time (median/mean)\n  Mobile ~4.3s/10.2s page load time (median/mean)\n\n\nD‚Äôautres infos sont partag√©es venant du tr√®s utile HttpArchive ‚Ä¶\n\nOn assiste ensuite la pr√©sentation des fonctionnalit√©s de ‚ÄúPrefetch‚Äù de google et du Prerendering en place dans la barre de recherche de Chrome : ‚ÄúOmnibox‚Äù, ceci ayant pour but de rendre le web encore plus rapide.\n\nTout cela donne des id√©√©es sur la fa√ßon de pr√©dire ce que vont faire les internautes, et sur nos gestions d‚Äô‚Äúautocomplete‚Äù.\n\n\n\n\n\n[WebPerf] Performance Implications of Responsive Web Design, par Jason Grigsby (Cloud Four)\n\nUne autre conf√©rence que j‚Äôattendais grandement, sur le Responsive Web Design. Le sujet est parfaitement maitris√©, et tous les reproches que je peux faire cette techno en ce moment, sont mentionn√©s, expliqu√©s, et certaines solutions ou id√©es sont donn√©es ! Du tout bon.\n\nA retenir, la m√©thode conseill√©e qui est de faire du Mobile First Responsive Web design, c‚Äôest dire commencer par la version mobile, puis faire la version web, et non l‚Äôinverse.\n\nLes slides ici : https://speakerdeck.com/u/grigs/p/performance-implications-of-responsive-design\n\nLa conf√©rence n‚Äô√©tant pas disponible en vid√©o, vous pouvez d√©j√©couter Jason Grisby lors d‚Äôune interview suite sa conf√©rence en vid√©o si dessous.\n\n(Cr√©dit photo : https://www.flickr.com/photos/stuart-dootson/4024407198/ )\n\n\nJason Grigsby interview la V√©locity Conf 2012\n\n\n\n[WebPerf] RUM for Breakfast - Distilling Insights From the Noise, par Buddy Brewer (LogNormal), Philip Tellis (LogNormal, Inc) &amp;amp; Carlos Bueno (Facebook)\n\nRUM aka Real User Monitoring est un terme qui est revenu tr√®s r√©guli√®rement durant cette Velocity. Nous avions pour cette conf√©rence notamment, deux personnes de LogNormal dont le cr√©ateur de Boomerang.js : https://yahoo.github.com/boomerang/doc/ et Carlos Bueno de Facebook.\n\nLa pr√©sentation expliquait comment mesurer des m√©triques de performances venant d‚Äôutilisateurs r√©el, comment analyser toutes les donn√©es, en filtrer le ‚Äúbruit‚Äù, et quoi en tirer. Le tout √©tait tr√®s instructif, surtout sur la partie filtrage de donn√©es (Band Pass Filtering, IQR Filtering ..).\n\nSlides : https://www.slideshare.net/buddybrewer/rum-for-breakfast-distilling-insights-from-the-noise\n\n\n\n[WebPerf] Rendering Slow? Too Much CSS3? Ask RSlow, par Marcel Duran (Twitter Inc.), David Calhoun (CBS Interactive)\n\nOn retrouve ici une pr√©sentation assez fun du r√©sum√© de la conf sous forme de Waterfall (voir photo).\n\nLe talk a ensuite abord√© les notions de rendering au niveau CSS avec au d√©part un cas d‚Äô√©tude : R√©aliser le logo de ySlow en CSS3 enti√®rement. On observe de mani√®re assez dr√¥le le rendu finale dans les diff√©rents navigateurs (√©clat de rire g√©n√©ral sur IE off course). Vous pouvez les retrouver sur les slides ci dessous.\n\nLa conf√©rence part ensuite sur quelques tests r√©alis√©s sur Chrome uniquement (√† prendre donc avec des pincettes) sur les performances CSS3 de chacun de ces cas :\n\n\n  background-image vs css3 gradient\n   vs css background-image\n  @font-face vs  vs sans-serif\n  animated gif vs css3 spinner\n\n\nL‚Äô√©tude est int√©ressante, et m√©riterais d‚Äô√™tre creus√©e sur d‚Äôautres navigateurs, mais cela est rendu tr√®s difficile par le fait que seul Chrome sait exporter les donn√©es de rendu de sa Timeline ‚Ä¶\n\nLes slides sont dispo ici : \nhttps://docs.google.com/presentation/d/1b7rdeXYdmL3lmT24GAaC14eOSkq5qt6FM-yLSeFykQk/edit?pli=1\n\n[WebPerf] Time To First Tweet, par Dan Webb, par (Twitter Inc) &amp;amp; Rob Sayre (Twitter)\n\nDan et Rob nous parle performances cot√© client chez Twitter, et la r√©ecriture du Front-End.\n\nLa notion de Time To First Tweet, correspond au temps de navigation jusqu‚Äôa l‚Äôaffichage du premier twiit sur la Timeline. Cette mesure est prise grace √† la Navigation Timing Api, support√©e dans IE&amp;gt;=9, Firefox &amp;amp; Chrome notamment.\n\nTwitter √† aussi abandonn√© progressivement, l‚Äôutilisation des hashbangs (les #! dans l‚Äôurl), pour utiliser la PushState Api, ainsi que le templating cot√© client (Mustache.js et Hogan.js) pour repasser sur un templating serveur avec leur migration de Ruby vers Java, avec au final 75% de temps client gagn√© sur le 95th Percentile !\n\nConf√©rence tr√®s int√©ressante, notamment sur la mani√®re de charger les Javascripts.\n\nPlus de d√©tail sur le blog technique de Twitter : https://engineering.twitter.com/2012/05/improving-performance-on-twittercom.html\n\nLes slides sont disponible : https://speakerdeck.com/u/danwrong/p/time-to-first-tweet\n\nConclusion Day 2 :\n\nEncore une journ√©e riche en informations et id√©es. Le rythme √©tant beaucoup plus soutenu, et la fatigue s‚Äôaccumulant, il n‚Äô√©tait pas √©vident d‚Äô√™tre √† 100% dans chaque talk :-)\n\nEn attendant le CR Ops, et celui du Day 3, vous pouvez relire le CR du Day 1 : \nhttps://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n\nP.S : Retrouvez moi sur twitter : @kenny_dee\n\nPlaylist Youtube Velocity US 2012\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 1 (Dev/Webperf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, mobile",
    "url"      : "/cr-velocity-conference-day-1-dev-webperf",
    "date"     : "June 27, 2012",
    "excerpt"  : "Nous voici √† Santa Clara, CA, ce lundi 25 juin pour notre premi√®re Velocity Conference (Web Performance &amp;amp; Operations) aux states (nous avions d√©j√† pu avoir un aper√ßu l‚Äôann√©e derni√®re avec la premi√®re Velocity Europe Berlin).\n\n\n\nL‚Äô√©venement se ...",
  "content"  : "Nous voici √† Santa Clara, CA, ce lundi 25 juin pour notre premi√®re Velocity Conference (Web Performance &amp;amp; Operations) aux states (nous avions d√©j√† pu avoir un aper√ßu l‚Äôann√©e derni√®re avec la premi√®re Velocity Europe Berlin).\n\n\n\nL‚Äô√©venement se situe au Convention Center, et la premi√®re chose que nous remarquons, c‚Äôest la taille d√©mesur√©e du lieu ! Et pour cause, 800 personnes sont attendues !\n\nPour cette premi√®re journ√©e sous le signe des tutoriaux, il y avait entre trois et quatre tracks parall√®les de 90 minutes chacun, dont un r√©serv√© aux sponsors. Dur de faire des choix parmi toutes les confs et le programme all√©chant de la journ√©e !\n\nJe m‚Äôoriente donc sur le cot√© D√©v / WebPerf / Monitoring, pendant que mon coll√®gue, Francois, part sur le cot√© operations qu‚Äôil couvrira dans une autre s√©rie de CR.\n\n[WebPerf] Understanding and Optimizing Web Performance Metrics, par Bryan McQuade de chez Google\n\nAu menu :\n\n\n  Explication des m√©triques de performance orient√©es r√©seau\n  Fonctionnement du parser HTML5\n  Explication des m√©triques de performance orient√©es rendu\n  D√©mo de Critical Path Explorer (PageSpeed Online)\n  Optimisation de l‚Äôaffichage per√ßu utilisateur\n\n\nLes slides parlent d‚Äôelles m√™me et sont disponibles ici : \nhttps://perf-metrics-velocity2012.appspot.com .\n\nElles parcourent l‚Äôint√©gralit√© des notions de WebPerf existantes ‚Äú√† jour‚Äù, dont certaines peu connues comme la Speculative loading, et permettent surtout de comprendre ce qu‚Äôelles signifient tr√®s pr√©cis√©ment.\n\nA retenir aussi le SSL Server Test ici https://www.ssllabs.com/ssltest/\n\nC‚Äôest donc un Must Read pour tout ceux que la WebPerf int√©resse.\n\n**Page Speed Insights : **\n\nNous avons eu droit ensuite une d√©mo tr√®s int√©ressante de la fonctionnalit√© Critical Path Explorer (d√©j√† entraper√ßue en version b√©ta √† la Velocity Europe), et qui sera je pense lanc√©e officiellement demain.\n\nEn attendant et pour la tester : \nhttps://developers.google.com/speed/pagespeed/insights?velocity=1\n\nCette fonctionnalit√© permet, comme son nom l‚Äôindique, de montrer le chemin critique de votre page. Sur les quelques tests que j‚Äôai pu effectu√©s, c‚Äôest tr√®s efficace. On appr√©cie le d√©tail au niveau du waterfall sur l‚Äôex√©cution des javascripts, l‚Äôaffichage de ‚Äúqui bloque quoi‚Äù, ou le render css. A approfondir de toute urgence !\n\n\n\nBryan McQuade (Google)\n\n\nLa Lightning D√©mo de Page Speed ayant eu lieu le lendemain\n\n[WebPerf] A Web Perf Dashboard: Up &amp;amp; Running in 90 Minutes, par Cliff Crocker et Aaron Kulick.\n\nL‚Äôid√©e ici √©tait de montrer en 90 minutes avec quels outils obtenir un dashboard orient√© WebPerf, qui sera fourni comme une VM √† la fin de la session.\n\nApr√®s une longue pr√©sentation orale d‚Äôoutils plut√¥t connus d√©sormais comme :\n\n\n  Boomerang.Js\n  WebPageTest instance priv√©e + API\n  Piwik (clone de Google Analytics)\n  StatsD (collecteur pour Graphite)\n  Graphite\n  REDbot.org\n  cUrl\n  ShowSlow\n  ‚Ä¶\n\n\nLes deux conf√©renciers nous pr√©sentent un site r√©alis√© pour l‚Äôoccasion : ‚ÄúSally Squirrel‚Äôs Dance Emporium‚Äù, hommage aux gifs anim√©s d‚Äô√©cureuils dansant, et nous font une d√©mo (un peu capricieuse) de leur dashboard bas√© sur Piwik, alternative Google Analytics avec un syst√®me de ‚Äúplugin‚Äù visiblement pour aggr√©ger un peu le tout.\n\nL‚Äôid√©e est clairement bonne, le r√©sultat ne m‚Äôa pas convaincu titre personnel. On vante au d√©part de la pr√©sentation, le faite qu‚Äôune image bien choisie suffit au monitoring, et qu‚Äôun dashboard ne doit pas √™tre complexe, et au final, on se retrouve avec un dashboard remplis d‚Äôimages en tout genre, de donn√©es tabulaires, ‚Ä¶ complexe quoi ‚Ä¶\n\nPiwik en alternative Analytics ?\n\nJe doute aussi de la robustesse de Piwik que nous avions d√©j√† √©tudier, et voir des pages listant les temps de latence ou de chargement utilisateur par utilisateur, me fait r√©ellement peur avec une audience d√©passant la centaine de personnes la journ√©e ‚Ä¶\n\nPour l‚Äôanecdote, sur le wiki de Piwik, on lit cette phrase que je vous laisse appr√©cier : ‚ÄúIf your website has more than a few hundreds visits per day (bravo!), waiting for Piwik to process your data may take a few minutes‚Äù\n\nJe vous invite tout de m√™me √† lire les slides : \n[https://assets.en.oreilly.com/1/event/79/A%20Web%20Perf%20Dashboard_%20%20Up%20%20Running%20in%2090%20Minutes%20Presentation.pptx](https://assets.en.oreilly.com/1/event/79/A%20Web%20Perf%20Dashboard%20%20Up%20_%20Running%20in%2090%20Minutes%20Presentation.pptx)\n\nAinsi qu‚Äô√† tester la VM mise √† disposition, car le travail derri√®re est cons√©quent, et peut correspondre √† certains, ou peut au moins donner des id√©es pour d‚Äôautres : https://t.co/uLv1fX1A\n\n**A retenir : **\n\nA retenir aussi dans cette pr√©sentation, tout le bien qui a √©t√© dit de Graphite (m√™me si je ne suis plus convaincre la dessus), et de quelques features pour lesquelles j‚Äô√©tais pass√©e travers :\n\n\n  Support du SVG ( &amp;amp;format=svg) qui va enfin nous permettre de tester l‚Äôenrichissement des graphs par du contenu ‚Äúconnexe‚Äù (liste des erreurs 404 sur le graph lorsque l‚Äôon clic sur un point, nom du d√©veloppeur ayant fait la mise en production etc)\n  Les fonctions de HoltWinter afin d‚Äôavoir des tendances hautes et basses pour mieux savoir quand alerter par exemple.\n\n\n[WebPerf] How to Walk Away From Your Outage Looking Like a HERO par Teresa Dietrich (WebMD), Derek Chang (WebMD)\n\nL‚Äôune des conf√©rences que j‚Äôattendais beaucoup : le titre annon√ßait un talk sur la gestion d‚Äôincident avec un cot√© humoristique.\n\nLes deux conf√©renciers ont pr√©sent√©s des templates tr√®s complet de gestion d‚Äôincident qu‚Äôils r√©alisent pour des posts-mortems qu‚Äôon peut retrouver ici : https://www.teresadietrich.net/?page_id=37\n\nPersonnellement, il me parait tr√®s important de r√©aliser des posts mortems. Mais si c‚Äôest pour passer plus de temps √† r√©diger des rapports d‚Äôincidents trop complet qu‚Äôon ne relira jamais qu‚Äôa en tirer un quelconque b√©n√©fice, cela me parait inutile.\n\nDu coup, la premi√®re demi-heure a consist√© √† pr√©senter ces templates, lire et expliquer quelques incidents ayant eu lieu chez WebMD.\n\nJ‚Äôai, comme une bonne partie de la salle, fait l‚Äôimpasse rapidement : entre le sujet dans lequel je ne suis jamais rentr√© ainsi que des slides avec beaucoup de texte illisible pass√©s les premiers rangs de la tr√®s grande salle, je n‚Äôai pas accroch√©.\n\nA revoir t√™te repos√©e : https://velocityconf.com/velocity2012/public/schedule/detail/23615 (slides non dispo √† l‚Äôheure actuelle)\n\n\n\nTeresa Dietrich (WebMD), Derek Chang (WebMD)\n\n[WebPerf] The 90-minutes Mobile optimization life cycle par Hooman Beheshti (VP strangeloop)\n\nConf√©rence orient√©e WebPerf Mobile.\n\nIci, on retrouve tout ce que j‚Äôaime dans les conf√©rences V√©locity :\n\n\n  Un conf√©rencier avec un grand talent d‚Äôorateur : pr√©cis, dr√¥le et captivant\n  Un sujet tr√®s maitris√©\n  Des slides propres et parlantes m√™me sans avoir assist√© au talk\n  Des d√©bats lanc√©s ‚Ä¶\n\n\nL‚Äôid√©e √©tait de partir d‚Äôun site, au hasard Oreilly.com, puis le site mobile velocityconf.com par la suite, et de d√©montrer les √©tapes d‚Äôoptimisation WebPerf, √©tape par √©tape, avec √† chaque fois, ce que l‚Äôon souhaite obtenir, ce que l‚Äôon obtient r√©ellement, et une comparaison vid√©o du changement.\n\nDes points ont √©t√© approfondis comme la gestion du cache, via LocalStorage, du fonctionnement des CDN (pour le mobile), du Pipellining HTTP, de la congestion TCP etc ‚Ä¶\n\nBeaucoup d‚Äôoutils ont aussi √©t√© mentionn√©s pour la WebPerf mobile :\n\n\n  Chrome remote debugging : Http://developers.google.com/chrome/mobile/docs/debugging/\n  iWebInspector for IOS simulator : www.iwebinspector.com\n  Weinre : Remote debugging from the desktop for what the phone is doing : https://people.apache.org/~pmuellr/weinre/\n  Aardwolf : Remote js debugging lexandera.com/aardwolf\n  Mobile Perf Bookmarklet : stevesouders.com/mobileperf/mobileperfbkm.php\n  Pcap2har : Turn packet captures to waterfalls https://pcapperf.appspot.com\n  ‚Ä¶\n\n\nBref, un tr√®s bon panorama pour la WebPerf mobile avec deux cas concret d‚Äô√©tude, chacun avec deux approches diff√©rentes :\n\n\n  Am√©liorer les m√©triques de WebPerf pour le site d‚ÄôOreilly\n  Am√©liorer la perception utilisateur sans regarder les m√©triques pour le site mobile de la V√©locityconf\n\n\nSlides dispo ici : https://www.strangeloopnetworks.com/blog/the-90-minute-mobile-optimization-life-cycle/\n\nJe vous invite aussi √† regarder son interview ci dessous.\n\n\n\n\n\n[Event] Akamai Pool Party\n\nCette journ√©e touche √† sa fin avec une Pool Party ext√©rieure par Akamai avec un orchestre (qui nous jou√© notamment le th√®me de Mario ! Tr√®s fun), beaucoup √† boire, et beaucoup √† manger (l√©gumes tremper dans du brie chaud, WTF ?). L‚Äôoccasion de rencontrer quelques sponsors et 2 autres fran√ßais. :-)\n\n\nDevOps drinking session / @jstinson\n\n[Event] Ignite Sessions\n\nA 19h30 avait lieu les Ignite sessions, des confs ‚Äúlightning talks‚Äù de 5 minutes sur des sujets divers, certains tr√®s int√©ressant comme :\n\n\n  le ‚ÄúPerceptual Diff‚Äù par un ing√©nieur de chez Google, pour √™tre alert√© (par l‚Äôint√©gration continue) lorsque la page du Service customers de chez Google change. Photos de la pr√©sentation ici + https://pdiff.sourceforge.net/\n  les #lolops, avec une s√©rie de Twitt orient√©e Devops √† mourir de rire : Voir ici https://www.slideshare.net/cwestin63/lolops-a-years-worth-of-humorous-engineering-tweets\n  ‚Ä¶\n\n\nLe concept est vraiment efficace avec des speakers ultra dynamique et pour la majorit√© tr√®s dr√¥le.\n\nMention sp√©ciale pour la partie centrale, o√π 11 personnes de la salle (dont certains speakers, Allspaw et Souders en t√™te), avait 1 √† 2 minutes pour improviser sur des slides plut√¥t tr√®s dr√¥le qu‚Äôils n‚Äôavaient jamais vu.\n\nJ‚Äôesp√®re que les vid√©os seront disponible car c‚Äô√©tait juste hilarant au possible. Je ne m‚Äôattendais pas √† pleurer de rire non stop ici :-)\n\n\n\nConclusion\n\nExcellente premi√®re journ√©e, d√©j√† des tonnes d‚Äôinfos √† condenser / retenir, et ce n‚Äô√©tait que le premier jour !\n\nSinon l‚Äôorganisation est impeccable, lieu exceptionnel, wifi public qui fonctionne, repas de tr√®s bonne qualit√© (et table qui plus est), pas mal de multiprises dans les salles, √† boire √† volont√© ‚Ä¶ La grande classe !\n\nLes comptes rendus des prochaines journ√©es et des sessions orient√©s Ops suivre ;-)\n\nN‚Äôh√©sitez pas √† faire un maximum de retour sur ce compte rendu, cela nous aidera et nous motivera pour les prochains ;-)\n\nP.S: Retrouvez moi sur Twitter : @kenny_dee\n\n"
} ,
  
  {
    "title"    : "Monitoring applicatif : Pourquoi et comment ?",
    "category" : "",
    "tags"     : " monitoring, graphite, statsd, conference",
    "url"      : "/monitoring-applicatif-pourquoi-et-comment",
    "date"     : "June 26, 2012",
    "excerpt"  : "Voici les slides de la pr√©sentation que j‚Äôai donn√©e au Forum PHP 2012, et au WebEvent 4 :\n\nVous √™tes d√©veloppeur, chef de projet technique ou m√™me responsable et vous souhaitez avoir de la visibilit√© sur le fonctionnement de vos applicatifs, ou su...",
  "content"  : "Voici les slides de la pr√©sentation que j‚Äôai donn√©e au Forum PHP 2012, et au WebEvent 4 :\n\nVous √™tes d√©veloppeur, chef de projet technique ou m√™me responsable et vous souhaitez avoir de la visibilit√© sur le fonctionnement de vos applicatifs, ou sur la plateforme sur laquelle ils sont h√©b√©rg√©s ?\n\nNous √©tudierons comment, gr√¢ce √† des outils simples (StatD / Graphite / Log BDD) et nos exp√©riences chez M6Web, mettre en place un monitoring applicatif ultra complet.\n\nCe monitoring vous permettra de retrouver la vue sur vos projets, pour mieux anticiper la charge, detecter la root cause en cas d‚Äôincident et connaitre l‚Äô√©tat de chacun de vos services ..\n\nMonitoring applicatif : Pourquoi et comment ?\n\n \n"
} ,
  
  {
    "title"    : "M6Web au Web Event Lyon #4",
    "category" : "",
    "tags"     : " webevent",
    "url"      : "/m6web-au-web-event-lyon-4",
    "date"     : "June 20, 2012",
    "excerpt"  : "\nUne partie de l‚Äô√©quipe de M6 Web au webevent de La ferme du Web.\n\n",
  "content"  : "\nUne partie de l‚Äô√©quipe de M6 Web au webevent de La ferme du Web.\n\n"
} ,
  
  {
    "title"    : "M6 Web √©tait pr√©sent au Forum PHP 2012",
    "category" : "",
    "tags"     : " afup, forumphp, conference",
    "url"      : "/post/24732185644/m6-web-tait-pr-sent-au-forum-php-2012",
    "date"     : "June 9, 2012",
    "excerpt"  : "Voici quelques photos des membres d‚ÄôM6Web prises lors du Forum PHP 2012.\n\nKenny a anim√© une conf√©rence sur le monitoring applicatif.\n\n\n\nMartin, notre CTO, a particip√© une table ronde.\n\n\n\nOlivier √©tait √©galement pr√©sent en tant que membre de l‚ÄôAfup...",
  "content"  : "Voici quelques photos des membres d‚ÄôM6Web prises lors du Forum PHP 2012.\n\nKenny a anim√© une conf√©rence sur le monitoring applicatif.\n\n\n\nMartin, notre CTO, a particip√© une table ronde.\n\n\n\nOlivier √©tait √©galement pr√©sent en tant que membre de l‚ÄôAfup\n\n\n\nAinsi que Didier et Julien, d√©veloppeurs dans nos √©quipes R&amp;amp;D, et trop pr√©ss√©s d‚Äôassiter toutes les conf√©rences pour √™tre photographi√©s.\n\n"
} ,
  
  {
    "title"    : "M6Web au Forum PHP 2012 et au WebEvent #4",
    "category" : "",
    "tags"     : " php, afup, monitoring, conference",
    "url"      : "/post/24184111542/m6web-au-forum-php-2012-et-au-webevent-4",
    "date"     : "June 1, 2012",
    "excerpt"  : "Cette ann√©e, M6Web sponsorise deux √©v√©nements fran√ßais majeurs dans le monde du Web :\n\n\n  le Web Event √† Lyon #4 (https://event.lafermeduweb.net/ au centre de congr√®s de Lyon le 15 juin 2012)\n  et le forumPHP 2012 (https://afup.org/pages/forumphp2...",
  "content"  : "Cette ann√©e, M6Web sponsorise deux √©v√©nements fran√ßais majeurs dans le monde du Web :\n\n\n  le Web Event √† Lyon #4 (https://event.lafermeduweb.net/ au centre de congr√®s de Lyon le 15 juin 2012)\n  et le forumPHP 2012 (https://afup.org/pages/forumphp2012/ la Cit√© universitaire √† Paris le 5 &amp;amp; 6 juin 2012) !\n\n\nA cette occasion, M6Web sera bien repr√©sent√© :\n\n\n  je (Kenny Dits) pr√©senterais chaque √©v√®nement des sessions sur le monitoring applicatif en regard de ce que nous faisons au quotidien chez M6Web. https://afup.org/pages/forumphp2012/sessions.php#632\nhttps://event.lafermeduweb.net/les-sessions#c6\n  Olivier Mansour en tant que Vice Pr√©sident de l‚ÄôAfup parlera lors de la Keynote de cloture du Forum Php le 6 juin. https://afup.org/pages/forumphp2012/sessions.php#732\n  Martin Boronski, notre directeur technique participera √©galement √† la table ronde DSI organis√©e par l‚ÄôAFUP Paris le 6 Juin.\n\n\nRendez-vous l√† bas ? ;-)\n\nBanni√®re du Forum PHP 2012\n\n[\n\nBanni√®re du Web Event Lyon\n\n[\n\n"
} ,
  
  {
    "title"    : "Performances PHP chez M6Web",
    "category" : "",
    "tags"     : " graphite, monitoring, nodejs, php, varnish, webperf, conference",
    "url"      : "/post/23671071384/performances-php-chez-m6web",
    "date"     : "May 24, 2012",
    "excerpt"  : "Voici les slides de la pr√©sentation du 23 mai r√©alis√©e √† l‚ÄôEpitech de Lyon.\n\nC‚Äôest un retour d‚Äôexp√©rience, qui survole un peu tous les axes sur lesquels nous travaillons chez m6web, ayant trait aux optimisations de nos sites.\n\nJ‚Äôesp√®re que certain...",
  "content"  : "Voici les slides de la pr√©sentation du 23 mai r√©alis√©e √† l‚ÄôEpitech de Lyon.\n\nC‚Äôest un retour d‚Äôexp√©rience, qui survole un peu tous les axes sur lesquels nous travaillons chez m6web, ayant trait aux optimisations de nos sites.\n\nJ‚Äôesp√®re que certains points feront l‚Äôobjet d‚Äôautres articles dans le futur ;-)\n\nPerformances php chez M6Web\n&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;\n\n"
} ,
  
  {
    "title"    : "Lancement du blog technique d&#39;M6Web",
    "category" : "",
    "tags"     : " ",
    "url"      : "/post/23664141031/lancement-du-blog-technique-dm6web",
    "date"     : "May 24, 2012",
    "excerpt"  : "Bienvenue sur le blog de la direction technique de M6 Web.\n\nVous retrouverez ici, une fr√©quence qu‚Äôon esp√®re des plus r√©guli√®res, quelques articles et autres retours d‚Äôexp√©rience de nos √©quipes technique.\n\nAttendez vous √† manger du PHP, Mysql, Nod...",
  "content"  : "Bienvenue sur le blog de la direction technique de M6 Web.\n\nVous retrouverez ici, une fr√©quence qu‚Äôon esp√®re des plus r√©guli√®res, quelques articles et autres retours d‚Äôexp√©rience de nos √©quipes technique.\n\nAttendez vous √† manger du PHP, Mysql, Node.js, entendre parler de performance, monitoring, vid√©o, html5 etc ;-)\n\nBonne lecture √† tous.\n\n"
} 
  
  
  
]
