[
  
  {
    "title"    : "Encrypt AWS AMIs: one way to do it wrong",
    "category" : "",
    "tags"     : " cloud, aws",
    "url"      : "/2022/07/08/encrypt-aws-amis.html",
    "date"     : "July 8, 2022",
    "excerpt"  : "At Bedrock, we build our own privately shared AMIs (Amazon Machine Images) for different parts of our stack: kubernetes platform, vod platform, etc. We build those AMIs to optimize kernel parameters,to embed some tools, and more. We have been usin...",
  "content"  : "At Bedrock, we build our own privately shared AMIs (Amazon Machine Images) for different parts of our stack: kubernetes platform, vod platform, etc. We build those AMIs to optimize kernel parameters,to embed some tools, and more. We have been using Packer for a couple of years, and everything has been working just fine.\n\nConcerned about following AWS best-practices, we recently added encryption by default to all new EBS volumes in all our accounts.\n\nWe didn’t expect it, but this decision impacted our AMI creation process. We thus began to update our Packer workflow to integrate this new constraint. We were telling ourselves that more security was for the best and we didn’t take enough steps back to analyze drawbacks.\n\nYou will find in this blog post multiple tips that may help you handle your AMIs encryption, but also why you shouldn’t handle it our way.\n\nBuild an encrypted AMI\n\nTo build our AMI, Packer launches an EC2 in a “builder” account, then a snapshot is created and copied in needed regions. To use this AMI, “user” accounts are listed in the AMI allowed users.\n\nWith account EBS encryption enabled, snapshots are now encrypted. The default behavior is to use the account’s default KMS Key. Our first “easy” problem while trying to build new AMI with Packer was the following error message:\n\nError Copying AMI (ami-xxxxxx) to region (xx-xxx-x): InvalidRequest: Snapshot snap-xxxxxxx is encrypted. Creating an unencrypted copy from an encrypted snapshot is not supported.\n\n\nTo avoid that, we enabled AMI encryption with Packer, but it resulted in another error :\n\nError modify AMI attributes: InvalidParameter: Snapshots encrypted with the AWS Managed CMK can&#39;t be shared.\n\n\nAs our AMI has to be shared to other accounts, it was impossible to encrypt our AMI with the account default KMS Key. So we created a dedicated KMS Key for Packer encryption.\n\nAnd it worked! We had our beautiful encrypted AMI, ready to be used in all our accounts.\n\n\n\nHow we build our encrypted AMIs\n\nRun an encrypted AMI\n\nThis is where it gets complex.\n\nWhen we tried to launch an EC2 instance with our newly encrypted AMI, it failed with this error code :\n\nClient.InternalError: Client error on launch\n\n\nIt means that AWS can’t use this AMI because it is encrypted.\n\nFirst step was to authorize the KMS Key to be used for encryption in user (external) accounts.\n\nThere are two methods to do that, for two different needs.\n\n\n\nPolicy method\n\nTo authorize an external customer managed role (ours), we had to authorize our role in KMS Key dedicated policy to use it, then authorize KMS Key in our role policy to be used. It is some kind of symmetric reference hard to correctly maintain with IaC (Terraform). And we had to do the same for KMS Key replicas in other regions, because they have a dedicated policy.\n\n\n\nPolicy method\n\nOne important thing to know here: some KMS Key permissions aren’t available for external account sharing. It means that when we try to add the permission kms:* to our role policy (for debug purposes only, we follow least privileges principles), it failed. You can find which permission is accessible in cross account use and which is not here.\n\nGrant method\n\nTo authorize an AWS managed role, like AWSServiceRoleForAutoScaling (to launch our EC2), we also needed to allow it to use our key. It is impossible to add a new policy on an AWS Managed role. So instead of using a policy method like before, we had to create a grant on that role to use our key. We tried to create that grant from the source account (where the key is created), but it didn’t work. We had to create that grant from the destination account (where AWSServiceRoleForAutoScaling is), using a role in the destination account that is allowed to create a grant… So we had to allow a role from the destination account to create a grant with Policy method, then use the previous role to allow an AWS Managed Role to use our KMS Key with Grant method. Pretty fun, right?\n\n\n\nGrant method\n\n\n\nOnce all needed roles were allowed, we tried to launch an EC2 with the allowed role attached as an instance role. It failed again, because we needed to also use the AMI KMS Key on root volume of our instance. By default, it was the account KMS Key that was used.\n\nWe attached that key on our root volume, and it worked. We also could launch our EC2 with ASG. It was all good.\n\nBut there was a big security vulnerability: instead of using one KMS key per account to encrypt our EBS volume, we were now using the same KMS key on all our accounts because of our encrypted AMI.\n\nKMS Key rotation\n\nA short word about Key rotation: it can easily be enabled to automatically rotate key materials each year. All new AMIs will be encrypted with new key material and nothing has to be changed to run encrypted AMIs.\nBut in case of a manual rotation: if a key is leaked for example, you will need to recreate a new KMS Key, its replicas, and all permissions and grants seen before.\n\nConclusion\n\nUsing privately shared encrypted AMI caused us multiple problems:\n\n  higher complexity to maintain.\n  lower security in cross-account configuration.\n\n\nFurthermore, we checked all our AMIs to see if they contain sensitive data. It isn’t the case : all sensitive data is uploaded at startup by Launch Template. We had no interest in continuing to use encrypted AMI, and we would have spared so much time if we had seen that sooner.\n\nThis is why we decided to disable encryption for all new EBS volume on our builder account and stop building encrypted AMI.\n\nDoing all the previous configuration took us several weeks. We are now more aware that doing security just for the beauty of it can be really counterproductive.\n\nIf your AMIs contain sensitive data, a better way to handle encrypted AMI may be to stop creating privately shared AMIs. Instead, copy and encrypt a private AMI in each of your “user” accounts with a dedicated KMS Key per account. As a result, there will be a larger amount of AMI to handle (one AMI per account per region), KMS Key permissions will still be complex, but security should improved.\n\nLogo used in thumbnail\nDeath by Imogen Oh from NounProject.com\nKey by Baboon designs from NounProject.com\nGears by Aybige from NounProject.com\n"
} ,
  
  {
    "title"    : "Debugging and reviewing your Android dependencies with apktool",
    "category" : "",
    "tags"     : " android, apktool, instrumentation, debugging, productivity",
    "url"      : "/2022/06/20/android-apktool-decompiling.html",
    "date"     : "June 20, 2022",
    "excerpt"  : "If you maintain an Android application, you might be relying on performance monitoring SDKs like Firebase Performance or New Relic, to name a couple. These plugins usually have a light setup process—just apply a Gradle plugin, and they provide the...",
  "content"  : "If you maintain an Android application, you might be relying on performance monitoring SDKs like Firebase Performance or New Relic, to name a couple. These plugins usually have a light setup process—just apply a Gradle plugin, and they provide the ability to collect statistics about every network call and database query in your app automatically.\n\nThe usual way to achieve this is to rely on a process called instrumentation, which is supported via the Android Gradle Plugin’s Transform API, or its successor, the Instrumentation API. This feature is very powerful, and potentially dangerous; in our case, a minor patch of one of these SDKs caused a production bug that left one of our core features crippled.\n\nThe visible cause of our bug, from a developer’s point of view, was that the video player saw the network requests as always being extremely fast, no matter the network quality. Therefore, it assumed the device had access to a very high bandwidth, and tried loading video segments with a very high bit rate. This did not go well for users with slower network speeds.\n\nTo understand what was going on, what went wrong, how to fix it and how to take measures so that it never happens again, we had to do some investigation.\n\nDiving into the Android build process\n\nBefore we get to the topic of instrumentation, we first need to know a little about the Android app build process. Don’t worry, we won’t need to dive too deep into the details.\n\nTo put it simply, during the build process, your source files (Kotlin and Java) are compiled to Dalvik bytecode, which is stored in .dex files. These files are then packaged into an APK file, which is basically just a ZIP file with all your code and resources.\n\n\nflowchart LR\n    kt[.kt files] -- kotlinc --&amp;gt; dex[.dex files] --&amp;gt; packaging[[packaging]]\n    java[.java files] -- javac --&amp;gt; dex\n    res[resource files] -- aapt --&amp;gt; resc[compiled resource files] --&amp;gt; packaging --&amp;gt; APK\n    subgraph APK\n    direction TB\n    dex1[.dex] -.- dex2[.dex] -.- dex3[.dex] -.- dex4[.dex]\n    res1[res] -.- res2[res] -.- res3[res] -.- res4[res]\n    signature -.- manifest\nend\n\n\nUnderstanding bytecode instrumentation\n\nNow, let’s say you want to take an existing application with its untouched source code, and automatically inject calls to your SDK every time a network call is made, to log whether it was successful or not. How would you achieve this?\n\nThe easiest way is to plug yourself into the build, right after the code is compiled into bytecode, and modify the bytecode to your will.\n\n\nflowchart LR\n    kt[.kt files] -- kotlinc --&amp;gt; dex[.dex files] --&amp;gt; transform[[transform]] --&amp;gt; packaging[[packaging]]\n    java[.java files] -- javac --&amp;gt; dex\n    res[resource files] -- aapt --&amp;gt; resc[compiled resource files] --&amp;gt; packaging --&amp;gt; APK\n    classDef transformed fill:#ff0000\n    class transform transformed\n\n    subgraph APK\n    direction TB\n    dex1[.dex] -.- dex2[.dex] -.- dex3[.dex] -.- dex4[.dex]\n    res1[res] -.- res2[res] -.- res3[res] -.- res4[res]\n    signature -.- manifest\n    class dex1,dex2,dex3,dex4 transformed\nend\n\n\nThe Android Gradle Plugin (AGP) offers APIs to do this, so SDK vendors can just develop a Gradle plugin and ta-da! Once you apply it, your app is automatically instrumented.\n\nNote that there are other ways to achieve this without the AGP. Notably, Kotlin now uses an Intermediate Representation (IR), before it gets compiled down to a target-specific format. You can write a Kotlin IR compiler plugin to transform the IR code and add your own hooks in an Android-agnostic way, although this API is still experimental at the time of writing.\n\nReverse-engineering a built APK\n\nNow, this is great. But when you open an APK file, what do you get?\n\nLet’s unzip one and look inside.\n\n.\n├── META-INF\n├── assets\n├── google\n├── okhttp3\n├── res\n├── AndroidManifest.xml\n├── classes.dex\n├── classes2.dex\n├── classes3.dex\n├── classes4.dex\n├── firebase-common.properties\n├── firebase-crashlytics.properties\n├── play-services-base.properties\n├── ...\n└── resources.arsc\n\n\nA bunch of noise, and four interesting .dex files. That’s where the app’s code is stored, but unfortunately, these files are not human-readable.\n\nTo turn them into low-level but understandable code, some tooling will be necessary. The easiest to use for this task is apktool, which is free and open-source.\n\nLet’s run apktool on our APK, and see what happens:\n\n\n\n\n\n~/Downloads\n❯ apktool d bedrock-sample-release.apk\nI: Using Apktool 2.6.1 on bedrock-sample-release.apk\nI: Loading resource table...\nI: Decoding AndroidManifest.xml with resources...\nI: Loading resource table from file: /Users/bcandellier/Library/apktool/framework/1.apk\nI: Regular manifest package...\nI: Decoding file-resources...\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nI: Decoding values */* XMLs...\nI: Baksmaling classes.dex...\nI: Baksmaling classes2.dex...\nI: Baksmaling classes3.dex...\nI: Baksmaling classes4.dex...\nI: Copying assets and libs...\nI: Copying unknown files...\nI: Copying original files...\nI: Copying META-INF/services directory\n\n\n\nThere we go! In our case, we can ignore the warnings. apktool created a new directory with a bunch of .smali files, organized by package: one file per class, containing their Dalvik bytecode.\n\n.\n├── AndroidManifest.xml\n├── res\n│   ├── values\n│   │   ├── strings.xml\n│   │   └── ...\n│   ├── layout\n│   │   ├── layout_home.xml\n│   │   └── ...\n│   └── ...\n├── smali\n│   ├── com\n│       ├── bedrockstreaming\n│       │   ├── app\n│       │   │   ├── mobile\n│       │   │   │   ├── R$anim.smali\n│       │   │   │   ├── R$layout.smali\n│       │   │   │   ├── R$string.smali\n│       │   │   │   ├── R$style.smali\n│       │   │   │   └── ...\n│       │   │   └── ...\n│       │   └── ...\n│       └── google\n│           ├── android\n│           │   ├── exoplayer2\n│           │   │   ├── AbstractConcatenatedTimeline.smali\n│           │   │   ├── AudioBecomingNoisyManager.smali\n│           │   │   ├── AudioFocusManager$AudioFocusListener$$ExternalSyntheticLambda0.smali\n│           │   │   ├── AudioFocusManager$AudioFocusListener.smali\n│           │   │   ├── AudioFocusManager.smali\n│           │   │   ├── BasePlayer.smali\n│           │   │   ├── BaseRenderer.smali\n│           │   │   ├── BuildConfig.smali\n│           │   │   └── ...\n│           │   └── ...\n│           └── ...\n├── smali_classes2\n│   ├── com\n│   │   └── bedrockstreaming\n│   │       ├── app\n│   │       │   ├── mobile\n│   │       │   │   ├── MobileApplication.smali\n│   │       │   │   └── ...\n│   │       │   └── ...\n│   │       └── ...\n│   └── ...\n└── ...\n\n\nIf you see files with mangled names and contents, make sure that you run apktool on an APK with R8 obfuscation disabled, or you’ll have a hard time figuring things out.\n\nUnderstanding Dalvik bytecode\n\nNow, if you open one of these files, it will contain code that looks like the snippet below. It will look unfamiliar; that’s normal.\n\n.method private final getContent()Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    .locals 2\n    .line 119\n\n    iget-object v0, p0, Lcom/bedrockstreaming/example/HomeViewModel;-&amp;gt;state:Landroidx/lifecycle/LiveData;\n\n    invoke-virtual {v0}, Landroidx/lifecycle/LiveData;-&amp;gt;getValue()Ljava/lang/Object;\n\n    move-result-object v0\n\n    instance-of v1, v0, Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    if-eqz v1, :cond_0\n\n    check-cast v0, Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    goto :goto_0\n\n    :cond_0\n\n    const/4 v0, 0x0\n\n    :goto_0\n\n    return-object v0\n  \n.end method\n\n\nIf you’ve ever worked with assembly code before, you might notice similarities in the way the code is written. Each line begins with an instruction, which can take comma-separated parameters. To work out what these instructions and their parameters mean, you will need to refer to the Dalvik bytecode documentation provided by Google.\n\nLet’s take an example line from the snippet and decode it together. Looking at the table in the documentation, we can see deduce this:\n\n# We&#39;ll decode this line:\ninvoke-virtual {v0}, Landroidx/lifecycle/LiveData;-&amp;gt;getValue()Ljava/lang/Object;\n\ninvoke-virtual                                                                   # We&#39;re calling a virtual method\n               {v0},                                                             # We&#39;re calling the method on the object referenced in register v0\n                     Landroidx/lifecycle/LiveData;                               # The method we&#39;re calling is defined by androidx.lifecycle.LiveData\n                                                  -&amp;gt;getValue()                   # We&#39;re calling a method called getValue()\n                                                              Ljava/lang/Object; # This method returns an Object\n\n\nWith some determination, we can figure out what the snippet does. Here, we’re defining a getContent() method that tries to cast a LiveData’s value to State.Content and returns it, or null otherwise.\n\nUsing a decompiled APK as a debugging tool\n\nInspecting suspicious code\n\nBefore doing anything else, we can already start looking at the generated code to identify patterns that could cause issues. Problem is… there can be a lot of code to look through.\n\nBefore going this deep in the rabbit hole, we already figured our issue was, somehow, related to instrumentation: disabling it fixed this issue; downgrading to the previous release of the SDK also fixed it. This means that if we want to get a clear look at what needs to change to go from a working APK from a broken one, we could just compare an APK instrumented by the previous SDK version with an APK instrumented by the current one!\n\nOf course, we want to do this on the human-readable smali files, not the raw dex files. We can generate a full diff with the help of the diff tool:\n\ndiff -bur normal/ instrumented/\n\n\nIn our case, it also proved useful to compare an APK that has been instrumented with one that hasn’t, to understand what that instrumentation is meant to achieve. Most of it was to notify the SDK of every HTTP request, along with its result.\n\nAs a simple example, the snippet below shows a class belonging to Picasso. We can see the HTTP calls it makes are being intercepted by the SDK.\n\n--- normal/smali/com/squareup/picasso/NetworkRequestHandler.smali\t2022-01-05 11:09:22.000000000 +0100\n+++ instrumented/smali/com/squareup/picasso/NetworkRequestHandler.smali\t2022-01-05 11:08:34.000000000 +0100\n@@ -128,10 +128,26 @@\n\n     .line 103\n     :cond_4\n+    instance-of v2, v1, Lokhttp3/Request$Builder;\n+\n+    if-nez v2, :cond_5\n+\n     invoke-virtual {v1}, Lokhttp3/Request$Builder;-&amp;gt;build()Lokhttp3/Request;\n\n     move-result-object v2\n\n+    goto :goto_1\n+\n+    :cond_5\n+    move-object v2, v1\n+\n+    check-cast v2, Lokhttp3/Request$Builder;\n+\n+    invoke-static {v2}, Lcom/vendor/instrumentation/okhttp3/OkHttp3Instrumentation;-&amp;gt;build(Lokhttp3/Request$Builder;)Lokhttp3/Request;\n+\n+    move-result-object v2\n+\n+    :goto_1\n     return-object v2\n .end method\n\n\nFinding the source of the issue by iteration\n\nWe haven’t talked about apktool’s greatest strength yet: its ability to recompile an APK from the smali sources it has decompiled! This means we can effectively decompile an APK, make modifications to its low-level code, recompile and run it.\n\nThis proved really useful during our investigation. Since we have one directory with our APK in a bad state, and one directory with our APK in a good state, we can process by elimination to point out exactly which single class, when modified, causes our bug.\n\nIn our case, a useful workflow was to start with a suspect—let’s say we think instrumenting the OkHttp classes might have caused the bug.\n\n\n  Copy the OkHttp classes from the “bad” APK, and only those, to our “good” APK.\n  Recompile and run the app.\n  Does the bug occur?\n    \n      If it does, then that means it is caused by the instrumentation of at least one of the OkHttp classes. We can go through this process again, this time by selecting only a subset of OkHttp’s classes, and check if the bug still occurs, etc.\n      If it doesn’t, revert the OkHttp classes and try again with another suspect.\n    \n  \n\n\nThis process can be accelerated with a very simple script, to iterate faster. The recompilation step occurs incrementally, and so only takes a few seconds.\n\n#!/bin/sh\n\n# rebuild-and-run.sh\n# Rebuild, sign and install an APK from its decompiled source.\n# (c) 2022 Bedrock Streaming\n\n# Inputs:\n# DECOMPILED_APK_PATH: path to your previously decompiled APK directory\n# KEYSTORE_PATH: path to your debug keystore\n# KEYSTORE_PASSWORD: your debug keystore password\n\napktool --use-aapt2 b &quot;$DECOMPILED_APK_PATH&quot; \\\n    &amp;amp;&amp;amp; apksigner sign -ks &quot;$KEYSTORE_PATH&quot; --ks-pass &quot;pass:$KEYSTORE_PASSWORD&quot; &quot;$DECOMPILED_APK_PATH/dist/*.apk&quot; \\\n    &amp;amp;&amp;amp; adb install &quot;$DECOMPILED_APK_PATH/dist/*.apk&quot;\n\n\nHere’s what it looks like in action:\n\n\n\n\n\n~/bytecode-playground\n❯ ./rebuild-and-run.sh\nI: Using Apktool 2.6.1\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether resources has changed...\nI: Building apk file...\nI: Copying unknown files/dir...\nI: Built apk...\nPerforming Incremental Install\nServing...\nSuccess\nInstall command complete in 445 ms\n\n\n\nIn our case, we narrowed down the issue to the instrumentation of a single class: okhttp3.internal.http.CallServerInterceptor: once it was reverted, the bug disappeared.\n\nIn fact, we narrowed it down to a very small patch with which the app runs fine:\n\n .../okhttp3/internal/http/CallServerInterceptor.smali         | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali b/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\nindex c916149f..c26eab15 100644\n--- a/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\n+++ b/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\n@@ -510,7 +510,7 @@\n \n     instance-of v8, v14, Lokhttp3/Response$Builder;\n \n-    if-nez v8, :cond_b\n+    #if-nez v8, :cond_b\n \n     invoke-virtual {v14, v15}, Lokhttp3/Response$Builder;-&amp;gt;body(Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n \n@@ -574,7 +574,7 @@\n \n     instance-of v15, v8, Lokhttp3/Response$Builder;\n \n-    if-nez v15, :cond_e\n+    #if-nez v15, :cond_e\n \n     invoke-virtual {v8, v14}, Lokhttp3/Response$Builder;-&amp;gt;body(Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n \n-- \n\n\nBasically, when the code went through this if statement, our request got wrapped by com.vendor.instrumentation.okhttp3.OkHttp3Instrumentation:\n\ninvoke-static {v8, v14}, Lcom/vendor/instrumentation/okhttp3/OkHttp3Instrumentation;-&amp;gt;body(Lokhttp3/Response$Builder;Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n\n\nAnd what does this method do, you ask? Let’s take a look at the decompiled source in Android Studio, so that it’s a bit easier to read:\n\npublic Builder body(ResponseBody body) {\n    try {\n        if (body != null) {\n            BufferedSource source = body.source();\n            Buffer buffer = new Buffer();\n            source.readAll(buffer);\n            return this.impl.body(ResponseBody.create(body.contentType(), buffer.size(), buffer));\n        }\n    } catch (IOException var4) {\n        log.error(&quot;IOException reading from source: &quot;, var4);\n    } catch (IllegalStateException var5) {\n        log.error(&quot;IllegalStateException reading from source: &quot;, var5);\n    }\n\n    return this.impl.body(body);\n}\n\n\nThe body is being read into memory!\n\nsource.readAll(buffer);\n\n\nWhen correlating this discovery with the source code from ExoPlayer, we could verify that, indeed, our player was expecting that the time it takes reading the response body would be the time it took to download the entire video segment. Here’s what this flow looks like in a functional app:\n\n\nsequenceDiagram\n    participant exo as OkHttpDataSource\n    participant nr as OkHttp3Instrumentation\n    participant okhttp as OkHttpClient\n    participant server as Server Endpoint\n\n    exo-&amp;gt;&amp;gt;nr: body()\n    nr-&amp;gt;&amp;gt;okhttp: body()\n    activate server\n    okhttp-&amp;gt;&amp;gt;server: \n    server-&amp;gt;&amp;gt;okhttp: \n    okhttp-&amp;gt;&amp;gt;nr: ResponseBody (length=0)\n    activate exo\n    nr-&amp;gt;&amp;gt;exo: ResponseBody (length=0)\n    server-&amp;gt;&amp;gt;exo: length=512\n    server-&amp;gt;&amp;gt;exo: length=1024\n    server-&amp;gt;&amp;gt;exo: length=1536\n    server-&amp;gt;&amp;gt;exo: length=2048\n    server-&amp;gt;&amp;gt;exo: length=2560\n    note right of exo: OkHttpDataSource controls the body reads  and can measure the time it took  to read the whole response\n    deactivate server\n    deactivate exo\n\n\nBut with this bug in the SDK, since the HTTP response has been buffered into memory by some SDK, the read was always almost-instantaneous, no matter the speed of the connection. Additionally, it messed with the overall performance since requests were no longer properly streamed by their rightful users.\n\n\nsequenceDiagram\n    participant exo as OkHttpDataSource\n    participant nr as OkHttp3Instrumentation\n    participant okhttp as OkHttpClient\n    participant server as Server Endpoint\n\n    exo-&amp;gt;&amp;gt;nr: body()\n    nr-&amp;gt;&amp;gt;okhttp: body()\n    activate server\n    okhttp-&amp;gt;&amp;gt;server: \n    server-&amp;gt;&amp;gt;okhttp: \n    activate nr\n    okhttp-&amp;gt;&amp;gt;nr: \n    server-&amp;gt;&amp;gt;nr: length=512\n    server-&amp;gt;&amp;gt;nr: length=1024\n    server-&amp;gt;&amp;gt;nr: length=1536\n    server-&amp;gt;&amp;gt;nr: length=2048\n    server-&amp;gt;&amp;gt;nr: length=2560\n    deactivate nr\n    activate exo\n    note right of exo: OkHttpDataSource is only notified  after everything is downloaded\n    nr-&amp;gt;&amp;gt;exo: ResponseBody (length=2560)\n    deactivate server\n    deactivate exo\n\n\nUsing a decompiled APK as a review tool\n\nIt’s no secret to developers in any software ecosystem that library updates can be a source of problems - security vulnerabilities, bugs, incompatibilities, and so on. It’s hard to vet them properly, especially in compiled form, like libraries distributed in the Java ecosystem. Things get even harder when arbitrary Gradle plugins start rewriting our own code!\n\nThe tooling needed to decompile an APK is free, fast, and easy to automate. It’s a really helpful tool to investigate obscure bugs in places your debugger won’t let you place a breakpoint, and it’s also really useful to be able to see a human-readable diff between two binaries.\n\nGenerating a diff of the effects of a library upgrade can seem overkill and hard to do in practice, but at least in the case of bug-fix releases with hopefully few changes, it can be very helpful to have an actual report of what changed. It’s an accepted practice to review the code your team checks in; why not review the code of others, since it ends up in the exact same artifact?\n"
} ,
  
  {
    "title"    : "Bedrock à la Kubecon 2022, 4ème partie : chaos, résilience, ressenti global et conclusion générale…",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/16/kubecon-2022-part-4.html",
    "date"     : "June 16, 2022",
    "excerpt"  : "Pour terminer cette série, un ou deux sujets divers que nous n’avons pas regroupé dans les trois articles précédents\n(les performances applicatives et la scalabilité, \nles performances bas niveau, le système et le réseau,\nla dev XP, l’outillage, l...",
  "content"  : "Pour terminer cette série, un ou deux sujets divers que nous n’avons pas regroupé dans les trois articles précédents\n(les performances applicatives et la scalabilité, \nles performances bas niveau, le système et le réseau,\nla dev XP, l’outillage, la CI/CD et l’observabilité), \npuis une conclusion globale avec ce que nous avons retenu de cette KubeCon Europe 2022.\n\n\nLa conclusion, @ KubeCon 2022 !\n\nChaos Engineering / Chaos Testing pour une meilleur résilience aux pannes\n\nC’est un des sujets sur lesquels nous avons commencé à travailler activement cette année : casser des choses dans nos clusters, dans notre plateforme, entre nos microservices.\nL’idée sous-jacente est, bien sûr, que tout va casser un jour ou l’autre, donc autant provoquer du chaos nous-même, en environnement contrôlé. Nous identifierons ainsi des points sensibles de notre plateforme et pourrons les corriger, évitant ainsi des incidents, parfois majeurs, au mauvais moment.\n\nCe thème du Chaos Engineering est régulièrement abordé en conférences et nous étions contents de voir que nous ne sommes pas les seuls à nous interroger sur “comment” en mettre en place.\nNous sommes repartis avec quelques pistes d’outils, comme chaos mesh ou Litmus Chaos, que nous allons peut-être prototyper pour les comparer à chaos-controller que nous avons récemment expérimenté.\n\nAu cours de la conférence, “Case Study: Bringing Chaos Engineering to the Cloud Native Developers” (vidéo) par Uma Mukkara, Litmus et Ramiro Berelleza, Okteto, nous avons pu avoir un aperçu de l’outil de chaos Litmus, sa force semblant résider dans le partage des scripts de chaos au sein la communauté.\nPuis, il a été décrit une approche CI des tests de chaos visant à intégrer certains tests de chaos dans le flux de développement plutôt qu’à la fin.\n\nEnfin, toujours sur des questions de résilience en cas d’interruption de service, dans sa conférence “Building for the (inevitable) Next Cloud Outage” (vidéo), Pavel Nikolov de Section nous a questionnés sur la manière d’être plus robuste à une catastrophe.\nLa question n’est pas de savoir si une catastrophe se produira, mais quand elle se produira. C’est pourquoi il est aussi préférable de disposer d’un plan de reprise après sinistre mais surtout de prévoir en amont un système d’auto-guérison permettant d’être plus résilient aux catastrophes.\nIl nous a ensuite présenté un use case spécifique au réseau, nous invitant à préférer au traditionnel “DNS à la rescousse”, la mise en place de BGP (Border Gateway Protocol).\n\nQuelques sujets divers\n\nÀ travers quelques talks, nous avons jeté des coups d’œil sur des sujets sur lesquels nous ne travaillons pas réellement au quotidien – appelez ça de la curiosité intellectuelle si vous le voulez ;-)\n\n\n  Nous avons vu un ensemble de design patterns pour le développement de controllers Kubernetes (vidéo), approche qui devient petit à petit un moyen répandu de répondre à des problématiques, en codant directement dans Kubernetes.\n  La conférence “A treasure map of hacking (and defending) K8s” (vidéo) était très sympathique, elle montrait à quel point il peut être “facile” de prendre le contrôle d’une infrastructure. Une façon de montrer que patcher est obligatoire !\n  Et dans un registre hors-technique, “Composability is to software as compounding interest is to finance” (vidéo) mettait en évidence à quel point, en construisant des outils, puis des projets, puis un écosystème, les uns profitent aux autres, on construit donc plus grand et plus gros. Il suffit de voir le landscape CNCF aujourd’hui par rapport à 4 ans en arrière.\n\n\nConclusion, KubeCon Europe 2022\n\nNous avons commencé à migrer vers Le Cloud, vers AWS et Kubernetes, il y a plus de quatre ans. Notre première KubeCon était à Copenhague, en 2018. Que dire, en conclusion de cette conférence annuelle ? Comment conclure ces articles ?\n\nAujourd’hui, les grandes idées que nous avons retenues de cette KubeCon Europe 2022, en résumant, sont les suivantes :\n\n\n  Les problématiques d’auto-scaling sont bien cernées, les outils sont plutôt matures. Comme beaucoup d’autres entreprises, nous arrivons sur l’étape suivante, qui est de dimensionner en tenant mieux compte des couts et pas uniquement des performances et/ou de la disponibilité.\n  La gestion des coûts dans Kubernetes n’est toujours pas simple. À la fois pour les suivre et les répartir, mais aussi pour décider du bon compromis entre performances / disponibilité / souplesse / autonomie des équipes / couts.\n  Service Mesh : nous n’avons toujours pas franchi le pas et Istio, qui était un sujet très à la mode il y a quatre ans, nous semble désormais presque oublié. Aujourd’hui, Cilium semble être la nouvelle approche qui s’impose, et il se pourrait que nous jouions avec “pour voir” prochainement…\n  L’observabilité, plus vraiment un problème.\n  Le chaos engineering / chaos testing : toujours une idée séduisante, mais pas encore réellement industrialisée ?\n  L’outillage autour de la CI/CD, le déploiement progressif, le rollback (possiblement automatisé) progresse, et ça fait plaisir !\n  L’écosystème progresse, mûrit, et on parle de sujets de plus haut niveau que quelques années en arrière. Par exemple, nous avons entendu plusieurs fois parler de base de données magiquement scalable hébergée dans Kubernetes, alors que l’époque où nous évitions de stocker quelque état que ce soit dans un cluster ne nous semble pas si lointaine !\n\n\nEt, pour finir, quelques points dont nous n’avons pas du tout ou très peu entendu parler :\n\n\n  Nous avons peut-être loupé des choses en créant nos programmes, mais nous n’avons vu aucun talk autour de “comment nous développons des applications cloud-native”. Pourtant, la problématique de l’environnement de développement, avec des services managés, des déploiements vers Kubernetes et des plateformes distribuées, ne nous parait pas encore réglée !\n  L’approche “FaaS” (Function as a Service) nous paraît encore moins répandue que quelques années en arrière ?\n  Nous n’avons pas entendu parler une seule fois de Fédération, alors que le terme revenait encore et encore il y a quatre ans. Nous avons bien fait de ne même pas essayer, on dirait ;-)\n\n\n\nRejoignez-nos équipes et venez vivre les prochaines conférences avec nous l’an prochain\n"
} ,
  
  {
    "title"    : "Bedrock à la Kubecon 2022, 3ème partie : Dev XP, outillage, CI/CD, observabilité…",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/15/kubecon-2022-part-3.html",
    "date"     : "June 15, 2022",
    "excerpt"  : "Pour notre troisième article de cette série sur ce que nous avons retenu de la KubeCon Europe 2022, après \nles performances applicatives et la scalabilité et \nles performances bas niveau, le système et le réseau, \npassons à la Developper eXperienc...",
  "content"  : "Pour notre troisième article de cette série sur ce que nous avons retenu de la KubeCon Europe 2022, après \nles performances applicatives et la scalabilité et \nles performances bas niveau, le système et le réseau, \npassons à la Developper eXperience, à l’outillage, à la CI/CD, aux rollback, à l’observabilité et aux incidents !\n\n\nUne nouvelle journée commence, @ KubeCon 2022 !\n\nLa prod est tombée !\n\nDans notre secteur d’activité, nous avons tous subis des incidents de production et le retour d’expérience, qu’il soit interne ou public, est important et formateur.\nEn effet, même si les incidents de production sont malheureusement inéluctables dans nos métiers, il est important de les analyser afin de mieux les comprendre et demieux s’en prémunir.\n\nPreuve de l’importance de ces sujets : nous avons assisté à deux conférences très intéressantes sur ce thème dans des salles pleines à craquer !\nToutes deux portaient sur des incidents de production majeurs suite à une modification de code qui peut paraître anodine : la première était donnée par Influxdata, la seconde par Skyscanner.\nLes conférences étaient particulièrement joviales et bienveillantes : les réactions du public à certains slides montraient bien que ce genre de situations sentait le vécu pour certains !\n\nNous avons tous à apprendre de ces cas concrets d’incident, aussi, nous vous conseillons de visionner les vidéos de ces conférences : skyscanner et influxdata.\nMais si nous devions les résumer : l’automatisation de bout en bout demande une grande maturité, beaucoup (beaucoup !) de tests et des reviews de qualité !\n\nEt comme il est dit dans une des slides :\n\n\n\nDebugger, en production, avec des conteneurs éphémères\n\nNous utilisons régulièrement la commande kubectl exec pour entrer dans conteneur / pod et y lancer des commandes de débogage – parce que certains problèmes ne sont pas reproductibles ailleurs qu’en production, ou parce qu’il faut comprendre ce qu’il se passe avant de savoir reproduire en environnement de développement.\n\nCela dit, cette approche n’est pas géniale : si nous modifions des choses dans un conteneur, ces modifications persistent.\nAussi, il faut pouvoir installer des outils de débug dans un conteneur (ce qu’on ne peut pas facilement faire chez Bedrock, où nos conteneurs ne s’exécutent pas en root et ont souvent un filesystem read-only), ou les embarquer dans les images (ce qui les grossit considérablement, sans compter l’augmentation du risque de failles de sécurité).\n\nPour remédier à cette problématique, la fonctionnalité de conteneurs éphémères (vidéo) arrive en bêta dans Kubernetes 1.23 et ça semble absolument génial !\nL’outil parfait pour lancer des conteneurs temporaires à l’intérieur de pods existant et incroyablement puissant pour débugger !\nNous allons pouvoir réduire le nombre d’outils de debug intégrés à nos images et parvenir à débugger plus aisément des problèmes qui ne surviennent qu’en production !\n\nLes risques de l’observabilité / Observabilité piratée\n\n“How attackers use exposed Prometheus Server to Exploit Kubernetes Clusters” (vidéo) par David de Torres et  Miguel Hernandez, ou “comment obtenir l’empreinte de vos clusters k8s à travers vos données de monitoring”.\n\nSysdig est venu nous remémorer que le monitoring, c’est bien, mais que ne pas exposer ses données de monitoring, c’est mieux !\nEn effet, attention aux informations qui sont exposées à l’extérieur, elles pourraient être recueillies par des attaquants externes pour acquérir des connaissances sur votre plateforme (provider cloud, version de l’OS utilisé…) et s’en servir ensuite pour s’introduire dans votre infrastructure (fuite de données, cryptominage ou ransomware).\n\nÀ travers un cas d’utilisation fictif, ils nous ont démontré la facilité de récupération de ces informations et comment elles sont utilisées pour monter une attaque.\nEnfin, ils nous ont rappelé que pour se prémunir de ces attaques, il suffit de suivre les recommandations de sécurité ! CQFD.\nIl est toujours bon d’avoir ces piqures de rappel et de toujours bien penser aux données que l’on expose vers l’extérieur.\n\nCI/CD, déploiement progressif\n\nChez Bedrock, nous sommes en pleine refonte de notre chaîne de CI/CD : nous basculons tous nos projets du bon vieux Jenkins “temporaire”, que nous avions monté au début de notre migration vers Le Cloud, vers Github Actions.\nAu passage, nous nous demandons forcément comment nous pourrions améliorer nos déploiements et les rendre plus sécurisés, tant pour la santé de notre plateforme que pour la paix d’esprit de nos équipes et de nos utilisateurs.\n\nLa conférence “Automated progressive delivery using gitops and service mesh” (vidéo) parlait de déploiement progressif avec Argo CD, pour améliorer l’excellence opérationnelle, réduire le MTTR, accroître l’automatisation et la fiabilité des processus de déploiement. Bref, des idées qui nous parlent !\n\nReste des fonctionnalités, qui nous semblent primordiales avant de se lancer sur un autre outil, qui ne sont pas encore gérées, hélas : mirroring de traffic, routing basé sur des en-têtes (typiquement : pour faire du déploiement progressif à la maille “utilisateur” et pas à la maille “requête HTTP”), détection d’anomalie et rollback automatisé…\nUn projet à suivre, donc, qui pourrait mûrir dans les prochains mois.\n\nAu niveau des aspects moins sympathiques : cette approche de déploiement progressif passe par un service mesh (envoy, ici).\nOr nous n’en avons pas en place et depuis quatre ans n’avons toujours pas trouvé les bons arguments pour en introduire dans nos clusters, notamment à cause de la complexité ajoutée…\n\nUne autre conférence (vidéo) mentionnait l’outil Flagger pour des déploiements Canary.\n\nQuelques autres idées à retenir\n\nNous avons aussi vu quelques autres conférences dont nous avons tiré quelques idées, en plus bref :\n\n\n  Kubernetes 1.23 apporte (en alpha) une nouvelle commande kubectl events, qui retourne ses résultats dans l’ordre chronologique. Ce que l’actuel kubectl get events ne fait pas et ça peut être bien embêtant. Vue comme de la culture générale, la conférence “The soul of a new command: adding ‘events’ to kubectl” (vidéo) racontait comment cette fonctionnalité a été implémentée et était fort intéressante.\n  Un speaker parlait de la mise en place de Crossplane dans son entreprise (vidéo). Sujet potentiellement intéressant, mais qui ne correspond pas à notre approche actuelle. Nous avons toutefois retenu quelques points autour de comment il fournit des outils à ses collègues développeurs : documentation, composition de services, management d’attentes, utilisation de l’écosystème… Des problématiques auxquelles nous nous sommes confrontés de nombreuses fois, pour encourager nos équipes à adopter des évolutions ou de nouveaux outils !\n  Si vous commencez à mettre en place votre stack de logs, la conférence “Show me your labels and I’ll tell you who you are” (vidéo) est faite pour vous. L’idée d’utiliser les labels assignés aux pods pour aller jusqu’à filtrer l’accès aux logs via RBAC, terrible ! Aussi, la création de flux de logs avec Logging Operator a l’air fort sympathique. Si ce talk était venu trois ans plus tôt, c’est quelque chose que nous essayerons !\n\n\nConclusion\n\nCes conférences nous ont permis d’approfondir les questions que nous nous posons actuellement alors que notre changeons de CI/CD (déploiement progressif, rollback automatisé ou non…).\n\nPlus globalement, nous sommes contents de voir que l’outillage autour de Kubernetes continue à progresser et que la Developer eXperience est un sujet pris au sérieux dans notre communauté.\n\n\nRejoignez-nos équipes et venez vivre les prochaines conférences avec nous l’an prochain\n"
} ,
  
  {
    "title"    : "Bedrock à la Kubecon 2022, 2nde partie : performances, système et réseau",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/14/kubecon-2022-part-2.html",
    "date"     : "June 14, 2022",
    "excerpt"  : "Pour ce second article de synthèse de la KubeCon Europe 2022, continuons sur le thème des performances, peut-être plus bas niveau,\net plongeons aussi dans des outils pouvant être déployés au cœur de nos clusters !\n\n\nCa va commencer, @ KubeCon 2022...",
  "content"  : "Pour ce second article de synthèse de la KubeCon Europe 2022, continuons sur le thème des performances, peut-être plus bas niveau,\net plongeons aussi dans des outils pouvant être déployés au cœur de nos clusters !\n\n\nCa va commencer, @ KubeCon 2022 !\n\nL’autoscaling, autrement\n\nUne des conférences : “Autoscaling Kubernetes Deployments: A (Mostly) Practical Guide” (vidéo) présenté par NewRelic présentait le principe d’autoscaling dans Kubernetes, avec les trois principales ressources associées à ce concept : ClusterAutoscaler, HorizontalPodAutoscaler et VerticalPodAutoscler.\n\nCette conférence présentait :\n\n\n  le fonctionnement du scale up/down des pods avec les périodes de stabilisation ;\n  le calcul par rapport aux indicateurs utilisés ;\n  les types de métriques utilisables par les HPA et VPA.\n\n\nPas de grande découverte technique pour nous, mais cette conférence nous a surtout permis de confirmer que, chez BedRock, \nnous sommes de plus en plus matures sur la scalabilité de nos clusters Kubernetes.\n\nLa conférence donnée par AWS (vidéo) portait sur deux aspects :\n\n\n  Premièrement, l’utilisation d’instances Spot (une option à ne pas négliger si vous souhaitez fortement réduire vos coûts de compute) et les bonnes pratiques à mettre en place en utilisant ce type d’instances EC2.\n  Le second traitait de la scalabilité des nœuds avec ClusterAutoscaler mais présentait un nouvel outil de provisionnement de nœuds Kubernetes proposé par AWS : Karpenter.\n\n\nUne des différences notables par rapport à cluster-autoscaler est que Karpenter ne fonctionne pas avec des AutoScalingGroup AWS \nmais provisionne directement des instances EC2.\nOutre cette fonctionnalité, Karpenter est actuellement à l’étude chez BedRock, notamment car il permet l’utilisation de la \ndimension région, ce qui n’est pas possible avec cluster-autoscaler et nous pose des problèmes avec nos statefullsets dans des ASG multiAZ.\n\nRéseau, Bande passante et GPU\n\nAutre point abordé lors de la KubeCon : comment intégrer la bande passante comme une ressource limitante, de la même façon que le CPU et la RAM actuellement.\nNous avons pu suivre deux présentations à ce sujet : “Network-aware Scheduling in Kubernetes” de José Santos, Ghent University (video et “Better Bandwidth Management with eBPF” de Daniel Borkmann et Christopher M. Luciano, Isovalent (video).\n\nLa première session proposait un nouveau plugin (repo github) pour permettre l’orchestration du déploiement de nouveaux pods en fonction de leur charge et coût réseau, afin de réduire la latence des déploiements.\nUne nouvelle fonctionnalité de ce plugin est par ailleurs en développement et permettra d’éviter de déployer sur un nœud ou la bande passante est déjà saturée.\n\nLa seconde présentation exposait comment eBPF permet de mettre en place de nouveaux pods en prenant en compte la bande passante. Le replay de la conférence est disponible ici, nous vous conseillons son visionnage.\nCette approche pourrait être très intéressante pour Bedrock si nous décidions de migrer notre plateforme VOD sur un cluster Kubernetes : en effet, elle nous permettrait de mieux gérer les burst réseaux et le throttling de la bande passante qui se produisent sur nos instances.\n\nCôté GPU, Google, dans son exposé “Improving GPU Utilization using Kubernetes” de Maulin Patel et Pradeep Venkatachalam (video), nous a présenté deux façons de partager des ressources GPU dans un cluster kubernetes :\n\n\n  soit en partageant le temps d’utilisation (timesharing, temporal multiplexing) entre conteneurs sur un même nœud,\n  soit en multi-instance GPU (MIG, spatial multiplexing) permettant de partager les ressources en parallèle entre conteneur en allouant une partie des cœurs GPU et de sa mémoire pour chaque conteneur.\n\n\nCette conférence sur l’utilisation des GPU dans un cluster k8s nous incite à réfléchir aux optimisations que nous pourrions faire sur nos plateformes vidéo et data…\n\nService Mesh : Cilium\n\nAu cours de diverses conférences, nous avons plusieurs fois entendu le nom de “Cilium” associé au concept de Service Mesh.\nLa conférence “A guided tour of Cilium Service Mesh” (vidéo) nous a permis d’en apprendre plus sur ce nouveau service qui ne se base plus sur des sidecars, mais sur eBPF.\n\nUn outil peut-être encore un peu jeune, mais clairement prometteur – et très certainement quelque chose que nous allons étudier lors d’un POC dans le courant de l’année ;-)\n\nRécapitulatif\n\nIl n’existe toujours pas d’outils magique pour passer à l’échelle et supporter les pics de charges.\nToutefois, les solutions présentées au cours de cette KubeCon EU 2022 viennent répondre à des besoins qui sont apparus au fil des années et dont peu d’utilisateurs avaient mesuré l’impact au début de leur périple avec Kubernetes.\n\nAussi, eBPF continue à faire parler de lui et son utilisation semble se répandre.\nL’idée d’un service mesh plus léger que Istio, par exemple, a l’air fort intéressante !\n\n\nRejoignez-nos équipes et venez vivre les prochaines conférences avec nous l’an prochain\n"
} ,
  
  {
    "title"    : "Bedrock à la kubecon 2022, 1ere partie : performances applicatives et scalabilité",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/13/kubecon-2022-part-1.html",
    "date"     : "June 13, 2022",
    "excerpt"  : "\n\nBEDROCK à la KubeCon 2022\n\nAprès 2018 à Copenhague et 2019 à Barcelone, cette année encore, nous étions trois, Coraline, Julien et Pascal, présents à la KubeCon CloudNativeCon Europe 2022, à Valencia !\n\nPlus de quatre ans après le début de notre...",
  "content"  : "\n\nBEDROCK à la KubeCon 2022\n\nAprès 2018 à Copenhague et 2019 à Barcelone, cette année encore, nous étions trois, Coraline, Julien et Pascal, présents à la KubeCon CloudNativeCon Europe 2022, à Valencia !\n\nPlus de quatre ans après le début de notre migration vers Le Cloud (AWS + Kubernetes) racontée dans Le Plan Copenhague, nous visions à découvrir de nouvelles idées, à confirmer certains de nos choix et à apprendre des retours d’expérience de nos pairs. Après tout, avec une communauté aussi large (plus de 7000 participants et participantes cette année), il serait dommage de rester seuls avec nos idées !\n\nSommaire\n\nÀ trois, nous avons assisté à une grosse quarantaine de conférences. Nous avons choisi d’organiser nos notes par thèmes, en quatre articles :\n\n  Un premier, celui-ci, centré sur les performances applicatives, sur la scalabilité des applications et la gestion des coûts.\n  Le second, consacré aux performances système, aux services mesh, aux fonctionnalités au niveau du cluster.\n  Le troisième, pour regrouper ce qui est Dev XP, outillage, CI/CD, rollback, observabilité…\n  Et un dernier, pour quelques sujets divers, dont le chaos engineering et la résilience, et pour conclure sur ce que nous avons retenu de cette édition de la KubeCon publication jeudi.\n\n\nAvec une plateforme de VOD et de replay déployée en marque blanche pour des broadcasters européen majeurs, des millions d’utilisateurs actifs, des milliers de CPU consommés, des centaines d’instances allumées et des dizaines de microservices, les performances sont au cœur de nos préoccupations.\n\nCet article reprend nos retours sur les nombreuses conférences consacrées à la scalabilité lors de cette KubeCon 2022. Cette fonctionnalité essentielle de Kubernetes est l’une des raisons de notre migration sur cette plateforme. En effet, notre activité nécessite que nous adaptions la taille de nos clusters en fonction du nombre d’utilisateurs connectés.\nNous avons donc assisté à la plupart des conférences consacrées à la performance et à l’adaptation de celle-ci en fonction de nos besoins.\n\nLe scaling vertical\nLa conférence “How Lombard Odier Deployed VPA to Increase Resource Usage Efficiency” (vidéo) nous présentait comment fonctionnent les requests et limits.\nUn sujet qui demande du temps pour être efficace afin de ne pas être en oversizing ou au contraire en undersizing.\n\nMais surtout, le conférencier nous a présenté son implémentation d’un composant Kubernetes assez rarement utilisé : Le VerticalPodAutoscaler. Le VPA à fait récemment l’objet de discussions au sein de nos équipes et cette présentation a confirmé notre ressenti : cette ressource est intéressante pour des cas d’usages spécifiques, notamment sur des “workloads” assez consommateurs en RAM et/ou en CPU et ne pouvant pas être découpés en multiples pods via un HorizontalPodAutoscaler.\n\nle VPA souffre toujours d’une limitation : l’ajout de RAM ou CPU à chaud n’est pas possible et nécessite la re-création du pod.\n\n\n\nAméliorer la scalabilité\nUne autre conférence, donnée cette fois-ci par Intel, présentait un projet récent : Telemetry Aware Scheduler (vidéo). Cet outil permet d’améliorer les choix du scheduler de Kubernetes en s’appuyant sur des métriques “customs”. Le projet est récent et en ALPHA, mais à surveiller dans l’avenir.\n\nLors d’une autre conférence intitulée “How Adobe is optimizing resource usage in K8s” (vidéo), Carlos Sanchez a présenté un outil interne permettant d’émettre des recommandations basées sur un historique de métriques, un peu comme fait VPA, mais au niveau d’un namespace ou du cluster entier. Il est également revenu sur comment ils parviennent à éteindre automatiquement des applications non utilisées par les clients pour réaliser des économies conséquentes.\n\nMais comment configurer les requests, limits et tout ça… sans y passer des mois ?\n\nNotre plateforme est composée de dizaines de services qui interagissent les uns avec les autres et sont soumis à un trafic qui varie au quotidien, avec des pics parfois impressionnants. Le paramétrage des requests et limits de chaque conteneur, ainsi que d’autres ressources, comme le nombre de processus php-fpm par conteneur, est un travail de fourmi, où nous devons itérer quotidiennement pendant une ou deux semaines, en travaillant application par application. Et tout ce travail est à refaire lorsque les applications ou leurs usages évoluent… un vrai casse-tête !.\nNous ne sommes pas les seuls à rencontrer ces problématiques et c’était le sujet de la conférence “Getting the optimal service efficiency that autoscaler won’t give you” (vidéo), où une approche basée sur de l’IA (ou, plutôt, sur du brute-force) était présentée.\n\nVoici les grandes lignes de la méthodologie présentée :\n\n  définition d’un scénario de load-test (ce qui reste difficile, il faut qu’il soit représentatif de la réalité)\n  Définition d’objectifs (les temps de réponses attendus, le pourcentage d’erreurs… en fait, des SLOs que chacun devrait déjà avoir pour ses services),\n  Lancer en boucle ces scenarios en retouchant request et limits (et configuration JVM) entre chaque itération.\n\n\nSur un cas réel, après la 34ᵉ itération (réalisées en 19 heures), environ 49% d’économies ont été réalisées. Mais surtout, cela a représenté un jour de travail grâce à cet outillage, au lieu de deux mois à la main.\n\nLe logiciel utilisé ne semble pas disponible en open-source, mais l’approche “automatiser les itérations” en retouchant les paramètres est très intéressante et nous saurions la reproduire. Elle nous permettrait de gagner beaucoup de temps, en supprimant beaucoup de tâches fastidieuses aujourd’hui. Reste à continuer à définir des SLOs, puis créer de nouveaux scénarios de load-testing représentatifs ! ;-)\n\nEt les coûts d’hébergement, alors ?\n\nNous avons aussi entendu parler plusieurs fois de coûts d’hébergement tout au long de cette KubeCon : comme l’illustrent les travaux de la FinOps Foundation, nous sommes de plus en plus nombreux à réaliser que si nous ne pensons pas à l’impact financier de nos infrastructures élastiques, où n’importe quel membre des équipes peut déployer des applications, la facture augmente vite et fort.\n\nLe talk “Why Kubernetes can’t get around FinOps - Cost Management best practices” (vidéo) était une bonne introduction aux principes de gestion de coûts sur Kubernetes. Rien de nouveau pour nous, sur la théorie… même s’il nous reste encore beaucoup de progrès à réaliser pour mieux maîtriser nos frais d’hébergement !\n\nConclusion\nSur ces sujets de scalabilité, les conférences auxquelles nous avons assisté confirment que bon nombre des choix que nous avons fait sont les bons, et que les problématiques qui nous font encore souffrir sont partagées par d’autres membres de la communauté.\n\nNous allons prochainement tenter de mettre en place VPA sur un de nos composants majeur, VictoriaMetrics, qui consomme beaucoup de ressources quelques heures par jour et pour lequel un scaling horizontal n’est pas adapté.\n\nNous n’en avons pas (ou peu) entendu parler pendant cette KubeCon, mais nous étudions en ce moment la solution Karpenter pour remplacer cluster-autoscaler, très utilisé dans la communauté, mais qui ne sait pas réellement tirer profit de spécificités liées à AWS.\n\nEnfin, sur les coûts… OK, il n’y a pas que chez nous que c’est compliqué. Et c’est clairement un sujet, dans Kubernetes comme au niveau d’AWS, sur lequel nous avons encore du boulot devant nous pour un an ou deux. Nous avons même un poste FinOps ouvert ;-)\n\n\n"
} ,
  
  {
    "title"    : "Bedrock&#39;s backend architecture and its front API Gateway",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front",
    "url"      : "/2022/06/10/backend-bff-intro.html",
    "date"     : "June 10, 2022",
    "excerpt"  : "What is a BFF, and how does it simplify the development of frontend applications?\n\nIntroduction\n\nAhoy there o/\n\nThis article is the first in a series explaining the backend architecture we use at Bedrock.\nThis first piece is dedicated to the BFF A...",
  "content"  : "What is a BFF, and how does it simplify the development of frontend applications?\n\nIntroduction\n\nAhoy there o/\n\nThis article is the first in a series explaining the backend architecture we use at Bedrock.\nThis first piece is dedicated to the BFF API. Without further delay, let’s jump into the subject!\n\nSo, what’s a BFF?\n\nFor years, we have been using a microservices pattern (1). Each with their own responsibilities.\n\nBackend and frontend development have long been decoupled.\nEvery frontend applications had to know about all microservices, call them and know what to do with their data.\n\n\n\nThis approach had three main limitations from a frontend point of view:\n\n  It forced Bedrock to duplicate logic in each application.\n  It prevented us from deprecating legacy APIs.\n  New features implied a frontend development and deployment.\n\n\nThere were other downsides, which were mainly derivatives from those listed above.\n\n  As an example, updating an icon into the menu bar required us to deploy all applications. It is not always easy or doable, and cannot be forced onto users without losing some of them.\n\n\nThe BFF tries to answer those limitations!\n\nIt’s a single API (2) that handles all the frontend applications queries to display contents, navigation, or even start downloads.\nIn addition, this gateway (3) gathers all business logic. This is done in order to avoid repeating the logic in each application.\nThat’s what we call a Back For Front!\n\nAbstracting the microservices\n\n\n\nThe first main advantage is to abstract the backend complexity for the frontend teams.\nIn the previous model, each application had to know where each data came from, how to parse it, and what to do with it.\n\nIn the new model, we can easily deprecate an API, replace it, change how the data is stored or returned.\nTo do so, we only need the team leading the change, and the team handling the BFF to work together at their own pace.\nThey can decide, depending on the change, how to handle the migration.\nThey might decide to use a new endpoint to be switched at some point, or add a new attribute in the response, etc.\n\nAll those changes will happen without any frontend application noticing it.\n\nSimplification of the data structure\n\nAnother advantage is to simplify the data representation.\n\n\n\nBy taking all this responsibility in a single API, it now translates the data from the APIs to a single unified representation that all applications can use.\n\n\n\nThis representation is maintained by the BFF in a single openapi schema (4). It shares the same concepts between the multiple endpoints of the API.\n\nThe main usage of the BFF is to handle the navigation between the pages of the application.\nIn the pictures above and below, the central block shows the application screen. The application page is split into two parts.\n\nThe top is answered by the navigation endpoint which gives a list of groups and entries.\nEvery entry can have nested groups, and an action.\n\nThe second part is what we call the layout. It’s a representation of the page, composed of multiple blocks, each with a list of items.\nEach item has a title, a description, an image, and an action (the same type as in the entries).\n\nThis makes the BFF responsible for what to display in the page, and in which order and how to display it.\nHow to display things is described through template strings that tell how to display each block.\n\nIt’s important to understand that the BFF does not return HTML! It returns a JSON string that needs to be parsed and interpreted by the application.\n\nEvery application still has to care about its design system, what font to use, which iconography.\nThis means that a template Card might not be displayed exactly the same between a computer, a mobile phone or a television; even if the data are the same.\n\n\n\nThere are other usages to the BFF (5), such as handling downloads, and some others to come, but it shares the same concept by answering to the front something to display.\n\nKeeping all logic in one place\n\nThe last main gain with the BFF, is that we’re able to put all the logic in one place.\nThis allows us to update and change the business rules at any time.\n\nHere are a few examples\n\n\n  When a user tries to navigate the application, if he uses a new device while he has already reached the limit of allowed devices, we can display a layout asking him to delete a device first.\n\n\nThis limit can be removed or changed at any time in all applications.\n\n\n  In France, explicit contents must be filtered out during daytime\n\n\nIf this rule changes, we will do so directly in the BFF, and no application will ever notice it.\n\nConclusion\n\nThe model known as back for front or API Gateway is nothing new and other major services already use it.\nWe’ve been using this model for more than 3 years now. It has undergone some major updates (6) but this is a model we’re happy with.\n\nWe plan to expand this pattern to handle even more logic inside the BFF in the coming years and keep being frontend application’s best friend.\n\n\n\nThat’s all for today’s post!\n\nIn the next part we will talk about handling the failures of the dependencies the BFF is calling, and what to do to always answer something usable by the applications.\n\nNotes\n\n\n  For more details about microservices, you can read this piece from AWS\n  There are some other APIs called by our applications, such as the authentication service, but let’s not get lost into details…\n  There’s a lot of resources about API Gateway, here is one from nginx\n  Open API is used to define the communication standards between our BFF and the clients, more explanation on the dedicated website of the organization\n  In addition to note 1, we are currently moving to the api gateway model, and some behaviors still require the application to call dedicated microservices.\n  ( in French 🇫🇷 ) An old conference from 2020 given by Benoit VIGUIER, previous Team Lead in charge of the BFF, about API gateway and asynchronous development.\n\n\nFrom the same series\n\n\n  What’s a BFF\n\n\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  🇺🇸 Announcing BedrockStreaming/pr-size-labeler github action\n  🇫🇷 Retour sur l’AFUP Day Lille 2022\n\n"
} ,
  
  {
    "title"    : "Migration progressive vers Redux Toolkit",
    "category" : "",
    "tags"     : " redux, lyonjs, meetup, react, javascript, conference",
    "url"      : "/2022/06/08/migration-progressive-vers-redux-toolkit.html",
    "date"     : "June 8, 2022",
    "excerpt"  : "Redux est le gestionnaire d’état global le plus populaire au sein de la communauté JS.\nSes créateurs encouragent désormais l’utilisation de Redux Toolkit (RTK). Une suite d’utilitaires facilitant l’usage de Redux et réduisant notamment sa verbosit...",
  "content"  : "Redux est le gestionnaire d’état global le plus populaire au sein de la communauté JS.\nSes créateurs encouragent désormais l’utilisation de Redux Toolkit (RTK). Une suite d’utilitaires facilitant l’usage de Redux et réduisant notamment sa verbosité.\nDans cette présentation, je vous propose un live coding pour migrer pas-à-pas une application React/Redux vers RTK.\n"
} ,
  
  {
    "title"    : "Comment ne pas jeter son application Frontend tous les deux ans ?",
    "category" : "",
    "tags"     : " conference, js, react, lyonjs, meetup",
    "url"      : "/2022/06/08/comment-ne-pas-jeter-votre-application.html",
    "date"     : "June 8, 2022",
    "excerpt"  : "Bonnes pratiques pour la maintenance d’une application web\nRefaire son front tous les 2 ans, c’est devenu une pratique plutôt courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la même codebase et ç...",
  "content"  : "Bonnes pratiques pour la maintenance d’une application web\nRefaire son front tous les 2 ans, c’est devenu une pratique plutôt courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la même codebase et ça depuis plus de 7 ans! En plus, ce n’est pas une petite application puisqu’il s’agit de 6play et de salto.\nVous pourriez vous dire: “Oh les pauvres, maintenir une application vieille de presque 10 ans ça doit être un enfer !”\nRassurez-vous, ce n’est pas le cas ! Nous avons tous travaillé sur des projets bien moins vieux mais sur lesquels le développement de nouvelles fonctionnalités était bien plus pénible.\n\nQuel est notre secret ? C’est ce que vous allez découvrir pendant ce talk !\nAutomatisation des tâches courantes, gestion de la dette, testing et architecture seront des sujets abordés.\nCe talk propose des thématiques qui ne concernent pas que le frontend !\n\nPlus de détails dans l’article suivant.\n\n"
} ,
  
  {
    "title"    : "Announcing BedrockStreaming/pr-size-labeler github action 🎉",
    "category" : "",
    "tags"     : " oss, github, devops",
    "url"      : "/2022/05/31/github-action-pr-size-labeler.html",
    "date"     : "May 31, 2022",
    "excerpt"  : "\n\nSmaller PR for a reduced mental load\n\nFor several years at Bedrock Streaming the technical teams have used the Pull Requests code review for each project. \nBetween collective ownership, quality improvement, regression detection, knowledge sharin...",
  "content"  : "\n\nSmaller PR for a reduced mental load\n\nFor several years at Bedrock Streaming the technical teams have used the Pull Requests code review for each project. \nBetween collective ownership, quality improvement, regression detection, knowledge sharing, learning, there is no question in this article to further legitimize the immense interest to implement this practice in your teams.\n\nThis practice can however lead to some problems, each developer who proposes Pull Requests for review by his colleagues can sometimes propose monstrous diffs.\nSometimes constrained by certain project mechanics or tools, but sometimes also by the “Wheelbarrow” effect.\n\n\n  While I was there, I took the opportunity to modify this too.\n\n\nIt always starts from a good will, however, to make PR that changes several intentions. \nBy creating his wheelbarrow, the developer is adding diff to a pull request that deviates from the original intent.\n\nLimiting the number of intentions of a pull request often simplifies the proofreading of it.\n\n\n  Has anyone ever had the pleasure of reviewing a Pull Request with more than 1000 lines of changes with more than 100 modified files?\n\n\nWe also forget that making a “big” Pull Request can also generate a mental load on the person or persons assigned to its development. \nWe have to remember the modified files, we are more likely to generate conflicts.\n\n\n  Ok, lets make smaller PR’s! We promise!\n\n\nYou can’t improve anything without measuring it\n\nSaying “from now on we do smaller PR1” is a pious hope.\nWe have been doing application monitoring for a long time, we know that thanks to these measurements we are able to understand if the evolution is rather positive or not.\nWhy not do it on our PR sizes?\nWhy not implement monitoring on our devs?\n\nThe idea is absolutely not to measure/comparison the performance of our developers. \nIt would not be positive for the engineering manager and the dev to compare the performance of one developer against another. \nWe are all different after all!\n\nThe size of a dev’s PRs does not reflect his productivity at all, it just allows to evaluate the personal and collective mental load produced.\nThere are other measures we would like to follow, but let’s start with the size of the PR.\n\nBe warned, the purpose of this metric is not to say “Oh! you made an XL size PR that’s not right” 😡.\nIt happens from time to time, and it’s not bad.\nYou should rather look at the distribution of PR sizes of a dev.\n\nLet’s take the example of a dev named Bob who would have this distribution over the last month:\n\n\nHere we see that Bob is globally making large PRs (taking arbitrary t-shirt sizes), seeing this we can say: As a TechLead, how can I best accompany Bob to make smaller PRs?\n\nNext, let’s look at Alice’s profile, which has a more centered distribution:\n\n\nHere, we can say that overall the majority of RPs are of moderate size (in this absolute scale), so the mental load should be lower than for Bob.\nThis remains an interpretation that will require some discussion to be sure.\n\nHow to set it up?\n\nIf you are interested in this measure and like us you use the Github Actions solution for your automation, it will be very easy for you to implement our brand new pr-size-labeler in your projects.\n\nTo do so, you can add a workflow to your Github repository:\n\nname: 🏷 PR size labeler\n\non: \n  pull_request:\n\njobs:\n  pr-labeler:\n    runs-on: ubuntu-latest\n    name: Label the PR size\n    steps:\n      - uses: BedrockStreaming/pr-size-labeler@v1.1.0\n        with:\n          token: $\n          exclude_files: .lock # RegExp of your excluded file pattern\n\n\nThe action will then put Size/S, Size/XL tags on your PRs automatically according to the number of modified files and the number of added or deleted lines.\n\n🧙‍ You can change the text of the labels used and even the thresholds for each size as you wish.\nTake a look at Github presentation page of this Github action.\n\nOnce set up, you should also notice the added labels can allow to evaluate the time needed for the review before starting it.\n\n\n  I’ve got 30 minutes to spare, I’m not going to start reviewing this PR XL.\n\n\nIt’s now your turn to play!\n\n  \n    \n      Alias for Pull Request &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Retour sur l&#39;AFUP Day Lille 2022",
    "category" : "",
    "tags"     : " backend, php, conference",
    "url"      : "/2022/05/30/afup-day-lille-2022.html",
    "date"     : "May 30, 2022",
    "excerpt"  : "\n\nCette année encore, Bedrock participait à l’AFUP Day 2022, nous avons eu la chance de profiter de conférences de qualité et aux sujets variés.\n\nPHP 8.1 en détail\n\nDamien Seguy nous a parlé des nouveautés de PHP 8.1 mais aussi de celles de PHP 8....",
  "content"  : "\n\nCette année encore, Bedrock participait à l’AFUP Day 2022, nous avons eu la chance de profiter de conférences de qualité et aux sujets variés.\n\nPHP 8.1 en détail\n\nDamien Seguy nous a parlé des nouveautés de PHP 8.1 mais aussi de celles de PHP 8.2 qui arriveront prochainement.\n\nDamien nous a parlé en vrac :\n\n\n  de l’ajout du format AVIF dans GD\n  l’ajout des fonctions fsync et fdatasync qui permettent de synchroniser les données d’un fichier sur l’OS. Cela veut dire que le fichier sera bien écrit sur le disque.\n  l’arrivée de la prise en compte du format ristreto255 avec le libsodium\n  de la fonction array_is_list\n\n\nLe conférencier a insisté sur le fait de ne pas hésiter à monter de version régulièrement. Il est possible d’utiliser les polyfills déjà existants (pour la 8.1 et même la 8.2) ou d’ajouter ses propres fonctions, mais de bien faire attention d’utiliser function_exists pour préparer les migrations.\n\nUne des nouveautés phare de la version 8.1 est l’ajout de la gestion des enums. Une énumération est un objet et non un type scalaire. Il est donc possible de l’étendre (via des interfaces, …). Cela veut aussi dire que nous ne pouvons pas les utiliser comme clefs de tableau par exemple. Il est par contre possible d’utiliser la propriété $myEnum-&amp;gt;value. Attention, les énumérations et les classes partagent le même espace de nom, nous ne pouvons donc pas avoir une enum et une classe s’appelant pareil dans le même namespace.\n\nLes Fibers, solution pour rendre les programmes interruptibles, a été rapidement introduite, mais nous vous conseillons de regarder la conférence sur ce sujet de notre cher et estimé collègue Benoit Viguier.\n\nDans la dernière version, à ce jour, a été introduit le type de retour never. Une fonction ou méthode qui retourne ce type ne pourra pas faire de return (même vide) ni même un yield. Il sera, par contre, possible d’appeler die, exit, throw ou suspend.\n\nLes constantes peuvent maintenant être finales, cela bloquera la possibilité de surcharger leurs valeurs par héritage.\n\nIl est désormais possible de faire des propriétés readonly. Cette propriété devra forcément être typée, et ne pourra pas avoir de valeur par défaut. readonly ne peut pas être utilisé avec static. Si la propriété est un objet, l’instance pourra être modifiée (par exemple avec des setters), mais pas remplacée.\n\nIl est maintenant possible d’instancier des valeurs par défaut. Par exemple :\n\nfunction serialize(\n    string \\$data, \n    Formatter \\$formatter = new DefaultFormatter()\n) { ... }\n\n\nCette instanciation est possible dans les arguments de fonction ou de méthode, les variables statiques ou encore les constantes globales. Par contre, ce n’est pas compatible avec les constantes de classes ou les propriétés de classes (sauf si ces dernières sont des propriétés promues).\n\nDans sa version 8.1, PHP apporte aussi les types d’intersections. Un exemple présenté serait de vouloir une instance de type Traversable ET Countable. Les types scalaires ne sont pas acceptés, c’est uniquement avec plusieurs classes.\n\nDe l’humain à l’ordinateur, ou découvrir le sens d’un texte avec ElasticSearch\n\nMathias ARLAUD nous a ensuite parlé d’ElasticSearch et de comment il est possible d&#39;attribuer un score de corrélation entre un texte donné et une multitude d&#39;autres.\n\n\nIl a décortiqué cette équation (déjà simplifiée) en nous expliquant les mécanismes en place pour calculer ce score.\nIl nous a parlé de Term Frequency (la fréquence à laquelle un mot apparaît dans un document), d’Inverse Document Frequency (la pertinence des mots) ainsi que de Coordination Factor (le fait de valoriser un document avec le plus haut pourcentage de mots présents dans la requête).\nAvec un exemple simple (Les Développeurs 🥰 développer avec,VIM&amp;amp;excl;), il nous a montré comment les filtres de caractères tels que html_strip, le mapping, les générateurs de tokens (whitespace -&amp;gt; 1 token = 1 mot) ou bien encore les filtres de tokens (phonetic, stopwords) permettent d’enlever le bruit des phrases humaines pour ne récupérer que les informations les plus pertinentes pour calculer ce score de corrélation. Après application de ces différentes étapes, son exemple se transforme en [developp] [aim] [developp] [vim], ce qui permet alors à ElasticSearch d’éliminer des documents non pertinents et qui seraient remontés si ces filtres n’avaient pas été appliqués.\nPour plus d’informations, Mathias a mis à disposition les slides de son\ntalk.\n\nGuide pratique d&#39;une méthodologie UX pour la conception de features\n\nJessica Martel nous a présenté une méthodologie UX pour la conception de features qu’elle a mise en place et suivie lors d’une expérience chez Decitre et maintenant chez Unow.\nElle nous a parlé de l’importance de la constitution d’une équipe projet regroupant tous les acteurs (PO, devs, le pôle Design et les équipes métier). Diversifier les acteurs permet d&#39;accroître l’adhésion du projet, d’apporter différentes cultures et de cibler le besoin.\n\nS&#39;ensuivent plusieurs étapes :\n\n\n  Product concept : évaluation du besoin, de la criticité\n  UX Research et audit : bench global, entretiens\n  User journey : identification des différentes étapes (déterminées suite au bench et entretiens), mise en place du workflow\n  Specs fonctionnelles et design : cas d’usages, règles métier, versions, KPI / création de wireframes, maquettes UI et prototypes\n\n\nCette méthodologie comprend cependant des limites ! Elle prend beaucoup de temps et est soumise au contexte, aux priorisations d’autres features, au downsizer…\n\nLe Prométhée moderne : embarquer PHP dans Go\n\nKévin Dunglas nous a parlé de comment embarquer PHP dans du Go. Après avoir listé les différentes SAPI (Module Apache, FPM, …) et nous avoir expliqué brièvement le langage Go et sa librairie standard net/http, Kévin nous a présenté FrankenPHP et toute la réflexion et les contraintes rencontrées pour le créer. Ce projet est un nouveau serveur Web en Go qui est capable d’appeler l’interpréteur PHP et donc de faire tourner nos applications Web. Le projet est bien avancé, mais pas terminé. N’hésitez pas à le contacter si vous voulez essayer avec vos applications, les retours lui seront utiles. En tout cas, chez Bedrock, on va suivre ça de près !\n\nLes subtilités du e-commerce à la française\n\nAprès nous avoir présenté les différentes taxes françaises et chez quelques-uns de nos voisins européens, David Buros nous a résenté les différents problèmes qu’il a rencontrés avec Sylius:\n\n\n  l’affichage du prix HT et du prix TTC\n  la gestion des réductions avec ce double affichage\n  la gestion des écotaxes\n  le paiement par mandat administratif\n\n\nComment on est passé de 1 800 emails à 70 000 par jour chez Trustt en 1 mois avec RabbitMQ\n\nCédric Driaux nous a expliqué comment ils ont mis en place RabbitMQ chez Trustt pour gérer l’envoi de plus de 70 000 mails par jour, afin de remplacer une ancienne solution qui lançait un CRON toutes les 15 minutes dans le but de faire les calculs et la distribution. Il y avait également des appels API à des outils externes, causant des ralentissements. De plus, certains reliquats de mail n&#39;étaient pas envoyés.\n\nPour répondre à ce problème, l’idée était de mettre les mails dans une file ou queue en anglais, permettant une mise en attente de l’envoi des messages, puis utiliser un consumer pour traiter les messages.\n\nCédric a décidé de choisir et de mettre en place (en à peine un mois !) RabbitMQ comme solution à cette problématique. Cela a permis : une baisse de charge des serveurs, notamment due à la suppression des CRON, une augmentation des mails envoyés, dont ces derniers peuvent désormais tous être traités dans la journée. Les erreurs sont mises de côté pour être traitées plus tard et ne pas ralentir le processus. Enfin, il est dorénavant possible pour eux d’ajouter d’autres mails dans la queue dans la journée.\n\nGrâce à RabbitMQ, ils ont pu fortement augmenter leurs capacités d’envoi de mails tout en soulageant les serveurs.\n\nCecil, mon générateur de site statique\n\nArnaud Ligny nous a présenté son projet perso de générateurs de sites statiques en archive phar : Cecil. Il voulait une solution rapide à prendre en main, intuitive et avec une séparation entre le contenu et la mise en forme. Ce side project avait pour but de remettre ses connaissances à jour en appliquant les bonnes pratiques. L’application est automatisée, le paquet .phar est automatiquement généré par GitHub Action lors de la création d’une release, scrutinizer qui fait des corrections, des previews sont réalisables avec netlify.\n\nEt si on étendait SQL avec du PHP\n\nAntoine BLUCHET nous a présenté plus en détail les projets Doctrine et principalement l’ORM. Cet outil est extensible, mais a quelques limites. Comment pouvons-nous faire des requêtes complexes avec Doctrine ? Peut-on utiliser des Common Table Expression ? La réponse à ces questions, proposée, est ESQL. Cet outil permet de construire des requêtes SQL complexes facilement sans se soucier des noms des tables ou des colonnes, car il permet d’utiliser ces métadonnées depuis Doctrine.\n\nPourquoi vous n’attirerez et ne retiendrez pas les femmes dans vos équipes tech.\n\nMarcy Charollois fait un constat sur le monde du travail dans le numérique, dominé largement par les hommes et ne laissant que trop peu de place aux femmes. Marcy commence par introduire la notion d’habitus, qui désigne un système de préférences, de style de vie particulier à chacun, qui influence les pratiques des individus au quotidien. Ces pratiques sont intériorisées inconsciemment, car l’individu s’adapte et s&#39;intègre à son environnement social. Cela crée un groupe majoritaire qui devient décisionnaire. Se met alors en place un statu quo qui va soit inclure ou exclure et qui est fortement dominé par la pensée masculine. Marcy nous dévoile que sur 100 % de freins ressentis par les femmes dans la tech, 30% proviennent des biais d&#39;oppression de groupe, une part donc assez conséquente.\n\nLe constat est réel, les femmes dans la tech vivent mal leur condition de femmes, il faut changer ce sentiment, mais les attitudes face au changement sont variées. 15% de personnes sont réfractaires, il sera donc difficile de faire évoluer les choses avec eux, 15% sont déjà partantes et 70% sont neutres, potentiellement pour ce changement, mais ne savent pas comment le faire.\n\nMarcy nous donne alors des clés qui permettront d’attirer les femmes dans nos équipes en mettant en avant les freins ressentis par celles-ci : une expression du genre, une légitimité face au métier exercé, des a priori sur la provenance des profils féminins qui sont souvent reconvertis et donc potentiellement juniors :\n\n\n  féminiser les postes et en particulier sur les offres d’emploi, une femme est développeuse, pas développeur.\n  mettre en avant les témoignages de femmes qui montent dans votre entreprise pour donner des exemples concrets de ce qu’elles pourraient trouver en venant chez vous\n  s’intéresser réellement à elles et non pas de voir en vous ce qu’elles mettent en avant\n\n\nLa conférence continue sur les actions à mener pour garder les femmes dans nos équipes :\n\n\n  parler d’égal à égal pour éviter la posture sachant(e)/ignorante\n  soyez clair, transparent sur les salaires, les évolutions de poste\n  mettez en place des moments conviviaux plus portés sur des préférences féminines\n  minimiser les interruptions pendant les prises de parole\n  écoutez des besoins spécifiques inhérents aux femmes et accommodez-les en offrant des ressources sans juger : parentalité, menstruation, assistance psychologique, adaptation à l’emploi du temps\n  encouragez les femmes à prendre la parole, à devenir un rôle modèle parce que compétente\n\n\nMarcy termine sa conférence par la présentation de quelques chiffres sur l’évolution de carrière des femmes et des enjeux psychosociaux résultant\nde cette évolution et conclut en montrant les bienfaits de l’inclusion des femmes au sein des entreprises et en nous donnant quelques noms de femmes célèbres dans ce combat.\n\nConclusion\n\nEncore une fois, l’AFUP a réussi à faire un événement chaleureux, intéressant et diversifié\n\nNous sommes ravis d’avoir pu participer à cette manifestation qui nous a permis de rencontrer les membres de la communauté ainsi que de visiter rapidement la ville de Lille et manger des Welsh.\n\n\n"
} ,
  
  {
    "title"    : "You Build it, you run it",
    "category" : "",
    "tags"     : " conference, docker, meetup, devops",
    "url"      : "/2022/05/13/docker-meetup-you-build-it-you-run-it.html",
    "date"     : "May 13, 2022",
    "excerpt"  : "L’une des grandes étapes de l’autonomie d’une équipe de développement dans la méthodologie DevOps est de s’intéresser à l’alerting lié à son infrastructure. Comment sommes-nous arrivé à proposer aux équipes de développement de s’intéresser et de m...",
  "content"  : "L’une des grandes étapes de l’autonomie d’une équipe de développement dans la méthodologie DevOps est de s’intéresser à l’alerting lié à son infrastructure. Comment sommes-nous arrivé à proposer aux équipes de développement de s’intéresser et de maîtriser cet alerting ?\n"
} ,
  
  {
    "title"    : "Retour sur l&#39;Android Makers 2022",
    "category" : "",
    "tags"     : " android, mobile, conference, makers",
    "url"      : "/2022/05/09/bedrock-android-makers-22.html",
    "date"     : "May 9, 2022",
    "excerpt"  : "Que c’est bon de se retrouver !\n\nAprès deux ans sans conférence en présentiel, l’Android Makers a fait son grand retour les 25 et 26 avril 2022, pour le plus grand bonheur de la communauté Android. \nL’équipe de développeurs Android de Bedrock (don...",
  "content"  : "Que c’est bon de se retrouver !\n\nAprès deux ans sans conférence en présentiel, l’Android Makers a fait son grand retour les 25 et 26 avril 2022, pour le plus grand bonheur de la communauté Android. \nL’équipe de développeurs Android de Bedrock (dont je fais partie) a partagé ce bonheur en assistant à ce rendez-vous incontournable. Jetpack Compose, accessibilité, optimisation de build et autres sont autant de sujets en maturation constante : essayons d’en faire le tour ensemble.\n\n\n\n\n  Que c’est bon de se retrouver !    \n      Penser         \n          Design System et Jetpack Compose \n          Accessibilité \n          Modularisation \n          Support de Chrome OS \n        \n      \n      Développer         \n          Splashscreen Android 12 \n          Tester les coroutines \n        \n      \n      Optimiser         \n          Toujours plus de CI \n          Optimisation du temps de build \n        \n      \n      Extras         \n          Création d’un UI Toolkit avec Romain Guy et Chet Haase \n        \n      \n      En conclusion \n    \n  \n\n\nPenser \n\nDesign System et Jetpack Compose \n\nPlusieurs conférences ont évoqué le sujet du Design et plus particulièrement l’implémentation d’un Design System (DS) avec Jetpack Compose. François Blavoet nous a partagé l’expérience d’Instacart à ce sujet, en dévoilant quelques détails d’implémentations de leurs API mises à disposition des features engineers, afin de leur faciliter l’intégration des éléments du DS.\nParallèlement, il nous a aussi invité à réflechir sur la nécessité d’intégrer Material Design dans notre implémentation du DS. En effet, si le cadre Material peut parfois s’avérer utile, il est quelques fois trop contraignant et pas toujours adapté aux besoins spécifiques de nos applications.\n\nAccessibilité \n\nVoilà un sujet qu’il est important d’évoquer, tant il est facile d’oublier d’adresser une application à tous. Cette édition de l’Android Makers a eu la chance d’accueillir une très belle conférence de Fanny Demey et Gerard Paligot sur le sujet de l’accessibilité. Dans une séance de Live Coding teintée d’un jeu de rôle sur le thème de l’émission C’est pas sorcier !, nous avons pu faire le tour de plusieurs points d’attention afin d’inclure au mieux nos utilisateurs porteurs de handicaps :\n\n  ne pas donner d’informations inutiles via TalkBack, comme les contentDescription des icônes décoratives\n  penser à la manière dont TalkBack va assembler les informations provenant de plusieurs vues distinctes\n  donner un retour d’action sur les clics de boutons et mieux placer ces actions lorsque le mode accessibilité est activé\n  et bien d’autres !\n\n\nLe Live Coding a pu également démontrer à quel point Jetpack Compose considère l’accessibilité comme une fonctionnalité cruciale grâce à des APIs très complètes (je pense ici à Modifier.sementics par exemple).\n\nModularisation \n\nJean-Baptiste Vincey, développeur chez Deezer, a partagé l’expérience de son équipe concernant la modularisation de leur code pour gérer le nombre grandissant d’applications dans leur catalogue. Basé sur la création de bibliothèques internes, plusieurs stratégies ont été explorées avec leurs bons et mauvais côtés. Lancé dans un chantier similaire, il est important pour Bedrock de voir comment d’autres acteurs du milieu ont répondu à ces questions, sans oublier que chaque entreprise a sa propre réponse qui doit s’adapter à ses process, son organisation et son produit.\n\nSupport de Chrome OS \n\nFrédéric Torcheux et Pierre Issartel, lors de leur conférence sur l’adaptation Chrome OS des applications Android, ont fait un constat intéressant : le nombre de Chromebook vendu a explosé récemment pour dépasser le nombre de Mac vendu. Sachant qu’un nombre grandissant de Chromebook a accès au Play Store, il est de plus en plus important d’adapter ses applications pour cet usage.\nEn vrac : exploiter le potentiel du curseur de la souris, naviguer dans l’application sans jamais quitter le clavier, supporter l’environnement multi-fenêtré et le redimensionnement de celles-ci, autant de points d’améliorations comportant pièges à éviter et bonnes pratiques.\n\nDévelopper \n\nSplashscreen Android 12 \n\nDeux développeurs de chez Google nous ont plongé dans les entrailles du WindowManager d’Android, ce composant qui est chargé d’orchestrer les applications que nous utilisons tous les jours : charger une application, la placer à l’écran puis la dessiner, la déplacer et gérer son cycle de vie, autant de responsabilités pour un WindowManager complexe à maitriser.\nÀ travers cette conférence pointue, Vadim Caen et Pablo Gamito ont rebondi sur le nouveau système de SplashScreen d’Android 12 pour nous expliquer quel problème il doit résoudre (essentiellement le ressenti de lenteur au lancement d’une application) et comment en tirer parti. À ce titre, la documentation de Google sur la migration vers le SplashScreen d’Android 12 est incontournable.\n\nLe nouveau Splashscreen pour Android 12 comporte son lot de challenges, notamment pour tenir compte des animations. Chez Bedrock, les reflexions à ce sujet ont démarré, et nous comptons partager un retour d’expérience sur notre propre migration !\n\nTester les coroutines \n\nMárton Braun, aussi développeur chez Google, a présenté les nouveautés de la bibliothèque kotlinx-coroutines-test en version 1.6+. Exit runBlockingTest, place au runTest qui permet, grâce à son TestCoroutineScheduler, de gérer les délais et l’ordre d’execution de toutes les coroutines lancées dans un test.\nL’ancienne version des API de test étant maintenant dépréciée, cette nouvelle version est encore experimentale mais vouée à passer en état stable, tant elle parait plus mature que la précédente.\nLes Flow et StateFlow n’ont pas été oubliés puisqu’ils ont aussi leurs spécifités en matière de tests.\n\nOptimiser \n\nToujours plus de CI \n\nChez Bedrock, la CI tient une place particulière dans nos process de release, et il est toujours intéressant de voir comment d’autres entreprises se saisissent de cet outil et améliorent leur process.\nAprès les rappels toujours pertinent sur l’importance de déléguer le maximum de tâches répétitives à nos environnement de CI, Xavier F. Gouchet, développeur chez Datadog, a présenté divers outils pour y parvenir.\n\nDetekt, un plugin Gradle permet d’aller encore plus loin qu’Android Lint en offrant l’analyse statique de n’importe quel code Kotlin. Son extensibilité nous est exposée via une API sur le pattern visiteur, redoutablement efficace pour parcourir le PSI (Program Structure Interface) de Kotlin. D’autres outils peuvent également être efficaces pour parcourir cette interface.\n\nXavier Gouchet présente également KSP (Kotlin Symbol Processor), le projet sponsorisé par Google voué à remplacer KAPT, son ancêtre basé sur Java. Combiné avec Kotlin Poet, cet outil permet d’automatiser la génération de code Kotlin à partir d’un code source annoté dans le projet.\n\nPour aider au développement autour du PSI Kotlin, Xavier Gouchet recommande l’excellent plugin PSIViewer pour IDE Jetbrains.\n\nOptimisation du temps de build \n\nZac Sweers est venu nous présenter la manière dont Slack, entreprise pour laquelle il travaille, optimise les builds Gradle. Les projets se complexifiant avec toujours plus de code et de modules, les temps de build ont tendance a augmenter.\n\nCette conférence a mis en lumière diverses optimisations pour tirer pleinement parti de Gradle et de ses nouvelles fonctionnalités :\n\n  désactiver les fonctionnalités non utilisées du plugin Android\n  profiter du cache, y compris sur serveur\n  éviter d’utiliser buildSrc pour factoriser du code\n  écrire son propre plugin de convention gradle\n  avoir un compte Gradle Entreprises pour profiter des Gradle Build Scans afin de déterminer quels sont les points de friction du projet, que ce soit pour l’utilisation du cache, la parallélisation, l’invalidation des builds, l’optimisation des arguments de la JVM…\n  parfois même, acheter du nouveau matériel ! (Apple Silicon)\n\n\nRéduire le temps de build est un enjeu constant, et participe au confort du développeur au quotidien.\n\nExtras \n\nCréation d’un UI Toolkit avec Romain Guy et Chet Haase \n\nUne conférence très intéressante a vu Romain Guy et Chet Haase nous présenter un projet experimental d’UI Toolkit maison, Apex, très proche de Jetpack Compose dans son API. \nCet exercice original a été un moyen de faire valoir le concept d’Entity component system, un pattern se basant sur la composition pour enrichir les comportements des entités d’un système.\n\nLeur présentation a mis en lumière la philosophie d’un UI Toolkit mais a aussi et surtout souligné la quantité de travail à accomplir pour passer d’un projet experimental à un toolkit utilisable en production. Enrichir sa boite à outils avec le maximum de widgets différents, permettre une personnalisation maximale aux développeurs, rendre le moteur de rendu multi-plateforme, autant de tâches nécessaires pour rendre votre Toolkit vraiment utile pour la communauté aujourd’hui.\n\nEn conclusion \n\n\n\nCette édition de l’Android Makers 2022 s’est conclue sur une conférence humoristique inédite de la part de Chet Haase et Romain Guy. Il a été question de tourner en dérision la communauté des développeurs sur le nombre de nouveaux patterns de développement qui sortent régulièrement et les discussions acharnées (et parfois virulentes) autour de ces derniers.\nCela a été une très bonne manière de prendre du recul sur notre communauté Android, et de se féliciter, tout de même, de la recherche constante d’amélioration des pratiques de développement au service de la qualité de nos produits et de leur accessibilité.\n"
} ,
  
  {
    "title"    : "How did we streamline the delivery of our internal conferences aka LFTs?",
    "category" : "",
    "tags"     : " lft, talks, live, stream, obs, streamyard, conference",
    "url"      : "/2022/04/29/how-we-simplify-our-lft-broadcast.html",
    "date"     : "April 29, 2022",
    "excerpt"  : "\n\nSome time ago, we shared with you an article explaining how we managed to capture and broadcast our conferences in the Bedrock auditorium. \nWe must admit, it worked great, but we wanted to make it simpler.\n\nAs a reminder, our internal conference...",
  "content"  : "\n\nSome time ago, we shared with you an article explaining how we managed to capture and broadcast our conferences in the Bedrock auditorium. \nWe must admit, it worked great, but we wanted to make it simpler.\n\nAs a reminder, our internal conferences or meetups are called LFT. \nIf you want to know more about our Last Friday Talk and the motivations that push us to do it, we invite you to read the above-mentioned article.\n\nAfter the success of this broadcasting, the (voluntary) organization team started thinking about how to make it simpler. \nIt was already a challenge in itself!\nIf you read this part of the article, you can see that we already had some ideas for improvements.\n\nImprovement areas\n\nIn the previous version, you could see that a very important quantity of material was necessary (meters of various cables, multi outlets, a camera, etc…)\nA large part of this material was kindly lent by Pascal (and we thank him for that), but we could not decently borrow it for each LFT.\n\nMoreover, this very specific equipment did not allow each Bedrock employee to manage the control room, quickly and with his or her own machine.\nFinally, as you can imagine, setting up and putting away such a large amount of equipment takes a lot of time. \nThere were no less than four of us setting up and tidying up.\n\nIn another topic, let’s talk about video quality. We used to use Google Live Stream before, but the 720p broadcast, with its very low bitrate and aggressive compression, sometimes made it difficult to follow.\nText and images were often very pixelated.\n\nAlso, we wanted to, easily, handle a hybrid mode to our LFTs.\nBecause of the pandemic, telecommuting and the fact that some of Bedrock’s employees are located in Paris, we need to broadcast and capture the LFT both in person and remotely.\nBy hybrid mode we also meant that during the same event, we need to host speakers from the location of their choice.\n\nWhat we did?\n\nAfter a few exchanges with other conference organizations, we decided to give Streamyard a chance.\nAfter some quick tests, we bought a license and started the new version of the LFT.\n\nWhat is Streamyard?\n\nStreamyard is a live streaming platform that runs directly in the browser.\n\nIt does not have all the customization and possibilities of OBS, but its huge advantage is its versatility.\nThis solution allows Bedrock employees to manage the LFT from any computer.\n\nHere are some sample images and overviews of the Streamyard UI:\n\n\n\n\n\nThe positive points of Streamyard:\n\n\n  Allows us to have several people in the control room at the same time\n  Streams in 1080p\n  Personalization: manage banners, chat questions, music on hold,…\n  Handles multi-speakers management\n  Re-stream to Youtube / Facebook / Twitter / Linkedin / Video recording\n  A free trial mode allows one to test it before taking out their credit card.\n\n\nNew setup\n\nAs a reminder, here is the organization of our auditorium during the live broadcast of our LFT.\nThe room is large enough to accommodate all Bedrockers who wish to attend in person the presentations of their colleagues. \nThe remote speakers have their conference broadcasted on the two screens of the room.\n\n\n\n\n\n\n\nWhat has changed since we switched to Streamyard is mainly related to the way we capture the image and sound of what is displayed on the screen.\nPreviously, we were using an Elgato HD60S+ device, to capture the HDMI output from the speaker broadcast. \nHowever, we had to try several times to get it to work each time we switched speakers. Not fun at all.\n\nNow, with Streamyard, every time we switch speaker, we just need to:\n\n  share the link of the “Streamyard broadcast”\n  The speaker joins the stream (and switches off her microphone and webcam)\n  Connect the HDMI cable of the speaker’s computer\n  The speaker displays her screen via the room’s projectors\n  The control room operator puts the RF microphone on the speaker(s)\n\n\nFor a remote speaker, it’s even easier.\nJust pass them the Streamyard link and they can use their webcam and their own microphone.\n\nThis is what our Stream setup now looks like with the cabling:\n\n\n\n\n\nWhat did we achieve?\n\nLFT is also a group of volunteers who give their energy to offer the best event possible, to offer to all Bedrock members a space where they can share their passion, technical subjects and others.\nFrom the proposal of their subject, through rehearsals and during the broadcast, the team is there to help speakers – either beginners or confirmed.\n\nThe switch to 720p and then to 1080p has been a real positive point for the quality of the live show, but also for the recording and the replay.\nMore than 200 participants during the last LFT on the day, the organizing team is delighted.\n\nThe simplification of the broadcasting setup since the switch to Streamyard has also made it easier to set up the room.\nSwitching from one speaker to another is less complex and can be done in just a few minutes.\n\nThe youtube lives allow participants to pause and rewind the broadcast.\nThis is really convenient for them.\nAlso, by going through the youtube chat, we can share questions directly on the screen and on the replay.\n\nThe LFT replays are also available on Bedrock’s Youtube channel in a private way accessible to all employees.\n\n\n\nNext steps\n\nWe don’t want to stop there.\nFor the next editions, we will try to do even better.\nWe are working on the matter of sharing some talks in public on our Youtube channel, so more people in our communities can learn from them.\nIn order to simplify the setup, we will try to put in place a more fixed table to avoid wiring and moving furniture.\nWe also wish to propose and train our employees to the use of this setup in order to allow us to host meetups and conferences in the best conditions.\n\nNow, it’s your turn: if your company or user-group does this kind of talks, how do you manage broadcasting and recording?\n"
} ,
  
  {
    "title"    : "⚡️ Vite ⚡️ the Webpack killer",
    "category" : "",
    "tags"     : " conference, js, webpack, vite, devoxx",
    "url"      : "/2022/04/21/vite-the-webpack-killer.html",
    "date"     : "April 21, 2022",
    "excerpt"  : "Toute application web a besoin d’être packagée afin d’être livrée en production. Pour répondre à cette problématique, de nombreux outils, connus sous le nom de modules bundler, sont apparus, et ces dernières années, c’est Webpack qui semble s’être...",
  "content"  : "Toute application web a besoin d’être packagée afin d’être livrée en production. Pour répondre à cette problématique, de nombreux outils, connus sous le nom de modules bundler, sont apparus, et ces dernières années, c’est Webpack qui semble s’être imposé comme l’outil incontournable.\n\nOn ne va pas se le cacher, si vous avez mis les mains dans une configuration webpack, c’est loin d’être un outil simple ni rapide.\n\nÇa n’a pas échappé à Evan You, le créateur de Vue.JS, qui voulait répondre à ces problématiques avec une nouvelle façon de procéder, avec des idées novatrices, reposant sur les dernières fonctionnalités des navigateurs : Vite.\n\nQuelles sont ces idées novatrices à la base de Vite ? En quoi concurrence-t-il Webpack ? C’est ce que nous allons voir dans ce talk et live coding!\n"
} ,
  
  {
    "title"    : "Bedrock à l&#39;AWS Summit 2022",
    "category" : "",
    "tags"     : " aws, summit, cloud, sysadmin, conference, kubernetes",
    "url"      : "/2022/04/20/aws-summit-2022-notre-retour-dexperience.html",
    "date"     : "April 20, 2022",
    "excerpt"  : "\n\nRetour à l’AWS Summit \n\nDeux années se sont écoulées depuis le dernier AWS Summit à Paris, il a fait son retour ce 12 avril !\nCet évènement, qui a lieu au printemps dans plusieurs pays, est l’occasion de rencontrer la communauté AWS française, d...",
  "content"  : "\n\nRetour à l’AWS Summit \n\nDeux années se sont écoulées depuis le dernier AWS Summit à Paris, il a fait son retour ce 12 avril !\nCet évènement, qui a lieu au printemps dans plusieurs pays, est l’occasion de rencontrer la communauté AWS française, d’assister à de nombreuses conférences et de bénéficier de retours d’expérience d’autres clients.\nC’était aussi pour nous, comme en 2019, l’occasion de partager les nôtres !\n\nDepuis notre migration vers le Cloud, AWS et Kubernetes entre 2018 et 2021 (plus d’informations dans Le Plan Copenhague), nous sommes plusieurs centaines à travailler au quotidien avec AWS.\nCette année, cinq de nos DevOps, SysOps et Développeurs ont eu la chance de se rendre à l’AWS Summit.\n\nNous partageons quelques notes que nous avons prises lors de cette journée, sur des sujets qui nous ont marqués et qui vont sans doute nous occuper une partie de l’année à venir.\n\nSommaire\n\n\n  Retour à l’AWS Summit\n    \n      Rendez vos équipes de Data Science 10x plus productives avec SageMaker\n      Comment le cloud permet à France Télévisions d’innover dans la diffusion de contenu live\n      Découvrez comment Treezor utilise AWS comme moteur de sa plateforme de Banking-as-a-service\n      Tracer votre chemin vers le Modern DevOps en utilisant les services AWS d’apprentissage machine\n      Innover plus rapidement en choisissant le bon service de stockage dans le cloud\n      Sécuriser vos données et optimiser leurs coûts de stockage avec Amazon S3\n      Minimiser vos efforts pour déployer et administrer vos cluster Kubernetes\n      Serverless et évènement, les nouvelles architectures\n    \n  \n  Nous étions aussi intervenants\n    \n      Transformer le load balancing pour optimiser le cache : objectif 50 millions d’utilisateurs\n      Etes-vous bien architecturé ?\n      Préparez et donnez votre premier talk\n    \n  \n  Conclusion de l’article\n\n\nRendez vos équipes de Data Science 10x plus productives avec SageMaker \n\nConférence présentée par :\n\n\n  Olivier Sutter - AWS Solution Architect\n  Yoann Grondin - IA Team Leader Canal+\n\n\n\n\nCette conférence présentait le produit Amazon SageMaker, d’abord dans sa globalité, puis appliqué au cas des équipes de Data Scientist chez Canal+.\n\nSageMaker a été lancé fin 2017 pour créer, entraîner et déployer des modèles de machine learning. C’est une solution tout-en-un, avec une interface graphique intuitive et un accent porté sur l’automatisation. Amazon promet également de gagner en performance sur SageMaker via l’implémentation de nombreux algorithmes d’apprentissage supervisés ou non-supervisés (XGBoost, kNN, PCA…).\n\nDe manière générale, les équipes de Canal+ utilisent des solutions d’apprentissage pour différents cas d’usages :\n\n\n  Personnaliser l’expérience utilisateur et proposer du contenu ciblé\n  Mieux connaître, labelliser, classifier leurs contenus vidéo\n  Anticiper les besoins des abonnés ou des prospects\n\n\nIls se sont tournés vers SageMaker pour diminuer le temps passé dans les étapes de preprocessing, data cleaning et déploiement en production.\n\nChez Bedrock, nous avons aussi rencontré ces problématiques, nous avons réalisé des PoC de différentes solutions (dont SageMaker) et nous avons retenu la plateforme Databricks.\n\nEn effet, Databricks répond à nos besoins de fine-tuning des paramètres des clusters Spark et d’intégration avec Terraform (ce qui est important pour nous car nous utilisons exclusivement de l’Infra-as-Code). Nous avons également automatisé le déploiement en production de nos modèles d’apprentissage, de la même manière que SageMaker.\n\nCette conférence nous a conforté dans l’approche et l’utilisation que nous avons de nos outils actuels, tout en nous confrontant à d’autres solutions techniques et d’autres cas d’usages au sein de notre industrie.\n\nRésumé par Gabriel FORIEN - DevOps\n\nComment le cloud permet à France Télévisions d’innover dans la diffusion de contenu live \n\nConférence présentée par :\n\n\n  Raphaël Goldwaser - AWS Solution Architect\n  Guillaume Postaire - Directeur de la Media Factory, France Télévisions\n  Matthieu Parmentier - Responsable de l’Al factory, France Télévisions\n  Nicolas Pierre - Al factory Lead Tech, France Télévisions\n\n\n\n\nNous avons assisté à une conférence présentée par les responsables Média et AI de France Télévisions et Raphaël Goldwaser Solutions Architect chez AWS. Ils nous ont parlé de l’évolution de leur usage du cloud dans la diffusion de vidéo en direct et des différentes étapes de la construction d’un système de sous-titrage automatique en direct.\n\nPour ce système de sous-titrage, ils utilisent les services Media &amp;amp; Entertainment fournis par AWS.\n\nLes flux vidéo sont envoyés directement dans le cloud via Elemental MediaConnect pour générer des sous-titres automatiquement en utilisant Media-Cloud AI et Speechmatics. Une fois les fichiers de sous-titres générés, ils sont insérés et synchronisés sur le flux en direct.\n\nCes outils peuvent également être utilisés pour analyser des vidéos afin de contextualiser les publicités affichées et/ou choisir le meilleur moment pour les afficher.\n\nChez Bedrock comme chez France Télévisions, nous challengeons régulièrement les solutions Médias proposées par AWS pour améliorer nos infrastructures et apporter de nouvelles fonctionnalités à nos produits.\n\nRésumé par Christian VAN DER ZWAARD - SysOps\n\nDécouvrez comment Treezor utilise AWS comme moteur de sa plateforme de Banking-as-a-service \n\nConférence présentée par :\n\n\n  Armel Negret - AWS Central Sales Representative\n  Nicolas Bordes - Technical Lead and AWS Sponsor, Société Générale\n\n\nTreezor, une filiale du groupe Société Générale, fournit une plateforme complètement APIsée qui permet aux fintechs et plus généralement aux acteurs de la finance d’accéder à leurs services bancaires. Cette plateforme est hébergée sur AWS et utilise une stack de services entièrement serverless : API Gateway, CloudWatch, Lambda, SNS et SQS entre autres. Les Lambdas sont développées en PHP grâce au framework Bref.\n\nA l’instar de Treezor, Bedrock possède également de nombreuses Lambda développées en PHP avec Bref. Ces Lambdas sont majoritairement déployées via le framework serverless et maintenues par le pôle backend. Au pôle infrastructure, nous essayons d’utiliser d’autres langages comme Python ou Go avec lesquels nous sommes plus à l’aise et qui sont nativement supportés par Lambda.\n\nL’approche “full serverless” est intéressante car elle permet de s’abstraire de la gestion de l’infrastructure sous-jacente et donc de se concentrer sur des problématiques intrinsèques au métier.\nEn sus, les services AWS serverless apportent souvent nativement de la haute disponibilité ainsi que de l’auto-scaling, deux problématiques très importantes pour garantir un service de qualité à nos utilisateurs finaux. C’est pour ces raisons que Bedrock utilise de nombreux services AWS serverless : Athena, CloudWatch, DynamoDB, Lambda, S3, SNS, SQS, Kinesis, …\nLe pôle infrastructure de Bedrock étant relativement “petit” par rapport au nombre total de développeurs (23 devops/sysops pour 250 fullstack en date du 15 avril 2022), l’utilisation du serverless est un réel enjeu business.\nServerless ne répond pas à tous les besoins non plus, particulièrement sur de très forts pics de charge où nous préférons utiliser Kubernetes.\n\nRésumé par Timothée AUFORT - DevOps\n\nTracer votre chemin vers le Modern DevOps en utilisant les services AWS d’apprentissage machine \n\nConférence présentée par :\n\n\n  Patrick Lamplé - AWS Principal Specialist SA\n\n\nLa conférence nous proposait d’en apprendre un peu plus sur les nouveaux produits AWS Code Guru et DevOps Guru.\n\nCode Guru\nse découpe en deux parties :\n\n\n  Reviewer, qui a pour ambition d’accélérer la revue de code ;\n  Profiler, qui peut aider à optimiser les performances d’une application.\nA ce jour, ces services ne supportent que les langages Python et Java.\n\n\nDevOps Guru\npermet d’identifier les comportements anormaux des applications au runtime.\nPar exemple, si une application utilise une table DynamoDB qui n’est pas suffisamment provisionnée, une alerte va être déclenchée. Cette dernière pourrait permettre d’identifier un souci de configuration avant même que l’application ne soit déployée en production.\n\nChez Bedrock, les langages utilisés étant principalement Javascript, PHP et Python, Code Guru ne sera pas une solution adéquate dans toutes les situations. Nous avons donc mis en place la solution KICS qui nous permet, à l’aide de règles Open Policy Agent, d’effectuer automatiquement de nombreuses validations sur le code infrastructure (Terraform, Docker, YAML, …). KICS est utilisé au travers de GitHub Actions pour ajouter des commentaires sur les pull requests comme Code Guru est capable de le faire.\n\nUne analyse au runtime effectuée par DevOps Guru pourrait permettre de venir compléter la liste de services AWS que nous utilisons déjà et qui vérifient la configuration de notre infrastructure comme : Config, Trusted Advisor, CloudWatch,  …\n\nLes outils du Modern DevOps d’AWS pourraient venir en complément d’outils de qualité actuellement utilisés chez nous. À tester en complément de KICS pendant une de nos journées R&amp;amp;D (journées organisées le dernier vendredi du mois, un mois sur deux).\n\nRésumé par Valentin CHABRIER &amp;amp; Mickaël VILLERS - DevOps\n\nInnover plus rapidement en choisissant le bon service de stockage dans le cloud \n\nConférence présentée par :\n\n\n  Thomas Barandon - AWS Enterprise Support Manager\n  Laurent Dirson - Directeur des Solutions Business et des Technologies, Nexity\n\n\nThomas Barandon a rappelé les solutions de stockage d’AWS : S3 pour du stockage objet, EBS pour le stockage bloc et EFS/FSx pour le stockage fichier.\nIl a par la suite présenté Storage Gateway, qui permet d’utiliser les services de stockage AWS dans une infrastructure on-prem via un montage NFS/Samba ou iSCSI.\n\nLaurent Dirson de chez Nexity a ensuite partagé la stratégie adoptée pour concevoir leur SI comme un service. Tous leurs documents sont désormais stockés dans un bucket S3 mis à disposition des agences via un montage NFS opéré par l’outil Storage Gateway. Une politique d’Object Lock permet d’utiliser le modèle WORM (write-once-read-many).\n\nChez Bedrock, nous stockons déjà la grande majorité de nos données dans des buckets S3. Nous aimerions aussi bénéficier des avantages de ce service pour le stockage de nos métriques. Mais, comme nous utilisons VictoriaMetrics, ce mode de stockage n’est pas disponible et ces données sont stockées dans EBS. Peut-être que Storage Gateway nous permettrait d’écrire nos métriques directement sur un bucket S3 ?\n\nRésumé par Coraline PETIT - SysOps\n\nSécuriser vos données et optimiser leurs coûts de stockage avec Amazon S3 \n\nConférence présentée par :\n\n\n  Meriem Belhadj - AWS Storage Specialist Solutions Architect\n\n\nPendant cette présentation, Meriem Belhadj est revenue sur les classes de stockage disponibles sur S3, en mettant une attention particulière à Glacier Instant Retrieval et à Intelligent-Tiering. Le second permet d’appliquer une politique de stockage basée sur la fréquence d’accès aux données au cours des 30 derniers jours.\nEn effet, pour déterminer la “bonne” classe à utiliser, il faut notamment connaître la disponibilité des données. Les autres points à prendre en compte sont la fréquence d’accès, les performances recherchées, la taille des objets à stocker et enfin la durée de rétention.\n\nNous appliquons ces pratiques chez Bedrock depuis plusieurs années. Toutefois, il serait judicieux de mettre en place des règles, type AWS Config, pour s’assurer que ces recommandations soient bien appliquées sur tous nos buckets S3.\n\nRésumé par Coraline PETIT - SysOps\n\nMinimiser vos efforts pour déployer et administrer vos cluster Kubernetes \n\nConférence présentée par :\n\n\n  Abass Safouatou - AWS Lead Solution Architect\n  Sébastien Allamand - AWS Solution Architect Specialist Container\n  Patrick Chatain - CTO Contentsquare\n\n\nContentsquare, analyste de l’expérience numérique, est venu nous parler de son utilisation d’EKS Blueprint avec AWS CDK (Cloud Development Kit) pour la configuration et le déploiement de leurs infrastructures Kubernetes. Cet outil leur a permis de rapidement migrer leurs infrastructures dans le Cloud.\n\nÀ l’occasion de cette conférence, un début de comparatif a été amorcé entre les solutions de passage à l’échelle automatique : Cluster Autoscaler et Karpenter.\n\nCette analyse a particulièrement attiré notre attention : nous souhaitons migrer nos clusters Kubernetes, actuellement déployé par Kops vers des clusters EKS, pour gagner en maintenabilité et en rapidité de scaling. Karpenter est l’une des solutions que nous étudions dans le cadre de ce projet, afin de tirer partie de cet outil qui a été développé par AWS et qui semble mieux tirer profit des fonctionnalités spécifiques d’AWS que Cluster Autoscaler.\n\nContentsquare a mentionné son besoin de développer un outil de passage à l’échelle basé non pas sur la consommation CPU et mémoire mais sur des métriques custom. C’est un besoin que nous avons également chez Bedrock, nous avons donc développé un outil pour y répondre, vous trouverez plus de détails dans l’article de blog dédié à cet outil. Cette remarque nous a conforté dans notre volonté de continuer à open-sourcer les outils que nous développons, pour qu’ils bénéficient à la communauté.\n\nRésumé par Coraline PETIT &amp;amp; Christian VAN DER ZWAARD - SysOps\n\nServerless et évènement, les nouvelles architectures \n\nMainframe, monolithe, système distribué, microservices, la conception d’un SI ou d’un projet est en constante évolution.\n\nCeci dit, depuis plusieurs années, beaucoup d’entreprises font la transition sur des architectures orientées évènements pour limiter les couplages forts entre les microservices.\nÀ cela s’ajoute l’essor du tout Serverless : fini le temps où on gérait nous-mêmes le dimensionnement de nos serveurs.\n\nCette année, plusieurs conférences étaient consacrées à ces sujets.\n\n3 designs patterns pour bien démarrer avec Serverless\n\nMatthieu Napoli, Hero AWS Serverless et créateur de la librairie Bref, a présenté trois designs patterns pour bien démarrer avec Serverless.\n\nApplication HTTP\n\nIl est maintenant très simple et peu coûteux de créer des applications HTTP en Serverless, en combinant différents services AWS :\n\n\n  Lambda function et Lambda function URL (nouvelle fonctionnalité sortie une semaine avant le Summit) ;\n  CloudFront CDN ;\n  API Gateway ;\n  S3.\n\n\nUn exemple concret :\n\n\n  CloudFront CDN délivre les assets JS/CSS/image depuis S3 ;\n  il transmet également les retours d’API Gateway ;\n  et API Gateway communique avec la/les Lambdas.\n\n\nAjoutons une base de données DynamoDB ou Aurora et nous voilà avec une application full Serverless.\n\nFile de messages avec worker\n\nLorsqu’on met en place ce type de pattern, nous déployons :\n\n\n  un projet qui pousse un message dans une file “producer” ;\n  une file de messages (SQS/SNS/MQ) ;\n  un second projet qui lit les messages depuis la file “consumer”.\n\n\nDans cette architecture, le “consumer” se connecte à la file de messages, lit les messages et se charge de toute la gestion d’erreur et de retry.\n\nEn utilisant une Lambda comme “consumer”, AWS a mis en place une intégration spécifique entre les files de messages et les Lambdas. C’est maintenant la file de messages qui appelle directement la Lambda en lui donnant le message et qui gère également le retry : votre code applicatif est déchargé d’autant de responsabilités sans valeur métier.\n\nCommunication entre microservices\n\nQuoi de plus contraignant que de gérer la communication de plusieurs services ? Il faut gérer :\n\n\n  les erreurs : que faire si plusieurs microservices partent en timeout ou échouent dans un workflow ?\n  le couplage : lors de la création d’un nouveau microservice, il doit être lui aussi appelé dans les chaines d’appels ;\n  l’authentification entre les différents services ;\n  et la latence : les appels de services en cascade augmentent la durée totale d’exécution.\n\n\nDe ce constat, Matthieu propose une solution que nous avons déjà mise en place chez Bedrock depuis plusieurs années : communiquer avec des évènements.\nPour cela, AWS fournit EventBridge : un service serverless de routage d’évènements sans stockage.\nAinsi, si un microservice doit en informer d’autres, il lui suffit d’envoyer un événement dans EventBridge. Les autres services n’auront qu’à “écouter” l’événement.\nSNS, plus ancien, permettait la même approche, mais EventBridge propose de créer des règles de filtrage sur la totalité du message d’un événement.\n\nConstruire des applications serverless orientées événements\n\nNicolas Moutschen, Solution Architect AWS et Guillaume Lannebere de chez Betclic ont fait un retour d’expérience sur la mise en place de différents services serverless AWS orientés événements.\n\nBetclic absorbe à chaque match/course, une quantité énorme de données (plusieurs millions d’événements) en quelques minutes.\nPar exemple, lors d’un match de football, les paris sont effectués à tout instant : avant le match, à la mi-temps, dans les dernières minutes…\nLeur SI est donc soumis, fréquemment, à de très forts pics de charge pendant des laps de temps très courts.\n\nAfin d’éviter de provisionner énormément de machines pour se mettre à l’échelle, Betclic à fait le choix du full serverless. Les applications de paris et de paiement communiquent par des messages d’événements envoyés dans le service AWS SNS : les Lambdas reçoivent les messages et les traitent avec une mise à l’échelle quasi immédiate en fonction du trafic.\n\nRésumé par Fabien LALANNE - Développeur\n\nNous étions aussi intervenants \n\nNous aimons tout particulièrement apprendre en lisant des articles écrits par d’autres membres de notre communauté ou en assistant à des conférences présentées par d’autres clients. Il est donc normal et important pour nous, de partager aussi notre expérience, ce que nous faisons régulièrement, y compris sur ce blog.\n\nCette année, nous avons eu la chance d’intervenir et de partager avec notre communauté lors de trois conférences. Merci à AWS pour la confiance qui nous a été accordée !\n\nTransformer le load balancing pour optimiser le cache : objectif 50 millions d’utilisateurs \n\nVincent Gallissot @vgallissot, Lead Cloud Architect, a expliqué comment Bedrock a amélioré le Load Balancing chez AWS, pour optimiser le cache de sa diffusion de vidéos, avec comme objectif 50 million d’utilisateurs :\n\nDémarrage des conférences à l’#AWSSummit. Et voici un REX intéressant pour partager l’une des problématiques intéressantes pic.twitter.com/kuKGZZyE2A&amp;mdash; Akram BLOUZA (@akram_Blouza) April 12, 2022\n\n\nGuillaume Marchand, Senior Solutions Architect chez AWS a débuté notre talk en parlant de Load Balancing chez AWS, des différentes solutions et des bonnes pratiques, ainsi que des exemples d’architectures possibles. J’ai ensuite expliqué notre besoin de scaler des serveurs de cache et comment nous avons relevé ce challenge, en développant notamment Haproxy Service Discovery Orchestrator. Ce talk n’a pas été enregistré, mais les slides sont disponibles sur ce lien.\n\nEtes-vous bien architecturé ? \n\nPascal Martin @pascal_martin, Principal Engineer, est intervenu pour partager notre retour d’expérience client pendant une conférence de présentation du Well-Architected Framework :\n\nPour cette conférence, Rémi Retureau, Partner Management SA Lead chez AWS, a commencé par présenter les pratiques Well-Architected. Un ensemble de recommandations basées sur 10 ans d’expertise de Solutions Architects AWS.\nJe suis ensuite intervenu pour partager un retour d’expérience : comment nous utilisons Well-Architected Framework chez Bedrock pour nous aider à valider l’architecture de composants de notre plateforme, à prioriser des évolutions ou même, à en identifier de nouvelles.\nEn quelques mots : nous passons une revue Well-Architected une fois par an et, si nous ne nous posons pas explicitement l’ensemble des questions du Framework à chaque nouveau projet, nous l’intégrons de plus en plus à nos pratiques et habitudes.\nSi vous commencez à travailler sur AWS, le Well-Architected Framework et ses recommandations, bien que peut-être effrayantes au premier abord, sont un ensemble de bonnes pratiques qui vous aideront à concevoir et à construire une plateforme plus solide, plus résiliente et moins coûteuse.\n\nPréparez et donnez votre premier talk \n\nPascal est aussi intervenu, cette fois en tant qu’AWS Hero pour guider la préparation de vos talks :\n\nPour cette seconde intervention, j’ai choisi de parler d’un sujet qui n’est pas lié à AWS.\nJ’aime assister à des conférences : je le fais depuis très longtemps et j’apprends beaucoup ainsi.\nJe suis aussi toujours très content de voir d’autres speakers monter sur scène et partager leur expérience. Je sais que beaucoup de personnes, dans notre communauté, ont des connaissances et des idées géniales et j’aimerais qu’elles les partagent plus souvent !\n\nJe sais toutefois que cet exercice est effrayant et que se lancer sur scène pour la première fois est difficile. J’espérais donc, à travers ce talk déjà donné chez Bedrock lors d’un Last Friday Talks (une journée de conférences internes le dernier vendredi du mois, un mois sur deux) aider de nouvelles personnes à se lancer.\nL’idée vous intéresse mais vous n’avez pas pu assister à cette conférence ? Et bien, j’ai aussi écrit un livre pour vous accompagner : « Préparez et donnez votre première conférence (quand ce n’est pas votre métier) »\nEt j’ai hâte, l’année prochaine, de vous voir monter sur scène et partager avec notre communauté !\n\nConclusion de l’article \n\nAvec des milliers de participants et participantes, l’AWS Summit est toujours une excellente occasion d’échanger et d’apprendre. Nous étions également très heureux de pouvoir, cette année encore, partager notre expérience lors de trois interventions.\nCet événement était aussi le premier pour certains et certaines d’entre nous, une très bonne découverte !\n\nComme beaucoup d’autres speakers et entreprises rencontrés mardi, nous recrutons : des SysOps, des DevOps, des développeurs et des développeuses, une ou un FinOps. Vous voulez nous aider à construire et à faire grandir notre plateforme ? Nous avons encore de super projets et challenges, faites-nous signe !\n"
} ,
  
  {
    "title"    : "Comment faire un trailer vidéo qui déchire avec des technos web ?",
    "category" : "",
    "tags"     : " remotion, react, video, js, frontend, conference, lyonjs",
    "url"      : "/2022/04/04/comment-faire-un-trailer-qui-dechire-avec-des-technos-web.html",
    "date"     : "April 4, 2022",
    "excerpt"  : "Avec Antoine Caron on est allé mettre des paillettes dans les yeux des participants du LyonJS en leur montrant comment créer des vidéos avec des technos web ! ✨\n\nIl était une fois … 📖\n\nUn jour, alors que j’arrivais fraichement à Bedrock, j’ai eu l...",
  "content"  : "Avec Antoine Caron on est allé mettre des paillettes dans les yeux des participants du LyonJS en leur montrant comment créer des vidéos avec des technos web ! ✨\n\nIl était une fois … 📖\n\nUn jour, alors que j’arrivais fraichement à Bedrock, j’ai eu le malheur de demander à Antoine Caron ce sur quoi il bossait entre midi et deux et qui semblait fort l’amuser.\n\nSa réponse : “J’essaie de générer des vidéos en MP4 à partir de composants React, tu veux voir ?”\n\nAprès des heures à tester chaque fonctionnalité de Remotion, il était temps de présenter ça à la communauté Javascript de Lyon lors du meetup n°71 du Lyon JS ! 🦁\n\n\n  \n\n\n\nPas le temps de regarder le replay ? ⏱\n\nPour vous donner une petite idée de ce que l’on a fait, on vous partage un site qui génère dynamiquement des trailers vidéo en fonction d’un programme et d’une couleur ! 🤯\n\n\n  \n\n\n\nℹ️ On vous conseille quand même de regarder le replay, même le créateur de Remotion a aimé 😉\n\n\n  A demo of Remotion in French at @LyonJS!Thanks for organizing this awesome talk @Slashgear_ @CruuzAzul 😃https://t.co/xujfC7tR6e&amp;mdash; Remotion (@remotion_dev) April 3, 2022 \n\n\n\nOne more thing… Petite surprise du chef ! 👨🏻‍🍳\n\nVoilà un petit aperçu d’une vidéo surprise que l’on a fait grâce à Remotion uniquement avec des composants React ! (Si des gens sont nés avant 2000, ça doit vous rappeler quelque chose 😉)\n\n\n  \n\n\n\nVous trouvez ça incroyable et vous voulez essayer ? N’hésitez pas à venir nous montrer vos vidéos, ou directement sur twitter avec @Slashgear_ et @CruuzAzul 🎞\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #16",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/03/26/bedrock-dev-facts-16.html",
    "date"     : "March 26, 2022",
    "excerpt"  : "Sur ce début 2022 les équipes de Bedrock se sont lachées ça promet pour le reste de l’année.\nLes autres articles de cette série sont disponibles ici.\n\nPromis !\n\n\n\nL’arbre qui cache la forêt\n\n\n  Quand c’est la couleur des noms de fichiers qui te pe...",
  "content"  : "Sur ce début 2022 les équipes de Bedrock se sont lachées ça promet pour le reste de l’année.\nLes autres articles de cette série sont disponibles ici.\n\nPromis !\n\n\n\nL’arbre qui cache la forêt\n\n\n  Quand c’est la couleur des noms de fichiers qui te permet de te repérer dans l’arborescence du projet…\n\n\n\n\nLa boucle a bouclé\n\nLe débat est et sera éternel.\n\n\n  A: La meilleure extension de VSCode c’est d’installer Webstorm.\n\n  B: Et la meilleure extension de Webstorm c’est d’installer Phpstorm.\n\n  C: Et la meilleure extension de Phpstorm c’est d’installer vim.\n\n  D: La meilleure extension de Vim c’est d’installer Neovim.\n\n  E: La meilleure extension de neovim est le bloc-note.\n\n  F: Mon bloc-note c’est VSCode.\n\n\nQuand on va à la pêche à l’exception\n\nException message (1) :\n------------------------\nAttempted to load class &quot;TruiteException&quot; from namespace &quot;Bedrock\\Stores\\Infra\\HttpClient&quot;.\nDid you forget a &quot;use&quot; statement for another namespace?\n\n\n“Ce n’est pas un échec… ça n’a pas marché”\n\n\n  Y’a pas de bug, y’a juste un truc qui fonctionne pas\n\n\nLes bottleneck c’est la vie\n\n\n  La vie est un bottleneck\n\n\nAtlas, le géant ?\n\n\n  Qui c’est qui a déplacé la terre ?\n\n\nTranquilou bilou\n\n\n  Le pair programming n’est pas du programming pépère\n\n\nTester c’est …\n\n\n  A: Faire des tests, c’est quand même moins chiant à faire à deux\n\n  B: Ouais, comme le sexe\n\n\nLabyrinthe\n\n\n  Pourquoi vous vous faites chier à mettre les secrets dans SOPS. \nVous l’auriez mis dans votre doc, personne ne les aurait trouvé.\n\n\nVoilà qui voilà !\n\n\n  A: Je crois que la branche master elle est pas protégée.\n\n  …\n\n  Ah bah heureusement que je suis là pour tester\n\n  B: Le plus étonnant c’est que ‘A’ ait le droit de push non ?\n\n  A: Le plus étonnant c’est que j’ai quelque chose à push non ?\n\n  …\n\n  On a résolu une faille de sécurité sur le projet, grâce à mon investigation.\n\n  C: Le mec c’est inspecteur gadget en fait, on sait pas comment mais il finit par être utile.\n\n\n🕯\n\n\n  The Domain is a sanctuary\n\n\nQuand il y a trop de viennoiserie chez Bedrock\n\n\n  A: Fait gaffes tu es en ordre croissant, l’ordre pain au chocolat est bien meilleur\n\n\nMais oui c’est clair !\n\n\n  On complexifie pour faire plus simple !\n\n\nLa PRO-crastination\n\n\n  Lead: C’était qui le chargé du monitoring hier ?\n\n  A: C’était moi mais j’ai rien fait, mais si vous voulez je peux le refaire aujourd’hui.\n\n  B: Si tu veux je peux même t’aider à rien refaire.\n\n\nÇa sent un peu la poussière ici\n\n\n  A: Eh eh, là on a effectivement sur-gonflé l’estim … (mais le côté pas dans l’inconnu l’explique)\n\n  B: Ouais c’était carrément la peur de toucher à un projet legacy de chez legacy, qu’on ne connait pas, pas dans notre scope…\n\n  A: Mais pas de souci majeur pour un développeur qui pratiquait Symfony sous Pompidou (le code date de cette époque)\n\n\nUn nouveau mot\n\n\n  Hybriditance\n\n\nDouble hit combo\n\n\n  “I want to identify pages where users hit”, le developper traduit alors “Je veux identifier la page où l’utilisateur se frappe”\n\n\nUne histoire de dev enrhumé\n\n\n\nQui s’appeleriot le CPU\n\n\n  Il fait des soustractions, des additions… il fait des trucs de fous\n\n\nUn nouveau KPI\n\n\n  Alors mon piffomètre du sprint…\n\n\nC’est simple non ?\n\n\n  I talked to A that said that B said that C said that it is possible to parallelize steps on Jenkins.\n\n  So I asked B how to do that, saying that A said that he said that C said that it was possible to do it!\n\n"
} ,
  
  {
    "title"    : "How AWS Cloudfront is helping us deliver our Web streaming platform? - Part 1",
    "category" : "",
    "tags"     : " cloudfront, aws, cdn, node.js, react, javascript, frontend",
    "url"      : "/2022/03/08/cloudfront-web-streaming-platform-part-1.html",
    "date"     : "March 8, 2022",
    "excerpt"  : "A bit of context\n\nThe web is a major platform for the distribution of our customers’ content at Bedrock.\nMillions of users connect every month to watch their live, replay or the series and movies of their choice.\nThe broadcasting of sports events ...",
  "content"  : "A bit of context\n\nThe web is a major platform for the distribution of our customers’ content at Bedrock.\nMillions of users connect every month to watch their live, replay or the series and movies of their choice.\nThe broadcasting of sports events such as the Euro 2020 soccer tournament represents a real technical challenge when it comes to maintaining the stability and performance of such a platform.\n\nThe web application works in SSR (Server Side Rendering) mode: we have NodeJS Express servers returning pre-rendered HTML pages.\nWe made this choice several years ago, for two reasons: SEO and to improve the first display time on slow devices.\nIn addition to the HTML pages, the web platform is also a huge collection of assets that allow the website to function: Javascript bundles, CSS, images, manifests.\n\nToday our customers have users distributed over a large part of the globe.\n\nTo meet these challenges, a CDN is a very good solution.\n\nWhat is a CDN then ?\n\nCDN for Content Delivery Network is a service delivering content to users across the internet (here our HTML pages and assets).\nWherever the user is in the world.\nTo all users, even if they are many.\n\nCloudfront is the CDN service of AWS.\nWith a large number of POPs (Point Of Presence) around the world, it helps us provide a response to each user as close as possible to their location.\nThis allows us to significantly reduce the time to first byte of our responses.\nDifferent price classes allow you to choose the global “area” in which your application should be available in order to achieve savings.\n\n\n\nBeing in Lyon (France), we sometimes get answers from the POP of Milan (Italia).\nIndeed, Lyon ↔ Milan is almost as closer as Lyon ↔ Paris.\n\nNote that it is very easy to know which Cloudfront POP answered you.\nEach POP is identified by a three letter code that corresponds to the code of the nearest international airport (here: CDG corresponds to Paris Charles de Gaulle airport).\n\nx-amz-cf-pop: CDG50-C1\n\n\nDelivering content as close to the user as possible is great, it theoretically reduces waiting time but it does not solve the problem of heavy load.\n\nThe best solution for load problems is caching.\n\n\n  You put 1 second of cache-control, and you already won!\n\n  Y. Verry, our Head of Infrastructure and Ops\n\n\nCloudfront service makes it easy to cache responses at the edge servers.\nIf we take the example of sports broadcasting, users arrive in large numbers in a very short period of time.\nCaching (telling Cloudfront to cache a web page) takes a lot of the load off our Node servers because they are not called.\n\nCaching objects in Cloudfront is also about improving response times.\nNo need to wait for our servers, the user receives the cached object directly.\nCloudfront even takes advantage of this to apply more powerful compression algorithms like Brotli on these cached objects.\nThese compressions, performed directly by the CDN, allow you to drastically reduce the size of your objects on the network.\nReducing objects size make our applications load even faster for our users.\n\nHere is our Cache hit ratio in production on 6play.fr website.\n\n\n\nCloudfront also allows us to do “Edge computing”: run code directly in Amazon edges and POPs instead of doing it in our applications.\n\nLambda at edge (on regional edges servers), Cloudfront function (function that runs on POP servers), Web Application Firewall, here are some very cool features that will allow you to do usual manipulations on your requests/responses.\n\nFinally, by using regional Pop, hundreds of end server edges do not contact your origin (your application) when the cache is invalidated or exceeded.\nYou can even activate the Origin Shield feature that allows you to further limit the load on your origins.\n\n\n\nGood per-level cache management even allowed us to completely invalidate the cache of a Cloudfront distribution a few minutes before the start of an event without generating huge traffic on our servers.\n\n\n\nAnd that’s it for this first article, in the next part (and normally the last one) you will discover how we have implemented some patterns on our sites.\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  More efficient Load Balancing and Caching at AWS, using Consistent Hashing and HAProxy\n  Scaling Bedrock video delivery to 50 million users\n\n"
} ,
  
  {
    "title"    : "Streaming recommendations at Bedrock",
    "category" : "",
    "tags"     : " recommender systems, machine learning, data, data science",
    "url"      : "/2022/02/27/streaming-recommendation.html",
    "date"     : "February 27, 2022",
    "excerpt"  : "Personalised recommendations are everywhere. No exception for the streaming world. To improve user experience, recommender systems with machine learning are uplifting.\nAt Bedrock, until recently, there was no recommendation shaped this way.\n\nBut w...",
  "content"  : "Personalised recommendations are everywhere. No exception for the streaming world. To improve user experience, recommender systems with machine learning are uplifting.\nAt Bedrock, until recently, there was no recommendation shaped this way.\n\nBut we are writing a new story.\n\nA quick win solution\n\nWe wanted to find a way to get a solution that would be quick to integrate.\n\nWe chose to use Amazon Personalize. This service aims to construct recommender systems with machine learning. The promise is to Create real-time personalized user experiences faster at scale. Perfect! It was exactly what we were looking for.\n\nRapidly, we encountered an obstacle. You can’t deploy Personalize with Terraform. Yet, Terraform is the tool we use to manage our infrastructure.\n\nHow to deploy Amazon Personalize?\n\nAs Personalize is supposed to be a temporary solution in our stack, for once, we accepted not using Terraform. We developed a Python script to interact with Personalize. Apache Airflow schedules and monitors the script.\n\nPersonalize is a black box. You can’t have access to explanations about the generated models. But, with Personalize, you have different ways to evaluate your recommendations.\n\nWith recommender systems, using the offline metrics to judge your model is not enough. It’s better than nothing! But to check that a recommender system works, you need to evaluate it online with real users.\n\nWe have millions of users. Releasing a recommender system to all our users is definitely not the best idea ever.\n\nHow to release a recommender system?\n\nFirst of all, check offline metrics. They’re still a valuable hint. Then, analyse the recommendations with people from the editorialist team.\n\nNote that this kind of analysis is very subjective.\n\nFinally, deliver the functionality to a small portion of your users.\n\nWe configured an AB test that gives recommendations to 5% of users. With dashboards, we study the impacts.\n\nPerfect! We have a way to check the success of a full broadcast.\n\nBut how to be sure that the new product will support the load? We have millions of users. It means that 5% of users still represent a lot of people.\n\nHow to assess the performance of a recommender system?\n\nLaunch load tests. Today you have a myriad of tools to do that. At Bedrock, we use Artillery.\n\nDuring the load tests, we had a bug. We discovered that by default, the limit of requests per second with Personalize is 500. In our context, that’s not acceptable.\n\nWe asked Amazon to help us and they changed the option for us.\n\nThe results\n\nIf you’re a big fan of reality TV shows, you will see that:\n\n\n\nIf you prefer reports, you will see that instead:\n\n\n\nWhat now?\n\nWe’ve deployed an AB test for our first recommender system built with machine learning.\n\nWe don’t have the results of the AB test yet. But, we’ve noticed that many users interact with the recommendations.\n\nAfter different challenges, we nailed it.\n\nBut, Personalize is expensive and a black box that we can’t integrate with Terraform easily (we’ll have to develop something for that, at least). It doesn’t suit our context. That’s why we’ve started to develop our first models.\n"
} ,
  
  {
    "title"    : "Tonight&#39;s football time, let&#39;s prescale Kubernetes to avoid a crash!",
    "category" : "",
    "tags"     : " kubernetes, scaling, high availability, aws, cloud",
    "url"      : "/2022/02/03/prescaling.html",
    "date"     : "February 3, 2022",
    "excerpt"  : "Are you experiencing peak loads on your Kubernetes-hosted platform? Rest assured, you are not alone.\nAt Bedrock, we have developed a prescaling solution. It allows us to handle sudden and abrupt, but predictable, \ntraffic spikes, like soccer games...",
  "content"  : "Are you experiencing peak loads on your Kubernetes-hosted platform? Rest assured, you are not alone.\nAt Bedrock, we have developed a prescaling solution. It allows us to handle sudden and abrupt, but predictable, \ntraffic spikes, like soccer games.\n\nKubernetes provides HorizontalPodAutoscalers to handle traffic variations. \nWe’ll look at their limitations in the case of meteoric spikes in load and how prescaling helps us deal with the \nsudden arrival of several hundred thousand users.\n\nTable of Contents\n\n\n  Load and traffic vary\n  Beginning of the scaling problems\n  How does reactive scaling work in Kubernetes?\n    \n      An HorizontalPodAutoscaler\n      What scale out looks like in a real case\n      How fast is reactive scaling?\n    \n  \n  Prescaling… What is this about?\n  Prescaling our applications\n    \n      Enabling and configuring prescaling on an HPA\n      The prescaling exporter\n      How can the HPAs prescale?\n      Prescaling works!\n    \n  \n  What about special, huge, events?\n    \n      The prescaling API\n      Let’s see how an application scales during a very special event\n    \n  \n  Prescaling external services: another challenge\n\n\nLoad and traffic vary\n\nLoad and traffic have always varied over time on our platform:\n\n\nCPU per instance over time\n\nTo deal with these load variations, several tools help us to automatically adapt our Kubernetes clusters’ capacity:\n\n  HorizontalPodAutoscaler \n(HPA): adds/removes Pods (= capacity) on a workload resource such as Deployment or a \nStatefulSet.\n  Cluster Autoscaler: \nadjusts the size of a Kubernetes cluster by adding/removing nodes.\n  Overprovisioning: starts “empty” \npods (and new “useless” nodes, as a consequence), so the cluster has available capacity that will be used to start \napplications pods quicker.\n\n\n\n  If you wish to know more about which tools we use and why we use them in our Kubernetes clusters, I advise you to \ncheck out another dedicated blog post named \n“Three years running Kubernetes on production at Bedrock”.\n\n\nUnfortunately, all those tools are not sufficient to deal with heavy and sudden traffic spikes on some special \nevenings such as the final of a football game or a successful show. During this kind of events, users arrive massively, \nall at the same time. On some evenings, some of our applications see their load rise by 5 in 2 minutes, others even \nsee theirs multiplied by 10 in 2 minutes!\n\nThe predictable aspect of those arrivals is very important because it means we are able to prepare our platform \nbeforehand. That’s why our prescaling solution was born.\n\nBeginning of the scaling problems\n\nOn an ordinary evening, when the multiple Kubernetes scaling tools were kicking in, here is basically what was \nhappening:\n\n\n\nAs the load increased, our capacity was increasing as well and we always had spare capacity. We were able to double our \ninitial capacity every 5 minutes thanks to reactive scaling when load started to rise.\n\nAfter a while and on some special evenings, we began to see this kind of behavior:\n\n\n\nSometimes, load was increasing faster than what reactive scaling could handle, that is to say about x2 in capacity \nevery 5 minutes. The consequence is that we could not serve everyone. For a while, our platform would fail for some \nusers, until autoscaling kicked in or until load stopped rising so fast.\n\nLet’s see how reactive scaling works to understand how we can leverage it to prepare the platform in advance.\n\nHow does reactive scaling work in Kubernetes?\n\nAn HorizontalPodAutoscaler\n\nFor a Deployment to be autoscaled (reactively, according to varying load), the number of replicas of a Kubernetes \nDeployment is reconfigured by an HorizontalPodAutoscaler:\n\n\n\nIts manifest usually looks like this:\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\n# …\nspec:\n  # …\n  minReplicas: 2\n  maxReplicas: 100\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 70\n  - type: Pods\n    pods:\n      metricName: phpfpm_active_process_percentage\n      targetAverageValue: &quot;40&quot;\n  # …\n\n\nHere:\n\n  minReplicas is the minimum number of Pods that will run (under normal conditions).\n  maxReplicas is the maximum number of Pods that will run.\n  metrics is a list of metrics the HPA will analyze to determine if it must scale or not. If usage gets higher \nthan the target specified for a metric, the HPA will add new Pods (= scale out). If usage gets lower than all targets \nspecified for all metrics, the HPA will remove Pods (= scale in).\n\n\nWhat scale out looks like in a real case\n\nAt Bedrock, we use three types of metrics to scale: Resource, Custom and External. Here is an example of \nan application scaling out on a Custom metric:\n\n\nProcess status (%) over time\n\nIn this graph, around 20:52:30 and 20:55:00, the percentage of active processes rises to go above \nthe target 40 we saw in the YAML example before. This triggers the scale out of the HPA of this application:\n\n\nPod status over time\n\nWe can see that around 20:53:00 (30 seconds after we first go above the target), the number of unavailable pods rises. \nThe scale-out has just started. A few moments after, the pods become available and are able to serve users.\n\nHow fast is reactive scaling?\n\nScaling in Kubernetes with HPA is not instantaneous:\n\n\n\n\n  Metrics are updated every 30 seconds (for CPU or memory) or every 60 seconds (for external and custom metrics) or so.\n  HPA analyses the metrics to know if scale-out is required every 30 seconds or more.\n  We might not have enough EC2 servers running to host the new Pods that are trying to start (we usually have between \n10% and 20% of spare capacity). If starting new EC2 servers is necessary, this takes between 2 and 4 minutes.\n  Pods (as our applications and Docker images are big) usually need between 30 and 60 seconds to start.\n\n\nThis means when a spike in load occurs, we need between 1’00’’ and 6’00’’ for new Pods to be able to handle that load. \nScaling is clearly not instantaneous.\n\nPrescaling… What is this about?\n\nMost our applications deployed in Kubernetes use an HorizontalPodAutoscaler. As we’ve seen, this autoscaling \nmechanism is reactive in nature: Pods are added when load (or another metric) gets higher than a target. \nIt can handle load that increases slowly (say, instant +20% or x2 in 5 minutes), but not huge instantaneous spikes \n(say, x10 in less than 5 minutes). It works fine for most of our usual workloads, but cannot absorb spikes we receive \nduring special events like Top Chef or when the France football team plays.\n\nThe only way we can handle a huge and sudden spike in traffic on an application is by pre-provisioning capacity. \nIn Kubernetes, this is done by running more Pods than necessary so that they are ready to handle the additional load. \nRunning additional, mostly superfluous, capacity has a cost…\n\nWe know our applications receive a sudden and brutal traffic spike once a day, between 20:50 and 21:00 Paris time. \nNot taking special events into account, that’s the only time load increases violently enough for reactive autoscaling \nto be unable to handle it – and, most days, it actually does quite fine. So, we could pre-provision more capacity \naround that time (only), and not pay for it the rest of the day…\n\nPrescaling our applications\n\nEnabling and configuring prescaling on an HPA\n\nTo enable and configure prescaling on an HorizontalPodAutoscaler, we add two sets of information:\n\n  Three annotations to define:\n    \n      Time when prescaling starts.\n      Time when prescaling stops.\n      The minimum number of Pods we want between those two times.\n    \n  \n  A new metric to scale on.\n\n\nThe annotations are set in the metadata block:\n# …\nmetadata:\n  # …\n  annotations:\n    annotations.scaling.exporter.replica.min: &quot;25&quot;\n    annotations.scaling.exporter.time.start: &quot;19:30:00&quot;\n    annotations.scaling.exporter.time.end: &quot;23:30:00&quot;\nspec:\n  scaleTargetRef:\n    # …\n    name: &quot;service-6play-images&quot;\n\n\nIn this example, we indicate we want at least 25 pods between 19:30:00 and 23:30:00. As a result, \nthe number of pods of this application will rise up to at least 25 pods during this period of time.\n\nTimes are expressed in the local timezone of the Kubernetes cluster as prescaling is linked to events on the platform. \nThose events are usually linked to events on live TV and we deploy one cluster per TV broadcaster.\n\nIn the metrics block, you need to configure a new External metric:\n# …\nmetrics:\n # …\n - type: External\n   external:\n     metricName: &quot;annotation_scaling_min_replica&quot;\n     metricSelector:\n       matchLabels:\n         deployment: &quot;service-6play-images&quot;\n     targetValue: &quot;10&quot;\n\n\nThe label used for deployment must be set to the value of scaleTargetRef.name (= the name of the Deployment the \nHPA reconfigures). The targetValue must always be set to 10.\n\nYou now know how we configure an HPA for prescaling. What you do not know yet is how the HPA annotations are used \nand which application exposes the metrics called annotation_scaling_min_replica. It’s now time to talk about the \nprescaling exporter.\n\nThe prescaling exporter\n\nThe prescaling exporter is a Prometheus exporter we developed (in Python). \nIt exposes metrics, used to scale Kubernetes applications on a day-to-day basis during a given time range.\n\n\n  Prometheus is one of the tools we use in our monitoring stack at Bedrock. One of its purposes, among others, is to \ncollect metrics from Pods. This article will not present our Prometheus stack in detail.\n\n\nHere is how the prescaling exporter works:\n\n\n\n\n  Every 15 seconds or so, Prometheus scrapes the prescaling exporter pod to get the metrics it exposes.\n  Scrapping triggers the generation of the metrics. Before they are exposed, the exporter first \ncalls the k8s API to list all HPAs in the cluster.\n  Then, it will:\n    \n      Filter those HPA with the required annotations (the annotations we added a bit earlier on an HPA).\n      Calculate the new annotation_scaling_min_replica metric for each HPA with the prescaling annotations.\n      The prescaling exporter can now expose the metrics.\n    \n  \n  And Prometheus can retrieve them.\n\n\nHow does the prescaling exporter calculate the metrics of the HPA subscribed to the prescaling? Well, it depends \non the content of the annotations you configured on the HPA.\n\nHere is how a Prometheus metric of the prescaling exporter looks like:\n\n\n\nIn each metric, you will find several labels. I chose to put only one here to simplify.\nThe metric can take one of three values, depending on the content of the annotations we saw earlier:\n\n  When we are not within the time range of the prescaling annotations of the HPA, it means that we do not \nneed to prescale. As a result, the metric is set to 0.\n  If we are within the time range of the prescaling annotations, it means we are in the prescaling time range. \nFrom there, two possibilities:\n    \n      If “Current number of replicas” &amp;lt; “Number of minimum replicas in the HPA annotation”, we need to add replicas so \nthe metric is set to 11.\n      If “Current number of replicas” &amp;gt;= “Number of minimum replicas in the HPA annotation”, we have enough replicas so \nthe metric is set to 10.\n    \n  \n\n\n\n  When the metric is set to 10 and we already have enough replicas running, the number of replicas will never go below \nthe minimum chosen in the prescaling annotation annotations.scaling.exporter.replica.min.\n\n\nHere is what it looks like on Grafana for the application service-6play-images during the evening:\n\n\nannotation_scaling_min_replica over time\n\nIn this example:\n\n  Until 19:30, annotation_scaling_min_replica is set to 0.\n  From 19:30 until about 19:35, annotation_scaling_min_replica is set to 11 (scale-out will happen).\n  From 19:35 until 00:00, annotation_scaling_min_replica is set to 10 (scale-out is done, we have enough Pods).\n  From 00:00 until the following evening, annotation_scaling_min_replica is set to 0 again.\n\n\nWe can guess two things from this example:\n\n  The prescaling period for this application was 19:30 until 00:00.\n  It took about 5 minutes to prescale (= add more Pods) the application.\n\n\nHow can the HPAs prescale?\n\nTo understand how the HPAs can prescale, we need to talk about \nthe prometheus adapter. It is an implementation \nof the Kubernetes metrics APIs. We use it to expose custom and external metrics for HPAs to use in order to scale:\n\n\n\n\n  Prometheus adapter collects metrics from prometheus once every minute and exposes them as External and\nCustom metrics.\n  The HPA controller manager fetches metrics provided by metrics-server and prometheus-adapter to scale out or \nscale in Deployment/Replica Set/Stateful Set resources. To do so, it uses k8s aggregated \nAPIs (metrics.k8s.io, custom.metrics.k8s.io and external.metrics.k8s.io). \nMetrics-server provides resource metrics (only CPU and memory). \nPrometheus adapter provides all non-resource metrics (external and custom).\n\n\nFor more information: HPA Kubernetes documentation.\n\nPrescaling works!\n\nAfter prescaling has been deployed into production and teams started to add annotations in their projects, \nthis is what happened:\n\n\nNumber of pods regarding their status over time\n\nAround 19:30, the number of pods for this application goes from 25 to a bit more than 55. It means its HPA was \nscaled out, based on the prescaling metric of that application. Mission accomplished!\n\nWhat about special, huge, events?\n\nSome days, during very special events, “normal” prescaling was not enough to handle the load that was rising \nway faster than what we usually see on our platform:\n\n\n\nAs you can see, even with prescaling doing its job before the start of the TV program (we can see capacity \nrising at the beginning of the graph), the traffic rises so quickly at 20:55 that we are still unable to scale fast \nenough to serve all users.\n\nFor these special events, we have developed an additional mechanism that allows us to set a multiplication coefficient \nto all prescaling. We use it to say “I want a 5x higher minimum number of Pods than what’s configured in the annotation \nwe’ve seen before, for all HPAs bearing this annotation”.\n\nTo deal with those very special events, we added another component in the prescaling stack: the prescaling API.\n\nThe prescaling API\n\nThe prescaling API is a backend application also developed in Python. It was designed to store prescaling \nsettings for future events on the platform in AWS DynamoDB. We chose DynamoDB because it’s a serverless database \neasily maintainable through Terraform code. By “event”, understand a football game or another big show such as \nTop Chef. Those settings define when and how we must enlarge the platform to sustain bigger \ntraffic spikes than on standard days (= on normal prescaling evenings).\n\n\n\nWith this API, our prescaling workflow has evolved:\n\n\n  Prometheus scrapes the prescaling exporter pod, same as before.\n  Scrapping triggers the exposition of the Prometheus metrics. Before the metrics are exposed, the exporter first \ncalls the Prescaling API server to get the current special event if there is one.\n  After calling the prescaling API, the exporter calls the k8s API (as before) to list all HPAs in the cluster.\n  Then, it:\n    \n      Filters those HPA with the required annotations.\n      Calculates the new annotation_scaling_min_replica metrics by merging information from the HPA annotations and \nthe special event from the prescaling-api server.\n    \n  \n  The prescaling exporter now exposes the metrics so that Prometheus can retrieve them.\n\n\nLet’s see how an application scales during a very special event\n\nHere is how the number of pods evolves with a special prescaling event configured:\n\n\nNumber of pods by status over time\n\nFor this specific application, we had around 25 pods during the day and standard prescaling was configured at \n40 pods in the HPA annotations. On normal days, we would have had about 40 pods throughout the evening. \nOn this particular day, we had around 125 pods: a ”x3” multiplier was applied, thanks to the prescaling API.\n\nPrescaling external services: another challenge\n\nReactive scaling still answers most of our needs. We are still able to do “x2 every 5 minutes” in Kubernetes. \nPrescaling is great, it can help critical applications to sustain sudden and expected traffic spikes we had \nproblems dealing with before. On top of that, prescaling for special events even allows us to deal with extreme cases.\n\nStill, the applications we prescale often depend on external services: a database, a cache, a search engine… \nMost of these external services will not prescale as easily as with our prescaling solution. \nSome services, like AWS DynamoDB or AWS Aurora serverless, come with a reactive autoscaling solution, \nbut not all of them. And still, even those autoscaling services have limits…\n\n\n\nVery special thanks to all my Bedrock Streaming colleagues who helped me improve this blog post.\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #15",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/12/20/bedrock-dev-facts-15.html",
    "date"     : "December 20, 2021",
    "excerpt"  : "Le père noël 🎅🏻 vous apporte en avance une hotte pleine de devfacts !\n\nTempora mori, tempora mundis recorda\n\n\n  L’autre jour, on allait au ski. C’était l’année dernière\n\n\nLe pass sanitaire de la prod\n\n\n  Un test de charge c’est comme un test PCR, ...",
  "content"  : "Le père noël 🎅🏻 vous apporte en avance une hotte pleine de devfacts !\n\nTempora mori, tempora mundis recorda\n\n\n  L’autre jour, on allait au ski. C’était l’année dernière\n\n\nLe pass sanitaire de la prod\n\n\n  Un test de charge c’est comme un test PCR, c’est valable 3h\n\n\nUn pair programming un peu trop random\n\n\n  _ “Tu peux aller sur un site web random pour vérifier un truc ?”\n\n  _ “P***hub ça te va ou ça te dérange ?”\n\n\nLa précision\n\n\n  Alors, pour X, tu as un gros tas d’éléments, pour Y un tas d’éléments, et pour Z, un petit tas d’éléments.\n\n\nOn n’est pas sorti du sable\n\n\n  “stdout n’existe plus”\n\n\nL’effet papillon\n\n\n  “Bon tu viens manger ?”\n\n  “Attends je suis d’astreinte et il y a des orages à Paris qui font sauter le live en Croatie”\n\n\nA new hope\n\nQuand tu envisages de refacto un bout de code, que tu regardes la PR à l’origine de ce code et que tu trouves un commentaire que tu avais laissé qui dit:\n\n\n  I really hope this will solve our problems and not introduce new ones.\n\n\nLe retour du first try\n\n\n\nUn bug ? Où ça un bug ?\n\n\n  Comme j’ai pas réussi à le résoudre, on peut le mettre en terminé\n\n\nAh la boulette !\n\n\n  Est-ce que vous savez si on peut abort un git rebase --abort ?\n\n\nRetour de congés compliqué\n\n\n  Possible de m’unlock mon compte Okta ?\n\n\nTODO\n\n\n\nUn bon conseil\n\n\n  Regardez le côté fonctionnel du succès\n\n\nLe re-retour du first try\n\n\n\nTout est planifié depuis le début\n\n\n  C’est prévu mais ça fait pas ce que je voulais.\n\n\nAh !\n\n\n  On lui a demandé “qu’est-ce qu’on en fait ?“, ils nous a répondu “rien”\n\n\nMon nom est Paco 🦜\n\n\n  il répète tout ce que tu dis … mais en faux !\n\n\nNe pas sous-estimer le miss-click\n\n\n  Tu peux pas miss-click trois fois de suite c’est pas possible, même en vacances tu peux pas.\n\n\nOn se la pète un peu\n\n\n  \n    \n      Ça montre bien qu’on est trop fort!\n    \n    \n      Ne confond pas le hasard avec une compétence!\n    \n  \n\n\nDifficulty Driven Design\n\n\n  Sommes-nous convaincus par le DDD ou des cons vaincus par le DDD ?\n\n\nL’importance de se relire\n\n\n  After more reflection I said bullshit.\n\n\nVers l’infini et l’au-delà\n\n\n  L’engineering manager m’a confirmé que l’on pourrait aller bien au delà, voire un peu plus…\n\n\nUn env de dev ? Pourquoi faire ?\n\n\n  \n    Tu as testé en local ?\n    Bien sûr que non. AH ça ne fonctionne pas en local\n  \n\n\nQuand on vient annoncer une bonne nouvelle\n\n\n  Bonjour ! Comment ça allait ?\n\n\nJ’y crois moyen\n\n\n  Pour une fois ça marcherait même mieux sous Windows.\n\n\nPlus simple que simple\n\n\n  Avec les nouvelles fonctionnalités, c’est compliqué de faire au plus simple.\n\n\nSa vision est basée sur le mouvement\n\n\n  Gris c’est bien parce que c’est ni vert ni rouge !\n\n\nLa concurrence est rude\n\n\n  \n    (P.O. Mobile) : “C’est moche”\n    (Tech Lead Android) : “C’est Android”\n    (Dev iOs) : “Si on peut même plus troller”\n  \n\n\nEddy malou en marque blanche\n\n\n  La customiseration\n\n\nLe crime parfait\n\n\n  Ah oui, y’a des voyous ici qui mettent des espaces insécables dans les noms de fonctions.\n\n\nOn l’avait pas vu venir celle-là\n\n\n  C’est hyper compliqué de prévoir des choses qu’on ne peut pas prévoir.\n\n\nC’est trop calme\n\n\n  C’est l’encéphalogramme de l’huitre ton dashboard là\n\n\nThree magic words\n\nÀ la cafétéria:\n\n\n  \n    👋 Il faut qu’on discute !\n    Si c’est pour parler boulot c’est même pas la peine !\n    Incident en prod\n    Ah oui j’ai vu passer une notif, je regarde ça de suite !\n  \n\n\nL’expérience ou l’age\n\nMontre un raccourci clavier dans un outil de gestion de tickets\n\n  T’inquiète pas, j’ai roulé ma bosse, je gère !\n\n\nLes geeks no-life\n\n\n  \n    tu peux bouger stp pour la lumière\n    pfff, j’ai bougé il y a pas 5mn\n  \n\n\nNe t’inquiète pas, on va trouver ça dans les log\n\n\n\nFatigue du soir\n\n\n  500 c’est pour succès, c’est ça ?\n\n\nEn rouge et noir !\n\n\n  Quand c’est rouge c’est que c’est OK.\n\n\nDEL\n\nQuand on supprime du code et qu’on explique comment on l’a fait dans la PR.\n\n\n  How?\n\n  Del key on the keyboard.\n\n"
} ,
  
  {
    "title"    : "Scaling Bedrock video delivery to 50 million users",
    "category" : "",
    "tags"     : " aws, cloud, sysadmin, HAProxy, video, high availability, Unified Streaming, VOD, OTT, video delivery",
    "url"      : "/2021/12/15/scaling-bedrock-video-delivery-to-50-million-users.html",
    "date"     : "December 15, 2021",
    "excerpt"  : "Here’s our journey to migrate tens of thousands of videos, accessed by millions of users, to the cloud. How we minimized our costs without losing the biggest benefit of the cloud: scaling.\n\nThe purpose of this article is to show you the evolution ...",
  "content"  : "Here’s our journey to migrate tens of thousands of videos, accessed by millions of users, to the cloud. How we minimized our costs without losing the biggest benefit of the cloud: scaling.\n\nThe purpose of this article is to show you the evolution of this cloud video delivery platform, from the first draft to the current version.\n\nTable of Contents\n\n\n  How we do streaming\n  Just In Time Packaging\n  Version 1: The quest for self\n    \n      Local cache with Nginx\n      Network Load Balancer: manages TLS and helps with scaling\n      Content Delivery Network: Keep it Simple and Stupid\n      A first conclusion: V2 needs Consistent Hashing\n    \n  \n  Version 2: Let the requests flow\n    \n      HAProxy to make Consistent Hashing\n      EC2 costs are reduced by using only Spot instances\n      Production launch on this V2\n      EC2-other: the financial abyss\n    \n  \n  Version 3: Cost Explorer Driven Development\n    \n      Be multi-AZ without inter-AZ traffic\n      Mono-AZ AutoScalingGroups\n    \n  \n  Optimizations\n    \n      Adapt HAProxy config for EC2 bandwidth throttling\n      Adjust the hash balance factor to correctly trigger scaling\n    \n  \n  Conclusion\n\n\nHow we do streaming \n\nTo stream video, we cut each video file in 6 seconds chunks. The video player loads the associated manifest, which lists these pieces and in which order it must read them. It then downloads the first video chunk, plays it, then loads the second chunk, etc.\n\n\nPictorial explanation of video streaming\n\nA video is composed of several chunks.\nFor example, a 90 minutes movie, with a duration of 6 seconds per chunk, means 90×60÷6=900 video chunks called from the player plus another 900 audio chunks. A total of 1800 different chunks for a single video.\n\nJust In Time Packaging \n\nA client calls a manifest and chunks to play a video.\nDepending on the format (Dash, HLS, Smooth) a client supports, it will request one of three kinds of manifests+chunks.\n\nThe Unified Streaming software handles these calls. Unified Origin (which we call USP) fetches the associated video from a AWS S3 bucket. It relies on a server manifest (.ism file), stored with the video, to respond with the video format the client requested: Dash, HLS, etc.\n\nSo, we store a complete video and its server manifest on S3, and USP provides the client with a client manifest and specific chunks: this is Just In Time Packaging (JITP).\n\nAnother way is to compute all the chunks and manifests in advance and write them to S3: this is offline packaging.\nIn this case, once packaging is done, there is no need to do these calculations anymore: it lightens the architecture and avoids the availability challenges of doing real-time computing.\n\n\nComparing Just-In-Time Packaging with Offline Packaging\n\nStill, this causes a big cost problem. On AWS S3, you pay for data access (GET requests), as well as storage. The more you store, the more you pay and the more you access, the more you pay.\n\ni.e, a 90mn video, played in Dash, is cut into 900 chunks plus a single Dash manifest. The same video in HLS, it’s 900 different chunks and another manifest: 1802 files written on S3. Add the Smooth Streaming format and you get 2703 files stored on S3, for a single video.\n\nOffline packaging is interesting, but incompatible with our need to manage a large number of equipments and vast catalogs: tens of thousands of program hours per customer.\n\nAnother approach, which uses the best of the two above solutions, is possible: the CMAF (Common Media Application Format) standard.\nThe player is able to chunk the video itself by adding the HTTP header Range: Bytes.\nMany devices, especially connected TVs or old Android versions, are not compatible with CMAF, which is why the rest of this article will focus on Dash/HLS in JITP.\n\nVersion 1: The quest for self \n\nWe were using USP on-prem. We decided to migrate it to the AWS cloud.\n\nThe goal of the V1 was to quickly provide a platform to our video teams to work on and certify the video players. For Bedrock Ops team, it was a first stepping stone building this platform.\n\nLet’s detail the components of this V1.\n\n\nv1 of our VOD platform\n\n\n  Players send their requests to a CDN.\n  The CDN uses a network load balancer as its origin.\n  The network load balancer forwards requests using a Round Robin algorithm, to multiple AWS EC2 instances we call “USP Origin”. These EC2 instances are controlled by an AutoScalingGroup and are dynamically scaled based on their network or CPU usage.\n  EC2s retrieve files from the S3 bucket.\n\n\nLocal cache with Nginx \n\nOn EC2 instances, USP runs as a module of Apache HTTPD.\n\nWhen a player requests a specific video chunk, it sends an HTTP request to HTTPD. The USP module it embeds will:\n\n\n  load the according .ism file from S3 (the server manifest)\n  load the video metadata, stored in the first 65KB and the last 15B of a .mp4 file on S3\n  load the specific chunk from the mp4 container, according to the player’s information: bitrate, language, etc. (still on S3)\n\n\nFor each video chunk called from a player, the USP module does another call to the S3 bucket, loading the same .ism manifest and the same metadata (first 65K and latest 15B).\nTo avoid these calls and reduce S3 costs by 60%, we added Nginx on these EC2s. It goes between HTTPD and S3, to cache the manifest .ism files and metadata of .mp4 video files.\nWe’re using LUA in the Nginx vhost, to cache these 65KB and 15B requests made by USP to the S3 bucket.\n\n\nDetails on the composition of a USP origin\n\nWe use Nginx for caching because we have a solid experience with it, under heavy load, on our on-prem edge servers, which each delivers up to 200Gbps of video traffic. We want to capitalize on this expertise and avoid spreading ourselves thin on multiple tools (e.g, Apache Cache module).\n\nI recommend reading the article published by unified streaming, which uses a similar method: caching via httpd directly.\n\nNetwork Load Balancer: manages TLS and helps with scaling \n\nWe’re using Network Load Balancers to offload TLS. They are cheaper than Application Load Balancers and we don’t need to interact with the HTTP layer: this is not the role of the load balancer, we prefer to keep a KISS principle.\n\nThe major advantage of NLBs is a single entry point (a CNAME domain name), which distributes the load over n EC2 instances. This is essential for auto-scaling: nothing to configure at the CDN level, the load balancer will distribute the load among all Ready instances, whether there are 2 or 1000.\n\nAWS managed load balancers are also interesting because certificates are auto-renewed. Another advantage is that they are distributed over all the availability zones, which was one of our prerequisites in our multi-AZ strategy.\n\nContent Delivery Network: Keep It Simple and Stupid \n\nWe’re using Cloudfront CDN with a basic configuration: we respect standards and use Cache-Control header.\n\nWe’re also using our on-prem Edge servers and other CDNs. Likewise, they all respect the HTTP protocol RFCs and we provide a valid Cache-Control header to be CDN agnostic.\n\nA first conclusion: V2 needs Consistent Hashing \n\nV1 of this platform allowed our video teams to work on new features and new software versions quicker, compared to on-prem. It was also new for Infra teams: we wanted to understand how to scale the platform on AWS to meet our load requirements, making the best use of managed services and auto-scaling, which we did not have on-prem.\n\nWe have identified the problem of version 1 during our tests: the cache is ineffective under heavy loads. The Round Robin algorithm (used by the NLB) is not adequate in front of cache servers because each server will try to cache all the data and will not be specialized to a part of the data. The more requests we have, the more servers we will add and the less each server will have a relevant cache.\n\n\nInefficiency of a Round Robin algorithm in front of cache servers\n\nTo use the cache as much as possible, we need an adapted load balancing method: Consistent Hashing.\n\n\nConsistent Hashing is an ideal method for caches\n\nLast two images are from our recent blog post about doing advanced load balancing at AWS.\n\nWith Consistent Hashing, we can send all requests for the same video to the same cache server. This would optimize the local Nginx cache and reduce S3 costs.\n\nVersion 2: Let the requests flow \n\nV2 will be used in production, with thousands of requests per second: we need it to handle the load, to be reliable and robust.\nAccording to our strong experience with HAProxy, we know that it is able to do Consistent Hashing with Bounded Loads, which is exactly what we need.\n\nWe started by adding HAProxy servers between the load balancer and the USP servers.\n\nHAProxy to make Consistent Hashing \n\nHAProxy is running on EC2 instances, in a dedicated AutoScalingGroup. As with the USP AutoScalingGroup, this one scales with AWS scaling policies: network bandwidth or CPU consumption. We can launch hundreds of HAProxy servers if we need to, and their scaling is independent from the number of USP servers (but they are often linked).\n\n\nv2 of our VOD platform\n\nTo send requests to USP origin, HAProxy needs to know all the healthy EC2 instances running in their AutoScalingGroup.\nWe started by using Consul, to automatically populate our HAProxy backend with these USP servers.\n\nSee the dedicated blog post to know why we preferred to develop a tool dedicated to this task, which we called HAProxy Service Discovery Orchestrator (HSDO).\n\nEC2 costs are reduced by using only Spot instances \n\nIn addition, HSDO is very responsive to movements in the AutoScalingGroup, which allowed us to replace all EC2 On Demand instances with Spot instances.\nAnd by all instances, I mean all USP servers (with cache), as well as HAProxy servers: 70% reduction in server costs.\n\nNote that replacing USP origins with Spot instances has almost no impact on the cache, as we follow the AWS best practices for Spot: use many different instance types, be multi-AZ and use the “Capacity Optimized” strategy. This way we observe very few reclaims, which translates to a longer cache life.\n\nProduction launch on this V2 \n\nThe production launch of this V2 has confirmed the stability and performance of the platform. We were happy to see that our objectives were met, so we started migrating all our VOD content from on-prem to the cloud, video by video, client by client.\n\n\nNginx cache hit ratio\n\nWith Consistent Hashing, the cache becomes quite efficient and we saved 62% of calls to S3.\n\nIn addition, the cache speeds up the video packaging and we have reduced the overall origin response time. Win-win.\n\nEC2-other: the financial abyss \n\nEC2-other, in this case, means network traffic between Availability Zones.\nThe private network between two data centers (AZs) at AWS is re-billed and accounted for 48% of the bill for our VOD platforms at the time.\n\n\nAWS Cost Explorer, October 2020\n\nWhen HAProxy servers sent/received traffic from USP servers, the latter might not be in the same Availability Zone and the traffic between the two was charged at full price.\n\nIt was necessary to quickly find a solution for these costs which torpedoed the project. We started the creation of version 3 as soon as we had these metrics from version 2, at the beginning of our VOD content migration.\n\nVersion 3: Cost Explorer Driven Development \n\nThe idea is to do as well for half the price.\n\nBe multi-AZ without inter-AZ traffic \n\nWe have updated HSDO so each HAProxy only sends requests to USP origins of the same AZ.\nAnd the Network Load Balancers still send traffic to the HAProxys, on multiple AZs.\n\nRemoving inter-AZ traffic was not that much work and we quickly saw the difference: 45% cost savings.\n\n\nThe costs of this platform, where v3 was deployed in mid-November 2020\n\nWe can see on the picture above that the EC2-other costs (in orange) have disappeared in December.\n\nThe work on version 3 began shortly after the start of our cloud migration.\nWe were still migrating on-prem to the cloud when we put V3 on prod. That’s why you can see all costs have increased from October to December: we’ve doubled the number of viewers during the period.\n\nMono-AZ AutoScalingGroups \n\nWe have also replaced the multi-AZ AutoScalingGroups by several mono-AZ ones. It gives us a finer scaling that corresponds to the real needs in each AZ. The randomness of the round robin and client requests means that, from time to time, one AZ receives a significantly higher load than another.\n\n\nv3 of our VOD platform\n\nSince the NLBs are using a Round Robin algorithm, each HAProxy can receive traffic for any video. Now that the HAProxy servers of an AZ only send traffic to the USP origins of the same AZ, everything that is cached exists in as many copies as we have configured of AZs.\nIt makes us all the more resilient to an AZ failure.\n\nOptimizations \n\nSince V3, we have not made any major architectural changes. However, some optimizations were necessary.\n\nAdapt HAProxy config for EC2 bandwidth throttling \n\nOn AWS, an EC2 instance has a baseline network capacity and a burst capacity (see UserGuide).\nBaseline capacity is the network bandwidth you can consume all the time.\n\nThe Burst capacity is what you may be able to consume temporarily before being throttled to the baseline capacity.\nIn the EC2 presentation, the value “Up to” refers to the burst.\n\nLess visible in the EC2 documentation, one can find the baseline capacity for each instance type (which is public knowledge since July 2021).\n\nFor example, c5.large instances have a network bandwidth Up to 10Gbps (burst) but only 0.75Gbps baseline bandwidth.\n\nTroubles start when HAProxy sends a little more traffic to one instance than to the others: USP origin’s bandwidth may be throttled at some point… And we will observe poor performance or even service interruptions of this server.\n\n\nA server whose bandwidth is throttled (seen from CloudWatch)\n\nWe added observe layer7 to the default-server in our HAProxy backends, to remove servers returning HTTP error codes (5xx) from its load-balancing.\n\nWe also added the retry and redispatch options, which allow to retry a request sent to an unhealthy server on a healthy server. It’s not optimal for the cache, but what matters is that a client’s request is successfully answered.\n\nWe observed that with throttled bandwidth, the connection time from HAProxy to a USP server increases dramatically.\nSo we’ve also reduced timeout connect to 20 milliseconds.\n\nHighlights of our HAProxy configuration:\n\ndefaults\n   timeout connect     20ms\n   retries     2\n    # We do not use &quot;all-retryable-errors&quot; because we don&#39;t want to retry on 500,\n    # which is an USP expected error code when it goes wrongly\n   retry-on     502 503 504 0rtt-rejected conn-failure empty-response response-timeout\n   option     redispatch\n   timeout server     2s\n   default-server     inter 1s fall 1 rise 10 observe layer7\n\n\nNow, if a USP origin throttles on its network bandwidth or if there is a degradation of service, HAProxy will immediately redispatch the request to another server.\n\nWe are working on adding an agent-check, so that the weight of the servers in HAProxy can be directly defined by an USP origin, if it detects that its bandwidth is throttled.\n\nAdjust the hash balance factor to correctly trigger scaling \n\nOur scaling depends on the average server utilization in an AutoScalingGroup. If a few servers are overloaded but the majority is not doing anything, we don&#39;t scale.\n\nBut all contents on our platforms are not equally popular. This affects Consistent Hashing which would result in few servers receiving way more traffic than others. Few servers would be overloaded and the majority would not do much.\n\nHere is an example in a load test:\n\n\nGraph showing few overloaded servers, using classic Consistent Hashing\n\nWe want to benefit from Consistent Hashing while being able to scale on average consumption.\nThis is what Consistent Hashing with Bounded Loads allows: to benefit from Consistent Hashing, while balancing load.\n\nThe Bounded Loads are controlled by the hash-balance-factor option in HAProxy.\nAccording to the doc:\n&amp;lt;factor&amp;gt; is the control for the maximum number of concurrent requests to\n         send to a server, expressed as a percentage of the average number\n         of concurrent requests across all of the active servers.\n\n\nWe played the same load test, once using classic Consistent Hashing, a second time using bounded loads:\n\n\nGraph showing the effects of Bounded Loads over Consistent Hashing\n\nWe did dozens of load tests before finding the best value for our use: 140.\nFor each load test, we looked at the evolution of:\n\n\n  Nginx Cache Hit Ratio\n  Number of requests to S3\n  HAProxy Backend retries and redispatches\n  HAProxy backend 5xx response codes\n  Free disk space on USP origins\n\n\nOur configuration of the Consistent Hashing with Bounded Loads remains simple:\n\nbackend usp-servers-AZ-C\n    balance hdr(X-LB)\n    hash-type consistent sdbm avalanche\n    hash-balance-factor 140\n\n\nThanks to Consistent Hashing with Bounded Loads, our cache is optimized without impacting our autoscaling.\nThere may be contents much more solicited than others, the load will be balanced and our autoscaling will be activated.\n\nConclusion \n\nWe migrated our video delivery to the cloud, moving from static servers to an end-to-end auto-scaling and multi-AZ infrastructure. We are now able to handle very high loads, which we could not do on-premise.\nWe had the opportunity to review our architecture three times, within a few weeks of each other, even though the migration had begun.\n\nThe v3 is not perfect, but it is quite well optimized, reliable and scalable.\n\nWe are thinking about V4 and saving 20% of the costs by removing the NLB. We also identified some possible improvements, adding cache on HAProxy for example, or using HAProxy Agent Check so that the weight of the servers in HAProxy is driven directly by the servers, using the Amazon metrics on network performances. Another promising performance improvement could be to use HAProxy on ARM as Graviton type instances offer significant discounts, it will be worth testing.\n\nIn parallel, we also invest time on CMAF which is for us, the long-term objective.\n\n\n\nSpecial thanks to all my colleagues at Bedrock Streaming and members of Unified-streaming, for re-re-re-and-rereading this blog post. ❤️\n"
} ,
  
  {
    "title"    : "More efficient Load Balancing and Caching at AWS, using Consistent Hashing and HAProxy",
    "category" : "",
    "tags"     : " aws, cloud, sysadmin, HAProxy, video, opensource, high availability",
    "url"      : "/2021/11/18/hsdo.html",
    "date"     : "November 18, 2021",
    "excerpt"  : "AWS ALB &amp;amp; NLB currently supports Round-Robin (RR) and Least Outstanding Requests (LOR) balancing algorithms. But what happens when you try to load balance cache servers with these algorithms? How to implement an effective cache in Cloud at sca...",
  "content"  : "AWS ALB &amp;amp; NLB currently supports Round-Robin (RR) and Least Outstanding Requests (LOR) balancing algorithms. But what happens when you try to load balance cache servers with these algorithms? How to implement an effective cache in Cloud at scale?\n\nContext\nAt Bedrock we use both ALB &amp;amp; NLB for different use-cases (like in front of our Kubernetes clusters) in our platforms. For our last VOD platform, we needed to be able to load balance heavy content (videos) to our cache servers. We knew that the balancing algorithm was a key factor for our cache in Cloud at scale, and that ALB &amp;amp; NLB won’t be sufficient to achieve our goals.\n\nBalancing algorithms\n\nRound-Robin is a widely used balancing algorithms.\n\n\nRound robin or Least Outstanding Requests algorithms\n\nThe load balancer cycles through cache servers sequentially, so each cache server should receive an equal share of requests. Each cache server has to possibly store every requested object (♠️, ♥️, ♦️, and ♣️ represent different objects).\n\nWith Least Outstanding Requests, load balancers send requests to the cache server with least awaiting requests. The cache server still has to store every requested object.\n\nConsistent Hashing is a very interesting balancing algorithm for caching purposes.\n\n\nConsistent Hashing algorithm\n\nThis algorithm allows to distribute the load so that all requests for the same object will always go to the same cache server. This way, each cache server has to store half of the objects.\n\nAnd more cache servers means more load balancing between the servers.\n\n\nConsistent Hashing at scale\n\nWhile at scale, Round-Robin or Least Outstanding Requests will look like this:\n\n\n\nRound Robin or Least Outstanding Requests at scale\n\nCache servers are not efficient at scale with these algorithms. There is a higher chance for the cache to miss, because all servers do not store all objects. Every time a cache expires or cache server boots, objects need to be cached again to be hit. You also need more resources, as the same object needs to be cached on all cache server disks.\n\nWith Consistent Hashing, once an object has been cached you have a greater chance for the cache to hit. And you save money by using smaller disks on cache servers and reducing network bandwidth.\n\nHAProxy implementation\n\nConsistent Hashing at AWS is not available with ALB, ELB and NLB. You need to implement it yourself.\n\nTo do this, we chose to use HAProxy.\n\nHAproxy is fast and reliable. We use it often, we know it well, and it can use consistent hashing.\n\nThis is how we architected it.\n\n\nLoad Balanced Cache Architecture\n\nHAProxy servers and Cache servers are deployed with Auto Scaling Groups (ASG). A Target Group is registering HAProxy ASG instances so NLB will load balance between them. Having separated ASG for HAProxy and Cache allows it to have dedicated automatic scaling and management.\n\nWe need to implement something for HAProxy so it could discover and register Cache ASG instances. And there is one thing important to do consistent hashing with HAProxy: a centralized and consistent configuration.\n\nCentralized configuration\n\nAll HAProxy instances need to have the same configuration with the same list of servers, or requests will be split differently depending on HAproxy instances.\n\n\nConsistent Hashing with different list of servers per HAProxy\n\nYou have a higher chance to have a miss on your cache request as a single object may be at different locations depending on the HAproxy instance.\nWith a centralized configuration, you can be sure that each HAProxy instance will request the same cache server for the same object.\n\nConsistent configuration\n\nHAProxy consistent hashing is based on backend server IDs. These IDs match the position of the server in the backend server list. For example, this HAProxy configuration:\n\nbackend cache\n   server CacheA 192.168.0.1:80 #ID = 1\n   server CacheB 192.168.0.2:80 #ID = 2\n   server CacheC 192.168.0.3:80 #ID = 3\n   server CacheD 192.168.0.4:80 #ID = 4\n\n\nwill be seen as the following:\n\n\n\nTo keep consistent hashing efficient, cache servers need to change ID rarely. HAProxy backend server list must be consistent across all HAProxy instances.\n\nIf CacheC is removed, configuration has to be like:\n\nbackend cache\n   server CacheA 192.168.0.1:80          #ID = 1\n   server CacheB 192.168.0.2:80          #ID = 2\n   server CacheC 192.168.0.3:80 disabled #ID = 3\n   server CacheD 192.168.0.4:80          #ID = 4\n\n\nCacheC backend server is now disabled until another Cache server takes its place.\n\n\n\nWith consistent configuration: ♠️, ♣️ and ♥️ requests are always balanced to the same cache servers, while ♦️ requests are balanced to another available cache server.\nWithout consistent configuration: all requests could be rebalanced to other cache servers. This would mean that for each cache server scale up or down, we no longer have the cached objects: we MISS the cache. This would be inefficient: we would lose the advantage of the cache.\n\nSolutions\n\nConsul\n\nAt first, we started to configure HAproxy through Consul. We already used it at BedRock, and an article gave us hope to quickly achieve what we wanted. \nConsul Service Discovery with DNS won’t provide sorted/consistent DNS records by design. We can’t have a consistent configuration with it.\n\nAnother way of doing so would be to use consul-template for generating backends and registering servers into an HAProxy configuration file. With this approach, we would reload systemd to add new servers to HAProxy.\n\nBut HAProxy Runtime API is the recommended way to make frequent changes on configuration, service reloads are not considered safe.\n\nAWS EC2 Service Discovery\n\nHAProxy also released a new functionality called AWS EC2 Service Discovery in July 2021. We haven’t tested it yet, but it lacks the possibility to keep a consistent list of servers between HAProxy instances, which isn’t good for consistent hashing as discussed before. We opened an issue on HAProxy dedicated Github repository.\n\nAdded to the fact that we were starting to think that Consul was overkill for our needs, we start to implement our own solution.\n\nHAProxy Service Discovery Orchestrator\n\nWhat we wanted to achieve was to use maximum managed service from AWS, meet our standard of stability and resilience, and keep things simple. We choose Python with boto3 to implement our solution as it is one of our team’s favorite languages.\n\nHAProxy Service Discovery Orchestrator (or HSDO) is open-source.\n\nHSDO is composed of a server and a client.\n\nHSDO Server\n\nHSDO Server runs in standalone. It could be a Lambda, but we were more comfortable with system processes when we designed it.\nIts job is to keep track EC2 instances of one or multiple Cache ASGs and update a list accordingly.\nHSDO server provides a consistent sorted list of instances. Every time a new cache server appears in the ASG, it is added to the list at a given ID that will never change.\nThis list is stored in DynamoDB.\n\n\nDynamoDB Items View\n\nHSDO Client\n\nHSDO client is reading the DynamoDB table to get the cache servers. The client run on the same instance as HAProxy and use the runtime API to update HAProxy config.\n\n\nHAProxy Status Page\n\nBrown lines are disabled servers, while green lines are servers stored in DynamoDB as seen above.\n\n\nHSDO in Load Balanced Cache Architecture Schema\n\nWith this architecture, we achieve a centralized and consistent configuration to make consistent hashing work at scale for cache servers.\n\nConclusion\n\nWe have been using HSDO since September 2020. We are distributing VOD content for Salto and 6play streaming platforms and are able to handle at least 10.000 requests/s. This wasn’t possible without a few improvements (on platform cost, timeout funnels, …) and this will be presented in another post, so keep in touch. ;)\n\nSpecial thanks to all Ops team members in BedRock Streaming for re-re-re-and-rereading this blog post.\n"
} ,
  
  {
    "title"    : "Forum PHP 2021 - L&#39;édition des retrouvailles",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2021/11/02/forum-php-2021.html",
    "date"     : "November 2, 2021",
    "excerpt"  : "Cette année encore, Bedrock participait au Forum PHP où était proposé une grande diversité de conférences.\nDes sujets techniques et d’autres, plus génériques, étaient abordés : Symfony 6, Git, environnement, sous-représentation des femmes dans l’i...",
  "content"  : "Cette année encore, Bedrock participait au Forum PHP où était proposé une grande diversité de conférences.\nDes sujets techniques et d’autres, plus génériques, étaient abordés : Symfony 6, Git, environnement, sous-représentation des femmes dans l’informatique…\nAvec Sofia LESCANO, Benoit VIGUIER sur les planches et une quinzaine de participantes et participants dans le publique, l’occasion de rencontrer à nouveau la communauté PHP en chair et en os a été saisie avec une certaine impatience.\n\nPlusieurs conférences ont retenu notre attention et auront un impact à court terme sur nos projets :\n\nSuite à la conférence “Les exceptions : le trou dans la raquette du typage” de Baptiste LANGLADE, la bonne gestion des exceptions nous semble primordiale. Nos équipes sont donc en train de tester le bundle Innmind/Immutable pour mettre en place le pattern Monad (Maybe et Either) afin d’améliorer la gestion d’absence de données à différents niveaux de nos outils.\n\nLa conférence “Des tests unitaires pour nos règles de conception” de Frédéric BOUCHERY mettait en lumière l’importance de documenter, expliciter et tester les règles de conception d’un projet. La mise en place d’ADR (Architectural Decision Records) et des tests unitaires associés est une bonne pratique que nous souhaitons développer au sein des équipes. Nous attendons avec impatience le bundle que Klaxoon devrait bientôt open-sourcer: il permettra de tester facilement nos règles de conceptions avec PHPUnit et d’automatiser une partie de la revue technique.\n\nAnne-Laure DE BOISSIEU et Amélie DEFRANCE ont rappelé quelques règles fondamentales d’accessibilité pour nos sites Internet pendant “Accessibilité et SEO : et si on relevait le niveau ?”. Nous espérons désormais améliorer l’accessibilité de notre back-office. Par exemple : retravailler le contraste couleur de certains écrans ou ajouter du contenu dans nos balises HTML pour faciliter la compréhension.\n\nPendant leur conférence “Kairoi, et PHP se réconcilie avec les tâches planifiées”,  Emeric KASBARIAN et Jérémy JAMES nous ont expliqué que leurs clients ont un même besoin : “Déclencher une action automatique à un moment précis, sans aucune limite dans le temps”. Par exemple : “supprimer, automatiquement, un panier d’achat au bout de 15 minutes”.\nAprès de longues recherches, il n’existe rien sur le marché pour répondre à un tel besoin. Et nous, BedRock, confirmons : nous avons le même besoin et n’avons rien trouvé non plus.\nKairoi est donc né. C’est une application serveur Rust de planification de tâches, avec son propre protocole (inspiré de celui de Redis) qui permet de :\n\n  Récupérer des évènements à planifier\n  Connaître l’état d’un l’évènement\n  Le déclencher au moment opportun\n    \n      soit sur un protocole AMQP\n      soit sur un shell\n    \n  \n\n\nLe Forum PHP est l’occasion de parler de sujets pointus techniquement, mais aussi une occasion d’échange et de partage autour de sujets plus transversaux.\nNotre domaine, l’IT, comme bien d’autres, est sensible au sujet de l’écologie. Deux axes de réflexion ont été évoqués pendant deux conférences.\n\nEn ouverture, François ZANIOTTO nous a partagé avec entrain sa recherche de mesures fiables des dépenses énergétiques. Elle l’a menée à développer  GreenFrame, un outil en cours de construction chez Marmelab, qui a pour but de cibler au plus près la consommation énergétique afin de tendre “Vers la sobriété numérique”.\n\nHélène MAITRE-MARCHOIS a sû mettre en perspective le rôle de chaque développeuse et développeur en insistant sur le fait que la responsabilité du dérèglement climatique n’est pas forcément là où on l’attend. Avec “Comment sauver la planète en ne faisant rien”, elle entend faire prendre conscience que si, la production et consommation de contenu représentent des pôles sur lesquels en tant que tech, nous pouvons agir. Le renouvellement du parc reste une cause prépondérante dans l’impact écologique.\nLe renouvellement accéléré par le foisonnement de nouvelles fonctionnalités, trop souvent inutiles, est une obsolescence programmée.\nUtile, Accessible, Durable. Voilà trois notions simples qui peuvent, pourtant, nous permettre de faire la différence.\n\nNicolas GREKAS, principal engineer Symfony, nous a parlé de l’écosystème de ce framework à travers de sa conférence “Symfony 6 : le choix de l’innovation et de la performance”. Il nous a présenté le calendrier de livraisons et de maintenance des différentes versions, avec la sortie d’une nouvelle majeure prévue tous les deux ans. Chaque majeure voit la suppression du code déprécié dans la version précédente. Par exemple, Symfony 6 est un Symfony 5.4 sans ses dépréciations. Les dernières versions avant une majeure (comme la 4.4 ou 5.4) sont assurées d’avoir un support à long terme.\nPour les prochaines versions de Symfony, l’accent est mis sur la compatibilité avec PHP8. Puisque la majorité du travail consiste à remplacer les annotations @return par un typage natif, Nicolas a parlé de l’outil patch-type-declarations qui automatise cette tâche.\n\nPour finir cette série de conférences, nous avons suivi l’incroyable histoire de WorkAdventure, lors de “WorkAdventure de la genèse à aujourd’hui : Retour d’expérience sur 1 an d’univers virtuels” présenté par David NÉGRIER) : le résultat d’un hackathon fait pour pallier l’ennui des confinements, qui est devenu le support d’événements majeurs l’année dernière.\n\n\n\nLes speakers Bedrock\nLors de cette édition, deux Bedrockers ont eu l’opportunité de présenter un sujet, l’occasion pour nous de demander à Sofia LESCANO “Faites confiance aux développeurs.euses de votre équipe : voyez plus loin que les fonctionnalités” et Benoit VIGUIER “Fiber : la porte ouverte sur l’asynchrone” comment ils ont vécu cet événement.\n\nComment vous est venue l’idée de soumettre un sujet de conférence, et comment avez-vous abordé sa préparation ?\n\nSofia: Cela faisait longtemps que j’avais ce sujet en tête, et l’idée de refaire des événements en présentiel m’a fait me lancer. Pour moi les tech meetings étaient une grande découverte et un rituel que j’apprécie vraiment et je voulais partager cela avec la communauté. Pour la préparation, j’ai été accompagnée par Matthieu Napoli avec le programme de mentoring de l’AFUP et par mes collègues de Bedrock.\n\nBenoit: Le PHP asynchrone est un sujet qui m’occupe beaucoup à Bedrock, j’ai donc suivi attentivement la RFC Fiber. L’idée d’en faire un sujet de conférence est venue en me rendant compte que, même au sein de nos équipes, il n’était pas évident pour tout le monde de comprendre tout ce que cet outil pouvait changer. Et puis, refaire un événement en présentiel me manquait vraiment ! La préparation de ce format court était nouveau pour moi, heureusement j’ai pu faire quelques répétitions à Bedrock et à l’AFUP Lyon pour bien ajuster mon timing.\n\nMaintenant que l’événement est derrière nous, que retenez-vous de cette expérience ?\n\nSofia: C’était une très belle expérience et les échanges que j’ai pu avoir suite à ma conférence ont été très intéressants. C’est très enrichissant d’échanger avec la communauté et de voir que des pratiques similaires ont lieu ailleurs et pouvoir les enrichir dans les deux sens.\n\nBenoit: C’était un vrai plaisir de pouvoir échanger avec de vraies personnes, sans écrans interposés ! Côté speaker, l’organisation était au top et j’ai eu pleins d’échanges prometteurs sur le potentiel des Fibers. Côté conférences, j’ai vu plein de choses intéressantes, ça donne toujours matière à réfléchir, que l’on partage le point de vue exposé ou non. Merci encore à l’AFUP pour avoir mis toute cette énergie au service d’un si bel événement.\n\nLe forum, particulièrement cette édition en présentiel, c’est retrouver toute une communauté qui partage la même passion. Encore merci aux conférencières et conférenciers, merci aux organisatrices et organisateurs… Et à l’année prochaine !\n"
} ,
  
  {
    "title"    : "Fiber: the open door to asynchronous",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2021/10/21/fiber-the-open-door-to-async.html",
    "date"     : "October 21, 2021",
    "excerpt"  : "Parmi les nouveautés apportées par Php 8.1, les Fibers tiennent une place particulière. Il s’agit certainement d’une fonctionnalité qui aura un impact majeur sur l’écosystème Php, tout en ayant un impact mineur sur le code que vous écrivez tous le...",
  "content"  : "Parmi les nouveautés apportées par Php 8.1, les Fibers tiennent une place particulière. Il s’agit certainement d’une fonctionnalité qui aura un impact majeur sur l’écosystème Php, tout en ayant un impact mineur sur le code que vous écrivez tous les jours. Les Fibers sont comme des générateurs améliorés, des fonctions interruptibles, mais qui peuvent s’imbriquer de manière transparente avec d’autres fonctions. Il est donc enfin possible de créer des fonctions similaires à await et async pour rendre la programmation asynchrone moins intrusive dans notre code et permettre la compatibilité avec les frameworks existants. Voici une introduction à ces nouveaux concepts, ainsi que des exemples concrets de ce que cela permettra dans l’écosystème Php.\n"
} ,
  
  {
    "title"    : "Faites confiance aux développeurs.euses de votre équipe : voyez plus loin que les fonctionnalités",
    "category" : "",
    "tags"     : " conference, afup, php",
    "url"      : "/2021/10/21/confiance-aux-devs-de-votre-team.html",
    "date"     : "October 21, 2021",
    "excerpt"  : "Deadlines, besoins produit, pression forte et fonctionnalités à livrer : nos projets ont besoin de nous ! L’amélioration du quotidien se perd dans un second plan, alors qu’elle a un impact majeur sur l’augmentation de notre productivité et la qual...",
  "content"  : "Deadlines, besoins produit, pression forte et fonctionnalités à livrer : nos projets ont besoin de nous ! L’amélioration du quotidien se perd dans un second plan, alors qu’elle a un impact majeur sur l’augmentation de notre productivité et la qualité et maintenabilité de notre code.\n\nConstatant que nous voulions augmenter notre confort de travail, nous avons, depuis plus d’un an, mis en place des réunions techniques bi-hebdomadaires pour prendre le temps de discuter de notre plateforme et nos outils, au-delà des fonctionnalités. Chaque membre de l’équipe contribue ainsi à améliorer son expérience de travail et notre produit.\n\nÀ travers notre vécu, nos erreurs et des exemples techniques concrets, repensez vous aussi au développement de votre produit.\n"
} ,
  
  {
    "title"    : "Increase performance and stability by adding an Egress Controller in a Kubernetes cluster at AWS",
    "category" : "",
    "tags"     : " php, aws, cloud, performance, sysadmin, kubernetes, HAProxy",
    "url"      : "/2021/10/18/increase-performance-and-stability-by-adding-an-egress-controller.html",
    "date"     : "October 18, 2021",
    "excerpt"  : "Introduction\n\nWe recently encountered issues with our PHP applications at scale in our Kubernetes clusters at AWS. We will explain the root cause of these issues, how we fixed them with Egress Controller, and overall improvements. We also added a ...",
  "content"  : "Introduction\n\nWe recently encountered issues with our PHP applications at scale in our Kubernetes clusters at AWS. We will explain the root cause of these issues, how we fixed them with Egress Controller, and overall improvements. We also added a detailed configuration to use HAProxy as Egress Controller.\n\nContext\n\nBedrock is using PHP for almost all of the backend API of our streaming platforms (6Play, RTLMost, Salto, …). We have deployed our applications in AWS on our kops-managed Kubernetes clusters. Each of our applications is behind a CDN for caching purposes (CloudFront, Fastly). This means every time an application needs to access another API, requests go on the internet to access the latter through CDN.\n\nDuring special events with huge loads on our platforms, we started to see TCP connection errors from our applications to the outside of our VPC.\n\nErrorPortAllocation source\n\nAfter a few investigations, we saw that TCP connection errors were correlated with NAT Gateways ErrorPortAllocation.\n\n\nSome loadtesting on our platform, which you may see as no traffic, huge traffic, then no traffic again\n\nIn AWS, NAT Gateways are endpoints allowing us to go outside our VPC. They have hard limits that can’t be modified:\n\n  A NAT gateway can support up to 55,000 simultaneous connections […]. If the destination IP address, the destination port, or the protocol (TCP/UDP/ICMP) changes, you can create an additional 55,000 connections. For more than 55,000 connections, there is an increased chance of connection errors due to port allocation errors. AWS Documentation\n\n\nOur applications always request the same endpoints: other APIs CDN. Destination port, IP or protocol doesn’t change that much, so we start hitting max connections, resulting in ErrorPortAllocation.\n\nAt the same time, we found a very interesting blog post: Impact of using HTTP connection pooling for PHP applications at scale, which was a very good coincidence.\n\nAs you can read in Wikimedia’s post, PHP applications aren’t able to reuse TCP connections, as PHP processes are not sharing information from a request to another. Recreating new connections on the same endpoints is inefficient: adds latency, wastes CPU (TLS negotiation and TCP connection lifecycle) but also overconsumes TCP connections.\n\n\nPHP application calls another API on internet through the NAT gateway\n\nOutgoing requests optimization\n\nEgress Controller\n\nHAproxy is fast and reliable. We use it often and know it well. We already have it as Ingress Controller in our clusters and we know service mesh needs time to be production-ready. So we thought a service mesh might be overkill in our case and we tried to add HAProxy as Kubernetes Egress Controller in our clusters.\n\n\nOutgoing requests go through the Egress Controller, which pools and maintains TCP and TLS connections\n\nWe configured some applications to send a few outgoing requests to Egress Controller. The latter was configured to do TCP re-use and to forward to desired endpoints.\n\nEffects\n\nWith this optimization, we don’t encounter ErrorPortAllocation anymore. Requests duration are reduced by 20 to 30%, and apps are consuming less CPU. Ressources were spent to instantiate a new TLS connection, which is now handled by Egress Controller.\n\n\nApplication consumes less CPU, because Egress Controller is responsible of TLS and TCP connections to the outside world, which consumes a lot of resources\n\nDetailed configuration\n\nWe generally prefer to use what already exists rather than starting from scratch, so we tried to see if HAProxy Kubernetes Ingress Controller could be used as egress.\n\nHAProxy Ingress Controller loads its frontend domains in Ingress resource, and loads backend servers in the associated Service resource. To inject an external domain as a backend server, we have to use Service ExternalName.\n\n\n\nTo use HAProxy Kubernetes Ingress Controller as an Egress Controller, we will use Ingress Kubernetes resource as Egress to define domains handled by the Controller.\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app1\nspec:\n  type: ExternalName\n  externalName: app1.example.com\n  ports:\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: app1\nannotations:\n    haproxy.org/server-ssl: &quot;true&quot;\n    haproxy.org/backend-config-snippet: |\n      # See this article for the deep reasons of both parameters: https://www.haproxy.com/fr/blog/http-keep-alive-pipelining-multiplexing-and-connection-pooling/\n      # enforce SNI with the Host string instead of the &#39;Host&#39; header, because HAProxy cannot reuse connections with a non-fixed Host SNI value.\n      default-server check-sni app1.example.com sni str(app1.example.com) resolvers mydns resolve-prefer ipv4\n      # make HAProxy reuse connections, because the default safe mode reuses connections only for the same source.ip\n      http-reuse always\n\nspec:\n  rules:\n  - host: app1.example.com\n    http:\n      paths:\n      - backend:\n          serviceName: app1\n          servicePort: 443\n\n\nWhen everything is ready, you will be able to send requests:\n\ncurl -H &quot;host: app1.example.com&quot; https://haproxy-egress.default.svc.cluster.local/health\n\n\nBy default, HAProxy resolves domain names only at bootime. But it can be configured to resolves during runtime by adding a config snippet to Egress Controller configuration:\n\nglobal-config-snippet: |\n  resolvers mydns\n    nameserver local &amp;lt;MY_DNS&amp;gt;:53\n\n\nConclusions\n\nIt seems surprising to reduce requests latency by adding a hop in a network. But it does really work, even if it has some limits.\n\nThe main problem with this approach is the fact that we are effectively creating a Single Point of Failure in our clusters if we choose to send all our egress traffic through it. Instead, we are carefully selecting applications that should use an Egress Controller to refine the configuration little by little. Some applications are tightly tied to external services and would massively gain from this and others would only be less resilient.\n"
} ,
  
  {
    "title"    : "How did we live stream our Last Friday Talks?",
    "category" : "",
    "tags"     : " lft, talks, live, stream, obs",
    "url"      : "/2021/10/14/live-streaming-lft.html",
    "date"     : "October 14, 2021",
    "excerpt"  : "At Bedrock, the last Friday of every other month1 is Last Friday Talk – or LFT.\n\nThis event encourages sharing: technical topics, less technical topics, cross-team topics…\nWe also take the opportunity to meet and talk with colleagues we don’t work...",
  "content"  : "At Bedrock, the last Friday of every other month1 is Last Friday Talk – or LFT.\n\nThis event encourages sharing: technical topics, less technical topics, cross-team topics…\nWe also take the opportunity to meet and talk with colleagues we don’t work with daily. And the post-LFT snack, put aside during COVID but which I hope will come back, is also meant for that!\nFinally, these LFTs are an excellent opportunity for beginner speakers to practice in front of a friendly audience. We also use them to rehearse and validate talks we will give in public later.\n\nDuring the COVID full-remote period, we moved our LFTs to Google Live Stream, where each speaker shared their screen and showed their face through the camera.\nWith the return to the office2 and partial telecommuting, a new problem arose: how can we broadcast the talks given in our auditorium to 150+ remote colleagues? In good quality, to encourage people to attend several talks in a row? While remaining enjoyable and lively on-site?\n\nThe goal: to broadcast live\n\nWe can seat up to 70 people in our amphitheater, where most of our speakers were giving their talks3. And we were aiming for 100 to 200 people remotely, for whom we wanted to broadcast live4.\n\nFor the broadcast, we worked with one of our usual tools: the organizers and speakers broadcast in a Google Meet, and the audience follows the stream via Google Live Stream5. This solution supports a large number of participants, and access is filtered through our SSO, ensuring that only our colleagues have access to the stream.\n\nWe thought we were going to do this in a bit of an ugly way, like when we were all telecommuting: each speaker joins the Google Meet, shares their screen, and speaks into their microphone, often the headset provided by the company. But, still… We decided to make an effort and try to provide a better experience for our colleagues attending LFT remotely!\n\nSo, how we did it, this time\n\nHow did we capture and stream slides (or even code and video!), audio, and video of each speaker?\n\nFirst step: our amphitheater\n\nLet’s start from the room where we broadcast: our amphitheater.\n\n\n\nYou can see the room below. This photo is taken6 from the first row of the bleachers, on the right on the picture above:\n\n\n\nSo, the configuration of the room:\n\n\n  a speaker;\n  a lectern to put his or her PC;\n  from the HDMI output of this PC (possibly via a USB-C to HDMI adapter), we connect an HDMI cable that goes to the two projectors in the room;\n  these two projectors project (the same thing) on two screens, on the left and on the right of the speaker.\n\n\nBehind a screen, we placed a table for the capture and broadcast computer7. This way, it is not too far from the speaker8, without being visible from the audience.\n\nEquipment:\n\n\n  speaker’s laptop, placed on the lectern;\n  USB-C to HDMI adapter;\n  HDMI cable;\n  two projectors (fixed to the ceiling of the room);\n  two screens (fixed to the ceiling of the room).\n\n\nThis setup allows for efficient presentation in the room… But we haven’t started working on the live stream yet.\n\nA PC to manage the video and audio\n\nTo broadcast live, the speaker could join the Google Meet and share their screen. That’s what we were doing during the COVID… But we can also do much better!\n\nLet’s install a PC on the table behind the screen. It will broadcast, to the Google Meet, the video generated by OBS’ virtual camera – a free and open-source video recording and streaming software.\n\nA second PC, also placed on this table, is used to watch the Google Live Stream watched by all our remote colleagues. This feedback helps us validate that everything is working well, even if we suffer from a few dozen seconds of lag.\n\nEquipment:\n\n\n  a table, not too visible from the room;\n  a PC (laptop) – the one used this time had two USB-A ports and one USB-C port, which affects the cables/adapters required later on;\n  its charger and, perhaps, an extension cord;\n  a good quality internet connection;\n  a second PC (laptop) to view the live stream;\n  and a headset to listen to the live stream.\n\n\nVideo capture of the speaker’s slides / screen\n\nWe want to broadcast to the live stream what the speaker is projecting in the room. To do this, we place an HDMI capture box between her PC and the projector:\n\n\n\nThis capture box has an HDMI input (= the cable coming out of the speaker’s PC), an HDMI output (= the cable going to the projector) and a USB-C output (= the cable going to the control / broadcast PC).\n\n\n\nOn the control / broadcast PC, the capture box connected via USB is recognized as a webcam. It is then used as a video input device in OBS.\n\nHardware:\n\n\n  additional HDMI cable;\n  capture device + passthrough: Elgato HD60 S+;\n  USB-C to USB-A cable;\n  USB-A extension cable (because the control desk is a bit far).\n\n\nThe video recording of the room\n\nIt would be even cooler if the remote audience could see the speaker! As a matter of fact, part of the message of each talk is transmitted by the speaker’s gestures.\n\nSo let’s position a camera in the room, at the level of the audience sitting in the stands, to give the impression the speakers are looking at the camera when they are looking at the audience.\n\n\n\nWe did not have a real camera at hand. So we used an iPhone 11 Pro Max9. It has only a Lightning port… And, with an adapter, we can connect an HDMI cable. And the Filmic Pro10 application knows how to stream a clean HDMI11 video.\n\nTo capture the HDMI signal from the iPhone to the PC, we used a second capture box, simpler than the previous one: it has an HDMI input (= to connect the iPhone) and a USB output (= connected to the PC):\n\n\n\nFearing the iPhone’s battery wouldn’t last all day while filming, we powered it via an external battery12. The Lightning-to-HDMI adapter happens to include a lightning plug for power.\n\nHardware:\n\n\n  iPhone 11 Pro Max (another high-end model less than four or five years old would have done the trick as well);\n  lightning to HDMI adapter, with Lightning input for power supply;\n  10m HDMI cable (to reach the control PC);\n  external battery;\n  HDMI capture box: Elgato Camlink 4K;\n  USB cable (between camlink and PC);\n  USB to lightning cable (between the battery and the adapter connected to the iPhone).\n\n\nThe capture box is recognized as a USB webcam. We use it as a video input device in OBS.\n\nWe first put the camera on the side of the room, to not disturb. But the speakers never looked in its direction, and it was not very nice for the remote audience, which felt less included. So we moved the camera almost in front of the speaker. This way, the remote audience feels more like the person speaking is looking in their direction.\nAlso, the camera that films the speaker also films a screen, including when the speaker wants to point to a part of what they are presenting.\n\nAudio recording of the speaker\n\nOur amphitheater is equipped with microphones and speakers, but we don’t yet know how to get the sound from this audio system to inject it into a live stream.\n\nWith the resources and time we had, the best we could do for the speakers was a wireless lapel mic:\n\n\n\nWe didn’t have a wireless mic, but we had something that could act as such, and we ended up with a 3.5” jack to plug into the microphone port of the control PC. This microphone is then used as an audio source in the Google Meet.\n\nEquipment:\n\n\n  Lapel microphone with TRRS Jack 3.5 port: RODE SmartLav+;\n  TRRS female to TRS male adapter (between the microphone and the transmitter box);\n  wireless transmitter + receiver kit (TRS 3.5 jack input on transmitter, TRS 3.5 jack output on receiver): RODE Wireless Go (1st generation);\n  TRS 3.5 male to male cable (between the receiver box and the PC).\n\n\nSome other points and nice “bonuses”\n\nOf course, we didn’t stop at these main points, and lots of other little things came into play throughout the day.\n\nRemote speakers?\n\nI wrote above that I wouldn’t talk about it, and I lied a bit: one of the talks that day was given by a colleague who was working from home.\nHe joined the organizers’ Google Meet, shared his screen, activated his camera, and presented, exactly as many of us have done during more than a year of forced telecommuting because of COVID.\nFor those in the auditorium, one of the organizers plugged in his computer to the two screens in the room and its sound system.\n\nTransitions between sessions\n\nAt the end of each talk, we quickly took over the speaker’s desk and microphone. We wanted to set up the next person, plug in their PC and position the mic so as not to interfere with them, explain how we were broadcasting and validate the setup and configuration.\nAn organizer was in charge of managing the transition, indicating at what time we would resume. But this transition, in the room, was done without a microphone – and therefore, without being broadcast to the live stream.\nFor the live stream, another organizer made the same transition by joining the Google Meet of organizers and speakers. This way, our remote colleagues were not left in the dark and knew when the next talk would resume.\n\nSome useful, or even essential, utilities\n\nOf course, in a large room with few electrical outlets that are not always well placed, you need to bring extension cords and power strips.\nAlso, to avoid someone getting their feet caught in the extension cords or cables (USB, HDMI), bring a roll of duct tape to secure everything to the floor.\nAnd depending on where the lapel microphone is attached (shirt collar, shirt buttonhole…) and where its cable goes (above the shirt in front, under the shirt / under the shirt, on the shoulder and in the back…), more delicate tape13 is very handy.\n\nAt OBS level\n\nOn the control PC, in OBS, we configured several scenes to highlight the speakers or their slides. The images below are screenshots from the live stream, so in 720p maximum and much too compressed…\n\nScreen only\n\nTo display large code examples, videos…\n\n\n\nLarge screen\n\nWith speaker overlay in the bottom right corner: for slides written small, while reflecting an idea of the gestures. And we also had the same thing with the speaker inlay in another corner, for slides with important texts in the bottom right of the screen.\n\n\n\nMosaiq\n\nScreen on the left half of the screen, video of the speaker on the right half: for large written slides and to allow people at a distance to see the speaker clearly. We used this view a lot when the camera was on the side of the room – and we didn’t use it anymore once the camera was in front of the speaker.\n\n\n\nSpeaker only\n\nOnly the speaker’s video, framed to include one of the two screens. We used this view “as if the person watching the stream was sitting in the stands” for much of the afternoon, when the camera was placed in the middle of the audience.\n\n\n\nFor the practical aspects of managing the live stream\n\nChanging scenes in OBS can be done with the mouse (but it’s not convenient) or with keyboard shortcuts (but you have to remember the shortcuts and the scenes they correspond to).\n\nBut it’s much more fun with a Stream Deck: a command pad with big buttons, which you can customize! Yes, it’s a bit gimmicky and not at all essential, but it’s also very cool to use ;-)\n\nHardware:\n\n\n  Elgato Stream Deck;\n  since it connects to USB-A: an adapter or a USB hub.\n\n\n\n\nThe control desk, in photo\n\nA lot of unorganized cables… Here is what the table looked like with the control PC14:\n\n\n\nSome things to improve\n\nOf course, not everything was perfect… Still, for less than an hour and a half of installation and testing that morning, we were quite happy with the result!\n\nFirst of all, Google Live Stream: the solution, integrated to Google Workspace, is very practical. Including the aspect of “limiting access to our employees”15. However, the video quality, in 720p too compressed, is not optimal :-/.\n\nOur auditorium has two cameras fixed to the ceiling. Today, they are not yet functional16, but we hope they will soon replace the iPhone – and allow us to get a view of the audience for questions.\n\nAlso, having only one microphone is problematic when two speakers are speaking for a talk. It happened once during the day, and we put the lapel mic on the lectern and asked the speakers to move closer to it when it was their turn to speak. The RODE SmartLav+ being omni-directional and of good quality, it was just about right… Without being optimal. Also, we didn’t have a microphone for the questions, so the speakers had to repeat them17.\nWe have two wireless microphones in the auditorium. Currently, they are used to amplify the sound in the room and we didn’t have time in the morning when we were setting up to figure out how to capture their output and integrate it into the live stream. Maybe an improvement for next time, because they seem to be pretty good ;-)\nAlso, we didn’t have the time18 to talk with our colleagues who were managing the sound of these events before the COVID: somewhere in a closet, we have a physical mixer, which could have been useful! Another area for improvement!\n\nThat said, now that we’ve seen that it is possible, we want to do even better next time, in two months ;-). And, clearly, a prototype in 1h30 of setup, which worked that well all day long and which improved the experience of 150 or more remote colleagues, is a big success which will push us to do better - and we know we will be able to do so!\nAnd then, with time and successive improvements, we may spend less time each time installing and configuring!\nAlso, a more turnkey solution – like Streamyard19 – might make our lives easier, compared to OBS…\n\nEvery two months, we organize a LFT day at Bedrock: our Last Friday Talks.\nAfter COVID and with the partial return to the office, we wanted our remote colleagues to experience this LFT as best as they could.\nFor this iteration, we did with what we had. It’s up to us to iterate and do even better next time!\nAnd you, how do you share these kinds of events with your remote colleagues?\n\n  \n    \n      Historically, it was the afternoon of the last Friday of every month. We revised the format in 2019 to make it a full day every other month. &amp;#8617;\n    \n    \n      In France, some companies – including ours – started returning to the office, full-time or not, in June 2021, following recommendations from the government. &amp;#8617;\n    \n    \n      One of the talks was given by a remote speaker, I will not talk much about it in this article: he joined the Google Meet of the organizers and shared his camera and screen. An organizer then projected the Google Meet into the auditorium for the live audience. &amp;#8617;\n    \n    \n      We went up to 150 people watching the stream simultaneously. &amp;#8617;\n    \n    \n      Google Live Stream. We use this solution because Google Meet alone does not support enough people in a call. &amp;#8617;\n    \n    \n      Photo taken while projecting talks of the Demuxed conference, for those who wanted to see them on a big screen and together… &amp;#8617;\n    \n    \n      In the photo above, we see the legs of the table and high chairs placed around it. Yes, this space also serves as a break room. &amp;#8617;\n    \n    \n      And the organizers can tap them on the shoulder, to tell them to start their talk or ask them to take a break in case of technical problems… &amp;#8617;\n    \n    \n      High-end smartphones have had very good quality cameras for several years. An iPhone 11 is more than enough to film and broadcast a live conference. &amp;#8617;\n    \n    \n      Filmic Pro: a not cheap application, but very good when it comes to filming with an iPhone. &amp;#8617;\n    \n    \n      Clean HDMI: the filmed video, in real time, without the decorations of the application interface. &amp;#8617;\n    \n    \n      External battery: because there are no electrical outlet in this part of the room and we didn’t want to add more cables and extension cords. &amp;#8617;\n    \n    \n      I use medical tape, which is easy to cut, sticks well and comes off easily too; and is not too aggressive with clothes and skin. &amp;#8617;\n    \n    \n      Yes, it’s a bit of a mess… And, no, you can’t see everything: when we took this picture, we didn’t think we would post it ^^ &amp;#8617;\n    \n    \n      We may broadcast some talks in public in the future… But it was not a topic for this time and we know that some topics will remain internal no matter what. &amp;#8617;\n    \n    \n      We moved in recently and other more important items were configured first. &amp;#8617;\n    \n    \n      A person in the control room tapped the speakers on the shoulder when they forgot to repeat the questions… &amp;#8617;\n    \n    \n      We should have done it a few days in advance, not the morning itself… &amp;#8617;\n    \n    \n      Which we’ve already used at conferences, both as organizers and speakers. &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Best practices for Web application maintenance",
    "category" : "",
    "tags"     : " js, react, web, frontend",
    "url"      : "/2021/09/06/web-best-practices.html",
    "date"     : "September 6, 2021",
    "excerpt"  : "\n  How not to throw away your application every two years?\n\n\nFeedback based on best practices applied to the web platform developed at Bedrock Streaming\n\nA bit of context\n\nAt Bedrock Streaming many teams develop and maintain frontend applications ...",
  "content"  : "\n  How not to throw away your application every two years?\n\n\nFeedback based on best practices applied to the web platform developed at Bedrock Streaming\n\nA bit of context\n\nAt Bedrock Streaming many teams develop and maintain frontend applications for our customers and users.\nSome of those applications are not very young.\nIn fact, the application I’m mainly working on is a website whose developments started in 2014.\nI have already mentioned it in different articles of this blog.\n\n\n\nYou might think: “Oh poor people, maintaining an almost 10 year old application must be hell!”\n\nDon’t worry, it’s not the case!\nI have worked on projects that are much less old but where the development of new features was much more painful.\n\nToday the project is technically up to date, we must be on the latest version of React while it had started on a version 0.x.x.\nIn this world of often criticized web technologies where tools and practices are constantly evolving (eg: the many articles on the Javascript Fatigue), to keep a project “up to date” remains a real challenge.\n\n\n\nMoreover, in the context of this project, in almost 10 years, we have had about 100 contributors.\nSome have only stayed a few months/years.\nHow can we keep the maximum knowledge on “How we do things and how it works?” in such a moving human context?\n\n\n\nThis is what I would like to demonstrate in this post.\n\nWith the help of my colleagues, I have collected the list of good practices that still allow us to maintain this project today.\nWith Florent Dubost, we have often thought that it would be interesting to publish it.\nWe hope you will find it useful.\n\nSet rules and automate them\n\nA project that stands the test of time is first and foremost a set of knowledge that is stacked one on top of the other.\nIt’s like the Kapla tower you used to build as a child, trying to get as high as possible.\nA solid base on which we hope to add as much as possible before a potential fall.\n\nFrom the beginning of a project, we have to make important decisions about “How do we want to do things?\nWe think for example about “What format for our files? How do we name this or that thing?”\nWriting accurate documentation of “How we do things” might seem like a good idea.\n\nHowever, documentation is cool, but it tends to get outdated very quickly.\nOur decisions evolve, but documentation does not.\n\n\n  “Times change but not READMEs.”\n\n  Olivier Mansour (deputy CTO at Bedrock)\n\n\nAutomating the checking of each of the rules we impose on ourselves (on our codebase or our processes) is much more durable.\nTo make it simple, we avoid as much as possible to say “We should do things like that”, and we prefer “we’ll code something that checks it for us”.\nOn top of that, on the JS side we are really well equipped with tools like Eslint that allow us to implement our own rules.\n\nSo the reflex we try to adopt is the following:\n\n\n  “We should try to do it like this now!”\n  “Ok that’s interesting, but how can we make sure we do it like that automatically with our CI (Continuous Integration)?”\n\n\nContinuous Integration of a project is the perfect solution to not miss anything on every Pull Request we provide.\nReviews are only easier because you don’t have to worry about all the rules that are already automated.\nIn this model, the review is more for knowledge sharing than for typo copying and other non-compliance with the project conventions.\n\nIn this principle, we must therefore try to banish oral rules.\nThe time of the druids is over, if all the good practices of a project have to be transmitted orally, it will only take longer to guide new developers into your team.\n\n\n\nA project is not set in stone. These rules evolve with time.\nIt is therefore preferable to add rules that have a script that will autofix the whole codebase intelligently.\nMany Eslint rules offer this, and it is a very important selection criteria when choosing new conventions.\n\neslint --fix\n\n\nA very strict rule that will force you to modify your code manually before each push is annoying in the long run and will annoy your teams.\nWhereas a rule (even a very strict one) that can auto-fix itself at commit time will not be seen as annoying.\n\nHow to decide to add new rules ?\n\nThis question may seem thorny, take for example the case of &amp;lt;tab&amp;gt; / &amp;lt;space&amp;gt; in files.\nFor this, we try to avoid the endless debates and follow the trend and rules of the community.\nFor example, our Eslint configuration base is based on Airbnb’s which seems to have some success in the JS community.\nBut if the rule we want to impose on ourselves is not available in Eslint or other tools, we sometimes prefer not to follow the rule rather than say “We’ll do it without a checking CI”.\n\nThe almost exhaustive list 🤞\n\n\n\n\n  The file format is managed by Editorconfig, prettier and Eslint.\nWe have opensourced our own configuration, if it is of any use to you.\n  We use a specific commit name to generate our changelog.\nTo make sure devs follow it, a simple step in our CI checks it.\n  We don’t want a dev to make our JS bundles very big in production, so we track and measure their size in the CI.\nWe use an in-house tool but we recommend to use the BuildTracker tool.\n  Test coverage is not an indicator for the team, not all lines have the same need for us to be tested.\nSome teams at Bedrock however follow this indicator which at least has the interest to give a trend.\n  Our unit tests obviously run on the CI, these must pass.\n  Our functional tests (End to end: E2E) run on Chrome Headless, they must be green.\n  The logs of our E2E tests are retrieved and parsed to avoid errors or React warnings (the parsing script is however complicated to maintain)\n  Functional tests run in a sandbox where the whole network is proxied.\nWe make sure that our tests do not depend on a non mocked API that could slow down their execution.\n  During the E2E tests we check that no image request has generated a 404.\n  We perform some accessibility checks with Axe during our E2E tests.\n  We check some rules on the CSS with Stylelint and bemlinter (we don’t use BEM anymore but there is still some style managed in SCSS that we migrate little by little in StyledComponent)\n  The project is a monorepo on which we try to maintain the same dependencies versions for each package.\nWe developed a tool which automates this check: monorepo-dependencies-check\n  We check that our yarn.lock file has not been inadvertently modified or that it has been updated with respect to the modifications of the package.json.\n  Terraform is used to manage our cloud resources, we check that the file format is correct.\n\n\nTest, test, test\n\nI hope that in 2021 it is no longer necessary to explain why automatic testing of your application is essential to make it sustainable.\nIn JS, we are rather well equipped in terms of testing tools today.\nHowever, the eternal question remains:\n\n\n  “What do we want to test?”\n\n\nGlobally if we search on the internet this question, we see that different needs make emerge very different practices and testing tools.\nIt would be very presumptuous to think that there is a good way to automatically test your application.\nThis is why it is preferable to define one or more test strategies that meet defined and limited needs.\n\nOur test strategies are based on two distinct goals:\n\n\n  To automate the verification of the functionalities proposed to the users by putting ourselves in their place.\n  To provide us with efficient solutions to specify the way we implement our technical solutions to allow us to make them evolve more easily.\n\n\nTo do this, we perform two “types of tests” that I propose to present here.\n\nOur E2E tests\n\nWe call them “functional tests”, they are End-to-end (E2E) tests on a very efficient technical stack composed of CucumberJS, WebdriverIO with ChromeHeadless\nThis is a technical stack set up at the beginning of the project (at the time with PhantomJS for the oldest among you)\n\nThis stack allows us to automate the piloting of tests that control a browser.\nThis browser will perform actions that are as close as possible to what our real users can do while checking how the site reacts.\n\nA few years ago, this technical stack was rather complicated to set up, but today it is rather simple to do.\nThe site that hosts this blog post is itself proof of this.\nIt only took me about ten minutes to set up this stack with the WebdriverIo CLI to verify that my blog is working as expected.\n\nI recently published an article presenting the implementation of this stack.\n\nSo here is an example of an E2E test file to give you an idea:\n\nFeature: Playground\n\n  Background: Playground context\n    Given I use &quot;playground&quot; test context\n\n  Scenario: Check if playground is reachable\n    When As user &quot;toto@toto.fr&quot; I visit the &quot;playground&quot; page\n    And I click on &quot;playground trigger&quot;\n    Then I should see a &quot;visible playground&quot;\n    And I should see 4 &quot;playground tab&quot; in &quot;playground&quot;\n\n    When I click on &quot;playground trigger&quot;\n    Then I should not see a &quot;visible playground&quot;\n\n    # ...\n\n\nAnd it looks like this in local with my Chrome browser!\n\n\n\nHere is a diagram that explains how this stack works:\n\n\n\nToday, Bedrock’s web application has over 800 E2E test cases running on each of our Pull Request and the master branch.\nThey assure us that we are not introducing any functional regression and that’s just great!\n\n👍 The positives\n\n\n  WebdriverIO also allows us to run these same tests on real devices on a daily basis through the paid SAAS service Browserstack.\nSo we have every day a job that makes sure that our site works correctly on a Chrome last version on Windows 10 and Safari on MacOs.\n  These tests allow us to easily document the functionality of the application using the Gherkin language.\n  They allow us to reproduce cases that are far from nominal.\nIn a TDD logic, they allow us to advance on the development without having to click for hours.\n  These tests allowed us not to break the old version of the site which is still in production for some customers while our efforts are concentrated on the new one.\n  They give us real confidence.\n  Thanks to our library superagent-mock, we can fixturer (plug, mock) all the APIs we depend on and thus even check the error cases.\nAlso, mocking the browser’s XHR layer allows for a significant improvement in test execution time. 🚀\n  They give us access to extended uses like:\n    \n      checking accessibility rules\n      check the browser console logs (to avoid introducing errors or React Warning for example)\n      monitoring all network calls of the site through a proxy\n      and so on…\n    \n  \n\n\n👎 The complications\n\n\n  Maintaining this stack is complicated and expensive.\nSince few resources are published on this domain, we sometimes find ourselves digging for days to fix them 😅.\nSometimes we feel quite alone in having these worries.\n  It is very easy to code a so-called flaky E2E test (ie: a test that can fail randomly).\nThey make us think that something is broken.\nThey sometimes take us a long time to stabilize.\nIt is still much better to remove a test that will not give you a stable result.\n  Running all the tests takes a lot of time on our continuous integration.\nWe must regularly work on their optimization so that the feedback they provide you is as fast as possible.\nThese important times also cost money, because we have to run these tests on machines.\nFor your information, the infrastructure of the website (just the hosting of our Node servers + static files + CDN) cost much less than our continuous integration.\nThis obviously makes our Ops team smile! 😊\n  The new recruits in our teams have often never done this kind of testing, so there is a struggle phase of learning…\n  Some features are sometimes too complicated to test with our E2E stack (for example, payment paths that depend on third parties).\nSo we sometimes fall back on other techniques with Jest, especially with a less unitary scope.\n\n\nOur “unit” tests\n\nTo complete our functional tests we also have a stack of tests written with Jest.\nWe call these tests unit tests because we have as a principle to try to always test our JS modules independently from the others.\n\nLet’s not debate here about “Are these real unit tests?”, there are enough articles on the internet about this topic.\n\nWe use these tests for different reasons that cover needs that our functional tests do not cover:\n\n\n  to help us develop our JS modules with TDD practices.\n  to document and describe how a JS module works.\n  test very/too complicated edge cases with our E2E tests.\n  facilitate the refactoring of our application by showing us the technical impacts of our modifications.\n\n\nWith these tests, we put ourselves at the level of a utility function, a Redux action, a reducer, a React component.\nWe rely mainly on the automock functionality of Jest which allows us to isolate our JS modules when we test.\n\n\n\nThe previous image represents the metaphor that allows us to explain our unit testing strategy to newcomers.\n\n\n  You have to imagine that the application is a wall made of unit bricks (our ecmascript modules), our unit tests must test one by one the bricks in total independence from the others.\nOur functional tests are there to test the cement between the bricks.\n\n\nTo summarize, we could say that our E2E tests test what our application should do, and our unit tests make sure to check how it works.\n\nToday there are more than 6000 unit tests that cover the application and allow to limit regressions.\n\n👍\n\n\n  Jest is really a great library, fast, complete, well documented.\n  Unit tests help us a lot to understand several years later how it all works.\n  We always manage to unit test our code, and it complements our E2E tests well.\n  The automock feature is really handy for breaking down tests by modules.\n\n\n👎\n\n\n  Sometimes we found ourselves limited by our E2E test stack and couldn’t rely solely on unit tests.\nWe were missing something to be able to make sure that the cement between the bricks worked as we wanted it to.\nFor this, a second test stack Jest was set up called “integration test” where the automock is disabled.\n  The abuse of Snapshot is dangerous for your health.\nThe use of “Snapshot testing” can save time on the implementation of your tests but can reduce the quality.\nHaving to review a 50 line object in Snapshot is neither easy nor relevant.\n  With the depreciation of EnzymeJS, we are forced to migrate to React Testing Library.\nIt is of course possible to unit test components with this new library.\nUnfortunately, this is not really the spirit and the way to do it.\nReact Testing Library pushes us not to play with shallow rendering.\n\n\nOur principles\n\nWe try to always follow the following rules when asking the question “Should I add tests?”.\n\n\n  If our Pull Request introduces new user features, we need to integrate E2E test scenarios.\nUnit tests with Jest can complete / replace them accordingly.\n  If our Pull Request aims to fix a bug, it means that we are missing a test case.\nWe must therefore try to add an E2E test or, failing that, a unit test.\n\n\nIt is while writing these lines that I think that these principles could very well be automated. 🤣\n\nThe project stays, the features don’t\n\n\n  “The second evolution of a feature is very often its removal.”\n\n\nAs a matter of principle, we want to make sure that every new feature in the application does not base its activation on simply being in the codebase.\nTypically, the lifecycle of a feature in a project can be as follows (in a Github Flow):\n\n\n  a person implements on a branch\n  the feature is merged on master\n  it is deployed in production\n  lives its feature life (sometimes with bugs and fixes)\n  the feature is not needed anymore\n  a person unravels the code and removes it\n  new deployment\n\n\nTo simplify some steps, we have implemented feature flipping on the project.\n\nHow does it work?\n\nIn our config there is a map key/value that lists all the features of the application associated with their activation status.\n\nconst featureFlipping = {\n  myAwesomeFeature: false,\n  anotherOne: true,\n}\n\n\nIn our code, we have implemented conditional treatments that say “If this feature is activated then…”.\nThis can change the rendering of a component, change the implementation of a Redux action or disable a route in our react-router.\n\nBut what’s the point?\n\n\n  We can develop new evolutions progressively by hiding them behind a configuration key.\nWe deliver features in production without activating them.\n  In a test environment, we can overload this config to test features that are not yet activated in production.\n  In the case of a white label site, we can propose these features to our customers as possible options.\n  Before deleting code of a feature, we deactivate it and clean it up without risk.\n  Thanks to an in-house tool called Applaunch, this feature flipping config can be overloaded on time in a GUI without deployment.\nThis allows us to activate features without putting the code into production.\nIn the event of an incident, we can deactivate features that have been degraded.\n\n\nTo give you a more concrete example, between 2018 and 2020 we completely overhauled the application’s interface.\nThis graphical evolution was just a featureFlipping key.\nThe graphical redesign was not a reset of the project, we still live with both versions (as long as the switchover of all our customers is not completed).\n\n\n\nA/B testing\n\nThanks to the great work of the backend and data teams, we were even able to extend the use of feature flipping by making this configuration modifiable for sub-groups of users.\n\nThis allows us to deploy new features on a smaller portion of users in order to compare our KPI.\n\nDecision making, technical or product performance improvement, experimentation, the possibilities are numerous and we exploit them more and more.\n\nThe future flipping.\n\n\n  Based on an original idea by Florent Lepretre.\n\n\nWe regularly had the need to activate features at very early hours in the future.\nFor that we had to be connected at a precise time on our computer to modify the configuration on the fly.\n\nTo avoid forgetting to do this, or doing it late, we made sure that a configuration key could be activated from a certain date.\nTo do this, we evolved our Redux selector which indicated if a feature was activated so that it could handle date formats and compare them to the current time.\n\nconst featureFlipping = {\n  myAwesomeFeature: {\n    offDate: &#39;2021-07-12 20:30:00&#39;,\n    onDate: &#39;2021-07-12 19:30:00&#39;,\n  },\n}\n\n\n\n  Many coffees ☕️ at 9am have been saved by future flipping.\n\n\nMonitor, Measure, Alert\n\nTo maintain a project as long as Bedrock’s web application, testing, documentation and rigor are not enough.\nYou also need visibility on what works in production.\n\n\n  “How do you know that the application you have in production right now is working as expected?”\n\n\nWe assume that no functionality works until it is monitored.\nToday, monitoring in Bedrock on the frontend side takes the form of different tools and different stacks.\nWe rely on NewRelic, Statsd, a ELK stack and even Youbora for the video streaming part.\n\nTo give you an example, each time a user starts a browsing session we send an anonymous monitoring Hit to increment a counter in Statsd.\nWe then have to define a dashboard that displays the evolution of this number in a graph.\nIf we observe a too important variation, it can allow us to detect an incident.\n\n\n\nMonitoring also offers us solutions to understand and analyze a bug that occurred in the past.\nUnderstanding an incident, explaining it, finding its root cause are the possibilities that are open to you if you monitor your application.\nMonitoring can also allow you to better communicate with your customers about the impact of an incident and also to estimate the number of impacted users.\n\nWith the multiplication of our customers, monitoring our platforms well is not enough.\nToo much data, too many dashboards to monitor, it becomes very easy to miss something.\nSo we started to complement our metrics monitoring with automatic alerting.\nOnce we have enough confidence in the metrics, we can easily set up alerts that will warn us if there is an inconsistent value.\n\nHowever, we try to always trigger alerts only when it is actionable.\nIn other words, if an alert sounds, we have something to do.\nSounding alerts that do not require immediate human action generates noise and wastes time.\n\n\n\nLimit, monitor and update your dependencies\n\nWhat goes out of date faster than your shadow in a web project based on Javascript technologies are your dependencies.\nThe ecosystem evolves rapidly and your dependencies can quickly become unmaintained, out of fashion or completely overhauled with big breaking changes.\n\nWe therefore try as much as possible to limit our dependencies and avoid adding them unnecessarily.\nA dependency is often very easy to add but it can become a real headache to remove.\n\nThe graphic component libraries (e.g. React bootstrap, Material Design) are a good example of dependencies that we do not want to introduce.\nThey can make integration easier at first, but they often freeze the version of your component library later on.\nYou don’t want to freeze the React version in your application for two form components.\n\nMonitoring is also part of our dependency management routines.\nSince the addition of reporting security flaws in an NPM package, it is possible to know if a project has a dependency that contains a known security flaw with a simple command.\nSo we have daily jobs on our projects that run the yarn audit command to force us to apply patches.\n\n\n  Dependency maintenance is greatly facilitated by our E2E test stack which sounds the alarm if the version upgrade generates a regression.\n\n\nToday, except for security flaws, we update our dependencies “when we have time”, often at the end of sprint.\nWe are not satisfied with this because some dependencies can be forgotten.\nI personally use tools like yarn outdated and Dependabot on my personal projects to automate the update of my dependencies.\n\nAccepting your technical debt\n\nA project will always accumulate technical debt.\nThis is a fact.\nWhether it is voluntary or involuntary debt, a project that resists the years will inevitably accumulate debt.\nEven more so, if during all these years you keep adding features.\n\nSince 2014, our best practices, our ways of doing things have evolved well.\nSometimes we decided these changes but sometimes we underwent them (an example, the arrival of functional components with React and the Hooks api).\n\nOur project is not completely “state of art” and we assume it.\n\n\n\nWe try to prioritize our refactoring topics on the parts of the application on which we have the most concern, the most pain.\nWe consider that a part of the application that we don’t like but on which we don’t need to work (bring evolutions) doesn’t deserve that we refactor it.\n\nI could name many features of our application that have not evolved functionally for several years.\nBut since we have covered these features with E2E tests since the beginning, we didn’t really have to touch them.\n\nAs said above, the next evolution of a code feature is sometimes its deactivation.\nSo why spend time rewriting the whole application?\n\n\n  In any case, the code becomes “legacy”.\n  As long as the features are tested, nothing obliges us to refactor everything permanently so that our entire codebase is state of art.\n  We focus on our pain points, we re-factor what we really need to evolve.\n\n\nTo summarize\n\nThe best practices presented here are obviously subjective and will not be perfectly/directly applicable in your contexts.\nHowever, I am convinced that they can probably help you identify what can make your project go from fun to stale.\nAt Bedrock we have other practices in place that I haven’t listed here but that will be the occasion for a new article sometime.\n\nFinally, if you want me to go into more detail on some of the chapters presented here, don’t hesitate to tell me, I could try to dedicate a specific article to it.\n\n"
} ,
  
  {
    "title"    : "Bonnes pratiques pour la maintenance d&#39;une application web",
    "category" : "",
    "tags"     : " js, react, web, frontend",
    "url"      : "/2021/09/01/bonnes-pratiques-web.html",
    "date"     : "September 1, 2021",
    "excerpt"  : "\n  Comment ne pas jeter son application tous les deux ans ?\n\n\nRetour d’expérience basé sur les bonnes pratiques appliquées à la plateforme web développée chez Bedrock Streaming\n\nUn peu de contexte\n\nChez Bedrock Streaming de nombreuses équipes déve...",
  "content"  : "\n  Comment ne pas jeter son application tous les deux ans ?\n\n\nRetour d’expérience basé sur les bonnes pratiques appliquées à la plateforme web développée chez Bedrock Streaming\n\nUn peu de contexte\n\nChez Bedrock Streaming de nombreuses équipes développent et maintiennent des applications frontend pour nos clients et utilisateurs.\nCertaines ne sont pas toute jeune.\nEn effet, l’application sur laquelle je travaille principalement est un site web dont les développements ont commencé en 2014.\nJe l’ai d’ailleurs déjà évoquée dans différents articles de ce blog.\n\n\n\nVous pourriez vous dire: “Oh les pauvres maintenir une application vieille de presque 10 ans ça doit être un enfer !”\n\nRassurez-vous, ce n’est pas le cas !\nJ’ai travaillé sur des projets bien moins vieux mais sur lesquels le développement de nouvelles fonctionnalités était bien plus pénible.\n\nAujourd’hui le projet reste à jour techniquement, on doit être sur la dernière version de React alors que celui-ci avait commencé sur une version 0.x.x.\nDans ce monde des technologies web souvent décrié (ex: les nombreux articles sur la Javascript Fatigue) dont les outils et les pratiques évoluent constamment, conserver un projet “à jour” reste un vrai challenge.\n\n\n\nDe plus, dans le contexte de ce projet, en presque 10 ans, nous avons connu une centaine de contributeurs.\nCertains ne sont restés que quelques mois/années.\nComment garder au maximum la connaissance sur “Comment on fait les choses et comment ça marche ?” dans un contexte humain si mouvant ?\n\n\n\nC’est ce que je vous propose de vous présenter.\n\nAvec l’aide de mes collègues, j’ai rassemblé la liste des bonnes pratiques qui nous permettent encore aujourd’hui de maintenir ce projet en état.\nAvec Florent Dubost, on s’est souvent dit qu’il serait intéressant de la publier.\nNous espèrons que cela vous sera utile.\n\nS’imposer des règles et les automatiser\n\nUn projet qui résiste au temps c’est tout d’abord un ensemble de connaissances qu’on empile les unes sur les autres.\nC’est en quelque sorte la tour de Kapla que vous assembliez petit en essayant d’aller le plus haut possible.\nUne base solide sur laquelle on espère pouvoir ajouter le plus possible avant une potentielle chute.\n\nDès le début d’un projet on est donc amené à prendre des décisions importantes sur “Comment on souhaite faire les choses ?”.\nOn pense par exemple à “Quel format pour nos fichiers ? Comment on nomme telle ou telle chose ?”\nÉcrire une documentation précise de “Comment on fait les choses” pourrait paraitre une bonne idée.\n\nCependant la documentation c’est cool, mais ça a tendance à périmer très vite.\nNos décisions évoluent mais pas la documentation.\n\n\n  “Les temps changent mais pas les README.”\n\n  Olivier Mansour (deputy CTO à Bedrock)\n\n\nAutomatiser la vérification de chacune des règles qu’on s’impose (sur notre codebase ou nos process) est bien plus pérenne.\nPour faire simple, on évite dans la mesure du possible de dire “On devrait faire les choses comme cela”, et on préfère “on va coder un truc qui nous le vérifie à notre place”.\nEn plus de ça, coté JS on est vraiment bien équipé avec des outils comme Eslint qui nous permettent d’implémenter nos propres règles.\n\nLe réflexe qu’on essaie donc d’adopter est donc le suivant:\n\n\n  “On devrait essayer de faire comme cela à présent !”\n  “Ok c’est intéressant, mais comment peut-on s’assurer qu’on fasse comme cela automatiquement avec notre CI (Intégration continue) ?”\n\n\nL’intégration continue d’un projet est la solution parfaite pour ne rien louper sur chacune des Pull Request que nous proposons.\nLes reviews n’en sont que plus simples car vous n’avez plus à vous soucier de l’ensemble des règles qui sont déjà automatisées.\nDans ce modèle, la review sert donc plus au partage de connaissance qu’au flicage de typo et autre non respect des conventions du projet.\n\nDans ce principe, il faut donc essayer de bannir les règles orales.\nLe temps des druides est terminé, s’il faut transmettre oralement toutes les bonnes pratiques d’un projet, l’accompagnement de nouveaux développeurs dans votre équipe n’en sera que plus long.\n\n\n\nUn projet n’est pas figé. Ces règles évoluent donc avec le temps.\nOn préfèrera alors l’ajout de règles qui possèdent un script qui autofixera toute la codebase intelligemment.\nDe nombreuses règles Eslint le proposent, et cela est vraiment un critère de sélection très important dans nos choix de nouvelles conventions.\n\neslint --fix\n\n\nUne règle très stricte qui vous obligera à modifier votre code manuellement avant chaque push est pénible à la longue et énervera vos équipes.\nAlors qu’une règle (même très stricte) qui peut s’autofixer automatiquement au moment du commit ne sera pas perçue comme gênante.\n\nComment décider d’ajouter de nouvelles règles ?\n\nCette question peut paraitre épineuse, prenons par exemple le cas des &amp;lt;tab&amp;gt; / &amp;lt;space&amp;gt; dans les fichiers.\nPour cela, on essaie d’éviter les débats sempiternels et on se plie à la tendance et aux règles de la communauté.\nPar exemple, notre base de configuration Eslint) est basée sur celle d’Airbnb qui semble avoir un certain succès dans la communauté JS.\nMais si la règle qu’on souhaite s’imposer n’est pas disponible dans Eslint ou d’autres outils, il nous arrive de préférer ne pas suivre la règle plutôt que de se dire “On le fait sans CI qui vérifie”.\n\nLa liste presque exhaustive 🤞\n\n\n\n\n  Le format des fichiers est suivi géré par Editorconfig, prettier et Eslint.\nNous avons opensourcé notre propre configuration, si jamais celle-ci peut vous être utile.\n  Nous utilisons un nommage de commit bien spécifique pour générer nos changelog.\nPour s’assurer que les devs le respectent, une simple étape de notre CI le vérifie.\n  On ne souhaite pas qu’un dev fasse grossir énormément nos bundles JS en production, c’est pourquoi nous suivons et mesurons leur taille dans la CI.\nOn utilise un outil maison mais on peut vous recommander l’outil BuildTracker.\n  La couverture de tests n’est pas un indicateur pour l’équipe, toutes les lignes n’ont pas la même nécessité pour nous d’être testées.\nCertaines équipes à Bedrock suivent cependant cet indicateur qui a au moins l’intérêt de donner une tendance.\n  Nos tests unitaires tournent bien évidemment sur la CI, ceux-ci doivent passer.\n  Nos tests fonctionnels (End to end: E2E) tournent sur Chrome Headless, ils doivent être au vert.\n  Les logs de nos tests E2E sont récupérés et parsés afin d’éviter l’introduction d’erreur ou de React warning (Le script de parsing est cependant compliqué à maintenir)\n  Les tests fonctionnels fonctionnent dans une sandbox où tout le réseau est proxyfié.\nNous surveillons que nos tests ne dépendent pas d’une API non mockée qui pourrait ralentir leur exécution.\n  Durant les tests E2E nous vérifions qu’aucune requête d’image n’a généré une 404.\n  On réalise quelques vérifications d’accessibilité avec Axe durant nos tests E2E.\n  On vérifie quelques règles sur le CSS avec Stylelint et bemlinter (on n’utilise plus BEM aujourd’hui mais il reste encore un peu de style géré en SCSS qu’on migre petit à petit en StyledComponent)\n  Le projet est un monorepo sur lequel nous essayons de maintenir les mêmes versions de dépendances pour chaque package.\nPour cela nous avons développé un outil qui permet de faire cette vérification monorepo-dependencies-check\n  On vérifie que notre fichier yarn.lock n’a pas été modifié par inadvertance ou bien qu’il a été mis à jour par rapport aux modifications du package.json.\n  Terraform est utilisé pour la gestion de nos ressources cloud, nous vérifions que le format des fichiers est correct.\n\n\nTester, tester, tester\n\nJ’espère qu’en 2021 il n’est plus nécessaire d’expliquer pourquoi tester automatiquement son application est indispensable pour la rendre pérenne.\nEn JS on est plutôt bien équipé en terme d’outils pour tester aujourd’hui.\nIl reste cependant l’éternelle question:\n\n\n  “Qu’est-ce qu’on veut tester ?”\n\n\nGlobalement si on recherche sur internet cette question, on voit que des besoins différents font émerger des pratiques et des outils de testing bien différents.\nCe serait très présomptueux de penser qu’il y a une bonne manière de tester automatiquement son application.\nC’est pourquoi il est préférable de définir une ou plusieurs stratégies de test qui répondent à des besoins définis et limités.\n\nNos stratégies de tests reposent sur deux volontés bien distinctes:\n\n\n  Automatiser la vérification des fonctionnalités proposées aux utilisateurs en se mettant à sa place.\n  Nous fournir des solutions efficaces pour specifier la manière dont nous implémentons nos solutions techniques pour nous permettre de les faire évoluer plus facilement.\n\n\nPour cela, nous réalisons deux “types de tests” que je propose de vous présenter ici.\n\nNos tests E2E\n\nOn les appelle “tests fonctionels”, ce sont des tests End-to-end (E2E) sur une stack technique très efficace composée de CucumberJS, WebdriverIO avec ChromeHeadless\nIl s’agit d’une stack technique mise en place au début du projet (à l’époque avec PhantomJS pour les plus anciens d’entre-vous)\n\nCette stack nous permet d’automatiser le pilotage de tests qui contrôlent un navigateur.\nCe navigateur va réaliser des actions qui se rapprochent le plus de celles que nos vrais utilisateurs peuvent faire tout en vérifiant comment le site réagit.\n\nIl y a quelques années, cette stack technique était plutôt compliquée à mettre en place, mais aujourd’hui il est plutôt simple de le faire.\nLe site qui héberge cet article de blog en est lui-même la preuve.\nIl ne m’a fallu qu’une dizaine de minutes pour mettre en place cette stack avec le WebdriverIo CLI pour vérifier que mon blog fonctionne comme prévu.\n\nJ’ai d’ailleurs récemment publié un article présentant la mise en place de cette stack.\n\nVoici donc un exemple de fichier de test E2E pour vous donner une idée:\n\nFeature: Playground\n\n  Background: Playground context\n    Given I use &quot;playground&quot; test context\n\n  Scenario: Check if playground is reachable\n    When As user &quot;toto@toto.fr&quot; I visit the &quot;playground&quot; page\n    And I click on &quot;playground trigger&quot;\n    Then I should see a &quot;visible playground&quot;\n    And I should see 4 &quot;playground tab&quot; in &quot;playground&quot;\n\n    When I click on &quot;playground trigger&quot;\n    Then I should not see a &quot;visible playground&quot;\n\n    # ...\n\n\nEt ça donne ça en local avec mon navigateur Chrome !\n\n\n\nVoilà un schéma qui explique comment cette stack fonctionne:\n\n\n\nAujourd’hui, l’application web de Bedrock possède plus de 800 scénarios de tests E2E qui tournent sur chacune de nos Pull Request et sur la branche master.\nIls nous assurent que nous n’introduisons pas de régression fonctionnelle et c’est juste génial !\n\n👍 Les points positifs\n\n\n  WebdriverIO nous permet également de lancer de manière journalière ces mêmes tests sur des vrais devices en passant par le service payant SAAS Browserstack.\nOn a donc tous les jours un job qui s’assure que notre site fonctionne correctement sur un Chrome dernière version sur Windows 10 et Safari sur MacOs.\n  Ces tests nous permettent de facilement documenter les fonctionnalités de l’application grâce au langage Gherkin.\n  Ils nous permettent de reproduire des cas qui sont loin d’être nominaux.\nDans une logique TDD, ils permettent d’avancer sur le développement sans avoir à cliquer pendant des heures.\n  Ces tests nous ont permis de ne pas casser l’ancienne version du site qui est toujours en production pour quelques clients alors que nos efforts se concentrent sur la nouvelle.\n  Ils nous apportent une vraie confiance.\n  Grâce notre librairie superagent-mock, nous pouvons fixturer (bouchonner, mocker) toutes les API dont on dépend et ainsi même vérifier les cas d’erreurs.\nDe plus, mocker la couche XHR du navigateur permet une amélioration significative du temps d’exécution des tests. 🚀\n  Ils nous donne accès à des usages étendus comme :\n    \n      vérification de règles d’accessibilité\n      check les logs de la console navigateur (pour ne pas introduire d’erreur ou de React Warning par exemple)\n      surveiller tous les appels réseaux du site grâce à un proxy\n      et j’en passe…\n    \n  \n\n\n👎 Les complications\n\n\n  Maintenir cette stack est compliqué et coûteux.\nÉtant donné que peu de ressources sont publiées sur ce domaine, on se retrouve parfois à devoir creuser pendant plusieurs jours pour les réparer 😅.\nIl nous arrive de nous sentir parfois bien seul à avoir ces soucis.\n  Il est très facile de coder un test E2E dit flaky (ie: un test qui peut échouer aléatoirement).\nIls nous font croire que quelque chose est cassé.\nIls nous prennent parfois du temps à les stabiliser.\nIl reste cependant bien meilleur de supprimer un test qui ne vous donnera pas un résultat stable.\n  Faire tourner tous les tests prend un temps important sur notre intégration continue.\nIl faut régulièrement travailler sur leur optimisation pour que le feedback qu’ils vous apportent soit le plus rapide possible.\nCes temps importants coutent également de l’argent, il faut en effet bien faire tourner ces tests sur des machines.\nPour information, l’infrastructure du site web (à lui seul, juste l’hébergement de nos servers Node + fichiers statiques + CDN) coutent bien moins cher que notre intégration continue.\nCela fait bien évidemment sourire nos Ops ! 😊\n  Les nouvelles recrues de nos équipes n’ont souvent jamais réalisé ce genre de tests, il y a donc une phase de galère d’apprentissage..\n  Certaines fonctionnalités sont parfois trop compliquées à tester avec notre stack E2E (par exemple, les parcours de paiement qui dépendent de tiers).\nIl nous arrive alors de nous rabattre sur d’autres techniques avec Jest notamment en ayant un scope moins unitaire.\n\n\nNos tests “unitaires”\n\nPour compléter nos tests fonctionnels nous avons également une stack de tests écrits avec Jest.\nOn qualifie ces tests d’unitaires car nous avons comme principe d’essayer de toujours tester nos modules JS en indépendance des autres.\n\nNe débattons pas ici sur “Est-ce que ce sont des vrais tests unitaires ?”, suffisamment d’articles sur internet traitent de ce sujet.\n\nOn utilise ces tests pour différentes raisons qui couvrent des besoins que nos tests fonctionnels ne couvrent pas:\n\n\n  nous aider à développer nos modules JS avec des pratiques TDD.\n  documenter et décrire comment fonctionne un module JS.\n  tester des cas limites très/trop compliqués à tester avec nos tests E2E.\n  faciliter le refactoring de notre application en nous montrant les impacts techniques de nos modifications.\n\n\nAvec ces tests, on se met au niveau d’une fonction utilitaire, d’une action Redux, d’un reducer, d’un composant React.\nOn se base essentiellement sur la fonctionnalité d’automock de Jest qui nous propose d’isoler nos modules JS lorsqu’on teste.\n\n\n\nL’image précédente représente la métaphore qui nous permet d’expliquer notre stratégie de tests unitaires aux nouveaux arrivant.\n\n\n  “Il faut s’imaginer que l’application est un mur composé de briques unitaires (nos modules ecmascript), nos tests unitaires doivent tester une à une les briques en indépendance totale des autres.\nNos tests fonctionnels sont là pour tester le ciment entre les briques.”\n\n\nPour résumer, on pourrait dire que nos tests E2E testent ce que notre application doit faire, et nos tests unitaires s’assurent eux de vérifier comment ça marche.\n\nAujourd’hui ce sont plus de 6000 tests unitaires qui couvrent l’application et permettent de limiter les régressions.\n\n👍\n\n\n  Jest est vraiment une librairie géniale, rapide, complète, bien documentée.\n  Les tests unitaires nous aident beaucoup à comprendre plusieurs années après comment tout cela fonctionne.\n  On arrive toujours à tester unitairement notre code, et cela complète bien nos tests E2E.\n  L’automock est vraiment pratique pour le découpage de tests par modules.\n\n\n👎\n\n\n  Parfois, nous nous sommes trouvés limités par notre stack de tests E2E et nous ne pouvions pas uniquement nous baser sur les tests unitaires.\nIl nous manquait quelque chose pour pouvoir s’assurer que le ciment entre les briques fonctionnait comme on le souhaitait.\nPour cela, il a été mis en place une deuxième stack de tests Jest nommé “test d’intégration” ou l’automock est désactivé.\n  L’abus de Snapshot est dangereux pour la santé.\nL’usage du “Snapshot testing” peut faire gagner du temps sur l’implémentation de vos tests mais peuvent en réduire la qualité.\nAvoir à review un object de 50 lignes en Snapshot est ni facile, ni pertinent.\n  Avec la dépréciation d’EnzymeJS, nous sommes contraints de migrer sur React Testing Library.\nIl est bien évidemment possible de tester unitairement des composants avec cette nouvelle librairie.\nMalheureusement, ce n’est pas vraiment l’esprit et la façon de faire.\nReact Testing Library nous pousse à ne pas jouer avec le shallow rendering.\n\n\nNos principes\n\nNous essayons de toujours respecter les règles suivantes lorsqu’on se pose la question “Dois-je ajouter des tests ?”.\n\n\n  Si notre Pull Request introduit des nouvelles fonctionnalités utilisateurs, il faut intégrer des scenarios de test E2E.\nDes tests unitaires avec Jest peuvent les compléter / remplacer en fonction.\n  Si notre Pull Request a pour but de corriger un bug, cela signifie qu’il nous manque un cas de test.\nOn doit donc essayer de rajouter un test E2E ou à défaut un test unitaire.\n\n\nC’est en écrivant ces lignes que je me dis que ces principes pourraient très bien faire l’objet d’une automatisation. 🤣\n\nLe projet reste, les fonctionnalités non\n\n\n  “La seconde évolution d’une fonctionnalité est très souvent sa suppression.”\n\n\nPar principe, nous souhaitons faire en sorte que chaque nouvelle fonctionnalité de l’application ne base pas son activation sur le simple fait d’être dans la codebase.\nClassiquement, le cycle de vie d’une “feature” dans un projet peut être le suivant (dans un Github Flow):\n\n\n  une personne implémente sur une branche\n  la fonctionnalité est mergée sur master\n  elle est déployée en production\n  vis sa vie de fonctionnalité (avec parfois des bugs et des correctifs)\n  la fonctionnalité n’est plus nécessaire\n  une personne détricote le code et l’enlève\n  nouveau déploiement\n\n\nPour simplifier certaines étapes, il a été mis en place du feature flipping sur le projet.\n\nComment ça marche ?\n\nDans notre config il y a une map clé/valeur qui liste toutes les fonctionnalités de l’application associées à leur statut d’activation.\n\nconst featureFlipping = {\n  myAwesomeFeature: false,\n  anotherOne: true,\n}\n\n\nDans notre code, nous avons donc implémenté des traitements conditionnels qui disent “Si cette feature est activée alors…”.\nCela peut changer le rendu d’un composant, changer l’implémentation d’une action Redux ou bien désactiver une route de notre react-router.\n\nMais à quoi ça sert ?\n\n\n  On peut développer des nouvelles évolutions progressivement en les cachant derrière une clé de configuration.\nOn livre des fonctionnalités en production sans les activer.\n  En environnement de test, on peut surcharger cette config pour tester des features qui ne sont pas encore activées en production.\n  Dans le cas d’un site en marque blanche, on peut proposer ces fonctionnalités à nos clients comme des options possibles.\n  Avant de supprimer le code d’une feature, on la désactive puis on fait le ménage sans risque.\n  Grâce à un outil maison nommé l’Applaunch, cette config de feature flipping est surchargeable dans une interface graphique à chaud sans déploiement.\nCela nous permet d’activer des fonctionnalités sans faire de mise en production du code.\nEn cas d’incident, on peut désactiver des fonctionnalités qui sont dégradées.\n\n\nPour vous donner un exemple plus concret, entre 2018 et 2020 nous avons complètement refondu l’interface de l’application.\nCette évolution graphique n’était qu’une clé de featureFlipping.\nLa refonte graphique n’a donc pas été la remise à zéro du projet, on continue encore aujourd’hui de vivre avec les deux versions (tant que la bascule de tous nos clients n’est pas terminée).\n\n\n\nL’A/B testing\n\nGrâce au super travail des équipes backend et data, on a pu même étendre l’usage du feature flipping en rendant cette configuration modifiable pour des sous groupes d’utilisateurs.\n\nCela permet de déployer des nouvelles fonctionnalités sur une portion plus réduite des utilisateurs afin de comparer nos KPI.\n\nPrise de décision, amélioration des performances techniques ou produit, expérimentations, les possibilités sont nombreuses et nous les exploitons de plus en plus.\n\nLe futur flipping\n\n\n  Sur une idée originale de Florent Lepretre.\n\n\nNous avions régulièrement le besoin d’activer des feature à des heures très trop matinales dans le futur.\nPour cela nous devions être connecté à une heure précise sur notre poste pour modifier la configuration à chaud.\n\nAfin d’éviter d’oublier de le faire, ou de le faire en retard, nous avons fait en sorte qu’une clé de configuration puisse être activée à partir d’une certaine date.\nPour cela, nous avons fait évoluer notre selector redux qui indiquait si une feature était activée pour qu’il puisse gérer des formats de date et les comparer à l’heure courante.\n\nconst featureFlipping = {\n  myAwesomeFeature: {\n    offDate: &#39;2021-07-12 20:30:00&#39;,\n    onDate: &#39;2021-07-12 19:30:00&#39;,\n  },\n}\n\n\n\n  De nombreux cafés ☕️ à 9h ont été sauvés grâce au futur flipping\n\n\nMonitorer, Mesurer, Alerter\n\nPour maintenir un projet aussi longtemps que l’application web de bedrock, des tests, de la documentation et de la rigueur ne suffisent pas.\nIl faut également de la visibilité sur ce qui marche en production.\n\n\n  “Comment sais-tu que l’application que tu as en production en ce moment même fonctionne comme prévu ?”\n\n\nOn part du principe qu’aucune fonctionnalité ne marche tant qu’elle n’est pas monitorée.\nAujourd’hui le monitoring à Bedrock coté Frontend se matérialise par différents outils et différentes stacks.\nJe pourrais vous citer NewRelic, un Statsd, une stack ELK ou bien encore Youbora pour la vidéo.\n\nPour vous donner un exemple, à chaque fois qu’un utilisateur commence une session de navigation on envoie un Hit de monitoring anonyme pour incrémenter un compteur dans Statsd.\nOn a alors plus qu’à définir un dashboard qui affiche dans un graphique l’évolution de ce nombre.\nSi on observe une variation trop importante, cela peut nous permettre de détecter un incident.\n\n\n\nLe monitoring nous offre aussi des solutions pour comprendre et analyser un bug qui s’est produit dans le passé.\nComprendre un incident, l’expliquer, en trouver sa root cause sont les possibilités qui s’offrent à vous si vous monitorez votre application.\nLe monitoring peut également permettre de mieux communiquer avec les clients sur les impacts d’un incident et également d’estimer le nombre d’utilisateurs impactés.\n\nAvec la multiplication de nos clients, bien monitorer nos plateformes n’est plus suffisant.\nTrop de données, trop de dashboards à surveiller, il devient très facile de louper quelque chose.\nNous avons donc commencé à compléter notre suivi des mesures par de l’alerting automatique.\nUne fois que les mesures nous apportent suffisamment de confiance, on peut facilement mettre en place des alertes qui vont nous prévenir en cas de valeur incohérente.\n\nNous essayons cependant de toujours déclencher des alertes uniquement quand celle-ci est actionnable.\nDans d’autres termes, si une alerte sonne, nous avons quelque chose à faire.\nFaire sonner des alertes qui ne nécessitent aucune action immédiate humaine génèrent du bruit et de la perte de temps.\n\n\n\nLimiter, surveiller et mettre à jour ses dépendances\n\nCe qui périme plus vite que votre ombre dans un projet web basé sur des technologies javascript, ce sont vos dépendances.\nL’écosystème évolue rapidement et vos dépendances peuvent vite se retrouver non maintenues, plus à la mode ou bien complètement refondues avec de gros breaking changes.\n\nOn essaye donc dans la mesure du possible de limiter nos dépendances et d’éviter d’en ajouter inutilement.\nUne dépendance, c’est souvent très facile à ajouter mais elle peut devenir un vrai casse-tête à enlever.\n\nLes librairies de composants graphiques (exemple React bootstrap, Material Design) sont un bel exemple de dépendance que nous tenons à ne pas introduire.\nElles peuvent faciliter l’intégration dans un premier temps mais celles-ci bloquent souvent la version de votre librairie de composant par la suite.\nVous ne voulez pas figer la version de React dans votre application pour deux composants de formulaires.\n\nLa surveillance fait aussi partie de nos routines de gestion de nos dépendances.\nDepuis l’ajout du signalement de failles de sécurité dans un package NPM, il est possible de savoir si un projet intègre une dépendance qui contient une faille de sécurité connue par une simple commande.\nNous avons donc des jobs journaliers sur nos projets qui lancent la commande yarn audit afin de nous forcer à appliquer les correctifs.\n\n\n  La maintenance de dépendances est grandement facilité par notre stack de tests E2E qui sonnent direcement si la montée de version génère une regression.\n\n\nAujourd’hui, hors failles de sécurité, nous mettons à jour nos dépendances “quand on a le temps”, souvent en fin de sprint.\nCela ne nous satisfait pas car certaines dépendances peuvent se retrouver oubliées.\nJ’ai personnellement l’habitude d’utiliser des outils comme yarn outdated et Dependabot sur mes projets personels pour automatiser la mise à jour de mes dépendances.\n\nAccepter sa dette technique\n\nUn projet accumulera toujours de la dette technique.\nC’est un fait.\nQue ce soit de la dette volontaire ou involontaire, un projet qui résiste aux années va forcément accumuler de la dette.\nD’autant plus, si pendant toutes ces années vous continuez d’ajouter des fonctionnalités.\n\nDepuis 2014, nos bonnes pratiques, nos façons de faire ont bien évolué.\nParfois nous avons décidé ces changements mais parfois nous les avons subi (un exemple, l’arrivée des composants fonctionnels avec React et l’api des Hooks).\n\nNotre projet n’est pas complètement “state of art” et on l’assume.\n\n\n\nNous essayons de prioriser nos sujets de refactoring sur les parties de l’application sur lequel on a le plus de souci, le plus de peine.\nOn considère qu’une partie de l’application qui ne nous plaît pas mais sur laquelle on n’a pas besoin de travailler (apporter des évolutions) ne mérite pas qu’on la refactorise.\n\nJe pourrais vous citer de nombreuses fonctionnalités de notre application qui n’ont pas évolué fonctionnellement depuis plusieurs années.\nMais comme nous avons couvert ces fonctionnalités de tests E2E depuis le début, nous n’avons pas vraiment eu à y retoucher.\n\nComme dit plus haut, la prochaine évolution d’une feature de code est parfois sa désactivation.\nAlors pourquoi passer son temps à ré-écrire toute l’application ?\n\n\n  Le code devient dans tous les cas du “legacy”.\n  Tant que les fonctionnalités sont testées, rien ne nous oblige à tout refactorer en permanence pour que toute notre codebase soit state of art.\n  On se focalise sur nos pain points, on re-factorise ce qu’on a vraiment besoin de faire évoluer.\n\n\nPour résumer\n\nLes bonnes pratiques présentées ici restent bien évidemment subjectives et ne s’appliqueront pas parfaitement/directement dans vos contextes.\nJe suis cependant convaincu qu’elles peuvent probablement vous aider à identifier ce qui peut faire passer votre projet de fun à périmé.\nÀ Bedrock nous avons mis en place d’autres pratiques que je n’ai pas listées ici mais ce sera l’occasion de faire un nouvel article un jour.\n\nEnfin, si vous souhaitez que je revienne plus en détail sur certains chapitres présentés ici, n’hésitez pas à me le dire, je pourrais essayer d’y dédier un article spécifique.\n\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #14",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/07/16/bedrock-dev-facts-14.html",
    "date"     : "July 16, 2021",
    "excerpt"  : "Apportez vos devfacts à la plage cet été!\n\nLes phases de test\n\n  Pour une fois qu’on est payé à faire des 500\n\n\nLe panier de crabes\n\n  _ On attend le GoNoGo\n\n  _ On ne pourrait pas attendre le RustNoRust ce serait plus performant\n\n\n1… 2… 3… extinc...",
  "content"  : "Apportez vos devfacts à la plage cet été!\n\nLes phases de test\n\n  Pour une fois qu’on est payé à faire des 500\n\n\nLe panier de crabes\n\n  _ On attend le GoNoGo\n\n  _ On ne pourrait pas attendre le RustNoRust ce serait plus performant\n\n\n1… 2… 3… extincteurs\n\n  Parlant à un hébergeur cloud\n\n  “Tu sais la petite boite française en trois lettres qui a eu chaud ce matin?”\n\n\nÉchec et mat\n\n  We’ve been hacked by our incompetence\n\n\nLe vrai du faux\n\n  C’est pas le bon true\n\n\nDarladidadada\nQuand tu fais du ménage dans ton compte AWS de tests et que tu te dis qu&amp;#39;un collègue doit avoir l&amp;#39;oreille musicale 🎶 pic.twitter.com/yIINlboQBy&amp;mdash; Pascal MARTIN (@pascal_martin) March 12, 2021\n\nReview in progress\n\n\nIPV6 c’est pas encore hyper clair\n\n  Oui mais c’est une grosse IP\n\n\nLa confiance\n\n  Normalement, si tout va bien, ça marche!\n\n\nAlors cette mise en production ?\n\n  Elle s’est bien passée mais il a fallu revert\n\n\nEt oui mon petit\n\n  L’Internet, c’était nous\n\n\nUne API c’est pas si mal en fait\n\n  Quand je vois l’UX de la console AWS, je me demande s’il n’y a pas une véritable volonté à nous pousser à faire de l’infrastructure as code\n\n\nDe la part d’un dev backend Php 😱\n\n  J’aime mettre du javascript partout\n\n\nLe retour du come-back de l’automatisation manuelle\n\n  On pourra faire un rattrapage à moitié manuel, mais quand même automatique\n\n\nDans les méandres des fonctionnalités de Airtable\n\n  Bon, on va faire simple, on va écrire du JS\n\n\nUn mercredi à 15h55\n\n  Cette phrase est trop complexe pour un lundi matin\n\n\nÀ bicyclette 🚲\n\n  Le client il s’en fout que tu mettes des petites roues sur ton vélo, il veut juste que t’avances sans te péter la gueule\n\n\nEntendu dans une réunion à distance\n\n  Rassurez-vous c’est pas moi qui ronfle c’est mon chat\n\n\nEn parlant de choix techniques\n\n  Attention le choix pris aujourd’hui fera jurisprudence sur la suite\n\n\nC’est géré !\n\n  Je crois que l’on gère cette erreur, même si on ne la comprend pas\n\n\n1 + 1 = 11\n\n  30 est un nombre énorme\n\n\nEntendu juste après une grosse MEP\n\n  Oh! Il faut que je mette en place un mail de vacances.\n\n\nDes frites 🍟\n\n  Mes critères de succès, c’est les grosses patates\n\n\nPO, pas un métier facile\n\n  Le dev: “Est-ce que les PO peuvent le faire ?”\n\n  Le PO: “On ne sait rien faire nous”\n\n\nBougez pas\n\n  Je déploie le deployer\n\n\nUne légende raconte qu’il faut déployer deux fois le deployer\n\nUne petite équipe\n\n  On était 80 stagiaires sur le projet\n\n\n🎰\n\n  La magie du chmod -R 777. Je te dirais pas que j’en ai rajouté dans mes 3 dernières PR pour fixer de la CI\n\n\nIterm 2 secret features\nMa recherche Google:\n\n  Iterm2 big sur blinking\n\n\nGitlab issue reponse :\n\n  Just turn on Prefs &amp;gt; Advanced &amp;gt; Work around Big Sur bug where a white line flashes at the top of the screen in full screen mode.\n\n\nLes quadricolors\n\n  “_ Pourquoi avoir choisi du vert pour la doc ?”\n\n  “_ Écoute je suis daltonien.”\n\n\nPourquoi douter ?\n\n  “_ Je ne sais pas comment valider fonctionnellement mon développement.”\n\n  “_ Ben, pareil que tu l’as testé.”\n\n  “_ Je ne l’ai pas testé.”\n\n  “_ 😅”\n\n\nTimber !!!!\n\n  Dans la vie comme dans git tu peux pas supprimer la branch sur laquelle t’es.\n\n\nC’est du propre\n\n  La refacto, c’est comme le ménage, il faut en faire régulièrement.\n\n\nLes conflits de canard\n\n  C’est git checkout verdun ton rebase\n\n\nReconversion\n\n  De toute manière avec le réchauffement climatique, je vais tout plaquer et monter un business de vente de djellaba.\n\n\nIncident sur la ligne B\n\n  Parfois les réunions c’est comme les transports, tu es coincé dedans et ça n’avance pas\n\n"
} ,
  
  {
    "title"    : "No-code, ou le développement d’applications ouvert à d’autres métiers !",
    "category" : "",
    "tags"     : " conference, nocode, lowcode, afup",
    "url"      : "/2021/06/11/no-code-developpement-applications-ouvert-autres-metiers-pascal-martin.html",
    "date"     : "June 11, 2021",
    "excerpt"  : "Construire une application sans coder ? C’est une idée que j’entends depuis le début de mes études… Et c’est la promesse de no-code !\nD’ailleurs, pendant que des discours déclarent que nos enfants doivent apprendre à coder à l’école, nous écrivons...",
  "content"  : "Construire une application sans coder ? C’est une idée que j’entends depuis le début de mes études… Et c’est la promesse de no-code !\nD’ailleurs, pendant que des discours déclarent que nos enfants doivent apprendre à coder à l’école, nous écrivons nous-même déjà des applications no-code ! N’avez-vous pas lancé Excel récemment ?\n\nCes dernières années, l’approche no-code a évolué et devient petit à petit un concept viable.\nDes entreprises, startups ou mastodontes, se lancent sur ce marché et publient des outils et solutions qui aident à rivaliser avec certaines des applications que nous aurions pu développer nous-même…\nJe ne parle bien sûr pas (encore?) de supprimer nos métiers… Mais est-ce que no-code ou low-code ne permettraient pas à d’autres profils que les nôtres d’avancer plus rapidement sur leurs projets ?\n\nÀ travers cette introduction, vous découvrirez un pan de l’approche no-code et j’espère vous montrer que le développement d’applications n’est plus réservé qu’aux développeurs… Et que nos langages préférés ne sont plus la réponse à toutes les questions !\n"
} ,
  
  {
    "title"    : "The organisational challenge of building a Data team: lessons learnt",
    "category" : "Data",
    "tags"     : " Data, Data Science, Data Engineering, Agile, BigData, Organization",
    "url"      : "/data/2021/05/19/organisational-challenge-building-data-team.html",
    "date"     : "May 19, 2021",
    "excerpt"  : "Disclaimer: this is a post I’ve been meaning to share ever since the Dailymotion team published this post, so it’s going way back into our history but updated to our current status. It’s also an attempt to bring a counterpart to the blog posts of ...",
  "content"  : "Disclaimer: this is a post I’ve been meaning to share ever since the Dailymotion team published this post, so it’s going way back into our history but updated to our current status. It’s also an attempt to bring a counterpart to the blog posts of many web leaders that have written about what worked but rarely detail the difficulties they met and the lessons they learnt.\nHere’s a very personal (therefore biased) feedback of our story and the lessons we learnt down the road.\n\nAt Bedrock we’re building the best streaming platform in Europe to help our customers be local streaming champions.\nWithin Bedrock (and before that within M6 where our team was born), we embraced Data in 2016. From a top management perspective the move was straight forward once the company had understood the strategic importance of Data and the major use cases we could address: staff a team and deliver massive value (since Data was definitely the new oil back in those days). But for the teams we built (now north of 40 people), it was another story.\n\nFirst iteration: the Dream Team\n\n\n\nThe first step M6 took was to hire an amazing, PhD qualified, Business focused, Data Scientist whose mission was to build a Data Science team and explore the company’s data to find valuable use cases. A few months later, we added an experienced Product owner (me) to help set the vision, the roadmap and lead our Data initiatives. And some months later we were joined by a rock star Data/Machine learning Engineer who was to create an engineering team and the data platform we could build upon.\n\nOur collective mission was very ambitious but also very unclear. Basically, we were in charge of using Data to have a major impact on the company’s business. Nobody in the management had a very clear idea of what that meant and how to get there, that was up to us.\nEach one of us had many ideas on how to make that happen, and quite different approaches of the way to get there. The caricatural (but quite realistic) outline was:\n\n  Our lead Data Scientist wanted data, tools and manpower to explore the data, find awesome machine learning use cases that he would build with the DS team he had staffed, onboard the business, have them buy in and go to production (instantly, in his view the step to production was the tiny part).\n  Our lead Data Engineer wanted to start with small use cases, build a production infrastructure and enrich the science step by step later down the road.\n  I wanted to align with our stakeholders and management to build a shared vision, prioritize ideas and then organize the team work to iterate and increase value on a regular basis.\n\n\n\n\nThis misalignment led to an early split between the Data Scientists who did a lot of exploration + POC’s and the Data Engineers who worked on an industrial production platform. The projects were of different nature and everything was pretty smooth except for some frustration on my side because I was playing little to no role in the Data Science part of things. Overall, everybody was happy, and we started to deliver value… until our first real business (beyond tooling) cases entered the roadmap.\n\nThe first business case where we needed to leverage both engineering and scientific skills was A/B testing. After having scanned the market solutions to find a nice solution we chose to build our own because we wanted deep integration into all our frontends (including mobile apps and set-top boxes) and backends, a lot of freedom to build KPI’s upon all of our Data Lake and a few other things. \nTo build the complete toolbox, we had to create a backend serving layer that would return the correct feature flags for the A/B user groups, calculate KPI’s with a large combination of filters, ensure that the calculation pipeline was run every day and display the results in a decent dashboard. From a project management perspective, it was a headache because the KPI part needed to bring statistical expertise to production, combining statistical excellence and production grade reliability. In our first iteration, a Data Scientist did a POC, but it was no way near production standards. On a separate track, the Data Engineers did a totally separate POC that delivered a decent framework, the KPI’s were calculated incorrectly. Our third iteration (that took 6 months to make happen) was a 2-day session of peer programming between a Data Scientist and a Data Engineer that (at last!) delivered something real: a POC that actually calculated the right metrics in a way that could be industrialized.\n\nThe second business case was just as messed up. Our Data Science team headed off solo to build a POC of a program recommendation engine (we stream videos that belong to TV programs). Within a couple of months, they built something that seemed nice, but didn’t scale when run for our millions of users. They then went into optimising their code, rewriting and optimising again without any Data Engineer help for 4 more months until the algorithm was ready for an A/B test against our long standing business rules. And the POC lost the test. After some rounds of tuning, we stopped the initiative altogether because nobody believed in any potential success.\n\nWe repeated this type of scenario a few times, building up more and more frustration within the 2 teams and the stakeholders. We tried to break it down and make the collaboration work quite a few times, but 2 points couldn’t be resolved:\n\n  The Data Scientists didn’t want to embrace any product/project management, agile or not (though I believe the underlying issue was more about control in a “us” versus “them” mindset).\n  The Data Engineering team refused to put any Python code (the Data Scientists language of choice) into production and imposed that everything would be Java or Scala, versionned on git, unit tested and monitored.\nThis looks like very usual trolls, but they kept us stuck for more than a year.\n\n\nAt the end, our organization literally cracked up. The tensions became conflicts, some people left the team and our top management had to dive in to pacify and rebuild something that would work.\nAlthough we had delivered value in several places, we were clearly under effective.\n\nMy personal take away on our collective difficulties boils down to 2 things:\n\n  Starting off with a bunch of rock stars made collaboration impossible because each one wanted to lead in a very personal way, without much compromise.\n  Letting things slip towards a comfortable separation of 2 expertise teams with 2 very different organisations and agenda’s instead of insisting on aligning them led to a form of cold war with no collaboration at all.\n\n\n\n\nSecond iteration: pluridisciplinary teams\n\nThe conclusion of the management’s deep dive was that we needed to split the historic teams in a more official and long term way.\nThe Data Scientists would form the Data Lab in which they would do research and produce POC’s and whitepapers + staff their own engineers if they needed any. And we would create the Data Factory that would be focused on delivery with the Data Engineers and some new Data Scientists that would work on the team’s backlog with the Data Engineers on a daily basis.\n\n\n\nFrom the Data Lab perspective, that relieved most of the stress because they were now officially free to set a very scientific agenda for themselves. At the start it was cool, but over time (this was 3 years back), their disconnection from “production” put them very far away from the real world. The internal stakeholders turned away from them because their target was to make real things. It ultimately led them to having small business impact and a poor dynamic, ultimately reducing the team down.\nFrom the Data Factory perspective, the new organisation fixed it! Over the past couple of years, we have been building the vision and roadmap within the team with no distinction between the Data Science and Data Engineering roadmap. Our work with the stakeholders feels like we’re now walking on our 2 feet, the solutions we imagine for their challenges mix plain data engineering solutions with algorithms and statistics seamlessly, and we’re definitely delivering more value. And of course, there is no debate on what goes to production, everything the team does will end up live! At the time I’m writing, the Data Factory has scaled from ~10 people to more than 40 members.\n\n\n\nOrganization is one thing, collaboration is another\n\nOur approach to overcome our challenges was totally around changing the organization, but I truly believe the solution was also within the evolution of our mindset.\nWhen we officially split the Data Science and Data Engineering teams, we also increased staffing bringing new people into the game. During that round of staffing, we focused very much on soft skills and mindset, considering that authentic team players would ultimately bring us more value than very strong individuals. Somehow, we moved away from building the dream team and aimed to create a real team. Our idea was that we weren’t doing rocket science, we were working on a bunch of features that needed to incorporate some data science properly in our pipelines.\nOnce the newcomers joined the team, we put a lot of attention into the way people work together, the team spirit they build, the collective dynamic. That went through team building events, defining team values, helping each team member know and understand the other individuals around, creating collaboration rituals, maximizing pair programming between Data Scientists and Data Engineers, etc. And more than anything, entering a more agile test and learn approach, including within the organisation and ways to collaborate.\nI truly believe that this was the key to our success and it’s my major learning today: build a team.\n\nPutting the pieces together\n\nIn the Dailymotion team post I quoted in the introduction, there’s a quite precise and documented blueprint of the organisation about who does what between Data Scientists, Data Analysts and Data Engineers. We’ve never been that documented and structured for now, we’ve given each team a lot of freedom about who does what in the process depending on individual capabilities and what the team likes best.\nOne thing we did do is to sort out the language &amp;amp; tooling trolls. At first, anything that went to production was written in Scala, deployed and reviewed via our Git &amp;amp; continuous deployment pipeline, unit tested and monitored. So if a Data Scientist wanted to work on the production code it was following those rules (that has made some of our Data Scientists need to learn Scala and other things). This is now evolving, thanks to strong local collaboration and we now deliver more and more Python to production :)\n\nConclusion\n\nOver the past 4 years, our data team has scaled from 2 to more than 40 people. In the early days we underestimated the fact that putting excellent individuals together wouldn’t just work. We learnt the hard way that there was specific attention to be put into the organisation and the way we build teams and successful collaboration patterns between people with different domains of expertise and backgrounds. In that, our conclusion is totally similar to the learnings of the Dailymotion team.\nNow we have fixed the mindset and collaboration part of things, we feel nothing can go wrong, even if we changed our organization pattern.\nIf you want to go further, we spoke about this and some other challenges we faced with Morgiane Meglouli in a conference, the slides (in French) are available here\n\nLast but not least, Bedrock is scaling fast to help build streaming champions around Europe. If you’d like to join us, check out our open positions\n\n"
} ,
  
  {
    "title"    : "Comment nous réduisons l’augmentation de nos coûts AWS",
    "category" : "",
    "tags"     : " conference, aws, costs",
    "url"      : "/2021/03/30/comment-nous-reduisons-augmentation-couts-aws-pascal-martin.html",
    "date"     : "March 30, 2021",
    "excerpt"  : "Malgré les promesses du Cloud, votre facture AWS vous fait peur ? Je vous comprends !\n\nVenez découvrir comment, chez Bedrock, nous suivons et catégorisons les coûts d’infrastructure de notre plateforme de VOD et de Replay. J’enchainerai avec un re...",
  "content"  : "Malgré les promesses du Cloud, votre facture AWS vous fait peur ? Je vous comprends !\n\nVenez découvrir comment, chez Bedrock, nous suivons et catégorisons les coûts d’infrastructure de notre plateforme de VOD et de Replay. J’enchainerai avec un retour d’expérience basé sur trois ans de réponses à la question que nous nous posons tous : « comment réduire nos coûts AWS ? »\n\nDiviser par deux une facture DynamoDB ? Exploiter des instances spot à -70% ? Effectuer moins d’appels d’APIs ? Réduire les transferts inter-AZ, massifs lorsque nous manipulons des vidéos ? Ce ne sont que quelques-unes des pistes que nous avons envisagées…\n"
} ,
  
  {
    "title"    : "Migration de 6play vers Le Cloud, retour d’expérience.",
    "category" : "",
    "tags"     : " conference, cloud, migration, cloudsud",
    "url"      : "/2021/03/11/migration-6play-vers-le-cloud-retour-experience-pascal-martin.html",
    "date"     : "March 11, 2021",
    "excerpt"  : "En 2018, nous avons entamé la migration de la plateforme 6play vers Le Cloud.\nÀ présent, nous pilotons notre infrastructure AWS avec Terraform, utilisons des services managés et déployons nos applications sous Kubernetes.\n\nPendant cette conférence...",
  "content"  : "En 2018, nous avons entamé la migration de la plateforme 6play vers Le Cloud.\nÀ présent, nous pilotons notre infrastructure AWS avec Terraform, utilisons des services managés et déployons nos applications sous Kubernetes.\n\nPendant cette conférence, vous découvrirez comment nous avons réalisé cette migration. Vous trouverez des réponses aux questions que vous vous posez si vous envisagez de revoir votre hébergement.\nComment avons-nous transformé notre infrastructure ? Quels impacts sur nos projets ? Comment nous sommes-nous organisés ? Quels choix avons-nous effectués tout au long du processus ? Qu’avons-nous appris, qu’avons-nous fait évoluer ? Comment nos équipes se répartissent-elles les tâches ? Avons-nous dû adapter nos applications PHP ? Quelles difficultés avons-nous rencontrées ?\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #13",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/03/02/bedrock-dev-facts-13.html",
    "date"     : "March 2, 2021",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nDes branchements surprenants\n\n  Ah tu test ta branche en prod ? Tu peux y aller, si tu touches pas au staging pas de soucis\n\n\nAngular conventional commit\n\nTrouvé dans l’historique\n\n\n\nTime is relative\n\n\n ...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nDes branchements surprenants\n\n  Ah tu test ta branche en prod ? Tu peux y aller, si tu touches pas au staging pas de soucis\n\n\nAngular conventional commit\n\nTrouvé dans l’historique\n\n\n\nTime is relative\n\n\n  Non mais c’est normal, ce sont des mois de 5 minutes\n\n\nLe confinement à la campagne, c’est parfois compliqué\n\n\n  Dédicace a tous ceux qui comme moi, vont passer une bonne semaine à batailler avec internet ! (au moins, j’ai assez de débit pour un mp3).\n320 Kbps\n\n\nApple 💰\n\n\n  Quand ca parle d’argent, Apple ne se rate pas généralement\n\n\nLe courage\n\n\n  Une MEP le vendredi avec la CI KO, vous êtes mes héros 😍\n\n\nNote à moi-même\n\n\n  TODO: test if my code is the problem\n\n\nAucun abus\n\n\n  Bon, je crois que je vais m’auto-déclarer meilleur développeur de l’année!\n\n\nUne semaine qui commence bien\n\n\n  3 push master, lundi matin, 11h30. La semaine va être bonne\n\n\nSoit gentil, pas méchant, c’est pas gentil d’être méchant\n\n\n  A: “Je comprends pas l’API elle repond rien”\n\n\n\n  B: “T’as essayé de demander gentiment ?”\n\n\n{\n    &quot;error&quot;: {\n        &quot;status&quot;: 400,\n        &quot;message&quot;: &quot;Invalid parameters &#39;MAIS_TU_VAS_ME_REPONDRE_UN_TRUC_COHERENT_BORDEL_DE_MERDE_!?!?!?!?!?!?!?!?!!&#39; for route &#39;xxxxxxxxxxxxx&#39;&quot;\n    }\n}\n\n\nDes réponses magiques\n\n\n  “so this case is never expected, but obviously it happens” Ah 😅, nous voilà rassuré\n\n\nAlerte générale\n\n\n  Quand on crée une alerte pour pouvoir la mettre en silence juste après 👌\n\n\nLe consentement\n\n\n  A: “J’ai insisté et c’est passé”\n\n\n\n  B: “Tu as insisté mais est-ce que tu as pensé au consentement de GraphQL et de postgre ?”\n\n\n🦥, moi paresseux ?\n\n\n  Même ne rien faire c’est déjà pas mal de taf\n\n\n? Question ?\n\n\n\n🍗 Mnom Mnom\n\n\n  A: “Ah mince j’ai pas mis mon texte en escalope”\n\n\n\n  B: “?”\n\n\n\n  A: “Entre quote”\n\n\nLa documentation éternelle\n\n\n  Les temps changent mais pas les readme\n\n\nLes petits génies\n\n\n  Nan mais attendez vous à plein de commentaires disant que c’est codé avec les pieds, et que Jean-Kevin, 16 ans, aurait mieux fait en 2j pendant son week-end chez sa mamie Germaine, pendant qu’il faisait une pause dans le développement du nouveau Bitcoin.\n\n\nBlackhole sun\n\nEn parlant de lignes supprimées.\n\n\n\nTechnique de sioux\n\n\n  Nan mais réécrit les status 404 en 200 et ça marchera !\n\n\nOn peut revert une fois, mais pas quinze !\n\n\n\nDéfinition AWS\n\n\n  AZ = Ama Zones\n\n\nHeureusement pour nous\n\n\n  Les cas sans erreurs fonctionnent très bien\n\n\nManager === 🐐 ?\n\n\n  La formatrice : je vous donne un mot et vous me donnez le premier mot qui vous passe par la tête.\n\n\n\n  La formatrice : “Chef” ?\n\n\n\n  Un participant : Chèvre\n\n\nIl n’y a pas de “mais” !\n\n\n  Je ne veux pas remettre en cause toute l’architecture MAIS …\n\n\n🐒 Singe ?\n\n\n  Quand j’entends “Go/NoGo”, mon cerveau comprend “Bonobo”\n\n\nAvec du savon !\n\n\n  A: “Ils font du SOAP!”\n\n\n\n  A: “C’est pour être bien propre”\n\n\nL’automatisation DIY\n\n\n  C’est automatiquement fait à la main\n\n\n🦥 le retour\n\n\n  La paresse n’est pas un défaut, c’est une optimisation\n\n\nAstuce de pro!\n\n\n  Si vos collègues et votre conscience vous embêtent quand vous dites « je vais tester ce changement en prod », dites à la place « je vais valider ce changement en prod ». Vous verrez, ça ira tout de suite mieux !\n\n"
} ,
  
  {
    "title"    : "Machine Learning en production",
    "category" : "",
    "tags"     : " machine learning, Lyon Data Science, conference",
    "url"      : "/2021/01/21/machine-learning-en-production.html",
    "date"     : "January 21, 2021",
    "excerpt"  : "Une fois passée la phase de prototype, comment va-t-on en production quand on fait du machine learning ?\nComment s’assure-t-on que tout va bien une fois en production ?\nDéploiement, tests, monitoring, etc. Il y a beaucoup de choses à penser. Sur c...",
  "content"  : "Une fois passée la phase de prototype, comment va-t-on en production quand on fait du machine learning ?\nComment s’assure-t-on que tout va bien une fois en production ?\nDéploiement, tests, monitoring, etc. Il y a beaucoup de choses à penser. Sur ce long sujet, je vous propose ici une petite introduction basée sur mes expériences.\n"
} ,
  
  {
    "title"    : "Three years running Kubernetes on production at Bedrock",
    "category" : "",
    "tags"     : " Infrastructure, Cloud, Kubernetes, Kops, AWS, HAProxy",
    "url"      : "/2020/12/08/Three-years-running-kubernetes-on-production-at-Bedrock.html",
    "date"     : "December 8, 2020",
    "excerpt"  : "We migrated our first application to a Kubernetes cluster at AWS in 2018 (a colleague even wrote a book about it).\nThree years later, we manage a dozen clusters, to which we have added a lot of tools and we have a much better grasp of certain subt...",
  "content"  : "We migrated our first application to a Kubernetes cluster at AWS in 2018 (a colleague even wrote a book about it).\nThree years later, we manage a dozen clusters, to which we have added a lot of tools and we have a much better grasp of certain subtleties.\nEach cluster reaches, depending on the load, hundreds of nodes and thousands of pods.\n\nTable of Contents\n\n\n  Base\n    \n      Kops and templates\n      Tools we use\n      Keep tools up to date on all clusters\n    \n  \n  Resiliency\n    \n      DNS\n      Lots of AutoScalingGroups\n      Dedicated AutoScalingGroups by app\n      QOS Guaranteed Daemonsets\n    \n  \n  Scalability\n    \n      Cluster Autoscaler\n        \n          Expander Priority\n        \n      \n      Overprovisioning\n      PriorityClass\n      Low HPA targets\n      Long downscale durations\n    \n  \n  Observability\n    \n      Metrics\n      Logs\n      Alerting\n    \n  \n  Costs\n    \n      Spot instances\n        \n          Inter accounts reclaims\n          On-demand fallback\n          Draino and node-problem-detector\n          Spot Tips\n        \n      \n      Kube-downscaler\n      HAProxy Ingress Controller\n    \n  \n\n\n\n\nBase\n\nKops and templates\n\nEKS didn’t exist when we started to work on Kubernetes on AWS. So we use Kops which, by the way, works very well.\nKops creates, updates and deletes our clusters, but also associates resources on our AWS accounts: DNS zone + entries, AutoScalingGroups, SecurityGroups, etc.\nOur rolling updates and rolling upgrades are 100% handled by kops which never failed us.\n\nBecause we have several clusters, we use kops toolbox template instead of having a single YAML file per cluster. We have mutualized resources definitions, like AutoScalingGroups, DNS options or namespaces list, inside common files and use a dedicated template file per cluster, referencing mutualized configs through variables.\n\nFor example, the EC2 instance types will be defined as snippets:\n\n± cat snippets/spot_4x_32Gb_machine_type.yaml:\n- c5.4xlarge\n- c5d.4xlarge\n- c5n.4xlarge\n\nAnd used inside a generic template file:\n\n± cat templates/3_spot-nodes.yaml.tpl\n…\n  mixedInstancesPolicy:\n    instances:\n    { { if eq $index &quot;4x_32Gb&quot; } }\n    { { include &quot;spot_4x_32Gb_machine_type.yaml&quot; . | indent 4 } }\n    { { end } }\n…\n\nFinally, if the cluster requires an ASG with instances size 4x with 32GB RAM on Spot instances:\n\n± cat vars/prod-customer.k8s.foo.bar.yaml\n…\nspot_nodes:\n  4x_32Gb:\n    az:\n      - eu-west-3a\n      - eu-west-3b\n      - eu-west-3c\n    min: 1\n    max: 100\n\nA bash script orchestrates all this. It generates manifest files, creates/updates clusters and checks everything is operating normally.\n\nAll of the above lives as files in a git repository, ensuring we’re doing only Infrastructure as Code.\n\nWe never make any Infrastructure modification outside of code.\n\nTools we use\n\nWe add some tools to a raw Kubernetes cluster:\n\n\n  aws-iam-authenticator\n  cluster-autoscaler\n  cloudwatch-exporter-in-cluster\n  cni-metrics-helper\n  draino\n  elasticsearch-cerebro\n  fluentd\n  haproxy-ingress-controller\n  iam-role-for-serviceaccount\n  k8s-spot-termination-handler\n  kube-downscaler\n  logstash\n  loki\n  metrics-server\n  node-problem-detector\n  overprovisioning\n  prometheus\n  prometheus-dnsmasq-exporter\n  prometheus-pushgateway\n  statsd-exporter\n  statsd-proxy\n  victoria-metrics-cluster\n\n\nSome of those tools stand for compatibility reasons after our cloud migration, so our developers can still use our ELK stack, or a statsd format to generate metrics.\n\nWe need all these tools to have a production-ready cluster, so we can provide scaling, resilience, observability, security with controlled costs. This list isn’t even exhaustive.\n\nIt evolved a lot over the last two years and will surely evolve a lot in the near future, as both Kubernetes and AWS are moving playgrounds.\n\nKeep tools up to date on all clusters\n\nWe use a Jenkins job for that.\n\nWe deploy k8s-tools the same way we deploy our apis in the cluster: with bash scripts and a helm chart, dedicated per application.\n\n± tree app/loki/.cloud/       \napp/loki/.cloud/\n├── charts\n│   ├── Chart.yaml\n│   ├── templates/\n│   ├── values.yaml\n│   ├── values.customerX.yaml\n│   └── values.customerY.yaml\n└── jenkins\n    ├── builder.sh\n    └── deployer.sh\n\nA Jenkins job runs the builder.sh, then the deployer.sh script for every k8s-tool.\nbuilder.sh is run when we need to build our own Docker images.\ndeployer.sh handles the Helm Chart deployment subtleties.\nAll apps are first deployed on all our staging clusters, then on prod.\n\nConsistency is maintained over all our clusters through this Jenkins job.\n\nResiliency\n\nDNS\n\nLike everyone who’s using Kubernetes on production, at some point, we faced an outage due to DNS. It was either UDP failing because of a kernel race condition, or musl (Alpine Linux’s replacement of glibc) not correctly handling domain or search, or also the default ndots 5 dnsConfig, or even KubeDNS not handling peak loads properly.\n\nAs of today:\n\n\n  We are using a local DNS cache on each worker node, with dnsmasq,\n  We use Fully Qualified Domain Names (trailing dot on curl calls) as much as possible,\n  We’ve defined dnsConfig preferences for all our applications,\n  We use CoreDNS with autoscaling as a replacement for KubeDNS,\n  We forbid as much as possible musl/Alpine\n\n\nExample of a dns configuration in prod:\n\n  dnsConfig:\n    options:\n    - name: use-vc\n    - name: single-request\n    - name: single-request-reopen\n    - name: ndots\n      value: &quot;1&quot;\n  dnsPolicy: ClusterFirst\n\n\ndnsPolicy: ClusterFirst makes sure we’re using the node’s loopback interface, so pods will send their DNS requests to dnsmasq installed locally on each node.\nDnsmasq forwards DNS queries to CoreDNS for cluster.local. sub-domains and to the VPC’s DNS server for the rest.\n\nLots of AutoScalingGroups\n\nWe had a dozen AutoScalingGroups per cluster.\nThis was both for resiliency and because we use Spot instances.\nWith Spot instance reclaims, we needed to have a lot of instance types and family types: m5.4xlarge, c5.4xlarge, m5n.8xlarge, etc.\nThis is an autoscaler recommendation to split AutoScalingGroups so that each ASG has the same amount of RAM and number of CPU cores when using mixed instances policies.\nAs a result, we had ASGs like:\n\n\n  spot_4x_32Gb\n  spot_4x_64Gb\n  spot_4x_128Gb\n\n\nLots of AutoScalingGroups doesn’t work well\n\nAZ rebalancing doesn’t work anymore when using more than one ASG. It becomes totally unpredictable and uncontrollable. It is even a total nightmare with a dozen ASGs.\n\nYou can see the difference of outgoing traffic between our 3 NAT Gateway over 4 hours time range :\n\nThe blue NAT gateway is used way more than the two others between 19h00 and 22h00. The green NAT gateway is used half as much as the other two during peak usage times.\nThis is because AZ-rebalacing has resulted in twice as many instances in one AZ than in the others.\n\nAlso, Kubernetes’s cluster-autoscaler isn’t really compatible with many AutoScalingGroups. We’ll cover how it works later in this post (Scalability/ExpanderPriority), but keep in mind that each application should run on no more than a maximum of 4 ASGs. This is due to the failover mechanism of cluster-autoscaler that doesn’t detect ASGs errors like InsufficientInstanceCapacity, which considerably increases the scale-up time. We are particularly concerned because we need to scale quickly and intensely.\n\nWe’ve rolled-back on the ASG number. We now have a maximum of 4 ASGs per application group (see next section: Resiliency/DedicatedAutoScalingGroups), with 2 being Spot and 2 on-demand fallbacks.\nFor this reason, we no longer respect the recommendation to split AutoScalingGroups so that each ASG has the same amount of RAM and number of CPU cores, in order to reduce ASGs number.\n\nRunning PHP, the CPU is our bottleneck, not RAM. So we made the choice to have mixed ASG with the same number of CPUs, but not the same amount of RAM.\nThis means that our ASG spot-nodes-8x is composed of m5.8xlarge as well as r5.8xlarge\n\nDedicated AutoScalingGroups by app\n\nWe started to dedicate AutoScalingGroups for some applications when Prometheus was eating all the memory of a node, ending up in OOM errors. Because Prometheus replays its WAL at startup and consumes a lot of memory doing so, adding a Limit over the memory was of no use. It was OOMKill during the WAL process, restarted, OOMKilled again, etc. . Therefore, we isolated Prometheus on nodes having a lot of memory so it could use up all of it.\n\nThen, one of our main API experienced a huge load, 60% IDLE CPU to 0% in a few seconds. Because of the brutality of such a peak, active pods started to consume all CPU available on nodes, depriving other pods. Getting rid of CPU limits is a recommendation that comes with drawbacks that we measured and chose to follow the recommendation to ensure performance. As a result, the entire cluster went down, lacking for available CPU. Airbnb shared the same experience: they removed CPU limits because of throttling, but the noisy neighbors forced them to re-introduce limits.\n\nWe tried to isolate this API on its own nodes, as such peaks can repeat in the future, because it’s uncacheable and userfacing. We added Taints on dedicated nodes and Tolerations on the selected API.\n\nSince then, we had to deploy a dedicated overprovisioning on those nodes as the overprovisioning pods didn’t have this Toleration. It turned out we’re also able to adapt the overprovisioning specifically for this API, which wasn’t the base idea, but it has proven to be very effective due to the API’s nature. We talk more about overprovisioning’s conf a little later on (Scalability/Overprovisioning).\n\nNow, we’re setting CPU limits, at least for all applications not using dedicated nodes and also because we’ve updated our kernels to the patched version. We follow their CPU usage through Prometheus alerting, with:\n\n- labels:\n    severity: notice\n    cluster_name: &quot;{ { $externalLabels.cluster_name } }&quot;\n  annotations:\n    alertmessage: &#39;{ { $labels.namespace } }/{ { $labels.pod } }/{ { $labels.container } } : { { printf &quot;%0.0f&quot; $value } }%&#39;\n    description: Container using more CPU than expected.\n      It will soon be throttled, which has a negative impact on performances.\n    summary: &quot;{ { $externalLabels.cluster_name } } - Notice - K8S - Container using 90% CPU Limit&quot;\n  alert: Notice - K8S - Container getting close to its CPU Limit\n  expr: |\n    (\n      sum(rate(container_cpu_usage_seconds_total{job=&quot;kubelet&quot;, container!=&quot;POD&quot;, container!=&quot;&quot;}[1m])) by (container, namespace, pod)\n    / sum(kube_pod_container_resource_limits_cpu_cores{job=&quot;kube-state-metrics&quot;, container!=&quot;POD&quot;, container!=&quot;&quot;}) by (container, namespace, pod)\n    ) * 100 &amp;gt; 90\n\n\nWe don’t currently have alerting on Throttling, only a Grafana graph using the metric:\n\nsum by (pod) (rate(container_cpu_cfs_throttled_seconds_total{job=&quot;kubelet&quot;, image!=&quot;&quot;,container!=&quot;POD&quot;}[1m]))\n\nAfter Prometheus, we later isolated Victoria Metrics and Grafana Loki on their own ASGs.\nWe’re also isolating “admin” tools, like CoreDNS, cluster-autoscaler, HAProxy Ingress Controller, on dedicated “admin nodes” group. That way, admin tools can’t mess with applications pods and vice versa.\n\n\nDevelopers only deploy to Worker nodes. An application’s pods can only be scheduled on 4 ASGs, including 2 on-demand backups.\n\nOur admin nodes are on-demand. Having an ASG of few nodes all Spot is a risk we didn’t want to take regarding the criticality of those pods.\n\nQOS Guaranteed Daemonsets\n\nAll our Daemonsets have Requests and Limits set at the same value.\nWe’ve found out that a lot of Daemonsets don’t define those values by default.\nEnforcing QOS Guaranteed Daemonsets:\n\n\n  ensures our daemonsets request all the resources they need, which is also important for the k8s scheduler to be more effective\n  daemonsets bad behaviours can be contained through Limits, and will not mess up with pods\n  it’s a good indicator of the overhead we add on each node and helps us choose our EC2 instance types better (E.g: 2x.large instances are too small)\n  it’s a reminder that a server with 16 CPUs has in fact only 80% of them usable by application pods\n\n\nScalability\n\nCluster Autoscaler\n\nWe automatically scale our EC2 Instances with cluster-autoscaler.\n\n\n\nAs mentioned before, we have several AutoScalingGroups per cluster.\nWe use the service discovery feature of cluster-autoscaler to find all ASGs to work with and to control them automatically.\nThis is done in two steps:\n\n\n  We add 2 tags on ASGs that the cluster-autoscaler should manage\n\n\nk8s.io/cluster-autoscaler/enabled: &quot;true&quot;\nk8s.io/cluster-autoscaler/{ { $cluster.name } }: &quot;true&quot;\n\n\n\n  Then, inside the Chart, we add those two labels to the node-group-auto-discovery parameter:\n\n\ncommand:\n- ./cluster-autoscaler\n- --cloud-provider=aws\n- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/{ { index .Values.nodes .Values.env &quot;clusterName&quot; } }\n…\n\n\nExpander Priority\n\nWe use cluster-autoscaler with the expander: priority.\nASGs will be chosen as:\n\n\n  spot-nodes-.*\n  on-demand-.*\n\n\nCluster-autoscaler will randomly add an EC2 instance in an ASG in the first group: spot-nodes-*. If a new instance hasn’t joined the cluster after the fallback timeout (--max-node-provision-time), it will try another ASG in the same group. It will try all the ASGs in this group before moving on to the next group: on-demand-*.\n\nWith a dozen ASGs, most of them being Spot, we’ve already waited for 45 minutes to actually be able to successfully add an EC2 instance.\n\nLaunching an EC2 instance sometimes fails with InsufficientInstanceCapacity, especially for Spot instances. With the autoscaler recommendation to split ASGs by the same amount of CPU/RAM, there were just too many ASGs to try before falling back on-demand. We’ve reduced the cluster-autoscaler fallback timeout to 5 minutes and still are facing many scaling problems at Paris, where it seems there are not many Spot instances available.\n\n\n\nExpander priority allows us to have resilience through an automatic fall back to on-demand when there is no more Spot.\nWe have already faced, multiple times, a fallback to on-demand instances even with a dozen different instance types. InsufficientInstanceCapacity errors are not a myth. Even on-demand instances can be in InsufficientInstanceCapacity, which we hope to never face with expander priority, 10+ Spot instance types, 10+ on-demand instance types and low --max-node-provision-time.\n\nOverprovisioning\n\nWe have overprovisioning pods inside the cluster.\nThe objective is to trigger a node scale-up before a legitimate pod actually needs resources. Doing so, the pod doesn’t wait minutes to be scheduled, but a few seconds. This need for speed is linked to our business and sometimes the television audience bringing us many viewers very quickly.\n\nThis works using overprovisioning pods which request resources without doing anything (docker image: k8s.gcr.io/pause). Those pods are also using a low PriorityClass (-10), lower than our apps.\n\nThis trick is the whole magic of this overprovisioning: we request space that can be reclaimed anytime and very quickly. When an app needs it, the Scheduler will free up this space by expelling overprovisioning pods (of lower priority) because the cluster doesn’t have enough free space. The expelled pods then change their state to Pending with the Reason: Unschedulable because we just filled the cluster with higher priority pods from the app. Presence of Pending Pods with Unschedulable reason trigger the cluster-autoscaler to add nodes.\n\nWe follow the efficiency of this overprovisioning with these Prometheus expressions:\n\n\n  kube_deployment_status_replicas_unavailable: we know which pods are waiting to be scheduled,\n  sum(kube_node_status_condition{condition=&quot;Ready&quot;,status=&quot;false&quot;}): we know if there are UnReady nodes, like when nodes are scaling-up and new nodes don’t have their daemonsets Ready.\n\n\nBecause we have some nice load peaks on our applications, we are using the ladder mode of the overprovisioning. That ensures that we always have a minimum amount of overprovisioning running in the cluster, so we’re able to handle huge loads at any time. Also, we ensure that we don’t waste too much resources when heavily loaded, so we don’t reserve 200 nodes in a cluster of 1000 nodes for example.\n\nThe configmap looks like:\n\ndata:\n  ladder: &#39;{&quot;coresToReplicas&quot;:[[16,4],[100,10],[200,20]]}&#39;\n\n\nWe chose to have big overprovisioning pods, bigger than any other pod in the cluster, to ensure that expelling one of the overprovisioning pods is enough to schedule any Pending pod.\n\nPriorityClass\n\nWe sacrifice some applications when overprovisioning is not enough.\n\nThe overprovisioning magic is based on PriorityClass objects.\nWe’re using the same logic for our other applications, using PriorityClass.\nWe have 3 of them which concern applications:\n\n\n  low: -5\n  default: 0\n  high: 5\n\n\nCritical applications are using the “high” PriorityClass.\nMost applications are using the “default” one, so they don’t even have to explicitly use it.\nWorkers doing asynchronous tasks can be cut off for several tens of minutes without any business impact. These are the ones with the “low” PriorityClass we sacrifice when needed.\n\nHere is an example, during a heavy load :\n\nHundreds of unavailable pods for 10 minutes.\n\nIf we filter out “low” PriorityClass pods in the graph above, there’s only one application having unavailable pods:\n\nNew pods for this application stayed in the Unavailable state for 15 seconds.\n\nLow HPA targets\n\nKubernetes takes time to scale-up pods.\n\nWithout overprovisioning, we’ve measured that we wait up to 4 minutes when there’s no available node where pods can be scheduled.\nThen, with overprovisioning, we mostly wait for 45seconds, between the moment the HorizontalPodAutoscaler changes the Replicas of a Deployment and for those pods to be ready and receive traffic.\n\nWe can’t wait so long during our peaks, so we generally define HPA targets at 60%, 70% or 80% of Requests. That gives us more time to handle the load while new pods are being scheduled.\n\nOn the following graphs, we can see two nice peaks at 20h52 and 21h02:\n\nAbove, in green, the number of consumed CPUs for one specific application: +55% in one minute.\n\nBelow, in blue, new pods are created in response to the peak.\n\n\nThis is obviously not a good way of managing resources, as we waste them as soon as the load balances.\nThis waste effect is amplified with the load: the more pods we have, the more we waste.\n\nYou can see it in this graph that shows the number of CPU reserved but not consumed:\n\n\nWe consume more CPU during peaks and therefore, we use more efficiently the reservations that do not have time to move, because we do not yet have scale-up.\nAs soon as the new pods added in response to the peak are Ready, 40% of CPU are wasted again.\n\nWe don’t have a viable solution to solve this.\n\nWe’re thinking about reducing scale-up duration to 10 seconds, so we won’t need these additional resources while we launch new pods. This is a challenge as the scaling mechanism is composed of several tools (metrics-server update frequency, autoscaler controller loop frequency, pod autoscaler initial readiness delay, probe launch times, etc.) and changing only one of them can have catastrophic behavior on the cluster stability. This huge subject will need its own dedicated blogpost…\n\nLong downscale durations\n\nRecently, we have increased the HPA’s downscale durations from 5 to 30 minutes.\n\nIt’s done through Kops spec:\n  kubeControllerManager:\n    horizontalPodAutoscalerDownscaleStabilization: 30m0s\n\n\nWhen an application fails, its traffic decreases. When front A fails, traffic on backend B decreases too. When front A comes back, both A and B will have a big peak load.\n\nThe default five minutes delay for scaling down pods is too short for us. Increasing this delay makes the return to life of front A transparent on the number of pods of the whole platform, at least for the first 30 minutes of shutdown.\n\nWe’ve seen blog posts where people turn off autoscaling for those very situations.\nFailure is not an extreme case. Failure is expected. Autoscaling strategies must adapt to it.\n\nYou can see that one waits 30 minutes after an upscale, before downscaling:\n\n\nDocumentation specifies that: “this duration specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed.”\n\nWe can observe on the graph above that it’s rather: “this duration specifies how long the autoscaler has to wait to perform a downscale after the last upscale”.\n\nObservability\n\nMetrics\n\nWe scrape application and system metrics via Prometheus.\nWe’re using Victoria Metrics as long term storage. We found it really easy to deploy and it needs really few time to administer on a daily basis, unlike Prometheus.\n\nDetails:\n\n\n  Prometheus scrapes metrics of pods having:\n\n\nannotations:\n  prometheus.io/path: /metrics\n  prometheus.io/port: &quot;8080&quot;\n  prometheus.io/scrape: &quot;true&quot;\n\n\n\n  Then, inside prometheus jsonnet files, we define a remoteWrite pointing to VictoriaMetrics:\n\n\nremote_write:\n- url: https://victoria-metrics-cluster-vminsert.monitoring.svc.cluster.local.:8480/insert/001/prometheus\n  remote_timeout: 30s\n  write_relabel_configs:\n  - separator: ;\n    regex: prometheus_replica\n    replacement: $1\n    action: labeldrop\n  queue_config:\n    capacity: 50000\n    max_shards: 30\n    min_shards: 1\n    max_samples_per_send: 10000\n    batch_send_deadline: 5s\n    min_backoff: 30ms\n    max_backoff: 100ms\n…\n\n\nWe have 2 Prometheus pods per cluster, each on separate nodes.\nEach Prometheus scrapes all metrics in the cluster, for resilience.\nThey have a really low retention (few hours, because of the WAL replay issue) and are deployed on Spot instances.\n\nWe have 2 Victoria Metrics pods per cluster (cluster version), each on separate nodes, separated of Prometheus pods through a podAntiAffinity\naffinity:\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 100\n      podAffinityTerm:\n        labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - prometheus\n        topologyKey: &quot;kubernetes.io/hostname&quot;\n\n\nEach Victoria Metrics pod receives all metrics in duplicate, from the two prometheus pods.\nWe use the command-line flag dedup.minScrapeInterval: 15s to deduplicate metrics.\n\nWe’re thinking about totally removing Prometheus from the mix, using only Victoria Metrics Agent to scrape metrics.\n\nLogs\n\nWe collect stderr and stdout of all our containers.\nWe use fluentd for that, as a DaemonSet, which uses the node’s /var/log/containers directory.\nWe use Grafana Loki as an interface to filter those logs.\n\nOur developers catch most of their logs and send them directly to Elasticsearch. Fluentd and Loki are used only for uncatched errors and have little traffic.\n\nFluentd uses around 200MB of memory per node and so we look at replacing it by promtail which uses only 40MB in our case.\n\n\nWe’re happy with Loki, because we have few logs to parse. We’ve tested to get our Ingress Controller access logs sent to Loki and it was a nightmare. Too many entries to parse.\n\nThere’s a default limit of 1000 log entries when querying, which we raised but then Grafana became very slow. Very very slow. 3000 log entries is the best fit for us.\n\nAlerting\n\nWe mostly use alerts defined in the official prometheus-operator repo.\n\nWe also added some alerts of our own. E.g: an alert when our Ingress Controller can’t connect to a pod:\n- labels:\n    severity: critical\n    cluster_name: &quot;{ { $externalLabels.cluster_name } }&quot;\n  annotations:\n    alertmessage: &#39;{ { $labels.proxy } } : { { printf &quot;%.2f&quot; $value } } requests in error per second&#39;\n    description: &#39;HAProxy pods cannot send requests to this application. Connection errors may happen when one or more pods are failing or there&#39;&#39;s no more healthy pods : Application is crashed !!&#39;\n    summary: &quot;{ { $externalLabels.cluster_name } } - Critical - K8S - HAProxy IC - Backend connection errors&quot;\n  alert: Critical -K8S - HAProxy IC - Backend connection errors\n  expr: |\n    sum(rate(haproxy_backend_connection_errors_total[1m])) by (proxy) &amp;gt; 0\n  for: 1m\n\n\nPrometheus generates alerts that it sends to 2 redundant AlertManager instances, in a separate account that centralises alerts from all our clusters.\nWe have several possibilities then:\n\n\n  Send alerts on Slack dedicated channels\n  Send alerts to PagerDuty for the on-call teams\n\n\nOur developers are managing their own alerts (Kubernetes CRD: PrometheusRule) that are following a different path regarding labels defined. They have their own alerts sent in their own channels.\n\nCosts\n\nSpot instances\n\nWe’re running 100% of our application workloads on Spot instances.\n\nIt was easy at first: implement spot-termination-handler and voilà.\nIndeed, but that was only the first step.\n\nInter accounts reclaims\n\nWe created AWS accounts for salto.fr platform, for which we did a lot of load tests with on-demand servers.\nThat’s when we reclaimed our own instances on our other accounts.\n\n\n\nYour accounts are not “linked” to each other in terms of Spot reclaims. Having resources in the same region with different accounts creates a relationship itself that we had never though about.\nIn this case, launching on-demand instances on one account triggered reclaims on our other accounts in the same region.\n\nOn-demand fallback\n\nWe didn’t have on-demand fallback for a year and it went well.\nThere was enough spot capacity and there was no need for fallback. Therefore, we didn’t prioritize automated on-demand fallbacks.\n\nThen, all our instance types (+10) went InsufficientInstanceCapacity at the same time.\nWe could only work around with a manual ASG we have from our first days on Kubernetes at AWS, on which we could launch on-demand instances as a last-resort fallback.\n\nNow, we’re using cluster-autoscaler with the expander: priority to automatically fallback on lower priority ASGs (see above Scalability/Cluster-autoscaler).\n\nIt takes us around 10mn to start a node when all our instances are InsufficientInstanceCapacity.\nThere are other mechanisms that directly detect InsufficientInstanceCapacity on an ASG, so we wouldn’t have to wait 5mn before moving on to the next one. We’re thinking about implementing them, but they’re not really compatible with cluster-autoscaler right now.\n\nAs of today, we have two ASGs per application group, as Spot, and also two ASGs as on-demand automatic fallback.\n\nDraino and node-problem-detector\n\nThe problem came when downscaling : cluster-autoscaler removes the least used node, no matter if it’s a Spot or an on-demand instance.\n\nWe found ourselves with a lot of on-demand nodes after load peaks and they stayed on. And they cost a lot more than Spot instances.\n\nWe were already using node-problem-detector, so we added draino, to detect if an instance is on-demand and try to remove it when it is. Draino waits for 2h after the node is launched before trying to remove it.\n\nSince then, we use on-demand only when there’s no Spot left and only for a few hours.\n\nWe can see on this graph, that we added automated on-demand fallback and we never stopped having on-demand instances, until we added draino:\n\n\nSpot Tips\n\n\n  You need to be in an “old” AWS region to have a large number of Spot available. I.E. consider eu-west-1 instead of eu-west-3, even if it adds latency,\n  Use the maximum number of instance types possible. A dozen is barely enough. I.E. use all instance family letters regardless of what they’re optimised for (compute, memory) as long as your workload can use it,\n  Use CapacityOptimized spot allocation strategy, to limit reclaims to the strict necessary,\n  Do not use Spot on a single AZ (this advice is not limited to spot),\n  Prepare yourself to large reclaims, dozens at a time,\n  Configure and test your on-demand fallback\n\n\nKube-downscaler\n\nOpen-source project from Zalando which allows us to scale down Kubernetes deployments after work hours: nights and week-ends.\n\nWe use it on all our staging clusters. We save 60% of EC2 instances.\n\nHAProxy Ingress Controller\n\nThe whole traffic of a cluster goes through a single ALB.\n\nWe load-balance traffic to the correct pod through HAProxy, which uses Ingress rules to update its configuration.\nWe explained the way HAProxy Ingress controller lives inside the cluster during a talk at the HAProxy Conf in 2019.\n\nReducing the number of managed load balancers at AWS isn’t the only benefit of HAProxy: we have tons of metrics in a single Grafana dashboard. Requests number, errors, retries, response times, connect times, bad health checks, etc.\n\nWhat’s next\n\nA lot has been done to have scalability, resilience, observability and reasoned costs over the last 3 years.\n\nUsing Kubernetes in production is not that simple.\nIt is necessary to be well equipped, to understand finely the workings of the kubernetes mechanics to find the balance that suits us. Avoid falling into the trap of adopting whatever tool everyone is talking about if you don’t need it. There’s a lot of hype around kubernetes and the cloud, it can be dangerous.\n\nThe next step will be for us to increase resilience as much as possible, while at the same time reducing costs.\nPerhaps it will be by speeding up the start-up of pods. Maybe it won’t work. Maybe we can make some modifications to cluster-autoscaler to make it more compatible with aws events (like InsufficiantInstanceCapacity). But we will certainly work around the costs.\n\n\n\nThanks to all the reviewers, for their good advice and their time ❤️\n"
} ,
  
  {
    "title"    : "PHP, c’est vous ! Et vous pouvez contribuer !",
    "category" : "",
    "tags"     : " conference, php, open-source, afup",
    "url"      : "/2020/11/23/php-cest-vous-et-vous-pouvez-contribuer-pascal-martin.html",
    "date"     : "November 23, 2020",
    "excerpt"  : "Quand nous posons la question « qui contribue à PHP ? » lors des évènements que nous organisons ou auxquels nous participons, nous n’obtenons que très peu de réponses. Est-ce parce que peu d’entre nous savent ou aiment coder en C ? Pourtant, parti...",
  "content"  : "Quand nous posons la question « qui contribue à PHP ? » lors des évènements que nous organisons ou auxquels nous participons, nous n’obtenons que très peu de réponses. Est-ce parce que peu d’entre nous savent ou aiment coder en C ? Pourtant, participer et contribuer ne se limite pas à des lignes de code, loin de là !\n\nAvez-vous trouvé l’éditeur en ligne de la documentation de PHP ? Avez-vous soumis des patchs à composer et symfony ou même à magento et wordpress, qui sont open-source et attendent vos contributions ? Avez-vous testé les versions alpha de PHP 8 ? Voyez-vous comment organiser un Apéro PHP ou un Meetup ? Ou même un AFUP Day ou le Forum PHP ? Savez-vous que l’AFUP a besoin de vous ? Que c’est une association qui est là pour vous aider et ce qu’elle peut vous apporter ?\n\nAvec mille et une façons de contribuer, venez faire un tour d’horizon de modes de contribution que vous n’aviez peut-être pas encore envisagés, ou dont vous vous étiez dit qu’ils n’étaient pas pour vous. Vous verrez que, vous aussi, vous pouvez contribuer à PHP ;-)\n"
} ,
  
  {
    "title"    : "L&#39;open source, ce n&#39;est pas que pour le web",
    "category" : "",
    "tags"     : " conference, afup, open-source",
    "url"      : "/2020/10/23/l'open-source-ce-n-est-pas-que-pour-le-web.html",
    "date"     : "October 23, 2020",
    "excerpt"  : "Une conférence sur l’open source hors des solutions informatiques uniquement, lors du forum PHP 2020 qui marquait les 20 ans de l’AFUP.\n\nConnaissez-vous l’open hardware ? Savez-vous ce que la NASA partage sur Github ? Vous avez certainement déjà é...",
  "content"  : "Une conférence sur l’open source hors des solutions informatiques uniquement, lors du forum PHP 2020 qui marquait les 20 ans de l’AFUP.\n\nConnaissez-vous l’open hardware ? Savez-vous ce que la NASA partage sur Github ? Vous avez certainement déjà écouté, ou produit de la musique open source, savez-vous qu’il existe des médicaments open source ? Répliquer une information, et la partager devient rapide et émancipateur, le monde se libère un peu plus.\n\nAprès une petite plongée dans les principes de partage de l’open source, nous ferons un tour d’horizon des initiatives open source dans d’autres domaines que l’informatique, pour en apprendre un peu plus sur la culture du libre, les rapports de force qui y conduisent, et revenir aux bases du partage.\n"
} ,
  
  {
    "title"    : "DevOps ? Je n&#39;ai jamais voulu faire ça, et pourtant …",
    "category" : "",
    "tags"     : " conference, afup, php, devops",
    "url"      : "/2020/06/24/devops-je-n-ai-jamais-voulu-faire.html",
    "date"     : "June 24, 2020",
    "excerpt"  : "Développeuse junior : première semaine. Mes collègues m’ont forcée à déployer ma première feature sur 6play ! Malgré un petit frisson, tout s’est bien passé, grâce aux outils et bonnes pratiques qui nous guident.\n\nCe n’était que le début ! Depuis,...",
  "content"  : "Développeuse junior : première semaine. Mes collègues m’ont forcée à déployer ma première feature sur 6play ! Malgré un petit frisson, tout s’est bien passé, grâce aux outils et bonnes pratiques qui nous guident.\n\nCe n’était que le début ! Depuis, je gère l’infrastructure de mon projet. Je choisis mes bases de données, caches, mécanismes de stockage, ressources… En prenant en compte leur coût, les modes de backups ou les compétences dans nos équipes. Et je suis libre d’expérimenter avec n’importe quel service que je voudrais tester.\n\nEn un an, je suis passée de “simple développeuse” à quelqu’un qui a conscience de sa plateforme, qui monitore son code et est responsable de sa production. Comment ai-je vécu cette transition ? Comment ai-je grandi en tant que développeuse ?\n\nVous aussi, profitez de votre nouvelle liberté : devenez DevOps !\n"
} ,
  
  {
    "title"    : "6play_API-v2-Final(1).doc",
    "category" : "",
    "tags"     : " conference, php, afup, api",
    "url"      : "/2020/06/24/6play_API-v2-Final(1).html",
    "date"     : "June 24, 2020",
    "excerpt"  : "Votre API est confrontée à des contraintes techniques mais elle doit surtout répondre à vos problématiques métier qui ne cessent d’évoluer. Nous avons souvent vécu cette situation pour 6play (service de Replay du Groupe M6), et il nous a fallu plu...",
  "content"  : "Votre API est confrontée à des contraintes techniques mais elle doit surtout répondre à vos problématiques métier qui ne cessent d’évoluer. Nous avons souvent vécu cette situation pour 6play (service de Replay du Groupe M6), et il nous a fallu plusieurs générations d’API avant d’arriver à une version adaptée à nos besoins. Micro-services, Rest/GraphQL, Developer eXperience… Un récit et des conseils pragmatiques pour concevoir et maintenir votre API.\n"
} ,
  
  {
    "title"    : "React/Redux: pitfalls and best practices",
    "category" : "",
    "tags"     : " js, react, redux, frontend",
    "url"      : "/2020/04/27/react-redux-pitfalls-and-best-pratices.html",
    "date"     : "April 27, 2020",
    "excerpt"  : "After 2 years using React with Redux for the video platform 6play, I was able to identify good practices and pitfalls to avoid at all costs.\nThe Bedrock team kept the technical stack of the project up to date to take advantage of the new features ...",
  "content"  : "After 2 years using React with Redux for the video platform 6play, I was able to identify good practices and pitfalls to avoid at all costs.\nThe Bedrock team kept the technical stack of the project up to date to take advantage of the new features of react, react-redux and redux.\n\nSo here are my tips for maintaining and using React and Redux in your application without going mad.\n\nThis article is not an introduction to React or Redux. I recommend this documentation if you want to see how to implement it in your applications.\n\nYou could also take a look at Redux offical style guide in which you could find some of those tips and others.\nNote that if you use the Redux Toolkit, some of the tips/practices presented in this article are already integrated directly into the API.\n\nAvoid having only one reducer\n\nThe reducer is the function that is in charge of building a new state at each action.\nOne might be tempted to manipulate only one reducer.\nIn the case of a small application, this is not a problem.\nFor applications expressing a complex and evolving business, it is better to opt in for the combineReducers solution.\n\nThis feature of redux allows to manipulate not one but several reducers which act respectively on the state.\n\n\n  When and how to split its application?\n\n\nWhat we recommend at Bedrock is a functional splitting of the application.\nIn my approach, we would tend to represent the business of the application more than the technical stuff implied.\nSome very good articles explain it notably through the use of DDD principles.\n\nIn Bedrock, we use a folder named modules which groups together the different folders associated with the feature of your application.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.reducer.js\n    product/\n      __tests__/\n        product.reducer.spec.js\n      components/\n      product.reducer.js\n    account/\n      __tests__/\n      account.reducer.spec.js\n      components/\n      account.reducer.js\n  store.js\n  index.js\n\n\nSo in store.js all you need to do is combine your different reducers.\n\nimport { createStore, combineReducers } from &#39;redux&#39;\nimport { user } from &#39;./modules/user/user.reducer.js&#39;\nimport { product } from &#39;./modules/user/product.reducer.js&#39;\nimport { account } from &#39;./modules/user/account.reducer.js&#39;\n\nexport const store = createStore(combineReducers({ user, product, account }))\n\n\nBy following this principle, you will:\n\n\n  keep reducers readable because they have a limited scope\n  structure and define the functionalities of your application\n  facilitate the testing\n\n\nHistorically, this segmentation has allowed us to remove complete application areas without having impacts on the entire codebase, just by deleting the module folder associated with the feature.\n\nProxy access to the state\n\nNow that your reducers have been placed in the functional module, you need to allow your components to access the state via selector.\nA selector is a function that has the state as a parameter, and retrieves its information.\nThis can also allow you to select only the props needed for the component by decoupling from the state structure.\n\nexport const getUserName = ({ user: { lastName } }) =&amp;gt; lastName\n\n\nYou can also pass parameters to a selector by wrapping it with a function.\n\nexport const getProduct = productId =&amp;gt; ({ product: { list } }) =&amp;gt;\n  list.find(product =&amp;gt; product.id === productId)\n\n\nThis will allow you to use them in your components using the useSelector hook.\n\nconst MyComponent = () =&amp;gt; {\n  const product = useSelector(getProduct(12))\n  return &amp;lt;div&amp;gt;{product.name}&amp;lt;/div&amp;gt;\n}\n\n\nIt is specified in the react-redux doc that the selector is called for each render of the component.\nIf the selector function reference does not change, a cached version of the object can be returned directly.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.reducer.js\n      user.selectors.js &amp;lt;--- This is where all module selectors are exported\n\n\nPrefix the name of your actions\n\n\n  I really advise you to define naming rules for your actions and if possible check them with an eslint rule.\n\n\nActions are in uppercase letters separated by ‘_’.\nHere an example with this action: SET_USERS.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.actions.js &amp;lt;--- This is where all module action creators are exported\n      user.reducer.js\n      user.selectors.js\n\n\nAction names are prefixed by the name of the module in which it is located.\nThis gives a full name: user/SET_USERS.\nA big advantage of this naming rule is that you can easily filter the action in redux-devtools.\n\n\n\nAlways test your reducers\n\nThe reducers are the holders of your application’s business.\nThey manipulate the state of your application.\n\nThis code is therefore sensitive.\n\n➡️ A modification can have a lot of impact on your application.\n\nThis code is rich in business rules\n\n➡️ You must be confident that these are correctly implemented.\n\nThe good news is that this code is relatively easy to test.\nA reducer is a single function that takes 2 parameters.\nThis function will return a new state depending on the type of action and its parameters.\n\nThis is the standard structure for testing reducers with Jest:\n\ndescribe(&#39;ReducerName&#39;, () =&amp;gt; {\n  beforeEach(() =&amp;gt; {\n    // Init a new state\n  })\n  describe(&#39;ACTION&#39;, () =&amp;gt; {\n    // Group tests by action type\n    it(&#39;should test action with some params&#39;, () =&amp;gt; {})\n    it(&#39;should test action with other params&#39;, () =&amp;gt; {})\n  })\n  describe(&#39;SECOND_ACTION&#39;, () =&amp;gt; {\n    it(&#39;should test action with some params&#39;, () =&amp;gt; {})\n  })\n})\n\n\nI also recommend that you use the deep-freeze package on your state to ensure that all actions return new references.\n\nUltimately, testing your reducers will allow you to easily refactor the internal structure of their state without the risk of introducing regressions.\n\nKeep the immutability and readability of your reducers\n\nA reducer is a function that must return a new version of the state containing its new values while keeping the same references of the objects that have not changed.\nThis allows you to take full advantage of Structural sharing and avoid exploding your memory usage.\nThe use of the spread operator is thus more than recommended.\n\nHowever, in the case where the state has a complicated and deep structure, it can be verbose to change the state without destroying the references that should not change.\n\nFor example, here we want to override the Rhone.Villeurbanne.postal value of the state while keeping the objects that don’t change.\n\nconst state = {\n  Rhone: {\n    Lyon: {\n      postal: &#39;69000&#39; ,\n    },\n    Villeurbanne: {\n      postal: &#39;&#39;,\n    },\n  },\n  Isère: {\n    Grenoble: {\n      postal: &#39;39000&#39;,\n    },\n  },\n}\n\n// When you want to change nested state value and use immutability\nconst newState = {\n  ...state,\n  Rhone: {\n    ...state.Lyon,\n    Villeurbanne: {\n      postal: &#39;69100&#39;,\n    },\n  },\n}\n\n\nTo avoid this, a member of the Bedrock team released a package that allows to set nested attribute while ensuring immutability: immutable-set\nThis package is much easier to use than tools like immutable.js because it does not use Object prototype.\n\nimport set from &#39;immutable-set&#39;\n\nconst newState = set(state, `Rhone.Villeurbanne.postal`, &#39;69100&#39;)\n\n\nDo not use the default case\n\nThe implementation of a redux reducer very often consists of a switch where each case corresponds to an action.\nA switch must always define the default case if you follow so basic eslint rules.\n\nLet’s imagine the following reducer:\n\nconst initialState = {\n  value: &#39;bar&#39;,\n  index: 0,\n}\n\nfunction reducer(initialState, action) {\n  switch (action.type) {\n    case &#39;FOO&#39;:\n      return {\n        value: &#39;foo&#39;,\n      }\n    default:\n      return {\n        value: &#39;bar&#39;,\n      }\n  }\n}\n\n\nWe can naively say that this reducer manages two different actions. It’s okay.\nIf we isolate this reducer there are only two types of action that can change this state; the FOO action and any other action.\n\nHowever, if you have followed the advice to cut out your reducers, you don’t have only one reducer acting on your blind.\n\nThat’s where the previous reducer is a problem.\nIndeed, any other action will change this state to a default state.\nA dispatch action will pass through each of the reducers associated with this one.\nAn action at the other end of your application could affect this state without being expressed in the code.\nThis should be avoided.\n\n\n\nIf you want to modify the state with an action from another module, you can do so by adding a case on that action.\n\nfunction reducer(state = initialState, action) {\n  switch (action.type) {\n    case &#39;FOO&#39;:\n      return {\n        value: &#39;foo&#39;,\n      }\n    case &#39;otherModule/BAR&#39;:\n      return {\n        value: &#39;bar&#39;,\n      }\n    default:\n      return state\n  }\n}\n\n\nUse custom middlewares\n\nI’ve often seen action behaviors being copied and pasted, from action to action.\nWhen you’re a developer, “copy-paste” is never the right way.\n\nThe most common example is handling HTTP calls during an action that uses redux-thunk.\n\nexport const foo = () =&amp;gt;\n  fetch(&#39;https://example.com/api/foo&#39;)\n    .then(data =&amp;gt; ({ type: &#39;FOO&#39;, data }))\n    .catch(error =&amp;gt; {\n      // Do something\n    })\n\nexport const bar = () =&amp;gt;\n  fetch(&#39;https://example.com/api/bar&#39;)\n    .then(data =&amp;gt; ({ type: &#39;BAR&#39;, data }))\n    .catch(error =&amp;gt; {\n      // Do something\n    })\n\n\nThese two actions are basically the same thing, we could very well make a factory that would do the code in common.\n\nBasically the meta action we want to represent here when it is dispatched:\n\nFetch something\n-- return action with the result\n-- in case or error, do something\n\n\nWe could very well define a middleware that would take care of this behavior.\n\nconst http = store =&amp;gt; next =&amp;gt; async action =&amp;gt; {\n  if (action.http) {\n    try {\n      action.result = await fetch(action.http)\n    } catch (error) {\n      // Do something\n    }\n  }\n  return next(action)\n}\n\n// in redux store init\nconst exampleApp = combineReducers(reducers)\nconst store = createStore(exampleApp, applyMiddleware(http))\n\n\nThus the two preceding actions could be written much more simpler:\n\nexport const foo = () =&amp;gt; ({ type: &#39;FOO&#39;, http: &#39;https://example.com/api/foo&#39; })\n\nexport const bar = () =&amp;gt; ({ type: &#39;BAR&#39;, http: &#39;https://example.com/api/bar&#39; })\n\n\nThe big advantages of using middleware in a complex application:\n\n\n  avoids code duplication\n  allows you to define common behaviors between your actions\n  standardize redux meta action types\n\n\nAvoid redux related rerender\n\nThe trick when using redux is to trigger component re-render when you connect them to the state.\nEven if rerenders are not always a problem, re-render caused by the use of redux really has to be prevented.\nJust beware of the following traps.\n\nDo not create a reference in the selector\n\nLet’s imagine the next selector:\n\nconst getUserById = userId =&amp;gt; state =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId) || {}\n\n\nThe developer here wanted to ensure that its selector is null safe and always returns an object.\nThis is something we see quite often.\n\nEach time this selector will be called for a user not present in the state, it will return a new object, a new reference.\n\n\n  With useSelector, returning a new object every time will always force a re-render by default.\nDoc of react-redux\n\n\nHowever in the case of an object, as in the example above (or an array), the reference of this default value is new each time the selector is executed.\nSimilarly for the default values in destructuring, you should never do this :\n\nconst getUsers = () =&amp;gt; ({ users: [] }) =&amp;gt; users\n\n\nWhat to do then?\nWhenever possible, the default values should be stored in the reducer.\nOtherwise, the default value must be extracted into a constant so that the reference remains the same.\n\nconst defaultUser = {}\n\nconst getUserById = userId =&amp;gt; state =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId) || defaultUser\n\n\nThe same goes for the selector usage that returns a new ref at each call.\nThe use of the filter function returns a new array each time a new reference even if the filter conditions have not changed.\n\nTo continue, it is important that useSelector does not return a function.\nBasically you should never do this:\n\nconst getUserById = state =&amp;gt; userId =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId)\nconst uider = useSelector(getUserById)(userId)\n\n\nA selector should not return a view (a copy) of the state but directly what it contains.\nBy respecting this principle, your components will rerender only if an action modifies the state.\nUtilities such as reselect can be used to implement selectors with a memory system.\n\nDo not transform your data in the components\n\nSometimes the data contained in the state is not in the correct display format.\nWe would quickly tend to generate it in the component directly.\n\nconst MyComponent = () =&amp;gt; {\n  const user = useSelector(getUser)\n\n  return (\n    &amp;lt;div&amp;gt;\n      &amp;lt;h1&amp;gt;{user.name}&amp;lt;/h1&amp;gt;\n      &amp;lt;img src={`https://profil-pic.com/${user.id}`} /&amp;gt;\n    &amp;lt;/div&amp;gt;\n  )\n}\n\n\nHere, the url of the image is dynamically computed in the component, and thus at each render.\nWe prefer to modify our reducers in order to include a profileUrl attribute so that this information is directly accessible.\n\nswitch (action.type) {\n  case `user/SET_USER`:\n    return {\n      ...state,\n      user: {\n        ...action.user,\n        profileUrl: `https://profil-pic.com/${action.user.id}`,\n      },\n    }\n}\n\n\nThis information is then calculated once per action and not every time it is rendered.\n\nDon’t use useReducer for your business data\n\nSince the arrival of hooks, we have many more tools provided directly by React to manage the state of our components.\nThe useReducer hook allows to set a state that can be modified through actions.\nWe’re really very very close to a redux state that we can associate to a component, it’s great.\n\nHowever, if you use redux in your application, it seems quite strange to have to use useReducer.\nYou already have everything you need to manipulate a complex state.\n\nMoreover, by using redux instead of the useReducer hook you can take advantage of really efficient devtools and middlewares.\n\n\n\nUseful resources\n\n\n  Use react with redux doc\n  redux flow animated by Dan Abramov\n\n  redux documentation about middlewares\n  immutable-set\n\n\nThanks to the reviewers: \n@flepretre, \n@mfrachet, \n@fdubost,\n@ncuillery,\n@renaudAmsellem\n"
} ,
  
  {
    "title"    : "How to boost the speed of your webpack build?",
    "category" : "",
    "tags"     : " js, webpack",
    "url"      : "/2020/03/05/hunting-webpack-performances.html",
    "date"     : "March 5, 2020",
    "excerpt"  : "How did I cut in half my project’s webpack build time ?\n\nWho never complained about the infinite duration of a webpack build on a project ?\nI’m currently working on a big web application coded in React/Redux with server side rendering.\nThe applica...",
  "content"  : "How did I cut in half my project’s webpack build time ?\n\nWho never complained about the infinite duration of a webpack build on a project ?\nI’m currently working on a big web application coded in React/Redux with server side rendering.\nThe application exists since 2015 and it has evolved a lot since then\n\n\n\nTLDR;\n\n\n  Never, ever, ever, ever work on performance improvements or optimization without monitoring!\n\n\nIf you want to optimize the duration of a job, you have to monitor precisely the duration of it and all its sub-steps.\nBy doing that, you can really focus on the most expensive task.\nThis will save you from wasting time on optimizations that will have little impact on the system as a whole.\nUse existing monitoring tools! Create them if they don’t exist!\n\nWhat was the problem with webpack ?\n\nFor several weeks/months my colleagues had been complaining about the duration of our yarn build command. \nThe purpose of this command is to build the distributable package of our application in a production target with webpack.\n\nI even heard:\n\n  “This command, I don’t run it locally anymore, it takes too much time.”\n  “My computer starts ventilating heavily every time I run this command. There’s nothing else I can do!”\n\n\nDepending on the machine on which the build was launched, it took between 5 and 12 minutes.\nIt is not possible to have a build that takes so long.\nwebpack is not a slow bundler. \nIt is our use of webpack that makes it slow.\n\nFocus error, a morning lost\n\nSince this command launches a webpack build in production mode, I figured out that the culprit was webpack config itself.\nGiven that I’ve dug deep into webpack, I thought it would be interesting to focus on this performance concern.\nI have indeed open sourced a set of workshop to learn how to use webpack from scratch (https://webpack-workshop.netlify.com).\nSo at the end of January I took one day to improve the situation.\n\nI had my own idea of the task that would take the most. So I tried to improve it, spending my entire morning on it. \nI just managed to gain 17 seconds.\n\nI’m not going to lie, I was very disappointed with what I achieved.\n\nThe concern in my strategy was however obvious. \nI started off with a preconceived idea “This is definitely the stage that takes the longest.”\n\nNothing was objective in my analysis.\nTo improve the performance of an application it is necessary to focus on objective facts.\n\nSuccessful afternoon\n\nWhen I came back from my lunch break, I was motivated to win more than those poor 17 seconds.\nThen I remembered the Pareto principle.\n\n\n  The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes.\nWikipedia\n\n\nThere is probably one step that takes up most of the webpack build time.\nPareto principle adapted to webpack could mean “80% of the build time is caused by 20% of the config”\n\nLet’s find the culprit ! 🎉\n\nI had to determine the build time of each loader, of each plugin.\nI was very lucky, the webpack community has already proposed a plugin that allows to measure everything.\nAnd it is very easy to install. ♥️\n\nSpeed Measure Plugin\n\nHere are the results I got:\n\nSMP  ⏱  \nGeneral output time took 4 mins, 5.68 secs\n\n SMP  ⏱  Plugins\nIgnorePlugin took 57.73 secs\nTerserPlugin took 39.022 secs\nExtractCssChunksPlugin took 3.13 secs\nOptimizeCssAssetsWebpackPlugin took 1.6 secs\nManifestPlugin took 1.55 secs\nWebpackPwaManifest took 0.326 secs\nContextReplacementPlugin took 0.129 secs\nHashedModuleIdsPlugin took 0.127 secs\nGenerateSW took 0.059 secs\nDefinePlugin took 0.047 secs\nEnvironmentPlugin took 0.04 secs\nLoadablePlugin took 0.033 secs\nObject took 0.024 secs\n\n SMP  ⏱  Loaders\nbabel-loader, and \nrev-replace-loader took 2 mins, 11.99 secs\n  module count = 2222\nmodules with no loaders took 1 min, 57.86 secs\n  module count = 2071\nextract-css-chunks-webpack-plugin, and \ncss-loader, and \npostcss-loader, and \nsass-loader took 1 min, 43.74 secs\n  module count = 95\ncss-loader, and \npostcss-loader, and \nsass-loader took 1 min, 43.61 secs\n  module count = 95\nfile-loader, and \nrev-replace-loader took 4.86 secs\n  module count = 43\nfile-loader took 2.67 secs\n  module count = 32\nraw-loader took 0.446 secs\n  module count = 1\n@bedrock/package-json-loader took 0.005 secs\n  module count = 1\nscript-loader took 0.003 secs\n  module count = 1\n\n\nAs expected, it’s not great! \nBut at least I’m starting to get who the culprits are.\nWe can see that for 2222 Javascript modules takes up 2mins but for only 95 Sass files 1min43 🤣.\n\n\n\nDamn node-sass\n\nOnce the migration from node-sass to sass (new Sass re-implementation) and the update of sass-loader, I was shocked!\nIt took me about 10 minutes because there were few breaking changes and I gained more than 1min30 on the build time.\n\nsass-loader made big improvements on performances, you should definitely make sure you use the last version.\n\nI lost a morning on gaining 17 seconds and I spent 10 minutes to win 1min30.🤣\n\nIgnorePlugin, TerserPlugin\n\n\n  \n    TerserPlugin is used to uglify the javascript code in order to reduce its size and readability. It’s a relatively long process, but 39 seconds is too much.\nJust by updating the version of TerserPlugin to use the one integrated in Webpack, I managed to reduce by 20 seconds the build time.\n  \n  \n    IgnorePlugin is a core plugin that was used a lot in our application to avoid loading certain scripts in order to reduce the weight of the site.\nIt was necessary, but today with Webpack we can use much better than that. Dynamic Import, ContextReplacement, there are plenty of solutions. As a general rule, we should avoid compiling files and then not using them.\n  \n\n\nRecommendations from the community\n\nTo improve the build perfs webpack provides a web page listing the actions to take to hunt what takes time.\nI strongly advise to have a look at it.\n\nhttps://webpack.js.org/guides/build-performance/\n\nFinal Result\n\n    SMP  ⏱  \n    General output time took 2 mins, 18.27 secs\n\n\n\n\nBased on precise and concrete measures, I was able to drastically improve the webpack build of my application.\nNo more computers suffering just to compile a bit of JS and SASS.\nI could have lost whole days on futile modifications if I had not measured precisely what penalized the build.\n\nℹ️\n\n  Use Speed Measure Plugin to debug webpack build time\n  Track your build time evolution to detect big evolution before merge\n  Follow webpack performances recommandations\n  Look at webpack 5 new caching strategies\n  Keep your webpack config up to date\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #12",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2020/02/08/m6web-dev-facts-12.html",
    "date"     : "February 8, 2020",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nEn parlant de slack\n\n  stalker ou bosser il faut choisir\n\n\nEn parlant d’ornithorynque:\n\n  Non mais l’australie, c’est le staging du monde\n\n\nLe dresseur\n\n  Mon charisme légendaire a encore frappé… les VPN...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nEn parlant de slack\n\n  stalker ou bosser il faut choisir\n\n\nEn parlant d’ornithorynque:\n\n  Non mais l’australie, c’est le staging du monde\n\n\nLe dresseur\n\n  Mon charisme légendaire a encore frappé… les VPNs filent au pas avec moi…\n\n\nCloudix\n\n  Les gaulois qui avaient peur que le ciel/cloud leur tombe sur la tête, ils étaient visionnaires en fait!\n\n\nLe meilleur support\nDans un bar de Lyon\n\n  Une inconnue: “ je peux pas me connecter à l’amour est dans le pré …”\n\n\n\n  Un dev d’M6web: “c’est quoi ta box ?”\n\n\nDes choix forts\n\n  Si on fait ce choix, c’est comme tuer un chaton\n\n\nObjets trouvés\n\n\nManger ou décider il faut choisir\n\n  Une réunion sans nourriture devrait être un email.\n\n\nQuand un sysadmin craque une allumette à la data\n\n  Scala de toute manière, ce n’est qu’un wrapper Java\n\n\nC’était prévu\n\n  “Quand on passera en prod le bug”\n\n\nChandeleur\n\n  J’ai de la pâte, j’ai du sucre mais je sais pas faire des grep\n\n\nLa fatigue\n\n  Définition de fatigue =&amp;gt;  “Ah des weekend comme ça je n’en veux plus” … Dit-il un vendredi\n\n\nSSO POWA\n\n  Vivement qu’on passe slack sous le SSO, comme ça les gens ne viendront plus nous dire sur slack qu’ils n’ont pas accès au SSO\n\n\nAquarium\n\n\nToutou\n\n  J’ai fait un fichier waf.tf dans le module waf, ca fait waf waf\n\n\nConfluence\n\n  Mais effectivement à ce niveau c’est aussi facile d’écrire des .txt qu’on se file par clé usb que d’utiliser Confluence.\n\n\nQuickwin\n\n  C’est un quickwin jusqu’à ce qu’on fasse F5\n\n\nJavascript\n\n  Un dev PHP: “C’est du JS mais ça se comprend presque”\n\n\nLa persévérance\n\n\nLes coûts\n\n  C’est le type de projet où il y a plus de jour-homme que d’utilisateurs\n\n\nJe l’ai pas touchéééo\n\n  Un sysadmin: “C’est quoi le foot?”\n\n\n\n  Un autre: “C’est le truc qui fait tomber ton infra quand on en diffuse.”\n\n\nLa concurrence et la mauvaise foi\n\n  Du coup tous les trucs inutiles de netflix on les fait, mais tous les trucs utiles on les fait pas\n\n\nOulà!\n\n  C’est fou ce qu’on peut faire aujourd’hui avec des spreadsheets…. Je crois bien que c’est la dernière fois que j’ai codé\n\n\nUn certain K.D\n"
} ,
  
  {
    "title"    : "Migration de 6PLAY - l&#39;amour est dans le cloud",
    "category" : "",
    "tags"     : " conference, video",
    "url"      : "/2020/02/07/pascal-martin-laduckconf.html",
    "date"     : "February 7, 2020",
    "excerpt"  : "Pascal Martin a eu le plaisir d’être invité par Octo pour un REX sur la migration de notre architecture dans le cloud.\n\nNous vous invitons à découvrir sa conférence en vidéo.\n\nSi vous voulez en savoir encore plus, Pascal a écrit également un livre...",
  "content"  : "Pascal Martin a eu le plaisir d’être invité par Octo pour un REX sur la migration de notre architecture dans le cloud.\n\nNous vous invitons à découvrir sa conférence en vidéo.\n\nSi vous voulez en savoir encore plus, Pascal a écrit également un livre sur le sujet.\n"
} ,
  
  {
    "title"    : "ScalaIO Lyon 2019",
    "category" : "",
    "tags"     : " scalaio, scala, lyon, 2019",
    "url"      : "/2019/11/25/retour-scalaio.html",
    "date"     : "November 25, 2019",
    "excerpt"  : "Nous étions à la ScalaIO 2019 organisée à Lyon ! \nNous avons assisté à de très bonnes conférences. Voici quelques mots sur les interventions qui nous ont le plus marquées cette année.\n\nA live-coding introduction to Mill: finally a build tool we ca...",
  "content"  : "Nous étions à la ScalaIO 2019 organisée à Lyon ! \nNous avons assisté à de très bonnes conférences. Voici quelques mots sur les interventions qui nous ont le plus marquées cette année.\n\nA live-coding introduction to Mill: finally a build tool we can all understand!\n\nMill est outil de build qui permet de coder en scala les différentes étapes de notre build. La puissance de cet outil vient principalement du fait qu’on écrit du code. On peut donc effectuer des opérations très complexes lors du build. Les fonctionnalités principales sont :\n\n  La possibilité de builder avec différentes versions de scala une même appli\n  La possibilité de ne builder que le code qui a été modifié (grâce à l’utilisation de zinc)\n  La possibilité de lancer le build automatiquement à la modification d’un fichier (option watch)\n\n\nContext Buddy: the tool that knows your code better than you\n\nContext Buddy est un plugin pour votre IDE (Intellij) qui permet de mieux parcourir l’historique des modifications de votre code. ContextBuddy vous permet de savoir grâce à la coloration syntaxique exactement quel élément de la ligne a été modifié. De plus, comme il se base sur les données du compilateur, il est capable de voir si une même classe utilise une nouvelle version de la lib voire même une nouvelle lib.\n\nRailway Oriented Programming - Une approche fonctionnelle pour la gestion d’erreurs\n\nCette conférence basée sur celle de Scott Wlaschin explique très bien la composition de fonction et l’intérêt dans le cas de la gestion des erreurs. En effet, le code devient plus lisible et plus facilement maintenable.\n\nMetals - your next IDE?\n\nMetals est un Language Server Protocole pour Scala, ce qui permet de l’utiliser en tâche de fond pour “n’importe quel” éditeur de texte ou IDE. \nConcrètement, Metals n’apporte pas aujourd’hui 100% des fonctionnalités d’Idea, mais les manques sont vraiment minimes. En revanche, tout ce qui est implémenté semble être plus efficace que sur Idea.\nLes principaux avantages que j’ai retenu :\n\n  Presque tout comme Idea mais beaucoup plus léger et rapide (pour certaines tâches)\n  Fonctionne avec Maven, SBT, Fury, Gradle, Mill\n  Gloop permet le GoToDefinition et plein de choses sympas, plus rapides que l’indexation intelliJ\n  Fonctionne partiellement pour Java (juste le nécessaire)\n  Compilation incrémentale avec Zinc\n  En plein développement, de super features dans les mois à venir\n  Tout ce que j’oublie ;-)\n\n\nRunning Amok: Igniting a Documentation Revolution\n\nL’idée de Jon Pretty (@propensive) est de décorréler la documentation du code (différent repo git) et de pouvoir rétro-documenter (mettre à jour la doc des version antérieures).\nAmok permet de relier chaque fichier de documentation à un commit, à partir duquel cette documentation est valide, permettant ainsi de sortir une release du document même si la documentation n’est pas entièrement terminée (problème récurrent en open-source).\n\nRefined, des Types sur mesure\n\nPermet de mettre des conditions sur un type. On peut ajouter des prédicats au type (notamment des regex sur les Strings, des range de valeurs pour les Int, etc) et ainsi réduire les valeurs possibles. Validation au compile-time quand possible, et pour le runtime des erreurs très explicites sont jetées.\nPour les avantages, voir T(ype)DD, principalement la sécurité apportée et plus besoin de tester ce qui est inclu dans les prédicats, Refined le fait pour nous.\n\nApache Spark et le machine learning : rêves et réalités\n\nA travers des exemples basés sur une population de citrouilles (c’était Halloween ;-)), Nastasia Saby (@saby_nastasia) nous a fait une présentation de Spark ML en prenant en exemple KMeans, un des algorithmes de clustering disponibles. Elle en a montré les limites et présenté KMedoids, n’existant pas dans Spark ML et plus complexe mais convergeant mieux. Elle a terminé sur la nécessité de mettre en balance l’utilisation d’outils communautaires testés et reconnus mais parfois limités, versus développer ses propres librairies parfaitement adaptées à ses besoins, au risque de se confronter à des problèmes que d’autres ont déjà réglés.\n\nLes slides.\n"
} ,
  
  {
    "title"    : "Machine learning sans magie et sans s&#39;arracher les cheveux",
    "category" : "",
    "tags"     : " machine learning, blendwebmix, conference",
    "url"      : "/2019/11/14/machine-learning-sans-magie-et-sans-sarracher-les-cheveux.html",
    "date"     : "November 14, 2019",
    "excerpt"  : "Comprendre le machine learning en prenant l’exemple d’un barbecue.\n",
  "content"  : "Comprendre le machine learning en prenant l’exemple d’un barbecue.\n"
} ,
  
  {
    "title"    : "Forum PHP Paris 2019",
    "category" : "",
    "tags"     : " forumphp, php, afup, 2019",
    "url"      : "/2019/11/04/retour-forum-php-2019.html",
    "date"     : "November 4, 2019",
    "excerpt"  : "Comme tous les ans, nous étions au Forum PHP 2019 organisé par l’AFUP ! \nNous avons assisté à de très bonnes conférences et échangé avec beaucoup d’entre vous. Voici quelques mots sur les interventions qui nous ont le plus marquées cette année.\n\n“...",
  "content"  : "Comme tous les ans, nous étions au Forum PHP 2019 organisé par l’AFUP ! \nNous avons assisté à de très bonnes conférences et échangé avec beaucoup d’entre vous. Voici quelques mots sur les interventions qui nous ont le plus marquées cette année.\n\n“PHP Pragmatic Development” et “L’architecture progressive”\n\nNous sommes plusieurs à avoir trouvé cette édition du Forum très pragmatique. Les deux conférences de Frederic BOUCHERY et de Matthieu NAPOLI y sont sans doute pour quelque chose !\n\nComme Matthieu l’a rappelé, pas besoin d’une architecture parfaite pour créer un produit qui marche. Au contraire, à nous développeurs de savoir choisir les bonnes solutions pour répondre à un besoin. Et faire simple peut apporter infiniment plus de valeur que mettre en place une architecture parfois trop complexe !\n\nFrederic a souligné qu’être pragmatique c’était savoir écouter son expérience. Peut-être même savoir dialoguer entre développeurs seniors et débutants, pour que l’expérience des uns limite les erreurs des autres ?\n\nPHP 8 et Just In Time Compilation\n\nLe passage de PHP 5 à PHP 7 a apporté des gains énormes en terme de performances, et nous sommes tous impatients de voir si PHP 8 nous réservera les mêmes surprises.\n\nLe JIT est une bonne piste, en permettant de compiler le PHP directement en langage machine, pour se passer de l’exécution sur la machine virtuelle de PHP. Benoit JACQUEMONT a très bien détaillé l’histoire du JIT dans l’écosystème PHP, son objectif et son fonctionnement. Même si les tests qu’il a effectué ne montrent pas de gains perceptibles, le sujet était très intéressant.\n\nÀ retenir : l’optimisation du CPU pour PHP n’a pas beaucoup d’intérêt si votre application passe son temps à attendre des I/O.\n\nAggressive PHP quality assurance in 2019\n\nMarco PIVETTA est très actif dans la communauté PHP, notamment pour l’ORM Doctrine. Il nous a présenté les outils qu’il considère comme indispensables pour assurer la qualité et la robustesse d’un projet, mais aussi l’ordre d’importance pour les mettre en place selon lui.\n\nSi nous étions déjà convaincus par l’importance de l’analyse statique, nous avons été intrigués par la place qu’il accordait à tous ces outils basés sur les annotations PHP. Par exemple, il n’hésite pas à laisser publiques les propriétés de ses classes immutables, sans méthode get ni set, et déléguer à la CI la responsabilité de vérifier que toutes les instances des classes avec l’annotation @psalm-immutable ne soient jamais modifiées… Déroutant, mais à méditer.\n\nMercure, et PHP s’enamoure enfin du temps réel\n\nPouvoir pousser, en temps réel, des informations depuis du code PHP server-side vers des centaines de milliers de clients, sans allumer des dizaines de serveurs ? C’est la promesse du projet Mercure, que Kévin DUNGLAS est venu nous présenter !\n\nNous avions entendu parler de ce projet sans jamais encore prendre le temps de le tester ni d’y penser plus en profondeur… Après cette conférence, un POC s’impose ;-)\n\nTout pour se préparer à PHP 7.4\n\nLa prochaine version de PHP, la 7.4, devrait être publiée en fin d’année. Comme tous les ans, elle apportera un petit lot de nouveautés que Damien SEGUY nous a présentées.\n\nNous avons hâte de pouvoir exploiter certaines d’entre elles. En particulier, le pre-loading, qui pourrait améliorer encore notre tenue à la charge lors de nos pics de trafic quotidiens !\n\n“En vrac”\n\nLa dernière conférence du premier jour de ce Forum, par Marie-Cécile GODWIN et Thomas DI LUCCIO, visait à nous ouvrir les yeux : en tant que designer, concepteurs ou développeurs d’applications et d’outils numériques, nous devons penser au futur ; les ressources de notre planète ne sont pas infinies.\n\nCelle du second jour était plus légère : Roland LEHOUCQ nous a parlé de physique, en tirant ses exemples et anecdotes de Star Wars. Qu’est-ce que la Force ? Quelle puissance est capable d’exploiter Palpatine ? Ou combien de gigawatts extrait un sabre-laser ? Une très bonne clôture pour ce Forum !\n\n\n\nNous avons aussi présenté deux conférences, autour de sujets que nous pratiquons au quotidien chez M6 Distribution :\n\n\n  Pascal MARTIN a donné quelques pistes pour améliorer la résilience d’applications, en insistant sur le fait que nos plateformes, de plus en plus complexes, ne sont jamais opérationnelles : elle se trouvent en permanence dans un état de service partiellement dégradé.\n  Benoit VIGUIER a continué dans la lancée de sa conférence de l’année dernière, en présentant cette fois-ci un retour d’expérience après un an d’utilisation de PHP asynchrone en production. Spoiler alert : PHP répond très bien au besoin et les générateurs sont le bien ! À noter aussi son intervention aux traditionnels Lightning Talks, où il nous a présenté une idée un peu folle : faire des interfaces graphiques avec Php.\n\n\nL’AFUP Day 2020 Lyon est déjà en train de s’organiser ! Nous y serons sans doute en nombre et espérons vous y rencontrer à nouveau !\n\n\n"
} ,
  
  {
    "title"    : "Une application résiliente, dans un monde partiellement dégradé",
    "category" : "",
    "tags"     : " conference, architecture, resilience, afup, cloud",
    "url"      : "/2019/10/25/une-application-resiliente-dans-un-monde-partiellement-degrade-pascal-martin.html",
    "date"     : "October 25, 2019",
    "excerpt"  : "Dans un monde en perpétuelle évolution, pouvons-nous toujours atteindre « four-nines » de disponibilité ?\nCloud et Kubernetes. APIs et Microservices… Nos architectures s’enrichissent et se complexifient. Au prix d’une certaine fragilité ?\n\nNous co...",
  "content"  : "Dans un monde en perpétuelle évolution, pouvons-nous toujours atteindre « four-nines » de disponibilité ?\nCloud et Kubernetes. APIs et Microservices… Nos architectures s’enrichissent et se complexifient. Au prix d’une certaine fragilité ?\n\nNous commencerons par définir SLA, SLO et SLI et rappeler la signification de ces X-nines.\nNous montrerons ensuite comment, dans un contexte en permanence partiellement dégradé, nos assemblages de services distribués nuisent à la fiabilité de nos plateformes.\n\nEn profitant de l’expérience acquise sur 6play, nous verrons quelques pistes pour améliorer la résilience de nos applications, pour qu’elles répondent à nouveau aux besoins de notre public. Nous prononcerons peut-être même le terme de « Chaos Engineering » ;-)\n"
} ,
  
  {
    "title"    : "One year of asynchronous PHP in production",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2019/10/25/one-year-of-asynchronous-code-in-production.html",
    "date"     : "October 25, 2019",
    "excerpt"  : "Oui, il est tout à fait possible de faire de la programmation asynchrone en PHP et il existe des librairies matures pour le mettre en place dans vos projets. Oui, ça peut améliorer considérablement la performance de vos applications, mais si c’éta...",
  "content"  : "Oui, il est tout à fait possible de faire de la programmation asynchrone en PHP et il existe des librairies matures pour le mettre en place dans vos projets. Oui, ça peut améliorer considérablement la performance de vos applications, mais si c’était aussi simple tout le monde le ferait déjà. Cela fait plus d’an que les équipes de 6play ont franchit le pas sur certains projets et les applications asynchrones tiennent toutes leurs promesses en production, mais la mise en place a soulevé beaucoup de questions. À quels critères se fier pour rendre une application asynchrone? Comment former les équipes sur ces nouveaux paradigmes? Comment adapter les outils existants et comment gérer ce nouveau type de charge sur les serveurs? Voici notre retour d’expérience sur le PHP asynchrone, du développement à la production, en passant par la vie de tous les jours.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #11",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2019/09/02/m6web-dev-facts-11.html",
    "date"     : "September 2, 2019",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nLa persuasion\n\n  L’important, c’est d’être convaincu qu’t’es convaincu\n\n\nParfois débug c’est compliquer\n\n  Comment je teste l’alerte kidnapping en local ?\n\n\n\n  Bah tu vas dans un parc et tu enlèves un en...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nLa persuasion\n\n  L’important, c’est d’être convaincu qu’t’es convaincu\n\n\nParfois débug c’est compliquer\n\n  Comment je teste l’alerte kidnapping en local ?\n\n\n\n  Bah tu vas dans un parc et tu enlèves un enfant, d’ici deux trois heures tu devrais voir l’alerte.\n\n\nÊtre à sec\n\n  Vous pourriez tirer vers nous! On a plus de flèches de nerf ?\n\n\nSeal of approval (lu sur slack)\n\n  Moi, Mr XXXX, Lead Dev de mon état, sain de corps et d’esprit, atteste de la validation et du bien fondé de cette requête, même si c’est un vendredi\n\n\nPrécision\n\n  Si il faut que ce soit précis, il faut préciser\n\n\nFrench please\n\n  _ Est ce qu’on peut parler en français maintenant ici ?\n_ Yes.\n\n\nLa difficulté\n\n  Il y a 2 choses de compliquées en développement : nommer les choses et invalider du cache.\nFigure toi que je suis en train de nommer une PR qui invalide du cache\n\n\nLa qualité avant tout\n\nif (value instanceof Collection) {\n    return ((Collection) value).isEmpty() ? false : true;\n} else if (value instanceof String) {\n    return StringUtils.isNotBlank((String) value) ? true : false;\n}\n\n\nBoire ou coder\n\n  _ J’ai fait un bateau à la ganane pour la rétro !\n_ Il y a du rhum dans la recette :rolling_on_the_floor_laughing: ?\n_ Même pas !!\n\n\nLe CDD\n\n  Conférence de presse Driven Development\n\n\nMais oui c’est clair\nswitch($categoryId) {\n  case &#39;16&#39;:\n    return 38;\n  case &#39;18&#39;:\n    return 40;\n  case &#39;26&#39;:\n    return 42;\n  case &#39;28&#39;:\n    return 36;\n}\n\n\nLe nommage c’est important\n\n$catId = &#39;turlututu&#39;;\n$programId = &#39;chapeaupointu&#39;\n\n\nLa magie nuagique\n\n\n  Le cloud est un état d’esprit, pas un endroit où on déploie\n\n\nLes index commencent à 0\n\n\n  La reproductivité est l’étape N°1 du debuggage\n\n\nOn fait quoi demain ?\n\n\n  Rappelle moi d’acheter un agenda stp !\n\n\nRIP\n\n// Remplissage d&#39;un tableau de données pour un template de job (pas Steve, il est mort)\n\n\nMme Irma\n\n  Montre moi ton diff, je te dirai qui tu es!\n\n\nLes projets à succès\n\n  Il parait qu’un projet legacy c’est un projet qui a réussi… bin on a certains projets qui ont vachement bien réussi !\n\n\nL’appétit des croissants\n\n  Pourriez-vous péter la prod plus souvent ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #10",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2019/08/08/m6web-dev-facts-10.html",
    "date"     : "August 8, 2019",
    "excerpt"  : "Ça faisait un très très long moment ! Voici le retour des devfacts !\n\nL’erreur est humaine\n\n  Il faut mieux recevoir une erreur 500, que 500 erreurs\n\n\nL’odeur du code\n\n  Le code de merde, tu sais quand c’est le tien, c’est comme les pets, pas la p...",
  "content"  : "Ça faisait un très très long moment ! Voici le retour des devfacts !\n\nL’erreur est humaine\n\n  Il faut mieux recevoir une erreur 500, que 500 erreurs\n\n\nL’odeur du code\n\n  Le code de merde, tu sais quand c’est le tien, c’est comme les pets, pas la peine de git blame.\n\n\nRestons zen !\n\n  _ Ah mais en fait !!! Je comprends pourquoi tu viens tôt le matin! Tu viens parce que c’est calme !!!!\n\n\n\n  _ Tais toi Mehdi !!\n\n\nPrévoir l’imprévisible\n\n  C’est la première étape de l’étape suivante.\n\n\nCe matin au chiffrage\n\n  J’ai pensé 5 mais j’ai mis 3…\n\n\nSchroedinger app\n\n  _ Ça marche ?\n\n\n\n  _ Je sais pas mais c’est en prod !\n\n\nL’effet de serre\n\n  _ Il fait froid dans le bureau\n\n\n\n  _ Bah démarre 2-3 docker sur ton Mac, ça devrait résoudre le problème\n\n\nLes priorités\n\n  _ Alleeeez, s’il te plaiiiiiiiit !\n\n\n\n  _ Les incidents d’abord, les détails graphiques après !\n\n\nPetite nouveauté, voici quelques mèmes maison !\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "React Europe 2019",
    "category" : "",
    "tags"     : " React, JS, 6play, Conference, 2019",
    "url"      : "/2019/05/27/react-europe-2019.html",
    "date"     : "May 27, 2019",
    "excerpt"  : "The M6 Distribution’s (M6 Web’s new name!) front team hasn’t posted for a long time. We took part as listeners of the 5th European React Conference in Paris on May 24th and 25th. It’s an opportunity to talk about what are the last moves in the Rea...",
  "content"  : "The M6 Distribution’s (M6 Web’s new name!) front team hasn’t posted for a long time. We took part as listeners of the 5th European React Conference in Paris on May 24th and 25th. It’s an opportunity to talk about what are the last moves in the React community and at M6.\n\nAs usual, we were waiting for lot of announcements in this conference and a lot of new tools or new libraries. There has been no big declaration, no surprise. But many subjects were interesting and several talks confirmed the way we have taken over the last few years.\n\nHooks, more hooks and suspense\n\nDuring the keynote, Jared Palmer put the emphasis on hooks through an example to simplify the use of GraphQL queries. At M6 Distribution, we use hooks since the beginning of the year and many are in production. That has changed the way we write components. We already have used functionnal components before, but using hooks simplify the readability and the evolutivity of old class component. The bad point is testing… Because we use Enzyme, testing hooks is painful for now. Until Enzyme fully supports hooks, we have implemented custom mocks.\n\nJared Palmer also showed us the interest of React Suspense to manage a main loading state in an app instead of many spinners that don’t offer a good user experience. We can’t use Suspense for our app because of the SSR. As it is recommended by the React team, we use Loadable Components instead. Jared  announced that a new asynchronous server renderer is in progress and could be released in 2019 supporting Suspense. Suspense will also include data fetching.\n\nOthers features or refactoring will come in the future, but there is very limited information:\n\n  React Fire\n  React Fusion (?)\n  React Native Fabric\n  React Flare\n\n\nDo the harlem shake\n\nSeveral conferences have addressed the theme of animations with React. This subject is very often complicated to solve. Finding a high-performance solution that is compatible with as many browsers as possible can be a real challenge.\n\nFor more than a year now, we have been migrating from Sass to styled-components. Despite some resentment, it should be noted that this new tool makes it very easy to create/compose our new visual components. It also helps us a lot with its management of the theme.\n\nBetween the react-spring library and the tips of Joshua Comeau, we now have many ways to integrate the new animations that designers can invent.\n\nTo summarize their comments, there is no single best way to make animations in all situations. Whether with canvas, SVG, web API, CSS, 2D sprite, you should try to do it in several ways to compare performance and rendering in order to choose the best option for your specific case.\n\nMake tests but make good tests\n\nLisa Gagarina gave us some tips to better testing the code. Even If we have been using ESLint and Prettier for a long time in our JS projects, we have chosen not to implement static typing for the moment. Indeed, we think that it is a huge step, it has a big impact on the code and it can make the onboarding of new developpers more complex. For now, we just use the React proptypes well.\n\nLisa also advices to better use Jest snapshots. They are often too many, too big, not clear, hard to review. And we have experienced it too. A best pratice can be to reduce snapshots size and inline it in the test file. The readability of a test is indeed very important and it is not recommended to refer to other files than the test file (this is valid for snapshots but also for fixtures). There are some ESLint rules to ensure this: jest/no-large-snapshots, jest/prefer-inline-snapshots. snapshot-diff can also helps by making a diff between the nominal case and the tested case of a component rendering.\n\nA difficult thing is to keep test agnostic to implementation details. We should consider the code as a black-box and only test the user interaction of the components, otherwise any refactoring of code will be painful and might discourage developers from writing tests.\n\nE2E tests are also complex to setup, write and debug but are absolutely necessary. For the back office app, unlike our front app where we use a custom stack, we choose Cypress a complete E2E framework that has saved us a lot of time.\n\nLisa concluded her very interesting lightning talk by saying that there is no such thing as a one-size-fits-all approach, the way of testing has to be adapted to the team and the project. For example on our 6play project, we have more than 3000 unit tests performed by Jest in less than 4 minutes and 450 E2E scenarios that save your life every day!\n\nIf you are interested in this subject, take a look at this article about JS testing best practices.\n\nFinally a little GraphQL in our projects\n\nOne of the main themes of the conference was also GraphQL. \nThis data exchange paradigm now seems to have its place among many users.\nFor more than a year now, we have been using GraphQL on our back office and we are already seeing a lot of benefits:\n\n\n  a front/back exchange contract materialized in a schema,\n  easy consumption thanks to queries,\n  cache management provided by Apollo (also providing a collection of great tools for GraphQL).\n\n\nKenny already talked about it in 2015 but now we use it!\n\nDevelopment worflow: the expected journey\n\nOne of the most interesting conferences in my opinion was about the need to optimize the development workflow. This is an element that is too often ignored but is very important in the life of a project. Paul Amstrong presents us here an analysis of the workflow of his team in charge of the development of the Twitter Lite application. He also presented some conclusions and solutions he implemented. In a standard development workflow there are 3 points that allow a significant margin of progress:\n\n  increased developer confidence and delivery speed,\n  automate the PR process to maximum,\n  detect errors as early as possible.\n\n\nThese words echo our own questions on the subject and these conclusions confirm our decisions. \nIt is important to have the shortest and most automated workflow possible between the developer and the user without sacrificing the developer’s experience because it goes hand in hand with all the other aspects of a project.\n\nAt M6 Distribution, we use most of the tools presented, but two in particular caught our attention.\n\nThe first, React Component Benchmark, is of particular interest to us because in the past we had started to investigate the subject and used tools that are depreciated today.\n\nThe second, Build Tracker, which allows us to test the evolution of bundle size, will allow us to replace an equivalent tool developed internally while providing a more detailed analysis in order to work more accurately on these issues.\n\nHere is an example of a message posted by our in-house build tracker on a Pull Request.\n\n\n\n\n\nA11y is our new challenge\n\nSeveral very inspiring talks on accessibility seem to show that this issue finally appears to be considered by web actors. In particular Facebook, which has distinguished itself by showing its assistant to detect accessibility errors in the development of its new version. However, it is regrettable that this toolkit is not accessible to the community because it could be a great help to avoid putting people from being in a situation of handicap.\n\nIt is clear that our front web app is not yet very accessible but we are working to correct this error, especially on the new screens we have been integrating for several months.\n\nOur expectations on Yarn finally fulfilled\n\nWe were waiting for it at M6, the new yarn release named berry offers almost everything we were missing in our favorite package manager.\nIndeed, by using yarn in monorepo mode for our front projects, we were confronted with several problems that had to be overcome with in-house tools. Example here with our monorepo-dependencies-check tool which is now becoming obsolete thanks to Constraints.\n\nWe are also delighted with the functionality of Zero install. But what we expected the most was about the workspaces. We will finally have a management of the publication of these.\n\nTake a look at this repository for more information.\n\nSome projects that interest us a lot: Next.js &amp;amp; Code Sandbox\n\nEven if these two tools are not used for the development of our applications, the new features of Next.js and Code Sandbox are clearly very interesting.\n\nAs for Next.js, our front web project has its own configuration of server side rendering (Florent explains it in ‘Last night isomorphic JS saved our life!’). However, NextJS is a great project that we use for many of our side projects. AMP support, client-only pages and API endpoints are clearly welcome.\n\nAs for Code Sandbox, Ives van Hoorne told us his personal story and that of his project. In addition to the great tool that is CodeSandbox, we have seen that the use of WebAssembly seems to have solved a lot of performance and implementation problems. For example, he cites the coloring of the code based on TextMate only available in C could not have been ported to the browser without going through WebAssembly.\n"
} ,
  
  {
    "title"    : "AFUP Day Lyon 2019",
    "category" : "",
    "tags"     : " php, afup, 2019",
    "url"      : "/2019/05/23/afup-day.html",
    "date"     : "May 23, 2019",
    "excerpt"  : "TechM6WEB était très fier de sponsoriser le premier AFUP DAY à Lyon.\n\nPour une première, c’était très réussie. Un programme au top et varié, qui mettait en avant de nombreuses problématiques techs et sociétales.\n\n\n(photo : Benjamin Lévêque)\n\nNos c...",
  "content"  : "TechM6WEB était très fier de sponsoriser le premier AFUP DAY à Lyon.\n\nPour une première, c’était très réussie. Un programme au top et varié, qui mettait en avant de nombreuses problématiques techs et sociétales.\n\n\n(photo : Benjamin Lévêque)\n\nNos conférences préférées :\n\n  OVH.com from 1999 to 2019, car c’est toujours intéressant les REX de projets importants en toute humilité,\n  L’architecture progressive, bien qu’elle ait soulevée pas mal de trolls en interne,\n  Les merveilles méconnues du SQL, car en vrai “you know no SQL” :)\n\n\nEt bien sur la table ronde des CTO pendant laquelle Olivier Mansour est intervenu.\n\nMerci l’AFUP pour cet évènement près de chez nous !\n"
} ,
  
  {
    "title"    : "Migrating production applications from on-premise to the cloud with no downtime",
    "category" : "",
    "tags"     : " Cloud, AWS, Kubernetes, Kops, HAProxy, GOReplay",
    "url"      : "/2019/03/11/Migrating-production-apps-from-on-premise-to-the-cloud-with-no-downtime.html",
    "date"     : "March 11, 2019",
    "excerpt"  : "We are migrating all our on-premise applications to AWS cloud.\nMost of them are/will be migrated to Kops-managed Kubernetes clusters, and some are/will be migrated as lambdas.\nTo secure this migration, we are using HAProxy in front of both on-prem...",
  "content"  : "We are migrating all our on-premise applications to AWS cloud.\nMost of them are/will be migrated to Kops-managed Kubernetes clusters, and some are/will be migrated as lambdas.\nTo secure this migration, we are using HAProxy in front of both on-prem and on-AWS deployments (first only sending 1% of each application’s requests to AWS, then 5%, then 25% and so on).\nDisclaimer: This article describes a feedback from production environment. We have changed the name of applications mentioned here, but everything else is true within the limits of our knowledge.\n\nYou can find the content of this blogpost (and more) in a 25mn talk at the HAProxy conf given in Amsterdam in 2019 :\n\n\nThe first migrated application\n\nIt’s an API written in PHP. It has no external dependency (database, redis…), except for another API, called over HTTP.\nWe have migrated this application to the cloud like we’ve done with other applications since.\n\nThe first step was to deploy this application to a kubernetes cluster and expose it over an ELB.\nThen, we wanted to send real-user requests to that app. We wanted to see how it would behave with real production traffic.\nBut we didn’t want to send 100% of our users over there at once: we’d first rather check everything works fine with just 1% of our users.\n\nHAProxy\n\nWe’ve been using HAProxy for several years now.\nBecause of its features, like advanced backend monitoring or its enormous number of metrics, it’s the perfect tool to help us on this migration.\n\nAt first, we didn’t know how the app, the Horizontal Pod Autoscaler, Liveness probes etc. would react with real live production requests.\nSo, we decided to migrate only 1% of production HTTP requests to our Kubernetes cluster. The other 99% of HTTP requests would remain on premise, where the application works for sure.\n\nHere’s a part of the associated HAProxy configuration:\n\nbackend application-01\n    http-response add-header X-Backend-Server %s\n    balance roundrobin\n    http-request set-header Host application-01.6play.fr\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-01.6play.fr\n    server aws-prod-Kubernetes-application-01 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-01.6play.fr ca-file ca-certificates.crt inter 1s fall 1 rise 2 resolvers m6dns observe layer7 weight 25\n    server onprem-prod-front-application-01 onprem-application-01.6play.fr:80 check resolvers m6dns weight 75\n\nSome explanations on key elements of this configuration:\n\n\n  http-response add-header: adds an HTTP header to the response, with the server chosen by HAProxy to handle the request. We added this for debugging purposes: to know who handled our request when requesting the service\n  balance roundrobin: because we’re using a stateless application\n  check .. inter 1s fall 1 rise 2 in the server directive: healthchecks have 1 second of timeout, only 1 bad healthcheck is enough to mark the server as unhealthy and we need 2 good healthchecks in a row to mark it as healthy again\n  observe layer7: It will simulate a bad healthcheck for each application server error (e.g: 500, 502, 503, etc.) making this server fall in such a situation\n  weight 25: We use weights from 0 to 100 (you can go up to 256), so that corresponds to traffic percentages in our case\n\n\nWith the above configuration, as soon as there is an error, HAProxy won’t send traffic to our AWS/Kubernetes application anymore; consequently, it will have a minimum impact on endusers.\n\nTests\n\nWe first tested this with staging proxies, with a temporary domain name and a 50-50% loadbalancer, to ensure the load-balancing worked fine.\nWe tried to kill the deployment on Kubernetes to check 100% of requests came back on-prem. We tested killing random pods to see if we had some user impact. We tested to slow down the application so it would be slower than 1s to respond. We also tested to slow down only one of two pods running the application.\nIt was all OK, so we were confident to go to production.\n\nMigration steps\n\nBefore its migration, the application infrastructure looked like this:\n\n\nWe inserted HAProxy servers in that schema, so the traffic passes through them before being sent to the caches.\nThat way, HAProxy controls where traffic is sent.\nTo make those migrations as transparent as possible, we first configured HAProxy to send traffic to on-prem servers only.\nIn the same time, developers have deployed all mandatory resources (RDS, DynamoDB, elasticache, etc.) with Terraform and verified the application works fine. The application itself could have changed: either the code or kubernetes manifests.\nWhen ready, both ops and devs gave their approval to send traffic to the cloud.\n\nWe started by load-balancing 1% to the AWS ELB with HAProxy:\n\n\nWe compared everything we could:\n\n  2xx, 3xx, 4xx and 5xx percentages\n  connect and response times\n  failed healthchecks and healthchecks return codes\n  backend retries and bad responses\n\n\nWe were amazed: only 3ms in average difference between on-prem and our Kubernetes cluster in AWS cloud.\nAnd no error. Everything worked as expected. It was almost suspicious.\n\n\nThis graph shows the average connect times from HAProxy.\n\nWe’re using Paris as AWS zone and our datacenters are located in Paris too, so that explains the few milliseconds to go back and forth from HAProxy (on prem) to the cloud. In fact, this one to two millisecond between our on-prem servers and AWS is one of the reasons adding HAProxy in the mix was possible.\n\n\nThis graph shows the average response times from the application.\n\nWe also had some PHP configurations to update to be ISO prod (OPCache, APCu, etc.). Why? Well, at first, we created a quick’n dirty (working, but not optimized) Docker image for our application and it went straight to production, before our sysadmins could take a better look at it.\n\n\nThis graph shows the number of 2xx HTTP codes with 25% of traffic sent to AWS.\n\nOnce those PHP optimisations were fixed, we had only 2ms difference between our on-premise and our Kubernetes on AWS. It’s low enough to allow us to test this setup a bit longer without any visible user impact.\n\nDeploying more and more\n\n1% on our kubernetes cluster was great, but it was not enough to see perfs issues.\nSo we raised HAProxy’s load balancing from 1%/99% to 10%/90%.\nStill not enough. We raised it to 25%/75%.\nWe checked application’s pods CPU usage, it was really low. Too low to even trigger the Horizontal Pod Autoscaler based on cpu usage. We couldn’t validate our pods Requests and Limits for that Kubernetes deployment therefore.\n\nSo far, it was enough for us to validate things we could check were working fine. The application cache was efficient, we had stable performances and no surprise on traffic peaks.\n\nOn this first application migration, we decided to stay at 25% of traffic sent to the cloud for the moment, to observe.\nWe did it because HAProxy would have saved us if something bad happened.\n\nIt did behave well: nothing happened for 26 days.\n\nSome errors encountered\n\nUnknown nodes\n\nOn the 27th day, we noticed two of our three nodes were in Unknown state. As our Replicas specified we wanted several pods, Kubernetes started new pods. But as we only had one Ready node left (3 worker nodes in the cluster, including two in Unknown state), all pods for our application were now on that single node. Not great for reliability.\nWe found that the CoreOS image we used was doing automatic updates that restarted nodes regularly. On those two nodes (and on the third one a few days later!), the restart did not go well and Kubelet wasn’t starting at bootime. After investigation, it appears we changed the KOPS STATE STORE bucket. Nodes started before that change were impacted as Kubelet couldn’t find its configuration. Starting new nodes and killing the old ones solved this issue.\nThat allowed us to identify two problems: we didn’t restrict when and how automatic updates were started; and we didn’t have enough cluster monitoring.\n\nFrom that, we knew we had to monitor:\n\n  Nodes in a state different from Ready for a certain time\n  If the number of Ready nodes is at least equal to the Auto Scaling Group minimum\n  If the number of Ready nodes is at most 90% the Auto Scaling Group maximum\n\n\nDifferences between cluster-autoscaler and ASG values\n\nWhile trying to solve the problem above, we also found that pods were living around 5mns before being destroyed and created again. After some investigation, we found that min and max EC2 instances were not configured to the same values between the AWS Auto Scaling Group and the cluster-autoscaler pod.\nSome day, we modified the Kops configuration for those worker nodes and the configuration was well applied to the ASG, but was not applied to the cluster-autoscaler. As a result, we had an ASG minimum of 4 worker nodes, but a cluster-autoscaler minimum of 3.\nAt some point in the history, the cluster-autoscaler defined that the current amount of worker nodes (four) was too high compared to the real need, so it tried to reallocate pods to free up a node. It have done that by draining pods from a node. At the same time, the cluster-autoscaler tried to change the ASG’s desired value to 3 nodes. Because the minimum nodes configured for the ASG was 4, the latter denied the request. Kubernetes scheduler chose to reschedule those recently-killed pods on available nodes, starting by the one with no running pods, AKA: the one that just drained them all.\nThis last phase started again every 5 minutes, making sure that our pods did not survive longer than that.\n\nApplication latencies undetected by health checks\n\nOur application was really slow on Kubernetes/AWS (due to a network misconfiguration) but HAProxy did not disable it. We specified a 1s timeout as shown in the example above, but this is only for healthchecks. Our global server timeout is upper than 1s. Because our application calls another webservice, those calls were timeouting. HAProxy was not aware of that, because the application’s /HealthCheck health page doesn’t check external webservices and thus, were not impacted by those external webservices timeouts. This is an application choice that we can encounter on-premise too, with the exact same behavior. For that reason, we decided to change nothing for now (and we’ll discuss this with the devs teams to see if there’s something we can do).\nWe don’t check external webservices in our /HealthCheck page on purpose, because that page is also tested by kubernetes for livenessProbe. Kubernetes restarts a pod when it is not healthy anymore but when it comes to an external service that is failing, restarting the current pod is non-sens. Kubernetes will restart pods again and again even if the application itself can’t to anything about it! The livenessProbe should test only what the pod does. The Amadeus team talked about that at the KubeCon EU 2018 while presenting Kubervisor.\n\nPedal to the metal\n\nWe were stabilized again.\nSo we raised HAProxy load-balancing to 50% on our application in the cloud.\nAfter seven days without any error, we pushed it to 75%.\nAfter another seven days, we passed the on-prem server as a backup in HAProxy, making the application in kubernetes receiving 100% traffic.\n\n\n\nWe stayed with that configuration for 2 months.\nThat gave us plenty of time to adapt pods Requests and Limits.\nThat is really important for us, because we use HorizontalPodAutoscaler resources with CPU metrics to scale most of our APIs. Here you can find slides deep diving one of our applications that autoscales in prod with kubernetes.\nWe had several events during those 2 months that helped us optimize Requests and Limits for that app. For example, we had holiday traffic, a football match and some special primetime sessions.\nWe also improved our knowledge of both Kubernetes and AWS during this time (I.e: What happens when we rolling restart worker nodes?). Finally, we have configured our Prometheus servers with effective and non-noisy alerts.\n\nAfter weeks of optimizations, we migrated this app’s DNS directly on ELB without HAProxy.\nEverything works perfectly as expected since that day.\n\nNext applications to migrate\n\nWe’ve done a lot of work for our first migration. We’ve capitalized that time for the next projects to finally be able to migrate them in few days.\nThe workflow stays unchanged:\n\n\n  Deploy the application into a kubernetes cluster\n  Add HAProxy servers in front of both on-prem and in-cloud instances\n  Load balance from 1% to 100% traffic to the in-cloud instance\n  Configure accurate Requests and Limits\n  Create efficient alerting\n  Point DNS to ELB\n\n\nMigrate an application path by path\n\nSome of our applications needed to be partially rewritten to be cloud native.\nOnly specific paths were affected by this rewrite.\n\nSo we decided to use HAProxy to migrate those applications, path by path.\nWe also used GOReplay to replicate production traffic for each path, to be sure we didn’t messed up things before sending end-users traffic.\n\n\nThis schema shows how HAProxy were routing traffic according to specific paths.\n\nThe workflow is almost the same as above, with few changes:\n\n\n  Deploy the application into a kubernetes cluster\n  Add HAProxy servers in front of both on-prem and in-cloud instances\n  Use HAProxy map_reg to route traffic, depending of the requested URL\n  Define path routing preferences in the map file created in step 3 (see example below)\n  Configure and test each path:\n    \n      Let developers rewrite paths, I.E: /HealthCheck\n      Replicate production traffic with GOReplay, to specific paths, including /HealthCheck, from on-prem to the application in the Kubernetes cluster in AWS\n      Stabilize the application: either code optimisations or Kubernetes Requests and Limits adaptations\n      Add this newly created path /HealthCheck on the HAProxy’s routing map file\n      Repeat for each new path\n    \n  \n  Create a specific HAProxy Backend section for each route to load balance traffic differently for each route\n  Increase traffic load balancing up to 100% to the cloud\n  Create efficient alerting\n  Point the DNS to the ELB\n\n\nTraffic replication with GOReplay\n\nWe use a lot GOReplay.\nNot only because it’s light and easy to work with, but because we can do whatever we want with it to replicate traffic. It can rewrite headers, catch only a specific domain or a specific url. It’s the perfect tool to complete our migration workflow.\n\nHere is a script we used in the step 5.b of the workflow above:\n\n#!/bin/bash\n\nreplicate_traffic() {\n    if [[ -z $1 ]]\n    then\n        local REPLICATION_PERCENTAGE=5%\n    else\n        local REPLICATION_PERCENTAGE=$1\n    fi\n\n    if [[ -z $2 ]]\n    then\n        local TIMEOUT=45s\n    else\n        local TIMEOUT=$2\n    fi\n\n    echo &quot;Replicating traffic at ${REPLICATION_PERCENTAGE} for $TIMEOUT&quot;\n\n    ./gor -exit-after $TIMEOUT \\\n    -input-raw :8080 \\\n    -http-disallow-url /v2/critical/sensible_datas/payments/ \\\n    -http-allow-url /v2 \\\n    -input-raw-bpf-filter &quot;dst host 127.0.0.72&quot; \\\n    -output-http &quot;https://application-02.6play.fr/|${REPLICATION_PERCENTAGE}&quot; \\\n    -http-original-host \\\n    2&amp;gt;/dev/null\n}\n\n# The above allows a normal ramp-up of the traffic.\n# That means application replicas can be low and increase naturally without an insane peak\nreplicate_traffic 1% 45s\nreplicate_traffic 2% 45s\nreplicate_traffic 5% 45s\nreplicate_traffic 10% 45s\nreplicate_traffic 20% 60s\nreplicate_traffic 40% 60s\nreplicate_traffic 60% 60s\nreplicate_traffic 80% 60s\nreplicate_traffic 100% 7h\n\nWe’re using this script and not directly the gor command, to do a slow ramp-up of traffic to the application in Kubernetes.\nOtherwise, since the application is not stressed before traffic is replicated, replicating 100% of traffic all of a sudden would not be representative of real user behavior. It would led to unwanted alerts that would disappear in minutes with auto-scaling, but that would have rang anyway. So we chose to avoid that noise by doing a slow ramp-up to make traffic replication more real.\n\nWe could follow the replication with HAProxy dashboard, like the following graph:\n\n\nHAProxy configuration\n\nTo achieve a path-by-path migration of an application, we used this HAProxy configuration:\n\nfrontend application-02\n    ...\n    # Defined with a &quot;map&quot; style, from file /etc/haproxy/domain2backend.map\n    # CF https://blog.haproxy.com/2015/01/26/web-application-name-to-backend-mapping-in-haproxy/\n    use_backend %[base,map_reg(/etc/haproxy/domain2backend.map,bk_default)]\n\nbackend application-02-on-prem\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns\n\nbackend application-02-on-cloud\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns\n\nbackend application-02-mixed\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns weight 75\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns inter 3s fall 1 rise 2 observe layer7 weight 25\n\nbackend application-02-mixed-critical\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns weight 99\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns inter 3s fall 1 rise 2 observe layer7 weight 1\n\nAnd here’s the associated map file:\n\n#HOST Header                                                            #HAP backend_name\napplication-02\\.6play\\.fr\\/v2\\/critical\\/(\\w{1,45})\\/payments\\/         application-02-mixed-critical\napplication-02\\.6play\\.fr\\/v2\\/customers\\/                              application-02-mixed\napplication-02\\.6play\\.fr\\/v2\\/                                         application-02-on-cloud\napplication-02\\.6play\\.fr\\/v1\\/                                         application-02-on-prem\n\n# Catch ALL for application-02.6play.fr\napplication-02\\.6play\\.fr\\/                                             application-02-on-prem\n\n\nSome examples of traffic routing made by HAProxy with the configuration above:\n\n\n  application-02.6play.fr/v2/critical/sensible_datas/payments/\n    \n      sent on the specific application-02-mixed-critical backend,\n      with 1% traffic sent to the cloud\n    \n  \n  application-02.6play.fr/v2/customers/\n    \n      sent on application-02-mixed,\n      load balanced at 25% on the cloud\n    \n  \n  application-02.6play.fr/v2/\n    \n      sent on application-02-on-cloud,\n      only on the AWS ELB: cloud only\n    \n  \n  application-02.6play.fr/v1/\n    \n      sent on application-02-on-prem,\n      only on-premise\n    \n  \n  application-02.6play.fr/admin/\n    \n      sent on-premise only,\n      this is the default\n    \n  \n\n\nWith HAProxy map files and the according backend sections, we’re able to migrate path by path any application from on-premise to our kubernetes cluster in the cloud.\nWith gor on top of it, it’s even easier to allow developers develop a specific path while another is being migrated, and all that, with no downtime.\n\nNext steps\n\nWe’ve done most of our cloud migration with workflows explained in this blogpost.\nThanks to HAProxy, most of our applications could be migrated at the same time with no impact from on migration to another.\n\nThere are still some applications to migrate though and one of them is a tough one. This application is heavily using Cassandra database. There is no Cassandra managed in AWS, so we are completely rewriting the application to adapt it to DynamoDB and also to face upcoming business needs.\nThe challenge is to keep existing pathUrl of the application, working. In other words: the new version have to give same functionalities, keeping the same URLs, but with totally different data management under the hood.\nGOReplay is a wonderful asset to help us in this task.\n\nIf you found this useful and you’d like more production return of experiences like this one, please let us know. We plan to write more in the coming weeks.\n"
} ,
  
  {
    "title"    : "7 conseils pour démarrer avec Spark",
    "category" : "",
    "tags"     : " spark, scalaio, conference",
    "url"      : "/2019/01/14/7-conseils-pour-demarrer-avec-spark.html",
    "date"     : "January 14, 2019",
    "excerpt"  : "Je suis entrée dans le monde de la data avec Spark. \nIl y a eu des moments clairement plus ou moins compliqués. \nAu début, c’était le Far West : un monde inconnu et dangereux (il ne fallait pas casser la production). \nAvec ce retour d’expérience, ...",
  "content"  : "Je suis entrée dans le monde de la data avec Spark. \nIl y a eu des moments clairement plus ou moins compliqués. \nAu début, c’était le Far West : un monde inconnu et dangereux (il ne fallait pas casser la production). \nAvec ce retour d’expérience, je vous propose de vous dire ce que j’aurais aimé qu’on me dise avant de me lancer. \nJe promets aussi de vous parler de ce que bien heureusement mes camarades plus expérimentés m’ont aussi donné comme astuces pour m’aider dans ce grand saut. \nCe sera donc une entrée en matière dans le monde de Spark au travers de 7 conseils qui m’ont été ou m’auraient été bien pratiques pour démarrer.\n"
} ,
  
  {
    "title"    : "Le Plan Copenhague : notre migration vers Le Cloud, retour d’expérience",
    "category" : "",
    "tags"     : " cloud, kubernetes, aws, terraform, livre",
    "url"      : "/2018/12/20/le-plan-copenhague.html",
    "date"     : "December 20, 2018",
    "excerpt"  : "Nous avons commencé à migrer notre plateforme 6play vers Le Cloud il y a un an.\n\nDepuis, nous avons découvert Kubernetes et Helm, AWS et ses services managés, Terraform, Prometheus et une multitude d’autres outils. Nous avons fait des choix, répon...",
  "content"  : "Nous avons commencé à migrer notre plateforme 6play vers Le Cloud il y a un an.\n\nDepuis, nous avons découvert Kubernetes et Helm, AWS et ses services managés, Terraform, Prometheus et une multitude d’autres outils. Nous avons fait des choix, répondu à de nombreuses questions, rencontré et franchi des obstacles. Nous avons mis en place des bases solides ou, parfois, pris des raccourcis pour avancer plus vite.\n\nVous aimeriez découvrir comment nous migrons une plateforme comme 6play vers Le Cloud, comment nous exploitons Kubernetes ou les services managés d’AWS ? Vous vous demandez comment nous optimiserons les coûts ? Vous migrez peut-être vous aussi votre hébergement, et comparer votre expérience à la nôtre vous aiderait à avancer ?\n\nPascal, DevOps qui accompagne cette migration vers Le Cloud, a commencé à rédiger un retour d’expérience autour de ce projet. Les sept premiers chapitres viennent d’être publiés : vous pouvez dès maintenant commencer à lire « Le Plan Copenhague » !\n\nPendant ces cent premières pages, vous découvrirez notre projet, les premières bases que nous avons construites et la mise en place de notre environnement chez AWS et sous Kubernetes. Nous irons jusqu’à présenter comment nous avons migré notre première application vers Le Cloud en toute sécurité. La rédaction des chapitres suivants va s’étaler sur une bonne partie de 2019. N’hésitez pas à vous inscrire pour être prévenu lors de leur publication ;-)\n\nBonne lecture !\n"
} ,
  
  {
    "title"    : "Forum PHP Paris 2018",
    "category" : "",
    "tags"     : " forumphp, php, afup, 2018",
    "url"      : "/2018/11/12/retour-forum-php-2018.html",
    "date"     : "November 12, 2018",
    "excerpt"  : "Comme tous les ans, nous étions au Forum PHP 2018 organisé par l’AFUP ! Encore une fois, nous avons pu assister à plusieurs conférences et échanger avec grand nombre d’entre vous. Voici quelques mots sur celles qui nous ont le plus marqué.\n\n“Boost...",
  "content"  : "Comme tous les ans, nous étions au Forum PHP 2018 organisé par l’AFUP ! Encore une fois, nous avons pu assister à plusieurs conférences et échanger avec grand nombre d’entre vous. Voici quelques mots sur celles qui nous ont le plus marqué.\n\n“Boostez vos applications avec HTTP/2”\n\nNous connaissons et utilisons tous HTTP au quotidien. Mais que se cache-t-il derrière ce protocole ?\nKévin Dunglas nous à présenté les nouvelles fonctionnalités apportées par HTTP/2.\nIl nous a d’abord fait un rappel sur les concepts HTTP et sur l’évolution de ce protocole conçu pour échanger des données documentaires.\nAprès plus de 20 ans en version 1, HTTP évolue et passe en version 2 !\n\nHTTP/2 était initialement propulsé par Google. Première bonne nouvelle, pas de changement nécessaire du côté de nos applications PHP. La version 2 introduit de nouvelles fonctionnalités intéressantes :\n\n\n  Priorisation des requêtes\n  Passage au binaire (optimisation de la taille des messages)\n  Notifications push\n  Et bien d’autre\n\n\nKévin nous a aussi présenté le protocole Mercure. Il permet de faire des notifications push server side vers différents clients. Mercure semble simplifier grandement les échanges push client / serveur. Toutes les parties dialoguent au travers d’un hub (rôle primaire de Mercure) et se synchronisent entre elles. Une petite démo à confirmé l’effet “whaou” de cette nouvelle solution.\n\n“Beyond the design patterns and principles - writing good OO code” et “How I started to love what they call Design Patterns”\n\nCe Forum a aussi été l’occasion de voir (ou revoir) quelques principes fondamentaux autour des Design Patterns, avec Matthias Noback et Samuel Roze.\n\nIls ont présenté des exemples concrets, des mises en application de certains Design Patterns incontournables, toujours dans l’optique de découpler notre logique métier, de mieux réutiliser notre code et en améliorer la maintenabilité. Le Domain Driven Design a donc logiquement été mis à l’honneur, ainsi que le typage fort pour donner du sens au code et favoriser sa bonne utilisation. Pour éviter la dette technique, il faut “acheter en avance la capacité de changer”.\n\n\n\n“We got rid of management”\n\nMichelle Sanver nous a présenté l’holacratie : un système d’organisation basé sur l’intelligence collective, utilisé chez Liip. Elle a démarré la conférence en expliquant les défauts de la hiérarchie pyramidale (la photo de la slide parle d’elle-même ;-)) :\n\n\n\nEn holacratie, la société s’organise en cercles et sous cercles de responsabilité et chaque collaborateur se voit donner des rôles et les moyens de l’assumer. Les décisions sont prises collectivement et le pouvoir n’est pas centralisé dans les mains de quelques personnes. Tout est basé sur la transparence, en particulier la rémunération et les budgets. Le but principal est d’assurer des conditions de travail bienveillantes et que chacun se sente en sécurité pour exprimer au mieux ses talents et réduire les tensions. Notamment, toutes les réunions sont facultatives :-D\n\nLiip a développé un outil communautaire en SAS (www.holaspirit.com) qui simplifie l’organisation en gérant les cercles, les prises de décisions et la remontée des propositions d’améliorations (ou “tensions”). Cette outil paraît indispensable, car central dans l’organisation.\n\nOn voit très bien comment une holacratie peut se mettre en place dans une nouvelle organisation ou de taille réduite mais la conférencière n’aborde pas trop cet aspect pour les entreprises de taille conséquente.\n\n“Vous n’avez pas besoin de ça”\n\nLe début de la conférence était volontairement critique sur l’utilisation des nouvelles technos puis peu à peu se dirigeait sur du bon sens dans le choix des technos avec de bonnes raisons (et non pas car elles sont “à la mode”).\n\nPour cela (Charles Desneuf) nous a cité les avantages et inconvénients de différents outils (micro-services, GraphQL, SinglePageApp, Microservices, GraphQL,…)\n\nLe but était vraiment de pousser à la réflexion dans ce type de choix en prenant bien en compte le contexte (utilisateurs, équipe, qualités attendues,…) et de ne pas apporter de la complexité inutilement (optimisation/abstraction prématurée, modélisation inadapté,..).\n\nUne conférence bien dirigée en concluant par :\n“Vous n’avez (peut-être) pas besoin de ça (maintenant).”\n\n“Cessons les estimations”\n\nFrédéric Leguédois nous a livré une conférence proche du one man show sur les estimations et les deadlines. Il nous a rappelé un point important que l’on oublie parfois : Les estimations ne sont QUE des estimations, on ne peut pas les prendre comme des engagements de la part de celui qui les fait. Ses exemples humoristiques sur les différentes façons dont sont faites les estimations provoquaient fréquemment l’hilarité de l’audience qui répondait par des applaudissements généreux.\n\nMême si le trait était forcément grossi pour le « spectacle » on a passé un très bon moment et ça fait réfléchir, notamment sur le fait que le changement est normal.\n\n“En vrac”\n\nQuelques autres conférences que vous pouvez visionner sur le site de l’AFUP :\n\n\n  « Serverless et PHP » : Matthieu Napoli nous a montré comment déployer des fonctions Lambda chez AWS en PHP – et pourquoi.\n  « Développeurs de jeux vidéo: les rois de la combine » : Laurent Victorino nous a complètement enfumé avec sa présentation interactive !\n  « Voyage au centre du cerveau humain, ou comment manipuler les données binaires » : un retour d’expérience enrichissant de Thomas Jarrand, parlant d’IRM, de binaire, ou encore de voxels.\n\n\nNous avons aussi présenté trois conférences, autour de sujets que nous pratiquons au quotidien chez M6 Web :\n\n\n  Benoit Viguier a parlé de programmation asynchrone avec les générateurs de PHP : une fonctionnalité extrêmement puissante, mais encore trop peu connue. Il a d’ailleurs annoncé la sortie de la bibliothèque Tornado. (vidéo de la conférence)\n  Guillaume Bouyge nous a raconté l’histoire de la migration à l’international de la plate-forme 6play : comment, en partant d’un produit développé pour M6, nous sommes arrivés à un produit en marque-blanche vendue à d’autres clients d’autres pays. (vidéo de la conférence)\n  Et Pascal Martin a présenté Kubernetes, l’outil que nous utilisons pour piloter des conteneurs Docker dans Le Cloud. Il a enchaîné avec plus de détails sur le processus que nous suivons pour migrer nos projets vers cet hébergement ; vous pourrez en apprendre plus en lisant Le Plan Copenhague. (vidéo de la conférence)\n\n\nVivement l’AFUP Day Lyon 2019, où nous aurons sans doute le plaisir de vous rencontrer à nouveau ?\n"
} ,
  
  {
    "title"    : "Generators for Asynchronous Programming: User Manual",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2018/10/26/generators-for-async-programming-user-manual.html",
    "date"     : "October 26, 2018",
    "excerpt"  : "Les générateurs sont souvent réduits à une simplification des itérateurs, mais ils sont surtout très pratiques et performants pour executer des traitements asynchrones. Nous aborderons le fonctionnement d’un programme asynchrone, le rôle des promi...",
  "content"  : "Les générateurs sont souvent réduits à une simplification des itérateurs, mais ils sont surtout très pratiques et performants pour executer des traitements asynchrones. Nous aborderons le fonctionnement d’un programme asynchrone, le rôle des promises, et approfondirons l’utilisation des générateurs pour simplifier l’écriture de notre code. Enfin nous détaillerons des cas pratiques « prêts à l’emploi » pour tout type d’application, avec un retour d’expérience sur ce qui a été mis en place chez M6Web.\n"
} ,
  
  {
    "title"    : "Docker en prod ? Oui, avec Kubernetes !",
    "category" : "",
    "tags"     : " conference, php, open-source, afup, docker, kubernetes",
    "url"      : "/2018/10/26/docker-en-prod-oui-avec-kubernetes-pascal-martin.html",
    "date"     : "October 26, 2018",
    "excerpt"  : "Kubernetes. À en croire certains articles, c’est une solution miracle. Développeurs, vous avez peut-être entendu ce mot ?\nC’est l’outil qui vous permettra de déployer du Docker en production ! Parce qu’autant utiliser Docker en dev c’est facile, a...",
  "content"  : "Kubernetes. À en croire certains articles, c’est une solution miracle. Développeurs, vous avez peut-être entendu ce mot ?\nC’est l’outil qui vous permettra de déployer du Docker en production ! Parce qu’autant utiliser Docker en dev c’est facile, autant en prod…\n\nMais qu’est-ce que Kubernetes ? Quelles possibilités si intéressantes nous fournit cet orchestrateur de conteneurs ?\nPods, nodes, deployments, services, ou auto-scaling et health checks : autant de primitives et de fonctionnalités que vous allez découvrir et adorer, y compris en tant que développeurs !\n\nAprès avoir présenté ces bases, je vous proposerai un retour d’expérience sur la migration vers Kubernetes que nous sommes en train d’effectuer pour 6play.fr. Comment développeurs et sysadmins se répartissent-ils les tâches ? Avons-nous dû adapter nos applications PHP ? Quelles difficultés avons-nous rencontrées, quels compromis avons-nous acceptés et quelle route nous reste-t-il à parcourir ?\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, global review",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/24/kubecon-2018-copenhaguen-global-review.html",
    "date"     : "May 24, 2018",
    "excerpt"  : "After those three days at KubeCon, we’ve seen and heard a lot of interesting ideas. You can read about our first day here, about our second day there, and about our third day there. If we had to do a short recap, here are the points we would list....",
  "content"  : "After those three days at KubeCon, we’ve seen and heard a lot of interesting ideas. You can read about our first day here, about our second day there, and about our third day there. If we had to do a short recap, here are the points we would list.\n\nFirst of all “Cloud native” seems to be the buzzword of the year. Not just cloud anymore, but cloud native! What does it mean? Instead of just deploying your application to the cloud, it should fully use the cloud.\n\nThen, Kubernetes. It is a mature solution in itself. There doesn’t seem to be any doubt left about that. Even if this is the case with Kubernetes, the majority of the ecosystem around it is not mature yet, and that’s a bit of a problem. We saw a lot of tools and some that were presented during talks are still WIP and some demos completely failed. If you’re surfing the Kubernetes wave, please be careful with the tools you choose, and don’t loose yourself adopting a fancy/non-working tool that will bring your infrastructure down. Continue to master what you do without being trapped by the hype brought by certain solutions.\n\nDeployment, CI and CD. Well, not so much. There are a few projects out there and several different approaches (kubectl apply, a bit of Jenkins around it, deployments from inside the cluster, several black boxes like CodeFresh…), but not one thing that everyone is doing/using. We are currently looking at Jenkins-X and hope we’ll be able to build our CI/CD stack using it.\n\nFor monitoring, use Prometheus. It’s pretty much what everyone is using.\n\nService mesh. One of the big things this year, everybody is talking about it, Istio seems to be taking the lead. It’s still moving fast, though and not enough people are truly mastering this in a production setup.\n\nDevelopment environment. Well, “LOL” would do it maybe. This is clearly not a priority yet. Some teams and projects (like Telepresence) have started working on this, but there is still road ahead.\n\nGitOps. Everybody is going this way. Versioning, of course. But also using Git events to pilot things.\n\nThings are beginning to move on the security side of things. Companies are starting to notice there is work to be done, startups are appearing with different services.\n\nAnd, finally, multi-clusters. We felt a few people are using multi-cluster, but it’s often done by hand. That doesn’t seem mature at all. Maybe a subject we’ll hear more about in the future?\n\nIn any case, we had a really great time during this KubeCon in Copenhagen, we saw many interesting talks and discussed with lots of people!\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 3",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/23/kubecon-2018-copenhaguen-day-3.html",
    "date"     : "May 23, 2018",
    "excerpt"  : "Back to KubeCon 2018 in Copenhagen, for the third and last day! You can read about our first day here and about our second day there.\n\nKeynotes\n\nCloud Native ML on Kubernetes - David Aronchick, Product Manager, Cloud AI and Co-Founder of Kubeflow,...",
  "content"  : "Back to KubeCon 2018 in Copenhagen, for the third and last day! You can read about our first day here and about our second day there.\n\nKeynotes\n\nCloud Native ML on Kubernetes - David Aronchick, Product Manager, Cloud AI and Co-Founder of Kubeflow, Google &amp;amp; Vishnu Kannan, Sr. Software Engineer, Google\n\nTrying to run Machine Learning on Kubernetes? Working with Jupyter and Tensorflow? Kuebeflow could be of interest to you.\n\nRunning with Scissors - Liz Rice, Technology Evangelist, Aqua Security\n\nEven if we never fall, running with scissors could be dangerous. We wouldn’t run with scissors, would we? Then, why do we keep running containers with privileges they should not need? And why do we mount more directories than needed as volumes?\n\n\n\nChaos Engineering WG Deep Dive – Sylvain Hellegouarch, ChaosIQ\n\nOne of the goals of Chaos Engineering being to break stuff, the talk started by a word about mindset: one must love supporting the team, and not try to save the day herself. Also, one must nurture empathy (including for the system), be assertive but not arrogant, not blame / be snarky (many of us are not great on that point).\n\nChaos Engineering follows a continuous loop. Start with a steady state (a baseline that comes from objective observation, which means you need business metrics and to collect data), formulate an hypothesis (not necessarily business oriented), and define an experimental method (the things we vary to prove / disprove the hypothesis). Warning: don’t vary too many things at once.\n\nThe talk finished with a few words about the Chaos Toolkit, which aims to be simpler than the famous Chaos Monkey.\n\nIstio - The Weather Company’s Journey - Nick Nellis &amp;amp; Fabio Oliveira, IBM\n\nSeveral istio talks weren’t enough for me, I wanted more. I didn’t learn much more on this new one, except those tips:\n\n  You can define route specific retries with Istio\n  They use vistio to visualize istio traffic. That tool seems based on Netflix’s Vizceral. Unfortunately, I couldn’t find any GH repo nor blog talking about vistio.\n  If you want to implement Istio: start small\n\n\nAre You Ready to Be Edgy? — Bringing Cloud-Native Applications to the Edge of the Network - Megan O’Keefe &amp;amp; Steve Louie, Cisco\n\nThere are a few common problems in a cloud computing setting. Many devices (with IoT for instance) can mean a bottleneck on the network side of things, the cloud being far away can cause latency problems. Edge computing, moving apps (or parts of apps) from a centralized cloud, could help with those problems. Think AR/VR for latency, Video for high bandwidth, facial recognition for temporary/secure data.\n\nThe talk introduced the idea of deploying Kubernetes in edge locations – like in cell towers – and pointing users to the closest location. Those Kubernetes clusters would only host APIs or applications with specific needs, while the main parts would remain in a centralized cloud, which means we would have to develop edge-ready applications, keeping in mind problems such as network splits, data synchronization, deployment, …\n\nIntegrating Prometheus and InfluxDB - Paul Dix, InfluxData\n\nI was a lot surprised in this talk as we were like 30 in a room for 300.\nI guess people don’t need to keep metrics more than 15days with the Prometheus engine, or maybe people are already using InfluxDB as a guaranteed long term storage.\nBecause we are in the second case, we are testing influxDB with Prometheus, so this talk came at the right time.\nI didn’t learn much on InfluxDB + Prometheus, nor on federated queries that comes with HA.\n\nPaul then questioned why not use a single query language for all query engines? That would be more practical and maintainable. The question is still open, even if Paul proposes IFQL to rule them all. The main idea is for all query engines to coordinate and stop creating a new language on each new engine.\n\nA Hackers Guide to Kubernetes and the Cloud - Rory McCune, NCC Group PLC\n\nFirst, a word about threat models. Pretty much everyone will see random Internet hackers looking for easy preys. Some of us will be specifically targeted by attackers. Only a few will be targeted by nation states. You should think about your threat model, which may or may not be the same as your neighbour’s.\n\nThen, time to think about your attack surface. Attackers will find the weakest point, which is not always where your might think. What about the cloud around your Kubernetes cluster? Github is a great way of getting accesses (many commit their credentials and/or do not remove them from history – bots are crawling this!). Developers’ laptop are generally full of interesting data, and are not necessarily protected enough.\n\nOn Kubernetes, external attackers will try to access the API server and etcd, the kubelets, or maybe inject malicious containers. You should turn off the insecure port, control access to kubelet and etcd, retrict the use of service tokens, restrict privileged containers, enable authentication and authorization on the API server, set pod security and network policies, and do regular upgrades. Also, don’t forget about cloud permissions.\n\nCloudbursting with Kubernetes - Irfan Ur Rehman &amp;amp; Quinton Hoole, Huawei Technologies\n\nThat might not be everyone’s problem, but still interesting to hear about. If you have multiple cloud providers with different pricings, you might want to optimize your costs by using the most expensive only on load peaks. That is exactly what they did, using Kubernetes clusters federation and specific annotations. We won’t go over this approach because we’ll stick with one cloud provider, but that may be interesting for some people.\n\nOperating a Global-Scale FaaS on Top of Kubernetes - Chad Arimura &amp;amp; Matt Stephenson, Oracle\n\nThis was about the Fn project. A couples of problems related to multitenancy: network isolation on Kubernetes, noisy neighbors (I/O being the bottleneck). Helm has a few limits, worked around with shell scripts.\n\nInside Kubernetes Resource Management (QoS) – Mechanics and Lessons from the Field - Michael Gasch, VMware\n\nResource management goes through cgroups. With containers, we see all the CPU/RAM, but this doesn’t mean we’ll be able to use them all: we may have to share with other containers. Works with requests. For now, cpu and memory are stable resources, but others (hugepages, ephemeral storage, device plugins) are in beta. You should align Kubernetes’ QoS with the underlying infrastructure, enable quotas in the cluster, and protect critical system pods.\n\nI have to admit I didn’t take much notes during this talk, but noted the slides contain a lot of informations – for more, go read them ;-)\n\nObserving and Troubleshooting your Microservices with Istio - Isaiah Snell-feikema, IBM &amp;amp; Douglas Reid, Google\n\nI promise, this is the last Istio conf I went to.\nIn case that wasn’t obvious, Istio is becoming the default service mesh, like Prometheus is for metrics. I couldn’t work much on it so I wanted to learn the most possible from it during this KubeCon. I can say this talk was one of the best for Istio discovery and even advanced skill. I won’t be able to summarize everything, so here are few tips I kept from it:\n\n  Envoy’s /stats route gives a lot of infos of servers\n  Istio system logs gives also traffic spikes\n  Istio’s access logs can be uploaded to fluentd/elk\n  Canary testing / blue-green deployments can be done via RouteRule CRD.\n\n\nIf you are considering using Istio, you must see the slides\n\nVitess and a Kubernetes Operator - Sugu Sougoumarane, YouTube\n\nI heard about Vitess for the first time during this KubeCon, even though it’s an old project. It’s a middleware for MySQL, sitting between the database and our applications. It helps scale it through sharding – the goal being to answer the common pain points for databases: scalability, cloud and making DBAs happy. It’s also one of the CNCF project I noted a few days ago I should take a closer look at.\n\nFinal words?\n\nThe weather was nice and our plane was only on Saturday, so we finished the day with a walk in the City.\n\n\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 2",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/22/kubecon-2018-copenhaguen-day-2.html",
    "date"     : "May 22, 2018",
    "excerpt"  : "After an interesting first day at KubeCon 2018, we are back for the second day!\n\nAutoscale your Kubernetes Workload with Prometheus - Frederic Branczyk, CoreOS\n\nThe goal of autoscaling, ultimately, is to fullfill SLO of SLA through SLI (yeah, you ...",
  "content"  : "After an interesting first day at KubeCon 2018, we are back for the second day!\n\nAutoscale your Kubernetes Workload with Prometheus - Frederic Branczyk, CoreOS\n\nThe goal of autoscaling, ultimately, is to fullfill SLO of SLA through SLI (yeah, you may have to think for a while after reading this ^^). Demand must be measured by metrics, which must themselves be collected, stored and made queryable. Autoscaling can be horizontal (increase replicas when necessary, the focus of this talk) or vertical (increase resources request/limits when necessary).\n\nAutoscaling on Kubernetes used to rely on Heapster. But Heapster’s API is unspecified, it doesn’t work with Prometheus, and vendor implementations are often unmaintained. Starting with 1.8, Kubernetes provides a resource and custom metrics API. It is a specification (not an implementation), the implementations are developed and maintained by vendors, and each metric returns a single value. Kubernetes’ HPA (HorizontalPodAutoscaler) uses these metrics to scale up/down.\n\nCore metrics are CPU and RAM, by pod/container/node, and there is a canonical implementation called metrics-server. The custom metrics API has the same semantics (a single value is returned) but no canonical implementation is provided (take a look at DirectXMan12/k8s-prometheus-adapter for an implementation). Each metric is related to a Kubernetes object (Pod, Service, Deployment, …). Finally, there is also an external metrics API (currently in alpha stage) for things not related to a Kubernetes object (like the queue length for a queuing service provided by a Cloud provider – SQS on AWS, for example).\n\nPod Anomaly Detection and Eviction using Prometheus Metrics - David Benque &amp;amp; Cedric Lamoriniere, Amadeus\n\nThis talk started with a reminder: stability is hard, especially for a distributed system. And using load-balancers doesn’t help at all, on the contrary. Solutions include proximity-based load-balancing (sharding by Availability Zone, which is not something Kubernetes does natively, but for which Istio can help with its pilot-agent proxy as it is AZ-aware). Using healthchecks (liveness kills the container when it fails, readiness only removes it temporarily from service discovery when it fails) is a good idea, but you should keep probes simple (complexity = bugs) and you shouldn’t check external dependencies. Also, don’t forget about circuit breakers and retries. But note all this suffers from limitations, as it’s only based on technical signals and depends on local decisions (pods/containers, service mesh/proxy).\n\nThis conference was about a tool called Kubervisor, which aims to detect mis-behaving Pods and remove them from the cluster (switching a label on the corresponding service). The decisions are based on metrics (using PromQL for Prometheus metrics) that can include business metrics, and not only technical ones nor data limited to a specific pod.\n\nI didn’t write much about this, I was too busy watching the demo, which was really interesting. In the end, I wrote down I should (and I probably will) take a better look at this in a few months, when we have deployed a few more pieces of software to Kubernetes.\n\nChallenges to Writing Cloud Native Applications - Vallery Lancey, Checkfront\n\nBuilding and deploying an application to the cloud has advantages: automatic scaling, load-balancing, replication, infrastructure provisioning and teardown is done for us, … It also has challenges.\n\nStorage of persistent data is one of these challenges (simple replication is rarely a good idea, working with unreplicated shards is a common pattern, using multiple volatile copies is a good strategy, but the best approach is runtime data replication – which requires a large setup and implies non-negligible maintenance costs). Services coupling (database + services interacting with it) and internal API calls (source of delays) are also common sources of troubles, even if the second can be mitigated with simpler API actions, endpoints for specific actions (not CRUD!), batch endpoints and caching. Testing is a huge concern, especially with microservices, and working with discrete compoments helps, as we can run one service and mock the others. Finally, local development is not a solved problem yet (in any case, you should remove the “build an image” step).\n\nGitOps for Istio - Manage Istio Config like Code - Varun Talwar, Stealth Startup &amp;amp; Alexis Richardson, WeaveWorks\n\nDuring this talk, WeaveWorks team talked about how Istio config can be managed like code through git based workflows.\nThey evoked using Terraform to describe cloud state.\nAs of gitops principles, devs shouldn’t use kubectl to interact with clusters. Additionally, they should push code, not containers. GitHub events must lead deployments, not humans.\nAs part of this automation, deployments must auto-rollout when things break. They either fail or succeed cleanly.\nOne can use operator patterns to help integrating those concepts. The WeaveWorks team also talked about flux to manage environments states.\nWeave Flux brings a lot more annotations for Istio, making automated releases deployments, etc.\n\nOPA: The Cloud Native Policy Engine - Torin Sandall, Styra\n\nPolicy enforcement is a fundamental problem for an organisation, and policy decisions should be decoupled from policy enforcement. Open Policy Agent is an open-source general-purpose policy engine. It uses a high-level declarative language, can be used to implement RBAC and has integrations with Istio or Terraform. This is not my current priority, but it could be worth taking a look at OPA if you need to add policy enforcement to your application.\n\nKubernetes Multi-Cluster Operations without Federation - Rob Szumski, CoreOS\n\nA lot of people are using multiple Kubernetes clusters. For example, Zalando uses 80. It can become a mess to manage all those clusters with their specific components like secrets, controllers, configmaps, etc.\nTo solve this problem, a new cli has been created: kubefed.\nBut Rob explains that this new tool doesn’t solve all the problems. I.E: You’ll have to give access to all-clusters to people and not only few clusters (that breaks isolation), the Federation API must be run by a top-root user (accessing everything), etc.\n\nCoreOs brought the concept of k8s operators.\nRob explains why that solves problems and why you should use that instead of Federation.\n\nClearly I wasn’t convinced at all by this presentation. Besides, we are working in the same building and problems brought by Rob (modifications by hundred of devs/SRE split across the world) do not concern us at the moment.\n\nBuilding a Kubernetes Scheduler using Custom Metrics - Mateo Burillo, Sysdig\n\nThere are so many possibilities when scheduling pods. The scheduler first applies filters (resource requests, volumes, selectors/taints), then ranks (including the default behavior of spreading pods of the same service), then goes to applying hard constraints (taints, node selector), and soft contraints (prefer no schedule, node affinity, pod affinity, weight) and pod priority and does taint-based evictions. To understand what actually goes on and prevent complex situations, you should only add important constraints.\n\nIn some cases, you might want to build a custom scheduler, using custom metrics (when the default scheduler is not good enough for you and/or you have very specific needs). An example, based on sysdig metrics: draios/kubernetes-scheduler. And more informations in this blog-post. Remember creating a scheduler is not an easy task and many things can go wrong (think about concurrency and race conditions).\n\nIn the end, the idea of implementing a custom scheduler might be interesting, but a bit scary: messing things up could mean no pod getting scheduled, which is not a nice scenario. I’m not sure I currently see a situation in which I’d go this way…\n\nClusters as Cattle: How to Seamlessly Migrate Apps across Kubernetes Clusters - Andy Goldstein, Heptio\n\nAs many people, Andy has a lot of clusters. To re-route traffic between clusters, he uses Envoy.\nTo maintain consistent configurations, he uses Ansible to provision everything.\nSo far, I don’t really see the point of having a lot of clusters and even less of migrating a single app between clusters, but that can be interesting for people who like that trend.\n\nParty, Tivoli Gardens\n\nFor the evening, we went to an all-attendees party at Tivoli Gardens, an amusement park and pleasure garden right in the middle of Copenhagen. We walked around for a bit, before settling for a beer and a few snacks, talking with other French Kubenetes fans.\n\n\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 1",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/21/kubecon-2018-copenhaguen-day-1.html",
    "date"     : "May 21, 2018",
    "excerpt"  : "At the very beginning of May, we (Pascal and Vincent) went to KubeCon 2018. It was a three-days long event, with more than 300 conferences. 4300 people met at Bella Center, a huge conference place close to Copenhagen in Denmark. Here are some of o...",
  "content"  : "At the very beginning of May, we (Pascal and Vincent) went to KubeCon 2018. It was a three-days long event, with more than 300 conferences. 4300 people met at Bella Center, a huge conference place close to Copenhagen in Denmark. Here are some of our notes about some talks we saw.\n\nKeynotes\n\nCNCF Project Update - Liz Rice, Technology Evangelist, Aqua Security; Sugu Sougoumarane, CTO, PlanetScale Data; Colin Sullivan, Product Manager, Synadia Communications, Inc. &amp;amp; Andrew Jessup, Co-founder, Scytale Inc.\n\nDuring this keynote, I realized the CNCF is helping a lot more projects than I thought it was: it’s not just Kubernetes. I will take a closer look to some of them in the future – they are all listed on l.cncf.io.\n\nCERN Experiences with Multi-Cloud Federated Kubernetes - Ricardo Rocha, Staff Member, CERN &amp;amp; Clenimar Filemon, Software Engineer, Federal University of Campina Grande\n\nOK, so, sometimes, it actually is rocket-science (or pretty close to it). It’s nice seeing Kubernetes used for science and research, on a federation of around 400 clusters!\n\n\n\nWhats Up With All The Different Container Runtimes? - Ricardo Aravena, Branch Metrics\n\nOverview of the different containers runtimes, starting with OpenVZ in 2006 (still maintained, but the last 2.7 version is not as stable as the previous one, and it doesn’t support Kubernetes), and LXC (Kubernetes support is low-priority WIP and LXC uses a specific image format) in 2011. Docker (initially based on LXC) arrived in 2013 and things have gone insane since then. With libcontainer and rkt in 2014, OCI in 2015 and CRI in 2016. Today, Kubernetes supports several runtimes.\n\nrkt could be interesting from a security point of view (supports TPM and VMs). With Kubernetes 1.10, the default runtime is runc. crun is the most interesting runtime for performances, but is WIP and isn’t currently used much. Kata, released in 2018 has the best security (it runs containers in VMs) and is OCI compliant, but is slower and more heavyweight. Other very specific runtimes include nvidia, railcar, pouch, lmctfy, systemd-nspawn…\n\nBasically, today and for most workloads, you should go with the standard docker/containerd runtime. There is a convergence towards OCI, it’s the default for Kubernetes and docker is going to adopt the CRI plugin.\n\nIntroduction to Istio Configuration - Joy Zhang, Google\n\nAn introduction to the Istio Service Mesh. All Istio components are CRDs. CRDs are becoming a standard when it comes to kubernetes customizations, here requests proxying.\n\nThis talk described Istio components, notably:\n\n\n  Mesh config - global Istio config\n  Service config - Istio operators config\n  Consumer config - overrides config model\n  Galley - Istio config per cluster\n\n\nFYI: you need as much Galleys as you have clusters + environments.\n\nContinuous Delivery Meets Custom Kubernetes Controller: A Declarative Configuration Approach to CI/CD - Suneeta Mall &amp;amp; Simon Cochrane, Nearmap\n\nKubernetes is great but, CI/CD is not really its job – and CI/CD depends a lot on the company you work at and its culture. Here, they started deploying their applications with kubectl and YAML files, and even if CI usually doesn’t cause much troubles, CD is not that easy. And using a managed CI (CircleCI, shippable, AWS Codepipeline…) means exposing Kubernetes’s control plane on the Internet, which is not great. Also, the Gitops approach with its git is the source of truth mindset is OK, but committing version numbers again and again pollutes the history with a lot of noise – when this history could be kept in the cluster itself.\n\nSo, they went with some kind of CD lite: a service running in the cluster, which monitors the registry and deploys the new version of an image when it sees it. This means the cluster doesn’t need to access git and the CI chain doesn’t need to access the cluster, making the configuration simpler.\n\nThe tool they developed for this is nearmap/cvmanager. It seems relatively easy to install and configure. And I like the idea of having a gap between Git/CI and CD. I may have to test this, especially to see what it can do when it comes to canary or blue/green deployments, but this is one of the things I saw at KubeCon I will discuss with my colleagues!\n\nPractical and Useful Latency Analysis using Istio and OpenCensus - Varun Talwar, Stealth Startup &amp;amp; Morgan McLean, Google\n\nOpenCensus (distributed tracing metrics system) + Istio is the combo provided for this talk to let devs debug the most of their apps.\n\nOpenCensus is a tracing tool, like the CNCF’s project Opentracing. It can be implemented in many languages, starting with those we use: PHP and javascript.\n\nOpenCensus can trace RPC and http APIs calls.\n\nYou can install a dedicated dashboard that gives a lot of metrics out of the box (like 90th percentiles, etc.) and allows customizable ones.\n\nMixer (Istio tool) makes the aggregation between those metrics from what is gathered by OpenCensus.\n\nThis definitely need to be tested, but not sure it is worth migrating from OpenTracing to OpenCensus.\n\nHabitat Builder: Cloud Native Application Automation - Elliott Davis &amp;amp; Fletcher Nichol, Chef.io (Habitat.sh)\n\nI had never heard of Habitat before, so I was kind of curious what this was about. This idea of platform-independent build artifacts (with exporters for docker, Kubernetes, Helm) could be interesting for some teams, but it’s not a need I currently have. The automated deployments might be interesting though, but we are already looking at other tools.\n\nKubernetes and Taxes: Lessons Learned at the Norwegian Tax Administration - A Production Case Study - Bjarte S. Karlsen, The Norwegian Tax Administration\n\nReally nice production case study from the Norwegian Tax Administration about their k8s platform. They are currently using Rancher 2.0 + OpenShift + CodeFresh on top of k8s. All their Docker images are alpine based. They develop Java applications.\n\nOne idea kept my attention is the tagging of their containers:\n\n\n  Pushing docker image tag 1.2.3 also pushes tag 1.2 and tag 1.\n  So going to v2 and rolling back to v1 effectively rolls out 2.0.0 to 1.2.3 without knowing the exact subversions.\n  You are assured that the tag of the major version always points to the latest subversion.\n\n\nIt’s not clear to me how to implement that. Maybe a codefresh hack. But still, I found the approach interesting.\n\nAnother lesson for those who want to migrate from on-premise to the cloud is to keep things that work on-premise and simply migrate them on cloud as it is. That will ease the migration. You can rethink all afterward if needed, but the first step is to migrate, not rebuild from scratch plus migrating.\n\nAs they use CodeFresh for CI/CD, it’s easy for them to automate their pipelines. That’s another lesson: automate everything. To automate, you need to standardize.\n\nAnother lesson is to use what is rock-solid. We all see a lot of tools and startups around the cloud nebula. A lot of them won’t last and some will, like Kubernetes. This is the tip: use what will last. Don’t build your whole infrastructure on something unstable or with poor pro/community support.\n\nThe final point I kept from this really good rex, is to create a predictable infrastructure. You cannot guess what will happen. You have to know and use the right tools/annotations to make it behave the way you want, predictably and repeatedly.\n\nHow to Export Prometheus Metrics from Just About Anything - Matt Layher, DigitalOcean\n\nThis presentation was about a few good practices to follow when it comes to exposing Prometheus metrics from a Go application. Basically, you should use the Go client library, be really careful about concurrency, build reusable packages, write unit-tests, use promtool check-metrics, and read and follow the Prometheus metrics best practices.\n\nContinuously Deliver your Kubernetes Infrastructure - Mikkel Larsen, Zalando SE\n\nAnother really good rex from Zalando from their utilization of k8s in prod and lessons learned.\n\nThey talked of Stups, a Zalando toolset around AWS. That definitely needs to be tested.\n\nFrom their experience of managing a k8s cluster on AWS EC2s, they gave us few tips:\n\n\n  Always upgrade to the latest k8s version\n  Manage the smallest possible number of clusters\n  Automate all the things. The only manual step should be merging PRs. This is a base GitOps principle.\n  Define an AWS HA control plane setup behind ELBs. That can be debated but this is a good first step.\n  All cluster config files must be git versioned (another GitOps principle). An upgrade is then only a git branch merge at some point.\n\n\nSome of the points above can be achieved via a CD tool. I remember they use Jenkins for that, but not 100% sure. Alongside this CD tool, there should be a CI tool (or one tool for both).\n\nThey gave us some points on CI tests too:\n\n\n  Run e2e conformance tests for k8s config files\n  Run statefulSet tests\n  Run any additional homemade tests\n\n\nFor those who are using AWS, keep in mind the following: volumes cannot be mounted across several AZ.\n\nKeep yourself away from unavailability by always setting minAvailable.\n\nFinal tip: If you go for a self-managed k8s cluster (not EKS, GKE, etc.), check that nodes are up and running before continuing upgrade.\n\nI really enjoyed this rex that was full of good prod-ready advices.\n\nI recommend you take a look at the slides\n\nSeamless Development Environments on Kubernetes using Telepresence - Ara Pulido, Bitnami\n\nKubernetes is a great production environment, but it feels like development environment is kind of an afterthought: even for a simple application, if you want to develop (locally?), things are not easy. People are currently using two distinct ways: using docker compose to replicate the production environment (but compose doesn’t do everything: rbac, job, ingress… and having to maintain everything twice is not fun), or build/push/deploy-to-k8s and wait many seconds everytime one wants to F5 on a page, which is unberably slow (I wouldn’t ask my developer colleagues to do this for even half a day!).\n\nThe solution proposed during this conference is Telepresence. It allows a developer to swap out a pod from a cluster and inject her own pod, running locally, at its place. Some sort of VPN is established between her computer and the cluster, which means the pod running locally behaves just like if it was still in the cluster (including DNS, service discovery, access to non-Kubernetes managed services and all).\n\nThere are still limitations and constraints (if two developers want to work on the same service, they’ll each need their own namespace in the cluster, as two people cannot swap out the same pod), but plans for this project are interesting and I will definitely take a closer look at it in a couple of months, when I start thinking more about our development stack!\n\nWe went to Datawire’s booth and saw a nice demo. And also learnt about other tools, such as Forge and Ambassador that can duplicate production requests to a local pod. We found that this feature is ultra dope!\n\nPerformance and Scale @ Istio Service Mesh - Fawad Khaliq, VMware Inc, Laurent Demailly, Google &amp;amp; Surya V Duggirala, IBM\n\nReturn of Istio devs on project’s recent updates: closed PRs, enhancements, etc.\n\nThat was not really what I was looking for so I went to some bootcamps to say hi, especially the HAProxy bootcamp one that is always a good moment. Special thanks to Baptiste for his time and the awesome talk we had!\nFor the record: HAProxy has it’s own Ingress Controller\n\nFrom Data Centers to Cloud Native - Dave Zolotusky &amp;amp; James Wen, Spotify\n\nThis last conference of the first day was about Spotify’s migration from their on-premise datacenters to the cloud. For many years, they were doing everything on-prem (including a 3000 nodes Hadoop cluster – the largest in Europe, at the time), often developing their own proprietary software (like custom monitoring, proprietary messaging framework, custom Java service framework, custom container orchestrator, … Some have been open-sourced). The first step for them has been to get out of their own datacenters, moving everything to another datacenter (but still using their proprietary stuff). It took them three years and a half, trying to make this migration as seamless as possible for the development teams.\n\nNext step is to become cloud native, especially moving to Kubernetes. They did this in several steps, starting small by sending production traffic to one service deployed to one cluster for one hour (allowed them to validate DNS, logging, service discovery, metrics system, networking). Then, three services on one cluster (permissions, namespaces, quotas for each namespace, developers documentation). After that, services on a volunteer basis (clusters, scripted clusters creation, secrets, deployment tooling based on a wrapper around kubectl, CI integration =&amp;gt; a lot of learning for a lot of people). Then, two high-traffic services, including a service receiving 1.5 million requests per second (horizontal auto-scaling, network setup, confidence, reference for other projects). And, finally, self-service migration, with teams migrating when they want, following the docs, and ops not always knowing what’s running in the cluster (reliability, alerts, on-call, disaster recovery, backups, sustainable deploy). Everything going pretty much fine by now, it’s time to investigate on a few odd things and specific needs, with a temporary ops team assembled to help.\n\nThe most important idea here is you don’t have to do everything right from the start. For example, they waited quite a long time before setting up a sustainable deployment method, which might seem odd to many of us. But it allowed them to move forward and validate a lot of things one after the other. That’s something I will keep in mind: if it worked for them (4000 employees, including 500 techies), it could work for many other companies!\n\nJenkins X: Easy CI/CD for Kubernetes - James Strachan, CloudBees\n\nThis might be one of the hottest project of this early 2018.\n\nWe already saw this project that was created in February, and we are using it for testing purposes. We hope to use it in production very soon.\n\nFor those of you who don’t know Jenkins-X:\n\n\n  It’s piloted by jx, a command line tool (Mac/Linux)\n  It drives a Jenkins instance + Docker Registry + Nexus + Chartmuseum + Monocular\n  It allows you to manage your app’s deployments via Jenkins blueocean’s pipelines with k8s endpoints\n  That means Jenkins will be able to run CI tests, Continuously Deploy your project to preview, staging, prod and so on with Skaffold/Helm to k8s\n  Jenkins will run pipelines from the Jenkinsfile in the repo to do that CI/CD part\n  In the provided pipelines given with jx import, you will use provided docker images that embed jx cli and other tools to manage the deployments of your app.\n  That allows you to promote your app between stages, build your docker image, etc. in your pipeline steps.\n  Those deployments are based on Helm Charts in the repo.\n  Jenkins-x follows GitOps strategy, that means anything useful is stored in each app’s repo: it is versioned and git events will trigger pipelines.\n\n\nJenkins-X brings this CI/CD part that was missing for k8s users. Gitlab + gitlab-ci were already doing that for some years now, but nothing was that fancy for GH users.\n\nWe are very excited about Jenkins-X, as it answers a lot of problematics and brings in gitops as core concept. We’re actively adopting it and we hope to give feedback asap.\n\nKeynote: Anatomy of a Production Kubernetes Outage - Oliver Beattie, Head of Engineering, Monzo Bank\n\nThis keynote was about  major outage at Monzo. You can read more about it in the post-mortem they posted after it happened. Basically, even when you are careful, an outage can still happen: several causes combined with a very specific bug happening with specific versions in a specific case and voilà. Nice talk, and nice to hear a bank being so open!\n\nKeynote: Prometheus 2.0 – The Next Scale of Cloud Native Monitoring - Fabian Reinartz, Software Engineer, Google\n\nPrometheus is the monitoring stack everyone seems to be using now. This keynote presented how much faster Prometheus 2.x is, compared to Prometheus 1.x. Having never used the 1.x versions, I have to admit I never suffered from it. It is still nice noting 2.x scales much better (requires less RAM/CPU and its performances don’t degrade much with a huge number of metrics).\n\nWelcome reception\n\nThis first day ended with a nice buffet at Bella Center, next to the sponsor booths. As we each one went to see different talks, it allowed us to chat about what we saw and heard, even if we didn’t stick around too long, after such a long day – especially knowing there would be two more just after!\n\n\n"
} ,
  
  {
    "title"    : "PHP Tour Montpellier 2018",
    "category" : "",
    "tags"     : " phptour, php, afup, 2018",
    "url"      : "/2018/05/17/retour-php-tour-2018.html",
    "date"     : "May 17, 2018",
    "excerpt"  : "Cette année encore, M6Web a sponsorisé le PHP Tour, organisé cette année par l’AFUP à Montpellier.\nNous étions donc nombreux pour assister à l’ensemble des conférences. Comme d’habitude avec l’AFUP, les conférences étaient de bonne qualité, et il ...",
  "content"  : "Cette année encore, M6Web a sponsorisé le PHP Tour, organisé cette année par l’AFUP à Montpellier.\nNous étions donc nombreux pour assister à l’ensemble des conférences. Comme d’habitude avec l’AFUP, les conférences étaient de bonne qualité, et il y en avait pour tous : débutants comme utilisateurs avancés.\n\nPour la première fois, les conférences étaient données dans un cinéma Gaumont. Un très bon choix en termes de configuration : visibilité, confort, son et lumière !\n\nEn attendant la mise en ligne des vidéos, nous remercions les conférencières et les conférenciers pour leurs présentations. Vous trouverez ci-dessous quelques mots sur les conférences que nous avons particulièrement appréciées.\n\n“Tirer le maximum du moteur PHP7”\n\nConférence donnée par Nicolas Grekas.\n\nL’approche de Nicolas était très intéressante et nous à permis de mieux comprendre le fonctionnement interne de Symfony, en lien avec les optimisations apportées par le nouveau moteur de PHP7.\n\nLes exemples cités nous ont permis de voir qu’avec quelques “tips”, il est possible de “bypasser” des étapes coûteuses lors de l’exécution de notre code.\n\n“100% asynchrone - 0% callback en PHP”\n\nUne présentation de Joel Wurtz.\n\nCette conférence nous a permis d’aborder un sujet assez peu connu dans l’univers PHP : l’asynchrone.\n\nDès qu’un projet commence à être complexe, il est souvent possible de réaliser des tâches en parallèle, non bloquantes, permettant d’optimiser les temps de réponse.\n\nPour répondre à ce besoin, Joel nous a présenté le concept de l’asynchrone : l&#39;event loop.\nVia cette boucle, Joel nous a expliqué comment les évènements sont “dispatchés” au travers de générateurs.\n\nPour aller plus loin, Joel nous a aussi parlé des outils existants qui implémentent cette logique d’event loop : AMP.\n\nEnfin pour terminer, pour être 0% callback, Joel nous a présenté Fiber. Cette extension implémente la RFC Fiber actuellement en cours d’homologation.\n\nNous vous recommandons de creuser ce sujet, qui selons nous, ouvre de belles perspectives dans l’univers PHP !\n\n“Bienvenue dans la matrice !”\n\nCette conférence était animée par Benoit Jacquemont.\n\nAujourd’hui encore, les développeurs ont trop peu de connaissance sur ce qu’il se passe à bas niveau sur nos serveurs.\n\nCette conférence, qui présentait notamment strace (pour suivre les appels système) et ltrace (pour suivre les appels aux fonctions de bibliothèques), était donc particulièrement rafraîchissante. La démo “comment voir les requêtes et réponse en HTTPS, en clair”, était complètement bluffante !\n\n“Sans documentation, la fonctionnalité n’existe pas !”\n\nCe talk était proposé par Sarah Haïm-Lubczanski.\n\nTout le monde, dans sa vie de développeur, a été confronté au problème suivant : écrire la documentation des fonctionnalités développées. \nC’est un challenge auquel nous nous sommes nous-même confrontés lorsque nous avons travaillé, l’année dernière, sur l’internationalisation de notre plate-forme, puisque nous avons dû documenter nos API, désormais appelées par des collègues basés dans d’autres pays.\n\nSarah nous a montré comment faire face à cette barrière souvent perçue comme insurmontable par bon nombre d’entre nous.\nElle nous a pour cela donné les clés et les bonnes pratiques pour créer, maintenir et rédiger une documentation cohérente.\n\nOn retiendra aussi la présentation des différents outils open source de gestion de documentation.\n\n“A la découverte du Workflow”\n\nConférence animée par Gregoire Pineau.\n\nCette conférence a retenu notre attention. Particulièrement bien faite, elle résume les fonctionnalités de ce nouveau composant de Symfony, en partant d’un workflow simple jusqu’au réseau de Pétri. Grégoire donnait des exemples d’utilisations concrètes.\n\nDifficile à résumer, je vous invite à consulter la documentation Symfony sur ce composant.\n\nMais encore ?\n\nDe plus, trois conférences ont attiré notre attention de par leur valeur pédagogique. Elles étaient à nos yeux particulièrement  intéressantes pour des débutants ou des personnes ne connaissant pas encore le fonctionnement de certains processus suivis par notre communauté.\n\nDans l’ordre, vous trouverez :\n\n\n  IT figures par Sara Golemon, qui revient sur ce qu’est le FIG, organisme important qui régit aujourd’hui une partie de l’organisation de la communauté PHP, et sur ce que sont les PSRs.\n  Nommer les choses ? Oui : avec le DNS par Julien Pauli. Cette conférence revient sur les bases du fonctionnement du DNS et son utilité.\n  Et, pour finir : Caching with PSRs par Hannes Van De Vreken. Dernière des conférences “à voir une fois”, celle-ci revient sur ce qu’est le cache en général, pourquoi on en utilise. Puis s’intéresse au cache applicatif via les PSRs.\n\n\nLe dernier PHP Tour\n\nCe PHP Tour était le dernier, puisque l’AFUP proposera à partir de 2019 un nouveau format pour les événements en région : l’AFUP Day. Nous aurons grand plaisir à vous y rencontrer à nouveau, à Lyon cette fois-ci !\n\nEncore un grand merci à l’AFUP !\n\nEnfin, retrouvez toute l’actualité de l’événement sur #phptour.\n"
} ,
  
  {
    "title"    : "How a fullscreen video mode ended up implementing React Native Portals?",
    "category" : "6play",
    "tags"     : " React, ReactNative, mobile",
    "url"      : "/6play/2018/04/15/how-a-fullscreen-video-mode-ended-up-implementing-react-native-portals.html",
    "date"     : "April 15, 2018",
    "excerpt"  : "This story introduces a declarative native side portal implementation module called rn-reparentable.\n\nMy teammate (Laetitia BONANNI) and I are working on a React Native module embedded in the 6Play application that aims to provide best moments of ...",
  "content"  : "This story introduces a declarative native side portal implementation module called rn-reparentable.\n\nMy teammate (Laetitia BONANNI) and I are working on a React Native module embedded in the 6Play application that aims to provide best moments of different TV shows from the M6 channel.\n\nThe module, called Refresh, is a list of videos that are playing while the user is scrolling. It also provides a “theater mode” which is a way to create an immersive user experience by obscuring the cards that are not focused:\n\n\n\nAs any other video application, it provides a fullscreen experience to the user by rotating the device.\n\nAt the time I’m writing this article, creating such a thing using React Native is a pain. Here’s the story why.\n\nCreating a fullscreen, the web developer way\n\nWhile being a web developer, we usually work with positioning to display something over the rest (like a popup for example).\n\nDealing with React Native and its style APIs (which really looks like the web one), we thought that it would be super-easy to simulate the exact same behaviour.\n\nThat’s why our main idea was to manage the fullscreen mode by adding a style that takes the screen size and an absolute position. Thus, the video would have followed the device edges while rotating:\n\n\n\nReact Native styles are not the same as the web ones\n\nThe idea of creating an almost equivalent API as the web one is really good for the learning curve of React Native. It is a real asset when you want to create simple user interfaces.\n\nBut there is a drawback. This approach makes us want to get the exact same result as we would have on the web.\n\nIn our case, the use of absolute positioning was sadly not working. In fact, it is written in the React Native documentation:\n\nPosition\n\nposition in React Native is similar to regular CSS, but everything is set to relative by default, so absolute positioning is always just relative to the parent.\n\nIf you want to position a child using specific numbers of logical pixels relative to its parent, set the child to have absolute position.\n\nIf you want to position a child relative to something that is not its parent, just don’t use styles for that. Use the component tree.\n\nSee https://github.com/facebook/yoga for more details on how position differs between React Native and CSS.\n\n\nThe fact that an element is always positioned relatively to its parent has been a problem for us since our player component is part of the list.\n\nOverflow and android are not friends\n\nAnd even if we would have found a way (we could have cheated by calculating negative values, relative to the parent, in order to stick to the edges of the device), we would have met other problems such as the fact the equivalent of overflow prop doesn’t work on Android.\n\nThere are actually multiple opened issues concerning this problem. Grabbou gave a shot on this one #7229:\n\n\n\nLet’s make a fullscreen, the native way\n\nHopefully, we are working with native developers, from both platforms. We have shared a lot of information and finally have found a solution.\n\nThis time, while rotating the device, we would have hidden everything around the VideoPlayer component. No more headers, no more footers, nothing except the player. Then, we would have set the player size so that it matches the device size:\n\n\n\nHere’s the result we have got:\n\n\n\nWhat is happening on here?\n\nThere are multiple interesting things here, at ~200 cards down:\n\n\n  \n    Special visual effects are appearing (gray and blue background color)\n  \n  \n    The list scrolls too high and then refocuses\n  \n  \n    The video restarts\n  \n\n\nExplanations\n\nThe first thing to know is that we only keep 5 players alive (2 above, the focused one, and 2 below) and otherwise we display images. It’s important because of memory. Without this limitation, the application would have thrown some OutOfMemory errors (we met this kind of problems with Bitmap objects).\n\nThe second thing to notice is that we are always playing the video that is the most centered on the screen.\n\nThe last thing to know is that we actually have multiple rendering cycles to hide the different components around the VideoPlayer.\n\nFor now, with that information, let’s imagine the following scenario:\n\n\n  \n    Scroll ~200 cards down\n  \n  \n    The most centered video is now playing\n  \n  \n    Rotate the device\n  \n  \n    It resizes all the images / VideoPlayer to match the device size\n  \n  \n    It removes ~200 headers + ~200 footers\n  \n\n\nDuring the 5th step, the list is scrolling up, because it has earned some space with the headers and footers disappearing. This creates the strange behaviour of “yo-yo” list scrolling. Moreover, when the list is scrolling, the application finds a new “most centered card”, and creates the associated player. If the previous player is not part of the 5 new conserved ones, it’s destroyed. Thus, the further we scroll in the list, the worse it becomes.\n\nThe combination of the 4th and 5th step creates the actual gray / blue screen in background.\n\nFor now, we have a quasi-functional solution. It’s not really user friendly but we have something close to work. The key point here is that improving the functional solution (avoid the “yo-yo” effect) would also give a better user experience.\n\nSo, how can we avoid this “yo-yo” behaviour ?\n\nPortal to the rescue\n\nRecently, we heard about React portals. It seems that it could have saved us from this specific situation. The idea is quite simple, we would have teleported the player from its current location to somewhere higher in the component tree, like the React Native documentation encourages us to, without triggering special state based rendering-cycles (aka: Headers + Footers removals):\n\n\n\nThe problem is that React Native doesn’t support them natively: portals are part of ReactDOM, not React itself. We can’t use it in our application.\n\nWe’ve found and experienced some great open source alternatives on the JavaScript side such as react-gateway and we even managed to create our own one for this specific case.\n\nThe problem is that React would have created a new instance of the VideoPlayer each time we would have moved it, instead of keeping the old one. It means that we would have created 2 VideoPlayer, and lost both context.\n\nEach time we rotate the device, the video will restart from the beginning.\n\nWhat can we do with portal ? On the native side?\n\nThe portal idea is quite interesting: we need to find a way to create a portal-like behaviour with React Native, but on the native side, so that we won’t lose the VideoPlayer native context.\n\nSince we had the chance to be at the React Native Europe, we have learnt the way React Native is managing views thanks to Emile Sjolander.\n\nTo demonstrate this idea, let’s take an example :\n\n\n\nThis is a simple application which provides two  components and displays some content. On the right, we can see the native tree view. The cursor shows the two native views that need to permute. The idea is to make First taking place of Second and vice versa.\n\nIt’s possible, using React Native, to use the module responsible of view management: UIManager (available directly from react-native module):\n\n componentDidMount() {\n    setTimeout(() =&amp;gt; {\n      // Permute child at indice 0 and 1 of parent tag 6\n      UIManager.manageChildren(6, [0], [1], [], [], []);\n    }, 3000);\n  }\n\n\nThis will end up making something like:\n\n\n\nIt seems that creating a portal-like behaviour is possible using ReactNative.\n\nThe main reason we didn’t choose this solution is the fact that we didn’t find a way to get the UIView native identifier from the JavaScript side (I’m not talking about nativeID or testID props, but the unique identifier of the view set on the native side).\n\nHere’s a tweet from me concerning unique identifier\n\nNative implementation of “portals”\n\nWe finally decided to implement a React Native native component called  that is able to move View children from a parent view to another one using a declarative API.\n\nUsing this approach, we gain more control over what we would like to do leveraging native side power.\n\nReparentable owns two props :\n\n\n  \n    name that represents the destination of the teleportation\n  \n  \n    target that represents the name of the target\n  \n\n\n&amp;lt;View style={styles.container}&amp;gt;\n  &amp;lt;Reparentable name=&quot;1&quot; target=&quot;&quot;&amp;gt;\n    &amp;lt;Text&amp;gt;First&amp;lt;/Text&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n\n  &amp;lt;Reparentable name=&quot;2&quot; target={this.state.shouldGo ? &quot;1&quot; : &quot;goNowhere&quot;}&amp;gt;\n    &amp;lt;Text&amp;gt;Second&amp;lt;/Text&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n&amp;lt;/View&amp;gt;\n\n\nOn this gist, &amp;lt;Reparentable name=”2” …/&amp;gt; will take place of &amp;lt;Reparentable name=”1” …/&amp;gt; when the state shouldGo will change.\n\nWhat does it mean?\n\nIn our context, it means that when the state isFullscreen is true, we are able to move the player from its current view to the higher one:\n\n&amp;lt;View style={styles.container}&amp;gt;\n  &amp;lt;Reparentable name=&quot;fullscreenView&quot; target=&quot;&quot;&amp;gt;\n    &amp;lt;FullScreenContainer /&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n\n  &amp;lt;Reparentable\n    name=&quot;videoPlayerId&quot;\n    target={this.state.isFullscreen ? &quot;fullscreenView&quot; : &quot;&quot;}\n  &amp;gt;\n    &amp;lt;VideoPlayer /&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n&amp;lt;/View&amp;gt;\n\n\nHere’s the result we’ve got:\n\n\n\nComparing both variants\n\n\n\nIt took us time to get this result, but we finally have something that meets our needs.\n\nLink to the library : https://github.com/mfrachet/rn-reparentable\n\nThanks for reading,\n"
} ,
  
  {
    "title"    : "The 6play platform goes international",
    "category" : "6play",
    "tags"     : " 6play, intl",
    "url"      : "/6play/2018/03/26/6play-goes-international.html",
    "date"     : "March 26, 2018",
    "excerpt"  : "Within less than a year m6web and techm6web managed to launch websites, android apps and ios apps for three RTL TV channels in Europe, all based on the 6play technology.\n\nThese deployments include:\n\n\n  broadcasted and themathic channels,\n  live st...",
  "content"  : "Within less than a year m6web and techm6web managed to launch websites, android apps and ios apps for three RTL TV channels in Europe, all based on the 6play technology.\n\nThese deployments include:\n\n\n  broadcasted and themathic channels,\n  live streaming, video catchup encoding and playout,\n  video resuming,\n  local adservers (for videos and display), DMP system and CDN,\n  contribution via our backoffice and automated through our API,\n  translations system,\n  and almost all the features of the 6play platform.\n\n\nRTL Play (Belgium)\n\n\n\nhttps://www.rtlplay.be/\n\nRTL Play (Croatia)\n\n\n\nhttps://play.rtl.hr/\n\nRTL Most (Hungary)\n\n\n\nhttps://www.rtlmost.hu/\n"
} ,
  
  {
    "title"    : "Useful (or not) M6Web OSS stuff",
    "category" : "OSS",
    "tags"     : " OSS, Open source, php, js",
    "url"      : "/oss/2018/03/20/useful-or-not-usefull-m6web-stuff.html",
    "date"     : "March 20, 2018",
    "excerpt"  : "At M6Web we do love open source and we are trying to be good open-source dev citizens! Here is a short presentation on our most interesting contributions:\n\n\n\nYou can find all our open source contributions on their dedicated page on our tech blog. ...",
  "content"  : "At M6Web we do love open source and we are trying to be good open-source dev citizens! Here is a short presentation on our most interesting contributions:\n\n\n\nYou can find all our open source contributions on their dedicated page on our tech blog. Enjoy !\n"
} ,
  
  {
    "title"    : "Migration to Spark 2.2",
    "category" : "",
    "tags"     : " Data, Hadoop, BigData, Airflow, Hive, Spark, Java",
    "url"      : "/2017/12/13/spark-2.html",
    "date"     : "December 13, 2017",
    "excerpt"  : "To value our data in order to understand better our service and improve it, we use Spark. You can find more information in a recent article about our datalake. We recently migrated our biggest project from Spark 1.5 to Spark 2.2 and wanted to shar...",
  "content"  : "To value our data in order to understand better our service and improve it, we use Spark. You can find more information in a recent article about our datalake. We recently migrated our biggest project from Spark 1.5 to Spark 2.2 and wanted to share that story.\n\nSpark 2 has been released a year ago (July 26, 2016). Maybe we are a bit late, but better late than never.\n\nWe are working with an official version from Cloudera with Spark 1.6 as the default version.\n\nOur project runs everyday to get data from different sources and send them to different destinations.\n\nIt is built with Java and Spark 1.5, but we encountered several problems with those technologies. First of all, the Java + Spark community is smaller than the ones for Python or Scala. Secondly, the Spark 1.5 community is also smaller than the one of version 2.2.\n\nThat sometimes made information hard to find.\n\nBut most of all, we did not succeed to integrate new components that work with more recent versions.\n\nWe wanted to migrate for bugs fixes in general and in a performance purpose too.\n\nI) Workflow\n\n\n\na) Spark 1.5 to Spark 1.6\n\nFirst, we had decided to migrate to 1.6 to do a progressive migration. But we bumped into a bug with a UDF. We had difficulties fixing it, and it was resolved in 2.2.\n\nWe finally decided to migrate directly to 2.2.\n\nb) First validation with unit tests\n\nWe did the migration and ran our unit tests to see and fix the problems.\n\nc) Functional tests\n\nThen, we ran our jobs with some data sets. The idea was to check the differences with Spark 1.5.\n\nWe wanted to be sure that our unit parts were working together.\n\nd) Double run\n\nThen, we set out for a double run. It means that we had our jobs running both with Spark 1.5 and Spark 2.2 and we compared the outputs each day.\n\nWe used Airflow to deal with that. If you know Airflow, you will understand that we added a new DAG to run our project with Spark 2.2.\n\nThe idea was to see the potential differences between the two on a daily basis.\n\nAt the end, we merged our branch into master.\n\n2) Changes to migrate to 2.2\n\nThere are different changes from Spark 1.5 to 1.6 to 2.2. You will find them described in the documentation.\n\nThe idea here is to focus on the problems we met, the noticeable changes for us and how we dealt with them.\n\na) Dataset\n\nOf course, the main change is that “dataFrame” does not exist anymore. You must replace it by “Dataset&amp;lt;Row&amp;gt;”.\n\nActually, “DataFrame” and “Dataset” were unified with Spark 2.0. In reality, for untyped API like Python, “DataFrame” still exists. But, we work with Java.\n\n\n\nUsing “Dataset&amp;lt;T&amp;gt;” is a way to apply a schema at the compilation. If there is a problem, you will get a logical exception. Before, with “DataFrame”, you could only have runtime exceptions.\n\nAs a first step, we replaced “DataFrame” by “Dataset&amp;lt;Row&amp;gt;”\n\nb) SparkSession\n\nA second major difference is “SparkSession”. It is the new entry to Spark. \nThere is no need anymore to create a “SparkConf”, a “SparkContext” and a “SQLContext”. It is possible to get all of it just with a “SparkSession”.\n\nBut, it is important to understand that if you just want to migrate your code in a first step to get it work with Spark 2, it is not a need to use “SparkSession”. “SparkConf”, “SparkContext” and “SQLContext” still work.\n\nThat is what we decided to do.\n\nc) Iterable to Iterator\n\nThe return type “Iterable” is incompatible with “PairFlatMapFunction”. We had to replace “Iterable&amp;lt;&amp;gt;” with “Iterator&amp;lt;&amp;gt;”.\n\nWe replaced code like that:\n\npublic Iterable&amp;lt;String&amp;gt; call(String s) throws Exception {\n    ...\n    return list;\n}\n\n\nby something like that:\n\npublic Iterator&amp;lt;String&amp;gt; call(String s) throws Exception {\n    ...\n    return list.iterator();\n}\n\n\nd) Creating a UDF using hiveContext is not possible anymore the same way\nBefore, you could do something like that :\nhiveContext.sql(&quot;CREATE TEMPORARY FUNCTION function AS ...&quot;)\n\n\nBut now, you have to enable hive support first. You must do it with the SparkSession:\n\nSparkSession spark = SparkSession\n    .builder()\n    .appName(&quot;Java Spark Hive Example&quot;)\n    .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)\n    .enableHiveSupport()\n    .getOrCreate();\n\n\nIf “enableHiveSupport” is not enabled, there is an error like this :\n\njava.lang.UnsupportedOperationException: Use sqlContext.udf.register(...) instead.\n\n\nWe decided not to use “SparkSession” in a first step and to follow the error instructions.\n\nWe replaced our direct call to Hive by a UDF registration.\n\ne) Deprecations\n\nWe noticed some deprecations like HiveContext or Accumulators for instance. But we decided not to deal with them for the moment.\n\nd) Performance\nWe have made some gains in performance. \nBefore, running our jobs lasted around three hours. Now, it lasts around two and a half hours.\n\nWe hope we will make some other gains by migrating to the Spark 2.2 philosophy (“SparkSession”, etc).\n\nConclusion\nAs there are many backward compatibilities with Spark 2, it is not so difficult to make a first migration to make your project work. Nonetheless, it could be long to validate. It depends on your tests stategy too.\n\nOur next step now will be to integrate the new philosophy of Spark 2.2 to get the best of the new version.\n\nNastasia Saby (Zenika consultant)\n"
} ,
  
  {
    "title"    : "Forum PHP AFUP 2017",
    "category" : "",
    "tags"     : " afup, php",
    "url"      : "/2017/11/07/forum-php-2017.html",
    "date"     : "November 7, 2017",
    "excerpt"  : "M6Web était sponsor de cette édition du Forum PHP organisée par l’AFUP et une grande partie de l’équipe backend avait fait le déplacement. \nCe forum était vraiment inédit de par sa taille sans précédent : plus de 650 participants ! Il a été aussi ...",
  "content"  : "M6Web était sponsor de cette édition du Forum PHP organisée par l’AFUP et une grande partie de l’équipe backend avait fait le déplacement. \nCe forum était vraiment inédit de par sa taille sans précédent : plus de 650 participants ! Il a été aussi pour l’équipe l’occasion de voir des présentations de grande qualité et très inspirantes. (sans compter celle de nos collègues Fabien et Nastasia sur l’AB testing).\n\n\n\nDe très nombreux retours exhaustifs sont disponibles sur le web et je pense que les vidéos seront rapidement en ligne sur la page listant tous les talks organisés par l’AFUP. On peut noter, comme à chaque Forum, les tendances qui se dégagent de l’ensemble des talks et suite aux discussions endiablées qui suivent les présentations :\n\n  DDD commence à être présent dans tous les talks type méthodo,\n  GraphQL fait parler de lui, et c’est tant mieux,\n  des reality checks sur les modes de ces dernières années (comme les micro services, la qualité au sens large), mais une maturation et un recul sur des pratiques modernes qui font plaisir à voir,\n  des considérations très intéressantes sur la gestion du code source (refactoring, clean code, nommer les choses ;) …).\n\n\nEnfin, j’ai vraiment (en tant que lyonnais) apprécié la régionalisation de l’AFUP, avec une multitude d’antennes locales créees ces dernières années.\n\nBravo l’ @afup pour la régionalisation des badges au #ForumPHP ! pic.twitter.com/ekewUkCoKS&amp;mdash; Olivier Mansour (@omansour) 26 octobre 2017\n\n\nUn excellent cru que M6Web était ravi de soutenir ! Et rendez-vous au PHP Tour !\n"
} ,
  
  {
    "title"    : "Genesis of M6&#39;s Datalake",
    "category" : "Data",
    "tags"     : " Data, Hadoop, BigData, Airflow, Hive, Spark, DMP",
    "url"      : "/data/2017/10/23/genesis-of-m6-datalake.html",
    "date"     : "October 23, 2017",
    "excerpt"  : "At M6 we work hard to leverage Data to help our ad sales team, our CRM teams and our product innovation process.\nOver the past 2 years, we have gone from using a market DMP to creating our second Hadoop platform. We now feel that our stack is stab...",
  "content"  : "At M6 we work hard to leverage Data to help our ad sales team, our CRM teams and our product innovation process.\nOver the past 2 years, we have gone from using a market DMP to creating our second Hadoop platform. We now feel that our stack is stable, reliable and scalable so it feels like the right time to share our experience with the community.\n\n\n\nStep 1: embracing the DMP\n\nOur first use case was to monetize data through targeted publicity.\nWe decided to start by installing a DMP (Data Management Platform) because it was a very fast solution to deliver our major needs, in particular :\n\n\n  Collect data from all our services and combine it with our user\nknowledge =&amp;gt; DMPs offer that off the shelf\n  Create segments for audience targeting =&amp;gt; The segmentation approach offered by DMPs was well adapted to the ads market\n  Activate our Data, both in house via our adservers and in the outer market =&amp;gt; DMPs generally offer simple integration with most adservers, and a very straight forward third party integration\n\n\nThe match seemed quite obvious and there’s a good reason for that: DMPs are designed for this use case above all others.\n\nWe chose Krux (now Salesforce) and deployed it over our ~30 sites and applications. Installing Krux on our network and plugging it to our video and display adservers ended up taking a few months and a decent effort. Convincing all our teams that the increase in ad revenue would make it worth the development time and the negative impact on webperf wasn’t trivial, but got through thanks to our top management sponsoring. Once on the job, the deployment was quite smooth on the web and mobile apps, but validating the quality of the ingested data turned out to be an endless project.\n\nAt the end of the day, Krux’s DMP did the job. In November 2015 we launched Smart6tem, our Data platform &amp;amp; advertisement offer based on segments (announcement here, articles here here or here). This move had a very positive effect on our advertisement market, and allowed to start making Data mean something at M6.\nTo give some detail of our use of the DMP, it turned out building our own segments was very successful, but we didn’t use any 3rd party interconnection because we didn’t find any valuable Data to buy and didn’t want to reduce the value of our own Data by sharing it out.\n\n\n\nKrux’s segment builder\n\nOnce the Advertisement use case was out in the market, we moved our efforts towards leveraging the DMP for our CRM teams. The rationale was simple: targeted emails are more efficient than newsletters. We were hoping to reduce the email pressure on our users while increasing the performance both for revenue and traffic.\n\nStep 2: first round testing Hadoop\n\nHaving a DMP is both a great accomplishment and frustrating.\nIt’s great because you can start to combine the use of your service with the user profiles to produce segments and activate use cases to address them.\nBut for CRM, the workflows to plug segments into our emailing systems weren’t native and we needed to build some custom workflows. No rocket science, but when we first received a 2 Billion line file for the user/segment map that we needed to filter and convert into another format, our developers went grumpy.\nWe also got frustrated very fast because we wanted to start to extract some unpreceded analytics insights combining our user knowledge (our major service, 6play had just switched to fully logged-in users) with usage stats or with external sources like our adserver logs. Advanced analytics was clearly not the field of Krux.\nLast but not least came some limitations (either due to the design of Krux or the pricing):\n\n\n  We could only work on 3 months of history if we wanted to keep the price reasonable (on 6play we have a lot of TV shows that run for 3 months per year, and segmenting the users who watched the show last year is important).\n  It’s based on cookies + device ids on mobile (it’s the best solution for most use cases, but if your users are logged in, it introduces quite a lot of risk to make mistakes).\n  We never managed to convince our users that the amount of cookies or users inside segments was correct. Every single study we made on this point led to doubt, and our DMP support team never came up with serious answers.\n\n\nAt this point, Hadoop came in as an evidence, so we created our first cluster.\nThe process of creating this proof of concept cluster was pretty much a black box for us since we charged a partner with the job. We ended up with the following setup, all hosted by AWS :\n\n\n  2 name nodes with 16 VCPUs and 30G RAM each\n  4 data nodes adding up to 64 vcpu’s and 120G RAM\n  Cloudera Enterprise with Hive, Impala, Hue, Python, R and a kinky crontab\n  Tableau Desktop + Tableau Server\n\n\nNothing crazy but that brought us into the world of Hadoop, and that was a major move. We also staffed our first Data Scientist to start to explore our Data and imagine use cases.\n\nOur first steps in Hadoop were hesitant, but within a few months, we had created our first Data Lake, our targeted CRM was live and we had produced a few dozen dashboards providing unpreceded insights throughout the company. From the business perspective, it was a success.\nFor the people who got their hands on a Data Lake for the first time the experience was ground breaking. For the first time, we could connect information from half a dozen different tools seamlessly.\nAn example: finding how many ads were seen by women from 25 to 49 years old during the NCIS TV show.\nBefore the Data Lake, this would have been impossible. The closest we could get would take the following process :\n\n\n  Extract the amount of ads viewed on NCIS from our adserver stats to a text file\n  Extract the NCIS traffic from our video consumption tracking tool (in Cassandra) to a text file\n  Extract our users Database with age and gender (in a third party tool named Gigya) to a text file\n  Load all this up into an Excel spreadsheet\n  Write a bunch of Excel formulas to produce the percentage of the traffic on NCIS that’s generated by women between 25 and 49 years old\n  Apply that percentage to the adserver stats\n\n\nAs you can see, combining information between our ecosystems involved some very manual processes and could only lead to approximations, so basically we never did them.\n\nWith our Data in Hadoop, all this turns out to be a simple SQL query in Hue (a PhpMyAdmin style interface for Hadoop):\n\nSELECT COUNT(*) FROM adserver_logs A\n   JOIN users U ON A.uid = U.uid\n   JOIN programs P on A.pid = P.id\nWHERE A.type = &#39;impression&#39;\n   AND U.age &amp;gt;= &#39;25&#39;\n   AND U.age &amp;lt;= &#39;49&#39;\n   AND U.gender = &#39;F&#39;\n   AND P.name = &#39;NCIS&#39;\n\n\nHadoop and our Data Lake, we could just jump over the barriers between tools and ecosystems within seconds. Combined with the ability to code in various languages, we could instantly start to industrialize such insights and start going further.\n\nWe convinced our top management very fast about the value of having our own Hadoop cluster, and since it was very (VERY) expensive, we decided to internalize it.\n\nStep 3: building our internal Hadoop cluster\n\nSo there we were with a quite simple roadmap: replace our v1 Hadoop cluster to reduce costs and improve performance as much as possible. We managed to divide the price by 3 while multiplying the resources by 8.\n\nThe first step on this road was to staff a tech team to design and create our platform. That ended up being very tricky and finally took us 10 months to complete.\n\nOnce the team was staffed, we got onto the job. We had 5 steps :\n\n\n  Choose the hosting platform (4 months)\n  Choose the hardware\n  Choose the software stack (2 months, done in parallel)\n  Set up the cluster (2 months)\n  Migrate all our projects to the new platform (3 months)\n  Check to be sure everything was done (1 month)\n\n\na) Hosting platform\n\nThis stage of the project was a very religious one. Many people at M6 had a very strong desire to go towards cloud and managed services, others were totally in favor of Hadoop and have full in house control over the platform. The major options were:\n\n\n  AWS\n    \n      Amazon EMR + S3\n      Amazon EC2 + S3\n    \n  \n  Google Cloud Platform\n    \n      Managed services (Dataflow, BigQuery, Compute engine, Pub/Sub…) + Cloud storage\n      Dataproc + Cloud storage\n    \n  \n  On premise\n    \n      Add servers to our 6play platform at Equinix\n      Work with our hosting subsidiary, Odiso\n    \n  \n\n\nWe spent 3 months talking to the different vendors and considering options.\n\nThe first decision we took was to use Hadoop instead of managed services.\nThe AWS and Google sales teams were very convincing, but we finally declined for 2 main reasons:\n\n\n  People in our company were starting to learn how to use Hadoop, changing the stack would have forced everyone to re-learn what they were just starting to dominate. Not very efficient while building up expertise.\n  Using proprietary solutions like Big Query involves a strong locking risk. If we developped all our projets to leverage a specific platform, changing providers in a few years would involve a lot of reworking on all our code base.\n\n\nThe next step was to choose between the 3 hosting options. On a side note, we compared the price for x4 and x10 resources compared to our v1 platform.\nAt the end of the process we wrote up an evaluation grid. Here is the summary version.\n\n\n\nThe decision was there, we went for a fully on premise stack with Odiso.\nTo detail some of that evaluation, here’s a few insights on what it came down to.\n\n\n  AWS is cool, but ultra expensive. I mean it’s 10 times more than our on premise option! We would have gone full AWS if the price was reasonable. The possibility to pop clusters up and down is very interesting and reduces costs, but our v1 platform was using our 4 EC2 Data Nodes at ~80% 24/7, so we could never go down to 0 servers.\n  Google feels better on the service side of things, but it involved taking chances because the commercial product is young and support + community experience seemed weak.\n  On premise was clearly much cheaper, and felt more secure for our low experience on Hadoop since we’re used to managing servers and our team had managed serious Hadoop before.\n\n\nb) Hardware\n\nGoing on premise means buying physical servers and building them.\nOur goal here was to massively upgrade our current platform to scale with the company’s usage of Big Data. Since the price was very reasonable, we settled down to x8 on CPU, RAM and storage compared to our initial Hadoop cluster. Here’s the stack we bought:\n\n4 KVM servers:\n\n\n  DELL PowerEdge R630\n  OS Disks: 2x 400GB ssd, RAID 1\n  Data disks: 8 * 2To\n  RAM: 256Go (8*32G)\n  CPU: 2x12 Cores (3.0Ghz)\n\n\n15 Data Nodes:\n\n\n  DELL PowerEdge R630\n  OS: 2x 400GB ssd, RAID 1\n  Data disks: 8 * 2To\n  RAM: 384Go (24*16G)\n  CPU: 2x12 Cores (3.0Ghz)\n\n\nBuilding and racking the servers was quite straight forward, there’s nothing special about Hadoop in this process except the high quality network connectivity.\n\nc) Software stack\n\nDesigning the software stack was very straight forward.\nWe had the desire to stay as close as possible to the stack our users were getting used to, and it was pretty much a standard Cloudera stack. That suited us very well because our first priority was to avoid any regression, both for the projects (during this period, they had massively multiplied as we’ll detail in the migration part below) and for the users.\nAnother early choice was to use virtual machines with Proxmox and not dive into the Kubernetes + Docker adventure. Although that was tempting and will probably be an option in future, we considered mastering the Hadoop stack was enough on our plate for the moment, we needed to reduce risk.\n\nHere’s the stack we chose:\n\n\n  Puppet\n  Centos 7\n  Proxmox\n  Cloudera Hadoop 5.11 (free version)\n  Hadoop 2.6\n  Hive 1.1\n  Spark 1.6 and 2.1 (we had 1.6 before but our Data Scientists really wanted to use new features)\n  Supervisord\n  MariaDB\n  LDAP\n  Ansible\n  OpenVPN\n  Python 2.7 and 3.6 with Anaconda\n  Java 8\n  R 3.3\n  Scala\n  Airflow 1.8 (this is out of the Cloudera stack, an important and epic part of our toolkit that we’ll surely talk about in more detail in a future post)\n  Sqoop\n  Hue 3 with Hive on Spark as default\n  Tableau Desktop + Tableau Online\n  Jupyter\n\n\nd) Install Hadoop and all our tools\n\nOne of the fun parts of our design process was to choose a name for our new cluster. We called it Cerebro (in reference to X-Men and the global view of Professor Xavier), and created a logo :)\n\n\n\nSetting this stack up felt very simple from my perspective, but that’s surely because our awesome team overcame the issues silently.\nOn the timeline, the biggest part of the setup was receiving the physical servers. That took about 3 months because some parts (SSD disks) were out of stock for a long time.\nWe received a first part of the Data Nodes a couple of months before the rest of the servers, so we decided to start building the cluster with temporary Name Nodes and services, and migrate them after.\n\nWe deployed Cloudera Hadoop via KVM servers (managed with Puppet) and the Cloudera Manager. Very straightforward.\nWe used Ansible to install our stack, manage all our configuration files and user access.\n\ne) Migrate our projects and Data\n\nMigration was a project in the project.\nBetween the day we decided to build our internal platform and the day we delivered, 20 months had gone by. During all that time, Big Data had been going through high pace growth inside M6. We scaled from ~1 to ~25 users, from 0 to ~200 Dashboards and ~60 projects. All of this relying on our “Proof Of Concept” platform created with a partner.\nTo be honest, it was an utter mess in any Software Engineer’s eye. Imagine: no version control, a unique user hosting all the projects and executing 6000 crontab lines each day. No job optimisation whatsoever. Moreover, most of our users had no developpement process knowledge, so they didn’t see any problem with all this and weren’t all in favour of any change. The context was challenging.\n\nThe first step of our migration project was to bring all this back into a “migratable” state. To do that, we went through the following steps :\n\n\n  Put all the code base in Git\n  Create a code deployment process\n  Split the production jobs down to a 1 user per project approach, both for code execution and data storage\n  Make all paths to data relative\n  Switch from crontab to Airflow\n  Add backups on S3\n\n\nWe reached this milestone after 4 months of a large rework of all our projects by all our teams. The collective investment in this process was a real team success.\n\nThe second step was to rebuild all the projects and databases on the new platform.\nThanks to our new backup system that copied all our Data to S3, rebuilding databases was easy. Basically it took creating a script to restore the backup in the new platform, and we could start checking integrity by querying the datasets. Rebuilding projects was a similar process, we just had to deploy each project and it was ready to test. Everything went fast and easy, proving that all the preparation moves we made were very valuable.\n\nThe third step was to double run all our projects so we could be sure everything worked on the new platform while not breaking production.\nThere’s a tricky part to this because a fair amount of our projects include an output towards external servers (either other teams within M6 or 3rd parties). For this we had to add an “only run on” logic. That lead us to create a unified configuration and a library for exports.\nWe also had to distinguish all our code execution monitoring so we could keep an eye on what each workflow was producing, both in production and on the new platform. For this we added the platform name to all our Graphite nodes and updated all our dashboards to filter by platform.\nWith those 2 moves, most projects managed to run “out of the box”. Some needed some refactoring, mostly for parts that had been forgotten in the first step.\n\nThe fourth step was validating that our double run was working well.\nThe theory of this validation was quite elaborate. For each table or output job we would count the number of lines in each partition produced, run checksums, dive into the details of the monitoring, and run manual tests.\nIn practice, that part cracked up quite fast because our v1 platform was being totally outscaled and therefore all our users really didn’t want to look back. We checked that the backups were good with file sizes and line counts, and for the rest we relied on our monitoring to be sure that the jobs runned and produced the same output volumes. For the most critical production jobs we went into some detailed manual checking, but we took the jump very fast.\n\nThe fifth and last step was migrating all our Tableau Dashboards to Tableau online.\nWe needed all our ingestion and treatment jobs to be up and running before we could migrate our 200+ Dashboards. Once that was done, most dashboards took nothing more that being opened in Tableau Desktop and published to Tableau Online. The only exceptions were the bunch of users who had missed some tables out in step 1. Those had to run through the whole process at fast speed… Not very pleasant for them.\n\nSo there we are, we now have our 2 feet in our second Hadoop platform. Now we’re looking forwards, both on how we make this platform evolve to empower our future use cases, and to raise our innovation pace for Big Data to count much more within M6.\nBy all means stay posted, we’ll update you on some of the awesome projects we’ve been working on!\n\nTake away\n\n\n  Deciding to create an internal Hadoop platform took time and a few previous steps for our organisation to start to understand what Big Data was about and the way to go around it.\n  Choosing our hosting solution was hard and very conviction driven.\n  On premise hosting is cheaper than cloud solutions, but obviously less flexible.\n  No surprise for tech people and it’s valid way beyond Big Data, migrating projects developed without any engineering good practices was hard and risky work.\n\n\n"
} ,
  
  {
    "title"    : "Elasticsearch: la grande migration",
    "category" : "",
    "tags"     : " Elasticsearch, Php",
    "url"      : "/2017/06/01/migration-elasticsearch.html",
    "date"     : "June 1, 2017",
    "excerpt"  : "Pour assurer la scalabilité des performances de l’API 6play, les données suivent tout un workflow pour être dénormalisées et stockées dans Elasticsearch.\nMi-2016, nous avons identifié des dysfonctionnements majeurs sur nos serveurs, entrainant par...",
  "content"  : "Pour assurer la scalabilité des performances de l’API 6play, les données suivent tout un workflow pour être dénormalisées et stockées dans Elasticsearch.\nMi-2016, nous avons identifié des dysfonctionnements majeurs sur nos serveurs, entrainant parfois des interruptions de service.\nSuite à quelques mesures d’urgences pour stabiliser l’existant, nous avons entrepris de mettre à jour notre version d’Elasticsearch pour bénéficier des dernières améliorations.\nNous étions alors sur la version 1.7, et souhaitions passer en version 2.0.\nAprès plusieurs mois d’efforts pour effectuer cette migration sans interruption de service ni gel technique, nous voici en version… 5.2!\nVoici le récit de cette grande migration, et ce que l’on a appris tout au long de ce périple.\n\n\n\nLa théorie\n\nIl n’y a pas de méthode magique pour changer de cluster sans coupure, la stratégie adoptée est assez classique:\n\n\n  dupliquer les écritures sur le nouveau cluster\n  basculer les lectures sur le nouveau cluster\n  arrêter les écritures sur l’ancien cluster\n\n\nIl n’est pas nécessaire d’enchaîner toutes les étapes dans la même journée, cela présente donc l’avantage de pouvoir étaler les déploiements dans le temps en fonction des disponibilités,\nainsi que de surveiller attentivement le monitoring pendant quelques jours pour vérifier que l’infrastructure supporte bien les changements apportés.\n\nÉcritures en Y\n\nNous utilisons des workers Php pour détecter les changements dans notre BDD,\nsuite à quoi un message est publié dans une file d’attente pour être traité par un autre worker qui se chargera de synchroniser les entités entre MySQL et Elasticsearch.\nIl était primordial que le cluster de production ne soit pas impacté par les éventuelles erreurs rencontrées sur le nouveau cluster.\nUne de nos premières intentions était de publier le message de mise à jour dans une deuxième file d’attente, consommée par des workers dédiés eux aussi au nouveau cluster.\n\n\nLe gros inconvénient est que cela impliquait de doubler toutes les lectures sur la BDD, toutes les requêtes devaient être executées une fois par cluster,\ncela risquait donc d’impacter d’autres services.\nNous sommes donc partis sur une solution purement logicielle, puisque pour chaque entité mise à jour ce sont les workers qui les envoient sur chaque cluster.\n\n\nIl est par contre nécessaire de gérer correctement les erreurs, que faire si une erreur intervient sur un cluster mais pas l’autre?\nSi le nouveau cluster devient instable et que l’on renvoie les messages systématiquement dans la file d’attente, on risque d’accentuer inutilement la charge en écriture sur le cluster stable en production.\nNotre compromis est de définir comme master le cluster de production (celui où les données sont lues), et seules ses erreurs provoquent la génération d’un nouveau message.\nLes erreurs sur le cluster slave sont monitorées, mais ne génèrent pas de nouveaux messages dans la file d’attente.\nEffectivement, puisque chaque soir nous resynchronisons toutes les données entre MySql et Elasticsearch, on peut se permettre d’avoir des données moins fraiches sur le cluster slave le temps d’une journée.\n\nInitialement, nous pensions que ce système master/slave serait temporaire, mais très rapidement nous avons pérennisé ces développements, cela nous permettait de tester facilement différents clusters,\nou encore de vérifier que les données étaient bien indexées de la même manière, faire des rollbacks en urgence…\n\nUne dernière difficulté était de faire cohabiter deux version différentes du Sdk Elasticsearch Php dans le même projet.\nIl y a effectivement une incompatibilité entre les versions 2.* et 5.*, et nous n’avons pas eu d’autre choix que de cloner la librairie concernée et de changer tous les namespaces pour éviter les conflits de noms.\nMalgré tout, ce ne sont pas les écritures dans Elasticsearch qui nous ont posé le plus de problèmes.\n\nMigration des requêtes\n\nIl y a eu de nombreux changements apportés entre la version 1.7 et 5.0 et souvent pour le mieux.\nLes différentes évolutions de syntaxes ont généralement vite été faites, car nous avions pris soin d’encapsuler la construction des requêtes via quelques fonctions helper (une sorte de query builder).\nIl nous a donc suffit de changer ces quelques fonctions pour traduire les anciennes requêtes vers la nouvelle syntaxe.\n\nUne erreur classique que nous faisions en 1.7 était de se contenter d’un mapping par défaut, qui avait le mérite de fonctionner sans efforts avec nos requêtes et nos données.\nLors du passage à la version 5.0, Elasticsearch a commencé à refuser certaines de nos requêtes car elles ne pouvaient pas être performantes.\nIl fallait choisir, soit activer explicitement des options de mapping en faisant un compromis sur les performances générales,\nsoit affiner le mapping pour qu’il soit plus adapté à la nature de nos requêtes.\nIl s’agit d’un bon exemple de Leaky Abstraction,\non a beau utiliser des outils pour s’abstraire de la façon dont sont stockées les données, nous sommes toujours obligés de comprendre ce qu’il se passe à l’intérieur pour en tirer les meilleures performances.\nHeureusement pour nous notre mapping était assez trivial à changer, car nous n’utilisons pas Elasticsearch pour faire de la recherche full-text\nmais seulement pour des recherches exactes sur des identifiants, des codes… Il nous a généralement suffit d’utiliser le nouveau type keyword\npour que nos requêtes puissent être acceptées.\n\nPour s’assurer du bon fonctionnement des APIs suite à ces nombreux changements, nous avons investi du temps pour écrire des tests fonctionnels de bout en bout,\npour vérifier que les résultats restaient inchangés malgré le changement de version de cluster.\nBien sûr, même si en local nos tests étaient au vert, des erreurs pouvaient apparaître en production.\nLe scénario était alors simple, faire un rollback, ajouter les tests correspondants aux nouvelles erreurs detectées, les faire passer en local, puis recommencer !\n\nCe qui nous a peut-être le plus éprouvé dans cette migration, c’est une regression introduite dans la version 5.2\nqui avait pour conséquence de changer certains de nos tableaux vides en valeur null.\nIl nous a fallut quasiment repasser sur chaque requête pour retransformer ces valeurs en quelque chose de cohérent. \nQuand on avait de la chance, ce bug faisait échouer nos tests, mais il est malheureusement arrivé que ce soient les parseurs json des applications 6play de production qui en fassent les frais, avec différents plantages à la clé…\n\nConclusions\n\nCette migration fut longue et parfois douloureuse, heureusement les résultats sont maintenant au rendez-vous!\n\n\nDe plus, cela nous a donné l’occasion d’investir un peu de temps pour améliorer nos tests fonctionnels, et pour développer un système robuste pour la réplication des données sur plusieurs clusters Elasticsearch.\nOn peut néanmoins se poser des questions sur la stratégie très offensive de changement de version de la part de Elasticsearch,\nautant de releases avec autant de changements en si peu de temps, il faut être capable de suivre!\nPendant que nous finalisions notre production sur la version 5.2, la 5.3 a eu le temps de sortir, et la 6.0 est apparue en beta.\nNous allons essayer de profiter un peu de ce nouveau cluster avant de poursuivre vers une nouvelle grande migration :)\n"
} ,
  
  {
    "title"    : "Last night isomorphic JS saved our life!",
    "category" : "",
    "tags"     : " SPA, SSR, isomorphic, javascript, node.js, high availability",
    "url"      : "/2017/05/17/spa-mode-isomorphism-js.html",
    "date"     : "May 17, 2017",
    "excerpt"  : "For more than a year and a half, we use Node.js and React together to make the best app possible for our users. These 2 technologies are complementary to write only once code executed on the server and the client side: that’s the isomorphic way! T...",
  "content"  : "For more than a year and a half, we use Node.js and React together to make the best app possible for our users. These 2 technologies are complementary to write only once code executed on the server and the client side: that’s the isomorphic way! This approach helped us to develop a reliable app with a fast first render and SEO friendly.\n\nSSR caching\n\nHere is the architecture we use for the 6play web app.\n\n\n\nYou can see that Node.js server responses are cached with Varnish. Indeed, React is not efficient with server side rendering because it just has not been designed for that. The React renderToString method blocks the event loop. Consequently the server can not process an acceptable responses rate for a service like 6play that can reach a lot of requests per second without cache. Particularly when the European Football Championship final bring together France and Portugal and is live or when the last episode of « Les Marseillais », one of the teenagers favorite programs, has just been released on the platform. So caching server responses, with a quite low caching time, is required for our application health!\n\nSPA mode\n\nIsomorphism enables search engines to parse our website without executing any line of JavaScript, only using the server side rendering. We thought that the opposite could be useful too. Imagine our Node servers are down, for various reasons. Our Varnish servers continue to deliver the application pages but only during the cache time. After, the user would get an error… or not!\n\nIn this case, we would switch to a Nginx server that simply delivers a blank page with the client JavaScript code. The server was responsible for the app state initialization before, the user browser has to do so now. Then it can render the page: our application becomes a simple SPA. And this is almost imperceptible for the user, the first render is just a little longer. This way secures the availability of our service.\n\n\n\nThe Varnish servers check the status of the Node ones via a specific route. When every instance is down, they route all requests to a static HTML file on the Nginx server.\n\nReally useful ?\n\nYes! It is not used until it is! A few months ago we went through a memory leak. Consequences? After some time, we saw an increase in CPU usage, then the servers fell down and SPA mode was enabled. We didn’t notice the memory leak immediately because we often deploy new versions of our app and it resets the memory. When we detected the problem, it was too late to rollback because the incriminated version was probably weeks or months old.\n\nYou certainly know how difficult it is to find the code responsible for a memory leak in Node.js. It is often not a matter of hours but of days or weeks. With our SPA mode, we could debug our code with serenity. When the Node servers were down, the SPA mode took the reins. Then we simply restart the server to restore the nominal state when we were alerted (sometimes immediately, sometimes several hours after because it happened in the night). This situation went on some weeks. And we finally fixed the memory leak. No user has been affected. For us, this SPA mode is a significant safety for the high availability of our app.\n"
} ,
  
  {
    "title"    : "Symfony Live Paris 2017",
    "category" : "",
    "tags"     : " symfony, symfony live",
    "url"      : "/2017/04/03/sf-live-2017.html",
    "date"     : "April 3, 2017",
    "excerpt"  : "In March we attended Symfony Live Paris 2017, and it was very interesting.\nHere are some special feedbacks about some of our favorite talks.\n\n\nVarnish tags and invalidation\nSpeaker : Jérémy Derussé\nDescription\nJérémy presented us how to host appli...",
  "content"  : "In March we attended Symfony Live Paris 2017, and it was very interesting.\nHere are some special feedbacks about some of our favorite talks.\n\n\nVarnish tags and invalidation\nSpeaker : Jérémy Derussé\nDescription\nJérémy presented us how to host applications on an production environment on a Raspberry PI using varnish cache.\nToday, the internet traffic and the number of unique visitors increase every days.\nPeople imagines that we need strong hardware servers to be able to respond to this traffic.\n\nThe interesting part of this presentation was the way of how to better use the cache in front of our applications.\nTo be able to minimize the number of requests to the server by managing the cache duration\nand being able to invalidate it on demand when a resource expire.\n\nHow to tag and segment the cache\nBy tagging every resources by a unique id (entityName_id by example), you’ll be able to invalidate later the cache linked to it.\nYou can simplify and automate this tagging by using event dispatcher and listen all entities creations / modifications. With\nthis solution you also centralize your code logic.\n\nCache tagging also helps you with caching / invalidating many-to-many relationships between content items.\n\nHow to invalidate the cache\nJérémy showed us a very interesting interface, displaying the entity content, automatically refreshed when the backend needs to be called because of a cache invalidation.\n\nOnce a resource is modified and needs to be uncached, your backend needs to call the Varnish to purge the tag linked to\nresources. Once again the FOS HttpCache bundle does it very well.\n\nAdvantages / drawbacks\nCaching everything is fine, but it has limitations :\nWorks well when :\n\n  You have mostly read access\n  You have more cache hit than miss\n  Your application is able to communicate with your cache servers\n\n\nCan be difficult when :\n\n  Operations are not atomic\n  It’s complexifying / slowing backend writes\n\n\nHosting on a Raspberry PI\nJérémy finally showed us an impressive demonstration of an application running on Raspberry PI.\nThe configuration :\n\n  Docker\n    \n      MySQL\n      NGINX\n      PHP7-FPM\n      Varnish\n    \n  \n\n\nThe results :\n\n  Number of calls with no caching handling : around 8 / secs\n  Number of calls with varnish cache response : around 900 / secs\n    \n      impressive !\n    \n  \n\n\nResources\nYou can manipulate cache by using the FOSHttpCacheBundle : https://github.com/FriendsOfSymfony/FOSHttpCacheBundle\n\nLink to the speaker presentation : https://www.slideshare.net/JrmyDeruss/grce-aux-tags-varnish-jai-switch-ma-prod-sur-raspberry-pi\n\nEverything a dev should know about Unicode\nSpeaker : Nicolas Grekas\nDescription\n\nNicolas Grekas made a talk about Unicode, from its origin to the latest advanced uses, and more precisely in the PHP ecosystem. He explained how utf8/16 works and the complexity of language management, especially the folding, graphs clusters, modifiers etc.\n\nPHP doesn’t natively handle the unicode #RIPphp6, so it is important to understand the specificities of utf8 in order to avoid some traps, especially concerning string length calculation, comparisons and insertions in database.\n\nIn order to manage the unicode, you must use the php functions of mbstring, iconv, graph and inttl. The “u” modifier allows the utf8 to be processed correctly for regular expressions. For MySQL, utf8_unicode_ci handles ligatures whereas utf8_general_ci does not handle them - but is therefore faster.\n\nConcerning security and avoiding typosquatting, there is a list of confusable characters for filtering: https://unicode.org/cldr/utility/confusables.jsp?a=6play&amp;amp;r=None\n\nThis presentation was very interesting and very useful. Thank you Nicolas ;-)\n\nResources\nLink to the speaker presentation : https://speakerdeck.com/nicolasgrekas/tout-ce-quun-dev-devrait-savoir-a-propos-dunicode\n\nPerformances optimisation with Php7\nSpeaker: Julien Pauli\nDescription\n\nAs usual, it was a real pleasure to listen Julien Pauli speaking about what’s under the hood of Php7.\nEverybody knows there is an actual performances gap between version 5 and 7, but this talk gave some clues to understand the technical reasons behind these major improvements.\n\nFirst, Php compiler has been totally rewritten (yes, Php is compiled in opcode and cached in opcache).\nThanks to an AST, the compiler really understands the analyzed code,\nand lot of optimizations can be done at compilation time instead of runtime.\nFor example, all constant expressions (like $a = 1024 * 2048) are now computed once for all.\nOf course, this compilation pass is now longer but it is exactly the reason why opcache\nhas been made, and why you should warm up your opcache during your scripts’ deployment.\nJulien also introduced the parameter opcache.interned-strings-buffer,\nused to configure string interning in Php.\nHe advised to increase its default value (or at least to check relevance),\nbecause even a minimal Symfony application contains a lot of huge DocBlocks (and then huge strings) so the default\nvalue could be underestimated in some cases. Ready to benchmark! :)\n\nWe had also some interesting information about packed arrays optimisations,\nand some string tips (&quot;$a - $b&quot; is better than $a . &#39; - &#39; . $b for example).\nThen he gave a very good overview of all the efforts done on internal Php structures in order to optimize memory access,\nCPU cycles… an impressive work!\n\nTo finish, we have now an (very) approximated date for Php8 release!! Not before 2020… be patient :)\n\nResources\n\nLink to the speaker presentation: https://www.slideshare.net/jpauli/symfony-live-2017php7performances\n\nDo not hesitate to visit Julien’s blog: https://jpauli.github.io/\n\nAnd many more\n\nWe also liked :\n\n\n  Blog posts about Symfony 4 here : https://fabien.potencier.org/\n  Micro-Services Symfony at Meetic: feedback after 2 years of redesign! https://fr.slideshare.net/meeticTech/php-symfony-microservices-migration-meetictech\n  Introduction to CQRS and Event Sourcing: https://www.slideshare.net/samuelroze/introduction-to-cqrs-and-event-sourcing-74061563\n  Web security: and if we continued to break everything? https://github.com/ninsuo/slides\n\n\nYan can find most of the slides here: https://joind.in/event/symfonylive-paris-2017/schedule/list\n"
} ,
  
  {
    "title"    : "Format all the things",
    "category" : "",
    "tags"     : " prettier, javascript, react, lint, eslint, 6play",
    "url"      : "/2017/03/30/prettier.html",
    "date"     : "March 30, 2017",
    "excerpt"  : "Prettier\n\nPrettier is a brand new javascript library. Its simple goal is to reformat your code. For instance, when I write this:\n\n\nand run prettier on it, I’ll ultimately get this:\n\n\nYou can try it for yourself at prettier’s website\n\nLooks awesome...",
  "content"  : "Prettier\n\nPrettier is a brand new javascript library. Its simple goal is to reformat your code. For instance, when I write this:\n\n\nand run prettier on it, I’ll ultimately get this:\n\n\nYou can try it for yourself at prettier’s website\n\nLooks awesome doesn’t it? You don’t need to be a stylish developer anymore! But let’s be honest: not all developers are enthusiastic about this new tool. There are two kinds of developers, those who like when a program helps them to format their code, and those who don’t.\n\nThere are some benefits: using prettier you won’t waste your time reformatting your code because your destructuring expression overreaches your config’s character cap. You will no longer have conflicts because your colleagues changed indentation on the code you’re currently working on.\n\nBut it also comes with some tradeoffs. The first one is your coding style. We generally add line breaks between chained calls on an API (example here). Prettier will make this fit on one line if it can, and your code will lose some clarity. The same thing goes with the parentheses you add in complex boolean operations (example here). You will have to remember operator priorities.\n\nFor a relatively big project like ours, we prefer adding some consistency to our code in order to avoid wasting our time on solving conflicts, at the expense of losing a little bit of readability and style.\n\nHow to configure it?\n\nWe already have eslint setup in our project with some rule tweaks. So for us, Prettier had to interface smoothly with eslint. Fortunately it comes with a bunch of useful plugins.\n\n\n  eslint-config-prettier: this library disables all eslint rules that conflict with the Prettier formatting. Without it we would need to turn off those rules manually.\n  eslint-plugin-prettier: this library includes Prettier proper formatting as an eslint rule. So if our code is not well formatted, eslint will throw errors. This is the most important plugin for us, as it makes the linting fail if the code is not well formatted. Formatting is now mandatory!\n  prettier-eslint (prettier-eslint-cli): this plugin just runs prettier followed by eslint --fix command. The CLI version allows us to use it from the command line.\n\n\nWith these libraries, we can format all our code base and apply Prettier and eslint rules.\n\nMigration\n\nFirst we had to format the whole repository. This results in a pretty big PR, 670 modified files, +11700 -11113 changes… The implication is obvious: if you choose to use Prettier on your project, it had better be set up from the start.\n\nAnyway, once this huge PR was merged, we had to rebase other PRs. You can see it coming: if your rewrite most of your code and rebase other changes on it, there is nearly no way you can avoid conflicts.\nBut in reality it’s (almost) easier than it seems. Since all modifications were generated by Prettier, we can simply discard them and regenerate them after the rebase.\n\nSo, the first thing to do was to rebase on the parent of the format commit in order to resolve all conflicts that were not related to the formatting. To put it differently, we are sure that in next rebase conflicts will only be related to Prettier’s changes.\n\ngit rebase 0404b07~\n# where 0404b07 is the git hash of the format commit parent\n\nAfter that, we rebased our branch on the “prettier” commit, and we asked git to automatically keep the conflicting changes from the branch and to discard those from Prettier.\n\ngit rebase 0404b07 -s recursive -X theirs\n\nThen we just needed to re-run Prettier to reformat the rebased code. After this, branches were well formatted and could get back to their normal life-cycle.\n\nHow does it work on daily basis?\n\nFirst, adding the following scripts in the package.json file enables us to use prettier as a yarn (or npm) command.\n\n\n\nThe first line is used to format files provided as parameters and is used in a git pre-commit hook. The second line was there to format the whole codebase and should not be used anymore. This command takes around 1 minute to execute which is a little too long to be used in the development process. It’s more interesting to plug Prettier in our IDE and only format modified files.\n\nEven though we now enforce a machine-generated code style, everyone is still free to use their favorite IDE with any formatting and syntax settings they like.\nThose using atom or sublime-text can use plugins for the save action (atom plugin here with “ESLint Integration” checkbox and sublime-text plugin here). Every saved file will be automatically formatted by Prettier. This is clearly the most comfortable solution.\n\nThose used to applying the format action in Webstorm will have to configure an external tool to do it. Here is a good article to help you setup an external tool if you are interested in this solution.\n\nFinally we wrote a pre-commit hook and added it to our documentation. It automatically runs prettier on all added files from our javascript sources. lint-stage does the same, but we don’t want to force the whole team to use it. It’s clearly not necessary to run it twice, for those who have a save action which already runs Prettier.\n\nHere’s an example of our pre-commit hook:\n\ngit diff --name-only HEAD | grep -E &quot;src/.*\\.js.?$&quot; | xargs yarn format\n\nIn conclusion\nPrettier is a new tool to add to your chain. Its role is to format the code for you in a very strict way. Thanks to a bunch of plugins that complement it, it also plays nicely with and applies eslint rules. Like we said, there are few sacrifices to make in terms of clarity, but it allows you to stop taking care of things that add no real value to the code you write. It also helps you to reduce meaningless conflicts and debates on “how we should write this”. We plan to use it on all our javascript repositories, for greater consistency and good.\n"
} ,
  
  {
    "title"    : "L&#39;équipe Player de 6play.fr au Paris Video Tech",
    "category" : "",
    "tags"     : " video, ott, react, dash, hls, mse, cmaf, 6play, html5",
    "url"      : "/2017/02/20/retour-de-paris-video-tech.html",
    "date"     : "February 20, 2017",
    "excerpt"  : "\n\nPrésentation du Paris Video Tech\nMercredi 1er février avait lieu la troisième édition du Paris Video Tech, un meetup orienté autour de tous les sujets techniques de la vidéo : players HTML5, formats, encodage, distribution, publicité, …\n\nL’équip...",
  "content"  : "\n\nPrésentation du Paris Video Tech\nMercredi 1er février avait lieu la troisième édition du Paris Video Tech, un meetup orienté autour de tous les sujets techniques de la vidéo : players HTML5, formats, encodage, distribution, publicité, …\n\nL’équipe Player de M6 Web (Frédéric Vieudrin, Nicolas Afresne, Malik Baba Aïssa et Vincent Valot) présentait le nouveau player HTML5 de 6play.fr : un player MSE multi-formats, développé entièrement en React, le framework JS qu’on ne présente plus et qui fait le succès du nouveau 6play.fr depuis 2015.\n\nLa rencontre se déroulait dans les locaux de France Télévision à Paris et proposait trois talks :\n\n\n  6play : un player MSE en React par l’équipe Player de M6Web\n  CMAF Démystifié par Cyril Concolato\n  Retour d’Expérience de Roland Garros 360 par l’équipe innovation de France Télévision\n\n\n\n\n6play : un player MSE en React\n\nPrésentation de 6play.fr\nDans la première partie, nous avons présenté le contexte technique de 6play.fr, autour de React, ainsi que les chiffres clés du site.\n\nAprès un rappel de l’historique des players du site de Replay des chaînes du Groupe M6, nous avons présenté les enjeux de la refonte de notre précédent player et évoqué nos contraintes.\n\n\n\nArchitecture du player en React / Redux\n\n\n\n\nEn octobre 2015 sortait le nouveau 6play.fr, une Single Page App développée en React-Redux et Isomorphique. Le succès de cette refonte nous a poussé à étudier le refonte du player 6play sur la même stack technique, historiquement en Video.js.\n\nEn complément de l’approche composant proposée par React, Redux nous a apporté la solution à la gestion de l’état du player dans le temps. En effet, son fonctionnement par événements et actions était parfaitement adapté aux événements de la balise &amp;lt;video&amp;gt;.\n\nMedia Engines\nInspiré du système multi-techs de Video.js, nous avons développé notre propre système de Bridge pilotant les différents SDK Video du marché : hls.js, dash.js, Adobe Primetime Browser TVSDK, et HTML5.\n\nTous les Bridges communiquent ainsi de la même manière avec notre player React au travers des MediaEvents HTML5.\n\nIntégration continue, tests, outils et méthodes de travail\n\nLes tests automatisés font partie intégrante des développements chez M6Web. Tester unitairement nous permet de valider nos classes et méthodes, tester fonctionnellement assure le bon fonctionnement du player sur plusieurs navigateurs et évite les régressions.\n\nNous utilisons les Webhooks de Github pour executer nos tests, déployer un environnement de recette dédié et nous notifier du statut de la Pull Request dans Slack.\n\nLes slides de notre présentation\n\nRevoir l’événement en replay\n\n\n\n"
} ,
  
  {
    "title"    : "Get your brownfield React Native app built on demand",
    "category" : "",
    "tags"     : " mobile, github, ci, react-native",
    "url"      : "/2017/01/31/get-brownfield-react-native-app-built-on-demand.html",
    "date"     : "January 31, 2017",
    "excerpt"  : "As you may know, at M6Web we decided to embrace React Native a few months ago.\nIt’s a really exciting piece of software that adds a lot of value in the mobile development ecosystem.\n\nWe already use it for a side project on a standalone app (not pu...",
  "content"  : "As you may know, at M6Web we decided to embrace React Native a few months ago.\nIt’s a really exciting piece of software that adds a lot of value in the mobile development ecosystem.\n\nWe already use it for a side project on a standalone app (not public yet, stay tuned!) to record table soccer games, that’s why, we (mostly @ncuillery 😏) decided to improve the upgrade process for apps made with the embedded generator. See Nicolas’ blog post on it: Easier Upgrades with React Native.\n\nAs a result, we wanted to start using React Native for our most popular app: 6Play (6play iOS, 6play Android).\nSo they would become what Leland Richardson from Airbnb calls “brownfield” apps.\n6play is the catchup TV platform for the French TV group M6. It offers live-streaming and full episodes for web, mobile and set-top box. Since the apps launched in 2016, there have been over 1.5 billion videos streamed. Our iOS (mostly Swift) and Android native applications, both important parts of the 6play platform, were exclusively developed externally until now.\n\nWe wanted to use React Native to develop this project in-house and to take advantage of the benefits this hybrid technology could bring into our native apps. Here are just some of the benefits we found when using React Native:\n\n\n  JavaScript development for mobile. We have a lot of awesome JavaScript developers internally who develop the 6play website using React. We love React &amp;amp; Redux and want to mutualize this piece of technology we use on most of the frontends of the 6play platform.\n  Hot fixing with CodePush. For our mobile apps, we want to accomplish the same continuous delivery process we have for the website. CodePush helps us to keep the same flexibility by allowing us to make deployments on a weekly or even daily basis.\n  Knowledge sharing. We would like to be closer to the external development of our mobile apps, which was difficult without native knowledge and without any Android or Swift internal developers. React Native allows us to be part of that, we started working closely with the native team, sharing all developments between the two teams and bringing the best of both worlds (native and web) into the same project.\n  Code Sharing. We also want to share major parts of the mobile code base between apps (Android &amp;amp; iOS). Today, the code bases for each app are completely separate and are managed by two separate teams. With React Native, we could have one common code base while being able to implement specificities for a particular platform if needed. We have also imagined some ways to share code with the 6play website.\n\n\nAs we mentioned in a previous blog post, we use Github pull requests extensively in our development process, especially for testing (automatically and manually) each new commits before merging them into the master branch.\n\nIn the past, we tried to use Appetize to preview  our apps in the browser. It was a first shot, but the functionality was quite limited: animations felt janky, some features wouldn’t work (in-app purchase, video with DRM, …), user identification was painful. We needed a better solution, and as a result we decided to rethink the way we develop the 6play apps.\n\nFor the second iteration of our development process, we had a few simple requirements:\n\n\n  Test each pull request in conditions as close to the reality as possible,\n  Use the same testing workflow for both iOS and Android apps.\n\n\nThis post outlines our new mobile development process for the 6play apps. We’ll walk through how we manage the environment of a brownfield React Native app, our Git repository structure, our build and release workflows, and how we’ve created a CI environment that mirrors our production environment.\n\nMono Repository / Multi Repositories?\n\nThe first thing we had to do, was to decide how we wanted to organize our Git repositories.\n\nFor this, we looked into how the AirBnb team work with their brownfield app.\n\nWe soon realized we had two options here:\n\nMulti-repositories:\n\n\n  the iOS one\n  the Android one\n  and one for React Native code\n\n\nMono-repository:\n\n\n  One giant repository that has iOS, Android, and React Native folders inside.\n\n\nLet’s take a look at the pros and cons of both solutions.\n\nThe Mono Repository\n\n├── app-6play/\n│   ├── app-android/\n│   ├── app-ios/\n│   ├── react-native-views/\n\n\nAt first glance, this solution seems like the Holy Grail:\n\n\n  (+) Everything is in the same place\n  (+) If a modification needs both native &amp;amp; React Native developments, changes can be contained in a unique pull request.\n  (-) Code, Documentation, Setup, are more difficult at the beginning (For example, how can we keep Git history of each existing repository?).\n  (-) For our workflow, we need to have everyone working the same way, with the same git workflow, and the same review process. Remember that our native team is an external team (in Belgium), the Android &amp;amp; iOS teams are two different teams (located in the same place) and the React Native one is an internal team (in France). If we succeed, we’ll have synchronous development between teams, this is a really positive point, but it may be difficult to reach.\n  (-) Android, iOS and Javascript CI environments are very different (different tools, different needs), so it is really complex to setup.\n\n\nUltimately, the initial cost of setup and maintenance outweighed the benefits of a mono-repository.\n\nThe Multi Repositories\n\n├── app-android/\n├── app-ios/\n├── react-native-views/\n\n\n\n  (+) Each team could have its own Git workflow, branching model, review process,\n  (+) Each platform has its own CI, code conventions,\n  (-) Building the native apps including the React Native bundle is complicated,\n  (-) Three pull requests (one on each repository) are needed if the functionality includes a native bridge and React Native development.\n\n\nNeither approach was perfect. So we decided to choose the safest one, and create multiple repositories. Also, this choice doesn’t forbid any change of direction toward the mono repository in the future… The reverse seems much more complicated.\n\nDevelopment workflow\n\nEach native developer is now forced to have the react-native-views to be able to work on the native app.\nYou need to know that the native apps need node_modules dependencies of the React Native project, because they also contain the native part of React Native, and maybe some native code for React Native 3rd party you use.\nSo, we will need to clone the native app and the React Native repository.\n\nFor Android\n\ngit clone app-android\ngit clone react-native-views\n\nSo we will have two sibling folders:\n\n├── app-android/\n├── react-native-views/\n\n\nWe decided to use symlink to have a cleaner structure (and that will make the CI configuration easy later, see Continuous Integration), so the setup for the Android project will look like this:\n\ncd app-android\nln -s ../react-native-views ./react-native-views\ncd ../react-native-views\nnpm install\n\n├── app-android/\n│   ├── react-native-views -&amp;gt; ../react-native-views\n├── react-native-views/\n│   ├── node_modules/\n│   ├── package.json\n\n\nFor iOS\n\nSimilar steps to the Android process, but it seems that Xcode has difficulty following package with a symlink … so we have to be a little smarter:\n\ngit clone app-ios\ngit clone react-native-views\n\ncd app-ios\nmkdir -p react-native-views/node_modules\ncd ../react-native-views\nln -s ../app-ios/react-native-views/node_modules ./node_modules\nnpm install\n\nWith this method, the node_modules files will be written in the symlink. So those files will be located in the source of the symlink, the app-ios/react-native-views/node_modules directory (This is pretty twisted, we had to admit).\n\n├── app-ios/\n│   ├── react-native-views/\n│   │   ├── node_modules/\n├── react-native-views/\n│   ├── node_modules -&amp;gt; ../app-ios/react-native-views/node_modules\n│   ├── package.json\n\n\nReact Native\n\nNow we can choose: JavaScript developers are able to develop on any native app with the React Native packager (npm start in the react-native-views directory) and native developers can develop either with the packager started or with a pre-built React Native bundle (if their developments don’t concern React Native) by switching a Scheme (iOS) or a flavour (Android).\n\nContinuous integration\n\nThe next step was to find a way to improve the mobile development workflow.\nDuring our research, we found a SAAS tool named buddybuild that’s able to build the iOS &amp;amp; Android apps on each pull request. The setup for the native apps (before the React Native integration) or the React Native side project was really straightforward. It just magically works!\n\nWith the 3 Git repositories of our brownfield apps, it’s a bit more complicated than that. For this, buddybuild provides two useful hooks during the CI process. We just have to add a shell file in the repository:\n\n\n  buddybuild_postclone.sh: This is the hook that happens just after the cloning of the current repository by buddybuild\n  buddybuild_prebuild.sh: This hook is called after postclone and after buddybuild gets all dependencies (npm, Pod, Gradle …), but just before the build starts\n\n\nTo allow our Product Owners to test the app’s functionality, whether it’s related to React Native or not, we’d need:\n\n\n  An iOS build on each pull request on the iOS repository\n  An Android build on each pull request on the Android repository\n  An iOS &amp;amp; Android build on each pull request on the React Native repository\n\n\nTo meet the specific needs of our app development, we required:\n\n\n  For iOS &amp;amp; Android, we need a way to include the React Native code which lies in another repository.\n  For the React Native repository, we need a way to build the iOS &amp;amp; Android apps which lie in other repositories as well, and including the React Native code in it.\n  Our iOS and Android apps up-to-date with both the master branch of the native app, and the master branch of the React Native repository.\n  If a feature needs modifications on both the native code and the React Native code (multiple pull requests, one on each concerned repositories), we want an app synchronized with all repositories.\n\n\nSo let’s dig in these 4 points.\n\nBuild the iOS &amp;amp; Android apps including the React Native bundle\n\nThe key here is to clone the React Native repository in the postclone buddybuild hook and reproduce the directory structure we have in development mode.\n\nfor iOS\n\nbuddybuild_postclone.sh:\n\ngit clone react-native-views\n\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n# Make Xcode able to access to the node dependencies\nln -s react-native-views/node_modules node_modules\n\nbuddybuild_prebuild.sh:\n\n# export React Native bundle:\nnode_modules/.bin/react-native bundle --platform ios --entry-file index.ios.js --bundle-output ../&amp;lt;appFolder&amp;gt;/main.ios.jsbundle --dev false\n\n├── buddybuild workspace/ (app-ios inside)\n│   ├── react-native-views/\n│   │   ├── package.json\n│   │   ├── node_modules/\n│   ├── package.json -&amp;gt; react-native-views/package.json\n│   ├── node_modules -&amp;gt; react-native-views/node_modules\n\n\nfor Android\n\nbuddybuild_postclone.sh:\n\ngit clone react-native-views\n\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n# When buddybuild will run `npm install`, the node dependencies will be at the right place\nln -s react-native-views/node_modules node_modules\n\nbuddybuild_prebuild.sh:\n\n# export React Native bundle:\nnode_modules/.bin/react-native bundle --platform android --entry-file index.android.js --bundle-output ../&amp;lt;appFolder&amp;gt;/main.android.jsbundle --dev false\n\n├── buddybuild workspace/ (app-android inside)\n│   ├── react-native-views/\n│   │   ├── package.json\n│   │   ├── node_modules/\n│   ├── package.json -&amp;gt; react-native-views/package.json\n│   ├── node_modules -&amp;gt; react-native-views/node_modules\n\n\nThe only thing you have to do in the buddybuild dashboard is to create the app for each platform and activate the build on pull request only (see screenshot below). Buddybuild will automatically trigger an iOS &amp;amp; Android build on each pull request for the native repositories.\n\n\n\nBuild the iOS &amp;amp; Android apps on each pull request from the React Native repository\n\nNow, we’d like to easily test each react-native-views pull request on both iOS and Android apps.\n\nFor that purpose, we used the buddybuild hook again. Here is the buddybuild_postclone.sh:\n\n# Create a react-native-views folder\nmkdir react-native-views\n# Move everything in it\nmv * react-native-views\n\n# The postclone hook is ran by buddybuild for both iOS and Android builds. We distinguish the platform here, thanks to the env variable BUDDYBUILD_APP_ID (set by buddybuild)..\nif [ &quot;$BUDDYBUILD_APP_ID&quot; = &quot;&amp;lt;buddybuildAndroidAppID&amp;gt;&quot; ]; then\ngit clone app-android\ncd app-android\nelse\ngit clone app-ios\ncd app-ios\nfi\n\n# Move the native app to the root of the workspace\nmv * ..\ncd ..\n\n# Create the future node_modules location folder\nmkdir -p react-native-views/node_modules\n# Create the symbolic link for the app to be able to found the node_modules at the good place\nln -s react-native-views/node_modules node_modules\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n\nFor iOS, you’ll have:\n\n├── buddybuild workspace/ (app-ios inside)\n│   ├── react-native-views/\n│   │   ├── package.json\n│   │   ├── node_modules/\n│   ├── package.json -&amp;gt; react-native-views/package.json\n│   ├── node_modules -&amp;gt; react-native-views/node_modules\n\n\nFor Android, you’ll have:\n\n├── buddybuild workspace/ (app-android inside)\n│   ├── react-native-views/\n│   │   ├── package.json\n│   │   ├── node_modules/\n│   ├── package.json -&amp;gt; react-native-views/package.json\n│   ├── node_modules -&amp;gt; react-native-views/node_modules\n\n\nBy doing that, buddybuild will automatically install the npm dependencies, then launch the same prebuild hook as the native repository to build the React Native bundle.\n\nUsing buddybuild, you can create the app for each platform, and trigger new builds only when pull requests are opened, or when commits are added to existing pull requests. Buddybuild also builds both apps when React Native pull requests are opened as well.\n\nWhen master of React Native change, update the master iOS &amp;amp; Android apps\n\nBuddybuild makes it very easy to trigger a build programmatically via the API. We also use Jenkins for unit tests and lint, so we have a job triggered every time a push is made on the master branch of react-native-views. We have reused this job and append the following:\n\n# Our credentials\nACCESS_TOKEN_BB=&amp;lt;AccessToken&amp;gt;\nAPP_ID_BB_IOS=&amp;lt;buddybuildiOSAppID&amp;gt;\nAPP_ID_BB_ANDROID=&amp;lt;buddybuildAndroidAppID&amp;gt;\n\n# Build iOS\ncurl -X POST -H  &#39;Authorization: Bearer &#39;$ACCESS_TOKEN_BB” -d &#39;branch=master’ &#39;https://api.buddybuild.com/v1/apps/&#39;$APP_ID_BB_IOS&#39;/build&#39;\n\n# Build Android\ncurl -X POST -H  &#39;Authorization: Bearer &#39;$ACCESS_TOKEN_BB” -d &#39;branch=master’ &#39;https://api.buddybuild.com/v1/apps/&#39;$APP_ID_BB_ANDROID&#39;/build&#39;\n\nNow, you can activate the master build on the native iOS &amp;amp; Android buddybuild build, and you’ll have those apps up-to-date with the master branch.\n\n\n\nCross platform feature (both native &amp;amp; React Native)\n\nAt this point, this is not enough, because if you develop a feature that needs native and React Native modifications, you will not have the corresponding app before merging everything.\n\nWe have decided here to add a rule: for a “cross platform feature” (like a bridge for a native component for example), we have to define the same name for the branches in each repositories.\n\nA bridge for a native component (the authentication bridge as an example) would have three Git branches with the same name, and three pull requests (one on each repository).\n\nBy following this convention, we only have to checkout that branch when we clone the external repository in our postclone hooks:\n\n{\n  # Detect with the env variable BUDDYBUILD_BRANCH (given by buddybuild) the branch we are on.\n  echo &quot;Git checkout branch: $BUDDYBUILD_BRANCH&quot;\n  git checkout $BUDDYBUILD_BRANCH\n} || {\n  echo &quot;Git default branch: master&quot;\n  git checkout master # if master is the name of your default branch\n}\n\nWe do that branch name checking on the three repositories. This way, the four buddybuild projects (app-ios, app-android, react-native-views-ios, and react-native-views-android) can build native applications with modification on both sides.\n\nConclusion\n\nThanks to React Native and buddybuild, we now have a complete workflow as powerful as we have on the website. Being able to review either React Native or native code, and testing a real app before the code lands on the master branch is a big improvement for code quality and a huge step forward towards more agility.\n\nBig up to Tapptic Team, M6Web React Native team for this work, to the buddybuild support team for the help when needed.\n\nSpecial thanks to Nicolas Cuillery and Alysha for their proofreading!\n"
} ,
  
  {
    "title"    : "Une donnée presque parfaite sur 6play",
    "category" : "",
    "tags"     : " lyon, conference, elasticsearch, video",
    "url"      : "/2016/11/24/une-donnee-presque-parfaite.html",
    "date"     : "November 24, 2016",
    "excerpt"  : "Benoit Viguier, prestataire de la société Elao pour M6Web, a fait un retour d’expérience au Forum PHP de l’AFUP sur l’architecture technique mise en place autour de la mise à  disposition des données nécessaires à 6play.\n\n\n\nLes slides sont égaleme...",
  "content"  : "Benoit Viguier, prestataire de la société Elao pour M6Web, a fait un retour d’expérience au Forum PHP de l’AFUP sur l’architecture technique mise en place autour de la mise à  disposition des données nécessaires à 6play.\n\n\n\nLes slides sont également disponibles en PDF.\n"
} ,
  
  {
    "title"    : "Enquête exclusive au coeur de la technique de 6play. Les slides.",
    "category" : "",
    "tags"     : " conference",
    "url"      : "/2016/11/03/blendwebmix-6play-conference.html",
    "date"     : "November 3, 2016",
    "excerpt"  : "Voici les slides de la conférence “Plus d’un milliard de vidéos vues par an sur 6play - Enquête exclusive au coeur de la technique” que nous avons donné le 2 novembre 2016 lors de la conférence Blend Web Mix à Lyon.\n\nhttps://docs.google.com/presen...",
  "content"  : "Voici les slides de la conférence “Plus d’un milliard de vidéos vues par an sur 6play - Enquête exclusive au coeur de la technique” que nous avons donné le 2 novembre 2016 lors de la conférence Blend Web Mix à Lyon.\n\nhttps://docs.google.com/presentation/d/1BZGvoiubQsIzVjH9Px22wQyYmkboXZjpiubX2UtkMA4/edit?usp=sharing\n\nNous étions ravis d’être sponsor de cet évenement. Merci encore à toute l’organisation.\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity New York Conference 2016",
    "category" : "",
    "tags"     : " conference, velocity, webperf, devops",
    "url"      : "/2016/10/12/velocity-nyc-2016.html",
    "date"     : "October 12, 2016",
    "excerpt"  : "Nous étions cette année à New York, à quelques blocs de Time Square, pour suivre l’édition New Yorkaise de la Velocity Conference 2016.\nC’est une conférence que nous apprécions particulièrement et à laquelle nous nous rendons quasiment chaque anné...",
  "content"  : "Nous étions cette année à New York, à quelques blocs de Time Square, pour suivre l’édition New Yorkaise de la Velocity Conference 2016.\nC’est une conférence que nous apprécions particulièrement et à laquelle nous nous rendons quasiment chaque année, soit dans son édition européenne (Berlin, Londres, Barcelone, et Amsterdam cette année en novembre), soit aux U.S. (précédemment Santa Clara, New York cette année, et San José l’année prochaine).\nC’est l’occasion de suivre une conf de très haute qualité composée de 4 ou 5 tracks en parallèle, dédiée aux problématiques de performance et de scalabilité.\nOn remarque que d’année en année la conférence s’est réorientée autour du mouvement DevOps, alors qu’elle était précédemment beaucoup plus centrée sur la WebPerf (desktop et mobile).\n\nLa conférence commence par l’Ignite (sorte de mini conférence dans la conférence), basée sur un format court (type Lightning Talk) de 5 minutes pour une présentation de 5 slides défilant automatiquement. On retiendra de cette première partie des talks intéressants exposant les tristes chiffres de la diversité dans la tech aux US, mais aussi une conférence très drôle de @beldhalpern de @ThePracticalDev sur des parodies des livres OReilly (voir le O RLY Cover Generator) :\n\nEnjoying the heck out of @ThePracticalDev at Ignite #velocityconf. Lololol! pic.twitter.com/ZlJWoP4cjh&amp;mdash; Bridget Kromhout (@bridgetkromhout) 20 septembre 2016\n\n\nL’ignite s’est fini sur le célèbre Ignite Karaoké où 16 volontaires se sont prêtés au jeu de cet exercice hilarant mais tellement difficile, consistant à improviser une conférence sur le sujet de son choix sur 5 slides inconnues de l’orateur et qui défilent automatiquement au bout de quelques secondes 😃. Ce qu’on fait aussi chez M6Web de temps en temps nommé Karaoké Slideshow et que Kenny avait animé lors d’un Forum PHP (voir la vidéo).\n\n\nCrédit : Flickr\n\nNous avons ensuite suivi deux jours de conférences dont les thèmes majeurs étaient :\n\n\n  Les Service Workers\n  Les microservices\n  Le monitoring\n  HTTP2\n  La sécurité des apps\n  Les détections d’anomalie\n  Les ChatOps\n  Le WebMobile, AMP et les PWA\n\n\nChatOps\n\nUn des sujets assez récurrent, notamment dans la mouvance DevOps est l’utilisation des ChatOps, sujet popularisé par Github (via Hubot).\nCela consiste généralement en un bot ou une IA posée sur un outil de Chat type Slack, Flowdock ou Hipchat, permettant de simplifier la communication entre différentes équipes et les différents outils (ticketing, alerting, monitoring, état d’une machine, etc). Une démo de l’IA de Dynatrace à reconnaissance vocale à été faite, montrant comment par la voix, on pouvait recevoir dans l’outil de Chat les infos sur les incidents de la veille, créer les tickets de support etc. Voir ici. Un peu gadget, mais rigolo.\n\nL’un des points à retenir, c’est que même si ces outils font partie de la « culture » DevOps, ce n’est pas l’ajout d’un de ces outils qui fera apparaître cette culture dans votre entreprise si vous ne l’avez pas.\n\n\n  Tools will not fix a broken culture\n\n\n\nCrédit : Flickr\n\nLe WebMobile, AMP, et les PWA\n\nPlusieurs conférences avaient pour but de comparer ce que l’on pouvait obtenir de nos jours via du WebMobile versus ce que l’on a sur les apps natives. Le fossé s’est énormément rétréci et les WebApps ont désormais accès à la plupart des fonctionnalités présentes côté natif :\n\n\n  Notifications\n  Ajout sur le Home Screen de l’icone\n  Full Screen\n  Orientation\n  Gestion hors ligne\n  …\n\n\nCe qui nous amène aux Progressive Web Apps : PWA\n\nPete Lepage @petele de chez Google nous a notamment présenté des projets open-source de Google pour mettre en place différentes politiques de cache via les « serviceWorkers » (voir https://developers.google.com/web/tools/service-worker-libraries/), ainsi que les futures api : Web Payments, Credential Management …\n\n\n  Les slides.\n  Exemple de la PWA du Washington Post\n\n\nToujours sur la partie mobile, Malte Ubl (@cramforce), core développeur de AMP, nous a présenté le futur de ce protocole de Google pour offrir des pages plus rapides pour la consultation de site média sur mobile.\n\n\n  AMP is a web component library, validator and caching layer for reliably fast web content at scale\n\n\nEn commençant par un bilan d’AMP, 3000 PR + 200 contributeurs (au bout d’un an seulement !), Malte nous a expliqué qu’un site mobile très optimisé pouvait logiquement être plus performant qu’AMP.\n\nArriveront prochainement sur AMP, le support des formulaires, des optimisations avancées d’images via le Google AMP Cache, des Service Workers pour AMP pour ne jamais télécharger AMP dans le « chemin critique » du chargement de la page.\n\nUn petit focus a aussi été fait sur les PWA et AMP avec amp-install-serviceworker qui est un Service Worker permettant d’installer la PWA après chargement de AMP, pour faire une upgrade transparente de AMP vers une PWA (Voir une démo ici choumx.github.io/amp-pwa)\n\n\n  AMP : « Start Fast, Stay Fast »\n\n\nNous avons aussi vu une conférence sur l’optimisation de la consommation des webApps en terme de CPU / temps de réponse, notamment via l’étude des capacités JS de chacun des devices/OS avec le benchmark JetStream Javascript.\n\nOn découvre notamment que l’iPhone 7 a des capacités assez impressionnantes, contrairement à l’iPhone 5C, que le mode « économie d’énergie » ou encore une bonne insolation rendent les devices beaucoup moins performants. D’excellentes slides à voir ici : hearne.me/2hot\n\nWebPerf\n\nCôté WebPerf, peu de grosses nouveautés, on retiendra @nparashuram qui nous a montré comment automatiser le “profiling” des ChromeDevTools dans Node.js via ChromeDriver !\n\nPlus d’infos ici : https://blog.nparashuram.com/2016/09/rise-of-web-workers-nationjs.html\n\nTammy Everts (@tameverts de Soasta) et Pat Meenan (@patmeenan de Google et créateur de WebPageTest) nous ont fait un gros retour basé sur toutes les métriques récoltées par Soasta mPulse (outils de Real User Monitoring en SAAS) afin de déterminer des corrélations entre les temps de chargement et d’autres métriques (taux de rebond, conversion, etc.) grâce à l’application de concept propre au Machine Learning sur une quantité énorme de data. Toujours intéressant.\n\nSlides ici : https://conferences.oreilly.com/velocity/devops-web-performance-ny/public/schedule/detail/51082\n\n\nCrédit : Flickr\n\nCôté Single Page App, le Server Side Rendering est revenu à plusieurs reprises afin d’avoir des SPA performantes dont le premier rendu est généré côté serveur, ce que permet nativement React, et désormais Ember et Angular 2. Voir notre article sur l’isomorphisme.\n\nCoté HTTP, on retiendra Hooman Beheshti qui nous a fait un retour d’expérience sur HTTP2. Après une explication des nouveautés du protocole (binary, single, long-lasting TCP connection, streams encapsulation, frames, bi-directional…), une comparaison avec HTTP 1 nous a été exposée. En conclusion, HTTP2 est complexe et la migration n’est pas une simple modification de paramètre. Bien que cette nouvelle version est légèrement plus rapide, en particulier sur un réseau lent (&amp;lt;1Mbps), le protocole supporte très mal les pertes de paquets ou les fortes congestions à cause de l’unique connexion (TCP slow start). La recommandation est de tester sur chaque site et d’optimiser ses pages selon la version d’HTTP utilisée. Une piste serait HTTP2 over UDP.\n\nLes slides\n\nDevOps\n\nDe nombreuses conférences avaient pour objectif d’aborder les bienfaits du DevOps et plus largement les bonnes pratiques liées au mouvement afin de gagner en qualité et fiabilité.\n\nOn retiendra notamment la conférence de Cornelia Davis (DevOps: Who does what?) explicitant les différents rôles dans un SDLC (Software Development Life-Cycle) et leur répartition en équipe dans l’organisation.\n\nLes rôles dans le SDLC :\n\n\n  Architecte : Ent Archi, Biz Analyst, Portfolio Mgmt\n  SCO : Info sec\n  Infra : Srv Build, Cap Plan, Network, Ops\n  Middleware/AppDev : Middleware Eng, SW Arch, SW Dev, Client SW Dev, Svc Govern\n  Data : Data Arch, DBA\n  Biz : Prod Mgmt\n  Ent Apps : DCTM (Documentum) Eng.\n\n\nLa répartition en équipe proposée :\n\nPlatform (unique / transverse) :\n\n\n  Middleware/AppDev : Middleware Eng, Svc Govern\n  Infra : Srv Build, Cap Plan, Network, Ops\n  SCO : Info sec\n  Data : DBA\n\n\nCustomer Facing App (de 1 à n équipes)\n\n\n  Middleware/AppDev : SW Arch, SW Dev, Client SW Dev\n  Data : Data Arch\n  Infra : Cap Plan, Ops\n  Biz : Prod Mgmt\n  Architecte : Biz Analyst\n\n\nEnablement (unique / transverse) :\n\n\n  Architecte : Ent Archi, Portfolio Mgmt\n\n\nDCTM - Documentum (Enterprise Content Management Platform) (unique / transverse) :\n\n\n  Infra : Ops, Cap Plan\n  Ent Apps : DCTM (Documentum) Eng.\n\n\nOn notera notamment la présence d’Ops dans les équipes Customer Facing App et inversement de Middleware Eng dans l’équipe Platform.\n\nDe même, la présence d’architectes transverses (enablement) permet de garder une architecture cohérente. (Pas de slides disponibles pour cette conférence)\n\n\nCrédit : Flickr\n\nMicroservices\n\nLa conférence de @susanthesquark, axée sur les microservices, rappelait quelques bonnes pratiques :\n\n\n  Architecture sans SPOF\n  Ne pas laisser la dette technique s’accumuler\n  Déploiement continu\n  Travail en équipe entre Dev / PM / SRE\n  Monitoring\n  Procédures standard de gestion des incidents\n  Post-mortem pour apprendre de ses erreurs…\n\n\nLes slides\n\nConcernant le monitoring des microservices, la conférence de Reshmi Krishna @reshmi9k s’intéressait à l’analyse de la latence, inhérente à ce type d’architecture. La principale technique proposée est celle du suivi d’une requête de bout en bout, grâce notamment à l’outil Zipkin. De même, une gestion des timeouts globale (pour chaque requête pour tous les microservices) et dynamique (selon le contexte) permet de maîtriser les problèmes en cas de ralentissement d’un service en particulier.\n\nLes slides\n\nSécurité\n\nConcernant la sécurité, la conférence de Kelly Lum @aloria, passait en revue le minimun vital :\n\n\n  La sécurité doit être pensée dès la conception\n  Permettre aux utilisateurs de reporter facilement des problèmes de sécurité et être à l’écoute des réseaux sociaux\n  Toujours remercier les utilisateurs signalant les failles\n  Avoir une équipe testant régulièrement la sécurité (Crack Team).\n  En cas de failles de sécurité, après correction, toujours analyser les causes et apprendre de ses erreurs.\n\n\nLes slides\n\nConclusion\n\nVous pouvez retrouver la plupart des slides ici.\net voir les vidéos de certaines conférences ici.\nou ici.\n\nLes photos officielles de la conf sont ici.\n"
} ,
  
  {
    "title"    : "Use the Sensiolabs Security Checker to check potential vulnerabilities on Symfony projects",
    "category" : "",
    "tags"     : " 6tech, lyon, symfony, security, php, jenkins",
    "url"      : "/2016/07/20/sf2-security-checker.html",
    "date"     : "July 20, 2016",
    "excerpt"  : "Numerous vulnerabilities are detected every day. That’s a good thing and a key benefit of using open source products. At m6web we don’t want to be exposed to known vulnerabilities, so we use a service provided by Sensiolabs in our continuous integ...",
  "content"  : "Numerous vulnerabilities are detected every day. That’s a good thing and a key benefit of using open source products. At m6web we don’t want to be exposed to known vulnerabilities, so we use a service provided by Sensiolabs in our continuous integration tool (Jenkins) to check it.\n\nJust add those lines in your ant build file (and adapt basedir) :\n\n    &amp;lt;!-- =================================================================== --&amp;gt;\n    &amp;lt;!-- Security checker                                                    --&amp;gt;\n    &amp;lt;!-- =================================================================== --&amp;gt;\n    &amp;lt;target name=&quot;sf2-security-checker&quot;&amp;gt;\n     &amp;lt;exec executable=&quot;bash&quot; dir=&quot;${basedir}/sources/bin&quot; failonerror=&quot;true&quot;&amp;gt;\n         &amp;lt;arg value=&quot;-c&quot;/&amp;gt;\n         &amp;lt;arg value=&quot;curl -Os https://get.sensiolabs.org/security-checker.phar&quot; /&amp;gt;\n     &amp;lt;/exec&amp;gt;\n     &amp;lt;exec executable=&quot;php&quot; dir=&quot;${basedir}/sources&quot; failonerror=&quot;true&quot;&amp;gt;\n         &amp;lt;arg line=&quot;${basedir}/sources/bin/security-checker.phar security:check composer.lock&quot; /&amp;gt;\n     &amp;lt;/exec&amp;gt;\n    &amp;lt;/target&amp;gt;\n\n\nAnd automatically check your composer.lock againts vulnerabilities. Your build will fail if something wrong is detected.\n\nFor example, with the recent Guzzle one :\n\n\n\nYou can contribute to the vulnerabilities database and the checker via Github.com.\n"
} ,
  
  {
    "title"    : "Retour d&#39;expérience sur l&#39;utilisation de Cassandra sur 6play en vidéo",
    "category" : "",
    "tags"     : " 6tech, lyon, conference, cassandra, video",
    "url"      : "/2016/07/04/rex-cassandra.html",
    "date"     : "July 4, 2016",
    "excerpt"  : "\n\nErratum : dans les phases de questions réponses, j’annonce une phase de test à 10K RPS (requêtes par seconde) ; il s’agissait de RPM (requêtes par minute).\n\nLors du match Suisse vs France, diffusé sur M6 pendant la coupe d’Europe de football, la...",
  "content"  : "\n\nErratum : dans les phases de questions réponses, j’annonce une phase de test à 10K RPS (requêtes par seconde) ; il s’agissait de RPM (requêtes par minute).\n\nLors du match Suisse vs France, diffusé sur M6 pendant la coupe d’Europe de football, la brique users est montée à 75K RPM (soit 1200 rps) et 84K pour Islande vs France.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Migrate smoothly your Flux isomorphic app to Redux",
    "category" : "",
    "tags"     : " react, flux, redux, fluxible, isomorphic, javascript",
    "url"      : "/2016/07/04/migrate-smoothly-flux-isomorphic-app-to-redux.html",
    "date"     : "July 4, 2016",
    "excerpt"  : "Flux, history reminders…\n\n« Flux is the application architecture that Facebook uses for building client-side web applications. » That’s the definition of Flux on the Facebook website. So Flux is just a pattern, not a framework, that goes well with...",
  "content"  : "Flux, history reminders…\n\n« Flux is the application architecture that Facebook uses for building client-side web applications. » That’s the definition of Flux on the Facebook website. So Flux is just a pattern, not a framework, that goes well with React, but not only. The model is focused on user interactions. Its main strength is the unidirectional data flow that enforces developers to be careful and ensures code consistency when application grows up.\n\nSeveral libraries propose tools to implement Flux pattern easily. If no one stood out from the crowd at the beginning, now Redux, created by Dan Abramov, is clearly the one that the community have chosen as you can see below. Most Flux based React start kits you can find are based on Redux.\n\n\n\nAt M6Web, our 6play web application is not designed with Redux, but we use Fluxible. Fluxible is another Flux library, developed by Yahoo. We chose it back in December 2014, when we started the project, because Fluxible was at the time one of the few tool designed for isomorphic applications. Moreover it was already used in production by Yahoo.\n\nWhy do we think Redux is a better choice\n\nEven though Fluxible did get the job done, we are now willing to upgrade our application to Redux. Why?\n\n\n  The popularity of Redux will certainly affect other libraries life and support in the future, maybe Fluxible will be concerned. Fluxible is only supported by a firm and not really by the community.\n  Fluxible has a powerful but complex structure based on contexts and plugins. This can be useful, however when new developers come on the project, this is not always easy to understand. We are always searching to make code simpler for maintainability and we think that Redux is a better alternative than Fluxible on this topic.\n  For a given feature, developers write less code using Redux because the design is very simple, there is no extra boilerplate, the flow is condensed as much as possible. As a consequence, unit tests are easier to write.\n  There are very useful tools about Redux that make Developer eXperience better. For instance, the Redux DevTools allows to time travel live between Flux events. The middleware concept extending capabilities of actions is also interesting.\n  We are beginning to make our development processes converge. Every new React project here starts on Redux, including the Proof Of Concept we made with React Native. Using the same libraries made code sharing easier for us.\n\n\nMigrating a big app from Fluxible to Redux is crazy, isn’t it?\n\n6play is a very big web application. How to migrate to Redux in a reasonable amount of time and without risk?\n\nWe were quite sure that Redux and Fluxible could work together. The goal would be to migrate gradually to Redux without having to remove Fluxible in one giant step. First of all, because we can’t mobilise enough resources to do this in a relatively short time. Secondly, we want to avoid a big deploy in production and potentially critical bugs (even though our application is well tested, there are always cases that we can’t control like memory load for example).\n\nWe tried it… And we succeeded! And this is quite simple.\n\nFirst, we define the store configuration like other Redux application.\n\n// configureStore.js\n\nimport {createStore, combineReducers, applyMiddleware, compose} from &#39;redux&#39;;\nimport thunk from &#39;redux-thunk&#39;;\nimport {canUseDOM} from &#39;fbjs/lib/ExecutionEnvironment&#39;;\n\nimport myReducer1 from &#39;./modules/myModule1/myModule1.reducer&#39;;\nimport myReducer2 from &#39;./modules/myModule2/myModule2.reducer&#39;;\n\nexport default initialState =&amp;gt; createStore(\n  combineReducers({myReducer1, myReducer2}),\n  initialState,\n  compose(\n    applyMiddleware([thunk]),\n    canUseDOM &amp;amp;&amp;amp; window.devToolsExtension ? window.devToolsExtension() : f =&amp;gt; f\n  )\n);\n\nThen we initialize Redux store in our server file. For isomorphic purposes, we have to serialize stores’ state and give it to the html so that client side can take control of the application with server’s data. So here, we build data for the client by combining Redux and Fluxible states.\n\n// server.js\n\nimport {provideContext} from &#39;fluxible-addons-react&#39;;\nimport {match, RouterContext} from &#39;react-router&#39;;\nimport configureStore from &#39;./configureStore&#39;;\n\nprocessAppRequest() {\n  // ...\n\n  const fluxibleContext = FluxibleApp.createContext();\n  const reduxStore = configureStore(initialState);\n\n  match({routes: FluxibleApp.getComponent(), location: url}, (error, redirectLocation, routerState) =&amp;gt; {\n    // ...\n\n    // Original Fluxible root element\n    const rootElement = React.createElement(\n      provideContext(RouterContext, customContextTypes),\n      {...routerState, context: fluxibleContext.getComponentContext()}\n    );\n\n    // Now with Redux\n    const markup = ReactDOMServer.renderToString(\n      React.createElement(Provider, {store: reduxStore}, rootElement)\n    );\n\n    // Build state for client\n    const finalState = {\n      ...FluxibleApp.dehydrate(fluxibleContext),\n      reduxStoreState: reduxStore.getState()\n    };\n\n    // Then build the response layout with the markup and the whole state as usual\n    // ...\n  }\n}\n\nOn client side, we do the opposite operation.\n\n// client.js\n\nimport {provideContext} from &#39;fluxible-addons-react&#39;;\nimport {Router, browserHistory} from &#39;react-router&#39;;\nimport configureStore from &#39;./configureStore&#39;;\n\nconst dehydratedState = window[stateVarName];\nconst reduxStore = configureStore(dehydratedState.reduxStoreState);\n\n// Fluxible rehydrate its state\napp.rehydrate(dehydratedState, (error, fluxibleContext) =&amp;gt; {\n  // ...\n\n  // Original Fluxible root element\n  const rootElement = React.createElement(provideContext(Router, customContextTypes), {\n    history: browserHistory,\n    routes: app.getComponent(),\n    context: fluxibleContext.getComponentContext()\n  });\n\n  // Now with Redux\n  ReactDOM.render(\n    React.createElement(Provider, {store: reduxStore}, rootElement),\n    document.getElementById(rootId)\n  );\n});\n\nAnd that’s it! We can now use Redux in our component as usual, in combination with Fluxible. We can define actions and reducers for new features (instead of using Fluxible) but we can also transform progressively some Fluxible stores and actions into Redux flow, this is very easy. API requests stay in actions but data processing moves to reducers. Then data sorting and filtering logic in Fluxible stores moves to selectors.\n\nComponents connection to stores\n\nTwo files to rule them all\n\nWith Fluxible, we linked components with stores through connectToStore in the same file and exported only the connected component. But we think now it is a bad practice:\n\n\n  splitting data fetching from stores and display logic is interesting for maintainability and code understanding,\n  it is much easier to unit test the component without the connection to store, connectToStores (Fluxible) or connect (Redux) methods are parts of a 3rd-party library, and we don’t need to test it.\n\n\nFrom now on, components are files named *.component.js and stores connections are in *.connector.js files in the same folder. We can link a component both with Redux and Fluxible stores.\n\n// myComponent.connector.js\n\nimport MyComponent from &#39;./myComponent.component&#39;;\n\n// Stores\nimport connectToStores from &#39;fluxible-addons-react/connectToStores&#39;;\nimport {connect} from &#39;react-redux&#39;;\nimport MyFluxibleStore from &#39;../stores/myFluxible.store&#39;;\n\n// Utils\nimport {getSomeDataFromState} from &#39;../myModule.selectors&#39;;\n\n// Redux\nexport const mapStateToProps = (state, props) =&amp;gt; {\n  return {dataFromRedux: getSomeDataFromState(state, props.myProps2)};\n};\n\n// Fluxible\nexport default connectToStores(\n  connect(mapStateToProps)(MyComponent),\n  [MyFluxibleStore],\n  (context, props) =&amp;gt; ({\n    dataFromFluxible: context.getStore(MyFluxibleStore).getSomeData(props.myProps1)\n  })\n);\n\nWe export mapStateToProps function because in a few cases it contains logic that may be interesting to unit test.\n\nStores connections order\n\nIn this example, the link to Fluxible store is higher in components tree than the Redux one as we can see below.\n\n\n\nIt means that if Redux state changes, the Fluxible wrapper component won’t be reloaded but in the reverse case, both Fluxible and Redux wrapper components will rerender. In most scenarios, it doesn’t matter. Connection order of Redux and Fluxible is significant in two situations:\n\n\n  If one connection depends on data stored in the other library state, it has to be lower in components tree.\n\n\n// myComponent.connector.js\n\nimport MyComponent from &#39;./myComponent.component&#39;;\n\n// Stores\nimport connectToStores from &#39;fluxible-addons-react/connectToStores&#39;;\nimport {connect} from &#39;react-redux&#39;;\nimport MyFluxibleStore from &#39;../stores/myFluxible.store&#39;\n\n// Utils\nimport {getSomeDataFromState} from &#39;../myModule.selectors&#39;;\n\n// Fluxible wrapper depends on data from Redux state\nconst MyComponentFluxibleConnector = connectToStores(\nMyComponent,    \n  [MyFluxibleStore],\n  (context, props) =&amp;gt; ({\n    dataFromFluxible: context.getStore(MyFluxibleStore).getSomeDataFromReduxState(props.myPropsFromRedux)\n  })\n);\n\n// Redux\nexport const mapStateToProps = state =&amp;gt; {\n  return {myPropsFromRedux: getSomeDataFromState(state)};\n};\n\nexport default connect(mapStateToProps)(MyComponentFluxibleConnector);\n\n\n  If the higher connection is made on Fluxible stores (like first example of myComponent.connector.js), data passed to props must be immutable otherwise it can cause edge effects. Indeed, Redux wrapper component checks if it has to rerender when props change by comparing their references. So, if we mutate data in Fluxible store when dispatch is handled, references don’t change and Redux wrapper (and sub-components) will not rerender (unless you tell the connect method that your component isn’t “pure”).\n\n\nIf we watch carefully to those particular cases, we will succeed in our quest!\n\nIn a nutshell, Redux can easily work in addition to Fluxible (and certainly to other Flux libraries), most likely because of the lightness of its implementation. It is very convenient to upgrade smoothly a big application on Redux! But be aware that it is only a transitory situation, the final goal is to use only Redux. We wrote 50% less code with this upgrade, not bad… Developers are lazy, don’t forget this! If you have some feedback on Redux and/or Fluxible, don’t hesitate to share your experience with us :)\n\n"
} ,
  
  {
    "title"    : "Retour d’expérience : réaliser des Workers en PHP - Fabien de Saint pern au PHP Tour 2016 ",
    "category" : "",
    "tags"     : " 6tech, lyon, conference, video, phptour, php, Symfony",
    "url"      : "/2016/06/23/video-phptour-worker-php.html",
    "date"     : "June 23, 2016",
    "excerpt"  : "Fabien de Saint pern - lead dev de notre team back-end 6play - était au PHP Tour et a fait une présentation sur la façon dont nous faisons des workers en PHP.\n\n\n\n",
  "content"  : "Fabien de Saint pern - lead dev de notre team back-end 6play - était au PHP Tour et a fait une présentation sur la façon dont nous faisons des workers en PHP.\n\n\n\n"
} ,
  
  {
    "title"    : "Preview your Android &amp; iOS React Native apps on your Github Pull Request",
    "category" : "",
    "tags"     : " reactnative, react, mobile, github, jenkins, fastlane, appetize",
    "url"      : "/2016/06/20/preview-android-ios-react-native-on-github-pull-request.html",
    "date"     : "June 20, 2016",
    "excerpt"  : "We are playing since a few weeks with React Native for a Proof Of Concept and wanted to have the same development workflow for mobile apps, as we have for the web.\n\nHere is the workflow we use for web development:\n\n\n  Branch  : every bugfix or fea...",
  "content"  : "We are playing since a few weeks with React Native for a Proof Of Concept and wanted to have the same development workflow for mobile apps, as we have for the web.\n\nHere is the workflow we use for web development:\n\n\n  Branch  : every bugfix or feature is developed on a new git branch,\n  Pull Request (PR)  : we make PR for each bugfix or feature to propose the modification to the « master » git branch,\n  Code Review  : other teammates have to review each PR and add :+1: when they agree with the modification,\n  Test  : a CI system (Jenkins) runs Unit and Integration tests, and Lint on each PR,\n  Preview  : an internal tool (Github Hooker) is called with a Github webhook on each PR to create a staging environment.\n\n\nWhen every step is ok, the PR is merged.\n\nThe « Branch step », « PR step » and « Code Review step » are mostly related to our CVS (Github Enterprise) and are not a problem.\nThe « Test step » is related to React Native. We already use Jest and ESLint, but we have to dig more for Integration test (Appium ?).\n\nThe « Preview step » is more interesting. It was not the simplest thing to do on our web project, but this is probably one of the most useful feature we have on our stack.\nHaving a staging environment for all open PR allows devs, PO, PM and scrum masters to play with this exact version of the code (on any browser they want), and really see if the bug is fixed, or if the feature correspond to the PO needs. It allows everyone to iterate and make feedbacks before the code lands on the master branch. It’s also a good way to be sure your app build didn’t fail.\n\nSo, what we want is to have on each of our React Native PR, a link to preview iOS and Android version of our app in a web browser, refreshed after every commit on the branch.\n\nThe goal of this blog post is just to show you, that it is something doable and really useful. If you are interested in, here are some more information, that maybe can help you.\n\nThe stack\n\nConcerning the CI, we already use Jenkins, so we will continue to. Beware that for building iOS apps, a CI running on OSX is needed. In our case, we had added a Jenkins slave to our Jenkins pool. If you don’t have CI system internally, you should take a look at Bitrise or CircleCi because they propose OSX CI systems.\nOur CVS is Github Enterprise, but everything is also possible with Gitlab (or any other CVS).\nWe use Fastlane.tools to automate build and credentials support. (Mostly because it was recommended by some of our iOS developers).\n\nIn order to preview iOS and Android app in a web browser, we use the amazing SAAS service Appetize.io (free for 100min/month).\n\n\n\nHow did we do ?\n\nWe had set up an OSX machine with a fresh Jenkins install, and created a job that triggers a build everytime a push is made on a PR, thanks to the “Github Pull Request Builder » Jenkins plugin. There is also a lot of things to configure on this machine (Nodejs, Ruby, xCode …), and i recommend you to do some builds (iOS and Android) manually to be sure everything is ready.\n\nFastlane is an open-source automation toolset for iOS &amp;amp; Android. It lets you write « lane » to automate a lot of things. We set up a unique Fastlane file at the root of our React Native project directory dealing with Android &amp;amp; iOS lanes.\n\nTo suit our needs, we created one lane « deployAppetize » for each platform: it performs the corresponding build, uploads it to Appetize.io via their API, and updates the Github PR Statuses during the process.\n\nI’m not a Ruby programmer, so please, don’t blame me, and feel free to improve the code below if you want (on this Github Gist).\nThis is neither the state of the art, nor a beautiful open source thing, we just share what we did in case it helps someone :-)\n\nBefore doing anything, you’ll have to set some variables on Fastlane, so go to the Fastfile file in your fastlane folder:\n\n#3rd party lib to do some http calls\nrequire &#39;httparty&#39;\n\nfastlane_version &quot;1.95.0&quot;\ndefault_platform :ios\n\nbefore_all do\n  # put here your token and iOS scheme app\n  ENV[&quot;GITHUB_TOKEN&quot;] = &quot;----&quot;\n  ENV[&quot;APPETIZE_TOKEN&quot;] = &quot;----&quot;\n  ENV[&quot;APP_IOS_SCHEME&quot;] = &quot;----&quot;\n\n  # get the last git commit information\n  ENV[&quot;GIT_COMMIT&quot;] = last_git_commit[:commit_hash]\n\n  # Use ghprbSourceBranch env variable on CI, git_branch lane elsewhere\n  if !ENV[&quot;ghprbSourceBranch&quot;]\n    ENV[&quot;ghprbSourceBranch&quot;] = git_branch\n  end\n\nend\n\nCreate a private lane to make the POST request to your Github statuses API to avoid DRY:\n\n# Update git statuses of your commit.\nprivate_lane :githubStatusUpdate do |options|\n\n  response = HTTParty.post(\n    &quot;https://&amp;lt;yourgithubenterprisedomain.tld&amp;gt;/api/v3/repos/&amp;lt;orga&amp;gt;/&amp;lt;repos&amp;gt;/statuses/#{ENV[&quot;GIT_COMMIT&quot;]}?access_token=#{ENV[&quot;GITHUB_TOKEN&quot;]}&quot;,\n    :body =&amp;gt; {\n      :context =&amp;gt; options[:context],\n      :state =&amp;gt; options[:state],\n      :description =&amp;gt; options[:description],\n      :target_url =&amp;gt; options[:url]\n    }.to_json,\n    :headers =&amp;gt; { &#39;Content-Type&#39; =&amp;gt; &#39;application/json&#39; }\n  )\nend\n\nAppetize allows you to create different apps. We want one app per PR, and update the corresponding app when a new commit is made on a PR. For that, we keep track of the branch name by storing it in the « notes » field of the app on Appetize.io.\n\nSo, here’s a private lane to get back the public key of the corresponding app on Appetize.io, to update the good one if it already exists.\n\n# get the publicKey of the appetizeApp corresponding to your git branch\nprivate_lane :getAppetizePublicKey do |options|\n  publicKey = &quot;&quot;\n\n  response = HTTParty.get(&quot;https://#{ENV[&quot;APPETIZE_TOKEN&quot;]}@api.appetize.io/v1/apps&quot;)\n  json = JSON.parse(response.body)\n\n  # Find branch name in notes\n  json[&quot;data&quot;].each do |value|\n    if value[&quot;note&quot;] == ENV[&quot;ghprbSourceBranch&quot;] &amp;amp;&amp;amp; value[&quot;platform&quot;] == options[:platform]\n      publicKey = value[&quot;publicKey&quot;]\n    end\n  end\n\n  publicKey\nend\n\nNow, we have everything ready to do the deployAppetize lane for iOS :\n\nplatform :ios do\n\n  desc &quot;Deployment iOS lane&quot;\n\n    lane :deployAppetize do\n\n      githubStatusUpdate(\n        context: &#39;Appetize iOS&#39;,\n        state: &#39;pending&#39;,\n        url: &quot;https://appetize.io/dashboard&quot;,\n        description: &#39;iOS build in progress&#39;\n      )\n\n      Dir.chdir &quot;../ios&quot; do\n        tmp_path = &quot;/tmp/fastlane_build&quot;\n\n        #seems not possible to use gym to do the simulator release ?\n        xcodebuild_configs = {\n          configuration: &quot;Release&quot;,\n          sdk: &quot;iphonesimulator&quot;,\n          derivedDataPath: tmp_path,\n          xcargs: &quot;CONFIGURATION_BUILD_DIR=&quot; + tmp_path,\n          scheme: &quot;#{ENV[&quot;APP_IOS_SCHEME&quot;]}&quot;\n        }\n\n        Actions::XcodebuildAction.run(xcodebuild_configs)\n\n        app_path = Dir[File.join(tmp_path, &quot;**&quot;, &quot;*.app&quot;)].last\n\n        zipped_bundle = Actions::ZipAction.run(path: app_path, output_path: File.join(tmp_path, &quot;Result.zip&quot;))\n\n        Actions::AppetizeAction.run(\n          path: zipped_bundle,\n          api_token: &quot;#{ENV[&quot;APPETIZE_TOKEN&quot;]}&quot;,\n          platform: &quot;ios&quot;,\n          note: &quot;#{ENV[&quot;ghprbSourceBranch&quot;]}&quot;,\n          public_key: getAppetizePublicKey({platform: &quot;ios&quot;})\n        )\n\n        FileUtils.rm_rf(tmp_path)\n\n      end\n\n      githubStatusUpdate(\n        context: &#39;Appetize iOS&#39;,\n        state: &#39;success&#39;,\n        url: &quot;#{lane_context[SharedValues::APPETIZE_APP_URL]}&quot;,\n        description: &#39;iOS build succeed&#39;\n      )\n    end\n\n    error do |lane, exception|\n      case lane\n        when /deployAppetize/\n          githubStatusUpdate(\n            context: &#39;Appetize iOS&#39;,\n            state: &#39;failure&#39;,\n            url: &quot;https://appetize.io/dashboard&quot;,\n            description: &#39;iOS build failed&#39;\n          )\n        end\n      end\nend\n\nFor Android, it’s almost the same things, except we have to do some small business logic to find the apk generated by Gradle, with this private lane :\n\n# find the path of the last apk build\nprivate_lane :getLastAPKPath do\n  apk_search_path = File.join(&#39;../android/&#39;, &#39;app&#39;, &#39;build&#39;, &#39;outputs&#39;, &#39;apk&#39;, &#39;*.apk&#39;)\n  new_apks = Dir[apk_search_path].reject { |path| path =~ /^.*-unaligned.apk$/i}\n  new_apks = new_apks.map { |path| File.expand_path(path)}\n  last_apk_path = new_apks.sort_by(&amp;amp;File.method(:mtime)).last\n\n  last_apk_path\nend\n\nAnd now you should be able to also deploy to Appetize.io on Android :\n\nplatform :android do\n\n  desc &quot;Deployment Android lane&quot;\n\n    lane :deployAppetize do\n\n      githubStatusUpdate(\n        context: &#39;Appetize Android&#39;,\n        state: &#39;pending&#39;,\n        url: &quot;https://appetize.io/dashboard&quot;,\n        description: &#39;Android build in progress&#39;\n      )\n\n      gradle(\n        task: &quot;assemble&quot;,\n        build_type: &quot;Release&quot;,\n        project_dir: &quot;android/&quot;\n      )\n\n      Actions::AppetizeAction.run(\n        path: getLastAPKPath,\n        api_token: &quot;#{ENV[&quot;APPETIZE_TOKEN&quot;]}&quot;,\n        platform: &quot;android&quot;,\n        note: &quot;#{ENV[&quot;ghprbSourceBranch&quot;]}&quot;,\n        public_key: getAppetizePublicKey({platform: &quot;android&quot;})\n      )\n\n      githubStatusUpdate(\n        context: &#39;Appetize Android&#39;,\n        state: &#39;success&#39;,\n        url: &quot;#{lane_context[SharedValues::APPETIZE_APP_URL]}&quot;,\n        description: &#39;Android build succeed&#39;\n      )\n    end\n\n    error do |lane, exception|\n      case lane\n        when /deployAppetize/\n          githubStatusUpdate(\n            context: &#39;Appetize Android&#39;,\n            state: &#39;failure&#39;,\n            url: &quot;https://appetize.io/dashboard&quot;,\n            description: &#39;Android build failed&#39;\n          )\n      end\nend\n\nIt’s over. You just have to add those commands to your CI to do the job :\n\nnpm install\nFastlane ios deployAppetize\nFastlane android deployAppetize\n\n\nYou have now two new checks on each PR with a link to the iOS or Android instance on Appetize.io.\n\n\n\nThe complete Fastfile on a Github Gist : FastFile\n\nConclusion\n\nAt M6web, we are glad to see the whole React Native promise taking a concrete shape: the developer experience is the same for both mobile &amp;amp; web development, even about tooling. We are continuing to play with it and we’ll certainly keep posting articles here, stay tuned !\n\nP.S.: You could look at the Fabric Blog post on the device grid for Fabric but with Danger commenting on the PR instead of Github Statuses, and iOS only.\n\nP.S.2: You could also look at Reploy.io, which try to improve this workflow with extra features and a more cleaner UX than Appetize.io, but it is “alpha” for now.\n"
} ,
  
  {
    "title"    : "M6web fera un retour d&#39;expérience sur l&#39;usage de Cassandra sur 6play le 14/06/2016",
    "category" : "",
    "tags"     : " 6tech, lyon, conference",
    "url"      : "/2016/05/25/m6web-retourdxp-cassandra.html",
    "date"     : "May 25, 2016",
    "excerpt"  : "Olivier Mansour, responsable R&amp;amp;D, sera présent au Cassandra Days le 14 Juin à Paris pour faire un retour d’expérience sur l’utilisation de Cassandra sur 6play.\n\n\n\nL’évènement est gratuit : https://www.eventbrite.co.uk/e/billets-datastax-day-pa...",
  "content"  : "Olivier Mansour, responsable R&amp;amp;D, sera présent au Cassandra Days le 14 Juin à Paris pour faire un retour d’expérience sur l’utilisation de Cassandra sur 6play.\n\n\n\nL’évènement est gratuit : https://www.eventbrite.co.uk/e/billets-datastax-day-paris-25165891860.\n\n"
} ,
  
  {
    "title"    : "Arrêtons de perdre du temps à débuguer !",
    "category" : "",
    "tags"     : " afup, php, debug, conference",
    "url"      : "/2016/05/24/arretons-de-perdre-du-temps.html",
    "date"     : "May 24, 2016",
    "excerpt"  : "Arrêtons de perdre du temps à débuguer ! Débuguer peut se révéler long et fastidieux. \nC’est du temps perdu qu’on pourrait passer à créer de la valeur ajoutée. \nC’est d’une manière ou d’une autre une perte pour le business. \nAyant commencé mon ent...",
  "content"  : "Arrêtons de perdre du temps à débuguer ! Débuguer peut se révéler long et fastidieux. \nC’est du temps perdu qu’on pourrait passer à créer de la valeur ajoutée. \nC’est d’une manière ou d’une autre une perte pour le business. \nAyant commencé mon entrée dans la vie active par une TMA, j’ai compris vite et de manière un peu brutale que ça fait pourtant partie de la vie du développeur qui devient parfois débugueur. \nQuelles solutions et astuces pouvons-nous mettre en place afin d’être plus efficace dans cette tâche rébarbative ?\n"
} ,
  
  {
    "title"    : "M6web sera présent au sfpot de Lille du 16/06/16",
    "category" : "",
    "tags"     : " 6tech, lille, sfpot, conference",
    "url"      : "/2016/05/19/6tech-sfpot-lille.html",
    "date"     : "May 19, 2016",
    "excerpt"  : "Pierre Marichez, Renaud Bougré et Nicolas Beze une partie de l’équipe PHP de M6Web Lille, vous feront part d’un retour d’expérience sur l’industrialisation des développements.\nCa parlera jenkins, gitlab, gitlab-ci, outil de gestion de projets, api...",
  "content"  : "Pierre Marichez, Renaud Bougré et Nicolas Beze une partie de l’équipe PHP de M6Web Lille, vous feront part d’un retour d’expérience sur l’industrialisation des développements.\nCa parlera jenkins, gitlab, gitlab-ci, outil de gestion de projets, api, sentry, capistrano, user scripts, docker, grafana, slack…\n\nLors de ce sfpot, Kevin Dunglas présentera le DunglasActionBundle et Alexandre Salomé et Luc Vieillescazes vous feront un retour sur le sflive 2016.\n\nAlors rendez-vous tous le 16 juin 2016 à partir de 19h00 au Liberch’ti, 169 Boulevard de la Liberté à Lille (Métro République).\n\nPour vous inscrire, ça se passe ici\n\n\n\nPour plus d’informations sur cet événement et les autres sfpot lillois, rendez-vous sur le site des Tilleuls.\n"
} ,
  
  {
    "title"    : "M6web sera présent au PHPTour Clermont-Ferrand",
    "category" : "",
    "tags"     : " 6play, afup, phptour, conference",
    "url"      : "/2016/05/09/6tech-phptour-clermont.html",
    "date"     : "May 9, 2016",
    "excerpt"  : "Fabien de Saint Pern, un des leads devs sur la plateforme 6play, aura l’occasion de présenter une conférence au PHPTour Clermont-Ferrand le 24 Mai. Il fera un retour d’expérience concret sur nos pratiques autour de la réalisation de workers asynch...",
  "content"  : "Fabien de Saint Pern, un des leads devs sur la plateforme 6play, aura l’occasion de présenter une conférence au PHPTour Clermont-Ferrand le 24 Mai. Il fera un retour d’expérience concret sur nos pratiques autour de la réalisation de workers asynchrones en PHP (et oui !).\n\nLe PHPTour est un cycle de conférences itinérant organisé par l’AFUP réunissant toutes les communautés PHP, professionnelles et open-source, dédié au langage et à son écosystème. Ne manquez pas cette conférence ainsi que cet évènement qui s’annonce particulièrement riche !\n\n"
} ,
  
  {
    "title"    : "La retrospective Agile ‘Garde à vous’",
    "category" : "",
    "tags"     : " agile, scrum",
    "url"      : "/2016/03/29/retro-agile-garde-a-vous.html",
    "date"     : "March 29, 2016",
    "excerpt"  : "Depuis quelques années les équipes d’M6Web se sont organisées autour des méthodes agiles. Scrum, Kanban, Lean, méthodes adaptées, nous nous efforçons de toujours garder en tête l’amélioration continue et le fun spirit au coeur du travail de nos éq...",
  "content"  : "Depuis quelques années les équipes d’M6Web se sont organisées autour des méthodes agiles. Scrum, Kanban, Lean, méthodes adaptées, nous nous efforçons de toujours garder en tête l’amélioration continue et le fun spirit au coeur du travail de nos équipes.\n\nAu delà des rituels “classiques”, l’équipe des scrum master cherche de temps en temps à thématiser et casser les routines en créant des jeux autour de l’agilité.\n\nNous souhaitons aujourd’hui au travers de ce blog, partager avec vous ces jeux et surtout vous permettre de les reproduire. Ainsi chaque jeu s’accompagnera de règles et d’un « kit » vous permettant d’imprimer le matériel nécessaire au bon déroulement.\n\nAu menu de ce premier jeu, nous avions choisi de profiter de l’arrivée de l’émission “Garde à vous” sur M6 afin de proposer une rétrospective pas comme les autres.\n\n\n  Nom : La rétro Garde à vous !\n  Type : Rétrospective d’équipe.\n  Durée : 1 heure.\n\n\nPunchLine :\n\nVotre équipe est sélectionnée pour une retro spéciale. Un défi difficile qui les mènera à dépasser leur limite. \nÉpreuve physique et mentale, il y en aura pour tous. Mais surtout c’est en groupe qu’ils réussiront les épreuves. :)\n\n\n\nObjectifs :\n\n\n  Changer de la rétro classique : 2 activités sur les 4 sont des mini-jeux.\n  Créer du team building : Le 1er jeu demande confiance et cohésion entre les membres de son équipe. Et accessoirement c’est très fun !\n  Stimuler les équipes entre-elles autour d’une compétition sympa (nos 5 équipes ont fait la même rétro lors de la rotation).\n  Garder à l’esprit l’amélioration continue au travers des 2 activités post-it.\n\n\nPréparation / matériel :\n\n\n  Vous trouverez ici un lien vers le kit de la rétro garde à vous.\n  Créer un décor : Mettez vos équipes dans l’ambiance et poussez l’aspect « jeux de rôle ». \nExemple : filet à chat, bâche de tente Quechua, palissade, affiche militaire, etc..\n  Costume de l’animateur : pantalon/veste militaire (demander autour de vous), cravache, casque avec lunette de ski, chemise beige, etc..\n  Pour l’étape #1 : Post-it, feutres pour écrire, l’affiche paperboard « Motivation ».\n  Pour l’étape #2 : 4 bandanas ou serviettes pour bander les yeux + gilets fluos de sécurité + un chronomètre + un parcours dans vos bureaux.\n  Pour l’étape #3 : Post-it, feutres pour écrire, l’affiche paperboard « 4 thèmes d’amélioration ».\n  Pour l’étape #4 : 2 Nerfs, 4 canettes vides, 4 peluches (ou autre), 4 balles en mousse, une poubelle, tapis de sol.\n  La feuille des scores.\n\n\n\n\nLe déroulement :\n\nVolontairement nous n’avons donné aucun détail à nos équipes sur cette rétro. \nQuelques jours avant, nous leur avons envoyé la vidéo bande annonce sous forme de « convocation ». \nLe jour même, ils ont eu la surprise de voir la salle décorée et leur scrum master déguisé.\n\nÉtape.1 - Motivation - 10 minutes :\n\nTexte possible : « Bonjour équipe [nom_équipe]. Je suis le sergent “Badass”, on vous a placé chez moi aujourd’hui pour évaluer votre trouillomètre.\n\n\n  « Cette rétro va se dérouler en 4 étapes. Comme je suis sympa, je ne vous dis rien. ça permettra de voir votre capacité d’adaptation.»\n\n\n\n  «  Sachez que nous aurons 2 épreuves physiques et 2 épreuves mentales. Lors des épreuves physiques, nous noterons vos scores afin de  déterminer quelle est la meilleure équipe du plateau. »\n\n\n\n  «  êtes vous prêts ? »\n\n\nPaperboard #1 : La motivation\n\n\n  « On va commencer doucement. Prenez vos post-it et vos crayons.»\n\n\n\n  «  Dites moi ce qui vous motive à vous levez le matin ? pourquoi vous aimez venir bosser ? »\n\n\n\n  «  Si un aspect du boulot vous ennuie, vous cloue au lit, dites le également.»\n\n\n\n  «  TimeBox : 2 minutes. »\n\n\nAu bout des 2 minutes : chacun passe au paperboard coller ses post-it et les expliquer.\n\nObjectif du scrum master :\n\n\n  récupérer les aspects positifs de l’environnement, du travail de vos équipes : ce sont des bases solides à avoir en tête et à maintenir dans le groupe.\n  récupérer les aspects négatifs : ça peut être la cantine, la distance des locaux, etc.. même si certains post-it sont difficilement ‘améliorables’ c’est toujours bien de l’exprimer.\n\n\nÉtape.2  - En avant, Marche ! - 10 minutes\n\nRègle du jeu :\n\n\n  On se met par 2. Si vous êtes un nombre impair, explication plus bas.\n  L’une des 2 personnes va avoir les yeux bandés. On l’équipe d’un gilet fluo de sécurité afin d’éviter de lui rentrer dedans… :p\n  L’autre personne devra le guider en utilisant les mots : « avance / recule / à droite / à gauche ».\n  Il est interdit de toucher son coéquipier pendant la course.\n  \n    Vous êtes chronométrés. Un classement général sera fait avec les autres équipes pour déterminer les plus rapides.\n  \n  Faites mettre le bandana ou la serviette. Prenez les guides et montrez leur le parcours.\n  Faites aligner les paires devant la ligne de départ.\n  3, 2, 1, Partez. N’hésitez pas à les encourager ou à parler fort afin de les stresser ^^.\n\n\nVersion à 3 : vous aurez 1 guide et 2 personnes avec les yeux bandés. Les 2 yeux bandés sont en file indienne.\n\nLa personne derrière pose ses mains sur les épaules sur la personne de devant. Bonne chance :p\n\nFeuille des scores : Notez le temps de chaque paires.\n\nÉtape.3 - les 4 thèmes - 20/25 minutes\n\nRègles :\n\n\n  Nous avons 4 thèmes affichés au paperboard.\n  Pour chaque thème, vous pouvez écrire au maximum 2 post-it positifs &amp;amp; 2 post-it négatifs\n\n\nNous limitons le nombre de post-it pour une question de temps. Libre à vous d’ajuster.\n\nLes thèmes sont :\n\n\n  Communication - Dialogue dans l’équipe, avec le PO, les clients finaux, utilisation des mails, etc…\n  Les outils - De développement, méthode agile, communication, de déploiement/MEP, de testing, etc..\n  Réactivité - Lors d’une demande PO, d’un incident de production, phase de cadrage avec PO, etc…\n  Leadership - Présence de votre Lead-Dev / Responsable R&amp;amp;D, écoute de vos managers, méthode d’organisation dans le travail, etc…\n\n\nNous laissons 7 minutes d’écriture de post-it (à ajuster selon vous).\nAu bout des 7 minutes : chacun passe au paperboard coller ses post-it et les expliquer.\n\nObjectif du scrum master :\n\n\n  Récupérer des axes d’amélioration de l’équipe.\n  Conclure sur les aspects positifs et définir les post-it négatifs sur lesquels on cherche à agir en 1er.\n  Éviter de définir les actions à mettre en oeuvre pour les post-it négatifs. Faites le en dehors sinon ça prendra trop de temps et cassera la dynamique.\n\n\nÉtape.4 - Duck Hunt - 15 minutes\n\nRègles : 3 stands sont proposés\n\n\n  Chacun choisit un stand. Les participants peuvent faire un essai rapide si c’est demandé.\n  Nous vous laissons ajuster la distance entre les cibles et le joueur.\n  Stand 1 - Le grenadier : 4 grenades (balle en mousse), une poubelle =&amp;gt; lancer les grenades dans la poubelle. 4 essais, 1 réussite = 1 point\n  Stand 2 - le chasseur : 4 canettes, un nerf =&amp;gt; toucher les canettes. 4 essais, 1 réussite = 1 point\n  Stand 3 - le sniper : 4 peluches éléphant PHP, un gros nerf =&amp;gt; mode allongé dans les bois, 4 essais, 1 réussite = 1 point\n\n\nCeci afin de répartir les personnes sur plusieurs stands.\n\nFeuille des scores : Notez le score de chacun.\n\n\n\nConclusion :\n\nAfficher le tableau des scores et féliciter tout le monde. \nNext step :\n\n\n  Les résultats des équipes seront affichés le lendemain / fin de journée / autres (à vous de voir)\n  Les post-it négatifs de l’étape 1 et 3 seront pris en compte par les scrum master qui travailleront avec les personnes adéquates pour continuer à s’améliorer. Vous pouvez ajouter à votre DSK (Do, Store, Keep) certaines actions.\n\n\nN’hésitez pas à nous envoyer vos feedbacks sur ce jeu.\n\nForce et Scrum !\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un développeur player vidéo JavaScript (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/2016/01/26/m6web-lyon-recherche-developpeur-player-video-web-h-f-en-cdi.html",
    "date"     : "January 26, 2016",
    "excerpt"  : "Mise jour : Le poste n’est à plus pourvoir. Merci\n\nAu sein de la team Tube (équipe Lecteur Vidéo), en charge entre autre du lecteur de 6play et des lecteurs vidéos des autres portails Internet d’M6 Web (Clubic.com, Deco.fr, …), vous participez à l...",
  "content"  : "Mise jour : Le poste n’est à plus pourvoir. Merci\n\nAu sein de la team Tube (équipe Lecteur Vidéo), en charge entre autre du lecteur de 6play et des lecteurs vidéos des autres portails Internet d’M6 Web (Clubic.com, Deco.fr, …), vous participez à la conception technique et au développement de nos lecteurs vidéos.\n\nVous maitrisez les problématiques et les technologies Web :\n\n\n  ECMAScript 2015 « ES6 »\n  VideoJS\n  Gulp / Grunt\n  Les outils de tests (Jasmine, qUnit, Jest, PhantomJS, BrowserStack …)\n\n\nVous avez une bonne connaissance des problématiques vidéos :\n\n\n  Les nombreux formats en diffusion continue (streaming) mais aussi en téléchargement progressif (Progressive Download)\n  Les contraintes d’encodage vidéo\n  Les problématiques autour de la sécurité (le chiffrement, les DRMs du marché, …)\n\n\nPar ailleurs, vous avez déjà eu à travailler sur l’intégration de formats publicitaires.\n\nEnfin, une connaissance d’ActionScript sera appréciée.\n\nVous aurez des interactions avec les équipes Produit de Paris, ainsi qu’avec nos autres développeurs basés à Lille.\n\nLe profil recherché se caractérise par :\n\n\n  Une très forte sensibilité sur les sujets Vidéo et Qualité de Service\n  Un goût prononcé pour l’innovation\n  Une bonne culture du web et du monde du numérique\n  Une aptitude à la prise d’initiatives, un grand dynamisme, une curiosité et une ouverture d’esprit\n  Une connaissance (idéalement validée par une première expérience) des Méthodes Agiles (Scrum), et une culture de l’amélioration continue\n\n\nPour postuler : https://www.groupem6.fr/ressources-humaines/offres-emploi/developpeur-frontend-javascript-video-h-f-258357.html\n\n"
} ,
  
  {
    "title"    : "On a testé fonctionnellement notre app JS",
    "category" : "",
    "tags"     : " tests fonctionnels, javascript, phantomjs, webdriver, Cytron",
    "url"      : "/2016/01/25/tests-fonctionnels-app-js.html",
    "date"     : "January 25, 2016",
    "excerpt"  : "L’utilité des tests fonctionnels pour les applications web n’est plus à démontrer (comment ça, vous ne testez pas encore vos apps ?). Malheureusement, tout ne peut pas être totalement testé fonctionnellement, ou de façon aisée : je pense par exemp...",
  "content"  : "L’utilité des tests fonctionnels pour les applications web n’est plus à démontrer (comment ça, vous ne testez pas encore vos apps ?). Malheureusement, tout ne peut pas être totalement testé fonctionnellement, ou de façon aisée : je pense par exemple au player chez nous, un composant stratégique mais pauvrement testé fonctionnellement de par sa nature un peu hybride (mélange de flash et de JS). Dans tous les cas, pour ce qui peut l’être, nous sommes partisans dans l’équipe Cytron d’user sans mesure (ou presque !) de cet outil de manière à être le plus zen possible au moment d’appuyer sur le bouton “deploy”.\n\nQuelle stack ?\n\nNotre application est codée en JS isomorphique (ou Universal JS) grâce à React et Node.js.\n\nPour les tests fonctionnels, nous utilisons le trio Cucumber.js + WebdriverIO + PhantomJS :\n\n\n  Cucumber.js est l’outil qui permet de dérouler la suite de tests écrits dans la syntaxe Gherkin,\n  WebdriverIO permet d’interfacer les tests traduits en JS avec un serveur Selenium (dialoguant grâce au protocole WebDriver Wire et permettant de contrôler un browser),\n  PhantomJS est le browser dans lequel les scénarios de tests seront exécutés, il embarque son propre serveur Webdriver, Ghostdriver.\n\n\nToutes nos Pull Requests lancent les tests indépendamment via Jenkins dans un environnement “dockerisé”, donc complètement autonome et isolé. De façon à respecter ce principe jusqu’au bout et à ne pas dépendre de données versatiles, nos API sont aussi mockées grâce à superagent-mock.\n\nSetup\n\nArborescence\nDans notre projet, nous avons un dossier pour les tests fonctionnels organisés comme suit :\n\n├─┐ tests\n│ ├─┐ step_definitions\n│ │ └── my_feature.steps.js\n│ ├─┐ screenshots\n│ │ └── my_scenario.png\n│ ├─┐ support\n│ │ ├── config.json\n│ │ ├── constants.json\n│ │ ├── hooks.js\n│ │ └── world.js\n│ └── my_feature.feature\n\nFeatures\nUne feature est un fichier testant une fonctionnalité de l’application et regroupant plusieurs scénarios de test. Il est écrit en langage naturel (Gherkin) de façon à être lisible par tous.\n\n# tests/support/cookie.feature\nFeature: Scenarios about the cookie banner\n\n  Scenario: See the cookie banner and close it\n    Given My browser storage is empty\n    When I visit the &quot;homepage&quot; page\n    Then I should see the &quot;cookie banner&quot;\n\n    When I click on &quot;Accept cookie&quot;\n    Then I should not see a &quot;cookie banner&quot;\n\n    When I visit the &quot;homepage&quot; page\n    Then I should not see a &quot;cookie banner&quot;\n\nWorld\nLe fichier world.js est le point de départ pour Cucumber.js. C’est ici que nous initialisons WebdriverIO et que nous mettons un place un contexte qui sera disponible pour tous les tests.\n\n// tests/support/world.js\nvar Webdriver = require(&#39;webdriverio&#39;);\nvar config = require(&#39;./config.json&#39;);\nvar assert = require(&#39;assert&#39;);\n\nvar browser = Webdriver.remote({\n  logLevel: config.logLevel || &#39;silent&#39;,\n  host: config.webdriver.host,\n  port: config.webdriver.port,\n  waitforTimeout: config.waitTimeout,\n  desiredCapabilities: {browserName: &#39;phantomjs&#39;}\n});\n\nfunction WorldConstructor() {\n  var world = {\n    browser: browser,\n\n    // Global visit method\n    visit: function (baseUrl, params) {\n      var pathUrl = url.format({\n        pathname: baseUrl,\n        query: params\n      });\n\n      return this.browser.url(pathUrl);\n    },\n\n    // Take screenshot\n    screenshot: function (filename) {\n      return browser.saveScreenshot(path.join(config.screenshot.path, filename));\n    },\n\n    assert: {\n      /**\n       * Assert if element(s) are visible\n       *\n       * @param selector    {String}   Can be query multiple DOM elements\n       * @param failMessage {String}   Fail message if no visible\n       */\n      visible: function (selector, failMessage) {\n        // ...\n      },\n    }\n\n    // ...\n  }\n\n  return world;\n}\n\nmodule.exports = WorldConstructor;\n\nHooks\nCucumber.js permet de déclencher des traitements sur certains évènements clés lors de l’exécution de la suite de tests. Nous utilisons ce système pour réaliser une capture d’écran sur chaque scénario de test en échec qui viendra s’ajouter dans le dossier screenshots.\n\n// tests/support/hook.js\nvar config = require(&#39;./config.json&#39;);\nvar sprintf = require(&#39;sprintf-js&#39;).sprintf;\n\nmodule.exports = function () {\n  this.Before(function (scenario) {\n    return this.browser.init().then(function () {\n      return this.browser.setViewportSize({\n        width: config.screenshot.width,\n        height: config.screenshot.height\n      });\n    }.bind(this));\n  });\n\n  this.After(function (scenario) {\n    if (scenario.isFailed()) {\n      return this.screenshot(sprintf(\n        &#39;%s_%d.png&#39;,\n        scenario.getName().toLowerCase().replace(&#39; &#39;, &#39;-&#39;),\n        new Date().getTime()\n      )).then(function () {\n        return this.browser.end();\n      }.bind(this));\n    } else {\n      return this.browser.end();\n    }\n  });\n};\n\nStep definitions\nCe sont les fichiers qui font le lien entre les features (écrit en langage naturel) et WebdriverIO (initialisé dans world.js).\n\n// tests/step_definitions/cookie.steps.js\nvar sprintf = require(&#39;sprintf-js&#39;).sprintf;\n\nmodule.exports = function () {\n  /**\n   * Visit a page\n   *\n   * @param page {String}\n   *\n   * @require config routes object\n   */\n  this.When(/^I visit the &quot;([^&quot;]*)&quot; page$/, function (page) {    \n    return this.visit(this.getRoute(page)).then(function () {\n      return this.assert.existing(&#39;#__main&#39;, &#39;React application is not loaded.&#39;);\n    }.bind(this));\n  });\n\n  /**\n   * I click on &quot;label&quot;\n   *\n   * @param label {String}   DOM selector label\n   */\n  this.When(/^I click on &quot;([^&quot;]*)&quot;$/, function (label) {\n    var selector = this.getDOMSelector(label);\n    \n    return this.action.click(selector);\n  });\n\n  /**\n   * Assert element matching the given selector is visible.\n   *\n   * @param label {String}\n   *\n   * @require config DOMSelectors object\n   */\n  this.Then(/^I should see a &quot;([^&quot;]*)&quot;$/, function (label) {\n    var selector = this.getDOMSelector(label);\n    var failMessage = sprintf(&#39;%s is not visible&#39;, label);\n    \n    return this.assert.visible(selector, failMessage);\n  });\n\n  // ...\n}\n\nDesign\nNous n’avons pas mis en œuvre le pattern Page Object. Ce n’était pas un choix délibéré mais le contexte et les enjeux du projet nous ont fait passer à côté, ou ce n’était peut être simplement pas le moment. Malgré tout, nous avons tenté de rationaliser au mieux l’organisation du code. Par exemple, afin de ne pas se retrouver avec des sélecteurs CSS éparpillés dans plusieurs fichiers de “features” ou de “step definitions”, nous avons choisi de les regrouper dans un fichier constants.json et d’utiliser seulement des labels ailleurs. Nous faisons le lien entre le label et le sélecteur CSS avec la méthode getDOMSelector, visible ci-dessus et définie dans le fichier world.js.\n\nRun\nPour lancer les tests, il faut :\n\n\n  lancer le serveur de l’app en local (l’URL du serveur est paramétrable dans le fichier de config),\n  lancer un phantomjs en mode webdriver phantomjs --webdriver=5024 où 5024 est le port du serveur (également configurable dans config.json),\n  lancer une suite de tests via Cucumberjs, au choix :\n    \n      tous les tests cucumberjs tests/,\n      une feature cucumberjs tests/cookie.feature,\n      un scénario cucumberjs tests/cookie.feature:3 où 3 correspond à la ligne du début du scénario ciblé dans le fichier cookie.feature.\n    \n  \n\n\nParticularité de l’isomorphisme\n\nDeux chemins sont possibles avec l’isomorphisme. Soit l’utilisateur arrive directement sur la page, auquel cas celle-ci sera générée sur le serveur, soit il y arrive en naviguant sur l’app et c’est le client qui aura exécuté le code. Il faut tester ces deux cas car le code concerné n’est pas toujours le même (la variable window par exemple n’est pas accessible côté serveur).\n\nIl est bien sûr impossible d’être exhaustif. L’idée est d’abord de couvrir les cas les plus fréquents et les plus critiques pour l’application. Ensuite, il faut s’astreindre à ajouter un test à chaque fois qu’un bug est détecté de façon à s’assurer qu’on ne le rencontrera plus dans le futur.\n\nPhantomJS, la stabilité en question…\n\nBasé sur Webkit, PhantomJS est le plus connu des navigateurs headless, c’est-à-dire exécutables sans interface visuelle. D’autres navigateurs légers et créés pour les tests fonctionnels existent comme SlimerJS (basé sur Gecko et pas vraiment headless) ou Zombie.js (pas de moteur de rendu). Cependant aucun n’offre toutes les fonctionnalités de PhantomJS qui se rapprochent le plus d’un vrai browser. Il émule de façon transparente tout le rendu graphique avec la possibilité de réaliser des screenshots par exemple ou de tester la visibilité d’un élément du DOM (non opaque, dans le viewport, sur la couche z-index la plus haute…).\n\nNéanmoins celui-ci n’intègre pas toutes les dernières avancées en terme de JS et de CSS. Flexbox n’est par exemple pas pris en charge ce qui nous a posé quelques problèmes sur les vérifications liées à la visibilité des éléments. Sa version 2.0 qui date de début 2015, malgré la bonne volonté des contributeurs, n’a toujours pas de build officiel sous Linux, ce qui oblige à compiler les sources sur sa machine de tests ou à trouver sur le net un build officieux correspondant à sa distribution. C’est ce que nous avons fait via M6Web/phantomjs2. Cependant, l’outil est assez instable (builds officiels ou pas) et nous avons rencontré beaucoup de crashs aléatoires ou reproductibles mais incompréhensibles (dus par exemple à l’ajout de quelques lignes de CSS anodines…).\n\nEn local, sur sa machine, PhantomJS est encore moins stable que sur Jenkins. Il semblerait qu’exécution après exécution, il garde des “choses” en cache quelque part qui, à terme, produisent des crashs systématiques de l’outil. Nous n’avons pas réussi à établir un scénario reproductible qui nous permette de poser une issue sur le projet. N’hésitez pas à réagir en commentaire si vous vous êtes trouvé dans un cas similaire.\n\nPour régler temporairement ce problème, nous avons utilisé l’image docker de Gabe Rosenhouse pour le faire tourner dans un environnement indépendant mais ce n’est pas faciliter la vie des développeurs qui veulent juste lancer des tests sans avoir à mettre en œuvre une usine à gaz derrière.\n\nEdit: hier, la version 2.1 de PhantomJS a (enfin) été publiée avec un build pour chaque plateforme. Plusieurs de nos soucis pourraient être réglés avec cette nouvelle release, à suivre…\n\nChrome+ChromeDriver, une alternative ?\n\nNous avons alors opté pour la solution Chrome+ChromeDriver. ChromeDriver a le rôle du serveur Selenium qui permet de faire communiquer WebriverIO avec Chrome. Les avantages de cette stack sont multiples. D’abord, l’ensemble est beaucoup plus stable, fini les crashs impromptus. Ensuite, le debug des tests en échec est bien plus aisé : on voit en effet la suite se jouer en temps réel dans son navigateur, on peut ainsi tout à fait mettre un point d’arrêt et utiliser la console de développement. Enfin, on utilise la version de Chrome que l’on souhaite, donc plus de problème de CSS non supportés.\n\nAlors pourquoi se cantonner à n’utiliser Chrome+ChromeDriver qu’en local et pas en intégration continue sur Jenkins ? Chrome n’est pas un browser headless et a besoin d’une interface visuelle qui n’est pas disponible sur Jenkins. Il existe des solutions pour simuler un affichage graphique avec Xvfb par exemple. Nous avons tenté de mettre en place une telle stack sur l’image docker utilisée pour créer notre environnement de test sur Jenkins en se basant sur l’image de Rob Cherry. Malheureusement, après y avoir consacré un peu d’énergie, le résultat n’a pas été au rendez-vous car :\n\n\n  l’exécution des tests dans Chrome est bien plus lente que sur PhantomJS (2 à 3 fois plus lent), notre intégration continue prenant déjà plus de 10 minutes sur ce projet,\n  il semble difficile d’obtenir ici aussi une stabilité du dispositif, les sessions Webdriver étaient souvent perdues, sans que nous en trouvions la cause.\n\n\nCes raisons nous ont conduit à abandonner cette piste.\n\nQuelques tips pour améliorer la stabilité de ses tests\n\nNous avons continué d’espérer avoir une stack stable pour nos tests fonctionnels. Avec persévérance, nous pouvons dire qu’à l’heure actuelle grâce à ces quelques tips, nous avons une plateforme de test stable (à 99%) !\n\nwaitUntil\nC’est la première chose à faire et la plus importante de notre point de vue. On ne sait jamais vraiment quand un élément s’affichera dans la page car son chargement dépend de trop de facteurs non prédictibles (la connexion, l’utilisation cpu, gpu, mémoire, etc.). Sur notre projet, nous avons par exemple beaucoup d’animations CSS qui retardent le timing d’apparition des pages et des éléments du DOM. Notre première approche a été de rajouter des sleep un peu de partout dans nos tests. Chose à ne pas faire. L’usage des sleep doit être cantonné à des cas très spécifiques. Pour tout le reste, il faut user et abuser du waitUntil de WebdriverIO, que ce soit pour des actions ou des vérifications dans la page, et en adaptant le timeout à votre projet (certaines de nos animations sont assez longues).\n\nrollover\nUn autre problème que nous avons rencontré est la bonne exécution des rollovers. En utilisant la méthode moveToObject pour pointer la souris sur un élément, il nous arrivait que le comportement “hover” ne soit pas déclenché, mettant en échec la suite du test. Nous avons donc changé notre manière d’effectuer le rollover : on répète l’action grâce au waitUntil tant que l’élement devant apparaître au hover n’est pas visible.\n\nNous n’écrivons plus\n\nI rollover the &quot;Header login icon&quot;\n\nmais\n\nI rollover the &quot;Header login icon&quot; to make &quot;Submenu&quot; appear\n\nrerun\n“Rerun” est une fonctionnalité existante sur d’autres frameworks de tests fonctionnels tel que Behat et créée pour les tests récalcitrants encore instables. Elle permet de stocker dans un fichier texte la liste des scénarios en échec pour les relancer ensuite afin de vérifier qu’ils le sont réellement. Nous avons mis en place ce process sur Jenkins, bien qu’il y ait quelques subtilités qui ne facilitent pas la tâche (mais qui devraient être bientôt corrigées), et nous en sommes satisfaits.\n\nisVisible\nA nos débuts, nous avons eu quelques problèmes avec la fonction isVisible de WebdriverIO car les éléments opaques ou en dehors du viewport étaient considérés comme visibles. Nous avons alors choisi d’utiliser une fonction custom injectée via execute. Récemment, dans la version 3 de WebdriverIO, la fonction isVisibleWithinViewport a fait son apparition mais nous n’avons pas encore tenté de l’utiliser dans nos tests.\n\nCet article est un retour d’expérience sur notre usage des tests fonctionnels sur un projet précis mais il est loin d’exposer des vérités absolues. Si vous avez des remarques ou n’êtes pas d’accord avec certaines choses, n’hésitez pas à nous le faire savoir !\n"
} ,
  
  {
    "title"    : "L&#39;envers du décor du nouveau 6play",
    "category" : "",
    "tags"     : " 6play, REST, Symfony, Elasticsearch, Cassandra",
    "url"      : "/2015/11/30/beta-nouveau-6play-backend.html",
    "date"     : "November 30, 2015",
    "excerpt"  : "Il y a quelques semaines, nous vous parlions ici même de la stack technique mise en place pour le nouveau front web de 6play.\n\nAujourd’hui, nous vous proposons un retour sur ce qui a été mis en place côté backend pour assurer la mise à disposition...",
  "content"  : "Il y a quelques semaines, nous vous parlions ici même de la stack technique mise en place pour le nouveau front web de 6play.\n\nAujourd’hui, nous vous proposons un retour sur ce qui a été mis en place côté backend pour assurer la mise à disposition des données aux différents frontaux 6play.\n\nTout d’abord, il faut commencer par expliquer que l’univers 6play ne se résume pas que à son application web. Il existe aussi une version iOS et Android, mais également une version par Box IPTV (disons une version par FAI).\n\nPas mal de REST …\n\nC’est donc tout naturellement que nous sommes partis sur la mise à disposition d’une API REST permettant à ces différents fronts de consommer simplement les données.\n\nNotre stack technique habituelle côté backend étant Symfony2, nous sommes donc partis sur ce framework, ainsi que les habituels bundles :\n\n\n  FOSRestBundle pour la gestion simple des controlleurs REST (validation des paramètres, routing adapté, view au format JSON, gestion des retours d’erreur)\n  BazingaHateoasBundle pour intégrer les liens entres les différents endpoints directement dans les différentes réponses.\n  NelmioApiDocBundle pour proposer une documentation complète et auto-générée depuis le code\n\n\nPour sécuriser tout ça, nous utilisons toujours notre bundle DomainUserBundle permettant de sécuriser et contextualiser les données par sous-domaine (voir notre article dédié à ce bundle).\n\n… mais pas que\n\nUne fois mise en place la théorie brute, nous nous sommes heurtés à la réalité des choses : face à un modèle de données complexe, si on reste très strict face à la philosophie RESTful, cela peux demander aux clients de réaliser un nombre conséquent de requêtes afin d’afficher une simple page.\n\nAinsi, nous avons un second applicatif, que nous nommons “middleware” qui est un hybride entre une API REST et un catalogue de données préformaté. Dans cet applicatif, nous réalisons les agrégations qui permettent de récupérer de manière unifiée les données liées, permettant aux frontaux de réduire leurs appels.\n\nDans ce middleware, nous essayons tout de même de respecter au maximum les verbes HTTP et le format de retour pour que les utilisateurs de ces API obtiennent des réponses cohérentes d’un service sur l’autre.\n\nDes données élastiques\n\nPour que ce middleware puisse retourner des données qui sont stockées dans plusieurs tables, de manière rapide, tout en gérant les contraintes de données non publiées (notre SI contient les anciennes émissions diffusées, mais également celles à diffuser), nous avons fait le choix d’utiliser Elasticsearch en le remplissant avec les données “publiables”.\n\nNon seulement nous disposons d’un système de recherche de données très performant, permettant des requêtes très puissantes et très rapides, dans lequel les données sont stockées de manière optimisée pour l’utilisation (pas de forme normale à respecter), mais nous nous permettons de n’y stocker que les données disponibles publiquement, simplifiant donc grandement les requêtes sur ces données.\n\nWorkerize all the things\n\nPour maintenir les données à jour dans cet index Elasticsearch, nous avons mutualisé sur l’expérience et le travail que nous avions réalisé pour RisingStar, qui nous a apporté l’expérience que des daemons sont beaucoup plus efficaces que des crons. Cette technique nous apporte plusieurs avantages :\n\n\n  Scalabilité : il est facilement possible de multiplier les process qui traitent les données, et donc d’augmenter la capacité de traitement\n  Rapidité : le fait d’avoir des daemons qui tournent en continue permet de traiter les demandes dès leur arrivée, et pas lors de la minute suivante. Cela permet aussi de lisser au maximum les traitements sans créer de piles d’attente inutiles.\n\n\nNous nous sommes donc appuyés sur notre DaemonBundle pour mettre en place un double système d’indexation :\n\n\n  une fois par jour, l’index est complétement reconstruit\n  un daemon tourne en continue pour détecter les modifications en base de données, et envoyer des messages dans une file RabbitMQ\n  un dernier daemon est dédié au traitement des messages de cette file pour mettre à jour de manière ciblée les données dans Elasticsearch\n\n\nAinsi, nous assurons une fraicheur des données quasi-immédiate et optimale.\n\nAu cours de ce travail, nous avons construit 2 nouveaux bundle : ElasticsearchBundle et AmqpBundle. L’un comme l’autre sont des bundles permettant de faciliter la configuration et l’utilisation des clients natifs dans Symfony2, en tant que service.\n\nEt la grosse donnée ?\n\nSi vous avez essayé la nouvelle version web de 6play, vous avez certainement remarqué que la personnalisation de votre compte est fortement mise en avant. Pour stocker ce fort volume de données, nous avons fait le choix d’utiliser Cassandra, pour son approche distribuée permettant une forte scalabilité, et un ratio rapidité/redondance optimal.\n\nComme pour le reste, nous avons là aussi créé un bundle Symfony2 permettant de configurer et manipuler simplement des clients Cassandra en tant que service : CassandraBundle\n\nTout le reste\n\nCôté monitoring, pour respecter nos bonnes habitudes, nous utilisons toujours Statsd à outrance, surtout via notre bundle StatsdBundle.\n\nCôté tests, tous les tests unitaires ont été écrits avec atoum.\n\nConclusion\n\nAu cours de ce projet, nous avons eu l’occasion de transformer l’essai de beaucoup de choses que nous avions faites pour RisingStar, de découvrir de nouvelles technos et de mettre en place une architecture moderne et adaptée aux nouveaux challenges des fronts.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #9",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2015/11/06/m6web-dev-facts-9.html",
    "date"     : "November 6, 2015",
    "excerpt"  : "C’est le digital\n\n\n  On a le doigt quelque part …\n\n\nUne vague histoire de pouce\n\n\n  Sache que sans ton pouce, je ne fais rien. (et non, c’est pas dégueulasse)\n\n\nJe croyais que ça rendait sourd plutôt\n\n\n  N’empeche je me trompe toujours quand je li...",
  "content"  : "C’est le digital\n\n\n  On a le doigt quelque part …\n\n\nUne vague histoire de pouce\n\n\n  Sache que sans ton pouce, je ne fais rien. (et non, c’est pas dégueulasse)\n\n\nJe croyais que ça rendait sourd plutôt\n\n\n  N’empeche je me trompe toujours quand je lis pom.xml hein\n\n\nDyslexie\n\n\n  \n    X : Il parait que Marven n’arrive pas à hériter de la config de java par défaut °°\n    Y : marven ? connais pas\n    Z : c’est l’équivalent Java de comrposer :)\n  \n\n\nRégime Fifa\n\n\n  Mince, j’ai oublié de finir de manger …\n\n\nLa faille\n\n\n  Le plus long dans cette MEP, ça va être de se connecter au Wifi\n\n\nSoif\n\n\n  \n    X : Rappel important : on a 12 litres de rosé à boire avant vendredi prochain (et un pack de heinekein).\n    X : si on ne s’y prend pas en avance ça va être la panique\n  \n\n\nLa prod de la dev ?\n\n\n  Est ce qu’on a la base de données de dev de prod ?\n\n\nError or not error\n\n\n  \n    X : c’est parce que tu pars du principe que le message d’erreur a un rapport avec l’erreur\n    Y : euh ce qui devrait être le cas non ?\n    X : ouais, mais bon…\n  \n\n\nC’était un message digital\n\n\n  \n    X : pour les massage d’erreur\n    Y : Les massages d’erreur ?\n    Z : humm les massages\n    Y : La première partie me tente, mais d’erreur ?\n    X : un laius digital\n    Y : oula … massage … digital\n  \n\n\nEn Famille\n\n\n  Les mecs ils regardent leur code ! On dirait que c’est leur photo de famille\n\n\nDeployception\n\n\n  Pour tester le deployer, il faut le deployer en test, mais pour ça, il faut deployer le deployer en prod pour pouvoir deployer en test\n\n\nFormalisme\n\n\n  En tant qu’ordinateur ayant accès à l’INTERNET MONDIAL et voulant naviguer sur le site internet de M6 se nommant 6PALY.FR mettre un titre dans la balise &amp;lt;title&amp;gt; … merci\n\n\nLes gouts et les couleurs\n\n\n  Moi, je ne fais pas confiance aux technos qui ont moins de 15 ans. Sauf si elles viennent du Brésil\n\n\nL’heure est à la blague\n\n\n  C’est l’heure dammer …\n\n\nUne vérité vrai\n\n\n  Quand on dit “24h” généralement c’est 24h\n\n\nAuto-troll\n\n\n  J’ai créé un fichier “echoUrl” qui écrit dans un fichier …\n\n\nIncohérence cohérente\n\n\n  Comme ça, on est cohérent dans l’incohérence !\n\n"
} ,
  
  {
    "title"    : "La bêta du nouveau 6play est disponible",
    "category" : "",
    "tags"     : " 6play, react, isomorphic, javascript, flux",
    "url"      : "/2015/10/21/beta-nouveau-6play-react-isomorphic.html",
    "date"     : "October 21, 2015",
    "excerpt"  : "Nous vous parlions en fin d’année dernière sur ce blog, de notre vision de la Single Page App parfaite.\n\nNous avons donc travaillé depuis le début d’année à la mise en place du nouveau 6play sur cette stack technologique :\n\n\n  React(isomorphic/uni...",
  "content"  : "Nous vous parlions en fin d’année dernière sur ce blog, de notre vision de la Single Page App parfaite.\n\nNous avons donc travaillé depuis le début d’année à la mise en place du nouveau 6play sur cette stack technologique :\n\n\n  React(isomorphic/universal) avec du Node.js en backend\n  Fluxible pour la gestion de Flux client et serveur\n  webpack pour la gestion du build js coté client\n  React Router pour le routing\n  ES6 avec Babel parce que.\n\n\nAu niveau tests, et parce que nous ne concevons plus de développer de tels projets sans une approche qualité complète :\n\n\n  ESLint pour le respect des conventions de codage\n  Jest pour les tests unitaires\n  Cucumber.js, WebdriverIO et PhantomJS pour les tests fonctionnels\n  superagent-mock (\\o/) pour mocker les requêtes HTTP des services externes\n  Jenkins pour l’intégration continue\n  React Hot Loader pour améliorer la DX (Developer eXperience)\n\n\nDepuis lundi, vous pouvez désormais tester la bêta de ce service vidéo à l’adresse suivante : https://beta.6play.fr\n\n\n\nPour ceux qui veulent en savoir plus sur cette refonte (notamment front-end), une conférence sera tenue par Kenny Dits (@kenny_dee) lors de Blend Web Mix, le 29 octobre à Lyon à 16h.\n"
} ,
  
  {
    "title"    : "Simplifiez vous la vie avec les hooks Git",
    "category" : "",
    "tags"     : " git, hooks, workflow, composer, coke",
    "url"      : "/2015/09/09/hooks-git.html",
    "date"     : "September 9, 2015",
    "excerpt"  : "La stack de base de tout projet un minimum sérieux à tendance à devenir de plus en plus lourde.\n\nChez M6Web, et particulièrement dans la team Burton, nous développons principalement des projets Symfony2.\nCette stack est donc composée, entre autre ...",
  "content"  : "La stack de base de tout projet un minimum sérieux à tendance à devenir de plus en plus lourde.\n\nChez M6Web, et particulièrement dans la team Burton, nous développons principalement des projets Symfony2.\nCette stack est donc composée, entre autre de coke et de composer.\n\nConcernant coke, l’idée est de ne jamais versionner de code qui ne respecte pas les standards de développements.\nInterdit les commits “fix standards” ou autre “fix coke” qui alimentent une PR avant d’être rebasés !\n\nConcernant composer, il s’agit de toujours travailler sur la “bonne version” des dépendances. \nCertes, composer (via le composer.lock) permet d’être sûr que toute personne qui lance un composer install aura la même version des dépendances.\nToutefois, il faut encore penser à lancer cette commande, surtout lorsque l’on récupère du code depuis le repository central où le composer.lock peut avoir évolué.\n\nPour répondre à ces besoins, nous nous sommes basés sur l’excellent système de hooks de git et nous avons développé 2 petits scripts.\n\ncheck-coke.sh\n\nCoke est un petit utilitaire qui facilite le lancement de PHP_CodeSniffer sur son projet.\nToutefois, ce dernier est relativement lent, surtout sur les projets composés d’un grand nombre de fichiers.\nPour optimiser son exécution, le script “check-code.sh” se charge de lancer coke uniquement sur les fichiers en cours de modification d’un point de vue Git.\nAinsi, son exécution est extrêment rapide, ce qui permet de l’exécuter en pre-commit pour ne jamais commiter un code ne respectant pas les standards de développement\n\ncheck-composer\n\nComme indiqué plus haut, le soucis avec composer est qu’il faut penser à lancer la commande composer install pour s’assurer que les dépendances sont à jour.\nNon seulement cette commande prend un certains temps, mais il faut surtout penser à la lancer, même pour des actions qui semblent anodines, comme changer de branche.\n\nPartant de ce constat, nous avons créé le script “check-composer.sh”, qui vérifie s’il y a une différence entre la version de départ et d’arrivée du fichier composer.lock, et qui lance la commande composer install si nécessaire.\n\nN’hésitez pas à les essayer et nous faire part de vos retours, voir de proposer vos hooks.\nLe but de ce repository partagé est de nous simplifier la vie en nous permettant de ne plus penser aux outils qui sont autour de notre code, mais de nous concentrer sur ce que nous avons à faire.\n"
} ,
  
  {
    "title"    : "CR React Europe Conférence 2015 - Day 2",
    "category" : "",
    "tags"     : " javascript, react, reactnative, video, graphql",
    "url"      : "/2015/07/10/cr-react-europe-2015-day-two.html",
    "date"     : "July 10, 2015",
    "excerpt"  : "Après une première journée pleine de nouveautés et d’annonces, voici la suite du compte rendu avec un programme encore très chargé pour cette deuxième journée de la React Europe.\n\nImproving Your Workflow With Code Transformation\n\n\n(crédits : Fabie...",
  "content"  : "Après une première journée pleine de nouveautés et d’annonces, voici la suite du compte rendu avec un programme encore très chargé pour cette deuxième journée de la React Europe.\n\nImproving Your Workflow With Code Transformation\n\n\n(crédits : Fabien Champigny - React Europe)\n\nNous commençons la journée avec le créateur du fameux Babel: Sebastian McKenzie.\nBabel est un transpiler JS permettant de transformer le code ES6/7 en code ES5.\nAprès un petit historique sur le nom, car cet outil s’appelait 6to5 avant l’arrivée d’ES7, pour finalement se renommer Babel :)\nL’adoption par la communauté a ensuite été assez massive !\n\nSébastian nous explique le fonctionnement interne de Babel avec le découpage en 3 sections : Parser / Transformer / Generator\nIls utilisent l’AST (abstract syntax tree) pour avoir une “data structure” du code et pouvoir faire des traitements sur cette structure.\nOn rentre très (trop ?) en profondeur dans les bas-fond de Babel, afin de partager les différentes difficultés et trucs et astuces pour les transformations que Babel réalise.\n\nLe talk se finit sur le futur de Babel, qui sera à chercher du côté de :\n\n\n  Dead code elimination/minification\n  constant folding/static evaluation\n  static analysis / linting\n\n\nSteven Lusher, l’un des dev Facebook travaillant sur Relay vient de mettre un blog post sur le site Babel concernant l’utilisation ES6 de React\n\n\n\nThe State of Animation in React\n\n\n(crédits : Fabien Champigny - React Europe)\n\n\n\nCheng Lou fait le point sur les animations en React et nous présente sa nouvelle approche react-motion.\n\nIl est convaincu qu’il faut abandonner les ReactCssTransitionGroup au profit des animations basées sur des interpolations “à-la-flash”.\n\nCSS Transitions\nLes Transitions CSS présentent plusieurs défaut, elles sont difficiles à controler et elles sont étroitement liées au DOM. En revanche, elles sont plutôt performantes, non bloquantes et répondent à la plupart des usages.\n\nDeclarative Tweens\nLes Declarative Tweens sont une solution alternative interessante qui permettent de composer une animation selon des critères précis (début, durée, direction, …). Cette solution présente aussi l’avantage de pouvoir créer des animations composées de plusieurs sous-animations et d’interrompre leur execution sur demande.\n\nSpring\nCheng Lou a développé une librairie appelée &amp;lt;Spring /&amp;gt; qui parlera aux anciens Flasheurs, tout comme lui, puisqu’elle reprend le principe d’interpolation et de courbes. Elle permet de définir une animation sur un composant React, en précisant sur chacune de ses propriétés des critères simples de transformation. La librairie se charge d’interpoler la structure du composant pour une animation fluide.\n\ndémo de Spring\n\n\n\nSimplifying the data layer\n\n\n(crédits : Fabien Champigny - React Europe)\n\nKevin Robinson nous présente comment Twitter utilise React. L’approche présentée fait la part belle au “fonctionnel”, à l’image de leur infrastructure backend.\n\nIl nous détaille les mécanismes mis en place pour l’accès aux données, notamment la possibilité de gérer de manière déclarative les dépendances aux données au niveau des composants.\nAu niveau des stores, il prone l’utilisation de structures immuables en stockant un “log” d’événement et en utilisant des “reducers” pour en extraire l’état des données réel.\n\nOn retrouve dans leur approche beaucoup de concepts de la programmation fonctionnel (de la même manière que Redux), mais malheureusement, aucun code n’est ouvert par Twitter à ce sujet.\n\nOn reste un peu sur notre faim en ne pouvant pas aller jouer “concrètement” avec leurs outils.\n\nGoing Mobile with React\n\n\n(crédits : Fabien Champigny - React Europe)\n\nJed Watson, créateur du framework TouchStone JS, un framework JS (basé sur React) permettant de faire des applications mobiles hybride (à base de Webview via Apache Cordova), nous explique comment réaliser des applis hybride grâce à React.\n\nLe débat ici est plutôt de démontrer qu’on peut malgré les dires de certains et en connaissant quelques astuces, faire une appli mobile hybride qui ressemblera à une appli native. Pour nous prouver cela, Jed annonce que l’appli de la React Europe, dispo sur iOS et Android, et que nous avons tous utilisé a été faite avec TouchStone JS !\n\n\n  If you have great developer experience, you are much more likely to get to a great UX\n\n\nJed conseille de ne pas faire de l’hybride lorsqu’on :\n\n\n  est Facebook ou Twitter\n  a beaucoup de données\n  a une utilisation processeur intensive\n  a des animations complexes sur l’UI\n  a des interactions complexes\n  a de la gestion mémoire avancée\n\n\nLes points les plus importants pour qu’une application hybride fonctionne sont :\n\n\n  React\n  la gestion du Touch\n  le Layout\n  la gestion de la Nav\n\n\n\n  You should not do everything in a webview, but you can\n\n\nLa démo présente l’ensemble des composants et des transitions disponibles. En plus de React, Touchstone.js utilise cordova, une bibliothèque d’APIs permettant d’accéder en Javascript aux fonctions natives du mobile, comme l’accéléromètre, le GPS ou l’appareil photo.\n\nLe code de l’application React Europe sera rendu open-source à la fin de la démo : Sketch &amp;amp; Code de l’app React Europe\n\n\n\nReact Router\n\n\n(crédits : Fabien Champigny - React Europe)\n\nBelle présentation de Michael Jackson (@mjackson), pas aussi spectaculaire qu’un concert de l’artiste homonyme, mais assez surprenante et délirante quand même ! Il introduit la librairie qu’il porte avec Ryan Florence (@ryanflorence) depuis plus d’un an et qui est majoritairement utilisée par les utilisateurs de React pour mettre du routing dans leur application : React Router, “The” Router.\n\nMichael commence par nous présenter les bases de la librairie : la définition des routes dans un composant React et le composant Link permettant de générer les liens. Il explique que ce sont des concepts simples et qu’ils permettent à de nouveaux développeurs peu expérimentés de rentrer facilement dans un projet.\n\nIl fait ensuite l’analogie entre les vues et les URLs, affirmant que de bonnes URLs, bien formées, augmentent la confiance de l’utilisateur envers l’application. Une notion importante dans React Router est celle des transitions permettant de changer de vues (et donc d’URLs) et de gérer le “browser history”.\n\nMichael nous annonce une nouveauté dans la prochaine version : l’attribut onEnter sur la définition de route, permettant d’executer une callback avant d’afficher la page (utile par exemple pour protéger une page par authentification).\n\nIl nous expose sa vision du composant comme une fonction prenant en entrée props et state et renvoyant en sortie une UI. Le router n’est finalement qu’un composant comme un autre qui reçoit en entrée l’URL. L’idée que ce qui est explicite est bien meilleur que ce qui est “magique” dans une implémentation lui permet de présenter les changements de l’API dans les dernières versions du React Router avec la récupération des paramètres de l’URL via les props du composant (et plus via une mixin) ou la disparition du composant RouteHandler qui peut simplement être remplacé par props.children pour utiliser les “nested routes” dans ses composants.\n\nDans les travaux en cours, on retiendra les transitions animées qui permettent à Michael de faire une démo “wahou”. L’animation est, bien entendu, répétée en sens inverse sur l’utilisation du back du navigateur. Cette fonctionnalité a d’autant plus d’importance que changement d’URLs et animations ne sont traditionnellement pas de bons amis et posent souvent problème.\n\nLe “dynamic routing” est la deuxième démo montrant la possibilité de contextualiser l’ouverture d’une URL : sur une page, un contenu peut être ouvert dans une popup, mais en copiant et ouvrant l’URL obtenue dans un autre onglet, on a une page avec le même contenu mais une présentation différente (plus de popup).\n\nEnfin, le clou du spectacle sera la dernière démo qui échouera (un dernier commit sur le repo qui aurait provoqué une erreur) qui nous aura valu une fabuleuse danse de Ryan Florence sur la scène (venu en renfort de Michael) ! L’idée initiale était de présenter une fonctionnalité assez énorme permettant de lazy loader les JS de sa SPA en fonction des besoins de chaque route (le “gradual loading”) évitant de charger dès le départ les 3Mo de son bundle webpack alors qu’on en utilise qu’une petite partie. Il faudra attendre pour voir cette fonctionnalité en action…\n\nCreating a GraphQL Server\n\nAprès la conférence de la veille sur GraphQL, Nick Schrock et Dan Schafer nous montrent comment réaliser un serveur GraphQL.\n\nGraphQL est une spécification d’échange et ne présuppose aucune technologie backend. Comme nous l’avons déjà vu dans la conférence précédente, l’idée est de faire de GraphQL une couche entre le client et le code backend déjà existant.\n\nFacebook, en ouvrant cette spécification et l’implémentation de référence, espère fédérer une communauté autour de cette solution. Si d’autres personnes réalisent des implémentations dans différents langages, cela permettrait à tout le monde de capitaliser sur ces techniques et de faciliter la réutilisation de code que ce soit côté client ou serveur.\n\nLa “stack” imaginée pour GraphQL est présentée en partant du serveur jusqu’au client :\n\n\n  GraphQL App Servers\n  Libs (Parse, SQL)\n  Core\n  Spec\n  Common tools (ex: graphicQL, IDE-like tool)\n  Client SDKs (Relay)\n  GraphQL Clients\n\n\nAu delà de la présentation théorique, on se pose quand même la question de la mise en oeuvre concrête au dessus de code existant.\nFacebook utilise maintenant intensivement GraphQL. Par contre, ils n’utilisent pas l’implémentation de référence mais sans doute une implémentation très imbriquée à leur backend (et donc difficile à rendre open-source).\nOn manque malheureusement de retours sur des questions de mise en oeuvre comme le cache ou la gestion des droits par exemple.\nEspérons que des “early-adopters” puissent nous faire des retours là-dessus dans les semaines/mois à venir.\n\nPour en savoir plus, un bon article sur le sujet : GraphQL overview : Getting start with GraphQL and Node.JS\n\nIsomorphic Flux\n\n\n(crédits : Fabien Champigny - React Europe)\n\nMichael Ridgway (@theridgway) aborde une notion souvent abordée ces 2 jours et sur laquelle nous avions fait un article en décembre dernier.\n\nSelon Michael, les avantages du “server rendering” sont multiples :\n\n\n  le SEO\n  le support des anciens navigateurs\n  le gain de performance perçu par l’utilisateur\n\n\nUn des objectifs de cette démarche est de partager le maximum de code entre le serveur et le client.\n\nLa stack proposée par Michael est la suivante :\n\n\n  pour la gestion des vues, React évidemment qui expose une API client et serveur\n  pour le routing, React Router (https://github.com/rackt/react-router)\n  pour le data fetching, superagent (https://github.com/visionmedia/superagent)\n  pour la logique applicative, un pattern léger et célèbre : Flux\n\n\nPour la mise en oeuvre de Flux côté serveur, nous avons déjà vu au cours de ces 2 journées : Redux et React Nexus. Il en existe d’autres comme marty.js, flummox ou alt. Michael nous propose Fluxible, la librairie développée par Yahoo.\n\nFluxible crée un contexte pour chaque requête côté serveur avec un dispatcher custom optimisé pour cette opération. L’état de l’application est transmis du serveur vers le client grâce à un mécanisme de déshydratation/réhydratation des stores.\n\nMichael précise que Fluxible force les développeurs à utiliser Flux de manière conforme sans transgresser les pratiques définies par le modèle. La librairie fournit des composants de haut niveau permettant une parfaite intégration avec React. Enfin, la particularité de Fluxible est son système de plugins permettant de faciliter l’ajout de nouvelles fonctionnalités.\n\nMichael nous montre un exemple de chat isomorphique et la différence observée au chargement avec une SPA classique. Il précise ensuite les outils de développement qu’il utilise :\n\n\n  Babel\n  ESLint\n  webpack\n  babel-loader\n  Grunt / Gulp\n  Yeoman Generators\n\n\nMichael termine sa conf en indiquant que plusieurs applications en prod chez Yahoo utilisent la stack présentée et Fluxible mais qu’il reste encore quelques améliorations à apporter pour les raisons suivantes :\nLes dépendances des composants envers les données ne sont pas facilement connues (rendant le data fetching en amont du rendering côté serveur délicat). Relay pourrait être une solution.\nLe rendu côté serveur de React est relativement lent (mais pourrait être amélioré dans les futures version de React).\nLe Hot Reloading (avec React Hot Loader) ne fonctionne pas avec les stores Fluxible.\n\nConclusion\n\nQue dire après ces deux jours de conférence ? \nDéjà que la communauté et l’engouement autour de React ne cesse de grandir, mais aussi que ca ne chôme pas coté Facebook avec Relay, GraphQL, Animated, React Native Android qui ne devraient pas tarder à pointer le bout de leur nez, avec aussi la mise en place d’une personne full time sur Jest ! C’est rassurant sur l’avenir court/moyen terme de React.\n\nL’organisation était vraiment impeccable (mise à part les soucis de climatisation) avec beaucoup de très bonnes idées, notamment, les bureaux au fond et sur les cotés de la salle de conférence pour que les personnes avec LapTop puissent suivre confortablement.\n\nC’est aussi plutôt étonnant, pour une conférence en France, d’avoir vu aussi peu de personnes francophones. Le public étant très majoritairement anglophone. On se dit que React n’a pas encore complètement pris en France.\n\nCoté tendance, on voit qu’au niveau des librairies Flux, Redux parait clairement être celle qui attire tous les buzz. A voir dans le temps si cela suit, mais le talent indéniable de son créateur, combiné aux bonnes idées (reducers, hot reload) donne vraiment envie de s’y pencher. On regrette aussi toujours le manque de sujets autour des tests.\n\nNous attendons aussi impatiemment React Native Android, pour voir si le buzz et les superbes promesses sont toujours présentes avec deux environnements cibles, et on espère voir sur nos stores de plus en plus d’applis React Native.\n\nGraphQL + Relay parait vraiment être la solution idéale pour réaliser simplement du data fetching coté client (React Web ou React Native), mais l’absence de Relay (toujours pas open-sourcé), combinée au manque de retour sur GraphQL pose encore de nombreuses questions.\n\nNous avons donc hâte d’être à la prochaine React Conférence ou React Europe pour voir la suite de l’évolution de React.\n\nVous pouvez retrouvez le compte rendu de la première journée ici\n"
} ,
  
  {
    "title"    : "CR React Europe Conférence 2015 - Day 1",
    "category" : "",
    "tags"     : " javascript, react, reactnative, video, graphql",
    "url"      : "/2015/07/06/cr-react-europe-2015-day-one.html",
    "date"     : "July 6, 2015",
    "excerpt"  : "Après la première conférence officielle sur React, que nous avons déjà couvert en janvier (Jour 1 et Jour 2), nous nous sommes rendus les 2 et 3 juillet à Paris sous une chaleur infernale pour cette première édition de la React Europe avec l’envie...",
  "content"  : "Après la première conférence officielle sur React, que nous avons déjà couvert en janvier (Jour 1 et Jour 2), nous nous sommes rendus les 2 et 3 juillet à Paris sous une chaleur infernale pour cette première édition de la React Europe avec l’envie de voir et de mesurer les évolutions autour de ReactJS.\n\nKeynote\n\nAu départ, React c’était simplement le V de MVC. Maintenant, on parle de “View First” ou “ User interface First”.\n\nChristopher Chedeaux @vjeux, l’un des core-dev de React, va faire un focus sur 4 axes principaux :\n\n\n  Data\n  Language\n  Packager\n  Targets\n\n\n1) Data\n\nDepuis l’annonce de Flux ont fleuri beaucoup d’autres implémentations du pattern, notamment :\n\n\n  Mcfly\n  Barracks\n  Reflux\n  Fluxy\n  Fluxxor\n  Redux\n\n\nD’après Christopher, certaines vont mourir dans les prochains mois laissant seulement la place aux implémentations les plus pertinentes (et Redux a fait un buzz sans pareil lors de ces 2 jours, voir plus bas).\n\nL’immuabilité revient aussi énormément en regardant du coté de ClojureScript ou ImmutableJS.\n\nCoté Data fetching, cela commence à bouger pas mal avec :\n\n  Relay et GraphQL\n  Falcon &amp;amp; JSON Graph\n  Flux over the wire\n  Om Next\n\n\nIl reste encore les cotés Persistence et Temps réel qui ne sont pas traités dans l’écosystème de React.\n\n2) Languages\n\nLe langage JS a énormement évolué avec CoffeeScript, jsTransform (utilisé chez facebook pour la gestion du jsx, “internalization pipeline”, …)\n\n\n  “think of js as a compile target”\n\n\nIl y a eu Traceur et Recast, et désormais Babel qui a tout ecrasé sur son passage. Facebook convertit en ce moment tout son code Front JS à Babel.\n\nOn retrouve aussi ESLint, un “linter” de code, et du typage de données avec TypeScript et Flow.\n\n3) Packager\n\nNous retrouvons Node.js, CommonJS, npm. \nDans le browser : Browserify et Webpack.\nMême s’il y a encore du travail à faire pour avoir de bonnes performances, et ne pas attendre une compilation via les mises à jour incrémentales, ou React Hot Loader sur lequel nous reviendrons.\n\n4) Targets :\n\nLes cibles de React sont désormais multiples grâce au Virtual DOM :\n\n\n  DOM\n  SVG\n  Canvas\n  Terminal\n\n\nUn focus est ensuite fait sur React Native, permettant de développer des apps natives sur iOS et Android tout en faisant du React.\n\n\n  “UX of a native app / DX of a web app”\n\n\nChristopher insiste sur le terme DX qu’on ne voit jamais dans des slides tech, signifiant “Developper Experience”. \nIl compare aussi le développement de l’appli Ads de Facebook, réalisé avec React Native sur iOS (7 ingénieurs pendant 5 mois), et celui qui a suivi avec React Native Android avec les mêmes 7 ingénieurs durant seulement 3 mois en réutilisant 87% du code !\n\nReact Native Android sera open-sourcé au mois d’Août.\n\n\n  “Learn once : write anywhere”\n\n\nUn appel est fait pour stopper le “bashing” sur les autres frameworks. C’est en travaillant main dans la main entre les communautés Ember, Angular et React notamment que le web avancera.\n\n\n\n\n\nInline Styles: themes, media queries, contexts, and when it’s best to use CSS\n\nStyle are not CSS\n\nMichael Chan @chantastic va nous soumettre une “terrible” idée lors de cette conf qui va en faire crier plus d’un ! “It’s time to learn CSS” est une phrase d’une autre époque, Michael n’hésite d’ailleurs pas à qualifier cette idée de bullshit !\n\nCitant Jeremy Ashkenas @jashkenas, créateur de CoffeeScript et de Backbone.js, il soumet une nouvelle vision : unifier les 3 syntaxes (CSS, HTML et JS) qui permettent de déclarer le style d’une application web car contrairement à ce qu’on pense “le style n’est pas le CSS”.\n\nMichael défend 2 autres axes importants dans React :\n\n\n  les changements de l’état de l’application (piloté en JS via les “states” des composants) sont des changements de l’UI,\n  les composants doivent être réutilisés comme partie entière et indépendante et ne doivent pas être détournés de leur vocation initiale, “je préfère avoir 1000 composants qui font 1 choses que 100 composants qui font 2 choses”.\n\n\nStyle over the time\n\nMichael reprend ensuite l’histoire des CSS. A l’origine, on déclarait les styles dans l’attribut HTML “style”. Puis, on s’est rendu compte de cette façon que le code était dupliqué, d’où l’introduction et la déclaration des classes CSS. Le web est devenu sémantique avec l’utilisation des balises &amp;lt;h1&amp;gt;,&amp;lt;p&amp;gt;, &amp;lt;b&amp;gt;, etc. séparant la présentation dans le HTML et le CSS. L’arrivée du web 2.0 a donné au JS le moyen d’intérargir avec le HTML pour diriger le comportement de l’application complétant la couche présentation HTML + CSS.\n\nNot coupled state\n\nAvec le web interactif actuel, l’état de l’application est noyé entre ces 3 parties constituantes. Heureusement, React permet d’organiser la structure en faisant du “state” la partie centrale de l’application et le “markup”, confondu avec le JS, devient l’interface. Néanmoins, l’état de l’application est toujours couplé avec la présentation et le CSS. Grâce à l’exemple d’une todolist basique, Michael explique comment extraire le “state” des CSS (représenté par la classe “is-complete”) pour l’intégrer en inline dans le render du composant React. Le CSS devient uniquement une couche gérant l’apparence de l’application et les composants (donc le JS) gère intégralement leur état.\n\nNo more CSS\n\nMichael nous montre enfin comment aller plus loin en gérant variables de style, pseudo-classes et pseudo-elements en inline dans le composant, et sans trop de difficultés. La gestion des hovers et des media queries est beaucoup plus ardue et n’est clairement pas recommandé. L’utilisation d’une librairie comme Radium (mais il en existe d’autres) permet de surmonter cet obstacle et d’écrire du style inline très clairement. On aborde quelques conseils pour gérer au mieux les couleurs et le layout. Pour voir un exemple illustrant tous les concepts abordés par Michael, vous pouvez explorer son projet React Soundplayer.\n\nPour conclure sa conf, Michael cite Sandi Metz @sandimetz, designeuse Ruby, défendant l’idée que l’objectif du design est de permettre de (re-)designer plus tard son application et donc de réduire les coûts du changement. Le composant React est l’interface, il se suffit à lui-même.\n\nLes slides sur SpeakerDeck\n\n\n\nFlux over the Wire\n\nElie Rotenberg @elierotenberg introduit Flux, le pattern créé par Facebook massivement utilisé avec React pour gérer le cycle de vie des données à l’intérieur de son application. Le fondement de Flux est de pouvoir partager les états de l’application (les “states”) de façon simple et scalable entre l’ensemble de ses composants car tous n’ont pas que des répercussions locales.\n\nElie nous montre qu’on peut voir Flux comme un modèle symétrique : les composants React sont le miroir des stores (là où sont stockés les states de l’application) et les actions déclenchées par les composants sont le pendant des évènements de mise à jour des stores. Le pattern tourne donc autour de 4 méthodes “symétriques” : onUpdate/dispatch côté composant et onDisptach/update côté store. La nouveauté mise en exergue par Elie est de considérer que le flux entre les composants et les stores peut être implémenté par n’importe quel canal de communication : callbacks/promises par exemple mais aussi streams/EventEmitter et, plus étonnant, websockets. Ce dernier canal permettrait de partager l’état de son application entre plusieurs composants existants sur de multiples clients grâce aux stores qui persisteraient sur un serveur node distant. Elie donne l’exemple d’un chat fonctionnant sur ce principe.\n\nIl présente ensuite les librairies qu’il a élaboré autour de ses idées :\n\n\n  nexus-flux implémentant le pattern Flux de manière “classique”, notamment autour de l’EventEmitter,\n  nexus-flux-socket.io, l’implémentation de Flux autour des websockets,\n  react-nexus une surcouche aux précédentes librairies permettant d’écouter les stores depuis les composants React en utilisant les decorators ES7,\n  react-nexus-chat, l’implémentation du chat donné en exemple.\n\n\nUne des forces de sa librairie est la facilité à mettre en oeuvre l’asynchronisme des actions Flux côté serveur.\n\nEnfin, on découvre l’utilisation réel de ces concepts chez Webedia :\n\n\n  Utilisation de PostgreSQL, Redis et Varnish pour la tenue en charge,\n  React Nexus est utilisé pour la gestion des commentaires et le système utilisateur de millenium.org,\n  Une refonte complète de jeuxvideo.com est en cours avec React Nexus,\n  Des modules React sont déjà présents sur d’autres sites de Webedia.\n\n\nLes slides sur SpeakerDeck\n\n\n\nReact Native: Building Fluid User Experiences\n\nSpencer Ahrens @sahrens2012 de chez Facebook nous présente une librairie, qui devrait être open sourcé sous peu pour gérer les animations dans React Native iOS : Animated.\n\n \nvar { Animated } = require(‘react-native’) \n\nCette librairie devrait marcher directement sur React Native Android et arriver ensuite sur le web.\nL’implémentation est 100% JS.\nNous avons suivi un live coding démo sur iOS d’une application sans animation au départ, consistant à enrichir l’expérience utilisateur en rajoutant des animations fluides via la librairie Animated.\n\nLe code des exemples et les slides, ainsi qu’un nouvel exemple sur l’animation “Tinder”\n\n\n\nExploring GraphQL + Relay: An Application Framework For React\n\n\n\nLee Byron @leeb a introduit GraphQL, une solution permettant de résoudre les problématiques d’accès aux données.\nL’idée est de résoudre les problèmes de l’approche RESTful (qui entraîne beaucoup d’aller-retours avec le serveur) et l’approche FQL (variante de SQL permettant de limiter les aller-retours, mais très compliquée à maintenir).\n\nGraphQL permet au client de définir très précisément les données qu’il souhaite obtenir via leur relations.\n\nLe principe de base est que la structure de la requête permet de définir le format de la réponse. Ex :\n\nQuery\n{\n  user(id: 4) {\n    id,\n    name,\n    smallPic: profilePic(size: 64),\n    bigPic: profilePic(size: 1024)\n  }\n}\n\nResponse\n{\n  &quot;user&quot;: {\n    &quot;id&quot;: 4,\n    &quot;name&quot;: &quot;Mark&quot;,\n    &quot;smallPic&quot;: &quot;https://cdn.site.io/pic-4-64.jpg&quot;,\n    &quot;bigPic&quot;: &quot;https://cdn.site.io/pic-4-1024.jpg&quot;\n  }\n}\n\nLe tout donne un code très facile à lire et à raisonner. Le serveur expose un schéma des données disponibles, ce qui permet :\n\n\n  au client de construire sa requête et de la valider\n  de générer du code côté client à partir du schéma\n  une bonne intégration dans les IDE (autocompletion)\n  génération d’une API Doc\n\n\nGraphQL ne s’occupe pas du stockage, c’est uniquement la couche de requêtage qui peut être implémentée avec votre code actuel.\n\nGraphQL est utilisé depuis plus de 3 ans chez Facebook et sert à l’heure actuelle environ 260 milliards de requêtes par jour.\n\nLee Byron a annoncé lors de sa conférence la diffusion d’un “working draft” d’une RFC GraphQL, ainsi qu’une implémentation de référence en Javascript.\n\n\n\nSuite à cette présentation de GraphQL, Joseph Savona introduit Relay, un framework proposé par Facebook qui permet de gérer côté client le data-fetching via GraphQL dans les applications React.\nLe principe de Relay est que chaque composant définit ses propres dépendances en utilisant le langage de requête de GraphQL. Les données sont mises à disposition dans le composant dans this.props par Relay.\n\nLe développeur fait ses composants React naturellement, et Relay s’occupe de composer les requêtes, permettant ainsi de fournir à chaque composant les données précises dont il a besoin (et pas plus), de mettre à jour les composants quand les données changent et de maintenir un store côté client (cache) avec toutes les données.\n\n\n\nDon’t Rewrite, React!\n\n\n\nRyan Florence @ryanflorence nous propose de profiter de la réécriture de code d’application historique pour introduire de nouvelles technologies et outils.\n\nLe problème avec les réécritures est que l’on est généralement obligé de le faire pour des morceaux assez important de l’application (en partant du haut de l’arbre fonctionnel de l’application). Cela peut bloquer la correction de bug sur le code historique, empêcher de faire quelques évolutions, obliger à maintenir des branches “à longue durée de vie”,…\n\nAu lieu d’utiliser cette approche, de haut en bas, Ryan nous propose d’utiliser React en partant du bas de l’arbre, c’est à dire par une fonctionnalité unitaire très limitée.\n\nReact se prête parfaitement à ce type de travail puisque son design permet de l’utiliser dans un contexte isolé très facilement. Petit à petit, on arrive à remonter de plus en plus, en réécrivant des fonctionnalités de plus en plus importantes, jusqu’à avoir réécrit l’application complète.\n\n\n\nLive React: Hot Reloading with Time Travel\n\n\n\nDan Abramov @dan_abramov nous présente son workflow React.\nIl est notamment le créateur de React Hot Loader, et de Redux, l’une des dernières implémentations de Flux jouissant déjà d’une très grande popularité.\n\nL’un des messages à retenir de sa présentation est l’importance de travailler sur ses outils de développement afin d’avoir plus de temps à passer sur ses applications.\n\nQuelques outils pour accélérer le workflow de développement :\n\n\n  amok\n  figwheel\n  livereactload\n  React Hot Loader\n  webpack\n\n\nNous faisons ensuite un focus sur son workflow autour de ces principaux outils :\n\n\n  Redux\n  Redux Dev Tools\n  React Hot Loader\n  webpack\n\n\nReact Hot Loader permet de rafraîchir son application instantanément à chaque modification de code, et ce, sans refresh de page, uniquement en rafraichissement les composants ayant changé !\nC’est très impressionnant en Live démo !\n\nRajouter à ça le Redux Dev Tools qui permet de suivre en temps réel les actions lancées, ainsi que l’état des states, de pouvoir revenir en arrière dans les actions “à la git”, mais aussi d’avoir un error handler très quali en live (inspiré j’imagine de la gestion d’erreur de React Native).\n\nL’idée derrière Redux (son implémentation du pattern Flux) est de faire un Store immuable. On peut résumer une action à une fonction prenant en entrée un état du store et donnant en sortie un nouvel état du Store (sans toucher au premier). En partant de ce principe, appliquer une série d’actions revient simplement à effectuer une réduction (un “reduce”). \nOn applique ici les principes d’Event Sourcing.\nL’immuabilité permet de stocker les différents états intermédiaires du store et donc de naviguer extrêmement facilement dans les différentes versions pendant le développement.\n\nPlus d’infos ici : The evolution of flux\n\n\n  Reducer + Flux = Redux\n\n\n\n\nBack to Text UI\n\nMikhail Davydov @azproduction a eu l’idée folle de créer une interface texte pour le terminal avec les outils web : HTML, CSS, JS et donc React.\nC’est complétement fou, assez impressionnant, mais on se demande quand même pourquoi ?\n\nVoir les slides\n\n\n\nLightning Talk\n\nPour finir la journée, nous avons eu le droit à quelques Lightning Talk de qualité inégale, abordant l’intégration de D3 avec React, de l’outil Cosmos permettant de tester dans un browser ses composants React un par un, de React Native Playground , un bel outil pour tester facilement online dans un simulateur des applis ou exemple de code de React Native voir vidéo du LT, et Turbine une sorte de remplacant de Relay en l’attendant (voir cet article).\n\nConclusion\n\nExcellente organisation (et on ne dit pas ca seulement pour les bières à volonté), un line-up du tonnerre et de belles annonces (React Native Android en Août, GraphQL etc).\nC’est déjà avec plein d’idées et de pistes d’améliorations pour nos projets React que nous sortons de ce premier jour très complet.\n\nVous pouvez retrouvez le compte rendu de la deuxième journée ici\n"
} ,
  
  {
    "title"    : "On était au PHPTour ! ",
    "category" : "",
    "tags"     : " conference, afup, phptour",
    "url"      : "/2015/06/04/m6web-au-phptour-luxembourg.html",
    "date"     : "June 4, 2015",
    "excerpt"  : "On était au PHP Tour et c’était bien !\n\n(y avait un gros gâteau et des biscuits en forme d’elephpant)\n\nLe voyage fut un peu épique, surtout les quelques kilomètres en plus quand le meilleur d’entre nous a oublié son sac à dos dans une station à 15...",
  "content"  : "On était au PHP Tour et c’était bien !\n\n(y avait un gros gâteau et des biscuits en forme d’elephpant)\n\nLe voyage fut un peu épique, surtout les quelques kilomètres en plus quand le meilleur d’entre nous a oublié son sac à dos dans une station à 150 km de là :)\n\n\n\nEt on n’a pas pu battre la team Blablacar et Jolicode au concours de levé de coudes - on est forfait les gars !\n\nPlutôt qu’un retour exhaustif (et parce qu’avec les aqueducs de Mai on cherche un peu le temps), voici quelque chose de plus informel, sur notre ressenti des tendances communautaires (forcément subjectif).\n\nRadio moquette !\n\nIl y a une bonne maturité autour des tests et du CI dans la communauté PHP. On commence aussi à voir de plus en plus des pratiques autour du partage de la responsabilité du provisionning entre ops et dev (avec Ansible et Vagrant notamment) mais, comme chez M6Web, c’est très balbutiant - et chacun a sa façon de faire. On voit des infras de dev qui passent dans le cloud (variabilisation des coûts, flexibilité, possibilité d’expérimenter). Les services managés n’ont pas la cote, on reste sur du IAAS, principalement chez AWS.\n\nDes solutions pour faire du PHP async se dessinent. Cela reste à expérimenter (libevent, ReactPHP, le tradeoff vitesse, consommation CPU étant inconnu. C’est à creuser, car cela peut sortir à moindre coût de quelques situations difficiles. L’intégration avec certaines librairies comme Guzzle est très intéressante.\n\nMySQL 5.7 est annoncé par Oracle avec pleins de features + 2x plus rapide que 5.6 et 3x que 5.5 (query) et encore plus sur le connection time. Ils annoncent une meilleur intégration avec FusionIO et ils semblent pousser des solutions de cluster multi-master (via Fabric) alors que c’était considéré expérimental avant, c’est maintenant annoncé stable.\n\nPHP7 va être important pour le langage. Pour la performance (au moins x2 vitesse, x0.5 mémoire), les nouvelles fonctionnalités (classes anonymes, scalar type hints, stricts type hints, return type declaration, exceptions on fatals, …). Presque pas de BC break, on devrait surement chez M6Web faire des tests avec la RC dès que possible et migrer rapidement quelques services à la sortie d’une stable.\n\nAnother (php) brick in the wall\n\nM6Web était représenté par Olivier qui a fait une présentation sur l’architecture backend du second écran.\n\n\n\nN’hésitez pas à commenter la conférence.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #8",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2015/05/07/m6web-dev-facts-8.html",
    "date"     : "May 7, 2015",
    "excerpt"  : "Une fois n’est pas coutume, cette nouvelle fournée des Dev Facts est publiée un jeudi. Mais c’est pour vous donner de quoi lire en ce grand week-end !\n\nEt comme ça fait longtemps, voici une grosse fournée. Enjoy !\n\nIncident planifié\n\n\n  \n    X : n...",
  "content"  : "Une fois n’est pas coutume, cette nouvelle fournée des Dev Facts est publiée un jeudi. Mais c’est pour vous donner de quoi lire en ce grand week-end !\n\nEt comme ça fait longtemps, voici une grosse fournée. Enjoy !\n\nIncident planifié\n\n\n  \n    X : normal les erreurs depuis 14h ?\n    Y : c’est pas la maintenance ?\n    Z : oui, maintenance\n    Z : ils me préviennent au moins\n    Y : un incident planifié quoi\n  \n\n\nLa finesse.\n\n\n  Ca va bien rentrer, à force qu’on leur en mette partout.\n\n\nUne bi-douille\n\n\n  Il vient de faire une bidouille pour bidouiller\n\n\nLe fullscreen fenêtré\n\n\n  Le fullscreen ne marche pas en plein écran !\n\n\nLa kalitay\n\n\n  Ils sont gardien de la qualité avec un K majuscule\n\n\nLa méthode argile\n\n\n  Nous on fait de l’agile en V\n\n\nAutomagique\n\n\n  C’est fait manuellement à la main\n\n\nLa dure loi du travail\n\n\n  J’ai même pas réussi à refourger mon travail aux autres !\n\n\nReproduction\n\n\n  Je ne sais pas si ça corrige le bug qu’on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nNos amis les belges\n\n\n  le script pour les belges, comme c’est un “one shot”, on peux dire qu’on va l’exécuter, [accent belge]une fois[/accent belge]\n\n\nProprement sale\n\n\n  C’est pas forcément plus propre, mais c’est moins sale\n\n\nPas pareil, mais différent\n\n\n  \n    X : tu peux vérifier que c’est différent ?\n    Y : Différent comment ?\n    X : bah, pas pareil …\n  \n\n\nÀ la louche !\n\n\n  C’est à peu près approximatif…\n\n\nOn a pas l’habitude\n\n\n  \n    X : Pourquoi tu penses que ça va prendre du temps ?\n    Y : Parce qu’il faut réfléchir\n  \n\n\nIl faut savoir ce qu’on veut\n\n\n  C’est pas prévu pour être utile\n\n\nC’est le Java bleu\n\n\n  Donc en fait tu regrettes Java car tu sais pas coder\n\n\nLe mythe … et la réalité\n\n\n  \n    X : C’est bien connu, quand tu installes Linux, il y a 18 mannequins topless qui défilent dans le bureau\n    Y : Oui, mais des mannequins pour linuxiens, donc bof quoi\n  \n\n\nROI !\n\n\n  y a autant d’utilisateurs que de jour homme pour ce projet !\n\n\nMais ca marche chez moi\n\n\n  \n    Y : navigation privée = pas de DRM = erreur 3365\n    Z : pas de DRM ? mais pourquoi ?\n    Z : mais sur les sites X ca marche, je comprends pas\n  \n\n\nPooh\n\nAu sujet d’une sombre histoire d’expression de besoin\n\n\n  Je ne peux pas toujours les aider à faire leurs besoins…\n\n"
} ,
  
  {
    "title"    : "Mix-IT 2015 - Jour 2",
    "category" : "",
    "tags"     : " mixit, conference, agile, technique",
    "url"      : "/2015/04/17/mixit-2015-jour-2.html",
    "date"     : "April 17, 2015",
    "excerpt"  : "Cet article est le retour du second jour du Mix-IT 2015, le vendredi 17 avril 2015.\nVous pouvez également consulter le retour du jour 1.\n\nAller sur Mars … ou presque\n— Florence Porcel\n\nFlorence est venue nous présenter son rêve : devenir une marso...",
  "content"  : "Cet article est le retour du second jour du Mix-IT 2015, le vendredi 17 avril 2015.\nVous pouvez également consulter le retour du jour 1.\n\nAller sur Mars … ou presque\n— Florence Porcel\n\nFlorence est venue nous présenter son rêve : devenir une marsonaute, à savoir une personne qui va aller physiquement sur Mars.\n\nAprès avoir pris le temps de faire le point sur tout ce qui a été fait pour permettre un jour à l’homme de faire le voyage pour mettre le pied sur Mars, Florence nous a expliqué les différentes actions qu’elle avait entrepris pour agir sur ce cheminement :\n\n\n  Participation à un projet de simulation de vie sur Mars : elle a fait partie d’un groupe de personnes qui se sont isolées pendant 15 jours dans des conditions de vie semblables à celles sur Mars (au milieu du désert de l’Utah, scaphandre pour sortir, nourriture lyophilisée, rationnement);\n  Participation à un programme de volontaire pour le premier départ habité vers Mars;\n  …\n\n\nLe but de tout ça ? Suivre son rêve.\n\nFlorence a fini sa conférence en nous rappelant que ce n’est pas notre éducation, notre formation ou notre passé qui dicte ce que nous pouvons faire de notre vie, mais ce sont nos rêves.\nElle nous a rappelé que, malgré sa formation littéraire, et son métier de comédienne, que son rêve d’aller sur Mars un jour l’a conduite à faire ses actions très concrètes, et que c’est grâce à l’action un peu utopique de beaucoup de gens qu’on peux changer le monde.\n\nEn 2 mots, poursuivons nos rêves !\n\nReactJS pour les néophytes\n– Nicolas Cuillery, Matthieu Lux et Florent Lepretre\n\nCet atelier était un peu corporate puisque que Nicolas et Florent sont actuellement en mission chez M6Web.\n\nLors de cet atelier, qui se décomposait en plusieurs TP permettant de découvrir une à une les différentes spécificités de React et du pattern Flux : React et sa notion de composant, le pattern Flux et une de ses variantes, ReFlux, le système de routing et les tests avec Jest.\n\nBien qu’il leur ait manqué du temps pour finir l’atelier, ils étaient très présents pour nous aider à franchir les premiers pas qui permettent d’entrer dans ce nouveau monde.\n\nEn tout cas, félicitations à tous les 3 pour leur implication pour ce difficile exercice qu’ils ont plutôt bien surmonté.\n\nStartups d’états\n— Pierre Pezziardi\n\nPierre, qui dirige un incubateur d’état, soit un incubateur qui sélectionne et héberge des petites équipes dont le but est de faire évoluer le système informatique public, mais à la sauce d’une startup : budget réduit, équipe réduite, mode “survie”.\n\nIl a eu l’idée de ce système lorsqu’il s’est posé la question sur les outils qui “marchent” et leurs raisons. Pourquoi utilisons nous les outils de Google ou Dropbox ? Parce qu’ils sont simples et efficace ! Ils vont au but, et correspondent à ce que l’utilisateur désire.\nEn poussant cette réflexion, il en est arrivé à la conclusion que tout système informatique est le reflet de l’organisation qui le pilote : plus l’organisation est tournée sur sa propre organisation, plus le produit final correspondra à ce qu’un grand manager a demandé, et pourra être décalé de ce que les utilisateurs attendent.\n\nÀ l’inverse, quand une organisation n’a pas de marge, elle va à l’essentiel pour que son produit convienne aux utilisateurs.\n\nVoici quelques exemples de projets qui sortent de cet incubateur d’état :\n\n\n  data.gouv.fr\n  Marchés Publics Simplifiés\n  mes-aides.gouv.fr\n\n\nCoding Dojo et Mob Programming dans les tranchées\n— Bernard Notarianni\n\nCette conférence était un retour d’expérience expliquant comment une équipe de développement habituée a travailler avec des releases fixes à dates régulières a tenté de mettre en place un découpage en sprint pour améliorer sa productivité.\n\nJe dis volontairement “en sprint” sans parler de Scrum parce qu’en fait, cette équipe s’auto-organisait de la sorte, mais sur un cahier des charges fixe, un périmètre fixe pour chaque release, et pas de feedback avec le produit.\n\nAu final, la conclusion de Bernard a été sans appel : ils ont essayés, l’équipe a fait son travail, a essayé l’amélioration continue, mais comme le client ne jouait pas le jeu, ça c’est mal passé.\n\nFabriquez votre devbox portable avec Docker\n— Jean-Marc Meessen et Damien Duportal\n\nLors de cette conférence, Jean-Marc et Damien nous ont expliqués comment ils avaient réussi à utiliser Docker pour réaliser une “devbox” portable.\nAvant cette conférence, je pensais qu’ils allaient nous expliquer comment ils avaient organisé leurs conteneurs pour que ça soit le plus efficace, mais en fait, une “devbox” est plus un poste de développement complet (bureau et IDE compris)\n\nLe résultat est assez impressionnant dans le fait qu’ils ont réussi à virtualiser une Debian avec son UI via Docker, et qu’ils peuvent le faire tourner sur n’importe quel poste.\n\nToutefois, je ne suis pas convaincu par cette approche. À mon sens, Docker permet à tous de développer dans l’environnement qui lui convient, tout en exécutant son code dans un environnement qui est le plus proche possible de la production.\n\nIl n’en reste pas moins que je les félicite pour le résultat qu’ils ont obtenu, et je suis encore plus convaincu de la puissance de Docker suite à cette présentation.\n\nReading code good\n— Saron Yitbarek\n\nSaron est venue nous partager sa vision sur le moyen qu’elle trouve le plus efficace pour apprendre un langage, un framework, une librairie : lire du code. \nDe la même manière, pour progresser, lire le code des autres permet d’aller au delà de ce que nous pensons faire. En se confrontant au code des autres, nous apprenons sur les autres manières de résoudre un même problème, sur d’autres approches de code, et nous élargissons notre connaissance.\n\nSuivre un tutorial, c’est bien, mais le soucis, c’est que le code est basique, sur un usage basique.\nLire un code réel, c’est voir un cas d’utilisation réel, c’est voir comment le développeur l’a résolu, voir les techniques qu’il utilise, … et le moyen le plus sûr de toujours découvrir et apprendre.\n\nPour aller encore plus loin, il ne faut pas hésiter à discuter avec l’auteur du code que l’on vient de lire.\n\nL’énergie de Saron et la conviction qu’elle met dans sa présentation ont fait de cette conférence un vrai coup de coeur de ma part !\n\nCome to the dark side\n— Stéphane Bortzmeyer - [Présentation sur InfoQ]\n\nLors de cette Keynote, Stéphane nous a fait part de son ressenti quand à l’impact de l’informatique dans notre vie.\n\nIl n’y a encore que 20 ans, l’informatique était au service de l’homme. Elle servait à améliorer son quotidien à faciliter son travail.\nAujourd’hui, c’est l’informatique qui dirige nos vies. Si vous êtes anti Facebook, vous perdez contact avec pas mal de gens. Si vous ne voulez pas d’ordinateur, il y a de plus en plus de démarches que vous ne pouvez faire.\n\nEncore plus important, nous avons délégués de plus en plus de décisions à l’informatique, sur des aspects qui impactent de plus en plus notre quotidien. Par exemple, lorsque vous faites un paiement, c’est un algorithme qui va décider si la transaction est autorisée ou non, de manière froide et automatique, sans chercher à comprendre si sa décision peux vous laisser dans une situation compliquée.\n\nÀ partir de là, nous, développeurs, avons une grande responsabilité. Le code que nous produisons, les algorithmes que nous acceptons d’implémenter sont ceux qui se retrouvent dans les systèmes qui régissent nos vies.\nIl est donc primordial que nous prenions conscience de cette responsabilité et que nous nous posions des questions sur ce que nous faisons, quitte à refuser de le faire si cela va à l’encontre de notre éthique.\n\nJ’ai été fortement touché par cette claque, enfin, cette conférence, car elle nous place devant nos responsabilités, et devant notre devoir de prendre du recul sur ce que nous faisons pour ne pas être un simple robot.\n\nConclusion\n\nComme à leur habitude, les organisateurs de ce Mix-IT 2015 ont réalisés un superbe travail.\nUn grand bravo à eux !\n\nRappel : cet article est découpé en 2 parties. N’oubliez pas de consulter le retour des conférences suivies lors du premier jour.\n"
} ,
  
  {
    "title"    : "Mix-IT 2015 - Jour 1",
    "category" : "",
    "tags"     : " mixit, conference, video",
    "url"      : "/2015/04/16/mixit-2015-jour-1.html",
    "date"     : "April 16, 2015",
    "excerpt"  : "Il est tout naturel que M6Web soit présent à une conférence qui mêle sujet technique d’avant-garde et agilité, 2 sujets qui nous sont chers, surtout lorsqu’elle se déroule à Lyon.\n\nJ’ai donc eu la chance de participer au Mix-IT 2015 qui se tenait ...",
  "content"  : "Il est tout naturel que M6Web soit présent à une conférence qui mêle sujet technique d’avant-garde et agilité, 2 sujets qui nous sont chers, surtout lorsqu’elle se déroule à Lyon.\n\nJ’ai donc eu la chance de participer au Mix-IT 2015 qui se tenait les 16 et 17 avril derniers au CPE Lyon.\n\nCet article est découpé en 2 parties. Dans l’article que vous êtes en train de lire, vous trouverez les retours des conférences que j’ai suivies le premier jour, mais vous pourrez également trouver le retour des conférences suivies lors du second jour.\n\nThe three ages of innovation\n— Dan North - [Slides]\n\nDan nous a partagé sa vision de l’innovation à travers l’évolution d’une technologie.\n\nSelon lui, il existe donc 3 âges dans l’évolution :\n\n\n  Explore (Maximize discovery)\n\n\nIl s’agit de la phase initiale, celle de la découverte, de l’expérimentation.\nDans cette phase, on essaye, on se trompe, on apprend.\n\n\n  Stabilize (Minimize variance)\n\n\nIl s’agit de la phase où on fait le tri sur tout ce qu’on a testé, initié, et qu’on essaye de catégoriser, stabiliser tout ça, voir retirer ce qui n’est pas une bonne idée.\n\nC’est le moment où on est capable de reproduire ces créations, et donc de les apprendre (repeatability, predictability, teachability).\n\nDans cette phase, nous apprenons à réduire l’incertitude autour de la manière de réaliser un code.\n\n\n  Commoditize (Maximize efficiency)\n\n\nCette phase est celle de l’industrialisation, celle où on essaye de réduire les coûts pour augmenter l’efficacité.\n\nEt même si ces 3 phases sont conflictuelles, l’innovation existe dans chacune de ces 3 phases, et il faut savoir les respecter et s’impliquer dans chacune.\n\nLe pourquoi du pourquoi de l’agilité\n— Cédric Bodin - [Screencast de la même conférence, mais à Nantes]\n\nAu cours de cette présentation, Cédric nous a poussé à réfléchir sur la raison profonde du succès de l’agilité.\n\nNous le savons, l’agilité est une solution efficace contre les plannings qui glissent, les cahiers des charges non tenables, le syndrome de la tour de cristal et autres dysfonctionnement organisationnels.\n\nDepuis 1994, le Chaos Manifesto publie un rapport indiquant le pourcentage de projets qui échouent ou réussissent, et il y a 2 fois plus de projets qui échouent que de projets qui réussissent. Il est courant que des projets qui s’éternisent, dérapent, … soient finalement abandonnés.\n\nDe même, il est courant que le produit fini ne corresponde pas à ce que l’utilisateur attend, à cause de la distance entre eux et les équipes qui développent le produit.\n\nMais pourquoi est-ce que nous avons besoin de l’agilité ? Pourquoi en sommes nous arrivés à cette situation où tout ces syndromes apparaissent ?\n\nTout simplement parce que nous avons construit l’Entreprise d’aujourd’hui sur la base du Taylorisme qui permet d’être le plus rentable possible en prévoyant tout les cas pour réduire le hasard et donc l’échec. Le socle de base de cette théorie est que rien ne change et que tout est prévisible.\n\nOr, la société actuelle va plus vite, veut pouvoir être très réactive, et notre branche en particulier.\n\nÀ partir de là, le modèle qui veux que certaines personnes pensent, prévoient, organisent, pour que les techniciens n’aient “que” à exécuter est dépassée. Et cela se voit ! Combien de “j’ai fait ce que le cahier des charges demandait”, combien de discussions stérile autour d’un changement de périmètre pour recalculer des délais ?\n\nVoici 2 exemples de phrases qui montrent le décalage entre la vision du Taylorisme dans l’informatique et la réalité :\n\n\n  En informatique, la production, c’est la compilation. Et elle est tellement efficace qu’on ne la facture plus.\n\n\n\n  Nos métiers de services ne sont pas des métiers de production, mais des métiers de conception.\n\n\nÀ partir de là, il faut considérer le travail du développeur comme du côté “pensant” et pas du côté “exécutant” : ne pas essayer de maximiser sa productivité en réduisant sa réflexion. Bien s’assurer de sa compréhension du besoin au lieu de lui lister des tâches à accomplir sans réflexion.\n\nLes principes mis en avant par l’agilité vont en ce sens : rapprocher l’utilisateur et le développeur, laisser l’équipe décider du planning, permettre l’imprévu.\n\nToutefois, l’agilité n’est pas un déclencheur. Une société qui passe à l’agilité sans modifier son fonctionnement va à l’échec. L’agilité n’est qu’un moyen de changer.\n\nN’oublions pas la loi de Conway :\n\n\n  Tout logiciel reflète l’organisation qui l’a créé.\n\n\nSolution focus in team\n— Vincent Daviet et Géry Derbier\n\nCet atelier destiné plutôt aux managers donnait des pistes et des exemples pour réussir à relancer de la synergie dans des équipes qui ont tendance à se bloquer lors de la mise en place de l’agilité.\n\nLe principal ressort de l’agilité est la communication. Si cette communication est brisée, tout est en péril, et c’est souvent la principale cause d’échec de son adoption.\n\nNous avons réalisé des mises en condition pour voir comment la communication peut être un véritable frein ou un formidable moteur pour avancer. En posant une même question de plusieurs manières, nous avons constaté qu’il est tout aussi possible de bloquer un échange qui était intéressant, que d’aller voir plus loin que les explications qui nous étaient proposées.\n\nSi le TDD est mort, alors pratiquons une autopsie\n— Thomas Pierrain et Bruno Boucard - [Slides]\n\nDerrière ce titre un peu provocateur, Thomas et Bruno voulaient faire une analyse à date du TDD, pour voir comment il est utilisé, et pourquoi il a tendance à être délaissé.\n\nComme il est un peu facile de dire que les développeurs peuvent avoir du mal à changer leurs habitudes, nous sommes allés voir un peu plus loin :\n\nLe développeur qui est habitué à écrire du code, voir s’il marche, l’éditer, voir s’il marche, … fonctionne selon un principe d’expérimentation. Il ne sait pas très bien où il va, il essaye du code jusqu’à ce qu’il fonctionne. Et une fois qu’il maitrise le code, il va continuer à travailler de la même manière, même si le code fonctionne beaucoup plus rapidement qu’avant.\n\nMais cette manière de travailler reste fortement exposée à 2 limitations :\n\n  perte de vue de l’objectif réel;\n  sur-architecture.\n\n\nAvec le TDD, le fonctionnement est de décrire ce qu’on veut faire d’abord (se fixer un objectif) en l’éclaircissant au maximum, au plus tôt.\n\nPour être efficace en TDD, il faut commencer par creuser son sujet et s’assurer de comprendre ce qu’on veut, comment, avec quelles limites (les 5 pourquois). Ensuite, il faut le formuler, de préférence à haute voix pour bien s’assurer de comprendre ce qu’on est en train de dire (méthode du canard en plastique), puis finalement lister une série de phrases en “mon code devrait” indiquant le fonctionnement nominal et les cas limites.\n\nUne fois que tout ça est respecté, il est possible de démarrer le TDD proprement dit, à savoir d’écrire les tests, de le voir échouer, puis de réaliser le code qui permet à ces tests de fonctionner. Ainsi, il est possible de dégager son esprit du fonctionnel pour se concentrer sur le technique, jusqu’à voir le code réussir à atteindre le but fixé.\n\nRappel : cet article est découpé en 2 parties. N’oubliez pas de consulter le retour des conférences suivies lors du second jour.\n"
} ,
  
  {
    "title"    : "Introduction à Immutable.Js, Relay + GraphQL et React Native",
    "category" : "",
    "tags"     : " javascript, react, reactnative, lft, video",
    "url"      : "/2015/04/01/immutablejs-relay-graphql-react-native.html",
    "date"     : "April 1, 2015",
    "excerpt"  : "Voici un petit compte rendu vidéo, filmé lors de notre Last Friday Talk de Mars, d’un retour de veille techno suite à la React Conférence.\n\nLe retour est une introduction sur 3 des sujets qui m’ont paru les plus importants lors de cette conférence...",
  "content"  : "Voici un petit compte rendu vidéo, filmé lors de notre Last Friday Talk de Mars, d’un retour de veille techno suite à la React Conférence.\n\nLe retour est une introduction sur 3 des sujets qui m’ont paru les plus importants lors de cette conférence :\n\n\n  Immutable.Js\n  Relay + GraphQL\n  React Native\n\n\nLes slides :\n\n\n\nPour plus d’informations sur la React Conférence, nos CR sont disponibles ici :\n\n\n  Compte rendu React Conférence Jour 1\n  Compte rendu React Conférence Jour 2\n\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Comment a-t-on bouchonné les développeurs backend ?",
    "category" : "",
    "tags"     : " javascript, superagent, mock, isomorphic, cytron, open-source",
    "url"      : "/comment-a-t-on-bouchonne-les-developpeurs-backend",
    "date"     : "March 30, 2015",
    "excerpt"  : "Chez M6Web, nous travaillons actuellement sur la nouvelle version d’un site web pour lequel sont dédiées deux teams :\n\n\n  l’équipe backend fournit l’accès aux données via des API sous Symfony2,\n  nous, l’équipe frontend, développons une applicatio...",
  "content"  : "Chez M6Web, nous travaillons actuellement sur la nouvelle version d’un site web pour lequel sont dédiées deux teams :\n\n\n  l’équipe backend fournit l’accès aux données via des API sous Symfony2,\n  nous, l’équipe frontend, développons une application SPA isomorphe utilisant React.JS et le pattern Flux.\n\n\nDévelopper le front avant les API…\n\nNous avons démarré le projet au même moment que l’équipe backend, donc sans avoir accès aux API qui nous fournissent les données nécessaires au fonctionnement de l’application. Nous nous sommes alors interrogé sur la meilleure façon de développer notre front sans dépendre des API tout en impactant un minimum le code cible.\n\nLe contrat d’interface\n\nLe choix technique pour notre SPA a été guidé par une réflexion poussée sur les app isomorphique. Cette approche, React, Flux et tout l’environnement qui tourne autour nous étaient alors totalement inconnu. Nous avons eu une phase importante en amont pour poser les bases de l’architecture du site, démontrer la faisabilité du projet et documenter l’ensemble.\n\nCe petit délai a permis à l’équipe backend d’établir des contrats d’interface pour les principales routes de l’API. À partir de ces informations, plus ou moins précises, nous avons établi des fichiers de fixtures. L’idée était donc de retourner les données bouchonnées pour chaque appel à une route d’API non existante.\n\nSuperagent et superagent-mock\n\nPour réaliser les requêtes aux API, nous utilisons la librairie superagent, un client HTTP javascript facilement extensible. Il est isomorphe, c’est-à-dire qu’il fonctionne aussi bien sur un serveur node.js via npm que côté browser dans une application packagée via un bundler (webpack, browserify).\n\nNous avons développé superagent-mock, un plugin pour superagent, dont le rôle est de simuler les appels HTTP lancés par superagent en retournant des données de fixtures en fonction de l’URL appelée.\n\nEn pratique\n\nComme superagent, superagent-mock s’installe via npm et peut être utilisé sur des applications serveurs ou clientes (via un bundler). Tout d’abord, il faut rajouter la dépendance à la librairie dans son  package.json.\n\nnpm install superagent-mock --save-dev\n\nIl faut ensuite créer le fichier de configuration. C’est ici que vous allez décider des routes à bouchonner. Prenons l’exemple d’une route qui n’existe pas et qui devra nous retourner la liste des auteurs du blog technique de M6Web : https://tech.bedrockstreaming.com/api/authors.\n\nVoici la structure du fichier de configuration à mettre en place :\n\n// ./config.js file\nmodule.exports = [\n  {\n    pattern: &#39;https://tech.bedrockstreaming.com/api/authors&#39;,\n    fixtures: &#39;./authors.js&#39;,\n    callback: function (match, data) {\n      return { body : data };\n    }\n];\n\n\n  L’attribut pattern peut être une expression régulière, dans le cas d’une route qui contiendrait des paramètres variables (ex : https://tech.bedrockstreaming.com/api/authors/(\\\\d+)).\n  L’attribut fixtures représente le lien vers le fichier de fixtures ou une callback.\n  L’attribut callback est une fonction à deux arguments. match est le résultat de la résolution de l’expression régulière et data correspond aux données retournées par les fixtures. match permet d’utiliser certains paramètres de l’appel (ex : l’id de l’auteur) pour retourner des données ciblées (ex : l’auteur dans le fichier de fixtures correspondant à cette id).\n\n\nEnsuite, il faut créer le fichier de fixtures. C’est un fichier JS qui exporte une fonction retournant les données bouchonnées.\n\n// ./authors.js file\nmodule.exports = function () {\n  return [\n    {\n      id: 1,\n      name: &quot;John Doe&quot;,\n      description: &quot;unidentified person&quot;\n    },\n    ...\n  ];\n};\n\nPour finir, au début du fichier JS appelé par node, il suffit de patcher superagent avec le plugin superagent-mock de cette manière :\n\n// ./server.js file\nvar request = require(&#39;superagent&#39;);\nvar config = require(&#39;./config.js&#39;);\nrequire(&#39;superagent-mock&#39;)(request, config);\n\nCes quelques lignes permettent de surcharger certaines méthodes de superagent pour lui appliquer la configuration et simuler les requêtes bouchonnées. Pour comprendre plus en détail le fonctionnement, c’est par ici.\n\nEt après ?\n\nAvec cette astuce, vous pouvez développer votre front sans qu’aucune API en face ne soit accessible. C’est très pratique pour travailler en local, sans accès au net, ou pour rendre les tests fonctionnels de son application complètement indépendants d’un service tiers externe.\n\nLa partie délicate de cette approche intervient lorsque l’on câble son application avec la vraie API… et que l’on s’aperçoit que le contrat d’interface n’a pas été respecté ! Nous avons souvent des corrections à réaliser dans notre code lors de cette étape, mais les changements sont généralement mineurs et le gain de temps apporté par l’utilisation du bouchon en amont n’est pas remis en cause. La partie fastidieuse reste de maintenir ses fichiers de fixtures avec l’évolution de l’API, particulièrement nécessaire si on s’en sert dans ses tests fonctionnels.\n\nToujours plus\n\nNotre application forge elle-même l’URL des images récupérées via l’API : elle nous fournit un id et nous reconstituons l’URL finale grâce à un paramètre de configuration. Ce n’est pas REST compliant mais nous avons de bonnes raisons de le faire. Cette génération d’URL utilise la librairie sprintf-js. Pour avoir une application complètement indépendante de toute requête externe, nous avons dû également bouchonner ces appels sur des images locales. Dans cette optique, nous avons développé sprintf-mock dont le mode de fonctionnement est étrangement similaire à celui de superagent-mock.\n\nLes projets superagent-mock et sprintf-mock sont open source. Très simple d’utilisation, ils nous permettent de paralléliser nos développements avec l’équipe backend et de rendre autonomes nos tests fonctionnels. Alors n’attendez plus la finalisation de vos API pour commencer vos développements front !\n\n"
} ,
  
  {
    "title"    : "How did we mock the backend developers?",
    "category" : "",
    "tags"     : " javascript, superagent, mock, isomorphic, cytron, open-source",
    "url"      : "/how-did-we-mock-the-backend-developers",
    "date"     : "March 30, 2015",
    "excerpt"  : "At M6Web we are currently working on a new version of a web site, with two separate teams:\n\n  the backend team providing data access through APIs;\n  us, the frontend team, building an isomorphic SPA application using React.JS and the flux pattern....",
  "content"  : "At M6Web we are currently working on a new version of a web site, with two separate teams:\n\n  the backend team providing data access through APIs;\n  us, the frontend team, building an isomorphic SPA application using React.JS and the flux pattern.\n\n\nDevelop the frontend before the APIs\n\nBoth teams started the project at the same time, meaning that at the beginning, we didn’t have the web services needed for our application. We looked for the best way to develop it without waiting for those web services to become available.\n\nInterface\n\nOur technical choices for the SPA has been guided by a deep thinking about isomorphic applications. This approach, with React, Flux and their surrounding environment, was at the time, totally unknown. Our first important task was to build the foundations of the web site architecture, demonstrate the feasibility of the project and document everything.\n\nThis resulting delay allowed the backend team to specify the output of the API. Based on those informations, we wrote fixtures. The idea was to have data from a nonexistent web service.\n\nSuperagent and superagent-mock\n\nTo request the API we use the superagent library, an easily-extensible Javascript HTTP client. It is isomorphic, so it can be used both on server and client sides.\n\nThen we developed superagent-mock, a superagent plugin dedicated to simulate HTTP requests returning fixtures data.\n\nApplication\n\nLike superagent, superagent-mock can be installed via npm, and be used by server or client side libraries. First, you need to add the library in your package.json.\n\nnpm install superagent-mock --save-dev\n\nThen, create the configuration file, where you will define which data will be mocked. Let’s take for example a nonexistent API, the authors list on our technical blog: https://tech.bedrockstreaming.com/api/authors.\n\nHere is the file structure we need:\n\n// ./config.js file\nmodule.exports = [\n  {\n    pattern: &#39;https://tech.bedrockstreaming.com/api/authors&#39;,\n    fixtures: &#39;./authors.js&#39;,\n    callback: function (match, data) {\n      return { body : data };\n    }\n];\n\n\n  The pattern attribute should be a regular expression, in case of a route containing variable parameters (ie: https://tech.bedrockstreaming.com/api/authors/(\\\\d+)).\n  The fixtures attribute represents the link to a file or a callback.\n  The callback attribute is a function with two arguments: match is the result of the regular expression and data the fixtures. match allows to use some call parameters (ie: the author id) to return relevant data (ie: the author in the fixture).\n\n\nNext, you have to create the fixture file. This is a JS file exposing a function returning the mocked data.\n\n// ./authors.js file\nmodule.exports = function () {\n  return [\n    {\n      id: 1,\n      name: &quot;John Doe&quot;,\n      description: &quot;unidentified person&quot;\n    },\n    ...\n  ];\n};\n\nFinally, at the top of the file called by node, you have to patch superagent with superagent-mock this way:\n\n// ./server.js file\nvar request = require(&#39;superagent&#39;);\nvar config = require(&#39;./config.js&#39;);\nrequire(&#39;superagent-mock&#39;)(request, config);\n\nThose few lines allow us to overload some superagent methods to apply the configuration of the mocked requests (check the source code).\n\nWhat’s next\n\nWith this tip, you can develop the frontend without access to any API. It’s very useful in order to work locally on your computer, without the internet, or to make your functional tests independent of any third party.\n\nHowever it gets tricky when you connect your application with the real API… and you realize that the interface was not respected. We often have to fix our code at this stage, but the changes are usually minor and time saved by the mock isn’t questioned. The tedious part is still to maintain fixtures with the API evolution, especially necessary if it’s used with functional tests.\n\nEven more!\n\nOur app build itself the URLs of images retrieved via the API: it provides us an id and we guess the final URL through a configuration setting. This isn’t REST compliant but we have good reasons to do this. The URL generation uses the library sprintf-js. To have a completely independent application of any external request, we also had to mock these calls to local images. With this in mind, we have developed sprintf-mock whose operating mode is curiously similar to that of superagent-mock.\n\nProjects superagent-mock and sprintf-mock are open source. Very easy to use, they allow us to parallelize our developments with the backend team and to make our functional tests autonomous. So don’t wait API completion to start your frontend developments!\n\n"
} ,
  
  {
    "title"    : "CR React Conférence 2015 - Day 2",
    "category" : "",
    "tags"     : " javascript, react, flux, isomorphic, conference",
    "url"      : "/2015/02/10/cr-react-conf-2015-day-two.html",
    "date"     : "February 10, 2015",
    "excerpt"  : "De retour à Menlo Park pour cette deuxième journée de la React conférence.\n\nKeynote React Native\n\nChristopher Chedeau, @vjeux, revient sur les origines de React Native et les raisons pour lesquelles ils ont décidé de le créer.\n\nLes 3 piliers d’une...",
  "content"  : "De retour à Menlo Park pour cette deuxième journée de la React conférence.\n\nKeynote React Native\n\nChristopher Chedeau, @vjeux, revient sur les origines de React Native et les raisons pour lesquelles ils ont décidé de le créer.\n\nLes 3 piliers d’une appli natives qu’ils ont dûs traiter pour React Native sont :\n\n\n  Touch Handling : la vraie différence entre appli native et web\n  Native Components : tout le monde essaye de s’en rapprocher mais personne n’y arrive, et il y a déjà beaucoup de très bons composants natifs\n  Style &amp;amp; Layout : le layout impacte énormément la façon dont on code, que l’on soit sur le Web, iOS ou Android\n\n\nNous voyons que chaque composant natif a été recréé comme un composant React : &amp;lt;View&amp;gt;, &amp;lt;Text&amp;gt; …, et Christopher explique comment un composant React est transformé en composant natif iOS.\n\nLa transformation du JS en natif se fait via JSCore (le moteur JS dans iOS).\n\nUne démonstration nous prouve qu’on peut utiliser la console Dev Tools de Chrome, pour débugger l’application et voir tout le code « DOM » React, comme si nous faisions du web classique.\n\nLa dernière partie explique l’approche des équipes de Facebook sur la manière de faire du CSS. Christopher avait déjà fait hurler pas mal de personnes lors de sa conférence “faire du CSS en JS” (React CSS in JS). Il déclare le style en javascript dans une variable styles, et utilise l’attribut style : &amp;lt;Text style={styles.movieYear}&amp;gt; qui inlinera le CSS.\n\nC’est assez déstabilisant mais aussi ultra prometteur. Cela permet de résoudre quasiment tous les défauts de CSS (Global Namespace, Dependencies, Dead Code Elimination, Minification, Isolation …)\n\nNous parcourons ensuite les manières de gérer du layout nativement dans iOS, que Christophe décrit comme « ultra-compliqué » ! Alors que coté web, nous avons le Box Model et Flexbox qui résolvent tous ces problèmes assez facilement.\n\nLes équipes de Facebook ont donc décidé de re-coder Flexbox et le Box Model en JS avec une approche TDD, de manière à pouvoir utiliser la plupart des bases de Flexbox dans React Native pour faire du layout facilement sur iOS !\n\nVous pouvez retrouver le résultat « Css-Layout » sur le Github de Facebook.\n\nLa démonstration continue sur un « live coding » montrant le « live reload » entre la modification du JS et le rafraîchissement instantané du Simulator iOS.\n\nNous apprenons aussi que les modules ES6 ou Node comme Underscore, ou le SDK de Parse par exemple, fonctionneront sans problème du moment qu’ils n’ont pas de dépendance dans le browser ! \nC’est encore une fois très prometteur, et si React Native vous intéresse, la vidéo ci-dessous est une excellente introduction.\n\nJ’ai, de mon coté, pu jouer quelques heures avec et c’est effectivement très sympa, intuitif et très rapide.\nLa version que nous avons ne contient pas encore tout ce que l’on voit dans la vidéo (je n’ai par exemple pas trouvé le Live Reload ou le Remote Debugging pour l’instant), mais cela ne saurait tarder, les équipes de Facebook travaillant d’arrache-pied sur le projet.\n\n\n\nThe complementarity of React and Web Components\n\nAndrew Rota, @AndrewRota, est de Boston, travaille pour Wayfair.com et explique comment utiliser des Web Components avec React, ou du React dans des Web Components.\nIl nous montre un exemple d’un player html5 vidéo avec un shadow dom qui contient tous les contrôles du player (de simple input HTML).\n\nLa communauté WebComponent a déjà partagé pas mal de WebComponents :\n\n\n  les x-\n  les core-\n  google-\n  paper-\n  et d’autres …\n\n\nPour conclure, Andrew partage les bonnes pratique pour créer un WebComponent :\n\npetit\ntrès encapsulé\naussi stateless que possible\nperformant\n\nPour en savoir plus sur les Web Components : Polymer-Project\n\n\n\nImmutable Data and React\n\nLee Byron, @leeb, enchaîne sur l’immuabilité !\nConcept passionnant que nous avons entendu dans presque l’intégralité des conférences.\n\n\n  Un objet immuable, en programmation orientée objet et fonctionnelle, est un objet dont l’état ne peut pas être modifié après sa création. Ce concept est à contraster avec celui d’objet variable. Source : Wikipédia\n\n\nLee Byron est donc le créateur de la librairie Immutable-JS permettant de gérer facilement des collections immuable en JS.\n\n\n  Immutable data cannot be changed once created, leading to much simpler application development, no defensive copying, and enabling advanced memoization and change detection techniques with simple logic. Persistent data presents a mutative API which does not update the data in-place, but instead always yields new updated data. Source : immutable-js\n\n\n\n  React is the V in MVC. We don’t need an M. We already have arrays and objects.\n\n\nD’ailleurs, on parle aussi d’objets immuables coté Angular 2 : Change Detection in Angular 2\n\n\n\nBeyond the DOM: How Netflix plans to enhance your television experience\n\nL’une des conférences que j’attendais le plus, par Jafar Husain, @jhusain, Technical Lead chez Netflix.\nPour plusieurs raisons, déjà parce que Netflix … qui en a profité pour annoncer la veille que « Netflix aimait React » mais aussi parce que Jafar est connu pour pas mal de choses (différents blog posts ou présentations), ainsi qu’un cours interactif sur la programmation fonctionnelle en Javascript sur lequel j’ai passé pas mal de temps.\n\nIl nous a donc expliqué les plans de Netflix pour améliorer l’expérience Télé sur leurs services, et comment React les a grandement aidés à le faire.\n\nPour connaître les raisons pour lesquels ils ont choisi React, je vous invite à lire l’article Netflix like React (Startup Speed et Server Side Rendering \\o/, Runtime Performance, Modularity).\n\nAujourd’hui, Netflix développe majoritairement en Javascript et ont 3 UI en JS, une pour le mobile, une pour le web et une pour les télés.\nIls ont vu assez vite que le DOM était très loin, c’est pourquoi ils ont créé et introduit Gibbon (une sorte de Webkit maison plus rapide et adapté à leur besoin sur les téléviseurs).\nIls ont donc fait évoluer React (un fork au départ) pour permettre de sortir vers quelque chose d’autres que du DOM afin de correspondre à leur moteur Gibbon et vont donc continuer en 2015 le déploiement de leur nouvelle UI avec React sur tous les services y compris télés.\n\nVous pouvez retrouver le Netflix Open Source Software Center pour découvrir le grand nombre d’outils Open source de qualité qu’ils délivrent.\n\n\n\nScalable Data Visualization\n\nZach Nation travaille chez Dato (anciennement GraphLab) et doit traiter de très grandes quantités de données dans ses applications.\n\nIl démontre l’intérêt de React couplée à d3.js (une librairie de visualisation exceptionnelle) pour représenter à l’écran des transactions Bitcoin (en parsant un fichier de 21G ! en live).\n\n\n\nRefracting React\n\nTalk par David Nolen, @swannodette, personne très influente dans la communauté JS (son blog). Créateur de Om, ClojureScript, il nous explique que React doit être vue comme une plateforme (plutôt que librairie ou framework). On apprend aussi les concepts à l’origine d’Om.\n\n\n\nFlux Panel\n\nBill Fisher (Facebook) a rassemblé une partie des utilisateurs (voir des contributeurs) à React pour confronter les différentes approches sur l’utilisation de Flux, ainsi que sur la manière de gérer de l’isomorphisme.\n\nOn parle notamment, via Michael Ridgway, @theridgway, de Fluxible, la librairie open source proposée par Yahoo (que nous utilisons), qui a annoncé le jour même la création de sa documentation en ligne isomorphique, elle-même open source et utilisant Fluxible.\n\nSpike Brehm est aussi intervenu pour AirBnb, qui fut la première société, je pense, à parler d’isomorphisme.\n\nAndres Suarez a aussi présenté la manière de gérer l’isomorphisme chez Soundcloud. Vous pouvez avoir beaucoup plus d’infos de sa part dans cette excellente vidéo.\n\nSi vous êtes intéressés par les différentes approches de Flux, vous pouvez comparez ici les implémentations : Flux Comparison\n\nQuelques questions sont aussi posées à Jing Chen sur Relay.\n\nL’approche était intéressante mais le résultat un peu décevant car les sujets et implémentations ne sont que trop peu effleurés.\n\n\n\nCodecademy’s approach to component communication\n\nBonnie Eisenman, @brindelle, travaille pour CodeAcademy.\nCodeAcademy est un service gratuit du secteur éducatif permettant d’apprendre à coder dans certains langages.\n\nBonnie a partagé la manière dont son équipe a appréhendé React, et notamment la communication entre composant. Ces réflexions ont eu lieu avant qu’ils n’aient connaissance du pattern Flux.\n\nLes slides\n\n\n\nStatic typing with Flow and TypeScript\n\nJames Brantly, de chez AssureSign, commence avec une citation vu le matin (merci le JetLag) sur twitter :\n\nOn the 1st day God created the web. On the 2nd day God wrote jQuery. Then God blacked out, 3 days later awoke &amp;amp; invented React. #reactjsconf&amp;mdash; Matt Huebert (@mhuebert) 29 Janvier 2015\n\n\nIl présente ensuite TypeScript et Flow, deux outils pour améliorer et sécuriser la production de code.\n\nEn partant d’une application React d’exemple, il nous montre comment on intégre TypeScript et comment il l’a “hacké” pour qu’il reconnaisse le JSX, puis comment il ajoute Flow.\n\nAu final il recommande d’utiliser plutôt Flow, même si TypeScript est un peu plus mature, et fonctionne lui sous Windows.\n\n\n\nQA with the team\n\nLes deux journées se sont finies sur une session de Questions/Réponses avec les équipes de React chez Facebook : Tom Occhino, Ben Alpert, Lee Byron, Christopher Chedeau, Sebastian Markbåge, Jing Chen, et Dan Schafer.\n\n\n\nConclusion\n\nPour une première conférence officielle sur React, ce fut une excellente surprise ! On retiendra clairement l’annonce et les démos de React Native, l’emballement général autour de React, ainsi que l’approbation global de beaucoup de gros acteurs du web (Yahoo, Mozilla, Netflix, Uber, …), le succès du pattern Flux (malgré le manque de clarté sur la manière de faire du Data Fetching), les promesses de Relay, les sujets récurrents autour de l’immuabilité …\n\nBref, une vraie belle réussite. Chapeau aux organisateurs (merci @vjeux ;-) ) et équipes de Facebook, ainsi qu’aux speakers pour cette superbe conférence.\n\nIl se passe réellement quelque chose de grand dans la communauté Front-End grâce à React. Il suffit de voir la vitesse à laquelle les tickets d’entrée sont partis (même chose au React Meetup parisien de Décembre 2014), de voir que tous les frameworks MVC tentent de s’en inspirer (pré-render, SSR, Virtual-Dom …).\n\nPour finir, je voulais aussi partager le travail d’une des personnes présentes lors de cette conférence, ayant une façon très particulière de prendre des notes sur chacun des talks : https://chantastic.io/2015-reactjs-conf/\n\np.s: Retrouvez les retours sur la première journée de la React conférence 2015.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "CR React Conférence 2015 - Day 1",
    "category" : "",
    "tags"     : " javascript, react, flux, isomorphic, conference",
    "url"      : "/2015/02/04/cr-react-conf-2015-day-one.html",
    "date"     : "February 4, 2015",
    "excerpt"  : "Les 28 et 29 janvier 2015, sur le campus de Facebook (à Menlo Park), avait lieu la première conférence officielle sur la librairie open-source React (créée par les équipes de Facebook).\n\n2 jours de conférences riches en talks et en annonces dont v...",
  "content"  : "Les 28 et 29 janvier 2015, sur le campus de Facebook (à Menlo Park), avait lieu la première conférence officielle sur la librairie open-source React (créée par les équipes de Facebook).\n\n2 jours de conférences riches en talks et en annonces dont voici un petit compte-rendu, pour ceux n’ayant pas eu la chance de pouvoir y assister ou de suivre les différents LT sur Twitter, en commençant par la première journée.\n\nL’ouverture de la conférence par Tom Occhino\n\nTom Occhino, @tomocchino, a permis de rétablir la vérité sur l’origine de React.\nCe sont les équipes de Facebook Ads qui sont à la genèse de ce projet.\n\nA l’époque, sur des applis MVC côté client, plus les applications et le nombre de développeurs grandissaient, plus elles étaient difficiles à maintenir et devenaient lentes !\nLe « Two Way Data Binding » rendait les mises à jour en cascade trop compliquées (tout l’écran devait être rafraîchi) et le code devenait vraiment non prévisible. Mais malgré tout, cela “marchait” ! L’appli de Chat Facebook fonctionnait aussi de la même manière.\n\nC’est ainsi que React a été créé pour améliorer le rendu, l’organisation et les performances de ces applications.\n\nInstagram a ensuite rejoint Facebook et les équipes ont voulu utiliser React pour refondre le site, mais React était à l’époque trop couplé à Facebook. \nPete Hunt a donc re-factoré l’ensemble pour créer le React “open-source” que l’on connaît aujourd’hui.\n\nAprès cette introduction sur React, Tom a expliqué que l’un des problèmes de React est qu’il n’est utilisé que pour le Web.\nAujourd’hui, tout le monde tente de créer des composants web ressemblant aux composants natifs, mais à chaque fois, le résultat est mauvais, l’environnement natif étant plus performant que celui d’un browser. Un exemple donné d’application native plutôt sexy est celui de “Facebook Paper”.\n\nIl a ensuite révélé l’annonce certainement la plus importante de ces 2 jours, l’arrivée prochaine sur Github de « React Native », permettant de développer en Js via React des composants entièrement natifs, avec comme exemple, l’application Facebook Groups présente sur l’App Store iOs ! \nLa conférence est lancée !\n\nAvec React Native, ils ne veulent pas faire du “write once, run anywhere », mais du “learn once, write anywhere » de manière à optimiser les composants et usages suivant les devices.\n\nLe code sera fourni sur un dépot privé à tous les participants de la conférence lors de la Keynote de clôture !\n\nPour finir sa keynote d’entrée, il a voulu lister les frameworks JS qui ont été influencés par React ces derniers mois : Tous !\n\n\n\nEbay : Tweak your page in real time, without leaving the comfort of your editor\n\nBrenton Simpsons d’Ebay, @appsforartists, nous a montré comment coder en live du React de son mac, avec le rendu affiché en temps réel sans reload sur un ipad.\n\nL’avantage d’un iPad étant sa taille qui lui permet de représenter 3 écrans d’iPhone 5 sur sa largeur, soit 3 états de son application.\nIl utilise « WebPack » et l’extension pour WebPack « react-hot-loader » de Dan Abramov.\n\nEbay a aussi open-sourcé un framework assez experimental (6 mois d’ancienneté) du nom d’Ambidex pour gérer du server side rendering avec React et Flux : https://github.com/appsforartists/ambidex\n\n\n\nData fetching for React Applications at Facebook\n\nJing Chen, @jingc, et Daniel Schafer, @dlschafer, nous ont présenté Relay, une nouvelle approche au pattern Flux orienté Data Fetching, permettant grâce à GraphQL de définir au niveau de son composant les data nécessaires.\nRelay se chargeant ensuite de générer les bons appels HTTP grâce à GraphQL.\n\nUne approche intéressante, mais qui parait très couplée au fonctionnement de Facebook et soulève pas mal de questions : dois-je modifier toutes mes API pour supporter GraphQL ? Quid de l’optimisation du cache coté API ? …\n\nRelay sera open-sourcé prochainement (ainsi que GraphQL j’imagine ?).\n\nBeaucoup plus d’infos sont disponibles ici : https://gist.github.com/wincent/598fa75e22bdfa44cf47\n\n\n\nCommunicating with channels\n\nJames Long, @jlongster, assez réputé via son blog https://jlongster.com et pour son travail chez Mozilla sur les Dev Tools de Firefox, a présenté une manière de communiquer entre composants via des « channels » en utilisant la librairie “ js-scp”  permettant de coder à la manière des « goroutine » de Go ou des « core.async » de Clojurescript.\n\n \n\n\n\nReact-router increases your productivity\n\nMichael Jackson, @mjackson, co-créateur du routeur le plus populaire de React “react router”, est venu avec Ryan Florence (l’autre co-créateur), nous expliquer les origines du routeur, l’inspiration très forte du router d’Ember.Js, ainsi que quelques techniques avancées d’utilisations (transitions, etc). Un excellent speaker et une introduction très drôle sur les origines de React-Router.\n\n\n  “Url should be part of your design process”\n\n\n\n\n\n\nFull Stack Flux\n\nPete Hunt, @floydophone, l’une des personnes responsable des origines de React et de son « open-sourcage », ancien Lead-Dev d’Instagram chez Facebook, a présenté un talk un peu particulier expliquant comment on pouvait, coté architecture serveur, reproduire le pattern Flux.\n\n\n  “shared mutable state is the root of all evil.”\n\n\n\n\nMaking your app fast with high-performance components\n\nJason Bonta, de l’équipe Facebook Ads, à l’origine de la création de React, a ciblé sa présentation sur les problèmes de performances que résout React.\nCoté Ads Manager, l’équipe doit faire des interfaces ultra complexes, avec notamment le besoin de présenter un nombre d’éléments très important dans un tableau.\n\nUn composant qui sera annoncé comme « open-sourcé » durant sa conférence : FixedDataTable\nVous pouvez aussi retrouver une « review » du composant ici : https://www.reactbook.org/blog/fixed-data-table-reactjs.html\n\nOnt été abordé :\n\n\n  le ReactAddons : PureRenderMixin\n  l’utilisation du shallowEqual sur le shouldComponentUpdate\n  Ainsi qu’une bonne pratique pour la réalisation des composants, qui est revenue plusieurs fois pendant la conf, consistant à englober le composant, dans un autre composant de type container ne contenant aucune « props ».\n\n\nEn résumé : \n\n\n\n\nFormat data and strings in any language with FormatJS and react-intl\n\nDernière conférence de la journée par Eric Ferraiuolo, @ericf, sur l’internationalisation et la manière de la gérer dans React, grâce à react-intl (open-sourcé par Yahoo).\n\nPour ceux qui douteraient encore de la complexité de gérer plusieurs langues, ainsi que les chiffres et pluralisations, et qui ont cette problématique sur un projet React, cette vidéo est un must-see.\nFormat.Js a aussi été cité et s’apparente à une collection de module Js pour l’internationalisation.\n\n\n\nHype\n\nRyan Florence a fini la journée sur un showcase d’exemple très intéressant. \nIl nous a aussi raconté son histoire, et comment il est devenu développeur : principalement, parce qu’il voulait toujours répondre “oui” quand on lui demandait si il pouvait faire quelque chose.\n\nBref, une excellente manière de finir la journée de manière fun avec quelques exemples très intéressants, notamment autour des “portals”.\n\nVous pouvez retrouver toutes les démos ici : https://github.com/ryanflorence/reactconf-2015-HYPE\n\nPour ceux qui douteraient encore des performances de React, je vous invite à regarder les 5-6 premières minutes de la vidéo.\n\n\n\nConclusion du premier jour\n\nBonne grosse claque sur cette première journée, notamment avec l’annonce de React Native. Nous avons eu le droit à une organisation absolument parfaite (snack, boisson chaude et froide à volonté) et des speakers de très grand talent (ce qui n’est pas toujours le cas de certaines conférences, surtout aussi ciblée que celle-là).\n\np.s: Retrouvez les retours sur la deuxième journée de la React conférence 2015.\n"
} ,
  
  {
    "title"    : "App Isomorphic: la Single Page App parfaite ?",
    "category" : "",
    "tags"     : " javascript, webperf, angular, react, flux, isomorphic",
    "url"      : "/2014/12/04/isomorphic-single-page-app-parfaite-react-flux.html",
    "date"     : "December 4, 2014",
    "excerpt"  : "Qu’est ce qu’une Single Page App (SPA) ?\n\n\n  « As rich and responsive as a desktop app but built with HTML5, CSS and Javascript »\n\n\nLes SPA se répandent de plus en plus, et deviennent un choix « commun » lorsque l’on veut développer un Front riche...",
  "content"  : "Qu’est ce qu’une Single Page App (SPA) ?\n\n\n  « As rich and responsive as a desktop app but built with HTML5, CSS and Javascript »\n\n\nLes SPA se répandent de plus en plus, et deviennent un choix « commun » lorsque l’on veut développer un Front riche (souvent câblé sur des API REST) que l’on souhaite :\n\n\n  testable (unitairement et fonctionnellement)\n  fluide (pas de rechargement d’url etc)\n  bien organisé\n  maintenable et évolutif\n  …\n\n\nLes Frameworks type AngularJs et EmberJs tiennent le haut du panier et ont largement fait leurs preuves, mais ils continuent à échouer sur deux sujets pourtant primordiaux dans beaucoup de cas :\n\n\n  La performance (dont le rendu initial)\n  Le référencement\n\n\nLa performance\n\nAujourd’hui, quand vous chargez une SPA, voici grossièrement ce qui se passe coté client :\n\n\n  Chargement du fichier HTML\n  Chargement des différents Assets (Css, image, scripts JS externe comme Angular et Jquery par exemple)\n  Ainsi que de l’intégralité du code JS de votre application (sauf si vous lazyloadez)\n  Execution de tout ce petit monde, qui va devoir savoir où vous êtes dans l’application afin de générer le HTML correspondant à l’état demandé.\n\n\nAvoir ces quelques secondes à attendre avant de se retrouver dans un état fonctionnel est peut être acceptable pour un backoffice. Mais ça l’est beaucoup moins pour un front riche.Et ce temps aura tendance à augmenter fortement, parallèlement à l’enrichissement de votre application.\n\nSi l’on se soucie un minimum des aspects de performances Web, c’est forcément dérangeant.\nEt d’un point de vue plus global, tout le monde sait aujourd’hui que la performance brute n’est pas le point fort de ces frameworks.\n\nLe référencement\n\nAutre sujet, qui peut être très problématique, si le site en question s’y prête. Ces applications vont fournir comme « source HTML » quelque chose de ce style (pour du Angular) :\n\n&amp;lt;!doctype html&amp;gt;\n&amp;lt;html class=&quot;no-js&quot;&amp;gt;\n&amp;lt;head&amp;gt;\n    ...\n&amp;lt;/head&amp;gt;\n&amp;lt;body ng-app=&quot;myApp&quot;&amp;gt;\n    &amp;lt;ng-view&amp;gt;&amp;lt;/ng-view&amp;gt;\n    &amp;lt;script src=&quot;scripts/vendor.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&quot;scripts/main.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n\n\n  Container qui servira à recevoir le HTML généré par votre appli JS une fois exécutée.\n\n\nDe base, Google (et autres moteurs/crawler) ne verra donc rien, tout votre contenu allant être injecté via JS dans votre balise ng-view. \nExcepté le fait qu’il parait que depuis des mois/années, Google commence à réellement crawler du JS … Si le site est important, cette supposition ne devrait pas suffire à vous convaincre, et vous avez raison.\n\nRassurez vous, à ce stade, des solutions existent pour fournir spécifiquement à Google une version correspondante aux snapshots HTML générés par vos applications.\nCes solutions sont accessibles soit en mode SAAS (payante et hébérgé), soit en mode Open-Source à héberger vous même. Je pense notamment à Prerender.io qui fait plutôt bien le job, et vous propose d’indiquer aux moteurs que vous faites une application de type « Ajax » en respectant les recommandations de Google.\n\nPrerender est composée de plusieurs briques :\nUn middleware applicatif (Rails, Node, Varnish, Nginx, etc selon votre infrastructure), qui va intercepter les moteurs et les renvoyer sur votre service de Prerender \nUn service de Prerender qui est une brique Node.js qui va lancer des HeadLess Browser (PhantomJS ou SlimerJs …) pour executer votre appli JS et renvoyer un snapshot HTML une fois le rendu JS terminé.\n\nLa solution permet à priori de faire le boulot, mais cela reste une gymnastique complexe, et beaucoup d’interrogations subsistent (pertinence, maintenance, stabilité, Page Rank, pondération vs sites classiques …)\n\nLa lumière au fond du tunnel ?\n\nVous l’avez donc compris, dans certains cas, les SPA basées sur des frameworks Js posent deux problèmes très gênants et difficilement résolvables.\nC’est là qu’entre en piste, une nouvelle façon de penser les SPA, grace à une librairie développée par Facebook : React.JS\n\nReact fait parler de lui car il commence à être utilisé massivement par des très gros acteurs Web, Facebook bien entendu pour ses composants Chat, ou son éditeur vidéo, Instagram pour l’intégralité du site, Yahoo Mail, Github avec l’IDE Atom, Khan Academy, NyTimes, Feed.ly …\n\nAu premier abord, React n’est qu’une librairie qu’on pourrait comparer à la partie Vue d’un Framework MVC (voir aux Directives d’Angular), mais il a la particularité d’être basé sur un Virtual DOM.\nCe qui parait au départ simplement une bonne idée pour avoir des performances bien supérieures à celle d’un framework MVC basé sur le DOM, et éviter par exemple les Dirty checking du DOM (qui explique en partie le manque de perf d’Angular), permet aussi d’utiliser ces mêmes composants coté serveur !\n\nC’est ce qu’on appelle l’approche « Isomorphic » .\n\nUn composant React n’est finalement qu’un module CommonJs et peut donc aussi bien être utilisé coté browser sur le client, que coté server dans du Node.Js (ou IO.js devrais-je dire maintenant ?).\nL’idée de l’isomorphisme est aussi d’être capable de servir le premier rendu directement par le serveur.\nExemple:\n\n\n  Vous accédez à votresite.com/votrepage.html\n  Votre serveur Node, construit votre page et sert le rendu HTML généré par votre appli au client\n  Il sert aussi votre application JS dans un Bundle (généré via du Gulp ou Grunt par WebPack ou Browserify)\n  Le client reçoit un fichier statique et l’affiche (sans attendre le moindre JS)\n  Il reçoit aussi le bundle Js\n  Une fois affiché, React sait reprendre la main sur votre appli afin de continuer en mode SPA pour la suite de l’application.\n\n\nEt là, vous répondez de manière parfaite aux deux points problématiques.\nGoogle n’y verra que du feu, et pourra crawler votre site entièrement comme si il n’était composé que de fichiers statiques. \nLa performance du premier rendu sera quasi imbattable, car ne nécessitant aucun JS !\n\nSur le papier, c’est juste le rêve ultime de tout développeur Front-end : tous les avantages d’une SPA sans les inconvénients !\n\nFacebook propose aussi sur son Github, une solution pour ceux ayant déjà un applicatif dans un autre language (ici PHP) : Server side rendering\n\nLa solution parfaite ?\n\nPresque.\nReact n’est au final que la partie Vue de votre application, il va falloir encore organiser tout ça. C’est ici qu’entre en compte Flux, un pattern d’architecture unidirectionnel proposé aussi par Facebook, à priori plus scalable que ne l’est le pattern MVC.\n\nMais là encore, l’approche de Flux est plutôt prometteuse, alors quel est le problème ?\n\n\n  Finalement c’est encore peu mature (déjà React et Flux, mais encore plus l’approche Isomorphic)\n  La montée en compétence n’est pas négligeable\n  Il n’y a pas vraiment de Framework comparable à date, et vous allez surement devoir réinventer la roue à certains moments (à suivre l’arrivée imminente de React Nexus notamment)\n  La documentation est très faiblarde encore\n  Les ressources très difficiles à trouver et de qualités très différentes\n  Pas vraiment de starter-kit ou générateur digne de ce nom\n  Le coté Isomorphic va aussi engendrer une certaine complexité :\n    \n      Est-ce que mon client reçoit bien le même état que celui qu’avait mon serveur au moment du rendu initial\n      Obligation de n’utiliser que des composants Isomorphic, typiquement un router qui fonctionne aussi bien coté client que serveur (React-Router ou Director), même chose pour les requêtes HTTP (Superagent par exemple) …\n    \n  \n\n\nSi malgré ces points, vous souhaitez tester cette approche, je vous conseille de regarder du coté de Yahoo, qui après avoir annoncé la migration de Yahoo Mail de PHP/YUI vers React/Flux Isomorphic a aussi publié quelques packages Open-Source très intéressants, pouvant constituer une bonne base de départ pour un projet isomorphic :\n\n\n  Fluxible-App\n  Flux-examples\n  ou cet exemple utilisant Fluxible-app : Isomorphic-React\n\n\nSi vous souhaitez plus d’infos sur React et Flux, je vous conseille ces deux articles en anglais de @andrewray:\n\n\n  React for stupid people\n  Flux for stupid people\n\n\nOu ce tuto chez nos amis de Jolicode, pour faire un Gifomatic avec React et Flux\n\nD’autres solutions existent aussi conservant la même approche, mais sur la base d’autres technos, notamment celle d’Airbnb: RendR, permettant d’utiliser du Backbone coté client et serveur.\n\nEt pour finir, si ces sujets vous passionnent tout comme nous, restez à l’écoute ici, d’autres posts pourraient arriver à l’avenir ;-)\n\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - troisième journée",
    "category" : "",
    "tags"     : " conference, velocity, webperf, devops, sysadmin",
    "url"      : "/velocity-europe-2014-day-3",
    "date"     : "December 3, 2014",
    "excerpt"  : "Velocity Barcelone, troisième journée\n\nLe troisième jour étant dédié aux tutoriaux, on passe de conférence de 45min à des ateliers de 1h30.\n\nExtreme Web Performance for Mobile Devices\n\nMaximiliano Firtman nous a dressé un portrait vraiment exhaust...",
  "content"  : "Velocity Barcelone, troisième journée\n\nLe troisième jour étant dédié aux tutoriaux, on passe de conférence de 45min à des ateliers de 1h30.\n\nExtreme Web Performance for Mobile Devices\n\nMaximiliano Firtman nous a dressé un portrait vraiment exhaustif du web mobile et de l’état actuel des navigateurs.\n\n\n\nEn gros c’est compliqué. Le marché est très fragmenté, certains constructeurs comme Samsung ajoute du bruit en diffusant massivement un navigateur modifié. L’usage des sites en webview depuis une application native n’arrange pas les choses (par exemple, l’application Facebook).\n\nAprès un rappel sur l’importance de la performance, l’orateur a distillé de nombreuses pratiques permettant de faire un web mobile plus performant.\n\nOn peut retenir :\n\n\n  Le RWD est un outil, pas une fin en soi,\n  il faut s’imposer de tester sur du hardware cheap avec une connection faible,\n  ne pas oublier le temps perdu sur le réseau (600ms mandatory network overhead),\n  ne pas oublier l’impact que le parsing du JS et le rendu CSS est bloquant,\n  utiliser les solutions de stockage coté client,\n  de très nombreux outils de simulation existent, il faut les maitriser.\n\n\nIl propose un site récapitulant toutes les informations délivrées : https://firtman.github.io/velocity/.\n\nSlides :\n\n \n  Extreme Web Performance for Mobile Devices - Velocity Barcelona 2014  from Maximiliano Firtman \n\n\n\nZero Downtime Deployment with Ansible\n\n\n\nSlides\n\nGithub Repo\n\nTutorial intéressant conduit par un développeur (sur un sujet à priori plus opérationnel) qui démontre bien la flexibilité et la simplicité d’Ansible.\n\nAprès avoir mener le tutorial à son terme vous aurez deployé deux machines avec du code Java, un load balancer NGINX, et une base de données PostgreSQL (utilisateur + base).\n\nA contre courant des systèmes de gestion de configurations comme SaltStack, Puppet ou Chef, Ansible est basé sur le modèle push et ne nécessite aucun agent, il repose entièrement sur SSH. D’autre part il mixe gestion de configuration et orchestration, ce que qu’on doit bien souvent faire via des outils tiers comme MCollective.\n\nLa simplicité de ce modèle en fait sa plus grande force. Ansible est capable de gérer dynamiquement les inventaires (de base c’est une liste statique contenue dans un fichier). Par exemple il est capable d’interroger les APIs Amazon, Google Cloud ou RackSpace pour récupérer la liste de vos machines, celles de votre Cluster VMWare ou n’importe quel script qui sortira une liste en JSON.\n\nAlors que Chef et Puppet offrent une DSL pour décrire votre infrastructure sous forme de code, Ansible a opté pour une description au format YAML. Sur l’Ansible Galaxy vous retrouverez tout les modules disponibles (quelques milliers) comme Nginx, PHP etc… Développés en Python, il est évidemment possible de faire soit même ses modules.\n\nLe déploiement avec zéro temps de panne peut être implémenté avec Ansible de la façon suivante:\n\n\n  récupération de la liste des machines\n  sortie du load balancer d’une machine (Ansible est compatible GCE et AWS)\n  mise à jour de la configuration (code et/ou logiciel)\n  période d’attente: vous spécifiez si un port TCP doit être disponible, un fichier, etc…\n  itération sur la machine suivante\n\n\nLe nombre de machine traitées en parallèle est bien entendu configurable.\n\n\n\nJe suis Ansible depuis quelques mois déjà et j’ai été conforté dans l’idée que c’est un excellent produit: pas d’agent, basé sur une brique solide qu’est SSH, et développé en Python :) La gestion de l’inventaire peut être délicate, mais un CMDB comme Collins de Tumblr ou un taggage précis peuvent résoudre l’équation.\n\nAnsible facilite le déploiement d’infrastructure immuable, le blue/green, violet et canary deployment de par son modèle. C’est un atout qui en fait à mon sens le meilleur système de gestion de configuration aujourd’hui.\n\nCependant je reste encore un peu dubitatif sur le déploiement et le rollback de code qui ne sont pas encore à la hauteur de Capistrano. Un aperçu du workflow et des schémas de développement auraient été aussi bienvenus.\n\n\n  Ansible Galaxy\n  Ansible Docs\n  How Twitter use Ansible\n  Ansible Tower (Payant)\n  Thoughts on deploying Symfony with Ansible\n\n\n\n\nLinux Containers from Scratch\n\n\n\nSlides\n\nQuelle est la différence entre le cloud, les containers et un repas gratuit ? Aucun n’existe :)\n\nJoshua Hoffman (SoundCloud) est dans le top 5 de mes orateurs préféré. J’ai beaucoup apprécié ce tutorial car il fait clairement la part entre virtualisation, containers, LXC et Docker (nom qui ne sera prononcé qu’à la fin lors des questions, pas de buzzword, de hype ni de marketing, merci Joshua).\n\n\n\nLe tutorial vous aménera à créer plusieurs containers portable, du plus simple ou plus complexe, avec les outils de bases du noyau. Vous apprendrez aussi à vous servir des cgroups, des namespaces process, network, et mount, et serez amené à utiliser des systèmes de fichiers unis, ici AUFS.\nJ’aurais bien aimé une démo avec le format de QEMU ou btrfs pour ce qui est des systèmes de fichiers unis au niveau bloc.\n\nCe tutorial est un must-do pour tout personne désirant s’initier aux architectures de containers. Le marketing relativement agressif de Docker ne doit pas faire oublier qu’il existe d’autres alternatives, et que Docker est un choix de design bien particulier pas forcement adapter à tous.\nEx: un container en 3 lignes:\n\n\n\nPour rappel:\n\nLXC/LXD = Ensemble d’APIs et d’outils dans l’espace utilisateur linux exposant les capacités d’isolation du noyau (cgroups, chroot, namespaces, selinux, iptables etc…), alternative légère à la virtualisation telle qu’on la connaît (avec Vmware par exemple)\n\nDocker = un des cas d’usage des containers, application unique, statique, immuable, single app delivery plateform\n\nLXC\n\nDocker F.A.Q\n\n\n\nCoreOps - CoreOS for Sysadmins\n\n\n\nGithub\n\nTutorial très attendu par beaucoup, Kelsey Hightower (CoreOS Inc.) nous a présenté l’écosystème de CoreOs et les problèmes qu’il tente de résoudre. Suite à la demande générale il nous a aussi fait une démonstration de Kubernetes, l’outil de gestion de containers de Google.\n\nCoreOS est distribution Linux accompagnée d’outils qui vise à penser le datacentre comme une seule machine (voir Mesos/Yarn). En d’autres termes, vous n’avez que faire de savoir quelle application tourne sur quel serveur. Le datacentre apparaît comme une entité unique où l’on déploie des applications.\n\nTechniquement, CoreOS est un Linux + systemd + docker + etcd + fleet. CoreOS est basé sur Chrome OS, épuré et léger, il bénéficie du système d’update en arrière plan bien connu de Chrome. On oublie donc le gestionnaire de paquets, les outils de debug (tcpdump etc..) et tout ce qui fait un Linux en mode serveur tel qu’on le connaît.\n\n\n  gentoo: parfum de distribution Linux (ex: Ubuntu, Debian, Centos)\n  systemd: alternative à SysV Init, le gestionnaire des démons, le premier programme lancé au démarrage (PID: 1)\n  docker: Système de containers légers, ensemble d’apis et librairies centrés sur le déploiement et la gestion d’application isolée du kernel.\n  etcd: base de données clé/valeur distribuée, utilisée pour centraliser la configuration et la découverte de service, fondée sur le protocole de consensus Raft.\n  fleet: SysV Init distribué (c’est la glue entre systemd et etcd), votre programme doit au minimum avoir 3 instances ? fleet s’en assurera !\n\n\n\n\n\n\nLa démonstration vous amènera à lancer 1 master et plusieurs machines “workers” et quelques containers Docker.\n\n\n\nKubernetes est la réponse de Google à la question des gestionnaires de containers disitribués.\n\nConstitué d’un certain nombre de composants qu’on ne détaillera pas ici, il permet de gérer des pods (un ou plusieurs containers qui doivent fonctionner localement sur le même host). Il intervient dans la répartition des applications dans le cluster, la distribution et l’ordonnancement des containers Docker.\n\nLiens:\n\n\n  CoreOS Doc\n  Kubernetes\n  Introduction to Kubernetes\n\n\n–\n\nResponsive and Fast: Iterating Live on a RWD Site\n\nCette conférence est globalement une redite des autres sur l’optimisation côté front. Colin Bendell d’Akamai nous présente plusieurs outils comme webpagetest, mais aussi des astuces pour tester sur Device depuis chrome. Il nous rappelle qu’il faut faire attention aux conditions de tests avec certains facteurs comme la connexion. Il faut faire aussi attention à limiter le nombre d’images, de ressources (js, css …). Un des gros problèmes sur un site responsive, est celui des images. Pour éviter de charger des images trop importantes, il faut utiliser la balise . Cette nouvelle balise n’étant pas disponible sur tous les navigateurs, il nous conseille d’utiliser un composant Picturefill. En ce qui concerne les CSS, il conseille d&#39;intégrer directement les css critiques dans le corps de la page et de ne charger, par la suite, que les css correspondants au device que l’on utilise. Pour conclure, l’utilisation d’un CDN avancé est hautement recommandée grâce à des options permettant de différencier navigateurs / devices.\n\nLiens :\n\n  Slide de la présentation\n\n\n\n\nBuild a device lab\n\n\n  “Qui a un placard avec pleins de devices en vrac qui n’ont ni câble, ni batterie et dont vous ne connaissez plus le mot de passe ?”\n\n\nJ’ai levé la main ;) .\n\nLara Hogan et Destiny Montague nous ont expliqué comment Etsy avait construit un device lab, permettant à leurs collaborateurs d’emprunter des appareils mobiles pour tester leurs applications, sites mobiles et newsletters.\n\nL’idée est d’outiller puissamment les équipes et de leur donner un accès extrêmement simple à un parc complet (même un chromebook pixel !) - afin d’assurer un maximum de tests sur les différents équipements.\n\nBien sûr il y a un device lab pour les équipes techniques et un autre pour le produit / marketing.\n\nLes sujets suivants ont été abordés :\n\n\n  choix des appareils\n  consommation électrique\n  le setup des devices (à l’aide d’un Mobile Device Management)\n  les tests\n  le réseau\n  un retour complet sur l’expérience utilisateur\n\n\nUn site complet dédié à leur conférence est disponible : https://larahogan.me/devicelab/.\n\nUne vidéo de la même conférence à New York est également en ligne :\n\n\n\n\n\nUne conférence un peu #old car déjà faite, mais toujours d’actualité concernant la problématique. Je suis bluffé par la capacité d’Etsy à mettre en oeuvre des moyens et des compétences sur des sujets qu’ils estiment importants. C’est sûrement en lien avec le succès que la société rencontre actuellement.\n\n\n\nConclusion\n\nUne conférence dense et intéressante, qui nous a donné l’opportunité de rencontrer pleins de gens intéressants et même de visiter (un peu) Barcelone !\n\n\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - seconde journée",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2014-day-2",
    "date"     : "November 24, 2014",
    "excerpt"  : "Velocity Barcelone, seconde journée\n\nDeuxième jour de conférence avec un programme encore plus chargé et quelques conférences alléchantes repérées au préalable.\n\nMorning Keynotes\n\nUpgrading the Web: Polyfills, Components and the Future of Web Deve...",
  "content"  : "Velocity Barcelone, seconde journée\n\nDeuxième jour de conférence avec un programme encore plus chargé et quelques conférences alléchantes repérées au préalable.\n\nMorning Keynotes\n\nUpgrading the Web: Polyfills, Components and the Future of Web Development at Scale - Andrew Betts (FT Labs)\n\nL’orateur fait remarquer que de nombreux systèmes existent pour packager et gérer les dépendances des applications backends, mais rien n’est disponible pour les composants webs. Il nous a présenté le projet Origami qui permet de réutiliser massivement des composants HTML.\n\n\n\nSlides : the Future of Web Development at Scale\n\nTroubleshooting Using HTTP Headers - Steve Miller-Jones (Limelight Networks)\n\nSlides : Troubleshooting Using HTTP Headers\n\nUn employé de Limelight nous a présenté comment l’ajout de headers dans une requête pouvait renvoyer des headers supplémentaires dans la réponse HTTP. Cela peut être utile pour débugguer et analyser un incident.\n  Cette présentation nous a rappelé, qu’en interne, nos gentils ops nous permettent déjà de faire ce genre chose sur nos proxy cache.\n\nMonitoring without Alerts - and Why it Makes Way More Sense than You Might Think - Alois Reitbauer (ruxit.com)\n\nAlois Reitbauer a évoqué la solution Ruxit développée depuis plus de trois ans. Cette solution consiste à installer un agent sur vos serveurs qui va automatiquement détecter des anomalies statistiques et corréler cette information avec d’autres déviations dans le but de trouver la root cause d’un incident.\n\nBeaucoup d’autres solutions de ce genre existent (et la plupart étaient dans le salon des sponsors). Nous n’avons pas été totalement convaincu de leurs capacités à détecter des root cause, mais elles sont toutes assez intéressantes et matures.\n\n\n\nLowering the Barrier to Programming - Pamela Fox (Khan Academy)\n\nPamela Fox nous a présenté l’initiative code.org, dont le but est de promouvoir l’enseignement de l’informatique (bon, apparement seulement aux US).\n\nElle a également donné quelques conseils si on veut s’investir dans l’enseignement de l’informatique à destination des plus jeunes. Par exemple créer un code club.\n\n\n\nSlides : Lowering the Barrier to Programming\n\nVelocity at GitHub - Brian Doll (GitHub)\n\nBrian a fait une présentation très … minimaliste. Il est revenu rapidement sur 7 ans de développement à GitHub et comment ils sont venus à développer l’Enterprise Edition. Il a évoqué différents problèmes que certains de leurs clients avaient et notamment avec l’utilisation de GE en environnement cloud.\n\nIl a donc annoncé le lancement de GitHub Enterprise 2.0 qui fonctionne maintenant sur AWS (et un changement de la grille tarifaire) !\n\n\n\nJ’ai profité d’un instant avec lui pour lui présenter GitHubTeamReviewer (un outil interne open-sourcé). Il était enchanté de découvrir ce qui avait été fait avec l’API de Github. Il a indiqué que l’entreprise travaillait actuellement sur des vues permettant de pallier aux problèmes résolus par GitHubTeamReviewer.\n\nHTTP Archive and Google Cloud Dataflow - Ilya Grigorik (Google)\n\nIlya Grigorik a présenté https://bigqueri.es/, un outil permettant d’interroger HTTP archive. La nouveauté est que le body des requêtes est maintenant conservé et que l’on peut l’analyser. Un engine Javascript a été intégré au SQL de bigqueries permettant de faire des requêtes très puissantes.\n\n\n\nPour ceux qui ne voudraient pas se plonger dedans, beaucoup de recherches faites par d’autres utilisateurs sont disponibles et abondamment discutées (exemple).\n\n\n\nWebpagetest-automation 2.0 - Nils Kuhn (iteratec GmbH), Uwe Beßle (iteratec GmbH)\n\nWebpagetest est un outil formidable mais il est difficile à automatiser. Les orateurs ont présentés un outil pour le faire, permettant donc de réaliser une mesure continuelle de la webperf avec un parcours utilisateur complet - démonstration à l’appui.\n\nLeur travail est disponible sur GitHub sous licence Apache : https://github.com/IteraSpeed/OpenSpeedMonitor. Un grand merci &amp;lt;3 ! (à 10 minutes sur la vidéo).\n\n\n\n\n\n\n\nEtsy’s Journey to Building a Continuous Integration Infrastructure for Mobile Apps - Nassim Kammah (Etsy)\n\nUne parmi les très nombreuses conférences Etsy sur la Vélocity (le moment de renouveller les conférenciers ?). Nassim Kammah nous a expliqué comment Etsy délivrait ses applications iOS.\n\nLa livraison des applications sous iOS est au même stade que la diffusion des logiciels via CD-ROMs. Partant de ce constat un système de build (avec 25 mac-minis derrière) a été mis en place à chaque commit sur le master. On ne peut pas délivrer une version de l’application tous les jours aux clients, mais on peut le faire pour les employés (and eat your own dog food) !\n\nIl y a également un système de gamification, autour de l’application livrée journalièrement, afin de motiver tout le monde à trouver des bugs.\n\n\n\nDes tests unitaires sont mis en place, ainsi que des tests fonctionnels avec AppThwack. Il est intéressant de constater qu’ils n’attendent pas, pour les tests fonctionnels, une réussite à 100% de la suite mais une tendance positive.\n\nLes équipes ont également mis en place des testing dojos dans lesquels les ingénieurs QA encadrent des salariés d’Etsy et testent à fond les applications.\n\nOn peut retrouver tous les éléments de cette conférence sur le blog technique d’Etsy.\n\n\n\nRecycling: Why the Web is Slowing Your Mobile App - Colin Bendell (Akamai)\n\nPourquoi recycler nos contenus pour les applications mobiles ?\n\n\n  accélérer le time to market.\n  réduire le risque\n\n\nLes APIs encouragent le recyclage.\n\n\n\nColin Blendel nous encourage à utiliser les mêmes recettes que pour les navigateurs web et à en ajouter d’autres :\n\n\n  gérer le pool de connexions en groupant les appels par domaine (quitte à les passer séquentiellement, par exemple, si des cookies sont utilisés),\n  surveiller les packet eaters (headers inutiles, Set-Cookies répétés),\n  setter correctement Content-Type sur des types standard (les exemples de content-type tirés des logs d’Akamai sont assez drôles, comme par exemple test/binary ^^ !),\n  faire un minimum de redirections,\n  fragmenter son cache au minumum (quitte à calculer des clés plus consistantes coté client),\n  ajouter du cache (Max-Age: 30s c’est à peu près du temps réel et ça change tout pour un CDN),\n  préfetcher les urls présentes dans les retours d’API, car on va surement en avoir besoin immédiatement après,\n  ne pas hésiter à mettre CRUD au placard et merger plusieurs appels API en un seul ; il faut trouver une balance efficace pour bien gérer la webperf.\n\n\nUne présentation dense et vraiment intéressante !\n\nSlides : Why the Web is Slowing Your Mobile App\n\n\n\nBreaking News at 1000ms\n\nLe Guardian est un journal Anglais présent sur le web et sur tout type de device. Ils ont récemment fait une refonte de leur site pour passer à une version Responsive avec pour challenge d’afficher son contenu en moins d’une seconde.\n\nLe Guardian c’est 110 000 utilisateurs, 7000 différents devices. L’ancien site avait un début de rendu en 8 secondes pour un affichage complet en 12. Avec la nouvelle version le site s’affiche en 1 seconde et le chargement complet au bout de 3. Quelles sont les principales optimisations ?\n\nPour commencer, il faut charge le contenu important pour l’utilisateur en premier, à savoir le menu, l’article, et le widget d’article populaire. Le reste du contenu sera chargé dynamiquement en JS.\n\nEn ce qui concerne le css, c’est la même chose. Les CSS importantes (critiques) qui concernent l’article et le rendu global sont intégrées inline. Ainsi, nous n’avons pas de blocage du rendu de la page. Le reste des css est chargé via Javascript. Avec ce système, on gagne au moins une demi-seconde sur le début d’affichage du contenu.\nPour gagner en fluidité pour les prochains affichages, le css est stocké en localStorage. On gagne ainsi des ressources pour les prochains chargements.\n\nPour les fonts ? C’est la même chose, elles sont mises en cache dans le localStorage pour supprimer de nouveaux chargements.\n\nEnfin le gros morceau : les images ! Elles sont chargées de façon asynchrone en lazyloading. Cela permet de ne pas bloquer le rendu principal de la page.\n\nEn complément, ils ont mis en place des outils, notamment pour monitorer dans Github la taille des Assets afin de vérifier qu’il n’y a pas de grosses variations.\n\nAvec ces optimisations et un système de Proxy qui va gérer les données mises en localStorage, le site peut même être accessible en mode offline.\n\n\n  Github du Front\n  Slide de la présentation\n\n\n\n\nOffline-first Web Apps\n\nMatt Andrews nous présente comment rendre une application web disponible Offline.\n\nPlusieurs contraintes peuvent nous pousser à avoir besoin d’une app (signet d’accueil) disponible même sans connexion. Que ce soit un article dans le métro ou une carte au milieu de nulle part sans connexion, il y a une réelle attente utilisateur.\n\nPremièrement, il faut activer AppCache en précisant qu’il faut faire un petit Hack pour qu’il soit vraiment utile (voir slide).\n\nEnsuite l’utilisation de plusieurs outils nous permet d’arriver à nos fins :\n\n  Utilisation de FetchApi : Il permet de remplacer nos appels Ajax avec une fonction succès , d’erreur et les Promises pour charger le contenu, ou lire le cache en cas d’absence de connexion.\n  Cache API : Il permet de choisir des Url a mettre en cache. Ainsi que de forcer le contenu de ces urls dans le code.\n  Service Worker : Il permet d’intercepter les events de chargement pour ensuite appeler le système de Cache API.\n\n\nToutes ces optimisations nous permettent d’accéder au site en Offline. Mais ces optimisations nous permettent aussi d’optimiser le chargement de nos pages puisqu’on limite le nombre d’appels HTTP avec la mise en cache de certaines ressources.\n\nSlide de la présentation\n\n\n\nLook, Ma, No Image Requests!\n\nPamela Fox nous présente comment elle a optimisé les images d’un site internet.\n\nLa première astuce est de compresser ces images au maximum. Il existe des outils online comme le site TinyPng qui compresse vos images et vous permet de les télécharger directement.\n\nDeuxième astuce, mettre les images dans les css en base 64. \nA noter qu’il existe des outils javascript qui effectuent la conversion dans les css à l’aide d’un petit commentaire en bout de ligne (voir les slides de présentation).\n\nTroisième solution : Les Fonts ! \nPour remplacer les petites images et surtout pour remplacer les sprites qui ne sont pas forcément adaptés, vous pouvez utiliser des Fonts. L’avantage des fonts est qu’elles peuvent s’adapter facilement en taille et en couleur … Des outils existent déjà pour les générer : Font Awesome.\n\nAutre astuce, le differ de chargement des images. Pamela nous propose son outils javascript, qui va permettre de vous simplifier les chargements. Il est aussi possible de ne charger que les images présentes à l’écran et de charger les suivantes lors du scroll. (lazyload).\n\nPour les vidéos la même astuce est possible. Puisque les vidéos sont à présent chargées dans des iFrame, leur contenu peut être chargé de façon différé. Attention, il ne faut pas remplir le href par une url blank, sinon on perd en temps de chargement.\n\nSlide de la présentation\n\n\n\nMicroservices - What an Ops Team Needs to Know\n\nSlides: Microservices - What an Ops Team Needs to Know\n\n\n\nLe buzzword est lâché. Le propos n’était pas ici de troller autour de la notion de micro-services, de l’implémentation ou de leur utilisation, mais plutôt du changement que cela implique pour les équipes d’exploitation.\n\nSouvent considéré comme le goulot d’étranglement de la chaîne de mise en prod, l’exploit’ regarde les architectures de micro-services avec circonspection : en plus d’avoir des dépendances entre eux, les composants sont mis à jour indépendamment et régulièrement, on peut donc vite tout casser en prod.\nPourtant en fournissant des services de bases et des outils aux équipes de développement, on peut augmenter leur autonomie et la disponibilité des infras.\n\nCela passe par:\n\n\n  automatiser les VMs ou les containers en prod comme en dev\n  un système de métriques “As a Service” (similaire à graphite / statsd)\n  un service de log central (logstash/heka/fluentd)\n  un outil de déploiement (capistrano/deployinator)\n\n\nCes outils et services ainsi fournis vont permettre à l’exploitation de se concentrer sur des problématiques plus complexes. En effet les microservices ont besoin d’outils de diagnostics plus poussés (on citera au passage Zipkin), d’alerting et de monitoring spécialisés par exemple.\n\n\n\nQui dit droits, dit devoirs, et là je paraphraserai notre orateur Michael Brunton-Spall:\n\n\n  Give developers pagers too !\n\n\n\n  Developers should be exposed to the pain they cause\n\n\nCela s’inscrit totalement dans le mouvement “You build it, you run it”, où les équipes de développement sont responsables de leur code depuis la conception jusqu’à la maintenance en production.\n\n\n\nIt’s 3AM, Do You Know Why You Got Paged ?\n\nSlides: It’s 3AM, Do You Know Why You Got Paged ?\n\nRyan Frantz nous a rappellé quelques éléments de bon sens concernant les alertes:\n\n\n  un contexte: quel hôte ? serveur ? service ? l’impact front / back ?\n  l’historique de l’alerte et de la métrique: état de la métrique il y a 5min, 15min, 1 jour, 1 semaine, combien de fois a sonné l’alerte aujourd’hui ?\n  la raison d’être du check (rédigée par le créateur du check)\n  des couleurs et mise en forme permettant de trouver visuellement l’information le plus rapidement possible (rappel il est 3 heure du matin, et peut être daltonien, pensez donc bien à vos codes couleurs)\n\n\n\n\n\nSeule une alerte critique doit vous faire lever à 3H du matin, un volume disque à 80% plein n’est pas réellement grave, cependant si son taux de remplissage est passé de 1% par heure à 300% par heure, cela peut devenir problématique.\n\nRyan nous a ensuite présenté nagios-herald. Ce plugin nagios permet de multiplexer une alerte dans différent services (cf schéma) ci dessous.\n\n\n\nPour ma part je préfère Sensu qui intègre de base ce type de mécanisme. On peut affecter à une alerte un groupe de handlers (alerte hipchat + graphite + logstash par exemple)\n\n\n\nCustomizing Chef for Fun and Profit\n\n\n\nSlides: Customizing Chef for Fun and Profit\n\nEn suivant les étapes d’application d’une recette Chef, Jon Cowie a distillé son savoir sur la personnalisation de Chef.\n\nIl nous a par exemple démontré qu’il était très simple de développer son propre plugin ohai et ses propres handlers.\n\nJ’ai apprécié le passage sur la gestion des événements Chef, en effet la sortie en ligne de commande n’est qu’une des façons de récupérer les logs, les événements sont basés sur un système de pub/sub, on pourrait très bien imaginer la publication en live stream dans un redis ou autre.\n\nPar ailleurs Jon vient de publier un livre sur le sujet:\nO’Reilly - Customizing Chef\n\n\n\nMega quiz Velocity\n\nPerry Dyball et Stephen Thair avaient préparé un quiz interactif avec les participants à la conférence. Des questions diverses et variées défilaient sur le grand écran et une application web permettaient à chacun d’y répondre. Un moment fun animé par deux animateurs survoltés.\n Malheuresement il semble que l’application n’aient pas tenu la charge et personne n’a pu voté après la seconde question (la prochaine fois ils devraient nous confier le projet :) ), mais un système de fallback a été prévu, basé sur des feuilles de papier de couleur à brandir bien haut pour répondre aux questions.\n\n\n  merci le papier ! :)\n\n\n\n\nConclusion\n\nFin des conférences et direction les soirées offertes par Facebook (où nous avons pu discuter avec Santosh Janardhan, responsable des infrastructures de Facebook ^^ !) et Dyn.\n\nLe résumé de la première journée est également disponible.\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - premier jour",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2014-day-1",
    "date"     : "November 19, 2014",
    "excerpt"  : "Velocity Barcelone, premier jour\n\nBaptiste, François et Olivier ont eu la chance de participer à la Vélocity Conférence Europe 2014 qui avait lieu cette année à Barcelone.\n\nVoici le compte rendu des conférences et des moments qui les ont marqués.\n...",
  "content"  : "Velocity Barcelone, premier jour\n\nBaptiste, François et Olivier ont eu la chance de participer à la Vélocity Conférence Europe 2014 qui avait lieu cette année à Barcelone.\n\nVoici le compte rendu des conférences et des moments qui les ont marqués.\n\nMorning Keynotes\n\nLes keynotes du matin semblaient être scénarisées sur différents points que les organisateurs de la conférence voulaient mettre en avant.\n\nLife after human error - Steven Shorrock (EUROCONTROL)\n\nSteven Shorrock n’est pas un homme de l’IT, mais travaille autour de la sécurité aérienne. Il se définit comme un ergonomiste des systèmes. Il a présenté comment, autour des erreurs humaines, “les mots créaient le monde” et entrainaient immédiatement un jugement social (“négligence” est évidement plus connoté que “erreur d’attention”). Il peut y avoir des erreurs dans la définition d’une erreur. Qualifier une erreur demandait une définition précise de standards et de contextes. \nIl a également conseillé d’étudier les cas de fonctionnement normaux ; ne pas faire seulement des post-mortem mais des pre et des no mortem.\n\n\n\nUne présentation intéressante sur l’incident et l’erreur.\n\n\n\nMaximize the Return of Your Digital Investments - Aaron Rudger (Keynote Systems)\n\nUne présentation sponsorisée bien faite, montrant les difficultés de communication entre deux populations (IT et biz en l’occurence) et comment un outil performant et agréable peut aider à combler ce gap.\nChez M6Web nous utilisons grafana, et il est vrai que cet outil pourrait largement sortir du périmètre de l’IT.\n\nSlides : Maximize the Return of Your Digital Investments\n\n\n\nAlways Keep an Eye on Your Website Performance - PerfBar Khalid Lafi (WireFilter)\n\nUne rapide démonstration d’un outil en javascript à installer sur les postes de vos développeurs et permettant d’afficher des alertes si un site en production (ou ailleurs) dépasse un certain seuil.\n\nA découvrir : PerfBar\n\n\n\nThe Impatience Economy, Where Velocity Creates Value - Monica Pal (Aerospike Inc.)\n\nIl y a une génération on attendait 10 jours un échange de courrier postal, aujourd’hui un adolescent vérifie son téléphone toutes les 10 secondes ! Nous sommes moins attentifs, plus impatients.\nDe ce constat Monica Pal explique comment les backend web doivent s’adapter et servir de plus en plus d’informations contextualisées : search, sort, recommand, personalize.\n\nSlides : The Impatience Economy\n\n\n\nRecruiting for Diversity in Tech - Laine Campbell (Pythian)\n\nUn thème récurrent de la velocity de cette année. Laine explique comment l’ascenseur méritocratique est cassé et que seule une démarche volontaire permettra d’augmenter la diversité dans les entreprises.\n\nSlides : Recruiting for Diversity in Tech\n\n\n\nBetter Performance Through Better Design - Mark Zeman (SpeedCurve)\n\nLa dernière keynote était vraiment excellente. Mark Zeman, venu de Nouvelle Zélande, a expliqué comment le processus créatif pouvait aider à améliorer la performance. Dans ce but il a proposé de redesign the design process.\n\n  se fixer certains principes/objectifs de performance dès le départ\n  ajouter les designers dans la feature team et itérer via des prototypes\n  de partager le savoir sous forme d’informations visuelles (graphique mais aussi sous forme d’un bookmarklet indiquant quelle partie d’un site met du temps à charger)\n\n\n\n\n\n\nJe vous invite vivement à regarder sa vidéo :\n\n\n\n\n\nIT Janitor, How to Tidy Up - Mark Barnes (Financial Times)\n\nCe manager au Financial Times a expliqué comment le journal a été touché de plein fouet par la révolution du web mobile et a dû s’adapter très rapidement.\n\n\n\nIl a expliqué quelle stratégie il a adoptée pour tuer ou refaire les vieux systèmes et comment, en premier lieu, il a vendu le projet à ses supérieurs.\nIl a tout d’abord présenté le TCO de ce qu’il a appelé la version ”classic” de ft.com (la carotte) puis a appuyé sur la peur de l’incident et les problèmes de sécurité (le bâton ; le journal ayant été la cible des pirates syriens).\n\nAprès une analyse fine du traffic il a ensuite appliqué ces stratégies :\n\n  tuer directement une application inutile (il y en avait), quitte à la rallumer si quelqu’un finalement en à l’usage :) (et couper un serveur Solaris avec 1833 jours d’uptime !)\n  reécrire l’application et la redéployer sur le nouveau système\n  tuer une application et écrire plusieurs autres (découpage en micro services)\n\n\nSon crédo était ”try to make the right thing easier.” Ainsi les projets basés sur la nouvelle stack disposait out of the box de fonctionnalités de monitoring et de log. Cela a beaucoup motivé les équipes de développements.\n\nAu final la purge du legacy a apporté :\n\n\n  un gain de 30% de performance\n  un meilleur TTM\n  de substantiels retours sur investissement\n\n\nSlides : IT Janitor - How to Tidy Up\n\n\n\nMansplaining 101: Cisadmin Edition - Marni Cohen (Puppet Labs)\n\nLa conférence la plus geek de la journée. La conférencière a ouvert un terminal et a tapé\n\nbrew install feminism\n\n\n\n\nLa conférence était très sincère et didactique sur comment mieux intégrer les femmes dans l’IT.\n\nVoici les scripts et les ressources qu’elle a présentés : https://gitlab.com/marni/mansplaining\n\n\n\nBuilding the FirefoxOS Homescreen - Kevin Grandon (Mozilla)\n\n\n\nSlides : Building the FirefoxOS Homescreen\n\nConférence de présentation de l’OS pour smartphone de Firefox.\n\nLors de cette présentation, Kevin Grandon Ingénieur chez Mozilla nous a présenté le nouvel OS, et nous a initié à la programmation sur ce dernier.\nCe nouvel OS est donc basé sur des langages simples : HTML / CSS / Javascript.\n\nLe développement est donc assez facile à prendre en main, le débugage aussi car on peux monitorer tout ce qu’il se passe sur le device de test via un firebug dédié.\n\n\n\nDon’t Kill Yourself : Mobile Web Performance Tricks that Aren’t Worth it, and Somme that Are - Lyza Gardner (Cloud Four)\n\nOptimisations pour le web mobile.\n\nLyza Gardner nous a présenté sa vision de l’optimisation sur web mobile. Elle nous a tout d’abord fait un compte rendu sur son expérience personnelle. Liza a cherché via différentes analyses (speedIndex…) à trouver une relation entre temps de chargement, nombres d’assets etc… Et la conclusion qu’elle mettait en avant, c’est qu’il n’y avait pas de recette magique. \nElle a ensuite fait la parallèle entre le web lors de ces débuts qui était limité par le débit de nos connexions de l’époque, et le web mobile tel qu’il est actuellement. Ainsi certaines optimisations de l’époque sont adaptables, et même toujours valables, à nos problématiques actuelles.\nSelon elle, il ne faut pas optimiser un site pour le mobile, mais l’optimiser tout court. Elle propose de se fixer des objectifs, par exemple se fixer une limite de nombre d’appel asset. Mais surtout d’optimiser / limiter les images puisque 62% du trafic d’un site correspond a ces dernières.\n\n\n\nWhat are the Third-party Components Doing to Your Site’s Performance? - Andy Davies, Simon Hearne (NCC Group)\n\nSlides : Third-party components and site performance?\n\nNous utilisons tous des « Third-Party » sur nos sites, mais est-ce une bonne idée ?\n\nUn Third-Party est un script que nous chargeons depuis un autre site. Par exemple : Google Analitycs. Il existe différents type de Third-Party : la publicité, les analyseurs de trafic … La problématique est que nous ne pouvons pas controller ces outils. Nous n’avons pas la main sur le temps de chargement, la disponibilité de l’outils, et cela peut influer sur l’expérience utilisateur et la qualité de nos services.\n\nPour conclure, il faut trouver le bon compromis entre ce que nous apporte le Third-Party et ce qu’il peut nous coûter …\n\n\n\nGuide to Survive a World Wide Event - Almudena Vivanco, Mateus Bartz (Telefónica\n\nSlides : Survive a World Wide Event\n\nRetour d’expérience de Movistar TV, une chaîne payante multi-support qui a diffusé la coupe du monde en Espagne, au Brésil et en Argentine.\n\nCette société s’est confrontée à une problématique de traffic avec des pics de connexions importants en peu de temps. La société devait diffuser la coupe du monde FIFA 2014 dans plusieurs pays et sur plusieurs devices différents. Après des tests en condition réelles avant le début de la compétition, ils se sont aperçus qu’ils ne pouvaient pas gérer le pic de connexion qui arrivait entre 5 minutes avant le coup d’envoi et 5 minutes après, ainsi qu’à la reprise du match et début de deuxième mi-temps.\nIl a donc fallu tout refaire à plusieurs niveaux :\n\n  Création d’un CDN en interne\n  Refonte globale du système de connexion pour pouvoir supporter les pics.\n  Mise en place de monitoring via Graphite\n  Mise en place de Tests\n\n\nMise en avant de beaucoup de problématiques :\n\n  Multi plateforme\n  Déploiement sur plusieurs continents (Amérique du Sud, Europe)\n  Rassembler 11 outils de monitoring en un seul.\n\n\n\n\nIs TLS Fast yet ?\n\nSlides : Is TLS Fast yet ?\n\nTL;DR = Oui, il pourrait l’être !\n\nLe talent d’Ilya pour les conférences techniques a une fois de plus fait ses preuves. \nTout en détaillant l’utilité de TransportLayerSecurity (compression, vérification d’erreurs, authentification, chiffrement…) Ilya nous prouve que dans le meilleur des cas, un RTT supplémentaire est nécessaire et l’impact CPU très faible.\n\nOutre l’utilisation des dernières versions du Kernel, d’OpenSSL et de votre OS serveur, la performance de TLS passe aussi par la réutilisation d’éléments négociés lors de la première (et coûteuse) poignée de main. Cette optimisation se fait coté serveur en conservant les “sessions identifiers” coté serveur ou coté client avec un “cookie” chiffré, le “session ticket”. Il faudra bien entendu ajuster la durée de cache et/ou les timeouts (~ 1 jour).\n\nUne erreur fréquemment commise consiste à ne pas intégrer le certificat intermédiaire (peu de CA s’autorise à signer votre certificat avec leur CA Root) dans le certificat serveur ce qui a pour conséquence de stopper le render, ouvrir une nouvelle connexion tcp et https pour récupérer ce dernier chez l’autorité de certification.\n\nL’OSCP stappling permet lui d’inclure directement la réponse OCSP et ainsi éviter le même problème de blocage du rendu, connexion à un tiers etc…\n\nL’utilisation hasardeuse de redirection 301 peut considérablement augmenter le Time To First Byte de votre site, il est donc fortement conseillé de bien analyser ses chaînes de redirections (ex: https://domain.com =&amp;gt; https://www.domain.com =&amp;gt; https://www.domain.com) et d’utiliser HSTS. Ce header émis par le serveur permettra au navigateur de mettre en cache la décision de redirection vers https.\n\nLe talk s’est terminé par un tableau comparatif fort intéressant des serveurs HTTP et des CDNs concernant tous ces aspects.\n\nQuelques liens supplémentaires:\n\n\n  https://www.ssllabs.com/ssltest/\n  https://www.feistyduck.com/books/bulletproof-ssl-and-tls/\n\n\n\n\nMonitoring: the math behind bad behavior\n\nSlides : the math behind bad behavior\n\nLa détection d’anomalies dans les flux continus de données de type Time Series n’est pas une chose aisée.\n\nSe baser sur un percentile, une moyenne ou une mediane uniquement ne permet pas de capturer les phénomènes de saisonnalité et d’anomalies.\n\nThéo nous a proposé une méthode de détection de ces dernières appelée “lurching windows”. Sur des fenêtres de temps glissantes, on applique la méthode CUSUM (Cumulative Sum), qui somme les données en affectant un poids relatif (en réalité la probabilité que cette valeur existe).\n\nA voir: https://en.wikipedia.org/wiki/CUSUM\n\n\n\nWhat ops can learn from design - Robert Treat (OmniTI)\n\nSlides : What ops can learn from design\n\n“Un designer est quelqu’un qui design”.\n\nDerrière cette lapalissade se cache en réalité plusieurs concepts importants à intégrer pour toutes personnes produisant un code, un service utilisé par un tiers.\n\nNous sommes tous des designers. Il est donc indispensable de mettre en oeuvre 3 mécanismes simples pour faciliter l’utilisation de votre code/service.\n\nLe “Feedback”: le bon code de retour lors de l’échec d’un script, un message intelligible et contextualisé dans un log d’erreur applicatif, le “natural Mapping”: -d dans une option en ligne de commande pour indiquer –database, et le “force functions”: sous Unix, kill est par défaut non destructif, il faut forcer avec kill -9 pour tuer définitivement un processus, tout comme on vous force à fermer la porte de votre micro-onde pour le mettre en marche.\n\nConférence intéressante qui vous fera sentir moins coupable de ne pas savoir si il fallait pousser ou tirer une porte :)\n\n\n\nStatistical Learning-based Automatic Anomaly Detection @Twitter\n\nArun Kejariwal est maintenant un habitué de la Velocity, j’avais particulièrement apprécié sa présentation l’année dernière à Londres sur la détection d’anomalies chez twitter.\n\nL’ojectif est toujours le même: prédire la capacité pour ajouter du matériel en datacenter, détecter des événements particulier, distinguer le spam du trafic normal etc…\n\nLeur méthode est relativement identique à ce qui avait été présenté l’année dernière: sur deux semaines de données on applique un traitement du signal pour décomposer et filtrer la saisonnalité. Il “suffit” ensuite d’appliquer une regression ou un ESD sur les résidus pour détecter d’éventuelles anomalies.\n\nChose à savoir: Twitter va publier un package R contenant ces fonctions et algorithmes, qui seront donc utilisables par le commun des mortels !\n\nConclusion\n\nUne première journée intéressante et intense, sous le soleil de Barcelone !\n\n\n\nLe résumé de la seconde journée est également disponible.\n"
} ,
  
  {
    "title"    : "Configuration dynamique avec Symfony ExpressionLanguage",
    "category" : "",
    "tags"     : " configuration, symfony, cytron",
    "url"      : "/symfony-expression-language",
    "date"     : "November 17, 2014",
    "excerpt"  : "Grâce à notre bundle MonologExtra, nous avons la possibilité d’inclure des informations statiques dans le contexte de nos logs.\nNous souhaiterions maintenant avoir aussi d’autres informations plus dynamiques comme le nom de l’utilisateur.\n\nPour ce...",
  "content"  : "Grâce à notre bundle MonologExtra, nous avons la possibilité d’inclure des informations statiques dans le contexte de nos logs.\nNous souhaiterions maintenant avoir aussi d’autres informations plus dynamiques comme le nom de l’utilisateur.\n\nPour cela, nous avons donc ajouté la possibilité de configurer une expression qui sera évaluée par le composant ExpressionLanguage de Symfony de cette manière :\n\nm6_web_monolog_extra:\n    processors:\n        userProcessor:\n            type: ContextInformation\n            config:\n                env: expr(container.getParameter(&#39;kernel.environment&#39;))\n                user: expr(container.get(&#39;security.context&#39;).getToken() ? container.get(&#39;security.context&#39;).getToken().getUser().getUsername() : &#39;anonymous&#39;)\n\nPour interpréter cette expression, nous avons injecté dans notre processeur Monolog une instance de ExpressionLanguage ainsi que le container :\n\nservices:\n  m6_web_monolog_extra.expression_language:\n    class: Symfony\\Component\\ExpressionLanguage\\ExpressionLanguage\n    public: false\n  m6_web_monolog_extra.processor.contextInformation:\n    abstract: true\n    class: M6Web\\Bundle\\MonologExtraBundle\\Processor\\ContextInformationProcessor\n    arguments:\n      - @service_container\n      - @m6_web_monolog_extra.expression_language\n    calls:\n      - [ setConfiguration, []]\n\nNous utilisons une définition de service abstraite qui sert de modèle pour les services qui sont générés à partir de la configuration sémantique gérée par l’extension du bundle :\n\n&amp;lt;?php\nforeach ($config[&#39;processors&#39;] as $name =&amp;gt; $processor) {\n    $serviceId = sprintf(&#39;%s.processor.%s&#39;, $alias, is_int($name) ? uniqid() : $name);\n\n    $definition = clone $container-&amp;gt;getDefinition(sprintf(&#39;%s.processor.%s&#39;, $alias, $processor[&#39;type&#39;]));\n    $definition-&amp;gt;setAbstract(false);\n\n    $tagOptions = [];\n    if (array_key_exists(&#39;channel&#39;, $processor)) {\n        $tagOptions[&#39;channel&#39;] = $processor[&#39;channel&#39;];\n    }\n    if (array_key_exists(&#39;handler&#39;, $processor)) {\n        $tagOptions[&#39;handler&#39;] = $processor[&#39;handler&#39;];\n    }\n    $definition-&amp;gt;addtag(&#39;monolog.processor&#39;, $tagOptions);\n\n    if (array_key_exists(&#39;config&#39;, $processor)) {\n        if ($definition-&amp;gt;hasMethodCall(&#39;setConfiguration&#39;)) {\n            $definition-&amp;gt;removeMethodCall(&#39;setConfiguration&#39;);\n            $definition-&amp;gt;addMethodCall(&#39;setConfiguration&#39;, [$processor[&#39;config&#39;]]);\n        } else {\n            throw new InvalidConfigurationException(sprintf(&#39;&quot;%s&quot; processor is not configurable.&#39;, $processor[&#39;type&#39;]));\n        }\n    }\n\n    $container-&amp;gt;setDefinition($serviceId, $definition);\n}\n\nEt l’expression est finalement évaluée par le processeur en utilisant le composant quand la valeur est de la forme expr(...), ceci permettant de garder une compatibilité ascendante avec les configurations statiques précédentes.\n\n&amp;lt;?php \nprotected function evaluateValue($value)\n{\n    if (preg_match(&#39;/^expr\\((.*)\\)$/&#39;, $value, $matches)) {\n        return $this-&amp;gt;expressionLanguage-&amp;gt;evaluate($matches[1], [&#39;container&#39; =&amp;gt; $this-&amp;gt;container]);\n    }\n    return $value;\n}\n\nAvec la configuration présentée au début, nous récupérons ainsi l’environnement et l’utilisateur connecté dans le contexte de nos logs.\n\nMonologExtraBundle est disponible en open-source sur le compte GitHub de M6Web.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #7",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2014/11/13/m6web-dev-facts-7.html",
    "date"     : "November 13, 2014",
    "excerpt"  : "Ça faisait un moment ! Voici le retour des devfacts !\n\nReproduction\n\n  Je ne sais pas si ça corrige le bug qu’on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nAu moins, c’est efficace\n\n  Quand je fais un “echo $id”, ça affich...",
  "content"  : "Ça faisait un moment ! Voici le retour des devfacts !\n\nReproduction\n\n  Je ne sais pas si ça corrige le bug qu’on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nAu moins, c’est efficace\n\n  Quand je fais un “echo $id”, ça affiche l’id\n\n\nProprement sale\n\n  C’est pas forcément plus propre, mais c’est moins sale\n\n\nA une vache près !\n\n  C’est à peu près approximatif …\n\n\nIl faut savoir ce qu’on veut\n\n  C’est pas prévu pour être utile\n\n\nC’est louche.\n\n  Commençons par comprendre pourquoi le code fonctionne\n\n\nToi aussi fais du marketing …\n\n  On pourrait leverager le ROI du big data avec de l’analytics predictif.\n\n\nPour une fois ….\n\n  Pour une fois c’est pas un bug ! c’est un truc qui marche.\n\n\nPooh\n\n  Au sujet d’une sombre histoire d’expression de besoin \n“Je ne peux pas toujours les aider à faire leurs besoins…”\n\n\nComprends moi !\n\n  “Comprends mon incompréhension !”\n\n\nROI !\n\n  Y a autant d’utilisateurs que de jour homme pour ce projet !\n\n\n"
} ,
  
  {
    "title"    : "Retour sur le forum PHP 2014 organisé par l&#39;AFUP",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2014/11/12/retour-sur-le-forumphp2014.html",
    "date"     : "November 12, 2014",
    "excerpt"  : "M6Web était présent en force avec 5 collaborateurs présent à l’évènement. Voici un retour des conférences qui nous ont le plus marquées.\n\nVers des applications “12 factor” avec Symfony et Docker\n\nCette session avait pour objectif de nous présenter...",
  "content"  : "M6Web était présent en force avec 5 collaborateurs présent à l’évènement. Voici un retour des conférences qui nous ont le plus marquées.\n\nVers des applications “12 factor” avec Symfony et Docker\n\nCette session avait pour objectif de nous présenter la méthodologie du “twelve-factor app”, à travers des exemples concrets pour PHP à l’aide de Symfony et Docker.\n\n“The twelve-factor app” est une suite de recommandations, indépendante d’un langage de programmation particulier et pouvant s’appliquer à toutes sortes de logiciels développés en tant que service.\n\nSans revenir sur l’ensemble de la présentation, voici un retour sur les 12 facteurs :\n\n\n  Codebase : une app = un repo (ou équivalent) servant de source à tous les déploiements (dev / preprod / recette / prod  etc.). Exemples : git, mercurial etc.\n  Dependencies : déclaration explicite et complète de l’arbre de dépendances, utilisé uniformément pour tous les environnements. Exemples : composer, npm etc.\n  Config : séparation stricte config/code (Resources, Backing services, Credentials, Hostname etc.). Exemples : parameters.yml pour Symfony 2 ou utilisation de variables d’environnement avec Docker notamment. Utilisation de fig pour l’orchestration des containers docker.\n  Backing Services : tous les services utilisés par l’application sont accessibles par le réseau. Il n’y a pas de distinction entre les ressources locales et distantes car toutes sont accessibles via URL et/ou Credentials. Exemples : MySQL, RabbitMQ, Postfix, Redis, S3 etc.\n  Build, release, run : séparation stricte entre\n    \n      “build stage” : téléchargement d’une version du code et des dépendances. Exemples : “docker build”\n      “release stage“ : utilise le “build” et le combine avec la configuration du déploiement (une version sur un environnement). Exemple : “docker push”, utilisation de capistrano…\n      “run stage” : lancement de la “release” sur l’environnement cible. Exemple “docker run” ou “fig run”\n    \n  \n  Processes : chaque composant de l’application est ‘sans état’ et ne doit pas partager directement des données. Tout doit être partagé en “backing service”.\n  Port binding : les services doivent être disponibles en mettant à disposition un port d’accès, directement accessible. Cela permet une utilisation aisée en environnement de dev mais également de réutiliser les services.\n  Concurrency : une application respectant les “12 factor” est facilement scalable, quel que soit son type (web, worker, etc.) car elle repose sur des composants systèmes pour son pilotage.\n  Disposability : robustesse par le lancement et l’arrêt rapide des services, pour rendre chacune de ses services scalables.\n  Dev/Pro parity : homogénéité des environnements dev/prod et gain de temps pour la prise en main d’un projet. (mais le développeur n’aura pas une vision précise de la configuration… boite noire ?)\n  Logs : traitement des logs en tant que flux, utilisés par des services. Exemples : ELK, StatsD/Grafana etc.\n  Admin process : Exécuter les tâches de maintenance sur les mêmes environnements/containers. Exemples : docker exec\n\n\nslides\n\nPersonnellement, j’ai trouvé cette conférence vraiment riche et instructive. Peut-être un peu plus d’exemples de configuration fig/docker aurait pu illustrer d’avantage.\n\nLa mesure ce n’est pas que pour le devops\n\nLes conférenciers ont commencé leur présentation sur un rappel de ce qu’est le Lean Startup, héritier de la méthode Lean mise au point par Toyota. Nous connaissions la démarche Lean mais pas du tout son approche spécifique au lancement d’un produit.\n\nLe concept pourrait se résumer à : la base du lean startup est de savoir écouter -ses utilisateurs- car le succès dépend d’un feedback mesurable.\n\nLe processus d’application est très simple : un cycle construit/mesure/apprend.\n\nS’en est logiquement suivi une énumération des manières de mettre en oeuvre le processus en sachant prendre en compte les mesures qui importent (AAA, AARRR), plutôt que des “mesures de vanité” (followers, nombre de visite,..).\n\nEnfin pour appliquer ces mesures, une présentation des outils a disposition a été faite.\n\n\n\nPHP dans les distributions RPM\n\nSlides\n\nCette session avait comme objectif de faire un état de PHP dans les distributions RPM RHEL/Centos/Fedora.\n\nRHEL / Centos :\n\n\n  Objectif de stabilité à 10 ans\n  Stabilité binaire et de configuration sur la durée de vie de la distribution\n  RHEL : version payante avec support (contacts avec les ingénieurs RedHat, ressources en ligne, cycles de mises à jour garantis etc.).\n  Centos : même code que RHEL (juste recompilé) mais uniquement un support communautaire (comme fedora, ubuntu, suse…).\n  RHEL 5 : PHP 5.1 / RHEL 6 : PHP 5.3 / RHEL 7 : PHP 5.4\n  Application des patchs de sécurités sur les versions anciennes de PHP pendant 10 ans.\n  Possibilité d’utiliser des repos tiers pour choisir une version plus récente spécifique (comme ceux de Remi Collet) - mais pas de support officiel.\n  Distributions plutôt destinées à des applicatifs maintenus sur le long terme.\n\n\nFedora 21+ :\n\n\n  3 sous distributions : Workstation / Server / Cloud\n  Dernière version de PHP (PHP5.5 pour f20 et PHP5.6 pour f21)\n  Intégration continue de PHP dans les cycles de Fédora. Permet d’éviter les régressions.\n\n\nA venir : Software Collections (scl) permet d’avoir TOUTES les versions de PHP souhaitées simultanément sur la même installation de Fedora. Vraiment prometteur !\n\nExemple d’utilisation des SCL en cli :\n\nscl enable php56 -f myscript56.php\nscl enable php56 bash\nscl enable php53 -f myscript53only.php\nscl enable php53 bash\n\nDans une config apache :\n\n&amp;lt;VirtualHost *:80&amp;gt;\n    ServerName php56scl\n    \n    # Redirect to FPM server in php56 SCL\n    &amp;lt;FilesMatch \\.php$&amp;gt;\n    SetHandler &quot;proxy:fcgi://127.0.0.1:9006&quot;\n    &amp;lt;/FilesMatch&amp;gt;\n&amp;lt;/VirtualHost&amp;gt;\n\nFrameworks: A History of Violence\n\n\n\nFrancois Zaninotto nous a offert un vrai show en se mettant dans la peau d’un homme politique candidat à la présidence du parti des développeurs. Avec beaucoup d’humour il a fait un retour sur l’évolution (sa propre évolution ?) du développement web et son futur hypothétique, tout en distillant (son programme) de précieux conseils pour être un meilleur développeur.\n\nSon programme :\n\n\n  Le domaine d’abord : lier son développement métier à un minimum de tierce partie (pas facile à faire !),\n  Dites non au full-stack (ça se discute !),\n  L’application plurielle : ne pas hésiter à mélanger différents langages et différents projets dialoguant via http sur une même application,\n  Repenser le temps : passons aux 32h pour nous permettre de faire de la veille.\n\n\nA la communauté PHP nous pourrions proposer une synthèse (entendu ailleurs) : “soyons plus des développeurs web que des développeurs PHP, soyons plus des développeurs que des développeurs web”.\n\n\n\nRetour d’expérience ARTE GEIE : développement d’API\n\nUne conférence donnée par un de nos confrères d’ARTE sur des problématiques très actuelles pour nous. François Dume a expliqué la stratégie de mise en place d’une API autour de JSON API et des microservices. L’utilisation de OpenResty et du langage Lua couplé à un serveur oAuth en Symfony2 gérant la validation des tokens et le throttling.\n\nIl a ensuite expliqué en détail l’implémentation de {json:api} dans Symfony2, en mettant en avant de nombreuses contributions open-source.\n\n\n\nUne conférence didactique et claire.\n\n\n\nVDM, DevOps malgré moi\n\nMaxime Valette nous expliqué comment il a (à 20 ans à peine) crée un business incroyable sur Internet et a surtout réussi à gérer une augmentation de 30 à 40K visiteurs de plus chaque jour avec pratiquement juste sa b* et son c*.\n\n- Comment on fait ? \n- Comme on peut ! \n\n\nDe vrai qualité d’orateur pour Maxime et une conf très rafraichissante. Une démonstration de lean startup par l’exemple. Même si ce choix n’a pas été discuté, PHP était un choix naturel pour lui à l’époque.\n\n\n\nAn introduction to the Laravel Framework for PHP.\n\nBien que Symfony soit très largement majoritaire en Europe, Laravel est très populaire en Amérique du Nord, c’est donc avec curiosité que nous avons assisté à cette présentation du framework faite par Dayle Rees, core developer.\n\nUne fois passé la très longue présentation des livres et autres activités du conférencier, nous avons eu droit à une présentation générale du framework qui nous as fortement rappelé Symfony1 : utilisation de singleton à outrance, MagicBox (équivalent du sfContext), beaucoup de magie pour réduire la configuration (nom des contrôleurs).\n\nAu final, l’impression laissée est mitigée : certes, le seuil d’entrée est relativement réduit, tout est simple d’apparence, mais c’était la même chose pour Symfony1, et l’expérience nous a montré que lorsque l’on essayait de sortir le framework des sentiers battus, cette simplicité devenait un vrai obstacle.\n\nAu final, Laravel est sûrement une alternative intéressante pour les nostalgiques de Symfony1, puisque le projet est actif et maintenu. Mais, pour les projets que nous développons, Symfony2 reste une solution tout à fait adaptée.\n\nLaisse pas trainer ton log !\n\nOlivier Dolbeau nous a fait un retour sur la problématique d’accès et l’interprétation des logs sur les serveurs de production.\n\n\n\nIl nous as donc présenté la solution qu’il utilise, à savoir la stack ELK, pour ElasticSearch/LogStash/Kibana, qui permet à chaque serveur d’envoyer ses logs vers un serveur central, qui a pour charge de les agréger, et de permettre leur utilisation avancée.\nFini la recherche dans des fichiers textes plats qu’il faut commencer par comprendre, désormais vos applicatifs peuvent enrichir leurs logs, les envoyer sur un système dédié à la gestion des logs disposant de vraies interfaces de recherche et de consultation.\n\nNous avons été confortés dans notre idée, puisque nous mettons également en oeuvre cette solution.\n\nTable ronde “Etat des lieux et avenir de PHP”\n\nPascal Martin a animé d’une main de maître une table ronde sur l’avenir de PHP. Avec Jordi Boggiano, lead developer de Composer, Pierre Joye, core dev de PHP, Julien Pauli, release manager de PHP 5.5 et co-RM de PHP 5.6.\n\nLa communauté se pose beaucoup de questions sur le devenir de l’engine PHP et comment va évoluer le langage. \nLes débats ont été intenses et les invités ont pu répondre à des questions posées via Twitter. Au final peu de conclusions définitives. On peut déduire que malgré les alternatives proposées par HHVM et HippyVM, la communauté reste majoritairement sur PHP et est toujours très friande d’évolutions du langage et de sa performance. Les invités de la table ronde ont exhortés les participants à contribuer au code de PHP en nous fournissant pas mal de conseils.\n\n\n\nSlideshow Karaoké\n\nUne honte ! En plus les slides n’avaient aucun sens ! :) Bravo à Mc Kenny pour l’animation.\n\n\n\n\n\nUn grand merci à l’AFUP pour ce joli évènement ! Retrouvez pas mal de ressources partagés pendant l’event sur eventifier.\n"
} ,
  
  {
    "title"    : "Github Team Reviewer pour gagner la course aux Pull Requests",
    "category" : "",
    "tags"     : " outil, github, pull-requests, cytron, open-source",
    "url"      : "/github-team-reviewer-pull-requests.html",
    "date"     : "November 7, 2014",
    "excerpt"  : "Les PR, c’est le bien\n\nChez M6Web, nous utilisons Github Enterprise en interne pour nos projets privés et Github pour nos projets open-source. Grâce à ces outils, nous avons adopté de manière systématique l’usage des Pull Requests pour faire relir...",
  "content"  : "Les PR, c’est le bien\n\nChez M6Web, nous utilisons Github Enterprise en interne pour nos projets privés et Github pour nos projets open-source. Grâce à ces outils, nous avons adopté de manière systématique l’usage des Pull Requests pour faire relire et valider notre code par nos collaborateurs. La qualité de nos développements a ainsi été grandement améliorée au fil du temps.\n\nOui, mais…\n\nNous utilisons aussi HipChat pour communiquer au quotidien et rapidemment au sein de nos équipes. Chaque création de Pull Request émet une notification sur HipChat. Cependant, le nombre de pull requests initiées augmente avec le temps et chacun tend à ignorer peu à peu les notifications ou y fait moins attention. Les Pull Requests s’accumulent sur certains projets et nous n’avions, jusqu’à présent, pas vraiment de moyen pour lister par équipe toutes les PR en cours. Cela nous permettrait d’avoir une vue globale et d’être plus réactifs et rigoureux.\n\nIl y a bien le nouveau Pull Requests Dashboard de Github avec ses filtres de recherche avancée qui permet de répertorier toutes ses PR ou celles d’une organisation. Cette mise à jour n’est pas encore entrée en application dans Github Enterprise. Mais surtout, nous avons plusieurs équipes au sein d’une même organisation et nous voulons pouvoir les gérer de manière indépendante : cette fonctionnalité ne résoud pas notre problématique.\n\nGTR !\n\nNous avons donc développé Github Team Reviewer, un outil ultra simple mais efficace qui permet en un coup d’œil de voir toutes les PR de ses équipes et leur statut, qu’elles soient sur un Github Entreprise interne ou sur Github. Le projet utilise AngularJS et l’API fournit par Github. L’installation se fait sur n’importe quel serveur web et requiert npm (via Node.js) pour builder l’application grâce à Bower et Gulp.js.\n\nL’application propose volontairement un nombre limité de paramètres de configuration éditables dans le fichier config/config.json:\n\n\n  l’intervalle de rafraichissement de la liste des PR,\n  la liste des équipes en définissant pour chacune :\n    \n      son nom,\n      les utilisateurs Github concernés,\n      les organisations Github concernées,\n      l’url de l’API à interroger (pour Github Enterprise, par défaut l’url de l’API public de Github est utilisée),\n      un token utilisateur (utile pour augmenter le rate limit de l’API public).\n    \n  \n\n\nUne select box permet de basculer d’une équipe à une autre très facilement.\n\nGithub Team Reviewer est disponible en open-source sur le compte Github de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Providers AngularJS et configuration dynamique",
    "category" : "",
    "tags"     : " configuration, angular, cytron",
    "url"      : "/surcharger-un-provider-angular",
    "date"     : "October 9, 2014",
    "excerpt"  : "Nous avons eu besoin de surcharger un provider AngularJS – AnalyticsProvider – pour le rendre configurable dynamiquement en fonction d’un paramètre de la route. Le service $route n’étant pas disponible dans la phase de configuration d’AngularJS, i...",
  "content"  : "Nous avons eu besoin de surcharger un provider AngularJS – AnalyticsProvider – pour le rendre configurable dynamiquement en fonction d’un paramètre de la route. Le service $route n’étant pas disponible dans la phase de configuration d’AngularJS, il a fallu ruser…\n\nLe but est donc de changer la méthode $get de ce provider afin de lui ajouter notre dépendance et ainsi finir notre configuration.\n\nIl existe bien une méthode decorator() dans le service d’injection de dependance d’AngularJS, mais celle-ci ne permet que de décorer des services et pas leurs providers.\n\nNous allons donc mettre les mains dans l’$injector pour récupérer et modifier à la volée le provider :\n\nangular.module(&#39;myModule&#39;)\n  .config(function ($injector) {\n    var AnalyticsProvider = $injector.get(&#39;AnalyticsProvider&#39;);\n    var $get              = AnalyticsProvider.$get;\n    // ...\n  });\n\nMaintenant que nous avons le $get, il faut le modifier pour ajouter notre dépendance. Et c’est assez simple vu qu’il utilise l’annotation sous forme de tableau :\n\n// https://github.com/revolunet/angular-google-analytics/blob/e821407fe0436677cb42eafd5b338d767990b723/src/angular-google-analytics.js#L99\nthis.$get = [&#39;$document&#39;, &#39;$rootScope&#39;, &#39;$location&#39;, &#39;$window&#39;, function($document, $rootScope, $location, $window) {\n\nNous devons modifier ce tableau en ajoutant nos dépendances et en remplaçant la fonction :\n\n// la fonction d&#39;origine est le dernier élément du tableau\nvar origFn = $get[$get.length - 1];\n// on la remplace par notre dépendance\n$get[$get.length - 1] = &#39;$route&#39;;\n// on ajoute notre nouvelle fonction à la fin du tableau\n$get[$get.length] = function () {\n    // $route est le dernier argument\n    var $route = arguments[arguments.length - 1];\n    // on fait notre traitement\n    AnalyticsProvider.setAccount($route.current.params.partner ? &#39;partner-account&#39; : &#39;own-account&#39;);\n    // et qui rappelle la fonction originale\n    return origFn.apply(AnalyticsProvider, arguments);\n};\n\nOn peut noter l’utilisation de l’objet arguments qui permet de rester générique et de garder la compatibilité en cas de changement des dépendances du module surchargé.\n\nGrâce à cette astuce, notre service Analytics est maintenant configuré dynamiquement selon nos souhaits avant son utilisation.\n"
} ,
  
  {
    "title"    : "Améliorer la webperf de son application JS avec GruntJs",
    "category" : "",
    "tags"     : " webperf, angular, grunt, performance",
    "url"      : "/2014/09/30/ameliorer-la-webperf-de-son-application-js-avec-gruntjs.html",
    "date"     : "September 30, 2014",
    "excerpt"  : "L’un des principaux problèmes que nous rencontrons sur nos développement chez M6Web est la tenue en charge.\nQuand elles sont liées à des sites à fort trafic ou à une émission télé (#effetcapital), nos applications doivent être conçues pour support...",
  "content"  : "L’un des principaux problèmes que nous rencontrons sur nos développement chez M6Web est la tenue en charge.\nQuand elles sont liées à des sites à fort trafic ou à une émission télé (#effetcapital), nos applications doivent être conçues pour supporter des pics de charge plus ou moins importants.\n\nC’est une problématique qu’on croit souvent liée uniquement aux backends (scripts serveurs, bases de données etc), en oubliant souvent que le front-end est aussi, voir tout autant concerné.\n\nC’est notamment le cas pour une “Single Page Application” Angular.Js que nous développons en ce moment.\n\nL’objectif ici est d’avoir une application qui exécutera le moins de requêtes possible pour s’afficher, et qui ensuite sera quasiment autonome en ne faisant que le minimum de requêtes HTTP. Ceci afin de garantir, que lorsque quelqu’un charge l’application, l’expérience est quasi parfaite, même si entre temps, le CDN ou l’hébergement connaît une surcharge temporaire.\n\nL’autre avantage de diminuer le nombre d’appels HTTP, c’est aussi de limiter l’impact de la latence réseau, encore plus imposante dans notre cas, car notre cible est majoritairement mobile.\n\nPour les applications “Client-Side”, nous utilisons Grunt.Js pour automatiser toutes les tâches de développement, build, déploiement … (Nul doute que la même chose existe avec Gulp pour les plus Hipsters d’entre vous). Grunt regorge de plugins en tout genre pour automatiser énormément de choses coté WebPerf, commençons par le plus évident et le plus simple.\n\nP.S : Je passe volontairement l’installation/initialisation de Grunt ainsi que de ses plugins. Le web regorgeant de ressources là dessus.\n\nMinification HTML\n\nAfin de gagner quelques octets, nous allons minifier (suppression des espaces, retours charriot, et commentaires HTML) notre code HTML généré.\nPour ceci, nous utilisons le plugin grunt-contrib-htmlmin.\n\noptions: {\n        collapseWhitespace: true,\n        collapseBooleanAttributes: true,\n        removeCommentsFromCDATA: true,\n        removeOptionalTags: true,\n        removeComments: true\n      }\n\nMinification CSS\n\nMême chose au niveau des feuilles de styles avec grunt-contrib-cssmin.\n\nCompression des images\n\nAfin d’éviter d’avoir des images « brutes » de taille trop importante, on utilise grunt-contrib-imagemin pour compresser au build nos différentes images, afin de gagner quelques ko toujours précieux.\n\nInlining des images d’interface\n\nDans notre cas, où nous souhaitons réduire le nombre de requêtes HTTP superflues, nous avons opté pour l’inlining des images dites d’interface (boutons d’actions, picto etc).\n\nNous utilisons aussi le pré-compilateur CSS Less, par simplicité et pour éviter le DRY CSS.\nNous avons donc un premier fichier .less qui va contenir toutes les images d’interface sous cette forme :\n@facelessImg: url(&#39;images/faceless.jpg’);\n\nLe plugin Grunt grunt-css-url-embed sera configuré pour remplacer les urls présentes dans ce fichier par la version data-uri (=source de l’image encodée en base64).\nIl est important de se concentrer uniquement sur les images « d’interface », car le poids des images sera ici augmenté d’environ 30% (à cause du base64).\n\nDans notre CSS principale, on pourra ensuite mettre cette image en background d’une classe CSS :\n\n.faceless {\n  background-image: @facelessImg;\n}\n\nEt dans notre code HTML, on pourra placer l’image de la manière suivante :\n&amp;lt;span class=&quot;faceless&quot;&amp;gt;&amp;lt;/span&amp;gt;\n\nGrâce à cet ajout, nous économiserons une requête HTTP pour chacune des images.\n\nVersionning des assets\n\nUne autre bonne pratique est de versionner les assets en production. Cela signifie, donner un nom unique à chaque fichier statique (JS, CSS, image), ne changeant pas, tant que le fichier en question n’aura pas subi de modification, dans le but de pouvoir mettre un cache navigateur (Expire) et un cache CDN/Proxy Cache le plus long possible (Cache-control).\nNous passerons de /images/info.jpg à /images/a21992d7.info.jpg par exemple.\n\nNous utilisons ici le plugin grunt-rev (en combinaison avec grunt-usemin), qui va d’abord versionner les assets ayant changés, et ensuite, mettre à jour les références vers les fichiers en question dans tous vos fichiers HTML, CSS, JS.\n\nConcaténation des fichiers JS\n\nDirectement dans le code HTML, toujours avec le plugin grunt-usemin, vous allez pouvoir mettre des commentaires HTML pour définir quels ensembles de fichiers devront être concaténés.\nLa bonne pratique est d’avoir un fichier app.js avec son code maison, un fichier vendor.js avec les librairies tierces, et potentiellement un fichier de config.js\nEtant donné que dans notre cas, 99% du poids Js est concentré dans “Vendor”, nous avons décidé de concaténer l’ensemble dans un seul fichier.\n\n&amp;lt;!-- build:js(.tmp) scripts/risingstar.js --&amp;gt;\n  &amp;lt;script src=&quot;bower_components/jquery/dist/jquery.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;bower_components/angular/angular.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;config.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;app.js&quot;&amp;gt;&amp;lt;/script&amp;gt; \n….\n&amp;lt;!-- endbuild —&amp;gt;\n\nInlining des templates\n\nPour finir, vous aurez peut-être remarqué, si vous développez des SPA avec Angular, ou un autre framework moderne, un changement de route (ou d’état) de votre application (ou l’affichage d’une directive) va impliquer des appels XHR pour charger les nouveaux templates à afficher. La bonne pratique ici étant de découper au maximum tous les templates dans des fichiers distincts.\nCela ne pose pas de problème en temps normal, mais dans notre cas, cela ne respecte pas nos ambitions de départ.\n\nAngular a la particularité de permettre d’utiliser la balise script pour charger des templates :\n\n&amp;lt;script type=&quot;text/ng-template&quot; id=&quot;views/info.html&quot;&amp;gt;Code HTML du template&amp;lt;/script&amp;gt;\n\nSi votre routeur ou une directive demande un template, avant de vérifier si le fichier existe, Angular vérifiera si une balise &amp;lt;script type=’text/ng-template’&amp;gt; a été déclarée avec l’identifiant correspondant au chemin demandé.\n\nGrunt via le plugin grunt-angular-inline-templates, nous permet d’automatiser cette tâche au build, afin de regrouper dans le index.html du build, tous les templates dans un script avec l’id correspondant au chemin du fichier html original. De cette manière, nous n’avons plus aucun appel HTTP à faire pendant toute l’utilisation de l’application.\nAttention toutefois, cela signifie que le poids du fichier HTML original va forcément augmenter.\n\nConclusion\n\nComme vous avez pu le voir, nous avons grandement optimisé notre application, en utilisant simplement des plugins Grunt à notre disposition. Nous travaillons donc sur un espace de développement respectant toutes les bonnes pratiques (découpages des fichiers JS, CSS, HTML au maximum, code commenté …) et toutes les opérations d’optimisation sont automatiquement effectuées au build, fait avant chaque déploiement.\n\nAttention, cela signifie aussi que votre projet en production devient relativement différent de celui que vous testé en développement. Il devient donc important de mettre en place des tests fonctionnels sur le build de production (avec Protractor par exemple, ou même Behat), et de tester régulièrement la bonne génération et le bon fonctionnement du build de prod.\n"
} ,
  
  {
    "title"    : "Tests E2E sur son application AngularJS avec Protractor",
    "category" : "",
    "tags"     : " qualite, tests, javascript, angular, protractor, cytron",
    "url"      : "/tests-e2e-application-angularjs-protractor.html",
    "date"     : "September 24, 2014",
    "excerpt"  : "Familier des tests fonctionnels avec Behat et Atoum pour des applications majoritairement PHP, nous l’étions beaucoup moins avec les tests end-to-end pour des applications pures Javascript, qui plus est, sous AngularJS. Les tests end-to-end ou tes...",
  "content"  : "Familier des tests fonctionnels avec Behat et Atoum pour des applications majoritairement PHP, nous l’étions beaucoup moins avec les tests end-to-end pour des applications pures Javascript, qui plus est, sous AngularJS. Les tests end-to-end ou tests e2e ne sont autres que des tests fonctionnels dans la domaine du Javascript. L’objectif de cet article est de montrer le cheminement que nous avons emprunté pour mettre en place ces tests sur une de nos applications et pour gérer les difficultés qui en ont découlé.\n\nLe contexte\n\nIl s’agit d’une application web présentant des écrans différents à l’utilisateur en fonction des données contenues dans un fichier distant requêté à intervalle régulier court (quelques secondes). L’utilisateur est invité ou non à agir avec les vues, principalement en appuyant sur des boutons, qui changent l’état interne de l’application et peut, a posteriori, influer sur les écrans suivants.\n\nMettre en place Protractor\n\nLa première étape consiste à installer Protractor, framework de tests e2e dédié à AngularJS et utilisant Node.js. Si vous utilisez Grunt pour gérer les tâches de build de votre projet, il suffit d’exécuter la commande :\n\nnpm install grunt-protractor-runner --save-dev\n\nPuis on crée le fichier de configuration dans le projet :\n\n/* protractor-local.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;\n};\n\nTous les tests e2e de notre application sont écrits dans des fichiers javascript dont le nom est suffixé par .e2e.js. Nous avons en effet fait le choix d’une architecture modulaire se retrouvant dans l’organisation des dossiers de notre projet : les fichiers de tests e2e se trouvent dans les mêmes répertoires que les controllers auxquels ils sont rattachés 1.\n\nUn navigateur pour mes tests\n\nPour exécuter ses tests dans les conditions réelles de son application, il faut un navigateur. Nous développons sur un serveur distant en SSH. Le seul navigateur utilisable est donc un browser headless, le plus connu et utilisé étant PhantomJS. Cependant, combiné à Protractor, ce dernier est particulièrement instable pour le moment et il n’est pas recommandé de l’utiliser. Nous optons donc pour Chrome (via le plugin chromedriver). Nécessitant une interface graphique, nous ne pourrons donc pas lancer nos tests sur le serveur de développement mais nous devrons le faire en local sur nos machines.\n\n/* protractor-local.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;,\n  maxSessions: 1,\n  multiCapabilities: [\n    { browserName: &#39;chrome&#39; }\n  ]\n};\n\nOn installe les binaires nécessaires au lancement de Chrome via Protractor :\n\n./node_modules/grunt-protractor-runner/node_modules/.bin/webdriver-manager update\n\nPuis on ajoute les tâches Grunt :\n\n/* Gruntfile.js */\ngrunt.initConfig({\n  connect: {\n    dist: {\n      options: {\n        port: 9000,\n        hostname: &#39;localhost&#39;,\n        base: &#39;dist&#39;\n      }\n    }\n  },\n  protractor: {\n    local: {\n      options: {\n        configFile: &quot;protractor-local.conf.js&quot;\n      }\n    }\n  }\n});\n\ngrunt.registerTask(&#39;test&#39;, [\n  &#39;build&#39;,\n  &#39;connect:dist&#39;,\n  &#39;protractor:local&#39;\n]);\n\nIntégration continue\n\nL’ensemble de nos projets joue automatiquement leurs tests sur un serveur Jenkins commun qui ne dispose pas de navigateurs graphiques. Nous aurions pu mettre en place au sein de notre infrastructure un serveur Selenium pour répondre à cette problèmatique. Mais les contraintes du projet ne nous autorisaient pas à y consacrer le temps nécessaire. Nous avons donc opté pour une solution tiers plus rapide à mettre en œuvre : SauceLabs, plateforme de tests hébergée dans le “cloud”.\n\nUne fois enregistré sur le site, on crée un nouveau fichier de configuration Protractor :\n\n/* protractor-saucelabs.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;,\n  allScriptsTimeout: 30000,\n  jasmineNodeOpts: {\n    defaultTimeoutInterval: 60000\n  },\n  maxSessions: 1,\n  sauceUser: &#39;mySauceUser&#39;,\n  sauceKey: &#39;mySauceKey&#39;,\n  multiCapabilities: [\n    {\n      browserName: &#39;chrome&#39;,\n      platform: &#39;Linux&#39;\n    },\n    {\n      browserName: &#39;firefox&#39;,\n      platform: &#39;Linux&#39;\n    },\n    {\n      browserName: &#39;safari&#39;,\n      platform: &#39;OS X 10.9&#39;\n    },\n    {\n      browserName: &#39;chrome&#39;,\n      platform: &#39;Windows 8.1&#39;\n    }\n  ]\n};\n\nNotons que l’on peut lancer ses tests sur autant de couples OS/navigateurs que l’on souhaite en remplissant le tableau multiCapabilities. Le fichier de configuartion Grunt doit être adapté pour lancer SauceConnect, l’interface entre SauceLabs et l’application, avant le démarrage des tests :\n\n/* Gruntfile.js */\ngrunt.initConfig({\n  connect: {\n    dist: {\n      options: {\n        port: 9000,\n        hostname: &#39;localhost&#39;,\n        base: &#39;dist&#39;\n      }\n    }\n  },\n  protractor: {\n    local: {\n      options: {\n        configFile: &#39;protractor-local.conf.js&#39;\n      }\n    },\n    saucelabs: {\n      options: {\n        configFile: &#39;protractor-saucelabs.conf.js&#39;\n      }\n    }\n  },\n  run: {\n    installsc: {\n      options: {\n        wait: true\n      },\n      cmd: &#39;bash&#39;,\n      args: [\n        &#39;-c&#39;,\n        &#39;test -d sc-4.2-linux || (wget https://saucelabs.com/downloads/sc-4.2-linux.tar.gz &amp;amp;&amp;amp; tar xvf sc-4.2-linux.tar.gz)&#39;\n      ]\n    },\n    sauceconnect: {\n      options: {\n        wait: false,\n        quiet: true,\n        ready: /Sauce Connect is up/\n      },\n      cmd: &#39;./sc-4.2-linux/bin/sc&#39;,\n      args: [\n        &#39;-u&#39;,\n        &#39;mySauceUser&#39;,\n        &#39;-k&#39;,\n        &#39;mySauceKey&#39;\n      ]\n    }\n  }\n});\n  \n\ngrunt.registerTask(&#39;test-e2e&#39;, function (target) {\n  var tasks = [\n    &#39;build&#39;,\n    &#39;connect:dist&#39;\n  ];\n\n  if (target === &#39;local&#39;) {\n    tasks.push(&#39;protractor:local&#39;);\n  } else {\n    tasks.push(&#39;run:installsc&#39;);\n    tasks.push(&#39;run:sauceconnect&#39;);\n    tasks.push(&#39;protractor:saucelabs&#39;);\n    tasks.push(&#39;stop:sauceconnect&#39;);\n  }\n\n  grunt.task.run(tasks);\n});\n\nAvec cette configuration, nous lançons les tests en local sur notre machine avec la commande grunt test-e2e:local ou à distance sur SauceLabs avec grunt test-e2e.\n\nNotre premier test\n\nLe premier test que nous avons écrit pour valider l’architecture est plutôt basique :\n\ndescribe(&#39;Controller: MainCtrl&#39;, function () {\n  it(&#39;should work&#39;, function () {\n    browser.get(browser.baseUrl);\n    expect(true).toBe(true);\n  })\n});\n\nOn remarque que l’écriture d’un test e2e utilise, comme les tests unitaires, la syntaxe du framework Jasmine : un bloc describe regroupe une suite de tests définis dans des blocs it. Les variables de configuration définies dans les fichiers de configuration Protractor sont utilisables via la variable globale browser, variable qui nous permettra d’entretenir le lien entre nos tests et le code exécuté dans le navigateur. Pour mieux appréhender les étapes du processus et les erreurs qui se produisent, il est en effet très important de bien comprendre la séparation entre le code Javascript exécuté dans Node.js via Protractor, qui correspond au déroulement des tests, et le code Javascript de notre application qui lui est exécuté dans le browser et avec lequel on ne peut interagir depuis les tests que par certaines fonctions du framework (element, executeScript, addMockModule, etc.)2. Ce sont deux univers d’exécution bien distincts.\n\nDébugger avec Protractor\n\nLorsque vous lancerez les tests en local, vous remarquerez que Chrome est réellement exécuté mais vous ne verrez pas grand chose car l’affichage est bien trop rapide. Il est possible de mettre des points d’arrêt dans ses tests pour y voir plus clair et pour, par exemple, consulter la console Javascript du navigateur. Pour cela, il faut utiliser la fonction browser.debugger() comme point d’arrêt et ajouter l’option debug dans la configuration Grunt :\n\n/* Gruntfile.js */\nprotractor: {\n  local: {\n    options: {\n      configFile: &#39;protractor-local.conf.js&#39;,\n      debug: true\n    }\n  }\n}\n\nPour passer d’un point d’arrêt à l’autre, on saisit c comme continue. Notez que cela ne fonctionnera pas si vous avez plus d’un navigateur dans le tableau multiCapabilities de votre configuration.\n\nOn peut également ajouter l’option --debug à la commande grunt test-e2e:local pour afficher l’ensemble des requêtes lancées par l’application.\n\nMocker sa config\n\nComme souvent dans les projets AngularJS, nous utilisons un module pour définir nos variables de configuration :\n\nangular.module(&quot;config&quot;, [])\n  .constant(&quot;config&quot;, {\n    &#39;ma_variable&#39;: &#39;une_valeur&#39;\n  });\n\nDans les tests e2e, on veut tout tester, en particulier les comportements qui diffèrent en fonction des valeurs de configuration. Comment faire puisque ce module est chargé une fois pour toute au lancement de l’application ? Protractor introduit la fonction addMockModule qui permet de bouchonner à la volée un module Angular.\n\nit(&#39;comportement avec une autre valeur&#39;, function () {\n  browser.addMockModule(&#39;config&#39;, function () {\n  \tangular.module(&#39;config&#39;, []).constant(&#39;config&#39;, {\n    \t\t&#39;ma_variable&#39;: &#39;une_autre_valeur&#39;\n  \t});\n  });\n\n  // mon test\n  \n  browser.removeMockModule(&#39;config&#39;);\n});\n\nMocker le service $http\n\nDans notre application, un fichier externe est requêté régulièrement via le service Angular $http. AngularJS fournit déjà un mock complet de ce service nommé $httpBackend. Pour y avoir accès, il faut ajouter la dépendance angular-mocks en devDependencies dans son fichier bower.json et inclure le fichier bower_components/angular-mocks/angular-mocks.js dans l’application en développement. $httpBackend permet de définir quels appels HTTP doivent être interceptés et quelles réponses doivent être renvoyées.\n\nLa difficulté dans notre cas réside dans le fait de pouvoir simuler le changement d’état du fichier distant dans un même test pour pouvoir vérifier les changements de vue qui en découlent. Il est possible de le faire directement via $httpBackend moyennant quelques acrobaties, mais la librairie HttpBackend simplifie grandement son utilisation pour ce type de tests 3.\n\nvar HttpBackend = require(&#39;httpbackend&#39;);  \nvar backend;\n\ndescribe(&#39;Test workflow&#39;, function() {  \n  beforeEach(function() {\n    backend = new HttpBackend(browser);\n  });\n\n  afterEach(function() {\n    backend.clear();\n  });\n\n  it(&#39;should display result when status is changed to RESULT&#39;, function(done) {\n    backend.whenJSONP(/status.json/).respond({status: &#39;initial&#39;});\n\n    browser.get(&#39;/&#39;);\n\n    var result = element(by.binding(&#39;result&#39;));\n    expect(result.getText()).toEqual(&#39;no result&#39;);\n    \n    backend.whenJSONP(/status.json/).respond({status: &#39;result&#39;, percentage: 70});\n    \n    browser.wait(function () {\n      return browser.getLocationAbsUrl().then(function (currentUrl) {\n        return currentUrl === &#39;https://localhost:9000/#/result&#39;;\n      });\n    }, 5000).then(function () {\n      expect(result.getText()).toEqual(&#39;70 %&#39;);\n      done();\n    });\n  });\n});\n\nOui, mais…\nProtractor nous a été indispensable pour implémenter les tests fonctionnels sur notre application car son intégration avec AngularJS offre des possibilités que les autres frameworks de tests fonctionnels n’ont pas. On pense principalement à la synchronisation qui est mise en œuvre entre les tests et l’initialisation d’Angular dans la page (“wait for angular”). Cependant, avec le recul que l’on peut avoir sur notre projet :\n\n\n  il faut l’avouer, Protractor n’est pas aussi simple à mettre en place que Behat par exemple,\n  le debuggage est assez pénible car les messages d’erreur sont souvent peu verbeux et, c’est l’inconvénénient de tester du javascript avec du javascript, on ne sait pas toujours où se situe l’erreur (dans les tests ou dans le code applicatif ?),\n  Protractor est parfois instable avec les webdrivers utilisés, ce qui nous oblige à relancer les tests manuellement,\n  nos tests dans SauceLabs sont (très) lents, ce qui nous a contraint à la longue à réduire le nombre de navigateurs testés (améliorant par la même occasion la stabilité des tests).\n\n\n\n  \n    \n      Scalable code organization in AngularJS &amp;#8617;\n    \n    \n      Protractor API &amp;#8617;\n    \n    \n      Angular e2e tests, Mock your backend. &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Contrôlez facilement votre cohérence de code sur votre projet Symfony2 avec coke",
    "category" : "",
    "tags"     : " code sniffing, coke, Symfony2",
    "url"      : "/2014/08/05/verifier-la-coherence-du-code-d-un-projet-symfony2-avec-coke.html",
    "date"     : "August 5, 2014",
    "excerpt"  : "Pour qu’un projet persiste dans le temps, il est important que le style de codage soit le même. Et quand vous vous reposez sur des outils, autant faire en sorte que le style de codage retenu soit proche, si ce n’est le même, que les briques que vo...",
  "content"  : "Pour qu’un projet persiste dans le temps, il est important que le style de codage soit le même. Et quand vous vous reposez sur des outils, autant faire en sorte que le style de codage retenu soit proche, si ce n’est le même, que les briques que vous utilisez. Et dans le cas où vous utilisez un framework, c’est d’autant plus important.\n\nAvec Symfony2, c’est d’autant plus facile que l’architecture des bundles est très marquée, et qu’un coding guide est publié.\n\nÇa, c’est pour la théorie, mais en pratique, si ce n’est pas super simple, automatique, une somme de toutes petites erreurs apparaissent et le sentiment d’abandon s’installe rapidement.\n\nCoke\n\nIl y a un peu plus d’un an, chez M6Web, nous avons développé coke pour configurer simplement l’exécution de PHP_CodeSniffer.\n\nDepuis quelques mois, il est possible d’installer coke via Composer :\n\n{\n  &quot;require&quot;: {\n    &quot;m6web/coke&quot;: &quot;~1.2&quot;\n  }\n}\n\nL’avantage de passer par Composer, c’est que coke va lui-même installer PHP_CodeSniffer en tant que dépendance Composer (dans le dossier vendor), permettant de ne pas avoir à suivre la fastidieuse procédure d’installation via PEAR.\n\nInstaller un coding standard via Composer\n\nLorsque nous voulons utiliser un coding standard qui n’est pas inclus par défaut avec PHP_CodeSniffer, il est possible de l’installer en utilisant Composer\n\nSymfony2-coding-standard\n\nChez M6Web, nous maintenons le standard Symfony2-coding-standard qui permet de valider que le code d’un projet respecte les coding standard de Symfony2.\n\nPour rendre à César ce qui appartient à César, nous avons récupéré la base du standard telle que créé par opensky.\n\nSi nous avons décidé de le forker, c’est que la structure ne correspondait pas à ce qui est nécessaire pour une installation de ce standard via Composer\n\nProcédure complète, pas-à-pas\n\nCréer le fichier composer.json suivant :\n\n{\n  &quot;require-dev&quot;: {\n    &quot;m6web/coke&quot;                       : &quot;~1.2&quot;,\n    &quot;m6web/symfony2-coding-standard&quot;   : &quot;~1.1&quot;,\n  }\n}\n\nInstaller les dépendances Composer :\n\ncomposer install\n\nCréer le fichier .coke suivant :\n\n# Standard used by PHP CodeSniffer (required)\nstandard=vendor/m6web/symfony2-coding-standard/Symfony2\n\nIl est désormais possible d’appeler la commande suivante pour valider le style de codage de votre projet\n\n./vendor/bin/coke\n\nConclusion\n\nAvec cette technique, il est très simple de valider le style de codage d’un projet. Du coup, plus d’excuse pour ne pas le faire ;)\n\nBonus\n\nL’idéal, pour ne jamais commiter un code ne respectant pas les conventions de codage, est d’utiliser les hooks de commit pour que cette vérification soit faite automatiquement.\n\nLa manière la plus simple de le faire est d’ajouter la ligne ./vendor/bin/coke dans le fichier .git/hooks/pre-commit, mais cette méthode a le défaut de vérifier tout le projet, et pas uniquement le code modifié et à commiter.\n\nPour aller plus loin, vous pouvez vous inspirer du script suivant qui ne lance coke que sur les fichiers dans le “staging” de Git (les fichiers à commiter).\n\n"
} ,
  
  {
    "title"    : "M6Web était présent au PHPTour Lyon 2014",
    "category" : "",
    "tags"     : " afup, phptour, conference, video",
    "url"      : "/2014/06/25/m6web-etait-au-phptour-lyon-2014.html",
    "date"     : "June 25, 2014",
    "excerpt"  : "Le Lundi 23 et Mardi 24 juin a eu lieu l’événement PHP de l’année à Lyon : le PHPTour Lyon.\nÀ cette occasion, les équipes d’M6Web ont présenté un talk, dont voici les slides et vidéos :\n\n#Nouveau socle pour une nouvelle vie, chez M6Web (par Kenny ...",
  "content"  : "Le Lundi 23 et Mardi 24 juin a eu lieu l’événement PHP de l’année à Lyon : le PHPTour Lyon.\nÀ cette occasion, les équipes d’M6Web ont présenté un talk, dont voici les slides et vidéos :\n\n#Nouveau socle pour une nouvelle vie, chez M6Web (par Kenny Dits)\n\nLa seconde conférence de @techM6Web a été tenue par Kenny Dits (@kenny_dee) : “Nouveau socle pour une nouvelle vie, chez M6Web”.\n\n\n\n\n\nVoir les commentaires sur Joind.in\n\nNous avons aussi retrouvé une bonne partie des développeurs de l’équipe (anciens ou actuels) qui ont joué le jeu de la borne photo Pixiway mise à disposition :\n\n\n\n#Conclusion\n\nL’Afup a encore réalisé un boulot considérable cette année, pour accoucher sans aucun doute, du meilleur PHP Tour jamais fait.\nBravo à toute la team pour l’organisation sans faille, et aux autres speakers pour la qualité de leurs talks.\n"
} ,
  
  {
    "title"    : "M6Web sera présent au PHPTour Lyon 2014",
    "category" : "",
    "tags"     : " afup, phptour",
    "url"      : "/2014/05/15/m6web-sera-present-au-phptour-lyon-2014.html",
    "date"     : "May 15, 2014",
    "excerpt"  : "M6Web sera bien représenté au PHPTour 2014 organisé par l’AFUP et est très heureux de soutenir l’évènement en étant sponsor Argent.\n\n\n\nVenez nombreux augmenter votre pilosité faciale, tel un vrai sysadmin.\n\nFaites le plein d’anecdotes croustillant...",
  "content"  : "M6Web sera bien représenté au PHPTour 2014 organisé par l’AFUP et est très heureux de soutenir l’évènement en étant sponsor Argent.\n\n\n\nVenez nombreux augmenter votre pilosité faciale, tel un vrai sysadmin.\n\nFaites le plein d’anecdotes croustillantes et découvrez l’histoire de M6Web Lyon avec Kenny Dits.\n"
} ,
  
  {
    "title"    : "Babitch, the story behind our table soccer web application",
    "category" : "",
    "tags"     : " opensource, babyfoot, angularjs, d3js, symfony",
    "url"      : "/2014/04/23/babitch-the-story-behind-our-table-soccer-web-application.html",
    "date"     : "April 23, 2014",
    "excerpt"  : "At M6Web, we love playing foosball!\nWe have one old (incredibly strong) soccer table in our « fun room », and at lunch time, a part of us enjoy playing it.\n\nThe soccer table in enterprise is awesome for a lot of things:\n\n\n  Team building between e...",
  "content"  : "At M6Web, we love playing foosball!\nWe have one old (incredibly strong) soccer table in our « fun room », and at lunch time, a part of us enjoy playing it.\n\nThe soccer table in enterprise is awesome for a lot of things:\n\n\n  Team building between each players,\n  Don’t think about work (almost) when we are playing,\n  Fun! a lot of!\n  Attract good people …\n\n\nOur rules are simple : Doubles (4 players) only, and the first team at ten win.\n\nSo, as programmers, we tend to have software ideas for each questions in our life ! and the questions we had with foosball were:\n\n\n  Who is the best player?\n  How many goals did I score ?\n  Who won the most games?\n  …\n\n\nSo, we begin to talk several months ago about a foosball app, allowing us to record each games, and each goals, to compute lot of stats about our games.\nEveryone had good ideas about it, but someone had many more than us. Even more that it ruined all motivation of the other folks wanting to do work on this webapp, because the first steps to begin the app with all the features we had in mind was too big for all of us …\n\nSo before having started the project, it was over !\n\nFew months later, an undercover part of the team began the development of a more simple and stupid foosball application : it just allowed to select 4 players, and to register matches by telling who scored and what kind of goal it was (normal or own goal).\nThis was ugly as possible, but it worked ! And it was an awesome start for improving it and giving back all the motivation developpers had lost before !\n\nBabitch was born :)\n\nArchitecture\n\nAt the beggining, there was only one project, with the server API, and the client part.\nThis was bad. It was a good way to start fast, but a bad way to allow each project to evolve on its own side.\nSo we decided to divide the Babitch Project into two parts, Babitch, the server API, and BabitchClient, a client to consume Babitch Api data.\n\nBabitch, the API\n\nThe Babitch API is a simple PHP/MySQL project, based on Symfony2, Doctrine, FosRestBundle, and NelmioApiDocBundle.\nThe documentation generated by the NelmioApiDocBundle is available at /api/doc and allows to view each Route of the API, and to send requests on them with the Sandbox menu.\nThe API is functionnaly tested by Behat.\nA Vagrant file is available if you want to try it easily. (More information in the Readme.md)\n\nBabitch, the Client\n\nThe Babitch Client, is the “official” client for the API.\nThe client side doesn’t require any webserver, it’s just an Angular.js app, doing REST queries to the Babitch API thanks to Restangular.\n\nWe used Yeoman to bootstrap the project because it helps in many ways:\n\n\n  adds grunt configuration and support for serving, building and testing the project,\n  have generators for controllers, service, etc …\n\n\nFor development on this client, we are heavily using Grunt, Karma for Unit Testing, and the new Protactor for E2E testing.\n\nThe client is divided into four major parts:\n\nNew Game\n\n\n\nThis is the main feature, it allows to begin a new game, choose 4 players, and assign each goals to the right players.\nThe game is saved only when the last goal is made.\nEach player is represented by his Gravatar for a nicer UI :)\n\nLive\n\n\n\nThe table soccer is not at the same floor than we are, so we are using our monitoring screen to show at lunch time the live state of the table score !\n\nWith a screen on this feature, we could see:\n\n\n  if a game is played right now,\n  who’s playing,\n  the live score,\n  for each goal, in live, who scored, and on which side :)\n\n\nThe live part use a Faye server, which you can host freely on Heroku (more information on Readme.md). You configure a channel name, and all actions done on the new game view are forwarded to the Faye server, forwarded back to the client listening on the Live view. It just rocks !\n\nStats\n\n\n\nAll of this would be useless if you don’t have any way to compare your … stats to others competitors, right ?\nSo stats section is here for that purpose.\nIt shows you :\n\n\n  the last played match,\n  a sortable table by each stat of each player,\n  data visualization on each type of stats,\n  an individual card by player,\n  a sortable table by each stat of each team.\n\n\nAnd for each player and team, you have access to a lot of stats:\n\n\n  Elo Ranking (According to the Bonzini Usa Player Ranking/Rating System),\n  Percentage of goals per ball played,\n  Percentage of victory/loose per game,\n  Number of games played,\n  Team Goalaverage,\n  …\n\n\nThere was some long debate about how stats have to be computed : on the server side ? (not really the goal of a REST Api …), or on the client side ?\nAfter some successfull tries, we decided to compute stats on the client side, in an Angular.Js Service.\nThe service loads the last 300 games, and computes team and player stats fastly.\nWe also use the awesome D3.Js framework for data visualization.\n\nAdmin\n\nProbably the first screen you will need, for a simple way to add, modify, or delete players.\n\nConclusion\n\nWorking on our free (or lunch) time on a side-project like this is awesome!\nIt allows us to use several technologies or tools we don’t use often, to improve our knowledge on tons of other things, to view the project on the product owner side and to mix teams who don’t work a lot together.\n\nOne other thing interesting to remember about that project: Keep things as simple and small as possible (according to KISS principles) ! And only when your simple project is done, iterate by adding more and more features.\n\nTry and contribute?\n\nSo, if like us, you love foosball and play at work, give it a try, and give us feedback if you use it :)\n\nAlso, it’s open-source, so you’re welcome to contribute on BabitchClient and BabitchService, by posting/reading/commenting issue and PR.\n\nThanks ! :)\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un Lead Developpeur / Architecte web (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/2014/04/11/m6web-lyon-recherche-un-lead-developpeur-architecte-web-h-f-en-cdi.html",
    "date"     : "April 11, 2014",
    "excerpt"  : "\n\nM6Web Lyon recrute, en CDI, un Lead Développeur LAMP, avec une très forte expertise sur les technologies PHP 5.5, MySQL, Symfony2, GIT, et capable d’encadrer une petite équipe de développement.\n\nNous recherchons quelqu’un de très passionné, enth...",
  "content"  : "\n\nM6Web Lyon recrute, en CDI, un Lead Développeur LAMP, avec une très forte expertise sur les technologies PHP 5.5, MySQL, Symfony2, GIT, et capable d’encadrer une petite équipe de développement.\n\nNous recherchons quelqu’un de très passionné, enthousiaste, et mordu de veille technologique : un missionnaire de l’open source, un intégriste de la qualité de code, des tests unitaires et fonctionnels, et un architecte de projets aguerri avec une première approche en méthodologie de développement agile, et une expérience de management de développeurs.\n\nSi, en plus, vous êtes un malade de l’optimisation back-end et front-end, que des technologies comme Node.js vous émoustillent, que, malgré la qualité de MySQL, vous envisagez dans certains cas des solutions NoSQL alternatives (Mongo, Redis…), votre profil nous intéresse !\n\nVenez apporter vos compétences aux équipes techniques de M6Web en travaillant sur des sites à très forte charge (6Play, m6.fr, clubic.com, jeuxvideo.fr …), et partagez-les grâce à des conférences internes ou externes et des articles sur notre blog.\n\nSi vous avez les qualités requises et l’envie de nous rejoindre, allez sur le lien ci-dessous et faites nous part de votre CV, de votre compte github, et d’une lettre attrayante pour nous motiver à vous rencontrer.\n\nSi vous souhaitez postuler ou avoir plus d’infos : https://www.groupem6.fr/ressources-humaines/offres-emploi/lead-developpeur-architecte-web-h-f-229879.html\n"
} ,
  
  {
    "title"    : "Conférence au Symfony Live 2014 : Symfony à la télé",
    "category" : "",
    "tags"     : " symfony, conference",
    "url"      : "/2014/04/10/SfLive2014-symfony-a-la-tele.html",
    "date"     : "April 10, 2014",
    "excerpt"  : "Invité par SensioLabs au Symfony Live 2014, j’ai pu présenter le travail des équipes de M6Web, et de nos partenaires, autour de Symfony 2.\n\nVoici les slides de la conférence :\n\n \n\nL’enregistrement audio (avec les slides) est disponible ici :\n\n\n\nJe...",
  "content"  : "Invité par SensioLabs au Symfony Live 2014, j’ai pu présenter le travail des équipes de M6Web, et de nos partenaires, autour de Symfony 2.\n\nVoici les slides de la conférence :\n\n \n\nL’enregistrement audio (avec les slides) est disponible ici :\n\n\n\nJe tiens à remercier toutes les personnes avec qui j’ai pu échanger autour des thématiques de la conférence. J’ai eu beaucoup de plaisir à m’apercevoir que de nombreux conférenciers citaient le travail de M6Web pendant leur talk !\n\nRendez vous au phptour à Lyon pour les prochaines conférences techniques M6Web.\n"
} ,
  
  {
    "title"    : "Utilisation du StatsdBundle avec le composant Console",
    "category" : "",
    "tags"     : " statsd, php, symfony, console, monitoring, cytron",
    "url"      : "/2014/03/04/utilisation-du-statsdbundle-avec-la-console.html",
    "date"     : "March 4, 2014",
    "excerpt"  : "Le StatsdBundle\n\nChez M6Web, nous utilisons StatsD et nous avons créé un bundle pour cela.\nCe bundle permet d’ajouter facilement des incréments et des timings dans StatsD sur des événements Symfony2.\n\nDe la Request à la console\n\nOr pour des raison...",
  "content"  : "Le StatsdBundle\n\nChez M6Web, nous utilisons StatsD et nous avons créé un bundle pour cela.\nCe bundle permet d’ajouter facilement des incréments et des timings dans StatsD sur des événements Symfony2.\n\nDe la Request à la console\n\nOr pour des raisons de performances, lors des événements Symfony, les incréments et timings sont seulement stockés dans une variable et ne sont envoyés réellement à StatsD que pendant le kernel.terminate qui se déroule après l’envoi de la réponse HTTP au client.\nCeci pose un problème pour les événements lancés depuis une commande Symfony puisque en console, il n’y pas de Request et donc pas de kernel.terminate.\nNous avons envisagé d’utiliser l’événement console.terminate pour palier à cela, mais cela pose deux problèmes :\n\n\n  pour une commande qui est censée tourner indéfiniment (par exemple un consumer), on ne veut pas attendre la fin de la commande pour envoyer les données,\n  dans le cas d’une exception pendant la commande, l’événement console.terminate est lancé avant console.exception.\n\n\nLa première solution était donc d’appeler manuellement $container-&amp;gt;get(&#39;m6_statsd&#39;)-&amp;gt;send() dans la commande ou dans un ConsoleExceptionListener mais cela nous fait perdre le principal intérêt du StatsdBundle à savoir le découplage entre la commande et le client StatsD.\n\nLa seconde solution a donc été de modifier le StatsdBundle et d’ajouter une configuration au niveau de l’événement pour forcer l’envoi instantané des données.\n\nAinsi, avec la configuration suivante :\n\nclients:\n    event:\n        console.exception:\n            increment:      mysite.command.&amp;lt;command.name&amp;gt;.exception\n            immediate_send: true\n        m6kernel.exception:\n            increment: mysite.errors.&amp;lt;status_code&amp;gt;\n\nL’incrément mysite.command.&amp;lt;command.name&amp;gt;.exception sera envoyé en temps réel, alors que les autres comme mysite.errors.&amp;lt;status_code&amp;gt; continueront à être envoyés pendant kernel.terminate.\n"
} ,
  
  {
    "title"    : "Refonte de notre système de vote",
    "category" : "",
    "tags"     : " api, symfony, redis, monitoring, qualite, cytron",
    "url"      : "/2014/02/18/refonte-de-notre-systeme-de-vote.html",
    "date"     : "February 18, 2014",
    "excerpt"  : "Notre système de vote est utilisé d’une part pour gérer l’ensemble des questions et des réponses associées utilisées dans nos quizz et d’autre part pour récolter le nombre de votes des internautes lors des jeux concours.\n\nActuellement, le trafic g...",
  "content"  : "Notre système de vote est utilisé d’une part pour gérer l’ensemble des questions et des réponses associées utilisées dans nos quizz et d’autre part pour récolter le nombre de votes des internautes lors des jeux concours.\n\nActuellement, le trafic généré par cette fonctionnalité varie entre quelques votes par minute la nuit à quelques dizaines de votes par seconde lors des premières parties de soirée.\n\nHistorique\n\nComme souvent, les besoins ont régulièrement évolué depuis la mise en place initiale du système en 2009, faisant parfois prendre des chemins tortueux à l’implémentation technique. Au fil des demandes, notre système a par exemple dû stocker ses données dans nos forums pour une fonctionnalité qui a ensuite été rapidement abandonnée.\n\nL’année 2012 a vu l’arrivée du second écran : à l’aide de l’application gratuite adéquate, les périphériques mobiles peuvent désormais se synchroniser avec l’émission en cours de visionnage, en direct ou en différé, sur la TV ou sur le web (la synchronisation se fait par la bande son). Cette synchronisation nous permet de pusher instantanément sur les périphériques mobiles du contenu adapté à ce que le téléspectateur regarde : le détail de la recette que le cuisinier prépare dans Top Chef ou un sondage concernant la dernière trouvaille linguistique d’un ch’tit face à sa ch’tite.\n\nLe second écran s’annonçait alors comme une source importante de trafic supplémentaire pour notre système de vote. Effectivement, en plus du trafic historique généré par les sites web, nous allions aussi recevoir tous les votes provenant des périphériques mobiles.\n\nCe nouveau trafic a une saisonnalité très marquée : il est principalement présent en début de soirée et reste très dépendant du programme diffusé et de la contribution apportée.\n\nProblématique\n\nLa principale problématique venait de l’architecture des bases de données MySQL. Étant fortement couplées sur l’ensemble de la plateforme, la moindre défaillance de l’une d’elles, due à une surcharge sur un sondage, risquait de pénaliser les internautes de tous nos autres sites (un sondage du second écran pouvait donc impacter l’expérience utilisateur de Clubic).\n\nLe code était aussi fortement couplé entre nos différentes applications : l’action PHP d’un vote était exécutée sur la même plateforme que notre BO permettant à tous nos web services de fonctionner ainsi qu’aux contributeurs d’ajouter du contenu. Une surchage sur les votes aurait donc pu entrainer des perturbations sur le fonctionnement global du site m6.fr et de ses web services, donc de beaucoup de produits par extension.\n\nPour résumer, l’imbrication du code et des bases de données dans l’usine logiciel ne permettait pas de calibrer le système de vote pour qu’il puisse recevoir la charge attendue par le second écran.\n\nC’est donc début 2013 que Kenny Dits m’a contacté pour que nous trouvions une solution permettant de découpler le système de vote tout en faisant évoluer son architecture interne afin qu’il puisse facilement s’adapter à la charge inconstante du second écran.\n\nSolution\n\nNous avons alors conçu un nouveau service dédié uniquement à la gestion des questions, réponses et votes des utilisateurs. Ce nouveau service Polls est autonome, ce qui nous permet de le découpler complètement de notre usine logicielle avec laquelle il communique via une API REST.\n\nConcernant le stockage des données, nous avons simplement choisi un moteur très performant qui supporterait la charge sur une seule machine bien calibrée. Cela nous évitait alors les problématiques complexes de clustering. Mais nous devions tout de même stocker quelques informations relationnelles : il fallait donc avoir accès à quelques primitives nous permettant d’émuler les relations minimum entre nos données. Redis s’est donc imposé comme la solution adéquate. Cela reste malgré tout une solution théoriquement insatisfaisante, car non réellement scalable. Mais en pratique, les très bonnes performances de Redis permettent de répondre à (bien plus que) nos attentes.\n\nLe code se trouve, pour sa part, complètement isolé sur son propre serveur.\nComme ce service est complètement stateless et que notre base de donnée est centralisée et suffisamment performante, nous pouvons donc facilement ajouter ou supprimer des serveurs web selon la charge attendue : on peut dire qu’en pratique le service Polls est scalable horizontalement.\n\nLorsque l’architecture mise en place permet de répartir la charge sur un nombre variable de machines, le contrat est rempli : ce n’est plus qu’une question d’argent pour supporter n’importe quelle charge. Et comme tout le monde le sait : l’argent n’est pas un problème, c’est une solution.\n\nDéveloppement\n\nLe service Polls a été développé en PHP avec Symfony et le FOSRestBundle. Nous avons d’abord suivi certaines références, puis nous avons ensuite développé un micro ORM maison pour faire persister nos données dans Redis et enfin nous avons monitoré tous ce que l’on pouvait à l’aide de notre bundle dédié.\n\nUne attention toute particulière a été portée à la qualité avec des tests unitaires couvrant un maximum de code et des tests fonctionnels couvrant la plupart des cas d’utilisation des clients. Les nombreuses mises en production journalières pendant la phase d’optimisation ont ainsi été grandement facilitées, notamment grâce à la sérénité apportée par l’intégration continue.\n\nMise en production\n\nL’intégration de ce nouveau service Polls a cependant été bien plus longue que son développement. Nous l’avons d’abord mis en production en doublon de l’ancien système : toutes les écritures étaient faites sur les deux systèmes, mais l’ancien était encore la référence lors de la lecture des résultats par les clients.\n\nPuis après deux semaines, lorsque nous avons validé l’exacte corrélation entre les deux courbes du nombre de votes par minute à l’aide de Graphite, nous avons alors changé les clients pour qu’ils viennent lire les résultats sur le service Polls.\n\nEncore deux semaines plus tard, lorsque tout était validé et que nous avions développé et exécuté un script d’import de l’historique, nous avons débranché l’ancien système.\n\nL’intégration a donc été au moins trois fois plus longue, et donc couteuse, que le développement du service en lui-même.\n\nOptimisation\n\nLa première optimisation est simplement conceptuelle : nous avons concentré la criticité sur une seule route, celle qui est utilisée par chaque client pour voter. Il est ainsi plus simple de mesurer et donc d’améliorer les performances du service Polls. Cette route est critique parce qu’elle est utilisée par tous les clients, qu’elle ne peut pas être cachée et qu’il faut écrire des données en base lors de chaque appel.\n\nIl existait plusieurs pistes d’optimisation connues (système de queue, node.js, etc.) mais dans une optique KISS, nous avons d’abord opté pour l’utilisation des technologies en place pour ensuite interpréter les résultats récupérés lors des tests de charge et s’adapter si besoin.\n\nDans un premier temps, nous avons légèrement ajusté notre modèle de données pour limiter le nombre d’action à réaliser sur la base de données : nous avons seulement deux instructions Redis de complexité constante O(1) à réaliser pour chaque vote. Puis nous avons utilisé les transactions pour grouper ces deux instructions et éviter la latence d’une connexion supplémentaire vers notre serveur Redis.\n\nNous avons enfin supprimé la vérification de deux contraintes d’intégrité sans importance. Le code retour en cas d’erreur est juste un peu moins cohérent (400 au lieu de 422) mais cela n’impacte ni l’intégrité des votes ni la sécurité du service.\n\n\n\n\n\nAfin de savoir si nous n’avions pas complètement pris une mauvaise direction dans notre utilisation de Symfony, nous avons alors fait appel à Alexandre Salomé, consultant SensioLabs, pour auditer notre code.\n\nLors de cette journée, durant laquelle nous avons beaucoup appris, nous avons simplement désactivé tous les bundles que nous n’utilisions pas réellement en production : principalement Twig. Cela a occasionné une légère modification de notre code car le FOSRestBundle nécessite Twig pour afficher les erreurs même lorsque celles-ci sont en JSON.\n\nUne fois cette modification apportée, nous avons gagné les ultimes millisecondes nous permettant de passer sous la barre symbolique des 10ms de temps de réponse sur notre route critique.\n\n\n\nVous remarquerez que nous avons d’abord déployé le système en production avant de chercher à l’optimiser : nous pouvions ainsi mesurer en temps réel l’impact de nos développements sur une multitude d’indicateurs dont le temps de réponse.\n\nMise en pratique\n\nLe service Polls a facilement tenu la charge pour la première émission mettant en avant le second écran : un épisode de Hawaï 5-0 durant lequel les internautes pouvaient choisir le coupable avec un sondage (sur leur téléphone, tablette ou PC).\n\n\n\nPlus précisément, nous sommes montés à 150 requêtes par secondes (ce qui est évidemment bien moins que nos tests de charge), mais nous savons que nous pourrons maintenant nous adapter très simplement à une charge beaucoup plus forte en ajoutant des serveurs web. Notamment lors d’émissions faisant grandement appel au second écran.\n\nDans le pire des cas, si le service Polls devient indisponible, aucune autre partie de notre infrastructure ne sera compromise.\n\nLeçons\n\nAu cours du développement, de la mise en production et de la maintenance de ce service, j’ai appris plusieurs choses que j’essaierai de ne pas oublier trop vite :\n\n\n  Yes we can! Il est possible de combler petit à petit la dette technique, mais uniquement si c’est la volonté des décideurs,\n  la sérénité apportée par les tests automatisés est sans égale pour le confort de développement,\n  Redis est très performant.\n\n\nHa ? Attendez ! On me dit dans l’oreillette que certains doutaient encore qu’il était possible de faire du code performant avec un framework full stack comme Symfony.\n\nPas moi :-)\n"
} ,
  
  {
    "title"    : "How we use StatsD",
    "category" : "",
    "tags"     : " statsd, graphite, php, nodejs, monitoring",
    "url"      : "/2014/01/28/how-we-use-statsd.html",
    "date"     : "January 28, 2014",
    "excerpt"  : "What we want\n\nAs developers, we (M6Web) want to keep our eyes open on what is going on in production.\n\nOur local CMO (chief monitoring officier ;) ) did a nice presentation about this (in french).\n\nAs someone very wise (Theo Schlossnagle) said: “I...",
  "content"  : "What we want\n\nAs developers, we (M6Web) want to keep our eyes open on what is going on in production.\n\nOur local CMO (chief monitoring officier ;) ) did a nice presentation about this (in french).\n\nAs someone very wise (Theo Schlossnagle) said: “It’s not in production unless it’s monitored”. Another cool mantra is: “I am wondering what to monitor ? everything dude !”. Finally “if you can not measure it, you can not improve it.” (Lord Kelvin).\n\nWe ship new apps very often, so we have to industrialise this practice.\n\nWhat is it?\n\nStatsD is a Node.Js daemon allowing you to send metrics (increment values and timers) over UDP. The fire and forget feature of UDP is great for reducing risks of introducing latency or crashes in your application.\n\nStatsD is open sourced by etsy. In our configuration, we use several StatsD deamons and aggregate metrics on Graphite - one point per minute. Many servers allows us to scale, because we don’t sample the data at all.\n\nOn client side, we use a simple consistent hashing algorithm to dispatch metrics overs StatsD nodes on the same server.\n\nCollecting metrics\n\nFrom raw PHP\n\nWe’ve created a simple PHP lib to dispatch metrics over UDP. Check it out on Github or Packagist.\n\nThe usage is pretty straightforward :\n\n&amp;lt;?php\n// client creation\n$client = new Statsd\\Client(\n                    array(\n                        &#39;serv1&#39; =&amp;gt; array(&#39;address&#39; =&amp;gt; &#39;udp://200.22.143.12&#39;),\n                        &#39;serv2&#39; =&amp;gt; array(&#39;port&#39; =&amp;gt; 8125, &#39;address&#39; =&amp;gt; &#39;udp://200.22.143.12&#39;)\n                    )\n                );\n// usage\n$client-&amp;gt;increment(&#39;a.graphite.node&#39;);\n$client-&amp;gt;timing(&#39;another.graphite.node&#39;, (float) $timing);\n\nFrom Symfony2\n\nAs basic Symfony2 fanboys, we’ve built a bundle on top of the StatsD component.\nIt provides these features:\n\n\n  manage multiple Symfony services with different configurations\n  bind any event to increment nodes and collect timers\n\n\nDuring Symfony 2 execution, metrics are collected and sent only at the kernel shutdown. A nice feature is that you can easily collect basic metrics based on events without touching your code.\n\nFor example, in conjunction with the M6Web\\HttpKernelBundle, just dropping this in config.yml is enough:\n\nm6_statsd:\n    clients:\n        default:\n            servers: [&#39;all&#39;]\n            events:\n              m6.terminate:\n                increment:     request.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;\n                timing:        request.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;\n                custom_timing: { node: memory.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;, method: getMemory }\n              m6kernel.exception:\n                increment: errors.&amp;lt;status_code&amp;gt;.yourapp\n\n\n\nOffering this to the tech team means that I am now pretty sure that almost all new PHP apps pop with those metrics out of the box.\n\nPlease checkout the bundle documentation on github.\n\nFrom anywhere else\n\nFrom Flex, mobile app or JS applications we’ve developed a simple Node.js app, translating an HTTP call to a StatsD UDP one. Like the PHP implementation, this application shards the metrics over multiple servers.\n\nPlease consider sending metrics asynchronously and add a timeout to this HTTP call.\n\nLiving with metrics\n\nAbout 120K metrics are collected on our platform. That’s a lot.\n\nGraphite dashboards are quite rustic. But surprisingly lots of non-techs people use this tool: SEO experts, advertising managers, contributors, …\n\n\n\n\n\nFor now we keep using Graphite. We try to keep our dashboards organised and well named.\n\nFor alerting purpose, a tool based on Graphite JSON output has been developed. It sends emails when it reaches some user defined conditions. Honestly, it does the job, but frankly we are still looking for something else, more flexible with more notification systems than emails.\n\nIf you use such a tool, and you’re happy with it, please let us know in the comments.\n\nFound a typo or bad english langage, just propose a pull request.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #6",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2014/01/20/m6web-dev-facts-6.html",
    "date"     : "January 20, 2014",
    "excerpt"  : "Parce que nous en avons encore une quantité incroyable en stock, voici une nouvelle sélection des meilleures phrases entendues dans nos bureaux !\n\nTo code or not to code\n\n\n  \n    Je veux faire du code qui marche, et qui sert à quelque chose !\n    ...",
  "content"  : "Parce que nous en avons encore une quantité incroyable en stock, voici une nouvelle sélection des meilleures phrases entendues dans nos bureaux !\n\nTo code or not to code\n\n\n  \n    Je veux faire du code qui marche, et qui sert à quelque chose !\n    C’est soit l’un, soit l’autre, mec …\n  \n\n\nImage trompeuse\n\nUn dév teste une appli sur ipad\n\n\n  \n    Ah mais l’image est toute moche\n    C’est ton reflet que tu vois\n  \n\n\nUne histoire de chocolat\n\n\n  \n    Qui a testé la Kindle HDX ?\n    Moi j’ai pas trop aimé\n    Pourquoi ?\n    Je préfére le Kindle Surprise\n  \n\n\nc’est automatique je te dis\n\n\n  Mais je te dis que c’est automatique … à 90%\n\n\nUne évidence évidente\n\n\n  C’est pas toujours simple parce que c’est compliqué\n\n\nTout tout tout\n\n\n  Ca couvre la quasi totalité de tout\n\n\nVers l’infini et au delà\n\n\n  Il faut que le forum soit caché intégralement autant que possible\n\n\nFichier introuvable\n\n\n  Je t’envoie par mail l’url du fichier\n\n\nPar mail :\n\n\n  file://C:/Users/**/Desktop/Sanstitre-1.html\n\n\nUnknown Notice\n\n\n  Les notices c’est tabous, on en viendra tous à bout\n\n\nLa minute de 30 secondes - le retour\n\n\n  Tu as pris six mois d’année sabatique\n\n"
} ,
  
  {
    "title"    : "Vagrant &amp; Cie, du Dév à la Prod avec Julien Bianchi",
    "category" : "",
    "tags"     : " lft, vagrant, video",
    "url"      : "/2014/01/18/vagrant-julien-bianchi.html",
    "date"     : "January 18, 2014",
    "excerpt"  : "Un grand merci à Julien Bianchi qui, à notre demande, est venu nous parler un peu de vagrant lors d’un de nos fameux Last Friday Talk.\n\nSes slides : https://speakerdeck.com/jubianchi/vagrant-and-cie-du-dev-a-la-prod.\n",
  "content"  : "Un grand merci à Julien Bianchi qui, à notre demande, est venu nous parler un peu de vagrant lors d’un de nos fameux Last Friday Talk.\n\nSes slides : https://speakerdeck.com/jubianchi/vagrant-and-cie-du-dev-a-la-prod.\n"
} ,
  
  {
    "title"    : "API à consommer avec modération",
    "category" : "",
    "tags"     : " outil, api, symfony, doctrine, cytron, open-source",
    "url"      : "/2014/01/08/api-a-consommer-avec-moderation.html",
    "date"     : "January 8, 2014",
    "excerpt"  : "Après avoir travaillé pendant plusieurs mois sur la création et les tests de nos API avec Symfony, le moment de leur publication est enfin arrivé !\n\nOr, les clients de nos API sont multiples : il peut s’agir d’applications mobiles, de sites web ma...",
  "content"  : "Après avoir travaillé pendant plusieurs mois sur la création et les tests de nos API avec Symfony, le moment de leur publication est enfin arrivé !\n\nOr, les clients de nos API sont multiples : il peut s’agir d’applications mobiles, de sites web mais aussi d’un back office interne. Chacun de ces clients peut nécessiter des “vues” différentes de l’API.\n\nEffectivement, alors que le BO devra pouvoir accéder à la totalité des ressources disponibles, l’application mobile ne devra avoir accès qu’aux ressources publiées. De la même manière, la gestion du cache ainsi que la disponibilité des routes doit pouvoir s’adapter facilement aux clients qui consomment l’API.\n\nNous avons opté pour l’utilisation d’un sous-domaine par client afin de l’identifier et ainsi de lui appliquer des configurations particulières. Ex :\n\n\n  https://bo.api.monservice.fr pour le BO,\n  https://mobile.api.monservice.fr pour l’application mobile.\n\n\n#### Authentification\n\nNous utilisons le composant sécurité de Symfony, qui permet de créer un utilisateur authentifié à la volée et de charger la configuration spécifique à celui-ci.\n\nNous avons tout d’abord besoin de créer une classe User implémentant Symfony\\Component\\Security\\Core\\User\\UserInterface, et contenant les informations de configuration spécifique.\n\nLes différents Users sont ensuite créés à l’aide d’un fournisseur d’utilisateurs implémentant Symfony\\Component\\Security\\Core\\User\\UserProviderInterface.\nDans notre cas, chaque utilisateur possède son propre fichier de configuration yml. Le fournisseur d’utilisateur vérifie donc que l’utilisateur demandé possède un fichier de configuration et instancie un objet User avec cette configuration. Ce UserProvider est défini comme service dans notre bundle et configuré dans security.yml.\n\nIl faut ensuite créer notre propre fournisseur d’authentification pour avoir une authentification par nom de domaine. Pour cela nous avons suivi et adapté le cookbook de Symfony. Cette authentification s’articule autour de 2 classes : un FirewallListener et un AuthenticationProvider. Pour que notre FirewallListener puisse facilement récupérer le client associé, nous avons ajouté un paramètre au routing Symfony :\n\nhost: {client}.api.monservice.fr\n\nLe FirewallListener utilise donc ce paramètre du routing comme nom d’utilisateur et le transmet à notre AuthenticationProvider. Celui-ci récupère le User grâce au UserProvider et profite de cette phase pour vérifier que l’adresse IP du client est bien autorisée dans sa configuration grâce au FirewallBundle.\n\nEffectivement, nous avons ajouté un filtrage initial (mais optionnel) sur les IPs pour chaque client, dans le fichiers app/config/users/{username}.yml :\n\nfirewall:\n    user_access:\n        default_state: false\n        lists:\n            m6_prod: true\n            m6_preprod: true\n            m6_dev: true\n            m6_lan: true\n            m6_local: true\n            m6_public: true\n\nPour plus de précisions, voir la documentation du FirewallBundle.\n\nAutorisation\n\nPour gérer les autorisations d’accès des utilisateurs aux différentes routes, nous avons créé un EventListener qui écoute kernel.request et qui décide de laisser passer la requête ou non en fonction de la configuration de l’utilisateur.\n\nallow:\n    default: true\n    methods:\n        delete: false\n    resources:\n        exam: false\n    routes:\n        get_articles: false\n\nDans cet exemple, l’utilisateur a accès par défaut à toutes les routes sauf les méthodes DELETE, les routes concernant les exams et la route spécifique get_articles.\n\n#### Durée de cache\n\nLes temps de cache sont différents en fonction de l’utilisation des données. Les données du backoffice ne seront pas cachées, tandis que les données de l’application mobile auront un temps de cache de 300s.\nNous avons là-aussi créé un EventListener qui écoute cette fois kernel.response et qui modifie les headers de cache de la réponse en fonction de la configuration utilisateur qui peut contenir une durée par défaut de cache et des durées de cache par route.\n\nFiltrage automatique avec Doctrine\n\nNous pouvons offrir une “vue” différente de nos données à chaque client en définissant des critères de filtrage pour Doctrine (ex: date de publication, ressource activée, etc.) dans les fichiers de configuration des clients :\n\nentities:\n    article:\n        active: true\n        publication: false\n\nAfin de ne pas modifier le comportement par défaut de Doctrine, nous avons ajouté une méthode findWithContext à nos repositories qui reprend les mêmes paramètres que la méthode findBy en injectant le SecurityContext. Cette méthode permet donc de récupérer des entités filtrées en fonction des paramètres d’un client :\n\n&amp;lt;?php\n$article = $this\n    -&amp;gt;get(&#39;m6_contents.article.manager&#39;)\n    -&amp;gt;getRepository()\n    -&amp;gt;findWithContext($this-&amp;gt;container-&amp;gt;get(&#39;security.context&#39;), [&#39;id&#39; =&amp;gt; $id]);\n\n#### Personnalisation avancée\n\nGrâce à l’utilisation du Bundle Security de Symfony, toute la configuration spécifique à un sous-domaine est stockée dans l’utilisateur courant. Et dans Symfony, l’utilisateur courant est facilement récupérable à partir du service security_context. Il est ainsi possible de personnaliser n’importe quelle brique de l’application en y injectant la dépendance sur ce service.\n\nDomainUserBundle\n\nAfin d’implémenter facilement ce fonctionnement sur nos API, nous avons développé un bundle dédié. Il peut donc aussi vous permettre de gérer l’authentification et la configuration de vos API par nom de domaine.\n\nDomainUserBundle est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Qui a bouchonné mon Redis ?",
    "category" : "",
    "tags"     : " qualite, outil, redis, cytron, open-source",
    "url"      : "/redismock-qui-a-bouchonne-mon-redis",
    "date"     : "December 11, 2013",
    "excerpt"  : "Les tests fonctionnels tiennent un rôle majeur dans la réussite et la pérennité d’un projet web, d’autant plus s’il est déployé continuellement. Nous nous étions donc déjà intéressés à cette problématique dans le cas d’un service proposant une API...",
  "content"  : "Les tests fonctionnels tiennent un rôle majeur dans la réussite et la pérennité d’un projet web, d’autant plus s’il est déployé continuellement. Nous nous étions donc déjà intéressés à cette problématique dans le cas d’un service proposant une API REST et utilisant MySQL et Doctrine. Mais nous développons aussi des services du même type utilisant d’autres systèmes de stockage de données comme Redis.\n\nAfin de tester fonctionnellement ces services, nous avons d’abord eu l’idée d’installer une instance Redis sur nos serveurs de tests. Mais nous allions inéluctablement retomber sur les mêmes obstacles qu’avec MySQL :\n\n\n  il n’est pas toujours possible de monter une instance Redis dédiée aux tests,\n  mais surtout une telle architecture n’est pas viable dans un système de tests concurrentiels.\n\n\nLa librairie RedisMock\n\nNous nous sommes alors penchés sur la possibilité de bouchonner Redis, chose qui parait au premier abord plus aisée que de bouchonner Doctrine : Redis propose une API simple et bien documenté (même si abondante). Nous pensions trouver une librairie PHP déjà existante mais nos recherches sont restées vaines.\n\nNous avons donc créé la librairie RedisMock qui reprend simplement les commandes de l’API de Redis et simule leur comportement grâce aux fonctions natives de PHP. Évidemment, toutes les commandes Redis n’ont pas encore été implémentées, seules celles qui sont utilisées dans nos tests sont présentes. Vous pouvez nous proposer l’implémentation de nouvelles fonctions Redis, selon vos besoins, via des Pull Requests sur le projet.\n\nToutes les commandes exposées par le mock sont testées unitairement via atoum en reprenant pour chaque cas les spécifications énoncées dans la documentation Redis.\n\nUtiliser RedisMock dans vos tests sur Symfony\n\nTout d’abord, il faut rajouter la dépendance à la librairie dans le composer.json et mettre à jour les vendors :\n\n\n\nL’utilisation du mock reste très simple dans un projet Symfony. Chez M6Web, nous utilisons notre propre composant Redis, lui même basé sur Predis. Afin que le mock puisse complètement se faire passer pour la librairie Redis lors de l’execution des tests, nous avons implémenté une factory qui crée à la volée un adapteur héritant de la classe à bouchonner. La méthode getAdpaterClass permet de récupérer le nom de la classe à instancier.\n\n\n\nPour simplifier la création de l’adapteur et son injection dans l’application via le fichier config_test.yml, on peut utiliser la méthode getAdapter qui instancie directement l’objet sans paramètre. Il nous suffit alors de modifier la définition du service Redis dans l’environnement de test.\n\n\n\nEt voilà, le tour est joué ! Les tests utilisent maintenant le mock à la place du véritable Redis. Attention cependant, si votre librairie utilise des fonctionnalités non implémentées dans RedisMock, vous pourriez faire face à des comportements aléatoires indésirables.\n\nRedisMock  est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Composer installation without github.com (nor packagist) dependency - like a boss !",
    "category" : "",
    "tags"     : " satis, composer, aws, s3, github, packagist, cloud",
    "url"      : "/composer-installation-without-github",
    "date"     : "December 2, 2013",
    "excerpt"  : "\n\nFirst a thought about github, composer, packagist : we like / adore / thanks the contributors, for those great services and all the open source people dropping great software on it.\n\nThat said, picture yourself operating an online PHP service, g...",
  "content"  : "\n\nFirst a thought about github, composer, packagist : we like / adore / thanks the contributors, for those great services and all the open source people dropping great software on it.\n\nThat said, picture yourself operating an online PHP service, generating hundreds euros per hour (cool isn’t it ?).\n\nIf you use Symfony2 and other public packages, like us, you’re probably deploying your application using composer.\n\n\n\nSuddenly the service is dealing with more and more and more traffic (maybe someone talk about on national tv … or Justin Bieber tweet something … maybe :) ). No problem, says the system administrator (yes our sysadmins are cool), lets pop more virtual machines and deploy more instance of the service !\n\nAnd then :\n\n\n\nboum ! =&amp;gt;composer install command can’t download distant packages on api.github.com (website is down, or the network connection or whatever).\n\nGood luck explaining to your boss that you rely on free hosting service to deploy your business critical website !\n\nThis is our situation. So here how we deal with that.\n\nPrinciples.\n\n\n\nWe chose to use Satis - a great tool provided by the Composer team. The main idea is, regulary download packages and their informations on our local servers. We (at M6Web) deployed services on our local infrastructure and on S3 servers in Amazon Web Services.\n\nHow to ? For your local network.\n\nWe set 2 different satis instance. One for our private packages, and another for all the dependencies we use (basically around Symfony2). The first one (satis-private) will build every 5 minutes, the second (satis-public) every half hour.\n\nfor example :\n\n\n  satis-private.yourcompany.com\n  satis-public.youcompany.com\n\n\nSatis for private package configuration (data/satis.json) :\n\n{\n    &quot;name&quot;: “satis-private&quot;,\n    &quot;homepage&quot;: &quot;https://satis-private.yourcompany.com&quot;,\n    &quot;archive&quot;: {\n        &quot;directory&quot;: &quot;dist&quot;,\n        &quot;absolute-directory&quot; : &quot;/srv/data/satis-private/dist&quot;,\n        &quot;format&quot;: &quot;zip&quot;,\n        &quot;skip-dev&quot;: true\n    },\n    &quot;repositories&quot;: [\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/great-bundle&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/great-component&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/awesomelib&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/raoul&quot; },\n…\n    ],\n    &quot;require-all&quot;: true\n}\n\n\nSatis for public package configuration (data/satis.json) :\n\n{\n    &quot;name&quot;: “satis-public&quot;,\n    &quot;homepage&quot;: &quot;https://satis-public.yourcompany.com&quot;,\n    &quot;archive&quot;: {\n        &quot;directory&quot;: &quot;dist&quot;,\n        &quot;format&quot;: &quot;zip&quot;,\n        &quot;skip-dev&quot;: false,\n        &quot;absolute-directory&quot; : &quot;/srv/data/satis-public/dist&quot;\n    },\n    &quot;repositories&quot;: [\n        { &quot;type&quot;: &quot;composer&quot;, &quot;url&quot;: &quot;https://packagist.org&quot; }\n    ],\n    &quot;require&quot;: {\n\n        &quot;m6web/firewall-bundle&quot; : &quot;*&quot;,\n        &quot;m6web/statsd-bundle&quot;   : &quot;*&quot;,\n\n        &quot;doctrine/orm&quot;               : &quot;~2.3&quot;,\n        &quot;doctrine/common&quot;            : &quot;~2.4&quot;,\n        &quot;doctrine/dbal&quot;              : &quot;~2.3&quot;,\n        &quot;doctrine/doctrine-bundle&quot;   : &quot;~1.2&quot;,\n\n        &quot;naderman/composer-aws&quot;      : &quot;~0.2.3&quot;,\n…\n    },\n    &quot;require-dependencies&quot;: true\n}\n\n\nOn the crontab, add this command for each satis instance :\n\nphp -d memory_limit=xx bin/satis build data/satis.json web\n\n\n(increasing memory_limit was mandatory for us with satis-public).\n\nPlease note the require-dependencies directive. It tell satis to digg on sub-dependencies on the required packages. And yes, it can take a while. You will probably hit the Github API rate limit. To increase it, add a Github key on your composer configuration file on the satis servers.\n\n$ cat .composer/config.json\n{\n    &quot;config&quot;: {\n        &quot;github-oauth&quot;: {\n            &quot;github.com&quot;: “xxxxxx&quot;\n        }\n    }\n\n}\n\n\nIn your projects, edit the composer.json and replace the repositories entry by\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://satis-private.yourcompany.com&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://satis-public.yourcompany.com&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nRemove your composer.lock and vendors then run composer update on the project.\n&quot;packagist&quot;: false&quot; mean : “do not search missing packages on packagist.com”. If a package is missing during install, you have to add it in satis-public configuration file then try again.\n\nthat’s it :)\n\nHow to ? For AWS.\n\nSync our 2 satis servers with an S3 bucket.\n\n\n\nOn satis servers, use s3cmd to keep in sync the S3 bucket. Let’s say : yourcloud-satis.\n\nAdd some commands after the build script of satis :\n\nphp -d memory_limit=xx bin/satis build data/satis.json web\ncd web\nsed &#39;s#https://satis-private\\.yourcompany\\.com#s3://yourcloud-satis/satis-private#&#39; packages.json &amp;gt; packages-cloud.json\ns3cmd put index.html s3://yourcloud-satis/satis-private/index.html\ns3cmd put packages-cloud.json s3://yourcloud-satis/satis-private/packages.json\ncd /srv/data/satis-private/\ns3cmd sync ./dist s3://6cloud-satis/satis-private/\n\n\n(do the same for satis-public).\n\nupdate your projects\n\nIn your projects, edit the composer.json and replace the repositories entry by\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-private/&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-public/&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nEnable the AWS plugin in EC2 servers\n\nAdd our repositories in ~./composer/composer.json file of the user used to deploy your code.\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-private/&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-public/&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nYou have to install the S3 plugin for composer on your EC2 instance.\n\n$ composer global require &quot;naderman/composer-aws:~0.2.5&quot;\n\n\nIf you don’t use IAM roles, add the following composer config on your EC2 servers (~/.composer/config.json) :\n\n{\n    &quot;config&quot;: {\n        &quot;amazon-aws&quot;: {\n            &quot;key&quot;:    &quot;KEYYYYYY&quot;,\n            &quot;secret&quot;: &quot;seeeeecret&quot;\n        }\n    }\n}\n\n\ncomposer install --prefer-dist command will now download all the packages files from S3 !\n\nThanks to Pierre and Jeremy for their help.\n\nFound a typo or bad english langage, just propose a pull request.\n"
} ,
  
  {
    "title"    : "JenkinsLight, mettez en lumière vos jobs Jenkins",
    "category" : "",
    "tags"     : " outil, jenkins, ci, cytron, open-source",
    "url"      : "/jenkinslight-mettez-en-lumiere-vos-jobs-jenkins",
    "date"     : "November 20, 2013",
    "excerpt"  : "L’idée de JenkinsLight a germé lorsque nous nous sommes fait taper sur les doigts pour la troisième fois (à juste titre) parce que l’on avait désactivé la publicité sur nos sites de chaîne lors d’une mise en production. Or la publicité est un poin...",
  "content"  : "L’idée de JenkinsLight a germé lorsque nous nous sommes fait taper sur les doigts pour la troisième fois (à juste titre) parce que l’on avait désactivé la publicité sur nos sites de chaîne lors d’une mise en production. Or la publicité est un point critique car directement reliée au chiffre d’affaires. Le pire est que nous testions déjà le bon fonctionnement de la publicité en intégration continue sur nos serveurs de preprod, avant la mise en production. Mais une configuration légèrement différente sur les serveurs de prod rendait le nouveau code instable. Cette situation rend donc impossible la détection de certaines anomalies avant la mise en production…\n\nD’où notre besoin d’avoir un tableau de bord nous permettant de vérifier chaque instant la disponibilité des fonctionnalités névralgiques de nos sites en production afin de réagir au plus vite en cas de problèmes. Et ce, avant même que l’anomalie ne nous soit remontée par les autres secteurs. Nous avions déjà nos tests dans Jenkins que nous avons alors fait pointer vers la prod. Il nous manquait donc juste une sorte de “Panic Board” sur un écran placé au sein de nos bureaux nous remontant rapidement le moindre problème sur nos sites en production.\n\nNous avons créé JenkinsLight qui permet d’afficher distinctement le statut des jobs d’une vue Jenkins en quasi temps réel. Le projet utilise AngularJS et l’API de Jenkins pour récupérer les informations nécessaires. L’installation se fait sur n’importe quel serveur web et requiert uniquement Bower pour installer les composants. Afin de permettre à l’API d’être appelée en crossdomain (CORS), il est également nécessaire d’installer un plugin spécifique sur votre serveur Jenkins.\n\nL’application propose quelques variables de configuration éditables dans le fichier “app/scripts/config.js” permettant de spécifier :\n\n\n  l’url du serveur Jenkins,\n  l’identification au serveur (si nécessaire),\n  la vue Jenkins par défault,\n  les types de jobs affichés,\n  une regexp pour exclure certains jobs,\n  le nombre maximum de jobs par ligne sur l’écran,\n  l’intervalle de rafraîchissement (en millisecondes),\n  une image de fond quand il n’y a aucun job à afficher.\n\n\nJenkinsLight est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 3",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-3",
    "date"     : "November 19, 2013",
    "excerpt"  : "\n\nDernière journée de cette Velocity Europe, avec en plus du track Performance et Ops, l’ouverture d’un track Culture.\n\nPour rappel, si vous les avez ratés, les CR des journées précédentes sont à retrouver ici :\n\n\n  CR Velocity Europe 2013 - Day 1...",
  "content"  : "\n\nDernière journée de cette Velocity Europe, avec en plus du track Performance et Ops, l’ouverture d’un track Culture.\n\nPour rappel, si vous les avez ratés, les CR des journées précédentes sont à retrouver ici :\n\n\n  CR Velocity Europe 2013 - Day 1\n  CR Velocity Europe 2013 - Day 2\n\n\nPour ce “Day 3”, nous nous retrouvons tous dans la grande salle avec la vidéo “Slow Motion Water Balloon Fight” en guise d’introduction :\n\n\n\nExtreme Image Optimisation: WebP &amp;amp; JPEG XR\n\nIdo Safruti (Akamai) @safruti\n\n\n\nA ce jour, d’après HTTP Archive, 62% du poids des pages Web correspond aux images sur desktop, 65% sur mobile.\n\nNous utilisons toujours des technos vieilles de plus de 15 ans ! Jpg, Png, Gif …\n\n\n  “Deploying new image formats on the web is HARD (but doable)” Ilya Grigorik\n\n\nLa conférence traite de deux formats bien plus récents :\n\n\n  WebP (2011)\n  JXR : Jpeg eXtended Range (2009)\n\n\nqui supportent le lossless et lossy, ainsi que la transparence (en lossless et lossy aussi)\n\nOn retrouve un tableau très intéressant sur une comparaison taille entre les différents formats sur un même niveau de qualité :\n\n\n\nAttention, certaines fois (quelques %), l’image peut être plus grosse qu’en jpg. Si vous partez du Jpg pour la compression, comparez les tailles et affichez le Jpg s’il est plus petit.\n\nLe support de ces formats reste toutefois minime :\n\n\n  WebP : Chrome &amp;gt;= 23, Opera &amp;gt;= 12, …\n  Jpeg XR : IE =&amp;gt; 10, …\n\n\nJXR, gère notamment le progressive, ce que ne gère pas encore date WebP. Plus d’infos sur les “progressives Jpeg” sur le blog de Patrick Meenan (Créateur de WebPageTest).\n\nUne petite anecdote intéressante sur WebP aussi. Facebook avait mis en place WebP mais est revenu en arrière car les utilisateurs râlaient ! Quand ils enregistraient ou partageait une photo de Chrome (donc en WebP), les utilisateurs IE notamment, ne pouvaient pas la consulter …\n\nIdo fera un article sur l’incontournable calendrier de l’avant sur le sujet : Performance Calendar en décembre 2013 !\n\n\n\nSLOWING DOWN TO GO FASTER: Responsive Web Design And The Problem Of Agility vs Robustness\n\nTom Maslen (BBC News) @tmaslen\n\n\n\nGros retour d’expérience des équipes de BBC News sur leur approche du “Responsive Web Design”, et sur la manière dont cela a impacté leurs workflows, ainsi que leur culture.\n\nLe RWD prend du temps, beaucoup plus de temps (3x), sur le Design, le développement, le test.\n\nTom parcourt les optimisations “classiques”, ainsi que la manière dont ils enrichissent l’expérience : Ils délivrent une “Core Experience” à tous, et une “Enhanced Experience” aux navigateurs qui le supportent, utilisent Grunt pour certaines automatisations (pour fournir les bonnes images à la bonne taille https://github.com/BBC-News/Imager.js/, versionner les assets https://github.com/kswedberg/grunt-version …).\n\nVous pouvez aussi découvrir Wraith, leur outil de comparaison de screeshot Responsive.\n\nBref, une excellente conférence avec un très bon speaker (très drôle sur la fin).\n\n\n  “Don’t do whoopsies on other people things” Tom Maslen\n\n\n\n\nAn Introduction to Code Club\n\nJohn Wards (White October)\n\n\n\nCode Club est un projet leadé par des bénévoles pour créer des clubs de coding dans les écoles, pour des enfants entre 9 et 11 ans. Plus de 1400 clubs ont déjété créés en Angleterre !\n\nLes enfants utilisent le projet Scratch, et qui permet via un langage de programmation assez simple, de programmer des jeux.\n\nLeurs vidéos de présentation sont de plus assez fun, notamment celle ci, qui nous à été montrée, à 6mn50 dans la vidéo ci dessous :\n\n\n\nLightning Demo: Automating WebPagetest with wpt-script\n\nJonathan Klein (Etsy) @jonathanklein\n\n\n\nAfin d’automatiser la prise de mesure synthétique à l’aide de WebPageTest (notamment si vous avez installé une instance privée), les gars d’Etsy ont développé un wrapper Php à l’Api de Webpagetest. Le wrapper permet aussi de pousser les résultats dans un Graphite ou un Splunk.\n\nL’outil est dispo sur Github : Wpt-Script\n\n\n\nLightning Demo: Introducing a New RUM Resource From SOASTA\n\nBuddy Brewer (SOASTA)\n\n\n\nSOASTA, société connue notamment pour avoir racheté LogNormal (outil de R.U.M. l’année dernière), propose aujourd’hui un outil de R.U.M. nommé : mPulse.\n\nIls ont publié de nombreuses statistiques sur leur site, sur les performances, suivant le navigateur, la localité etc : https://www.soasta.com/summary/\n\n\n\nLightning Demo: Automating The Removal Of Unused CSS\n\nAddy Osmani (Google Chrome) @addyosmani\n\n\n\nL’un des petits problèmes récurrents du développement web est situé dans nos fichiers Css. A force d’ajout de fonctionnalités ou de framework (notamment les fameux frameworks Css, Bootstrap &amp;amp; co), on finit par obtenir des fichiers CSS gigantesques, dans lesquels il devient très compliqué de savoir ce qui est utilisé ou pas sur votre site.\n\nAddy présente des solutions qu’on peut retrouver :\n\n\n  pour un nettoyage mono page, dans la DevTools de Chrome (Onglet Audit puis Run puis “Remove Unused Css Rules”)\n  pour un nettoyage d’un site complet, via des outils autour de Grunt, notamment Grunt Uncss https://github.com/addyosmani/grunt-uncss fait par Addy en personne, basé sur le module Uncss de Giakki\n\n\n\n\nLearning from the Worst of WebPagetest\n\nRick Viscomi (Google)\n\n\n\nRick travaille pour YouTube chez Google, comme WebDéveloppeur Front-end orienté performance.\n\nSa passion, se moquer des mauvais résultats sur les historiques du WPT public :-)\n\nC’est d’ailleurs pour lui, l’une des bonnes sources pour découvrir les “anti-patterns” de la perf, et les choses à ne pas faire.\n\nIl présente une Pull Request en cours sur WebPageTest avec le Multi Variate Testing, permettant de tester tout un site, sur plusieurs localités. Plus d’infos sur l’article de son blog sur le sujet : https://jrvis.com/blog/wpt-mvt/\n\nAre today’s good practices … tomorrows performance anti-patterns\n\nAndy Davies @andydavies​\n\n\n\nAvec l’arrivée d’HTTP 2.0, on se demande, si les optimisations WebPerf que nous réalisons aujourd’hui ne seront pas gênantes demain : Les dataURI, le JS inline, le domain sharding, les sprites …\n\nLes réponses ne sont pas aussi simples, et nous “développeurs” nous devons de nous poser les questions afin d’avoir les bonnes réponses avant l’arrivée d’HTTP 2.0. Andy à le mérite de lancer le débat, via des protocoles de test pour chacun des cas. en comparant HTTP 1.0 et SPDY.\n\n\n\nProvisioning the Future - Building and Managing High Performance Compute Clusters in the Cloud\n\nMarc Cohen, Mandy Waite (Google)\n\nMarc et Mandy nous ont présenté le Google Cloud, alternative AWS. Basé sur du KVM hautement modifié par les équipes de Google, on retrouve grossièrement des services identiques (stockage élastique persistent, SDN pour le réseau, load-balancing, des profils de machines highmem ou highcpu). Toutefois on notera l’absence d’un marketplace pour les images des VMs et seules Debian et Centos sont disponibles. Énorme avantage par rapport AWS: la facturation la minute au bout de 15min ! Mandy nous a fait la démo du lancement de 1000 Vms en 2min15. Google fournit une api complète et un outil en ligne de commande pour piloter absolument tout: gcutil.\n\nSecurity Monitoring (With Open Source Penetration Testing Tools)\n\nGareth Rushgrove (Government Digital Service)\n\nCombien d’entre nous testent la sécurité de leur applicatif en continu ? Elle devrait pourtant faire partie de l’assurance qualité du développement d’un logiciel. Gareth propose donc d’ajouter des tests de sécurité via Jenkins et des tests unitaires dans notre pipeline de développement. Parmis la liste d’outils (rkhunter, naxsi, logstash, fail2ban, auditd, la distrib BackTrack, clamav, Arachni) certains sont aisément intégrables au workflow. A tester le très bon OWASP ZAP (et https://www.dvwa.co.uk/ pour se faire la main !).\n\nLes slides :\n\n\n\nBeyond Pretty Charts…. Analytics for the cloud infrastructure\n\nToufic Boubez (Metafor Software) @tboubez\n\nToufic travaille depuis 20 ans dans la gestion des données des datacenters et la détection d’anomalies. Comme lors de la présentation de Twitter il explique qu’on ne peut pas appliquer ces données temporelles des méthodes statistiques classiques (holt winter forecast, régression linéaire, smooth splines), car elles sont non stationnaires (violant ainsi le principe d’homogénéité) et la plupart du temps elles ne sont pas distribués normalement (principal pré-requis). Il nous a donc présenté le test de Kolmogorov-Smirnov couplé aux techniques de bootstraping qui permet d’avoir des prédictions assez fiables. Comme les modèles ARIMA, il fait partie de la famille des méthodes statistiques non paramétriques, qui ne présupposent pas de la distribution des données)\n\nLes slides :\n\n\n\nAutomated Multi-Platform Golden Image Creation, Unlocking New Potential\n\nMitchell Hashimoto (HashiCorp) @mitchellh\n\nAvoir un environnement stable, clonable depuis la dev vers la prod est le rêve conjoint des développeurs et sysadmins. L’utilisation des images pour le déploiement de machines et de code peut être fastidieux, au moindre changement de version il faut instancier l’image, faire la modification, recréer l’image etc…Ce qui n’est pas forcement adapté au cloud computing et la virtualisation. A l’inverse n’utiliser que des logiciels de gestion de la configuration (Cfengine, Puppet, Chef) ne certifie pas qu’un serveur vierge aura le même comportement qu’un serveur sur lequel on aura passé 10.000 modifications. Il m’arrive régulièrement d’avoir des erreurs de run puppet cause d’une dépendance non satisfaite (packages) ou de problèmes réseau, (le plus souvent on souffre de la lenteur d’application d’un profil puppet).\n\nDe plus passé du VMWare du AWS ou Vagrant demande d’avoir autant d’images que de plateforme ! Ce saint graal du “serveur immuable” et agnostique de la plateforme est possible en utilisant une méthode intermédiaire : Packer permet de créer des images (Aws, Virtualbox, VMWare) partir d’une source et avec l’aide d’un chef/puppet/cfengine. Le workflow proposé est le suivant: commit dans le repository =&amp;gt; build avec Packer =&amp;gt; CI (Jenkins) =&amp;gt; Image ready !\n\nCela permet de garder la flexibilité d’un Puppet avec l’idempotence des images et leur facilité, rapidité de déploiement.\n\nL’orchestration peut être réalisée avec Serf, il implémente un protocole Gossip (tout le monde se parle, mais pas en même temps). C’est un agent installé sur le serveur qui gère des messages et des handlers (scripts personnalisés dans le langage de votre choix). Dans l’exemple de déploiement de multiples load balancer, l’image va permettre d’avoir un système fonctionnel rapidement, les utilisateurs, les logiciels et c’est Serf qui récupérera la configuration appliquer au load balancer.\n\nOn peut aller plus loin en déployant les code avec Docker, ce qui ajoute une couche d’abstraction supplémentaire extrêmement puissante.\n\nDOM to Pixels: Accelerate Your Rendering Performance\n\nPaul Lewis (Google - Team Chrome) @aerotwist\n\nPaul Lewis explique quelques principes mis en oeuvre lors du rendu graphique dans Chrome tel que la gestion des calques qui permet d’utiliser plus intensément la puissance du GPU mais dont la multiplication peut s’avérer contre productive : la gestion de trop nombreux calques par le CPU contrebalance la performance du rendu par le GPU (évidemment, sinon cela serait trop simple).\n\nPour bien appréhender cette présentation, il m’a semblé nécessaire d’avoir un petit background dans la programmation graphique : savoir, par exemple, pourquoi une ombre ou un flou sont couteux pour le rendu (cause des calculs entre les différentes zones de mémoire contenant les informations graphiques superposer).\n\nPaul présente ensuite en détail l’outil de debug du rendu dans les WebTools : comment enregistrer en temps réel les différentes frames affichées par Chrome et visualiser les différents temps de calculs.\n\nCette présentation, bien que très intéressante par son contenu fut aussi mise en valeur par l’interprétation de Paul Lewis : toujours précise mais simple, sérieuse et fun la fois.\n Ce fût pour moi, la meilleure présentation (show!) de la Vélocity.\n\nEt n’oubliez pas :\n\n\n  “Tools, not rules” Paul Lewis\n\n\nConclusion :\n\nEpuisé par trois jours de conférence d’une densité incroyable, La Velocity a encore aisément tenu toute ses promesses.\n\nNous espérons que ces comptes-rendus vous auront été utile, autant que les présentations nous l’ont été.\n\nN’hésitez pas à commenter l’un des CR pour donner votre avis, sur le CR, ou sur certains points couvert par les Talks.\n\nMerci.\n\nVous pouvez retrouver :\n\n\n  quelques vidéos de la conférence sur Youtube\n  les slides sur le site d’Oreilly\n  et les photos ici sur Flickr : https://www.flickr.com/photos/oreillyconf/sets/72157637657689424/\n\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 2",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-2",
    "date"     : "November 16, 2013",
    "excerpt"  : "\n\nDe retour à l’hôtel Hilton de Londres, afin de commencer cette deuxième journée qui s’annonce très chargée : jusqu’4 tracks en parallèle. Performance, Mobile, Ops, et Sponsors.\n\nMaking Government digital services fast\n\nPaul Downey @psd\n\n\n\nPaul e...",
  "content"  : "\n\nDe retour à l’hôtel Hilton de Londres, afin de commencer cette deuxième journée qui s’annonce très chargée : jusqu’4 tracks en parallèle. Performance, Mobile, Ops, et Sponsors.\n\nMaking Government digital services fast\n\nPaul Downey @psd\n\n\n\nPaul est un “Technical Architect” pour le gouvernement anglais. Il nous explique comment ils gèrent et priorisent les problématiques de performances pour offrir des services internet centrés sur les besoins des utilisateurs avant ceux du gouvernement.\n\nAvec notamment une réduction drastique du nombre de pages, ce qui leur a permis d’obtenir plus de visites au final !\n\nLe tout est entièrement documenté en ligne, en accès public, et regorge d’informations intéressante que vous pouvez retrouver sur le Gov.Uk Service Manual.\n\n\n\nStand down your smartphone testing army\n\nMitun zavery (keynote)\n\n\n\nMitun travaille chez Keynote, et fait une démonstration de deux de leurs outils :\n\nKeynote DA Free (DA = Device Anywhere)\n\nL’outil très intéressant propose un grand nombre d’appareils mobiles qu’on peut acquérir pendant 10 minutes, afin de lancer des tests. Le gros intérêt est qu’on parle ici de vrais appareils, pas de simulateurs.\n\nLe service est disponible sur : https://dafree.keynote.com après vous êtres inscrit gratuitement sur cette url https://www.keynotedeviceanywhere.com/da-free-register.html\n\n(Le service ne fonctionne pas sur Chrome mac pour ma part)\n\nC’est plutôt impressionnant techniquement, on lance les applications que l’on souhaite, rentre du texte, change l’orientation … ! A mémoriser.\n\nMITE, le deuxième outil qui à l’air très complet permet d’aller beaucoup plus loin, mais avec des simulateurs cette fois : https://mite.keynote.com/download.php\n\nDommage que l’on oublie les Macs dans l’histoire.\n\n\n\nTesting all the way to production\n\nSam adams (lmax exchange) @LMAX\n\n\n\nPour ceux qui ne seraient pas encore convaincu de l’intérêt des tests automatisés, Lmax exchange (site sur l’univers boursier gèrant des sommes d’argent assez phénoménales) présente le workflow de développement basé sur les tests pour déployer du code le plus souvent possible en évitant au maximum les régressions.\n\n\n\nMost of the time we measure the performance of others\n\nklaus enzenhofer (compuware) @kenzenhofer\n\n\n\nCourte présentation de Compuware qui édite des solutions de monitoring, et aussi Dynatrace Ajax avec une étude de cas assez simple sur la détection d’anomalies sur un site (142 domaines de 3rd party chargés !).\n\nLe blog de Compuware regorge d’article en tout genre sur la Webperf.\n\n\n\nMaking performance personal at Ft labs\n\nAndrew Betts @triblondon\n\n\n\nLes équipes du Financial Times sont très actifs dans le domaine de la webperf, avec notamment l’outil FastClick qui permet d’enlever le delay du touch sur mobile (entre 100 et 300 ms!). Ils developpent aussi une webapp html5 très riche, et expliquent comment rendre la problématique de performance importante aux yeux du “produit”. On apprend pas mal d’astuces pour mesurer la performance, gérer le appcache, éviter les sprites etc …\n\nVous pouvez retrouvez les slides ici : https://triblondon.github.io/talk-makingperfpersonal/#/\n\n\n\nLightning demo : Global web page performance\n\nJames Smith (Devopsguys) @thedevmgr\n\n\n\nJames est venu présenter lors d’une lightning démo : worldwidepagetest.com\n\nUn outil permettant de tester partout dans le monde les performances de son site, basé sur Webpagetest, et les locations et browsers disponible.\n\nEn plus de l’échec total de la démo (bug/plantage … “worst demo ever” d’après le speaker lui même), l’outil qui parait intéressant sur le papier me semble une fausse bonne idée et le risque de saturer les instances de WPT mondial à cause de ce type d’outil me parait bien plus gênant que les avantages qu’il apporte.\n\nLightning demo: HTTP Archive, BigQuery, and you!\n\nIlya Grigorik @igrigorik\n\n\n\nL’impressionnante quantité de données agrégées par HTTP Archive est maintenant disponible dans Google BigQuery : toutes les données statistiques sur les requêtes et réponses HTTP de plusieurs centaines de milliers de site différents sont donc simplement requêtables et disponibles la vitesse de la lumière (c’est une expression à la mode en ce moment par ici).\n\nUn article d’Ilya explique la marche suivre pour utiliser les données de HTTP Archive stockées sur BigQueries : https://www.igvita.com/2013/06/20/http-archive-bigquery-web-performance-answers/\n\nNous pouvons ainsi aisément effectuer quelques comparaisons avec nos “concurrents” et néanmoins amis présents la conférence :-) : https://denisroussel.fr/httparchive-bigquery-french-test.html\n\nIlya présente aussi le site communautaire BigQueri.es (Powered by Discourse), permettant de partager les réponses des questions statistiques sur les bases présentes dans BigQuery (notamment celle de HTTP Archive)\n\n\n\nGimme More! Enabling User Growth in a Performant and Efficient Fashion\n\nArun Kejariwal (Twitter) @arun_kejariwal, Winston Lee (Twitter Inc.) @winstl\n\nLa planification de la capacité (“capacity planning”) chez Twitter passe par l’utilisation de modèles statistiques et la prédiction sur des données temporelles. C’est absolument nécessaire pour le dimensionnement des plateformes techniques.\n\nL’utilisation d’une simple régression linéaire capture la tendance globale, mais ne prends pas en compte la saisonnalité ni les pics de trafic (positifs ou négatifs). Un modèle “smooth splines” correctement paramétré, de part son design, ne le permet pas non plus. Idem pour le Holt Winters (que vous pouvez tester avec graphite). Ils utilisent donc le modèle ARIMA (Autoregressive integrated moving average), qui permet d’effectuer des prédictions partir de données temporelles non stationaires (c’est dire que la moyenne et la variance change = pics de trafic). Le nettoyage des données et la vérification du modèle représente la majorité du travail. Les données journalières permettent de prédire jusqu’90 jours, et les données la minute un trimestre. Les prédictions sur 1 mois des métriques systèmes (cpu, ram) sont considérées comme fiables alors que les métriques business (nombre d’utilisateurs, nombres de photos ou vidéos stockées) le sont pour 3 ou 4 mois. Pour les évènements exceptionnels (superbowl, nouvelle année) ces prédictions ne sont pas assez fiables, ils se basent donc simplement sur les années précédentes.\n\nWhen dynamic becomes static: the next step in web caching techniques\n\nWim Godden (Cu.Be Solutions)\n\nLe monsieur effectue d’abord un récapitulatif des pratiques de cache dans le web depuis son commencement : sans cache, avec du cache applicatif, avec du reverse proxy cache et enfin avec beaucoup trop de systèmes de cache qui rende l’architecture très complexe (tiens tiens).\n\nPuis apparaissent les ESI, c’est en gros du reverse proxy cache par bloc (en réalité, ils sont parmi nous depuis bien longtemps). Mais une limitation conceptuelle évidente borne leur utilisation : les sites sont très souvent personnalisés en fonction de l’utilisateur (affichage du nom de l’utilisateur connecté par exemple). Et du coup, les blocs personnalisés, même simples, ne peuvent bénéficier du reverse proxy cache. Ce qui défie un peu le concept.\n\nPour pallier à ce problème, Wim et son équipe ont développé un langage spécifique dans Nginx (qui est aussi un reverse proxy en plus d’être un serveur http) permettant au serveur web de gérer des variables directement dans le reverse proxy afin que celui-ci les stock dans son propre memcache et puisse y accéder pour retourner la page au client sans faire un appel supplémentaire au serveur web : SCL.\n\nAlors oui. C’est pas forcément l’idéal de commencer poser des variables dans le reverse proxy. Mais n’ayons crainte, ce n’est pas pour tout de suite : la release publique ne devrait pas arriver avant mi-2014 :-)\n\nLes slides\n\nDeveloper-Friendly Web Performance Testing in Continuous Integration\n\nMichael Klepikov (Google, Inc)\n\nIntégrer les mesures/tests de régressions de performance dans nos outils d’intégrations continues et une tâche très compliqué. Michael présente une approche assez maligne consistant utiliser les tests fonctionnels déjen place, pour récolter les mesures des outils de R.U.M. déjà présent sur le site (soit parce que les mesures sont présentes dans l’url d’appel de l’outil de R.U.M.), soit en récupérant les valeurs dans les DevTools de Chrome.\n\nL’outil TSviewDB permet d’avoir une interface qui agrège plusieurs time-series sur une seule time-series (plus d’infos dans le Readme du projet).\n\nPas mal d’informations à creuser dans les slides, comme l’envoi de donnée directe WebPageTest pour utiliser l’UI sur le résultat, ou la façon de récupérer les infos de la DevTools de Chrome en vidéo\n\nLes slides :\n\n\n\nIntegrating multiple CDN providers at Etsy\n\nMarcus Barczak (Etsy), Laurie Denness (Etsy)\n\nPour des raisons de haute disponibilité, de résilience et de balance des coûts, Etsy a mis en place depuis 2012 un système qui leur permet d’utiliser de multiples CDN (parmi eux Akamai, Fastly, et EdgeCast). Leur critères d’évaluations sont le hit ratio et la décharge de trafic des serveurs origine, le reporting, le pilotage via des APIs, la personalisation et l’accès aux logs HTTP. Ils ont pour cela dû faire du ménage dans leur codes et dans leurs headers (cache-control, expires, etag, last-modified)\n\nIls ont commencé par les images (1% puis 100% du trafic), et se sont servis des CDNs pour effectuer leur tests A/B. L’equilibrage de charge entre les CDNs se fait manuellement via une interface web ou via un outil en ligne de commande qu’ils ont mis disposition de la communauté (cdncontrol sur leur github). L’inconvénient de cette solution est la multiplication des requetes DNS (puisqu’ils utilisent des CNAME type img1.etsystatic.com =&amp;gt; global-ssl.fastly.net par exemple), la non atomicité et le delais des modifications DNS qui engendrent une long tail importante et le debug plus complexe. Les requetes depuis les CDNs vers les origines sont trackés via un header HTTP et sont monitorés dans un graphite surveillé par un nagios selon un seuil déterminé.\n\n\n  “If you can do it at the origin, do it !”.\n\n\n\n\nWhat is the Velocity of an Unladen Swallow? A quest for the Holy Grail.\n\nPerry Dyball (Seatwave Ltd) @perrydyball\n\nRetour d’XP très utile, plein d’honnêteté et d’humilité, de SeatWave (site permettant d’acheter des tickets concerts/spectacle etc), sur l’effet douloureux de la première pub télé qu’ils ont acheté pour promouvoir leur service. On découvre la façon dont ils ont su optimiser leur application pour supporter les publicités suivantes, grâce un système de queuing avec un décompte en cas de fortes charges, ainsi que les impacts sur une multitude de métriques et le coté financier.\n\nLe phénomène est presque un running gag chez nous (ou même sur Twitter), quand votre (ou même l’un de nos …) site est présenté dans une pub ou Capital, et que votre application et/ou serveur ne supporte pas la charge.\n\nBref, encore un bel exemple de culture d’entreprise, qui démontre que la performance n’est pas un projet ou une feature “one shot”, mais une culture et une mentalité constante.\n\n\n  “Performance it’s not just for today, it’s for every day” Peter Dyball\n\n\n\n\nGetting 100B metrics to disk\n\nJonathan Thurman (New Relic)\n\nNew relic a présenté l’architecture MySQL qui stocke leur 196 milliards de métriques journalières. Elle est basée sur des shards MySQL, propulsée par de puissants serveurs (12 actuellement) équipés de SSD Intel et de shelf de disques Dell. Les shards sont fait (via leur API shardGuard) par numéro de client, et les tables MySQL sont construites sur le modèle numéroClient_year_julianDay_metricResolution. Il y a environ 200.000 tables par databases. Les métriques sont régulierement (toutes les heures), purgées et aggrégées, en utilisant le innodb_lazy_drop_table de percona 5.5 (et surtout par delete from ou drop table).\n\nLe code initialement en Ruby est passé Java/Jetty. Les inserts se font séquentiellement en batch de 5000 et sont buffurisés en RAM (ils doivent se faire en moins d’une minute, résolution minimale du produit). Ils prévoient d’utilisé de multiples instances MySQL par serveurs et de gonfler leur capacité hardware (SSD 800G, 96G RAM)\n\n\n\nHigh Velocity Migration\n\nJoshua Hoffman (SoundCloud) @oshu\n\nJoshua nous a conté l’histoire d’une startup (fictive mais pas vraiment car c’est celle de Tumblr), qui a commencé en 2006 entre deux amis souhaitant partager des images de parties d’échecs et comment en 2012 elle a dû, en 6H de maintenance, basculer 1200 serveurs et les données de 50M d’utilisateurs. Il nous a détaillé l’évolution année par année de l’infrastructure et du nombre de devs/sysadmins. Les ingrédients pour gérer une croissance comme celle ci sont selon lui : le provisionning automatique (ipxe, kickstart), la gestion de la configuration (puppet/chef/ansible), le monitoring et l’alerting et les outils de déploiement de code. Il précisé qu’il faut accepter l’imperfection de ses outils, ne pas chercher réinventer la roue mais plutôt utiliser l’open source, ne pas hésiter tuer les projets Zombies (ceux qui durent depuis trop longtemps et qui n’ont pas les fonctionnalités attendues !) et surtout respecter le principe KISS (keep it simple and stupid).\n\nLa migration en 2012 de leur plateforme gérée par une société tierce vers leur propre infrastructure a commencé 120 jours plus tôt avec l’installation des serveurs, machines, systèmes de déploiements, l’acquisition de leur numéro d’AS, et l’utilisation en front d’un proxy pour plus tard pouvoir rediriger le trafic de façon transparente vers la nouvelle infrastructure. Le jour J la fenetre de maintenance du site de 6H été suffisante pour synchroniser les données utilisateurs entre les deux infras, tester et mettre en production.\n\n\n\nCode is Evil\n\nDan Rathbone (British Sky Broadcasting)\n\nFace aux problèmes de performance du site https://www.skybet.com/, qui doit probablement attirer une quantité impressionnante de parieurs tout en devant afficher des données très fraîches (cela fait partie du business model), Dan a remis plat toute la logique de développement du site.\n\nLorsque la performance passe seule au premier plan, il est ainsi possible de renverser le paradigme du développement dans son ensemble : alors qu’en général, les données sont stockées structurées puis extraites pour peupler du code métier puis affichées par des templates élaborés, dans ce cas particulier, les données sont directement stockées de manières dénormalisées, directement prêtes être affichées par des templates simplistes. Le code métier est en amont et sert pré-calculé les données qui sont stockées en base.\n\nIl est ainsi possible de minimiser drastiquement la quantité de code critique. Et cela ouvre beaucoup de portes : peu de code = peu de maintenance, aucun framework nécessaire, aucun cache nécessaire, etc.\n\nC’était une présentation assez polémique mais particulièrement intéressante (ce qui n’était pas l’avis de l’audience, semblerait-il) et rafraîchissante car elle permet de sortir des cas standards du monde du web. Entre nous, tous ces principes étaient déjen vogue dans le développement des jeux vidéo dans les années 90 : nous devions constamment contourner la limitation du matériel (les optimisations étaient au cycle processeur près).\n\nBreaking 1000ms Mobile Barrier\n\nIlya Grigorik (Google)\n\n\n\nComment arriver afficher sa page web sans dépasser la barrière de 1000 ms ! Un dur challenge dont les épreuves sont détaillés par Ilya.\n\n\n\nDes problèmatiques de latence sur le “Touch” mobile, sur les communications 3G/4G, du fonctionnement TCP, du critical rendering path au niveau CSS et JS, mod_page_speed et ngx_page_speed, ainsi que des évolutions venir sur Page Speed Insights, c’est un panel ultra complet de la WebPerf qui été couvert sur cette heure ultra dense, mais oh combien indispensable. C’est donc, comment souvent avec Ilya Grigorik, un must read absolu pour ceux que la Performance Front-End et Mobile, ainsi que la latence, passionne.\n\nLes slides sont ici https://docs.google.com/presentation/d/1wAxB5DPN-rcelwbGO6lCOus_S1rP24LMqA8m1eXEDRo/present#slide=id.p19\n\nLive Sketching !\n\nAvant de conclure, petit hommage Natalia Talkowska , qui, sur chaque conférence, réalisait un live sketching d’une qualité incroyable\n\n@Natalka_Design #livesketching is back with @allspaw @souders and @courtneynash opening up #velocityconf, let&amp;#39;s go! pic.twitter.com/FYBQIVk8tr&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @psd at #velocityconf as first #keynote! pic.twitter.com/9zAMXJZNWW&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @keynotesystems at #velocityconf pic.twitter.com/S8XYaNFKzU&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @LMAX at #velocityconf pic.twitter.com/UYLYDTSuPO&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @kenzenhofer at #velocityconf pic.twitter.com/l2ndwj8V1G&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nMust follow: @NatiTal: #livesketching @psd at #velocityconf as first #keynote! pic.twitter.com/W8xTTjC581 #Awesomeness&amp;mdash; Mike Hendrickson (@mikehatora) November 14, 2013\n\n\n#livesketching @triblondon at #velocityconf pic.twitter.com/VTF2gZFEsH&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @thedevmgr at #velocityconf pic.twitter.com/iaaZgRxVwN&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nNot a bad likeness! “@NatiTal: #livesketching @triblondon at #velocityconf pic.twitter.com/Pq4NiAPEW7”&amp;mdash; Andrew Betts (@triblondon) November 14, 2013\n\n\n#livesketching @igrigorik at #velocityconf pic.twitter.com/5rBPelS9an&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @edgecast at #velocityconf last #keynote pic.twitter.com/rZ5Pa1MvhQ&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nConclusion :\n\nC’est complètement lessivé que nous sortons de cette journée, avec une quantité d’idées / projets tester incroyable.\n\nVous pouvez retrouver le compte rendu de la première journée ainsi que de la dernière sur notre Blog.\n\nN’hésites pas donner vos retours (positifs ou négatifs en commentaire). Merci :-)\n\n© des photos : Flickr officiel O’Reilly\n\nCR rédigé par Baptiste, Denis Roussel et Kenny Dits\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 1",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-1.html",
    "date"     : "November 14, 2013",
    "excerpt"  : "Introduction :\n\n\n\nNous voici de retour à Londres pour la troisième édition de la Vélocity Europe, qui se déroule, pour la deuxième fois à Londres (la précédente était à Berlin).\n\nPour rappel, la Vélocity est la conférence autour de la performance ...",
  "content"  : "Introduction :\n\n\n\nNous voici de retour à Londres pour la troisième édition de la Vélocity Europe, qui se déroule, pour la deuxième fois à Londres (la précédente était à Berlin).\n\nPour rappel, la Vélocity est la conférence autour de la performance web. Qu’elle soit Front-End, Back-End, Dév ou Ops. C’est l’événement de l’année à ne pas manquer en Europe, ou aux US (ou Chine) pour les plus chanceux\n\nCette première journée (ayant eu lieu le 13 novembre 2013) est axée sur le signe des “Tutorials”. De looongues conférences de 90 minutes dont voici le compte rendu écrit à 6 mains.\n\nLa conférence “classique” commence le 14 et se déroulera sur deux journées.\n\nGone in 60 frames per second\n\nAddy Osmani (Google Chrome) @addyosmani\n\n\n\nAddy est une figure incontournable du web. Créateur de TodoMVC, Lead dév de Yeoman et travail dans la Google Chrome Team sur les outils à destination des développeurs autour du navigateur.\n\nAprès la génération du code html par les serveurs et le transfert de ce code par les réseaux, le rendu graphique de la page par le navigateur est le dernier évènement significatif du chargement de la page lors de la consultation d’un site par un client.\n\nVoici donc un résumé des bonnes pratiques permettant d’obtenir un meilleur framerate (nombre de rafraîchissement de la page par seconde) et ainsi une meilleure fluidité lors de la navigation :\n\n\n  disposer des images à la bonne taille pour éviter les redimenssionnements à la volée,\n  limiter les handlers sur l’événement onScroll(),\n  limiter tous les éléments ‘fixed’ car cela force le navigateur à recalculer constamment la zone affichée (ou utiliser l’astuce translateZ(0)),\n  \n    limiter les directives CSS qui nécessites un calcul supplémentaire (lorsque tout est déjà affiché) :\n  \n  les ombres,\n  les flous,\n  et les dégradés : (Bootstrap a supprimé tous les dégradés sur ses boutons : +100% de rapidité l’affichage).\n\n\nEnsuite, il reste quelques conseils plus généraux :\n\n\n  Il faut se souvenir que les performances des téléphones ne sont pas celles des PC,\n  un framerate de 60 fps est parfait (c’est dû au matériel), mais un framerate de 30 fps peut aussi être suffisant pour peu qu’il soit constant,\n\n\nEnfin, comme souvent, tous les outils pour comprendre et améliorer le rendu graphique de ses pages web sont disponible dans tous les navigateurs. Dans Chrome, il suffit d’aller dans la section “Frames” de l’onglet “Timeline” des DevTools.\n\nLes slides sont disponible ici : https://speakerdeck.com/addyosmani/velocityconf-rendering-performance-case-studies\n\nLa présentation de la conférence par l’auteur lui-même : https://addyosmani.com/blog/making-a-site-jank-free/\n\n\n\n\n\nVidéo de la même conférence (donnée à la Smashing Conf 2013)\n\nBring the noise : Making effective use of a quarter million metrics\n\nJon Cowie (Etsy) @jonlives\n\n\n\nJon est “Ops Engineer” chez Etsy (Dont le VP, John Allspaw, co-organise avec Steve Souders, la Vélocity).\n\nQuelques données sur Etsy :\n\n\n  Ils font du déploiement continu\n  1.5 milliards de pages vues\n  250 contributeurs (tout le monde déploie du code, même les chiens)\n  ils utilisent Deployinator pour déployer leur code avec un unique “bouton”, et schemanator pour les migrations SQL\n  60 déploiements par jour / 8 commit par deploiement\n  ¼ millions de métriques !\n\n\n\n  “We optimize for quick recovery by anticipating problems instead of fearing human error” John Cowie\n\n\n\n  “Can’t Fix what you don’t measure” W. Edwards Deming\n\n\nLeurs outils pour le monitoring :\n\n\n  \n    Not homemade :\n  \n  Ganglia\n  Graphite\n  Nagios\n  \n    Homemade :\n  \n  StatsD : Simple Daemon for easy stats integration\n  Supergrep : Real time log streamer\n  Skyline : A real time anomaly detection system\n  Oculus : A metric correlation component\n\n\n\n  “Not All things that break throw errors” Oscar Wilde\n\n\n\n  “If it moves, graph it ! If it doesn’t move, graph it anyway” Jon Cowie\n\n\nLa présentation s’axe ensuite plus particulièrement sur la stack “Kale”, qui englobe deux outils que l’on va détailler : Skyline et Oculus. Voir l’article sur le blog technique de Etsy https://codeascraft.com/2013/06/11/introducing-kale/\n\nL’objectif de Skyline, est de détecter les comportements anormaux (gros pics par exemple), avec pour principal challenge, la récupération des données (via le “relay agent” de Graphite, ils envoient en continue les données dans Redis via redis.append() ), le stockage de 250 000 métriques (dans Redis) au format MessagePack. Oculus quand lui permet de corréler les métriques, en utilisant les données brutes de l’api de Graphite, car il est bien plus efficace de comparer des chiffres, que des images …\n\nIl n’y a pas un mais huit algorithmes de détections d’anomalies qui sont utilisés dans un vote à majorité, déterminant ainsi si l’anomalie est avérée (parmi ceux ci, OLS, Grubb’s test, l’histogramme bining etc…). La détection se fait sur une fenêtre d’une heure et une seconde de 24 heures. Skyline souffre encore de quelques faiblesses: l’absence de prise en compte de la saisonnalité, les pics qui peuvent en cacher d’autres plus faibles, le postulat pas toujours vrai que les données sont normalement distribuées et les corrélations négatives.\n\nIls comparent donc la distance euclidienne (slide 99), en gérant aussi le décalage temporel (dynamic time warping / DTW) (voir slide 100).\n\nLa partie la plus intéressante est la simplification d’une métrique temporelle, en la normalisant sur une courbe échelle réduite (de 0 à 25), et en la transformant en une chaine textuelle comportant cinq valeurs :\n\n\n  sharpdecrement\n  decrement\n  flat\n  increment\n  sharpincrement\n\n\nEt ceci en fonction de la valeur en cours par rapport à la valeur précedente.\n\nIls poussent toutes ces métriques normalisées dans Elastic Search dans un champ non tokenisé en réalisant des recherches de phrases afin de corréler les métriques ayant le même pattern et en scorant via un plugin codé par leurs soins (incluant une version “rapide” du DTW).\n\nUne fois les métriques corrélées affichées, il est possible de sauvegarder un snapshot de ces dernières et d’inclure des commentaires dans une “collection”. Cela permet notamment de construire une base de données de connaissance sur les incidents ou les comportements anormaux mais explicables.\n\nSkyline est visible par tous dans leur bureaux, sur l’un des 6 écrans de dashboards, devant lesquels on peut notamment lire le nombre de requêtes HTTP par seconde, le top 10 des pages, les temps de générations et d’affichage etc…\n\nLes slides sont disponible ici : https://www.slideshare.net/jonlives/bring-the-noise\n\n\n\nResponsive images Technique and Beyond\n\nYoav Weiss (WL Square) @yoavweiss\n\n\n\nYoav est un spécialiste de la WebPerf et travaille sur les problématiques des images liées au Responsive Web Design. Il est aussi Technical Lead au RICG (Responsive images community Group)\n\nLe principal problème des images responsive, c’est de charger l’image correctement dimensionnée par rapport à une page, de manière efficace.\n\n72% des sites RWD servent les mêmes ressources entre les résolutions petites et grandes …\n\nOn peut économiser 72% en taille d’image en compressant correctement (voir https://timkadlec.com/2013/06/why-we-need-responsive-images/).\n\nYoav a développé un outil utilisant PhantomJs, permettant de mesurer la différence entre les images chargées, et celle qui seraient correctement dimensionnées : Sizer Soze\n\nOn aborde ensuite les deux cas principaux gênant :\n\n\n  Servir une dimension différentes de l’image à différents support. (et les Retina uniquement aux devices le supportant)\n  et le “Art direction”, avoir une image qui correspond au layout\n\n\nAinsi que l’intérêt du Pre-loader, souvent peu connu. Beaucoup plus d’infos sur cet article d’Andy Davies (https://andydavies.me/blog/2013/10/22/how-the-browser-pre-loader-makes-pages-load-faster/)\n\nYoav parcours ensuite toutes les techniques des images responsive avec avantages/inconvénients et exemple pour chacune, que vous pouvez retrouvez dès la slide 57 de la présentation ci après : https://yoavweiss.github.io/velocity-eu-13-presentation/#/\n\nL’étude et les retours sont extrêmement complet, et immanquable, si vous travaillez ou allez travaillez sur le sujet. Il aborde aussi une approche en cours d’étude, qui verra peut être le jour prochainement (Responsive Image Container).\n\nPerformance Analysis of JVM components for non-specialists\n\nBen Evans (JClarity) @kittylyst\n\n\n\nLa performance et la complexité des applications qui fonctionnent sur la JVM ont suivi l’évolution de la loi de Moore. Malgré que nous ayons gagné de la puissance et des transistors, notre code s’est complexifié d’année en année et d’autant plus avec le boom d’Internet.\n\nLe tuning de la JVM est indispensable pour avoir une application performante et doit se faire de façon rigoureuse et scientifique, il faut comprendre, mesurer, tester, vérifier et répéter ce processus jusqu’ce que l’on considère la performance comme bonne.\n\nBen a ensuite détaillé l’anatomie de la JVM, les spécificités du langage Java, les “mid 90’s decisions design” qui ont été faites, comment est géré l’allocation mémoire, la heap, et le fonctionnement du garbage collector (mark and sweep, stop the world). La durée du “stop the world” est ridicule comparé aux temps de latence réseau, ceux engendrés par l’hyperviseur etc…\n\nIl a présenté quelques optimisations indispensables selon lui, et a insisté sur le fait que l’optimisation prématurée pouvait être la source de bien des soucis coté code.\n\nTuning Network Performance to Eleven\n\nIlya Grigorik (Google) @igrigorik\n\n\n\nAKA comment condenser un livre dans un tutorial d’1H30. Exercice encore plus difficile lorsqu’il faut résumer le résumé d’un livre aussi dense et complet. Ilya en tant que spécialiste de la webperf a examiné les mécanismes de la latence et de la bande passante, le fonctionnement du protocole TCP, la gestion de congestion, les problèmes structurels de HTTP 1.0 et HTTP 1.1, l’impact de TLS (le chiffrement) sur les performances. Il a donné ses recommandations pour optimiser TCP et bien utilisé TLS.\n\n“bandwidth + latence =~ performance”\n\n\n  “Video streaming is bandwidth limited, web browsing is latency limited” Ilya Grigorik\n\n\nIl a ensuite expliqué comment fonctionne le réseau radio 2G/3G/4G et les contraintes que ces architectures exercent sur les temps de chargement et la durée de vie des batteries pour les appareils mobiles.\n\nLe tutorial s’est achevé sur les défauts de HTTP 1.1 et les nouveautés (nombreuses et sexys) d’HTTP 2.0. Ce fut extrêmement plaisant d’assister à cette présentation, tant Ilya est pointu techniquement, précis et didactique dans ses démonstrations. Le livre est un MUST-READ !\n\nIl est d’ailleurs disponible gratuitement ici : https://chimera.labs.oreilly.com/books/1230000000545\n\nLes slides sont disponible ici\n\nBe Mean to your code with Gauntlt and the Rugged Way\n\nJames wickett (Mentor Graphics) @wickett\n\n\n\nCette présentation fut le seul et unique vrai “Workshop” du jour, dans le sens où une machine virtuelle (monter avec Vagrant) était fournie pour réaliser l’atelier au fur et mesure de la présentation sur sa machine.\n\nGauntlt est un framework autour de la sécurité, qui fournie des hooks pour de nombreux outils d’attaques (Xss, Sql injection etc …).\n\nAprès une introduction un peu longue autour de la place de la “sécurité” aux seins de nos services.\n\nL’approche de Gauntlt est basée sur le “Rugged Manifesto”\n\nGauntlt permet donc d’automatiser au sein de son système d’intégration continue, des tests autour de la sécurité de son applicatif et de son infra, basés sur Cucumber, utilisant le langage Gherkin (que certains connaissent peut être mieux dans le monde php via Behat), et interfaçant des outils tels que :\n\n\n  Garmr\n  Nmap\n  Arachni\n  Sqlmap\n  …\n\n\nSi vous voulez tester l’outil, qui à l’air très prometteur, vous pouvez suivre ce tutoriel : https://bit.ly/gauntlt-demo-instructions qui vous fourni la Virtual Box, les consignes d’installations, et les exemples ayant été réalisés pendant la conférence, ainsi qu’une application de test en Ruby Railsgoat pour servir de cible à vos tests.\n\nLes slides sont disponible ici\n\n\n\nHands-on Web Performance Optimization Workshop\n\nAndy Davies (Asteno) @andydavies , Tobias Baldauf (Freelancer) @tbaldauf\n\n\n\nDernière session de la journée, avec Andy et Tobias, sur un workshop axé Performance Web.\n\nOn commence par une présentation général d’un outil qu’on ne devrait plus présenter : WebPageTest, l’outil principal pour les problématiques de performances front-end.\n\nAndy aborde ensuite quelques autres outils :\n\n\n  PhantomJs (un headless browser)\n  Simple Website Speed Test\n  et surtout Phantomas, un module PhantomJs pour collecter les métriques de Webperf.\n  le wrapper Node.Js pour WebPageTest de Marcel Duran\n  SiteSpeed.io pour monitorer toutes les pages de son site, basé notamment sur Yslow\n  HttpArchive, l’excellent service de Steve Souders qui tracke le web avec une multitude de stats intéressante, que vous pouvez d’ailleurs installer pour une instance privée afin de tracker vos sites : https://bbinto.wordpress.com/2013/03/25/setup-your-own-http-archive-to-track-and-query-your-site-trends/ \\o/\n\n\nLa suite de la conférence consister a analyser en live certains sites dont quelques uns assez hilarant au niveau performance :\n\n\n  Dailymail.co.uk avec ces +de 800 requêtes HTTP et 7 mo !\n  Wildbit.com qui consomme un CPU énorme cause de l’animation sur le logo qu’on ne voit quasiment pas :)\n\n\nLes slides :\n\n\n\nConclusion :\n\nBonne première journée avec ce format “Tutorials” un peu trop touffu (90 minutes par conférence …). Déjà des tonnes d’idées qui ressortent, on a hâte de voir la suite.\n\nRetrouvez les autres CR :\n\n\n  Compte rendu du jour 2 \n  Compte rendu du jour 3 \n\n\n© des photos : Flickr officiel O’Reilly\n\nCR rédigé par Baptiste, Denis Roussel et Kenny Dits\n"
} ,
  
  {
    "title"    : "Tester fonctionnellement une API REST",
    "category" : "",
    "tags"     : " qualite, symfony, atoum, tests fonctionnels",
    "url"      : "/2013/10/tester-fonctionnellement-une-api-rest-symfony-doctrine-atoum",
    "date"     : "October 14, 2013",
    "excerpt"  : "Un des enjeux des tests fonctionnels est de pouvoir être joués dans un environnement complètement indépendant, dissocié de l’environnement de production, afin de ne pas être tributaires de données versatiles qui pourraient impacter leur résultat. ...",
  "content"  : "Un des enjeux des tests fonctionnels est de pouvoir être joués dans un environnement complètement indépendant, dissocié de l’environnement de production, afin de ne pas être tributaires de données versatiles qui pourraient impacter leur résultat. Il faut, cependant, que cet environnement soit techniquement similaire à celui de production pour que les tests aient une réelle validité fonctionnelle.\n\nAvec la Team Cytron, nous sommes tombés face à cette problématique lorsque nous avons voulu tester fonctionnellement un service agnostique de contenu mettant à disposition une API REST et utilisant Symfony2, MySQL, Doctrine et atoum.\n\nMonter un serveur de données dédié aux tests\n\nDans le cas d’une application utilisant MySQL, on pense alors monter un serveur applicatif de test relié à une base de données de test. Plusieurs problèmes peuvent alors découler d’un tel système :\n\n\n  il faut être en mesure de pouvoir mettre en œuvre un serveur MySQL dédié uniquement aux tests,\n  mais surtout cette architecture n’est pas exploitable pour exécuter des tests de manière concurrentielle (ce qui pose problème pour l’intégration continue). En effet, des collisions apparaîtraient en base de données et le résultat des tests ne seraient plus exploitables.\n\n\nMocker Doctrine\n\nNotre seconde réaction a été de vouloir mocker Doctrine pour devenir indépendant de MySQL. Lourde tâche.\n\nTant bien que mal, nous sommes arrivés à un résultat plutôt satisfaisant car notre API réalise des opérations simples : ajout, modification, suppression et consultation avec un filtrage élémentaire.\n\nLa première chose à faire est de s’assurer que notre serveur de test n’accède pas aux données de production dans MySQL en changeant la configuration Doctrine dans le fichier config_test.yml.\n\n\n\nEnsuite, nous avons créé une classe abstraite dont héritent toutes nos classes de test, et qui permet d’initialiser le mock de Doctrine.\n\nAutant vous dire que le développement de cette classe a été fastidieux car incrémental : chaque nouveau besoin de manipulation de données dans nos tests, il a fallu modifier le mock pour prendre en compte des méthodes ou des fonctionnalités de méthodes qui n’avaient pas été encore mockées (comme le filtrage par critères dans la fonction findBy).\n\nLes possibilités de ce mock reste limitées. Nous sommes, par exemple, tombés sur le cas où deux managers de données en relation (des recettes et leurs ingrédients) dépendaient d’un même EntityManager Doctrine : tel que nous l’avons développé, le mock ne sait pas gérer cette situation et engendre des erreurs à l’exécution. Il aurait fallu refactoriser le code pour parvenir à nos fins et passer encore plus de temps sur ce projet… et nous n’en avions pas beaucoup !\n\nAutre problème : nous utilisons des fonctionnalités de la librairie Gedmo/DoctrineExtensions pour la gestion automatique des dates de création et de modification. Évidemment, elles ne sont pas opérationnelles avec notre mock et nous aurions encore dû développer pour faire passer nos tests.\n\nUtiliser les transactions de Doctrine\n\nIl a donc fallu nous rendre l’évidence : cette solution ne correspondait pas à nos besoins ! Nous avons alors émis l’hypothèse d’une alternative qui nous permettrait peut-être de nous passer d’une config spécifique MySQL pour nos tests : l’utilisation des transactions via Doctrine.\n\nAu début de chaque test, nous aurions ouvert une transaction mais qui n’aurait jamais été commitée par la suite, évitant toute interaction avec la base de données de production. Mais avec cette solution, dangereuse à mettre en place et à maintenir, nous aurions couru le risque de modifier des données de production.\n\nRemplacer MySQL par un autre SGBD uniquement pour les tests\n\nFinalement, nous sommes partis sur une autre piste, celle qui fait actuellement tourner nos tests fonctionnels sur ce projet. Nous utilisons SQLite dans notre environnement de test la place de MySQL. Ce SGBD est très léger et simple mettre en œuvre : pas besoin d’une installation sur un serveur dédié, il suffit simplement d’activer une extension de PHP. SQLite se base sur des fichiers physiques pour gérer le stockage des données. Ainsi, chaque build de test peut avoir ses propres fichiers de BDD dans son répertoire évitant toute collision dans le cas de tests concurrentiels.\n\nNous avons donc configuré Doctrine, pour qu’il utilise SQLite lors de son exécution en environnement de test en modifiant le config_test.yml\n\n\n\nComme pour le mock de Doctrine, nous avons mis en place une classe abstraite qui permet de gérer la réinitialisation de la base pour chaque test.\n\nNous pouvons donc maintenant tester unitairement et fonctionnellement notre API REST développée en PHP l’aide de Symfony2 et Doctrine. Et nous ne nous en privons pas : notre API est couverte par bientôt 5.000 assertions.\n\nGénération des données de test\n\nAprès avoir trouvé une solution pour l’accès à la structure de données en environnement de test, nous nous sommes penchés sur la question du contenu de ces données de tests. Pas longtemps.\n\nNotre service REST permettant des opérations CRUD, nous partons pour chaque test d’un contenu vide que nous remplissons à l’aide de notre propre service. Cela permet de tester beaucoup plus de cas d’utilisation. Mais surtout cela permet aussi de tester des cas plus réels, plus proches de son utilisation par nos clients.\n"
} ,
  
  {
    "title"    : "Distribuez votre vidéo partout avec 3 euros en poche et devenez millionaire. Ou presque.",
    "category" : "",
    "tags"     : " lft, video",
    "url"      : "/2013/10/distribuez-votre-video-partout-avec-3-euros-en-poche-et-devenez-millionaire-ou-presque.html",
    "date"     : "October 9, 2013",
    "excerpt"  : "“Comment gagner des millions, sans sortir de chez vous, en robe de chambre, en distribuant des vidéos de chats sur les internets, grâce à ffmeg, h264, dash, tous pleins de buzz word, justin bieber” (Merci ! Toute l’équipe SEO).\n\nUne présentation d...",
  "content"  : "“Comment gagner des millions, sans sortir de chez vous, en robe de chambre, en distribuant des vidéos de chats sur les internets, grâce à ffmeg, h264, dash, tous pleins de buzz word, justin bieber” (Merci ! Toute l’équipe SEO).\n\nUne présentation de Ludovic Bostral, notre ex valeureux responsable R&amp;amp;D en charge - jusqu’il y a peu de temps - de la fabrication de toutes nos vidéos et du SI associé.\n\nSi vous vous posez des questions ce sujet, n’hésitez pas à venir lui faire un petit coucou virtuel, ou sur Nantes. Ca marche aussi pour discuter zombie ou nanar. Ou mieux, un nanar avec des zombies !\n\nRetrouvez Ludovic sur son site : https://digibos.com.\n\n"
} ,
  
  {
    "title"    : "Le NoSQL, Focus sur MongoDB par Cédric Derue (Altran)",
    "category" : "",
    "tags"     : " lft, nosql, mongodb, video",
    "url"      : "/le-nosql-focus-sur-mongodb-par-cedric-derue-altran",
    "date"     : "October 8, 2013",
    "excerpt"  : "Porte-étandard des bases de données NoSQL de type document, MongoDB nous a été présenté cet été par Cédric Derue (@cderue) , de la société Altran, lors de nos conférences internes.\n\nDans cette présentation d’environ une heure, il aborde un tour d’...",
  "content"  : "Porte-étandard des bases de données NoSQL de type document, MongoDB nous a été présenté cet été par Cédric Derue (@cderue) , de la société Altran, lors de nos conférences internes.\n\nDans cette présentation d’environ une heure, il aborde un tour d’horizon des différentes catégories de bases de données NoSQL, pour s’attacher ensuite sur un focus assez complet de MongoDB, agrémenté de quelques démonstrations.\n\nMerci à Altran et Cédric pour le partage de cette présentation.\n\nVous pouvez aussi retrouver d’autres sessions de nos Last Friday Talk :\n\n\n  Introduction Drupal par Claire Roubey (Clever Age)\n  Redis on Fire\n  La POO Canada Dry\n\n"
} ,
  
  {
    "title"    : "Vigo, le fléau des Carpates",
    "category" : "",
    "tags"     : " outil, qualite, javascript, tests fonctionnels",
    "url"      : "/vigo-le-fleau-des-carpates-la-tristesse-de-moldavie",
    "date"     : "August 13, 2013",
    "excerpt"  : "CasperJS permet d’écrire des scripts javascript qui vont automatiser des tests fonctionnels de pages web. Il exécute ces tests dans une instance de PhantomJS qui est un navigateur scriptable et sans interface graphique (“Headless” dit-on dans le m...",
  "content"  : "CasperJS permet d’écrire des scripts javascript qui vont automatiser des tests fonctionnels de pages web. Il exécute ces tests dans une instance de PhantomJS qui est un navigateur scriptable et sans interface graphique (“Headless” dit-on dans le milieu).\n\nAfin de mieux structurer nos tests, de faciliter leur écriture et de pouvoir les lancer avec une commande unique, nous avons créé VigoJS, une surcouche pour CasperJS.\n\nFonctionnalités\n\nToutes les fonctionnalités de base de CasperJS sont accessibles. Nous y avons simplement ajouté un mécanisme de configuration contenant plusieurs paramètres de base dont l’URL de test par défaut, l’authentification HTTP éventuelle ou encore la taille de l’écran virtuel. Il est également possible de spécifier des environnements (dev, preprod, prod…) pour différencier les comportements de certains tests. Ainsi, en fonction de l’environnement demandé dans la ligne de commande, les tests peuvent être joués sur des URL différentes avec la bonne authentification HTTP.\n\nQuelques fonctions utilitaires sont aussi disponibles pour réaliser rapidement certaines vérifications récurrentes et ainsi faciliter le développement des tests. On peut, par exemple, rechercher aisément la présence d’erreurs ou warnings PHP dans une page. Il est aussi possible de faire un retry lorsqu’un test a échoué afin d’être certain que ce n’est pas une erreur du type “MySql server has gone away” qui peut se produire de temps en temps sur les serveurs de tests. Par ailleurs, quand un test échoue, VigoJS exporte une capture d’écran qui s’avère très pratique pour comprendre ce qu’il s’est passé !\n\nTous les paramètres ajoutés à la ligne de commande et dans la configuration sont injectés et accessibles dans la classe de test. On garde, de cette manière, une certaine flexibilité. Cela peut permettre, par exemple, de découper les tests avec de la pagination :\n\n\n\n\n\nAffichage dans le terminal\n\nNous avons aussi amélioré l’affichage des résultats des tests. Il est ainsi possible de préciser pour chaque test : un titre et une description personnalisés afin de rendre les comptes-rendus plus compréhensible pour les utilisateurs. De même des commentaires utilisateurs peuvent être ajoutés plus simplement dans le déroulement des tests.\n\n\n\nIntégration continue\n\nCasperJS génère nativement des rapports xUnit. VigoJS intègre donc cette fonctionnalité pour être utilisé sur une plateforme d’intégration continue comme Jenkins. Il est aussi possible de modifier le paramètre classPath dans le fichier xUnit pour améliorer la lisibilité des résultats :\n\n\n\nLe chemin dans lequel est généré le rapport est configurable par l’option –buildPath (ou dans la configuration) :\n\n\n\nIl suffit ensuite de configurer le job Jenkins pour qu’il récupère le rapport de test dans ce dossier. Sans oublier de faire un job pour tester les Pull Requests de votre projet.\n\nVigoJS est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Introduction à la qualité logicielle avec Node.js",
    "category" : "",
    "tags"     : " nodejs, javascript, qualite",
    "url"      : "/introduction-qualite-logicielle-avec-node-js",
    "date"     : "August 12, 2013",
    "excerpt"  : "\n\n(Source : https://www.flickr.com/photos/dieselbug2007/414348333/)\n\nChez M6Web, nous avons récemment réecrit un de nos projets Node.js.\nLe monde Node.js évolue très rapidement et a fait d’énorme progrès dans le domaine de la qualité logicielle.\nN...",
  "content"  : "\n\n(Source : https://www.flickr.com/photos/dieselbug2007/414348333/)\n\nChez M6Web, nous avons récemment réecrit un de nos projets Node.js.\nLe monde Node.js évolue très rapidement et a fait d’énorme progrès dans le domaine de la qualité logicielle.\nNous avons donc decidé de monter en qualité sur nos projets Node.js en utilisant les derniers outils proposés par la communauté.\n\nPour cela, nous mesurons maintenant différentes métriques sur nos projets Node:\n\n\n  la qualité du code (checkstyle)\n  des tests unitaires et fonctionnels\n\n\net tout ceci est lancé par notre serveur d’intégration continue: Jenkins.\n\nTests unitaires\n\nPour tout ce qui est “tests”, nous avons choisi le très bon duo :\n\n\n  Mocha\n  Chai\n\n\n\n\nMocha c’est un “test-runner” javascript qui fonctionne aussi bien sur Node que dans un navigateur web. Plus simplement mocha est l’outil qui va contenir nos tests: il va exécuter les tests et afficher les résultats.\n\n\n\nChai est une librairie d’assertion assez complète, permettant plusieurs syntaxe :\n\n\n  assert.equal(foo, ‘raoul’);\n  foo.should.equal(‘raoul’);\n  expect(foo).to.equal(‘bar’);\n\n\nCes deux outils fonctionnent aussi bien pour tester vos javascripts Node que front.\n\nCe duo permet une écriture de test simple et très lisible, dont voici un exemple :\n\n\n\nTests fonctionnels\n\nPour les tests fonctionnels, nous avons choisi d’utiliser Supertest, un package Node.js qui permet de simplifier l’écriture de requête HTTP (une surcouche au package http disponible dans Node.js).\n\nCi-dessous, un exemple de tests fonctionnels :\n\n\n\nCheckstyle\n\nEn javascript, on peut aussi écrire du code propre et respecter des conventions de codage.\n Afin de vérifier que notre code respecte les standards en vigueur, nous utilisons JsHint.\n\nIntégration continue\n\nToutes ces métriques sont récoltées grâce à Jenkins-CI à l’aide du fichier Ant suivant :\n\n\n\n\n\nLe résultat de l’intégration continue dans jenkins.\n\nConclusion\n\nNode.js propose des outils très performants pour la qualité logicielle, et écrire des tests avec le duo “Mocha + Chai” devient vite quelque chose de simple. Et même les développeurs les plus réfractaires aux tests devraient apprécier.\n\nN’hésitez pas à commenter cet article et à indiquez la solution que vous utilisez pour vos projets Node.\n\n"
} ,
  
  {
    "title"    : "Introduction à Drupal par Claire Roubey (Clever Age)",
    "category" : "",
    "tags"     : " lft, drupal, video",
    "url"      : "/introduction-%C3%A0-drupal-par-claire-roubey-clever-age",
    "date"     : "July 19, 2013",
    "excerpt"  : "Drupal, le CMS très très connu mais que nous on connait pas ! A notre demande Clever Age, par l’intermédiaire de Claire Roubey, est venue nous présenter cet outil lors d’un de nos fameux Last Friday Talk.\n\nMalheureusement, la vidéo est coupée à en...",
  "content"  : "Drupal, le CMS très très connu mais que nous on connait pas ! A notre demande Clever Age, par l’intermédiaire de Claire Roubey, est venue nous présenter cet outil lors d’un de nos fameux Last Friday Talk.\n\nMalheureusement, la vidéo est coupée à environ la moitié de sa durée (fort dommage car les questions étaient très intéressantes). Les slides sont toutefois disponibles : https://fr.slideshare.net/claire_/drupal-m6-web310513.\n\nUn énorme merci Clever Age et Claire !\n"
} ,
  
  {
    "title"    : "Lâche moi la branch !",
    "category" : "",
    "tags"     : " qualite, jenkins, github",
    "url"      : "/lache-moi-la-branch",
    "date"     : "July 15, 2013",
    "excerpt"  : "Test continu des Pull Requests\n\nMaintenant que nous utilisons GitHub Enterprise chez M6Web, nous avons la joie de pouvoir utiliser les Pull Requests de façon abusive. Mais leur puissance n’est maximale que lorsqu’elles peuvent être testées individ...",
  "content"  : "Test continu des Pull Requests\n\nMaintenant que nous utilisons GitHub Enterprise chez M6Web, nous avons la joie de pouvoir utiliser les Pull Requests de façon abusive. Mais leur puissance n’est maximale que lorsqu’elles peuvent être testées individuellement avant d’être mergées sur le master.\n\n\n\nPour ce faire, nous avons utilisé le plugin GitHub Pull Request Builder de Jenkins, qui après une configuration assez simple, nous a permis de créer un job qui lance automatiquement un build lorsqu’une Pull Request est modifiée. Ce build se positionne sur la branch pointée par la Pull Request et exécute les tests.\n\n\n\nIl est donc nécessaire de créer un job dédié au test des Pull Requests pour chaque projet dont nous souhaitons voir les Pull Request automatiquement testées. Ça peut paraître évident, mais lorsqu’on a plus de 200 repositories, c’est tout de suite moins trivial.\n\nConfiguration du plugin\n\nLe fonctionnement par défaut du plugin GitHub Pull Request Builder est assez restrictif. Il nécessite qu’un contributeur ajoute un commentaire sur la Pull Request en demandant un test puis qu’un admin (parmi une liste à configurer) réponde avec un deuxième commentaire acceptant de lancer les tests (le tout avec des phrases types configurables). C’est uniquement ensuite que Jenkins lancera un build.\n\nOr dans notre contexte d’entreprise, nous souhaitons que l’automatisation soit totale, comme dans Travis : chaque modification d’une Pull Request lance l’ensemble des tests. Pour arriver ce fonctionnement, il suffit de cocher “Build every pull request automatically without asking (Dangerous!)” dans la section “Avancée” des options de lancement de build par “Github pull requests builder”.\n\nTest continu du master\n\nNous essayons tant que possible de suivre le workflow de déploiement de GitHub : on développe une fonctionnalité par branch, on fait une Pull Request sur le master et on ne merge que lorsque tout le monde est d’accord et que les tests sont passés. Cela nous permet de garder le master toujours déployable.\n\nNous avons donc, pour chaque projet, un second job qui lance l’ensemble des tests lors de chaque modification du master. Cela n’arrive normalement que lors du merge des nouvelles fonctionnalités contenues dans les Pull Requests, qui ont déjà été individuellement testées. Nous sommes donc sereins sur l’intégration croisée de toutes les nouvelles fonctionnalités sur le master.\n\nDéploiement\n\nAvant de déployer à l’aide de Capistrano, nous vérifions que les tests passent (résultat de l’intégration continue + lancement manuel des tests). Le manque d’automatisation concernant ces mises en production fait apparaitre une faille assez large. Pour la résorber, nous pourrions par exemple accepter le déploiement d’un service, uniquement si ses tests sont passés et si aucun autre n’est en cours ou en attente. Même si cela ajoute une dépendance aux serveurs d’intégration continue, cela sécurise les déploiements.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #5",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-5",
    "date"     : "July 12, 2013",
    "excerpt"  : "On est dredi, et on a encore pas mal de phrases chocs de nos développeurs, entendues dans nos locaux partager.\n\nEn voici une nouvelle sélection avec les Dev Facts #5\n\nFaille de sécu ?\n\n\n  C’est un peu complexe, c’est une back-office front !\n\n\nLa b...",
  "content"  : "On est dredi, et on a encore pas mal de phrases chocs de nos développeurs, entendues dans nos locaux partager.\n\nEn voici une nouvelle sélection avec les Dev Facts #5\n\nFaille de sécu ?\n\n\n  C’est un peu complexe, c’est une back-office front !\n\n\nLa boucle est bouclée\n\n\n  Il finit là où il s’est arrêté\n\n\nEnvoi impossible\n\n\n  Mais tu n’étais pas en pièce jointe !\n\n\nC’est l’histoire d’une fille\n\n\n  C’est un peu comme “Un gars, une fille” mais sans le gars\n\n\nLa minute de 30 secondes\n\n\n  J’ai pris une année sabatique de 6 mois.\n\n\nL’année des deux mains\n\n\n  Ca dépend si c’est une année ambidextre !\n\n\nLes dents de la neige\n\n\n  \n    Vous avez entendu, une championne de snowboard est morte.\n    Elle s’est faite manger par un requin ?\n  \n\n\nLe vrai du faux\n\n\n  Ils ont trouvé une faille infaillible\n\n\nLe roi de la combine\n\n\n  \n    En France, un truc comme ca, t’en as pour 280 euros !\n    Et tu l’as eu a combien ?\n    280 euros.\n  \n\n\nA bientôt pour un prochain épisode\n\n"
} ,
  
  {
    "title"    : "Benchmarking WebSockets avec NodeJs",
    "category" : "",
    "tags"     : " nodejs, websockets, benchmark, open-source",
    "url"      : "/benchmarking-websockets-avec-nodejs",
    "date"     : "July 5, 2013",
    "excerpt"  : "Nous avons récemment eu à repenser une application Node.js de timeline temps réel, basée sur les WebSockets afin de tenir une charge plus élevée.\n\nL’application timeline\n\nFonctionnellement, l’application timeline est relativement simple: elle cons...",
  "content"  : "Nous avons récemment eu à repenser une application Node.js de timeline temps réel, basée sur les WebSockets afin de tenir une charge plus élevée.\n\nL’application timeline\n\nFonctionnellement, l’application timeline est relativement simple: elle consiste à afficher un flux de message publiés par des contributeurs en temps réel pour les internautes présent sur la page. Pour cela l’application se base sur socket.io pour la partie websocket, et supporte à peu près 15 000 connexions simultanées.\n\nAfin d’augmenter la capacité de l’application, nous avons décidé de la rendre scalable horizontalement. C’est dire, répartir la charge sur un nombre X de serveurs communiquant entre eux, par exemple, par le biais de Redis.\n\n\n\nPour cela socket.io propose un store redis qui permet aux différentes instances de communiquer entre elles. Malheureusement les performances de ce store sont plutôt désastreuses car le store que propose socket.io est beaucoup trop verbeux et écrit absolument tous les évènements que reçoit un serveur sur un seul channel redis. L’application devenait inutilisable autour de 8 000 connexions. Il était donc inenvisageable de l’utiliser en production.\n\nNous avons donc décidé rapidement de passer une autre solution que socket.io. Après pas mal de recherche nous avons fait notre choix sur Faye, une implémentation du protocole de Bayeux, bien documenté et proposant aussi d’utiliser redis comme “store”. Après test, cette solution s’est révélée bien plus performante que socket.io.\n\nTests de charge\n\nUne des problématiques rapidement rencontrée sur ce projet a été de tester la charge de notre application: comment simuler 15 000 connexions simultanées ?\n\nEn faisant le tour des solutions de benchmark de websocket (thor, …) ,nous n’avons pas trouvé la solution qui nous permettait de faire les tests que nous souhaitions. Siege, ab ne le propose pas encore,Gatling, Jmeter, Tsung ont des plugins web-socket mais l’utilisation et le reporting ne sont pas des plus clair.\n\nLa solution ?\n\nWebsocket-bench\n\nNous avons donc décidé de développer notre propre outil de benchmark de websocket (Socket.io ou Faye), au nom très original : websocket-bench.\n\nCet outil se base sur les clients Node que proposent Faye et Socket.io. Il peut être facilement étendu à l’aide de “generator” (module Node), afin de rajouter la logique de votre application. Par exemple dans le cas de notre application, en se connectant, un client doit envoyer un message au serveur pour valider la connexion.\n\nCi dessous un exemple de générateur qu’on a pu utiliser lors de nos tests de charge.\n\n\n\nCet outil, lancé sur des instances Amazon, nous a permis d’exécuter nos tests de charge.\n\nUn exemple : la commande ci dessous va lancer 25 000 connexions, à raison de 1000 connexions par seconde en utilisant le generateur “generator.js” :\n\n\n\n\n\nNombre de clients connectés sur Graphite\n\nAu delà de 25 000 connexions, l’instance Amazon (large) qui lançait les tests ne tenait plus. Une solution pour tester un nombre plus élevés de connexions serait d’utiliser plusieurs machine de tests, peut être à l’aide de bees with machin guns et ainsi d’utiliser plusieurs instances pour lancer les tirs de charge.\n\nBonnes pratique de test de charge\n\nLors de votre test de charge (et pour la prod), n’oubliez pas d’augmenter le nombre maximal de descripteurs de fichiers coté client ET coté injecteur (ulimit -n 256000 par exemple dans la conf de supervisor, et dans le terminal avant de lancer le benchmark).\n\nSurveillez votre conntrack (si firewall iptables), augmentez votre plage locale de port, et si vous êtes amenés à tester plus de 25K connexions, utilisez plusieurs machines et/ou plusieurs IP sources différentes.\n\nComment contribuer au projet ?\n\nN’hésitez pas à remonter d’éventuels bug via les issues ou à contribuer au projet l’aide de pull request github (https://github.com/BedrockStreaming/websocket-bench)\n\n"
} ,
  
  {
    "title"    : "Performances web et &quot;Disaster case&quot; sur applications mobile native",
    "category" : "",
    "tags"     : " webperf, mobile",
    "url"      : "/performances-web-disaster-case-applications-mobile-native",
    "date"     : "July 2, 2013",
    "excerpt"  : "La performance Web (ou grossièrement temps de chargement) est devenue aujourd’hui une problématique majeure dans tout développement Web.\n\nLes outils pour mesurer / comprendre sont plutôt reconnus désormais et arrivent a une certaine maturité. Il y...",
  "content"  : "La performance Web (ou grossièrement temps de chargement) est devenue aujourd’hui une problématique majeure dans tout développement Web.\n\nLes outils pour mesurer / comprendre sont plutôt reconnus désormais et arrivent a une certaine maturité. Il y a toutefois encore un créneau plutôt peu documenté (à mon goût) dans le domaine, celui permettant de mesurer les temps de chargement dans des applications mobiles natives (Android / iOs …)\n\nVoici un retour des méthodes que nous utilisons pour mesurer les performances (notamment de chargement) de nos applications natives et générer des Waterfall Charts, mais aussi sur la mise en place de tests “disaster case” en cas d’indisponibilité de services utilisés par l’application.\n\nPour les besoins de ce tutoriel, nous allons prendre comme configuration, un Mac, avec une application native sur un iPhone 4 (relié au même réseau Wi-Fi que le Mac), ainsi que la version d’essai du logiciel CharlesProxy installé. (Mais la configuration et procédure est la même sur un autre OS, ou un autre mobile, et fonctionne aussi pour tester des webapps ou sites mobiles)\n\nCharlesProxy\n\nNous allons donc utiliser le logiciel payant CharlesProxy, qui est un proxy HTTP ou Reverse Proxy permettant de capturer le traffic HTTP de son ordinateur. Il existe une version d’essai sans limite de 30 jours. Il y a peut être des alternatives libres, mais Charles étant plutôt une référence, c’est l’outil que nous utilisons.\n\nCommencez donc par aller sur le site et installez CharlesProxy.\n\nUne fois installé, lancez le, il devrait automatiquement commencer à capturer le trafic réseau.\n\nConnexion Wi-Fi et récupération IP\n\nLa deuxième étape consiste à connecter votre Ordinateur et votre Téléphone sur le même réseau Wi-Fi.\n\nRécupérons ensuite notre adresse Ip via les “Préférences Système”, section “Internet et sans fil” et icone “Réseau” sur votre configuration Wi-Fi :\n\n\n\nConfiguration du proxy sur son iPhone\n\nPassons ensuite sur le téléphone, dans vos préférences Wi-Fi.\n\nCliquez ensuite sur la flèche bleu à droite du nom de la connexion sur le paramétrage Wi-Fi de notre iPhone, et descendre tout en bas du paramétrage pour configurer manuellement notre proxy HTTP :\n\nConfigurez le proxy de cette connexion pour passer par le Proxy Charles, avec l’adresse IP récupérée plus haut, et le port par défaut de Charles 8888.\n\nUne fois la connexion lancée avec le Proxy activé et Charles bien lancé sur votre Mac, une popup d’activation devrait apparaitre :\n\n\n\nAller ensuite sur un site mobile via Safari pour vérifier que le trafic est bien capturé par votre Proxy.\n\nA ce stade, tout est prêt pour commencer les mesures.\n\nPrise de mesure avec Charles\n\nPour prendre une mesure avec Charles, allez dans le menu “Proxy”, décochez le “MAC OS X Proxy” afin de ne pas parasiter vos mesures, et nettoyez l’écran de Charles pour commencer une “session” propre.\n\n\n\nVous n’avez ensuite plus qu’a lancer une application pour mesurer la liste des requêtes HTTP nécessaire à son démarrage.\n\nDans la partie Structure, un clic sur un domaine vous donnera plus d’infos (nombre de requête, et détails de chacune) …\n\nSélectionnez toutes les requêtes, puis cliquez sur “Chart” sur la droite, pour obtenir un premier Waterfall (made in Charles)\n\n\n\nGénération de Waterfall (plus complet)\n\nToujours sous Charles, avec toutes les structures sélectionnées, Fichier / Export puis selectionner le format Http Archive (.har)\n\nNous allons ensuite utiliser l’outil harviewer, pour visualiser le waterfall sous une forme plus complète que dans Charles.\n\nRendez vous ici (avec Firefox, plutôt que Chrome dont le rendu est buggé sur cet outil) : https://www.softwareishard.com/har/viewer/\n\nDécochez la case “Validate data before processing?” pour être moins embêté par des problèmes de compatibilité surement liés à l’export de Charles.\n\n\n\nEnsuite, faites un Drag &amp;amp; Drop de votre fichier .har dans le textarea de HarViewer pour obtenir votre waterfall, très proche de l’onglet Réseau de Firebug ou Network de la console de Chrome.\n\nVous retrouvez donc pour chaque requête tous les élements classique, avec détail des réponses, code HTTP de retour, taille etc, et le tout sur une timeline très précise.\n\n\n\nThrottling\n\nPour le moment, nous avons donc testé notre application sur notre connexion Wi-Fi, cas plutôt idéal. Mais comment simuler une connexion 3g par exemple, peut être plus proche de la réalité des utilisateurs de l’applications ?\n\nPour cela, il vous suffit d’aller dans Charles, puis le menu “Proxy” et “Throttle Settings”.\n\nLa latence par défaut configurée est un peu élevée (600ms), mais vous pouvez la modifier et affiner vos tests pour se rapprocher de conditions plus réelles.\n\nEnsuite, toujours dans le menu “Proxy”, activé l’option “Throttle” et vous pourrez tester sur une connexion différente.\n\n\n\nDisaster Case ?\n\nComment savoir comment se comporte votre application si vos Webservices sont injoignables ? ou si l’un des services tiers que vous utilisez est down ? Comment trouver les SPOF (Single Point Of Failure) de vos apps ?\n\nToujours dans Charles, Allez dans “Tools”, puis “Map Remote”.\n\nIci, vous allez pouvoir rediriger les domaines de vos choix, vers un domaine de type Blackhole.\n\nC’est à dire que le domaine choisi réagira comme si votre serveur web était dans un état de mort cérébrale ! Pas celui où il rejette la connexion immédiatement (trop facile), celui où il végète sans arriver à acquitter la réponse (le fameux “en attente de https:// ….”)\n\nPour ce besoin, nous allons utiliser le Blackhole fourni par Patrick Meenan pour l’outil de mesure de performance web : WebPageTest : https://blackhole.webpagetest.org\n\n\n\nVous pouvez ensuite jouer avec les domaines, et regarder comment se comporte votre application dans le cas où l’un d’entre eux est inaccessible.\n\nSur notre iPhone 4 de test, on remarque d’ailleurs un timeout sur les requêtes de 75 secondes ! Imaginez le cas, où le développement et l’appel à ce service est synchrone ? 75 secondes de loading dans votre application avant de passer aux requêtes suivantes …\n\n\n\nVoilà, vous avez désormais une solution vous permettant de générer des Waterfall Charts pour vos apps natives, et de tester des conditions de mauvaises connexions, ou d’indisponibilité de service.\n\nSi vous avez d’autres méthodes, plus simples ou plus complètes, ou tout autre remarque sur cette article, n’hésitez pas à le faire dans les commentaires ci-dessous.\n\nMerci.\n\nP.s: pour complément, n’hésitez pas à creuser le blogpost de Steve Souders sur les waterfall mobile, qui utilise une méthode très différente avec tcpdump et pcapperf https://www.stevesouders.com/blog/2013/03/26/mobile-waterfalls/\n\n"
} ,
  
  {
    "title"    : "Coke, pour bien sniffer son code",
    "category" : "",
    "tags"     : " outil, qualite, php, open-source",
    "url"      : "/coke-pour-bien-sniffer-son-code",
    "date"     : "June 27, 2013",
    "excerpt"  : "Afin d’uniformiser nos développements, nous avons décidé de suivre des conventions de code. Les projets deviennent ainsi plus homogènes et la revue de code, comme la maintenance, s’en trouvent simplifiées. Comme la majorité de nos services sont en...",
  "content"  : "Afin d’uniformiser nos développements, nous avons décidé de suivre des conventions de code. Les projets deviennent ainsi plus homogènes et la revue de code, comme la maintenance, s’en trouvent simplifiées. Comme la majorité de nos services sont en PHP, nous utilisons PHP CodeSniffer.\n\nLe manque\n\nCependant, l’éventail des frameworks utilisés en interne (Symfony, ZF, homemade) ne nous permet pas d’employer une seule et même convention. De plus, l’organisation des projets est assez hétérogène (ex: les répertoires de test ne se nomment pas tous de la même manière). Nous avions donc besoin de pouvoir configurer spécifiquement PHP CodeSniffer pour chacun de nos projets.\n\nLe deal\n\nA la manière de Travis, nous avons opté pour la méthode dite “du fichier .truc posé à la racine de chaque projet” (tm). Nous avons donc développé Coke, un script de sniff, qui lance PHP CodeSniffer avec la configuration contenu dans le fichier “.coke” la racine du projet :\n\n\n\nAinsi, lorsque le fichier est paramétré et que le script coke est correctement installé sur le système, il suffit d’exécuter la commande “coke” depuis la racine du projet sniffer.\n\nLe fix\n\nDans l’optique d’automatiser le plus possible nos processus, nous avons inséré la vérification des coding styles à l’aide de Coke, dans un hook git de pre-commit.\n\nCoke est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Encodage - packaging - DRM - tout sur la vidéo",
    "category" : "",
    "tags"     : " video, codec, drm, lft",
    "url"      : "/encodage-packaging-drm-tout-sur-la-vid%C3%A9o",
    "date"     : "June 26, 2013",
    "excerpt"  : "\n\nUne nouvelle vidéo de l’année dernière provenant d’un Last Friday Talk.\n\nSouvent le monde de vidéo est source d’imprécision, cette vidéo met à plat l’ensemble des termes qui sont utilisés dans le domaine :\n\n\n  encodage, transcodage\n  packaging (...",
  "content"  : "\n\nUne nouvelle vidéo de l’année dernière provenant d’un Last Friday Talk.\n\nSouvent le monde de vidéo est source d’imprécision, cette vidéo met à plat l’ensemble des termes qui sont utilisés dans le domaine :\n\n\n  encodage, transcodage\n  packaging (transformation du conteneur vidéo)\n  DRM\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #4",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-4",
    "date"     : "June 14, 2013",
    "excerpt"  : "Parceque c’est dredi et que ca nous fait toujours rire de partager les phrases chocs entendues dans nos bureaux, voici les Dev Facts #4.\n\nCaptain Obvious\n\n\n  \n    “Vous avez oubliez quelque chose ?”\n    “Le problème, c’est que quand t’oublie, t’y ...",
  "content"  : "Parceque c’est dredi et que ca nous fait toujours rire de partager les phrases chocs entendues dans nos bureaux, voici les Dev Facts #4.\n\nCaptain Obvious\n\n\n  \n    “Vous avez oubliez quelque chose ?”\n    “Le problème, c’est que quand t’oublie, t’y penses pas”\n  \n\n\nLa minute de 30 secondes\n\n\n  Si je gagne cette somme, je me prends 6 mois d’année sabatique\n\n\n$i++\n\n\n  Avec un ticket resto, tu peux manger 2, avec toute ta famille\n\n\n???\n\n\n  On t’a pas sonné les oreillettes\n\n\nla mémoire\n\n\n  La musique est mémorable, mais je m’en souviens plus\n\n\nAnonymous Proxy Land\n\n\n  Pour poster en anonyme, il faut être loggué.\n\n\nJésus multipliait les pains\n\n\n  On va dédupliquer les clics par quatre\n\n\nAbsent coupable!\n\n\n  “Dès que je ne suis pas là, j’ai toujours tord … :(“\n “Bein oui! Les innocents ont toujours tord”\n\n\nTransgiving\n\n\n  1 mec sur 3 qui regardent des pornos sur Internet sont des femmes\n\n\nFatal error never die\n\n\n  PHP : Fatal error: date() [function.date]: Timezone database is corrupt - this should never happen! in ///**/error.php on line 105\n\n\n"
} ,
  
  {
    "title"    : "Firewall applicatif PHP et bundle Symfony",
    "category" : "",
    "tags"     : " outil, php, symfony, open-source",
    "url"      : "/firewall-applicatif-php-et-bundle-symfony",
    "date"     : "May 30, 2013",
    "excerpt"  : "Nous publions aujourd’hui notre firewall applicatif sur notre compte GitHub. Il se compose :\n\n\n  d’un composant PHP (5.4+) gérant les IPs (V4 et V6), plages, wildcards, white/black lists, etc.\n  d’un bundle Symfony permettant d’utiliser le composa...",
  "content"  : "Nous publions aujourd’hui notre firewall applicatif sur notre compte GitHub. Il se compose :\n\n\n  d’un composant PHP (5.4+) gérant les IPs (V4 et V6), plages, wildcards, white/black lists, etc.\n  d’un bundle Symfony permettant d’utiliser le composant Firewall dans les controllers à l’aide des annotations et de retourner une réponse HTTP personnalisée.\n\n\nIls utilisent tous les deux Composer et sont disponibles sur Packagist.\n\nQu’est ce qu’un Firewall applicatif ?\n\nUn Firewall applicatif permet de restreindre l’accès de certaines IPs à certaines parties d’une application. Vous pouvez par exemple définir la liste des IPs autorisées dans la section d’administration ou au contraire celles que vous souhaitez bloquer dans un forum.\n\nPourquoi cette implémentation ?\n\nNous souhaitions éviter de redéfinir l’ensemble des IPs chaque point de restriction. Nous avons donc cherché centraliser la configuration. Le FirewallBundle permet de mettre en place des listes hiérarchisées ainsi que des configurations prédéfinies que nous pouvons réutiliser et adapter chaque besoin.\n\nComment contribuer ?\n\nSi notre firewall applicatif répond certaines de vos problématiques, mais que vous souhaitez le voir évoluer, n’hésitez pas participer son développement :\n\n\n  forkez les projets sur GitHub,\n  faites une branche par fonctionnalité,\n  proposez-nous vos évolutions et optimisations via les Pull Requests.\n\n\nVous pouvez également nous remonter les problèmes rencontrés lors de son utilisation dans les issues du composant ou les issues du bundle.\n\nEnfin, n’hésitez pas utiliser les commentaires de cet article pour nous faire part de vos réactions.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #3",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-3",
    "date"     : "May 24, 2013",
    "excerpt"  : "Episode 3 des devfacts ! Parce qu’on ne s’en lasse pas.\n\nFort Boyaux\n\n\n  Cassandra tête de tigre !\n\n\nla dev même en prod\n\n\n  C’est un environnement de dev même en prod !\n\n\nPCF\n\n\n  \n    J’adore le prénom de Staline !\n    Sylvester ?\n  \n\n\nglory and ...",
  "content"  : "Episode 3 des devfacts ! Parce qu’on ne s’en lasse pas.\n\nFort Boyaux\n\n\n  Cassandra tête de tigre !\n\n\nla dev même en prod\n\n\n  C’est un environnement de dev même en prod !\n\n\nPCF\n\n\n  \n    J’adore le prénom de Staline !\n    Sylvester ?\n  \n\n\nglory and consequences\n\n\n  OH PUTAIN j’ai été RT par un participant de la belle &amp;amp; ses princes ! : jour de gloire :\n\n\nl’optimiste\n\n\n  C’est moins pire que rien\n\n\nle blagueur\n\n\n  T’as trois poussins sur une table ; comment tu fais pour en avoir plus que deux ? … T’en pousses un.\n\n\nI can haz root access !\n\n\n  \n    Tu peux me donner les accès MySQL ?\n    Ils sont en root.\n    Et ils arrivent quand ?\n  \n\n\nle jimmy cliff\n\n\n  I can see clearly now, the regex’s gone.\n\n\nla honte\n\n\n  \n    j’ai un peu honte de ce que je fais là …\n    quoi tu fais du javascript ?\n  \n\n\nl’aveuglement\n\n\n  Tu peux être valide w3c, l’aveugle, il verra toujours rien !\n\n\nle choix dans la date\n\n\n  Date de sortie (jj/dd/yyyy)\n\n"
} ,
  
  {
    "title"    : "Redis on fire !",
    "category" : "",
    "tags"     : " redis, nosql, lft, video",
    "url"      : "/redis-on-fire",
    "date"     : "May 22, 2013",
    "excerpt"  : "On continue la diffusion de quelques LFT triés sur le volet.\n\nCette fois ci c’est Kenny Dits qui s’y colle avec une présentation de Redis et des cas d’utilisation de cette technologie.\n",
  "content"  : "On continue la diffusion de quelques LFT triés sur le volet.\n\nCette fois ci c’est Kenny Dits qui s’y colle avec une présentation de Redis et des cas d’utilisation de cette technologie.\n"
} ,
  
  {
    "title"    : "CR Conférence Agora Cms du 15 mai 2013",
    "category" : "",
    "tags"     : " conference, cms",
    "url"      : "/cr-conference-agora-cms-du-15-mai-2013",
    "date"     : "May 20, 2013",
    "excerpt"  : "\n\nLe 15 mai 2013 avait lieu à Paris, la première édition de l’AgoraCMS, Conférence axée sur les CMS et la gestion de contenu Web.\n\nCette conférence est organisée par des acteurs importants du milieu, de chez Microsoft, Epitech, Oxalide, Cap Gemini...",
  "content"  : "\n\nLe 15 mai 2013 avait lieu à Paris, la première édition de l’AgoraCMS, Conférence axée sur les CMS et la gestion de contenu Web.\n\nCette conférence est organisée par des acteurs importants du milieu, de chez Microsoft, Epitech, Oxalide, Cap Gemini …\n\nAu rendez-vous, des sujets sur Drupal, Wordpress, le Responsive, les Réseaux Sociaux d’entreprise, et des retours d’expérience de différents acteurs français sur leurs utilisations de CMS public, “home made” ou propriétaire.\n\nLes CMS - écosystème - état des lieux et tendance, par Marine Soroko (CoreTechs) et Frédéric Bon (Clever Age)\n\nPremière conférence, et bonne introduction en la matière avec une présentation de la typologie des CMS.\n\nOn nous met en garde sur les “éditeurs de contenu” notamment, ou un CMS ne doit pas permettre de générer du contenu dans les pages. Un CMS de nos jours, doit permettre de gérer un référentiel de contenu, que nos pages doivent pouvoir requêter, sinon nous devenons vite confrontés a des problèmes de ré-usabilité.\n\nLes différents acteurs du marché sont présentés via les fameux “Quadrant magic” qu’on nous annonce finalement très loin de la réalité.\n\nOn finit sur une projection du futur des CMS qui devront répondre aux problématiques suivantes :\n\n\n  multi-canal (support tablette / site tiers etc)\n  multi-source\n  personnalisation\n  intégration e-commerce\n  Analyses et statistique\n  interactions et e-services\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nVotre CMS intelligent grâce l’analyse des logs, par Jérome Renard (Belogik)\n\nJérome Renard, ancien développeur EzPublish, a monté ces derniers mois une start-up proposant du Log As a Service, avec une solution du nom de Belogik, un peu comparable un Loggly.\n\nLa conférence présente l’intérêt d’analyser les logs (ici sortant d’un CMS, mais transposable tout site/service web) :\n\n\n  Incident de production\n  Service de la preuve\n  SEO\n  Performances\n  Gestion applicative\n  Sécurité\n  Développement\n\n\nMais aussi la difficulté à traiter des logs de formats différents, pas forcément disposition, quand vous ne maitrisez pas ou peu l’hébergement.\n\nPour la recherche dans ses logs, on a des solutions comme SolR ou les différents produits basés sur Lucene comme ElasticSearch (je rajouterais aussi le couple LogStash + Kibana utilisant aussi ElasticSearch)\n\nBref, une excellente présentation, dans la veine de celles que nous avions pu présenter chez m6web au niveau du Monitoring, un sujet très complémentaire avec le Logging.\n\nPour plus d’informations, belogik.com, ou sur leur compte twitter : @belogikCom.\n\nSinon vous pouvez consulter les slides sur le lien ci-dessous.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nVotre CMS intelligent grâce l’analyse des logs\n\nhttps://jrenard.info/talks/agoracms2013/\n\nTendances du design et nouveaux usages, par Patrick Maruejouls (Think Think)\n\nTrès bonne présentation sur l’importance du design dans le premier sens du terme : un outil pouvant et devant permettre de servir les intérêts stratégiques d’une entreprise. Il en découle qu’il serait bon de placer le design en amont des décisions et non en aval comme c’est souvent le cas.\n\nDe ses nombreuses expériences, nous pouvons retenir qu’une des plus importantes tendances pour les années venir est l’adaptation du design l’utilisateur. Ainsi, une grande enseigne de prêt-à-porter masculin a inséré une puce RFID dans l’étiquette de ses vêtements pour que l’ambiance des cabines d’essayage s’adapte aux vêtements essayés (ex: une petite musique des îles se déclenche lorsque le client essaye une chemise hawaïenne).\n\nDe même, cette tendance d’adaptation du design l’utilisateur pourra se voir magnifiée par la TV connectée et le second écran.\n\n\n\n\n\nLe second écran chez M6Web\n\nLes meilleurs thèmes et modules Drupal, par Dorian Marchan (Kernel 42) et Romain Jarraud (Trained People)\n\nPetite introduction Drupal, présenté comme un CMS, mais surtout un CMF (Content Management Framework), avec la présentation de quelques modules très intéressants pour des développeurs ou pour réaliser son “Usine à Site”.\n\nOn retiendra GCC dont une démo très intéressante sera faite, Drupal Commerce, étant une distribution de Drupal avec un assemblage de modules et personnalisation pour orienter son drupal vers le e-commerce, mais aussi la présentation d’un des thèmes les plus évolués de Drupal : Omega, thème ultra complet, Responsive avec un back-office assez puissant.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nLes Réseaux sociaux d’entreprise, par Edouard Ly (Oxalide) et Marine Soroko (Core Techs)\n\nTerme la mode depuis ces dernières années, les RSE commencent envahir les entreprises.\n Présentation (sans démo ou screenshot malheureusement), des différents acteurs du marché (très peu d’acteurs open source d’ailleurs :( ) :\n\n\n  Jive (leader du marché)\n  BlueKiwi\n  Telligent\n  BuddyPress : extension Wordpress\n  Yammer\n  Elgg\n  Chatter\n  Sharepoint\n  Liferay\n  Drupal commons\n  …\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nResponsive design : un site mobile en moins d’une heure, par Raphael Goetter (AlsaCreations)\n\nRaphael Gotter ( @goetter), le créateur d’AlsaCréations, est complètement incontournable pour tout ce qui touche l’intégration HTML, ou le RWD (Responsive Web Design) dans la communauté francophone, nous explique les approches à avoir pour réaliser un site RWD.\n Notamment que le Mobile First, paraît toujours être la bonne approche, ainsi qu’un rappel sur le fait que le RWD doit être pensé et prévu en amont.\n\nRaphael nous démontre aussi que son titre, et la réalisation d’un site RWD en moins d’une heure est infaisable. En prenant l’exemple du site d’Agora CMS, et en nous présentant l’approche pour transformer sa HP en Responsive.\n Plus de 15 jours de boulot au final, et des slides très intéressantes découvrir ci-dessous, remplies d’astuces, de checklists, et autres notions à bien comprendre avant de s’intéresser et de se lancer dans le RWD :\n\n\n  Comprendre les surfaces d’affichage\n  Connaître les Media Queries css 3\n  le Box-sizing\n  Halte aux débordements\n\n\nJe vous invite à tester sur un mobile Mobitest.me pour bien comprendre la différence entre viewport, largeur en pixel réelle, …\n\nPrésentation de la propriété hyphens aussi pour gérer les débordements de texte, coupler avec la propriété word-wrap, le framework / css de base Knacss.com, les tailles de typo en “rem”.\n\nBref, conférence très riche, drôle et a creuser impérativement pour tout ceux qui travaillent de près ou de loin sur ces problématiques.\n\nCompte rendu par Raphael lui même : https://blog.goetter.fr/post/50567713227/conference-un-site-responsive-en-une-heure avec le résultat voir ici : https://kiwi.gg/rg/agora/\n\nEt les slides ci-dessous :\n\n\n\n20 minutes: gérer le multi-canal, apps, web, devices, par Arnaud Limbourg (20 minutes)\n\nArnaud ( @arnaudlimbourg) a rapidement expliqué la stratégie mise en place par 20minutes pour permettre le multi-support : centralisation des données dans un référentiel accessible via une API permettant aux différents devices de se fournir en données, chacun leur manière.\n\nIl a ensuite donné quelques conseils afin de fournir une bonne API, en précisant qu’il était très difficile d’en réaliser une bonne :\n\n\n  architecture REST en HTTP,\n  faibles temps de réponse,\n  bon monitoring,\n  facilité d’apprentissage et d’utilisation,\n  concepts simples,\n  documentation.\n\n\nEnfin, Arnaud explique qu’il n’y a pas de solution miracle entre l’internalisation ou l’externalisation des développements (l’app Android a été développée en interne alors que le développement de l’app iOS a été externalisée).\n De la même manière, le choix entre le HTML5 et le code natif dépends des besoins et des ressources.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nRetour d’expérience : France Télévisions, par Léo Poiroux (France Télévisions)\n\nNos confrères de chez France Télévisions nous présentent leur retour d’expérience sur leurs usines site, avec au micro, Léo ( @Leo_Px).\n\nAu départ, sur Spip, il nous explique pourquoi ils ont migré (douloureusement au début) sur Drupal, et qu’est ce que cela leur a apporté.\n\nUne conférence très transparente, drôle, agréable et intéressante.\n\nL’avenir de leur côté tend vers une transformation de l’usine à site, vers une “usine à interface” notamment pour gérer les multi-écrans, et une ouverture de leurs api/services vers de l’openApi, OpenData.\n\nFT c’est une très grosse DT (une centaine de personnes), une équipe d’expert transverse surnommée SWAT (composée d’Expert frontend JS/WebPerf, Expert Archi/Varnish, Expert Drupal, et de deux Coach Agile).\n\nD’autres excellentes idées comme leurs Dojo et Safari.\n Les Dojo sont des sessions d’une heure où les développeurs se relaient toutes les 5 minutes sur un même code, pour avancer un développement..\n Les safaris sont une sorte de “Vis ma vie” avec une journée en immersion dans une autre équipe de développement par exemple ou dans une équipe de journaliste utilisant l’un des sites qu’ils ont développés.\n\n\n  Pourquoi Drupal ? \n “La maison blanche utilise Drupal”\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nTable Ronde. Quel modèle choisir : créer mon propre CMS ? utiliser un CMS existant ? OpenSource / propriétaire ?\n\nLa journée se termine sur une table ronde composée de différents responsables techniques :\n\n\n  Olivier Grange Labat – @ogrange DT @ Le Monde interactif : Olivier est en charge de la technique de Lemonde.fr. Il a fait le choix de développer son propre outil de gestion de contenu\n  Damien Cirotteau – @cirotix : Damien est CTO chez Rue89 qui utilise et est spécialisé en Drupal.\n  Olivier Fouqueau – DSI de la mairie d’Aulnay-sous-bois : Olivier est en charge des Systèmes d’informations et de l’innovation la Mairie d’Aulnay-sous-bois. Il fait le choix du CMS Ametys.\n  Galdric Pons – @hebiflux Chef de projet digital @ BNP Paribas : Galdric est Chef de projet digital au pôle innovation de BNP Paribas ou il a mis en place une usine site avec WordPress.\n\n\nChacun des participants au cours d’une interview animée par Cyril Pierre de Geyer ( @cyrilpdg) , va expliquer son choix de CMS, les avantages et les inconvénients et donner de précieux conseils aux personnes dans la même situation.\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nConclusion\n\nUne journée haute en couleur, avec de très bonnes conférences et des retours d’XP toujours aussi intéressants.\n On salue une organisation impeccable et le tout pour un prix très accessible (20€).\n\nCe CR ne rend-compte finalement que d’une mince partie des conférences (4 conférences en parallèle pour cause), mais vous pouvez retrouver d’autres slides sur le site officiel de l’évenement.\n\nN’hésitez pas commenter ce CR si vous avez des remarques ;-)\n\nP.s: Merci @nsilberman pour les photos que vous pouvez retrouver en intégralité ici : https://photos.silberman.fr/Other/AgoraCMS/\n"
} ,
  
  {
    "title"    : "La POO Canada Dry",
    "category" : "",
    "tags"     : " php, poo, lft, video",
    "url"      : "/la-poo-canada-dry",
    "date"     : "May 6, 2013",
    "excerpt"  : "Nous vous avions parlé, il y’a quelques mois, de nos conférences interne, les Last Friday Talk.\n\nVoici une première vidéo de l’une de ces sessions sur “la POO Canada Dry”.\n\nLa POO (Programmation Orienté Objet) ne consiste pas à mettre du code dans...",
  "content"  : "Nous vous avions parlé, il y’a quelques mois, de nos conférences interne, les Last Friday Talk.\n\nVoici une première vidéo de l’une de ces sessions sur “la POO Canada Dry”.\n\nLa POO (Programmation Orienté Objet) ne consiste pas à mettre du code dans des classes, elle fait appel à des concepts vitaux pour le développeur moderne. Olivier nous présente quelques mauvais exemples tirés de code legacy et quelques bonnes pratiques pour faire de la POO (mais en fait c’est surtout du troll).\n\nEnjoy\n"
} ,
  
  {
    "title"    : "CR Real Time Conférence Europe 2013 - Day 2",
    "category" : "",
    "tags"     : " conference, nodejs, realtime",
    "url"      : "/cr-real-time-conference-europe-2013-day-2",
    "date"     : "April 26, 2013",
    "excerpt"  : "\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8679275805/in/set-72157633306379029\n\nAprès la première journée, on continue avec la deuxième journée de conférence. Toujours sur le format de 20 minutes pour présenter le sujet.\n\nWOOT, Arial B...",
  "content"  : "\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8679275805/in/set-72157633306379029\n\nAprès la première journée, on continue avec la deuxième journée de conférence. Toujours sur le format de 20 minutes pour présenter le sujet.\n\nWOOT, Arial Balkan\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326600103475425281/photo/1\n\n@aral a travaillé sur une solution d’édition partagée de contenu, une solution sans transformation opérationnelle (OT sur Wikipedia) : WOOT qui signifie Without Operational Transformation.\n\nLe concept exposé est de ne pas faire de suppression des caractères d’une chaîne, mais plutôt de travailler sur la notion de visible / invisible et de position du caractère au sein de la chaîne. L’objectif étant de garder la convergence et de préserver les intentions des utilisateurs.\n\nIl nous indique différentes ressources pour approfondir le sujet :\n\n\n  une vidéo de Google Tech Talk :Issues and Experiences in Designing Real-time Collaborative Editing Systems\n  une librairie JS : ShareJS\n  pour aller plus loin dans la gestion du travail collaboratif, l’Inria rend disponible différents travaux de recherches sur le sujet\n\n\nConvention-Driven JSON, Steve Klabnik\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326607298556489728/photo/1\n\n@steveklabnik avait déjà parlé la veille sur un autre sujet. Aujourd’hui, il nous expose la problématique de passer des objets (quelque soit le langage) JSON. Bien souvent, on utilise JSON pour la communication entre différents services (l’un en PHP et l’autre en Python par ex, ou deux services en PHP).\n\nPour un site web, on arrive souvent au résultat suivant :\n\nObjet -&amp;gt; Template -&amp;gt; HTML\n\nPour éviter certains problèmes lors des échanges de données, il propose par exemple en ruby d’utiliser active_model_serializers ce qui permet d’obtenir le résultat suivant :\n\nObjects -&amp;gt; Serializer -&amp;gt; HTML\n\nEn résumé, il recommande de passer par un outil de serialisation des données afin de ne pas perdre la structure de l’objet et donc d’avoir une plus grande réactivité entre le client et le serveur.\n\nRealtime vs Real world, Tyler Mac Mullen\n\n@tbmcmullen travaille pour Fastly. Sa société propose des solutions d’optimisation au sein des infrastructures de type CDN.\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326613272969228288/photo/1\n\nIl commence sa présentation en définissant les deux termes :\n\n\n  Realtime = réduire la latence\n  Realworld = notion d’infrastructure\n\n\nIl indique également l’impossibilité de construire des infrastructures en temps-réel. Seul les CDNs ont la possibilité de s’approcher du temps-réel. La notion de purge est également essentielle.\n\nTyler présente ensuite 3 possibilités de purge :\n\n\n  Utilisation de Rsyslog : soit via TCP (problème : lenteur), soit via UDP (problème : pas de retour d’erreur). Un noeud notifie tous les autres.\n  Love triangle : pas de serveur central mais notion de peer-to-peer. Chaque noeud interagit avec 2/3 autres noeuds. Problème : il n’y a pas d’état global ni de possibilité de scalabilité avec ce type d’infrastructure\n  Hybride : on met en place des switchs au niveau des noeuds. Les switchs interagissent entre eux, puis redistribuent l’information au niveau de ces serveurs.\n\n\nLa société a déjà fait d’autres sessions lors d’autres conférences tel que Velocity qui pourrait fortement intéressé les adminsys ;-)\n\nDiscoRank : optimizing discoverability on SoundCloud, par Amélie Anglade\n\n\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8680450402/in/set-72157633306379029\n\n\n\n@utstikkar est une française qui travaille pour Soundcloud en tant que MIR Software Engineer.\n\nElle nous a expliqué l’évolution effectuée au sein de leur moteur de recherche : Discorank. Ce système peut être assimilé au PageRank de Google.\n\nIls utilisent pour cela : MySQL puis HDFS et enfin tout est re-manipulé dans ElasticSearch\n\nBuddyCloud - Rethinking Social, par Simon Tennant\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326635400175161344/photo/1\n\nSimon Tennant est CEO de la société BuddyCloud. Il a tenté de faire passer les informations suivantes :\n\n\n  fédérer ou mourir\n  travailler sur les protocoles non sur les APIs\n  pour construire du social dans un produit, il recommande de se baser sur l’open source, les standards et protocoles ouverts\n\n\nL’objectif de sa société est de permettre aux personnes de construire un réseau social fédéré et bien entendu temps-réel.\n\nN’hésitez pas à fouiller dans leur source sur Github) qui fourmille de fonctionnalités.\n\nRealtime at Microsoft, Pierre Couzy\n\nPierre Couzy travaille depuis plus de 10 ans chez Microsoft. Il nous présente un projet réalisé par des développeurs US : SerialR (Github). Le projet utilise les Websockets.\n\n\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8679336549/in/set-72157633306379029\n\nLe projet possède deux dépendances principales :\n\n\n  Json.net côté serveur\n  Jquery côté client\n\n\nIl est noter que la négociation exacte entre le client et le serveur dépend du navigateur utilisé.\n\nA noter que cette présentation est l’une des rares qui n’a pas été réalisée avec un MacBook ;-)\n\nLearning from Past Mistakes, a new node http layer, par Tim Caswell\n\n@creationix était un des anciens core dev de NodeJS. Il nous expose les différents points cause desquels il a quitté le projet.\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326670519371980800/photo/1\n\nUn des autres problèmes avec NodeJS est que le changement est difficile :\n\n\n  utilisé en production par différentes sociétés\n  difficulté à modifier les APIs\n\n\nIl a donc développé Luvit basé sur la technologie Lua (légère, rapide et permettant les co-routines).\n Cette nouvelle couche HTTP donne la possibilité :\n\n\n  de suspendre et de reprendre la fibre actuelle\n  lorsque l’on a des fibres on peut faire d’autres choses\n  écrire sur les objets stream avec .write(item)\n  lire sur les objets stream avec .read()\n  de terminer un stream avec false item\n\n\nVous pouvez retrouver l’ensemble des slides sur Github\n\nHTTP Proxy, par Nuno Job\n\n\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8680421388/in/set-72157633306379029\n\n@dscape nous propose un bon article sur le load balancing avec nodejs.\n Pour le speaker nodejs c’est : net protocols &amp;amp;&amp;amp; libuv &amp;amp;&amp;amp; v8 &amp;amp;&amp;amp; npm\n\nVous pourrez retrouver l’ensemble des slides sur Github\n\nLearning How To Let Go, par Kyle Drake\n\n@kyledrake introduit d’autres solutions en remplacement de JSON : basés sur des données en binaire.\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326684373090963456/photo/1\n\nCependant, tout le monde n’utilise pas correctement les échanges binaires. Le speaker nous fait un très bon résumé de la situation pour effectuer des pushs sur la plateforme d’Apple (Apple Push Notification Service) et des types de retours effectués par Apple. Ceci résume assez bien la situation.\n\n\n\nCrédit : https://twitter.com/noel_olivier/status/326686457257406464\n\nJS ne propose pas d’API “native” mais un projet permet de traiter du binaire : binaryjs (du binaire via websockets).\n\nKyle effectue différents benchs sur la taille des contenus envoyés : l’un en JSON, l’autre en Binary JSON et le dernier via MessagePack. Bien entendu, ce sont les contenus en binaire qui sont les plus légers, mais il reste voir l’impact du téléchargement du JS associé et du traitement côté client.\n\nUn sujet donc à étudier qui rappelle AMF pour échanger des informations au format binaire entre PHP et Flash.\n\nSecuring socket applications, par James Coglan\n\nDans un premier temps, @jcoglan nous indique que la sécurité c’est difficile et que cela concerne :\n\n\n  l’authentification\n  la vie privée\n  les XSS\n  les CSRF\n\n\nPour répondre aux différentes problématiques, il nous présente Faye un système simple de message pub/sub pour le web. Ses slides sont disponibles.\n\nReal-time design, par Jan-Christoph Borchardt\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326711206209527809/photo/1\n\n@jancborchardt n’est pas un développeur, mais il nous rappelle quelques concepts importants :\n\n\n  Plus l’utilisateur s’ennuie, plus la confusion augmente\n  Sous 0,1 ms, l’utilisateur considère cela comme du temps réel\n  Attention aux transitions\n  Ne pas tuer la fluidité\n  “Interruptification” (exemple flagrant sur l’image ci-dessous)\n  Pas de notifications pendant l’utilisation (ex batterie faible 20% sur un mobile)\n\n\n\n\nEn résumé, l’interface et le design sont importants également pour s’approcher d’une expérience utilisateur temps-réel.\n\nFin de la seconde journée\n\nPour les plus courageux, le livestream est également disponible.\n\n\n\nCet événement était très intéressant :\n\n\n  par son avance de phase, la majorité des présentations correspondaient des résultats de R&amp;amp;D, voire d’innovation.\n  par l’ambiance\n  par le networking que l’on pouvait y faire\n\n\nOn peut en revanche peut être un peu regretter le nombre de français (la fois côté speaker et également côté public).\n\nPour terminer, un grand merci Julien Genestoux qui a organisé l’événement.\n\nRendez-vous pour la prochaine édition.\n"
} ,
  
  {
    "title"    : "CR Real Time Conférence Europe 2013 - Day 1",
    "category" : "",
    "tags"     : " conference, nodejs, zeromq, rabbitmq, realtime",
    "url"      : "/cr-real-time-conference-europe-2013-day-1",
    "date"     : "April 25, 2013",
    "excerpt"  : "Les 22 et 23 Avril 2013, ont eu lieu, la Real Time Conférence en version Européenne.\n\nPour cette première édition, les festivités se déroulaient à Lyon, à la Plateforme, une péniche posée sur les quais du Rhône très sympathique.\n\nPassé l’accueil “...",
  "content"  : "Les 22 et 23 Avril 2013, ont eu lieu, la Real Time Conférence en version Européenne.\n\nPour cette première édition, les festivités se déroulaient à Lyon, à la Plateforme, une péniche posée sur les quais du Rhône très sympathique.\n\nPassé l’accueil “la Titanic” avec l’orchestre dans le hall d’entrée, nous descendons au sous-sol pour commencer suivre la première journée de conférence, qui s’annonce déjà très chargée.\n\n\n\nLa vue de la salle de conférence (Crédit : https://twitter.com/frescosecco/status/326302218515017729/photo/1 )\n\nWebSuckets, par Arnout Kazemier\n\nPremière conférence autour des Websockets et des bugs ou difficultés d’implémentation que l’on peut rencontrer.\n\nOn parle notamment de Firefox qui en prend pour son grade : Si l’on appuie sur ESC après que la page soit chargée, toutes les connexions sont fermées … Firefox ne peut pas se connecter non plus sur une Websocket non sécurisée en HTTPS.\n\nCoté Safari Mobile, écrire dans une Websocket fermée plante votre téléphone, et cela arrive quand on revient sur un onglet qui utilisait des Websocket, ou lorsque l’on réouvre un safari précédemment réduit.\n\nBref, en gros, ca démotive un petit peu sur l’utilisation des Websockets !\n\nArnout ( @3rdEden ) déconseille aussi l’utilisation des Websocket sur mobile, et indique de ne les utiliser que quand c’est vraiment nécessaire sur desktop.\n\nQuelques présentations d’outillages :\n\n\n  HA Proxy\n  HTTP-Proxy\n  Nginx-devel\n\n\nVous pouvez retrouver une battle sur les perfs de ces proxys ici : github.com/observing/balancerbattle\n\nOn aborde aussi les problématiques de tirs de charge sur les Websocket avec :\n\n\n  wsbench\n  websocketbenchmark\n\n\nLes deux étant, d’après Arnout, incomplets ou dépassés …\n\nIl a donc développé son propre outil : Thor, “smasher of Websockets” à tester de toute urgence : https://github.com/observing/thor\n\nLes frameworks mentionnés pour en simplifier l’implémentation :\n\n\n  Faye\n  Signalr\n  xsockets\n  sockjs\n  socket.io\n\n\nAttention aussi aux éléments perturbateurs : firewall, extensions de browsers, antivirus, ou proxy qui peuvent bloquer les ports utilisés par les Websockets.\n\nBref, une première entrée en matière très complète et intéressante qui couvre vraiment toute la partie moins glamour des Websockets.\n\nJe vous invite aussi à consulter son blog si le sujet vous intéresse : https://blog.3rd-eden.com/\n\n\n\n(Crédit : https://twitter.com/hintjens/status/326243158109347841/photo/1 )\n\n\n\nSocketStream 0.4, par Owen Barnes\n\nVoici l’un des frameworks pour l’implémentation des Websockets, où son créateur ( @socketstream ) nous partagé ses idées de la conception et de l’utilisation d’un framework : découplage, simplicité, modularité etc.\n\nLe framework est “Transport Agnostics” et peut donc utiliser sockJs, Engine.io, ou Websockets native juste en changeant une simple ligne.\n\nLe FW est basé sur Prism, un module de serveur realtime, lui aussi open-sourcé sur github.com/socketstream/prism.\n\nLa 0.4 présentée est en cours de finalisation, et sera disponible prochainement en version finale sur le github https://github.com/socketstream/\n\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8672407195/in/set-72157633306379029/ )\n\nXSockets, par Magnus Thor\n\nMagnus ( @dathor ), nous présente son framework Xsockets pour l’utilisation des Websockets avec une démo “live coding” peut-être intéressante, mais tentée “online” et avec une connexion bien foireuse (comme dans toutes les conférences techniques, non ?) …\n\nBref, un peu douloureux à regarder, mais la démo avait l’air d’avoir du potentiel : une application web utilisant WebRPC pour partager en mode Peer To Peer la Webcam de l’utilisateur.\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673638532/in/set-72157633306379029/ )\n\nZeroMQ as scriptable sockets, par Lourens Naudé (Bear Metal)\n\nLourens ( @methodmissing) est l’un des “co-maintainer” de ZeroMq.\n\nIl nous présente ZeroMq comme une solution de messagerie instantanée pour les apps. Ca n’est pas un serveur, ni un broker, mais une librairie sur la communication et gestion de la concurrence.\n\nOn parcourt ensuite les différents types de sockets supportés :\n\n\n  Req / Rep\n  Pub / Sub\n  Push / Pull\n\n\nVoir la présentation ci dessous :\n\n\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8672695927/in/set-72157633306379029/ )\n\nWebRTC, par Sam Dutton (Google)\n\nSam Dutton, Developper Advocate chez Google ( @sw12 ), qu’on à déja vu/entendu par le passé à la Vélocity Conf (voir précédent CR) nous parle de WebRPC.\n\nOn parcourt les différentes API disponible, le support des navigateurs (Chrome, Firefox Nightly et IE Chrome Frame …).\n\nOn découvre ensuite de nombreuses démos très sympa :\n\n\n  Ascii Caméra\n  GetUserMedia\n  Webcam Toy\n  Magic Xylophone\n  Screen Capture (nécessite Chrome Canary)\n  …\n\n\nPour débugger plus facilement, utilisez le chrome://webrtc-internals\n\nLibs, apps et frameworks pour XML RPC :\n\n\n  easyRTC : full stack\n  conversat.io built with SimpleWebRTC\n  PeerJS : API abstraction\n  webRTC.io\n  Sharefest\n\n\nPlus d’infos/codes ou démos sur les slides : https://samdutton.net/realtime2013/\n\n\n  “WebRTC and HTML5 could enable the same transformation for real-time communications that the original browser did for information.” Phil Edholm / Nojitter\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673907134/in/set-72157633306379029 )\n\nEnemy of the state : An introduction to functional reactive programming with Bacon.Js, par Phiilip Roberts (Float)\n\n\n\nL’une des présentations les plus intéressantes de la journée par Philip Roberts ( @philip_roberts ), CTO et co-founder de Float avec l’introduction Bacon.Js et la “Functional Reactive Programming” en Javascript. Une façon différente de coder pour éviter les “callback hell” notamment.\n\nLe projet répond aussi à une problématique très courante des dév JS, avec l’exemple du “Check Username Availibility” qui lance une requête Ajax chaque KeyPress et dont l’ordre n’est pas maitrisé. (partir de la slide 38)\n\nBacon.Js est dispo sur Github : https://github.com/raimohanska/bacon.js\n\nP.s: la visualisation des streams sur ses slides était très sympa : https://latentflip.com/bacon-examples/\n\nPlus d’infos sur les slides : https://latentflip.com/bacon-talk-realtimeconfeu/\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673684632/in/set-72157633306379029 )\n\nQuick Wins with Redis for your website, par Cathering Jung\n\nCatherine Jung ( @bufferine ) travaille sur des services de paris en ligne. Elle explique les problématiques de temps réel qu’elle doit affronter, et comment Redis lui permet de mieux supporter la charge.\n\nAu final, on parle un peu plus de Scala que de Redis, mais tout retour d’expérience est toujours bon prendre.\n\nRetrouvez les slides ici :\n\nhttps://docs.google.com/file/d/0By6ZH5wplIR-MzgyOEJCMEkyWmc/edit?usp=sharing \n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673926016/in/set-72157633306379029/ )\n\nRealtime and Go : Leaving the frameworks behind, par Paddy Foran\n\nPaddy ( @paddyforan ) nous présente le language Go.\n\nA la question : “What is Go ?” la réponse est :\n\n\n  A better C, from the guys that didn’t bring you C++\n\n\nhttps://goonaboat.com/\n\nBref, Go c’est :\n\n\n  Compiled\n  Static typed\n  Fast\n  Elegant\n  Concurrent\n\n\nLes slides sont disponibles ici : https://goonaboat.com/ et le code de la présentation : https://github.com/paddyforan/goonaboat\n\nPlus d’infos sur le langage ici : https://golang.org/ avec un “Tour” qui parait très bien fait : https://tour.golang.org/#1\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673939104/in/set-72157633306379029 )\n\nCloud Messaging with Node.Js and RabbitMQ, par Alvaro Videla\n\nAlvaro ( @oldsound ) est le co-auteur de “Rabbit Mq In action”.\n\nIl a présenté l’intérêt d’utiliser un rabbitMQ dans un projet qui est un fork d’Instagram, mais Real Time : CloudStagram, sur une stack “Cloud Foundy”, Rabbit MQ, Redis, MongoDB et SockJS\n\nNotamment le concept de tout gérer via événement (slide 54 ci-dessous).\n\nBref, pas mal de bonnes idées à retenir et pas mal de projets intéressants sur son github : https://github.com/videlalvaro , comme le RabbitMqSimulator pour présenter clairement le fonctionnement des RabbitMQ\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673976982/in/set-72157633306379029/ )\n\n\n\nOffline first!, par Jan Lehnardt\n\nJan ( @janl ) bosse sur CouchDb. Apache CouchDB est une base de données de type document basée sur le format JSON et utilisant Javascript (notamment pour les MapReduce).\n\nIl commence sa présentation par un “You are all doing it wrong !”. En réexpliquant que le réseau est toujours rapide, mais que c’est la latence qui est problématique. (Voir l’excellent article de 2010 de @edasfr sur le sujet toujours aussi pertinent ).\n\nIl faut aujourd’hui travailler Offline First ! (un peu l’équivalent d’un Mobile First coté apps), et prend pas mal d’exemples de bonne ou mauvaise implémentation (de la gestion hors connexion, du passage dans un tunnel, en se moquant de la mauvaise couverture française dans le TGV).\n\nOn aborde ensuite les :\n\n\n  CouchDB\n  PouchDB : Javascript database that syncs!\n  TouchDB : CouchDB-compatible embeddable database engine for mobile &amp;amp; desktop apps\n\n\net la présentation du framework Hoodie : https://hood.ie/, basé sur le Offline par défaut.\n\n\n  “Think of CouchDB as Git for your application data” Jan Lehnardt\n\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8672884965/in/set-72157633306379029/ )\n\nBuilding Realtime HTML5 apps for Android and Firefox OS, par Anthony Ricaud\n\nPrésentation par Anthony Ricaud ( @rik24d ) des fonctionnalités HTML5 implémentées par les équipes de Mozilla, notamment pour connaitre l’état de la batterie, l’orientation, la gestion des apps supportant la sélection de photos par exemple…\n\nChaque site peut être une apps, à condition de mettre les lignes nécessaires dans un fichier manifest. Beaucoup de débats aussi autour des systèmes fermés de MarketPlace.\n\nPlus d’infos dans les slides ci dessous :\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8672894401/in/set-72157633306379029 )\n\n\n\nNodeCopter + Hackathon\n\nJe laisse la parole à Olivier Mansour pour la présentation du NodeCopter :\n\nRomain Huet ( @romainhuet ) nous a présenté et fait une petite démonstration du pilotage d’un AR Drone avec NodeJs.\n\nIssu du projet nodecopter https://nodecopter.com/, un ensemble de librairies Node.js est disponible et rend le pilotage du drone complètement accessible. Mouvements en vol, stream de la caméra, Romain nous a fait une démonstration fun et captivante de l’engin.\n\nEt faire voler un drone dans un bateau … on avait jamais vu ça !\n\nFin de la 1ère journée :\n\nUne première journée très sympathique, bourrée d’idées et d’outils en tout genre. L’organisation est vraiment au poil, et on repart en voulant refaire le monde techniquement :-)\n\nLe compte rendu de la deuxième journée est ici : https://tech.bedrockstreaming.com/cr-real-time-conference-europe-2013-day-2\n\nP.s : Merci &amp;amp;Yet pour la plupart des photos présentes ici : https://www.flickr.com/photos/andyet-photos/sets/72157633306379029/\n\nPour les plus motivés, la conférence été enregistrée en vidéo :\n\n\nVidéo de la première journée de la RealTime Conf Europe\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #2",
    "category" : "",
    "tags"     : " humour, devfacts",
    "url"      : "/m6web-dev-facts-2",
    "date"     : "April 9, 2013",
    "excerpt"  : "On continue la série des Dév Facts, phrases oh combien cultes prononcées par nos chers développeurs lors d’oublis cérébraux :-)\n\nPour ceux qui avaient raté la première série, c’est ici : https://tech.bedrockstreaming.com/m6web-dev-facts-1\n\nEnjoy\n\n...",
  "content"  : "On continue la série des Dév Facts, phrases oh combien cultes prononcées par nos chers développeurs lors d’oublis cérébraux :-)\n\nPour ceux qui avaient raté la première série, c’est ici : https://tech.bedrockstreaming.com/m6web-dev-facts-1\n\nEnjoy\n\nCaptain Obvious\n\n\n  Chef de Projet : “Vous avez oublié quelque chose ?”\n\n  Développeur : “Le probléme c’est que quand t’oublie, t’y pense pas”\n\n\nPaye ta culture\n\n\n  C’est une citation de la bible … et de Civilisation IV\n\n\nSeigneur non !\n\n\n  \n    Avant j’encodais les vidéos pour l’émission “Le jour du Seigneur”\n    C’est quoi ? une émission porno ?\n    Ca dépend comment t’écris “seigneur”\n  \n\n\nLes congés du fantastique\n\n\n  Le 4, je suis en RPG … euh RTT\n\n\nDouble compétence …\n\n\n  Je viens de recevoir le CV d’un gars, il a fait une formation “Maîtrise en patisserie” … il doit maîtriser les cookies non ? :)\n\n\nIncomparable pour ne rien comparer\n\n\n  \n    Ah bon ? C’est les TCL qui font le plus souvent grève en France ? Plus que la SNCF ?\n    Oui, à titre de comparaison, je crois que c’est incomparable …\n  \n\n\nRetour vers le futur\n\n\n  Donc vous savez que je ne suis pas là la semaine dernière\n\n\nTrouvé !\n\n\n  Je viens de trouver une découverte !\n\n\nLa preuve par dix\n\n\n  On a doublé la bande passante par dix\n\n\nLe flegme illustré\n\n\n  De toute facon, y’a pas de conséquence : au pire, on meurt.\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #1",
    "category" : "",
    "tags"     : " humour, devfacts",
    "url"      : "/m6web-dev-facts-1",
    "date"     : "March 21, 2013",
    "excerpt"  : "Depuis de nombreuses années, toutes les “phrases chocs” dites par les équipes techniques de M6Web sont archivées, loggées, historisées. Pas moyen de sortir une ânerie sans qu’elle soit gravée dans le marbre.\n\nDu coup, nous avons décidé de profiter...",
  "content"  : "Depuis de nombreuses années, toutes les “phrases chocs” dites par les équipes techniques de M6Web sont archivées, loggées, historisées. Pas moyen de sortir une ânerie sans qu’elle soit gravée dans le marbre.\n\nDu coup, nous avons décidé de profiter de cette période très calme sur le blog pour vous faire partager une petite sélection en vrac de quelques Dev Facts entendus dans les locaux d’M6Web Lyon :\n\ninvalider le cache, nommer les choses …\n\n\n  \n    On purge le cache infini tous les jours.\n    Non c’est le cache 7 jours qu’on purge tous les jours !\n  \n\n\nJ’ajoute -2\n\n\n  J’ai réduit de fois 10\n\n\nwork - not - flow\n\n\n  On a des workflows, on a aussi des worknotflows\n\n\nLa bonne affaire\n\n\n  Pour le même prix, t’as bien moins cher ailleurs !\n\n\nEt pour arrêter ? tu cliques sur démarrer !\n\n\n  La vidéo a été mise en erreur avec succès.\n\n\nBig or what ?\n\n\n  Il y a une librairie Rennes, elle est énorme! Tu y rentres, c’est tout petit… !\n\n\nNiveau CE2\n\n\n  je fais un mail de réponse avec une explication niveau CE2\n\n\nBref\n\n\n  \n    \n      J’ai demandé Pierre, qui m’a dit qu’il ne savait pas mais que si j’avais l’info, il la voulait bien. Du coup, j’ai demandé Kenny, qui m’a dit de demander a Antony… qui ne savait pas et m’a dit de demander Pierre\n    \n    \n      Bref, j’ai posé une question un admin\n    \n  \n\n\nCDP Junior\n\n\n  Raa mais chui un cdp junior moi je sais rien faire de mes 10 doigts :(\n\n\nLa suite dans un prochain épisode ;-)\n\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un Lead Developpeur / Architecte web (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/m6web-lyon-recherche-un-lead-developpeur-architecte-web-h-f-en-cdi",
    "date"     : "February 5, 2013",
    "excerpt"  : "\n\nMise jour : Le poste n’est à plus pourvoir. Merci\n\nM6Web Lyon recrute, en CDI, un Lead Développeur LAMP, avec une très forte expertise sur les technologies PHP 5.4, MySQL, Symfony2, GIT, et capable d’encadrer une petite équipe de développement.\n...",
  "content"  : "\n\nMise jour : Le poste n’est à plus pourvoir. Merci\n\nM6Web Lyon recrute, en CDI, un Lead Développeur LAMP, avec une très forte expertise sur les technologies PHP 5.4, MySQL, Symfony2, GIT, et capable d’encadrer une petite équipe de développement.\n\nNous recherchons quelqu’un de très passionné, enthousiaste, et mordu de veille technologique : un missionnaire de l’open source, un intégriste de la qualité de code, des tests unitaires et fonctionnels, et un architecte de projets aguerri avec une première approche en méthodologie de développement agile, et une expérience de management de développeurs.\n\nSi, en plus, vous êtes un malade de l’optimisation back-end et front-end, que des technologies comme Node.js vous émoustillent, que, malgré la qualité de MySQL, vous envisagez dans certains cas des solutions NoSQL alternatives (Mongo, Redis…), votre profil nous intéresse !\n\nVenez apporter vos compétences aux équipes techniques de M6Web en travaillant sur des sites très forte charge (m6.fr, clubic.com, jeuxvideo.fr …), et partagez-les grâce des conférences internes ou externes et des articles sur notre blog.\n\nSi vous avez les qualités requises et l’envie de nous rejoindre, allez sur le lien ci-dessous et faites nous part de votre CV, de votre compte github, et d’une lettre attrayante pour nous motiver vous rencontrer.\n\nSi vous souhaitez postuler ou avoir plus d’infos : https://www.groupem6.fr/ressources-humaines/offres-emploi/lead-developpeur-architecte-web-h-f-229879.html\n\n"
} ,
  
  {
    "title"    : "Organiser des conférences technique en interne",
    "category" : "",
    "tags"     : " conference, culture, lft",
    "url"      : "/organiser-des-conferences-technique-en-interne",
    "date"     : "December 5, 2012",
    "excerpt"  : "\n\nLes “Last Friday Talk” : Le concept\n\nDepuis 10 mois désormais, chez M6Web, nous organisons chaque “dernier vendredi” du mois, une manifestation que nous avons nommée “Last Friday Talk”.\n\nLe concept : 2 heures de 13h30 15h30, où 4 sessions “type ...",
  "content"  : "\n\nLes “Last Friday Talk” : Le concept\n\nDepuis 10 mois désormais, chez M6Web, nous organisons chaque “dernier vendredi” du mois, une manifestation que nous avons nommée “Last Friday Talk”.\n\nLe concept : 2 heures de 13h30 15h30, où 4 sessions “type conférence” de 25 minutes, suivies de 5 minutes de questions, sont présentées par des personnes de la Direction Technique. La participation (orateur ou public) est bien entendue facultative.\n\nL’idée, est que chacun a quelque chose dire dans le web de nos jours, quelque chose présenter/partager aux autres. Soit un retour d’experience sur une techno, un outil, une méthodologie, soit même une présentation de ses développements ou projets passés.\n\nDes exemples ?\n\nQuelques exemples de présentations qui ont été faites :\n\n\n  ZéroMq, la bibliothèque réseau\n  Présentation du langage Python\n  VIM pour les nuls\n  Les méthodes agiles\n  Doctrine 2\n  La sécurité SQL\n  Le déploiement chez Facebook\n  La WebPerf avancée\n  Présentation de CoffeeScript\n  …\n\n\nL’interêt ?\n\nLes apports pour vos équipes sont nombreux :\n\n\n  Toute la direction technique participe et partage une partie de la veille technologique de chacun.\n  Les conférenciers améliorent leur communication de “groupe”.\n  Ils deviennent souvent le référent sur le sujet dans l’entreprise.\n  C’est du Team Building, et un rendez-vous mensuel avec vos équipes.\n  Et cela donne des idées à tous les autres développeurs pour de futurs projets (perso ou d’entreprise bien sûr), et attise leur curiosité.\n\n\nNous avons donc tous les mois entre 5 et 10 présentations proposées pour n’en choisir que 4, et une trentaine de participants par session au niveau du public, et nous filmons toutes les conférences, et les archivons sur un site interne.\n\nLes “Karaoké Slideshow” pour plus de fun !\n\nPour combler les mois les plus creux, nous avons aussi tenté une session “Karaoké Slideshow” qui fut hilarante (dans l’esprit des “Ignite Karaoké” pour ceux qui connaissent) ! \n Le concept : Entre 5 et 10 volontaires font une présentation tour tour, sur une série de 5 slides qu’ils n’ont jamais vus, dont chaque slide défile toutes les 15 secondes. Le thème est libre et est improvisé en fonction des slides !\n\nC’est un bel exercice d’improvisation et le fun est garanti :-)\n\nConclusion\n\nAlors si votre entreprise ou votre univers le permet, nous vous conseillons vraiment de tenter l’aventure. C’est très formateur, instructif, et intéressant pour tout le monde, et cela apporte une vraie culture de la veille dans votre environnement ;-)\n\n\n\nOlivier Mansour nous présente la Programmation Orienté Objet “Canada Dry” !\n\n"
} ,
  
  {
    "title"    : "M6Web au banquet de la cuisine du web",
    "category" : "",
    "tags"     : " conference, lcdw",
    "url"      : "/m6web-au-banquet-de-la-cuisine-du-web",
    "date"     : "November 23, 2012",
    "excerpt"  : "Une partie de l’équipe de M6Web était présente au banquet de la cuisine du web avec la fine fleur du web Lyonnais !\n\n\n\n",
  "content"  : "Une partie de l’équipe de M6Web était présente au banquet de la cuisine du web avec la fine fleur du web Lyonnais !\n\n\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conférence Europe 2012 : Day 3",
    "category" : "",
    "tags"     : " conference, velocity, webperf, mobile, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-3",
    "date"     : "October 9, 2012",
    "excerpt"  : "\n\nTroisième et dernière journée la Vélocity Europe.\n\nOn arrive déjà fatigué et gavé d’informations et idées en tout genre, mais on a hâte de démarrer cette journée ! :-)\n\n[Mobile] The Performance of Web Vs Apps, par Ben Galbraith et Dion Almaer (W...",
  "content"  : "\n\nTroisième et dernière journée la Vélocity Europe.\n\nOn arrive déjà fatigué et gavé d’informations et idées en tout genre, mais on a hâte de démarrer cette journée ! :-)\n\n[Mobile] The Performance of Web Vs Apps, par Ben Galbraith et Dion Almaer (Walmart.com)\n\nBen (@bgalbs) et Dion (@dalmaer) nous reprennent dans les grandes lignes, la conférence faite la Vélocity Us (voir le CR + la vidéo de ce talk : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-3-devops-webperf )\n\nL’idée est de comparer les experiences possibles sur WebApp et Apps Native, avec toujours cette comparaison très drôle entre le mode de distribution des apps natives ce que cela donnerait si les show tv devraient être distribués de la même manière en prenant l’exemple de la série Friends : Hilarant !\n\nVoir la vidéo ci-dessous vers 17min 30 :\n\n\n\nBen Galbraith et Dion Almaer (Source : https://royal.pingdom.com/2012/10/05/report-from-velocity-europe-day-3/ )\n\n\n\n[WebPerf] Lightning Démos, par Steve Souders et Patrick Meenan (Google)\n\nPatrick nous a montré les derniers ajouts intéressants de WebPageTest.org, avec notamment le “Block Ads Feature”, l’onglet “SPOF” dans les paramètres avancés pour tester si nos scripts tiers sont des SPOF sur nos sites (j’y reviendrai) … L’outil s’enrichit progressivement et reste toujours LA référence ultime du domaine !\n\n\n\nSteve Souders de son côté, est revenu sur un cas étudié la veille, à savoir l’implémentation d’un LazyLoader sur un caroussel, afin de déterminé via Browserscope.js si cela repoussait le OnLoad event, et c’est le cas !\n\nPetite parenthèse sur les caroussels, je vous invite lire cet article : Don’t Use Automatic Image Sliders or Carousels, Ignore the Fad\n\n\n\n[WebPerf] Do All Users Benefit Equally from Web Performance Optimizations? , par Arnaud Becart (ip-label)\n\nTalk sponsorisé assez intéressant, qui étudie les données récoltées par Ip-label afin de voir si tout le monde profite de la WebPerf de la même manière. La réponse est évidente, mais c’est intéressant de rappeler qu’il faut toujours comparer tests synthétiques au réel, et qu’en fonction du navigateur, du terminal, de la puissance de votre machine, …, des optimisations WebPerf auront un impact différent, du négatif au très positif.\n\n\n\n[DevOps] From DevOps to Operation Science, par Christopher Brown (Opscode)\n\nSon twitter : @skeptomai\n\nTalk orienté “Culture” intéressant par l’un des créateurs de EC2. Mon moment d’absence de la journée :)\n\n\n\n[WebPerf] Performance and Metrics on lonelyplanet.com, par Mark Jennings et Dave Nolan (Lonely Planet)\n\nRetour d’experience des gars de Lonelyplanet (sorte de Routard) très enrichissant. Notamment sur la façon de communiquer à des équipes non techniques, les différentes expérimentations réalisées, et le changement de culture opéré, l’utilisation de Graphite avec notamment les Holt-Winters …\n\n\n  “Being Right isn’t Always Enough” !\n\n\n\n  “Give your metrics a public presence”\n\n\nLes slides : https://fr.slideshare.net/mbjenn/performance-and-metrics-at-lonely-planet-14589911\n\n\n\nMark Jennings et Dave Nolan (Source : https://twitter.com/smcinnes/status/253805752312033280/photo/1 )\n\n\n\n[WebPerf] Third-Party Scripts and You, par Patrick Meenan (Google)\n\nPatrick Meenan nous parle ici de SPOF (Single Point Of Failure ou Point Individuel De Defaillance en français …) et des 3rd party scripts.\n\nL’idée est de montrer comment suivant l’intégration Javascript de scripts tiers, vous pouvez rendre l’affichage de votre site dépendant du bon fonctionnement des serveurs du script tiers.\n\nLes navigateurs mettent en général 20 secondes (45 sous mac et linux) avant de rejeter une connexion sur un script tiers en rade. Vous pouvez voir des vidéos de l’effet que ça peut avoir sur vos pages dans les slides.\n\nAfin de les détecter, il existe l’extension SPOF-O-Matic :\n\nhttps://chrome.google.com/webstore/detail/spof-o-matic/plikhggfbplemddobondkeogomgoodeg\n\nEn surfant, vous saurez rapidement si un SPOF est présent sur votre site ou non et combien de contenu il bloque, et pourrez générer un WebPageTest comparatif en simulant le plantage du script en question (Redirection sur domaine blackhole.webpagetest.org)\n\nPour régler ces problèmes, plusieurs solutions : Script à charger dynamiquement via Js de manière asynchrone, script avec async et defer, ou au pire, script avant le /body.\n\nLes slides : https://www.slideshare.net/patrickmeenan/velocity-eu-2012-third-party-scripts-and-you\n\n\n\n[Ops] How Draw Something Absorbed 50 Million New Users, in 50 Days, with Zero Downtime, par J Chris Anderson (Couchbase)\n\nNous arrivons à l’entrée de la salle où a lieu cette présentation, barré par des commerciaux CouchBase, nous empéchant de rentrer sans prendre le prospectus CouchBase et sans se faire scanner son QRcode présent sur nos badges … ça commence mal …\n\nAu bout de 2 minutes de talk par J Chris Anderson ( @jchris ) , co-fondateur de Couchbase, le malaise est confirmé : on ne parlera pas ici de Draw Something, mais de Couchbase 2.0 uniquement, le nom Draw Something n’étant là que pour appâter du client potentiel, et ça marche, la salle est comble …\n\nDifficile du coup d’être concentré dans cette approche plus que douteuse … les questions au final seront aussi assez violentes sur le sujet : “pourquoi appâter les gens avec Draw Something, si ça n’est que pour parler de CouchBase ?” La réponse est évasive … Nous n’avons pas eu le droit d’en parler …\n\nBref, le produit Couchbase a tout de meme l’air très intéressant, et plutôt costaud, avec de très chouettes Dashboard de monitoring temps réels built-in.\n\nJ’en sors quand même avec l’impression très désagréable de m’être fait piéger …\n\nLes slides (non dispo) ressemblait fortement à cette autre présentation de J Chris : https://speakerdeck.com/u/jchris/p/nosql-landscape-speed-scale-and-json\n\n\n\n[WebPerf] WebPagetest - Beyond the Basics, par Aaron Peters (Turbobytes), Andy Davies (Asteno)\n\nPas mal de conférences parlaient de WebPageTest, mais celle-ci promettait d’aller en profondeur. Le créateur n’a jamais caché son manque de talent pour les interfaces, et WPT regorge de richesses en tout genre cachées dans les méandres de ses pages :-)\n\nEnormement d’informations et tips sont donc présents dans les slides de cette conférence.\n\nPour rappel : instruction d’Andy pour monter une instance privée de WPT : https://andydavies.me/blog/2012/09/18/how-to-create-an-all-in-one-webpagetest-private-instance/\n\nLes slides : https://www.slideshare.net/AndyDavies/web-page-test-beyond-the-basics\n\n\n\nAndy Davies et Aaron Peters (Source : https://royal.pingdom.com/2012/10/05/report-from-velocity-europe-day-3/ )\n\n\n\n[DevOps] What HTTP/2.0 Will* Do For You, par Mark Nottingham (Akamai)\n\nL’une des conférences les plus intéressantes de la Vélocity pour ma part avec notamment l’annonce que HTTP/2.0 sera basé sur SPDY déj…\n\nMark Nottingham ( @mnot ), Chair of the IETF HTTPbis Working Group, excellent conférencier, nous explique donc ce que sera HTTP/2.0 :\n\n\n  Aucun changement la sémantique HTTP\n  Basé sur Speedy\n  Multiplexing (voir slide 22)\n  Header Compression, technique très intéressante, pour éviter de ré-envoyer les memes headers pour chaque requête HTTP\n\n\nPas mal de ressources sont disponibles sur son site : https://www.mnot.net/\n\nLes slides sont un modèle du genre, simples et efficaces : https://www.slideshare.net/mnot/what-http20-will-do-for-you\n\n\n\n[DevOps] Web &amp;amp; Native Cross-Platform Multiplayer, par Ashraf Samy Hegab (Orange)\n\nComment développer une expérience de Gaming multi-plateforme : Web, Android, Iphone ? C’est la question à laquelle Ashraf a essayé de répondre, sur cette dernière conférence avec un humour et une énergie très communicative.\n\nPas mal de bonnes ideés applicables au web traditionnel, sur une stack NodeJs/Mongo/Socket.io, pour faire la majorité du travail et communiquer avec les parties natives d’app Android et Ios.\n\nNous avons aussi fait une démo live sur le jeu Phone Wars (Disponible sur Appstore et Google Play) d’une éxperience Gaming Multi-plateforme.\n\nTrès rafraîchissant pour finir ces 3 journées marathons !\n\nConclusion :\n\nÇa y’est, la Vélocity Europe est finie pour cette année. Les bouchées doubles ont été mises par rapport à la Vélocity Berlin de l’année dernière, et cette conférence reste vraiment la conf incontournable pour tous ceux que la Webperf, les Devops, et les sites fort traffic intéressent !\n\nOn regrette simplement que seule la grande salle ait été filmée, que la chasse aux slides soit toujours aussi tordue (trop peu renseigné sur le site de la Vélocity). Le reste est juste parfait !\n\nA la prochaine, et merci pour vos retours.\n\nP.s: Merci aussi aux équipes de Pingdom pour leurs Twitt Live et les chouettes photos prises ( https://royal.pingdom.com/ )\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-2\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-1\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conférence Europe 2012 : Day 2",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-2",
    "date"     : "October 8, 2012",
    "excerpt"  : "\n\nDeuxième journée, avec le début “officiel” de cette conférence, où l’on nous donne rendez vous dans l’immense King’s Room.\n\nPour bien démarrer, on commence avec en quelque sorte l’hymne de la Vélocity : Speed &amp;amp; Velocity !\n\n\n\n[DevOps] Move Fa...",
  "content"  : "\n\nDeuxième journée, avec le début “officiel” de cette conférence, où l’on nous donne rendez vous dans l’immense King’s Room.\n\nPour bien démarrer, on commence avec en quelque sorte l’hymne de la Vélocity : Speed &amp;amp; Velocity !\n\n\n\n[DevOps] Move Fast and Ship Things, par Girish Patangay (Facebook)\n\nPremière présentation de la journée par l’un des Manager chez Facebook, (maintenant dans les bureaux londoniens), sur la capacité de Facebook, et leur volonté, d’ évoluer et de déployer rapidement.\n\nIl nous raconte les débuts de Facebook, avec peu de serveurs, des changements inférieurs 5Mb, Rsync pour pusher en prod etc … puis la migration vers HipHop.\n\nDésormais chaque changement nécessite de recompiler un gros binaire de 1.2Go, et d’y envoyer sur plus de 10 000 serveurs, et ce plusieurs fois par jour !\n\nAvec Bittorent, ils envoient 500Mb en moins d’une minute sur les 10 000 serveurs.\n\nOn a eu le droit une présentation de GateKeeper, outil interne, permettant de faire du feature flipping géolocalisé. La timeline a par exemple plus d’une centaine de GateKepper.\n\nAujourd’hui, Facebook cherche trouver le moyen de scaler de plus de 1000 développeurs à 10000, et d’évoluer sur ce système de “Move Fast” dans le mobile natif.\n\nPour ceux que ça intéresse, en plus de la vidéo çi dessous, Quora est une mine d’or d’infos sur FB : https://www.quora.com/Girish-Patangay\n\n\n\n(Source https://royal.pingdom.com/2012/10/03/report-from-velocity-europe-day-2/ )\n\n\n\n[WebPerf] Keynote KITE and MITE, par Robert Castley (Keynote)\n\nVient ensuite la conférence de Keynote (Sponsorisée), qui nous présente deux outils intéressant :\n\n\n  KITE pour Keynote Internet Testing Environnement : https://kite.keynote.com/\n  MITE pour Mobile Internet Testing Environnement : https://mite.keynote.com/\n\n\nMITE est d’ailleurs utilisé par le site de Google Howtogomo.com\n\nBref, si vous êtes sur Windows (…), jetez y un oeil. Plus d’infos en vidéo :\n\n\n\n[WebPerf] Lightning Démos\n\nDémo 1 : Chrome Dev Tools :\n\nPremière démo de Iliya Grigorik ( @igrigorik) sur les capacités avancées de la chrome Dev Toolbar.\n\nOn peut par exemple faire un clic droit sur l’onglet Network pour récupérer le har (voir le format HTTP Archive) en json, et utiliser d’autres outils avec ce har, notamment Yslow dont je parlais dans le compte rendu suivant, qui permet d’ajouter les régressions possible WebPerf dans votre CI Jenkins.\n\nOn parle aussi du chrome://tracing , du débugger mobile, que la devtools est une WebApp avec une url propre et scriptable, du Chrome Benchmarking (extension) …\n\nVoir la présentation suivante pour plus d’infos, bien plus compléte : https://www.igvita.com/slides/2012/devtools-tips-and-tricks/\n\n\n\nDémo 2 : Box Anemometer :\n\nGavin Towey ( @gtowey), DBA MySql chez Box.com nous présente une interface pour visualiser et traiter correctement le slow log de Mysql. Basé sur Php 5.3, les outils Percona et Bootstrap pour l’interface, l’outil est un vrai bonheur pour tous ceux qui font un peu d’optimisation MySql au quotidien, développeur, sysadmin ou dba. Nous l’avions déjà découvert aux DevOpsDays Mountain View cet été, et l’utilisons massivement depuis.\n\nLe projet est sur Github : https://github.com/box/Anemometer\n\nVoir aussi le projet Rain Gauge dans la lignée de Anemometer, toujours par Gavin Towey : https://github.com/box/RainGauge\n\nVoici la démo en vidéo :\n\n\n\n[WebPerf] Emerging Markets / Growth Markets, par Jeff Kim (CDnetworks)\n\nJeff Kim, Chief Operating Officer chez CDnetworks nous a partagé quelques données et chiffres intéressants sur les marchés émergeants comme l’Inde, Indonésie, les Philippines, le Brésil etc\n\nOn apprend que Chrome a une part de marché de 62% au Brésil, 51% en Inde, et Opéra de 26% en Russie. Que l’e-commerce au final n’a pas vraiment encore démarré en Inde, Brésil et Russie…\n\nAprès des études d’Eye Tracking View, on remarque aussi entre la population chinoise et américaine, que sur une page de résultats de recherche, les américains ne regardent que le coin haut gauche de la page pour se contenter des premiers résultats, alors que les chinois consultent vraiment toute la hauteur de la page, pour regarder tous les résultats.\n\n\n\n[WebPerf] Why page speed isn’t enough, par Tim Morrow (Betfair)\n\nSon twitter : @timmorrow\n\nAncien de ShopZilla, et déja présent Berlin l’année dernière, Tim a partagé son retour d’expérience sur la refonte de BetFair (très gros site de pari en ligne). Les gens se plaignaient d’une mauvaise expérience (alors que les pages refondues étaient plus rapides), mais nécessitaient à priori beaucoup plus de temps pour parvenir au pari final que dans l’ancienne version. En gros, le temps de chargement de vos pages n’est pas suffisant, il faut aussi regarder les scénarios fonctionnels de vos sites. Ils sont passés sur une navigation typée Ajax pour ne pas rafraîchir dans certains cas l’intégralité de la page.\n\nVoir les slides et la vidéo : https://fr.slideshare.net/timmorrow/why-page-speed-isnt-enough-tim-morrow-velocity-europe-2012\n\n\n\n\n[WebPerf] W3C Status on Web Performance, par Alois Reitbauer (Compuware, Dynatrace)\n\nBeau récapitulatif du status du “W3C performance working group” sur la performance Web, avec des rappels sur la NavTiming et les différents standards, quelques échanges autour de la NavTimingV2, d’une Resource time Measurement etc …\n\nSon twitter : @compuware et @AloisReitbauer\n\nPlus d’infos dans la vidéo ci dessous :\n\n\n\n[WebPerf] 3.5s Dash for attention and other stuff we found about RUM, par Philipp Tellis (Log Normal)\n\nSon twitter : @bluesmoon\n\nLe transcript et détail complet de la présentation est présent sur un blogPost très complet de Philip Tellis, créateur de Log Normal (racheté par SOASTA) :\n\nhttps://www.lognormal.com/blog/2012/10/03/the-3.5s-dash-for-attention/\n\nBeaucoup de chiffres et d’infos intéressantes autour de la WebPerf, avec notamment cette métrique basée sur un Bounce Rate &amp;gt;= 50%\n\nLes slides sont disponibles : https://fr.slideshare.net/bluesmoon/the-35s-dash-for-user-attention-and-other-things-we-found-in-rum\n\n\n\n[Mobile] Escaping the uncanny valley, par Andrew Betts (FT Labs)\n\nSon twitter : @triblondon\n\nConférence très intéressante par Andrew, le directeur d’FT Labs (Financial Times), qui a parcouru l’ensemble des travaux que ces équipes ont effectués autour des capacités mobile pour faire une webapp HTML5 intégrée dans une appli native avec la meilleure experience possible.\n\nAu menu, rappel sur la lourdeur de parcours du DOM en JS, les incohérences du SetTimeout (notamment sur IOs), les problématiques de l’AppCache ou localStorage. Le fait d’utiliser au maximum l’accélération matérielle sur les CSS, l’optimisation des paint, les spinners et loading bar, le progressive rendering …\n\nRappel de la latence sur les “clics” tactiles avec le projet Fastclick, qui enlève les 300ms de délai sur le clic mobile : https://github.com/ftlabs/fastclick/\n\n[Mobile] Make your mobile web apps fly, par Sam Dutton (Google)\n\nSon twitter : @sw12\n\nSam Dutton, Developers Advocate (?) chez Google, nous fait un parcours assez complet des bases d’optimisations WebPerf pour le mobile. Un peu de redondance versus d’autres confs vues plus tôt, mais le sujet est bien maîtrisé et bien couvert.\n\nQuelques petits outils intéressants, notamment le Multires pour la gestion des images Retina : https://fhtr.org/multires/\n\nA noter aussi une phrase que j’ai beaucoup aimé sur l’ergnonomie mobile, et la position des boutons de contrôle :\n\n\n  “Controls should be beneath content: think calculator”\n\n\nLes slides : https://www.samdutton.com/velocity2012/ \n\n[DevOps] Scaling Instagram, par Mike Krieger (Instagram)\n\nSon twitter : @mikeyk\n\nPetit debrief orienté Ops de la success story d’Instragram. Le rythme de la présentation était vraiment pushy, donc plutôt dur suivre pour nous autre francophones …\n\nEn plus d’avoir l’impression de voir un demi milliard bouger sur scène, nous avons quand même appris certaines choses sur la Stack Instagram : EC2, Python Django, Postgres, Gearman, RabbitMQ …\n\nLa présentation n’étant pas nouvelle, est disponible ici : https://fr.slideshare.net/iammutex/scaling-instagram\n\n\n\nMike Krieger (Source : https://royal.pingdom.com/2012/10/03/report-from-velocity-europe-day-2/ )\n\n\n\n[WebPerf] Bringing HTML5 into Nativelandia: A Tale of Caution, par Jackson Gabbard (Facebook)\n\nL’une de conférences que j’attendais beaucoup, de Jackson Gabbard, Mobile Engineer chez Facebook, qui nous explique le passage du HTML5 au native pour les applications mobile (IOs pour le moment?).\n\nPassé les stats toujours aussi hallucinantes, il explique tout ce qui n’était pas convainquant sur l’ancienne webApp HTML5, et que ce qui a vraiment échoué, est le marriage entre la WebView et le natif. Que l’expérience “native” est bien plus concluante pour l’utilisateur par rapport ce qu’ils souhaitent obtenir niveau fluidité, performances, efficacité, utilisation réseau etc …\n\nIls ont du coup redeveloppé pas mal de leurs outils pour s’adapter a ce mode de fonctionnement (un GateKeeper plus light) etc …\n\nConférence vraiment passionnante, avec un gars plutôt très transparent sur Facebook et les raisons qui ont poussées ce changement.\n\nConclusion :\n\nDu lourd encore une fois, avec énormement de sujets très intéressants, même si l’on commence à tourner un peu en rond autour de la WebPerf Mobile.\n\nA noter aussi qu’un grand nombre de livre Oreilly ont été donné et dédicacé par Steve Souders et John Allspaw ;-)\n\nLes salles sont déjà bien plus sympa, et l’organisation toujours au top ! Vivement demain.\n\nPour finir, voici quelques vidéos diffusées pendant les breaks la Vélocity :\n\n\nHot Wheels World Record: Double Loop Dare at the 2012 X Games Los Angeles\n\n\nJeb Corliss “ Grinding The Crack”\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-3\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-1\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conférence Europe 2012 : Day 1",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-1",
    "date"     : "October 4, 2012",
    "excerpt"  : "Nous voici de retour pour une Vélocity Conference, le paradis de la WebPerf et des Devops !\n\nAprès l’excellente moisson de la Vélocity US de Santa Clara, dont voici nos 3 CR :\n\n\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-da...",
  "content"  : "Nous voici de retour pour une Vélocity Conference, le paradis de la WebPerf et des Devops !\n\nAprès l’excellente moisson de la Vélocity US de Santa Clara, dont voici nos 3 CR :\n\n\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-2-devops-webperf\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-3-devops-webperf\n\n\nEt après la session de l’année dernière qui avait lieu à Berlin, nous nous retrouvons cette fois dans la capitale anglaise Londres, au Hilton hôtel.\n\nVoici le compte rendu des conférences de la première journée, journée un peu à part placée sous le signe des “Tutorials” (2 octobre 2012)\n\n[OPS] Monitoring and Observability in complex architecture, par Theo Schlossnagle (OmniTI)\n\nPremière conférence de la journée, avec Theo, habitué des Velocity, et plutôt expert dans les domaines “infra” et “monitoring”. Créateur de Omniti, MessageSystems et Circonus.\n\nSon twitter : @postwait\n\nTheo nous explique comment monitorer et observer des archi complexes avec une présentation très bas niveau.\n\nLes outils de collectes de statistiques qu’il cite :\n\n\n  Metrics.js : https://github.com/mikejihbe/metrics\n  Resmon : https://labs.omniti.com/labs/resmon\n  Folsom : https://github.com/boundary/folsom\n  Metrics : https://metrics.codahale.com/\n  Metrics-net : https://github.com/danielcrenna/metrics-net\n  StatsD : https://github.com/etsy/statsd\n\n\nEt pour le stockage :\n\n\n  Reconnoiter : https://labs.omniti.com/labs/reconnoiter\n  Graphite : https://graphite.wikidot.com/\n  OpenTSDB : https://opentsdb.net/\n  Circonus : https://circonus.com/\n  Librato : https://metrics.librato.com/\n\n\nOnt suivi ensuite des démos tcpdump assez poussées plutôt intéressante :\n Par exemple, pour voir les nouvelles connexions entrantes (récupération des packets SYN) : tcpdump -nnq -tttt -s384 ‘tcpport 80 and(tcp[13] &amp;amp; (2|16) == 2)’\n\n\n  d’exemples sont disponibles dans les slides, avec aussi d’autres exemples live sur “strace” et “dtrace”.\n\n\nBref, ca commence très (trop?) fort, surtout pour nous, pas forcément de nature très “OPS” !\n\nLes slides sont disponible ici et plutôt parlantes pour ceux qui voudraient creuser le sujet : https://www.slideshare.net/postwait/monitoring-and-observability\n\n\n\n\n  “You cannot correct what you cannot measure” Theo Schlossnagle\n\n\n\n\nTheo Schlossnagle (Source : https://img.ly/o0Ht )\n\n[DevOps] Escalading Scenario : a deep dive into outage falls, par John Allspaw (Etsy)\n\nOn prend toujours autant de plaisir écouter John Allspaw, VP d’Esty et Co-organisateur de la Vélocity avec Steve Souders, nous parler d’incident, et de la meilleure manière de les gérer.\n\nSon Twitter : @allspaw\n\nBeaucoup de parallèles sont faits avec des incidents dans l’aviation, l’industrie, voir l’armée, avec des références aussi au PostMortem d’appolo 13 … Un joli condensé de bouquins “Must Read” d’après Allspaw comme : Normal Accidents ou encore Naturalistic Decision Making, nous sont présentés.\n\nDifficile d’en faire un résumé, tellement la conférence était bourrée d’informations, graphiques et anecdotes en tout genre ! Malgré tout, dans les “choses” retenir, en vrac :\n\n\n  Attention au contexte de vos graphiques : un graph qui parait anormal sur une heure, peut s’avérer normal sur une échelle de temps d’une journée par exemple\n  Des bons conseils sur la résolution d’incident en équipe, notamment au niveau de la communication avec des exemples de conversation pendant des incidents chez Etsy trop ambigus. Il est important de confirmer les réponses, corriger la communication des autres, afin d’éviter tout soucis de compréhension\n  Ne pas hésiter à demander des “pre-mortem”. Dire à l’auteur du projet par exemple, que cela va planter dans plusieurs mois, et lui demander d’essayer de trouver la ou les raisons qui pourraient amener le projet au plantage.\n  …\n\n\nJe vous invite encore consulter les slides pour plus d’informations et de références : https://fr.slideshare.net/jallspaw/velocity-eu-2012-escalating-scenarios-outage-handling-pitfalls\n\n\n\n\n\nJohn Allspaw (Source : https://twitter.com/lozzd/status/253074489540239360 )\n\n[WebPerf] Running a WebPerf Dashboard in 90 minutes, par Jeroen Tjepkema\n\nSon twitter : @jeroentjepkema\n\nL’objectif de cette conférence était de proposer en 90 minutes, les étapes nécessaire pour monter un dashboard orienté WebPerf.\n\nOn re-parcourt du coup un peu tout le classique de la performance web, en présentant déjà quelques exemples de Dashboard (rien de très sexy …, hormis peut-être celui de Nrc.nl orienté “audience éditoriale” plutot intéressant), la pertinence de certains types de “graphs” comme les “heatmap” et aussi en comparant les différentes solutions pour mesurer la performance web, avec avantages et inconvénients :\n\n\n  Synthetic Monitoring (Gomez, Keynote, IpLabel, Pingdom etc) (slide 104-105)\n  Real User Monitoring (LogNormal dont l’acquisition par Soasta a aussi été annoncé ce jour, Boomerang.js, Torbit, Google Analytics …) (slide 121-122)\n  Real User Benchmarking (WebPageTest) (slide 134)\n\n\nQuelques idées sympa de design sont disséminées tout au long de cette longue présentation, on regrette simplement de survoler toujours un peu tous les concepts, mais malgrés tout, cela reste l’une des rares tentatives de faire un dashboard WebPerf accessible à des “non-techniciens”. Chapeau pour cela.\n\nUne démo du dashboard est testable ici : https://app.measureworks.nl/secured/dashboard (Login : demo@measureworks.nl , password: performance )\n\nLes slides sont disponibles ici : https://www.slideshare.net/MeasureWorks/measureworks-velocity-conference-europe-2012-a-web-performance-dashboard-final\n\n\n\n\n\nJeroen Tjepkema (Source : https://twitter.com/pingdom/status/253135289327951872 )\n\n[WebPerf] Deep Dive into Performance analysis, par Steve Souders et Patrick Meenan (Google)\n\nLeurs Twitter : @souders et @patmeenan\n\nDernier “Tutorials” du jour avec Steve Souders (Chief performance Officer chez Google), et Patrick Meeman (aussi chez Google désormais, créateur de Webpagetest).\n\nHistoire de s’adapter au public Anglais, les deux compères ont décidés de s’attaquer aux sites des équipes de Premier League du Foot Anglais :-)\n\nComme imaginé, ca n’est pas “folichon”, et on étudiera en profondeur en live les sites de Chelsea et Tottenham, qui chacun enchaine un nombre d’aberrations plus grandes les une que les autres !\n\nLe tableur utilisé pendant la présentation avec les liens vers les tests WebPageTest : goo.gl/YfbRn\n\nQuelques exemples sur le site de Chelsea : https://www.chelseafc.com/\n\nLes tests WPT annoncent un Load Time 21 secondes, 203 requêtes HTTP et 3mo4 téléchargés !\n\nLe Waterfall que vous pouvez voir ici, est on ne peut plus parlant, avec une mention spéciale pour la liste des images présentes sur la HP : https://velocity.webpagetest.org/pageimages.php?test=120925_0_13&amp;amp;run=2&amp;amp;cached=0\n\nJ’imagine que le problème va vous sauter aux yeux, entre les fonds énormissimes et images non compréssées, les images de chacun des joueurs de l’équipe (oui, le pauvre carroussel en bas de page …), et le nombre incroyable de logo et ou picto en tout genre, on voit vite comment améliorer la page :-)\n\nLe carroussel du haut donne aussi un effet assez comique sur le “FilmStrip View” où l’ont voit vers les 10 secondes, un début d’image se charger, pour s’effacer car le carroussel passe déja au panel suivant … Merci au passage au jCaroussel qui charge bêtement toutes les images …\n\nOn remarque aussi un nombre conséquent de JS qui retarde grandement le Start Render. Optez autant que possible pour les positionner juste avant le /body, ou les charger en async/defer ou via un chargement asynchrone en Js.\n\nPas mal de petites astuces sont partagées par Patrick et Steve, notamment sur l’utilisation de la courbe de Bande Passante, pour voir les parties pouvant être optimisées (celle ou la bande passante n’est pas utilisée fond par exemple), on remarque aussi quelques ajouts récents comme l’affichage des évenements Paint sur la FilmStrip View (Screenshot encadré de Orange), ou encore la possibilité via clic droit dans la Chrome Dev Tools de vider le cache et les cookies rapidement etc …\n\nNous avons aussi eu la confirmation que Google prenait le Onload Time comme référence pour ses algorithmes de ranking.\n\nBref, superbe application de tous les concepts WebPerf avec des cas concrets d’étude et une conférence très interactive avec suffisament d’astuces pour combler aussi les habitués de WPT.\n\n\n\nPatrick Meenan &amp;amp; Steve Souders (Source : https://twitter.com/simonox/status/253146271156670464 )\n\n[DevOps] Ignite Talk\n\nPour finir cette journée, rendez vous dans l’immense salle (dans laquelle aura lieu la majorité des conférences suivantes), pour un Ignite Talk combiné entre la Vélocity Conférence et Strata Conférence qui ont lieu au même moment dans l’Hilton hôtel Londres.\n\n\n\nSalle King’s Room (Source : https://twitter.com/cmsj/status/253139957093367808/photo/1 )\n\nNous suivrons une série de 7 ou 8 Ignite Talk dont le concept est de présenter un sujet sur 20 slides défilant automatiquement toutes les 15 secondes. C’est assez décalé, fun, et l’exercice parait très “sport” … :)\n\nPas mal de sujets tournaient autour de l’Open Data ou Big Data, les DataViz …\n\nVoici par exemple un talk sympa sur les “Dataviz as interface” par @makoto_inoue : https://fr.slideshare.net/inouemak/data-viz-asinterfacemakotoinoue\n\n\n\n\n\nMakoto Inoue, de l’énergie, de la danse, et de la bière ! (Source: https://royal.pingdom.com/2012/10/02/velocity-europe-1/ )\n\n[WebPerf] Step by Step Mobile Optimization, par Guy Podjamy (Akamai)\n\nConférence auquelle je n’ai pas pu assisté : https://fr.slideshare.net/guypod/step-by-step-mobile-optimization\n\n\n\nConclusion :\n\nBonne première journée avec déjpas mal de choses retenir et appliquer quotidiennement !\n\nOn regrette le fait d’avoir été dans des petites salles (sûrement à cause de la dernière journée de la Strata Conférence), et du coup d’avoir alterné le manque de place, avec la chaleur des salles … Et je n’ai pas l’impression que les sessions du jour étaient filmées malheuresement !\n\nC’est en tout cas un très bon avant-gout de ce qui nous attend demain ;-)\n\nEnjoy !\n\nP.s : N’hésitez pas nous faire des retours sur ce CR ! :)\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-2\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-3\n\n"
} ,
  
  {
    "title"    : "Tentative d&#39;explication des Fast-Forward sous Git",
    "category" : "",
    "tags"     : " git",
    "url"      : "/tentative-d-explication-des-fast-forward-sous-git",
    "date"     : "July 13, 2012",
    "excerpt"  : "\n\nTous les projets M6Web sont passés récemment sous le système de gestion de contenu Git.\n\nGit, c’est super cool ! On peut faire facilement des branches, les “merger” les unes aux autres et “switcher” d’une branche une autre. Pratique donc (dans l...",
  "content"  : "\n\nTous les projets M6Web sont passés récemment sous le système de gestion de contenu Git.\n\nGit, c’est super cool ! On peut faire facilement des branches, les “merger” les unes aux autres et “switcher” d’une branche une autre. Pratique donc (dans l’idée) !\n\nIl a été finalement assez facile de se faire au vocabulaire et au fonctionnement de git. Je ne dis pas que je ne fais pas non plus mes commit sur la bonne branche chaque fois, mais on arrive tout de même assez facilement à s’en sortir (“git reset –help” si vous êtes dans ce cas) !\n\nLe très intéressant article “A successful Git branching model” a mis en avant la gestion des fast-forward, cela dit l’utilité m’est resté assez floue et ne couvrait pas l’ensemble de mes questions.\n\nJ’ai donc fouillé la documentation de Git afin de débroussailler les “fast-forward”.\n\nQue fait git lors d’un merge\n\nGit “merge” deux branches lorsque :\n\n\n  La commande “merge” est utilisée, par exemple git merge feature-myfeature,\n  la commande pull est utilisée, exemple git pull origin master\n\n\nGit “fast-forward”\n\nMettons que Bob fasse une modification sur une branche, il crée un commit Y.\n\nIl fait une autre modification qu’il commit, il crée alors un commit Z.\n\nAutomatiquement git “fast-forward”, c’est dire qu’il fait pointer la branche, qui pointait sur le commit Y, vers le commit Z. Sur les graphiques de git log, les deux commit sont liés par un trait continu.\n\n- Y - Z\n\nTant que Bob continue faire des modifications + commit sans toucher au fast-forward, git va automatiquement “fast-forwarder”. On aura donc un enchaînement de commit qui sont liés par un trait continu.\n\n- Y - Z - AA - AB - AC - ...\n\nGit ne “fast-forward” pas\n\nNous sommes maintenant sur une branche qui en est la révision X.\n\nAlice travaille sur son projet et crée la révision A.\n\nCela dit, Bob travaille aussi sur le projet et crée la révision B.\n\nAlice pousse ses modifications :\n\nLe commit A a pour parent le commit X, qui est le dernier commit connu par la branche, Git peut donc “fast-forwarder”.\n\n- X - A\n\nBob pousse ses modifications :\n\nLe commit de bob ne connaît pas de commit A dans son historique (son commit parent est le commit X).\n\n\n- X - A\n   \\\n    B\n\nSi git “fast-forwardait” ici, il ferait pointer la branche sur le commit B, et perdrait le commit A. Comme on ne souhaite pas perdre les modifications d’Alice, il va donc passer en mode “no fast-forward” automatiquement.\n\nGit va donc récupérer les modifications de A et les mélanger (merge) aux modifications de B en créant un commit C.\n\nLe commit C a pour parent les commit** B** et** A, le pointeur de dernier commit peut donc être placé sur **C sans risque de perte d’historique.\n\n- X - A\n   \\   \\\n    B - C\n\nOption* –no-ff*\n\nL’auteur de l’article cité précédemment conseille d’utiliser l’option –no-ff sur les merge.\n\nCette option force git a créer un commit de “merge” qui aura pour parents notre commit de modification, et le dernier commit connu sur la branche, même si il n’y a pas eu de modification sur cette dernière.\n\nCela permet de revenir facilement à la version antérieure de la branche, sans avoir à fouiller dans les nombreux commits ayant pu amener un bug : on revient à la version initiale avant de passer plus de temps pour corriger le bug.\n\n- M1 -- -- -- -- M2\n    \\            /\n     B1 - B2 - B3\n\nDans l’exemple ci-dessus, on peut facilement revenir au commit M1, et exclure ainsi toute la branche B. Si on avait “fast-forwardé”, il nous aurait fallut retrouver le commit M1 en regardant tous les commit précédent.\n\nMode auto contre –no-ff\n\nForcer le *–no-ff *ne sera finalement utile que lorsque vous développez une fonctionnalité pour laquelle vous allez beaucoup “commiter”, sans que personne d’autre ne commit entre-temps.\n\nA vous de l’utiliser de manière intelligente !\n\nSources\n\n\n  nvie.com : A successful Git Branching Model\n  git push –help\n\n\n"
} ,
  
  {
    "title"    : "Retrouvez l&#39;intervention du CTO de M6 Web, Martin Boronski, à la table ronde du Forum PHP 2012",
    "category" : "",
    "tags"     : " forumphp, afup, video, php",
    "url"      : "/retrouvez-l-intervention-du-cto-de-m6-web-martin-boronski-a-la-table-ronde-du-forum-php-2012",
    "date"     : "July 11, 2012",
    "excerpt"  : "\n\n",
  "content"  : "\n\n"
} ,
  
  {
    "title"    : "Intégration continue avec Jenkins et Atoum",
    "category" : "",
    "tags"     : " php, atoum, jenkins, ci",
    "url"      : "/integration-continue-avec-jenkins-et-atoum",
    "date"     : "July 11, 2012",
    "excerpt"  : "\n\nChez M6 Web nous tentons de créer une approche open-source intra entreprise. L’objectif est que certains composants génériques adaptés notre métier puissent être crées et diffusés largement parmis les dizaines de projets gérés chaque année. Un p...",
  "content"  : "\n\nChez M6 Web nous tentons de créer une approche open-source intra entreprise. L’objectif est que certains composants génériques adaptés notre métier puissent être crées et diffusés largement parmis les dizaines de projets gérés chaque année. Un prochain post traitera de cette problématique.\n\nDans cette optique, il faut nous assurer de la qualité et la non régréssion de ces composants. Pour cela nous avons mis en place Jenkins afin d’assurer l’intégration continue de nos tests unitaires. Voici un exemple d’intégration avec Atoum (ce n’est pas forcement la meilleur méthode, n’hésitez pas à la commenter).\n\nStructure du composant :\n\n\n  ./src contient les classes du composants au format PSR-0\n  ./tests contient les TU Atoum\n  ./build-tools/jenkins contient les fichiers de configuration pour Atoum et Ant\n  ./vendor contient les dépendances externes du projet (gérées avec Composer)\n\n\nVoici le composer.json utilisé.\n\n\n\nVoici le fichier de configuration de Atoum : build-tools/jenkins/atoum.ci.php et celui de jenkins build-tools/jenkins/build.xml\n\n\n\n(cette configuration inclut l’ensemble des outils d’analyse statique que l’on utilise)\n\nEnfin, voici la configuration faire sur Jenkins (en image).\n\n\n\n\n\n\n\nVia cette conf on obtient le résultat des tests (naturellement) ainsi que la couverture de code des tests avec la coloration des lignes couvertes ou non couvertes dans les classes testées.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 3 (DevOps/WebPerf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops, mobile",
    "url"      : "/cr-velocity-conference-2012-day-3-devops-webperf",
    "date"     : "June 28, 2012",
    "excerpt"  : "Dernière journée de cette monstrueuse conférence qu’est la Vélocity Conférence.\n\nOn commence dans la joie et la bonne humeur avec la “Seven Databases Song” :D\n\n\n\n[Mobile WebPerf] The Performance of Web vs. Apps, par Ben Galbraith (Walmart.com) &amp;am...",
  "content"  : "Dernière journée de cette monstrueuse conférence qu’est la Vélocity Conférence.\n\nOn commence dans la joie et la bonne humeur avec la “Seven Databases Song” :D\n\n\n\n[Mobile WebPerf] The Performance of Web vs. Apps, par Ben Galbraith (Walmart.com) &amp;amp; Dion Almaer (Walmart.com)\n\n\n\nPetit sujet assez trollesque sur les WebApps vs Apps. Conférence hyper énérgétique et très drôle ! Notamment le passage à 12mn dans la vidéo, où l’on compare le mode de distribution des apps natives à ce que cela donnerait si les show tv devraient être distribués de la même manière en prenant l’exemple de la série Friends : Hilarant !\n\nL’idée intéressante sur la fin du talk, concerne le rendu de l’application, qui grâce Node.js (dispo désormais en v0.8.0 enfin) peut être aussi bien fait coté client que serveur suivant le client qui demande. A creuser.\n\n\n\n[WebPerf] Akamai Internet Insights, Stephen Ludin (Akamai)\n\n\n\nPetit talk de Stephen Ludin “Chief Architect for Akamai’s Site Acceleration and Security group”.\n\nAprès une présentation assez hallucinante en quelques chiffres du traffic et des données qui passent chez Akamai :\n\nToutes les 60 secondes =&amp;gt; 1 milliard 3 de logs, + de 6200 heures de vidéos streamés …\n\nIl a aussi partagé une initiative louable et très intéressante sur un projet de partage des données récoltées chez Akamai : https://www.akamai.com/io\n\nOn y observe quelques statistiques (relativement peu date) sur les browsers notamment. On voit d’ailleurs quelque chose d’assez fun sur les IE8 : chaque weekend, on apercoit une baisse de présence sur IE8 (qui se retrouve sur d’autres navigateurs plus récent) … Bref, on voit encore que c’est le monde de l’entreprise qui ralenti la propagation des navigateurs récents !\n\nSource : \nhttps://www.akamai.com/html/io/io_dataset.html#stat=browser_ver&amp;amp;top=5&amp;amp;type=line&amp;amp;start=20120601&amp;amp;end=20120626&amp;amp;net=n\n\nEt slides ici : \nhttps://assets.en.oreilly.com/1/event/79/Akamai%20Internet%20Insights%20%20Presentation.pptx\n\nLightning Demos, par Marcel Duran (Twitter Inc.), Nat Duca (Google), Lindsey Simon (Twist)\n\n\n\nEnsuite, viennent trois sessions de Lightning Talk : 5 minutes pour présenter un sujet.\n\nOn commence par Marcel Duran, créateur de Yslow, célèbre extension WebPerf de Firebug à l’origine, qui fait son petit bonhomme de chemin depuis :\n\n\n  Disponible dans quasiment tous les browsers\n  Ruleset personnalisable (cf C3PO voir plus bas)\n  Une version en ligne de commande (en Node.Js) pour extraire les données YSlow partir d’un HAR : https://github.com/marcelduran/yslow/wiki/Command-Line-%28HAR%29\n  Un serveur Node.js que vous pouvez tester ici nécessitant aussi un HAR : https://yslow.nodester.com/\n  et le meilleur pour la fin, une version pour Phantom.Js (Projet très impressionnant d’Headless Browser) qui vous permet de simplement mentionner l’url et d’avoir le résultât en sortie ! Avec en plus la possibilité via le format TAP (Test Any Protocol), d’intégrer les résultats dans votre Intégration Continue pour éviter les régressions. Juste ultime, tout est expliqué sur ce Github : https://github.com/marcelduran/yslow/wiki/PhantomJS\n\n\nJ’ai hâte d’implémenter tout ca chez M6Web :) Une vidéo à voir donc absolument :\n\n\n\n\n\nOn continue dans le lourd, avec Nat Duca qui travaille sur le développement du navigateur Chrome et qui nous démontrer une feature très bas niveau mais au combien intéressante : le chrome://tracing/\n\nCette fonctionnalité va vous permettre de profiler les actions du navigateur au plus bas niveau possible. Encore un excellent nouvel ajout au niveau du panel d’outillage du browser Chrome destination des développeurs. Voir vidéo ci dessous :\n\n\n\n\n\nEt pour finir cette jolie session de Lightning Talk, Lindsey Simon, nous présenté Browserscope : https://www.browserscope.org/\n\nOutil dont la puissance et l’interêt pour tout développeurs Front-end Desktop ou Mobile n’est plus démontrer.\n\nSi vous ne connaissez pas, passez 5 minutes de votre temps sur cette vidéo :\n\n\n\n[WebPerf] Browsers, par Luz Caballero (Opera Software), Tony Gentilcore (Google), Taras Glek (Mozilla Corporation)\n\nPetite déception sur cette classique des Vélocity, où les talks ce sont plutôt concentré sur les nouveautés des browsers mobile de Google et Opéra Mini, et où le gars de Mozilla n’a pas jouer le jeu et préféré parler de la lenteur du SetTimeout Javascript ainsi que de l’api LocalStorage …\n\nSlide Mozilla : \nhttps://people.mozilla.com/~tglek/velocity2012/#/step-1\n\nSlide Opéra mini avec notes : \nhttps://www.slideshare.net/gerbille/speed-in-the-opera-mobile-browsers-13476236\n\nConcernant Google, la conférence par Tony Gentilcore (créateur de FasterFox pour ceux qui ce souviennent) était plus intéressante, déja par l’annonce suivante :\n\n\n  Chrome for Android will be the default browser starting with Jelly Bean\n\n\nTony Gentilcore\n\nIl a aussi parlé du fonctionnement de WebKit, du Compositor Thread, ainsi que du Chrome Remote Debugging\n\nPour info, Google a peu de temps après annoncé la présence de Chrome sur iOs !\n\n[DevOps] Simple log analysis and trending, par Mike Brittain (Etsy)\n\n\n\nOn retrouve Mike sur un sujet un peu différent : Comment analyser des logs Apache pour en sortir des graphites. Quelques astuces sur la fonction PHP apache_note() sont mentionnées, sur le traitement des logs avec les commandes linux “awk” et “sed”, et l’utilisation assez étonnante de Gnuplot pour grapher : https://www.gnuplot.info/ !\n\nLes slides sont dispos ici : https://www.mikebrittain.com/blog/2012/06/22/velocity-2012/ , et les codes d’exemples sur Github : https://github.com/mikebrittain/presents\n\nEncore pas mal d’idées piocher ! (Ca commence faire beaucoup d’idées …)\n\n\n\n[WebPerf] Social Button BFFs, par Stoyan Stefanov (Facebook)\n\nStoyan n’est plus présenter dans l’industrie des performances web. Il est désormais chez Facebook, travailler sur les performances des plugins, dont le “Like” ! Suivez le sur Twitter, c’est bourré de superbes infos @stoyanstefanov ainsi que son blog : https://www.phpied.com/ !\n\nL’idée du talk est de faire en sorte que les boutons sociaux (et widgets tiers) en général, deviennent vos BFF ! (Best Friend Forever :D) : https://www.phpied.com/social-button-bffs/\n\nIl explique de quel manière doit-on intégrer ces widgets sur nos sites, et vous permet de le vérifier par l’extension Chrome qu’il a développé 3PO#Fail (3PO = 3rd Party Optimization) ou via une extension de RuleSet pour YSlow.\n\nLes slides ici : https://www.slideshare.net/stoyan/social-button-bffs\n\n\n  “Friends don’t let friends do document.write” Stoyan Stefanov\n\n\n[WebPerf] 5 Essential Tools for UI Performance, par Nicole Sullivan (Stubbornella)\n\nEncore un excellent talk pour ce dernier jour avec Nicolas Sullivan, Experte et consultante dans l’optimisation CSS, sur le fonctionnement très précis de la gestion des CSS par vos navigateurs et toutes les optimisations récentes qu’ils y ont apportées, ainsi qu’une démo (qui fait toujours son petit effet dans une salle Geek) de Tilt sur Firefox\n\nLes slides ne sont malheuresement pas encore en ligne, mais cela ne devrait tarder sur son Slideshare.\n\nVous pouvez retrouvez l’idée du talk sur l’interview ci dessous réalisée elle aussi lors de la Vélocity.\n\n\n\nConclusion\n\nVoil, c’est terminé pour ce compte rendu en 3 actes de ce que j’ai vécu et retenu cette Vélocity Conférence 2012. François prendra le relais pour présenter sa vision d’autres talks, mais orientés Ops (Sysadmin).\n\nJ’espère que ces comptes rendu auront servi à partager quelques outils, liens ou best practices qui vous donnerons des tonnes d’idées de nouvelles choses à faire coté Web dans votre société. De mon coté, comme la Vélocity Berlin l’année dernière, j’ai appris beaucoup et apprécié une grande partie des conférences. Cette conf reste pour moi (et nous chez M6Web) la plus importante au monde sur les aspects de Performance.\n\nPour finir, je vous remercie pour vos retours (et je vous invite à continuer m’en faire un maximum) et lectures. Vous pouvez en connaitre d’avantage sur les autres talks avec quelques vidéos gratuite disponible sur \nhttps://www.youtube.com/playlist?list=PL80D9129677893FD8, Ainsi que les slides qui continuent d’arriver sur\nhttps://velocityconf.com/velocity2012/public/schedule/proceedings\n\nEt pour ceux que ca intéresse, sachez que Oreilly mettra disposition un pack complet des vidéos pour généralement un tarif autour des 400$, et qu’une Vélocity Europe aura lieu Londres les 3 et 4 octobre 2012.\n\nMerci tous !\n\n\n  CR Vélocity Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n  CR Vélocity Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-2-devops-webperf\n\n\n(Crédit photo : https://www.flickr.com/photos/oreillyconf/sets/72157630300659948/)\n\n\nLe Job Board assez hallucinant !\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 2 (DevOps/WebPerf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops, mobile",
    "url"      : "/cr-velocity-conference-2012-day-2-devops-webperf",
    "date"     : "June 28, 2012",
    "excerpt"  : "Compte rendu des tracks DevOps/WebPerf de cette deuxième journée de cette Vélocity Conférence à Santa Clara (Californie) qui marque l’ouverture “officielle” de la conférence, la veille étant considérée comme des conférences bonus orientées Tutoria...",
  "content"  : "Compte rendu des tracks DevOps/WebPerf de cette deuxième journée de cette Vélocity Conférence à Santa Clara (Californie) qui marque l’ouverture “officielle” de la conférence, la veille étant considérée comme des conférences bonus orientées Tutoriaux.\n\nLa matinée offrait un track unique dans une salle gigantesque.\n\n\n\nVidéo d’intro à la Vélocity !\n\n\n\nL’ouverture officielle est donc présentée par Steve Souders (Google) et John Allspaw (Etsy), toujours dans un show l’américaine, et même déguisés. \n\n\nS’enchaine ensuite un condensé de session plutôt courte par des acteurs très prestigieux du web.\n\n(Crédit photo : https://instagr.am/p/MV5xAAJLSt/ )\n\n[DevOps] Building for a billion Users, par Jay Parikh (Facebook)\n\nPremière conférence du matin, avec une présentation du “VP of Infrastructure Engineering at Facebook”.\n\nOn suis avec attention, une présentation très dense de l’infrastructure de Facebook, avec quelques chiffres hors normes.\n\nLa philosophie Facebook est présentée en 4 points :\n\n\n  Focus on Impact\n  Move Fast\n  Be Bold\n  Be Open\n\n\nAvec une explication sur les fameux Bootcamp cher Facebook, formation obligatoire auquelle tout le monde participe en rentrant chez Facebook.\n\nUne présentation très brève des outils internes utilisés et développés par Facebook : Perflab, GateKeeper (sorte de Feature Flipping), Claspin, Tasks, SevManager …\n\nUne explication sur les procédures de déploiement chez Facebook et leur gestion du cache, sur leur volonté de constamment tout refaire, pour toujours être meilleur.\n\n\nEt pour finir, une anecdote assez drôle sur un incident ayant eu lieu chez Facebook, où toutes les fonctionnalités “non terminées” se sont un jour retrouvées en production.\n\nBref, une conférence intéressante, mais très dense, dont je vous conseille de regarder la vidéo çi dessous.\n\n\n  “Fix More, Whine less.” Jay Parikh\n\n\n\n\n[DevOps] Investigating Anomalies, par John Rauser (Amazon)\n\nBelle surprise de la journée, avec cette conférence sur la gestion d’incident, qui raconte l’histoire de l’épidémie de Cholera ayant eu lieu à Londres en 1854, et comment John Snow, trouver l’origine de l’épidémie, en se concentrant sur les données, et non pas seulement sur les chiffres.\n\n\n  “Explaining anomalies often makes your theroy bulletproof” John Rauser\n\n\nUne deuxième partie était concentrée sur le fait d’étudier les extremités sur vos échantillons de manière à trouver ce qui n’allait pas. Point de vue très instructif.\n\nLa vidéo ci dessus est un “must-see” de la Vélocity.\n\n\n  “Look at the extremes and you’ll find things that are broken” John Rauser\n\n\n\n\n[DevOps] Building Resilient User Experiences, par Mike Brittain (Etsy)\n\nLe message autour de cette présentation, est que votre application DOIT s’adapter aux incidents. Si possible faire en sorte que cela ne soit même pas percu par la plupart de vos internautes. En découpant chacune des fonctionnalités de votre site, vous devez pouvoir ne pas afficher celle qui ne fonctionne pas correctement sans que cela impact vos utilisateurs (Graceful Degradation).\n\nLes slides sont disponible ici : https://www.slideshare.net/mikebrittain/building-resilient-user-experiences-13461063\n\n\n\n[WebPerf] Predicting User Activity to Make the Web Fast, par Arvind Jain (Google), Dominic Hamon (Google)\n\nLa présentation commence avec un rappel sur “How Fast is the web today”. \n En quelques chiffres :\n\n\n  Chrome ~2.3s/5.4s page load time (median/mean)\n  Google Analytics ~2.9s/6.9s page load time (median/mean)\n  Mobile ~4.3s/10.2s page load time (median/mean)\n\n\nD’autres infos sont partagées venant du très utile HttpArchive …\n\nOn assiste ensuite la présentation des fonctionnalités de “Prefetch” de google et du Prerendering en place dans la barre de recherche de Chrome : “Omnibox”, ceci ayant pour but de rendre le web encore plus rapide.\n\nTout cela donne des idéées sur la façon de prédire ce que vont faire les internautes, et sur nos gestions d’“autocomplete”.\n\n\n\n\n\n[WebPerf] Performance Implications of Responsive Web Design, par Jason Grigsby (Cloud Four)\n\nUne autre conférence que j’attendais grandement, sur le Responsive Web Design. Le sujet est parfaitement maitrisé, et tous les reproches que je peux faire cette techno en ce moment, sont mentionnés, expliqués, et certaines solutions ou idées sont données ! Du tout bon.\n\nA retenir, la méthode conseillée qui est de faire du Mobile First Responsive Web design, c’est dire commencer par la version mobile, puis faire la version web, et non l’inverse.\n\nLes slides ici : https://speakerdeck.com/u/grigs/p/performance-implications-of-responsive-design\n\nLa conférence n’étant pas disponible en vidéo, vous pouvez déjécouter Jason Grisby lors d’une interview suite sa conférence en vidéo si dessous.\n\n(Crédit photo : https://www.flickr.com/photos/stuart-dootson/4024407198/ )\n\n\nJason Grigsby interview la Vélocity Conf 2012\n\n\n\n[WebPerf] RUM for Breakfast - Distilling Insights From the Noise, par Buddy Brewer (LogNormal), Philip Tellis (LogNormal, Inc) &amp;amp; Carlos Bueno (Facebook)\n\nRUM aka Real User Monitoring est un terme qui est revenu très régulièrement durant cette Velocity. Nous avions pour cette conférence notamment, deux personnes de LogNormal dont le créateur de Boomerang.js : https://yahoo.github.com/boomerang/doc/ et Carlos Bueno de Facebook.\n\nLa présentation expliquait comment mesurer des métriques de performances venant d’utilisateurs réel, comment analyser toutes les données, en filtrer le “bruit”, et quoi en tirer. Le tout était très instructif, surtout sur la partie filtrage de données (Band Pass Filtering, IQR Filtering ..).\n\nSlides : https://www.slideshare.net/buddybrewer/rum-for-breakfast-distilling-insights-from-the-noise\n\n\n\n[WebPerf] Rendering Slow? Too Much CSS3? Ask RSlow, par Marcel Duran (Twitter Inc.), David Calhoun (CBS Interactive)\n\nOn retrouve ici une présentation assez fun du résumé de la conf sous forme de Waterfall (voir photo).\n\nLe talk a ensuite abordé les notions de rendering au niveau CSS avec au départ un cas d’étude : Réaliser le logo de ySlow en CSS3 entièrement. On observe de manière assez drôle le rendu finale dans les différents navigateurs (éclat de rire général sur IE off course). Vous pouvez les retrouver sur les slides ci dessous.\n\nLa conférence part ensuite sur quelques tests réalisés sur Chrome uniquement (à prendre donc avec des pincettes) sur les performances CSS3 de chacun de ces cas :\n\n\n  background-image vs css3 gradient\n   vs css background-image\n  @font-face vs  vs sans-serif\n  animated gif vs css3 spinner\n\n\nL’étude est intéressante, et mériterais d’être creusée sur d’autres navigateurs, mais cela est rendu très difficile par le fait que seul Chrome sait exporter les données de rendu de sa Timeline …\n\nLes slides sont dispo ici : \nhttps://docs.google.com/presentation/d/1b7rdeXYdmL3lmT24GAaC14eOSkq5qt6FM-yLSeFykQk/edit?pli=1\n\n[WebPerf] Time To First Tweet, par Dan Webb, par (Twitter Inc) &amp;amp; Rob Sayre (Twitter)\n\nDan et Rob nous parle performances coté client chez Twitter, et la réecriture du Front-End.\n\nLa notion de Time To First Tweet, correspond au temps de navigation jusqu’a l’affichage du premier twiit sur la Timeline. Cette mesure est prise grace à la Navigation Timing Api, supportée dans IE&amp;gt;=9, Firefox &amp;amp; Chrome notamment.\n\nTwitter à aussi abandonné progressivement, l’utilisation des hashbangs (les #! dans l’url), pour utiliser la PushState Api, ainsi que le templating coté client (Mustache.js et Hogan.js) pour repasser sur un templating serveur avec leur migration de Ruby vers Java, avec au final 75% de temps client gagné sur le 95th Percentile !\n\nConférence très intéressante, notamment sur la manière de charger les Javascripts.\n\nPlus de détail sur le blog technique de Twitter : https://engineering.twitter.com/2012/05/improving-performance-on-twittercom.html\n\nLes slides sont disponible : https://speakerdeck.com/u/danwrong/p/time-to-first-tweet\n\nConclusion Day 2 :\n\nEncore une journée riche en informations et idées. Le rythme étant beaucoup plus soutenu, et la fatigue s’accumulant, il n’était pas évident d’être à 100% dans chaque talk :-)\n\nEn attendant le CR Ops, et celui du Day 3, vous pouvez relire le CR du Day 1 : \nhttps://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n\nP.S : Retrouvez moi sur twitter : @kenny_dee\n\nPlaylist Youtube Velocity US 2012\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 1 (Dev/Webperf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, mobile",
    "url"      : "/cr-velocity-conference-day-1-dev-webperf",
    "date"     : "June 27, 2012",
    "excerpt"  : "Nous voici à Santa Clara, CA, ce lundi 25 juin pour notre première Velocity Conference (Web Performance &amp;amp; Operations) aux states (nous avions déjà pu avoir un aperçu l’année dernière avec la première Velocity Europe Berlin).\n\n\n\nL’évenement se ...",
  "content"  : "Nous voici à Santa Clara, CA, ce lundi 25 juin pour notre première Velocity Conference (Web Performance &amp;amp; Operations) aux states (nous avions déjà pu avoir un aperçu l’année dernière avec la première Velocity Europe Berlin).\n\n\n\nL’évenement se situe au Convention Center, et la première chose que nous remarquons, c’est la taille démesurée du lieu ! Et pour cause, 800 personnes sont attendues !\n\nPour cette première journée sous le signe des tutoriaux, il y avait entre trois et quatre tracks parallèles de 90 minutes chacun, dont un réservé aux sponsors. Dur de faire des choix parmi toutes les confs et le programme alléchant de la journée !\n\nJe m’oriente donc sur le coté Dév / WebPerf / Monitoring, pendant que mon collègue, Francois, part sur le coté operations qu’il couvrira dans une autre série de CR.\n\n[WebPerf] Understanding and Optimizing Web Performance Metrics, par Bryan McQuade de chez Google\n\nAu menu :\n\n\n  Explication des métriques de performance orientées réseau\n  Fonctionnement du parser HTML5\n  Explication des métriques de performance orientées rendu\n  Démo de Critical Path Explorer (PageSpeed Online)\n  Optimisation de l’affichage perçu utilisateur\n\n\nLes slides parlent d’elles même et sont disponibles ici : \nhttps://perf-metrics-velocity2012.appspot.com .\n\nElles parcourent l’intégralité des notions de WebPerf existantes “à jour”, dont certaines peu connues comme la Speculative loading, et permettent surtout de comprendre ce qu’elles signifient très précisément.\n\nA retenir aussi le SSL Server Test ici https://www.ssllabs.com/ssltest/\n\nC’est donc un Must Read pour tout ceux que la WebPerf intéresse.\n\n**Page Speed Insights : **\n\nNous avons eu droit ensuite une démo très intéressante de la fonctionnalité Critical Path Explorer (déjà entraperçue en version béta à la Velocity Europe), et qui sera je pense lancée officiellement demain.\n\nEn attendant et pour la tester : \nhttps://developers.google.com/speed/pagespeed/insights?velocity=1\n\nCette fonctionnalité permet, comme son nom l’indique, de montrer le chemin critique de votre page. Sur les quelques tests que j’ai pu effectués, c’est très efficace. On apprécie le détail au niveau du waterfall sur l’exécution des javascripts, l’affichage de “qui bloque quoi”, ou le render css. A approfondir de toute urgence !\n\n\n\nBryan McQuade (Google)\n\n\nLa Lightning Démo de Page Speed ayant eu lieu le lendemain\n\n[WebPerf] A Web Perf Dashboard: Up &amp;amp; Running in 90 Minutes, par Cliff Crocker et Aaron Kulick.\n\nL’idée ici était de montrer en 90 minutes avec quels outils obtenir un dashboard orienté WebPerf, qui sera fourni comme une VM à la fin de la session.\n\nAprès une longue présentation orale d’outils plutôt connus désormais comme :\n\n\n  Boomerang.Js\n  WebPageTest instance privée + API\n  Piwik (clone de Google Analytics)\n  StatsD (collecteur pour Graphite)\n  Graphite\n  REDbot.org\n  cUrl\n  ShowSlow\n  …\n\n\nLes deux conférenciers nous présentent un site réalisé pour l’occasion : “Sally Squirrel’s Dance Emporium”, hommage aux gifs animés d’écureuils dansant, et nous font une démo (un peu capricieuse) de leur dashboard basé sur Piwik, alternative Google Analytics avec un système de “plugin” visiblement pour aggréger un peu le tout.\n\nL’idée est clairement bonne, le résultat ne m’a pas convaincu titre personnel. On vante au départ de la présentation, le faite qu’une image bien choisie suffit au monitoring, et qu’un dashboard ne doit pas être complexe, et au final, on se retrouve avec un dashboard remplis d’images en tout genre, de données tabulaires, … complexe quoi …\n\nPiwik en alternative Analytics ?\n\nJe doute aussi de la robustesse de Piwik que nous avions déjà étudier, et voir des pages listant les temps de latence ou de chargement utilisateur par utilisateur, me fait réellement peur avec une audience dépassant la centaine de personnes la journée …\n\nPour l’anecdote, sur le wiki de Piwik, on lit cette phrase que je vous laisse apprécier : “If your website has more than a few hundreds visits per day (bravo!), waiting for Piwik to process your data may take a few minutes”\n\nJe vous invite tout de même à lire les slides : \n[https://assets.en.oreilly.com/1/event/79/A%20Web%20Perf%20Dashboard_%20%20Up%20%20Running%20in%2090%20Minutes%20Presentation.pptx](https://assets.en.oreilly.com/1/event/79/A%20Web%20Perf%20Dashboard%20%20Up%20_%20Running%20in%2090%20Minutes%20Presentation.pptx)\n\nAinsi qu’à tester la VM mise à disposition, car le travail derrière est conséquent, et peut correspondre à certains, ou peut au moins donner des idées pour d’autres : https://t.co/uLv1fX1A\n\n**A retenir : **\n\nA retenir aussi dans cette présentation, tout le bien qui a été dit de Graphite (même si je ne suis plus convaincre la dessus), et de quelques features pour lesquelles j’étais passée travers :\n\n\n  Support du SVG ( &amp;amp;format=svg) qui va enfin nous permettre de tester l’enrichissement des graphs par du contenu “connexe” (liste des erreurs 404 sur le graph lorsque l’on clic sur un point, nom du développeur ayant fait la mise en production etc)\n  Les fonctions de HoltWinter afin d’avoir des tendances hautes et basses pour mieux savoir quand alerter par exemple.\n\n\n[WebPerf] How to Walk Away From Your Outage Looking Like a HERO par Teresa Dietrich (WebMD), Derek Chang (WebMD)\n\nL’une des conférences que j’attendais beaucoup : le titre annonçait un talk sur la gestion d’incident avec un coté humoristique.\n\nLes deux conférenciers ont présentés des templates très complet de gestion d’incident qu’ils réalisent pour des posts-mortems qu’on peut retrouver ici : https://www.teresadietrich.net/?page_id=37\n\nPersonnellement, il me parait très important de réaliser des posts mortems. Mais si c’est pour passer plus de temps à rédiger des rapports d’incidents trop complet qu’on ne relira jamais qu’a en tirer un quelconque bénéfice, cela me parait inutile.\n\nDu coup, la première demi-heure a consisté à présenter ces templates, lire et expliquer quelques incidents ayant eu lieu chez WebMD.\n\nJ’ai, comme une bonne partie de la salle, fait l’impasse rapidement : entre le sujet dans lequel je ne suis jamais rentré ainsi que des slides avec beaucoup de texte illisible passés les premiers rangs de la très grande salle, je n’ai pas accroché.\n\nA revoir tête reposée : https://velocityconf.com/velocity2012/public/schedule/detail/23615 (slides non dispo à l’heure actuelle)\n\n\n\nTeresa Dietrich (WebMD), Derek Chang (WebMD)\n\n[WebPerf] The 90-minutes Mobile optimization life cycle par Hooman Beheshti (VP strangeloop)\n\nConférence orientée WebPerf Mobile.\n\nIci, on retrouve tout ce que j’aime dans les conférences Vélocity :\n\n\n  Un conférencier avec un grand talent d’orateur : précis, drôle et captivant\n  Un sujet très maitrisé\n  Des slides propres et parlantes même sans avoir assisté au talk\n  Des débats lancés …\n\n\nL’idée était de partir d’un site, au hasard Oreilly.com, puis le site mobile velocityconf.com par la suite, et de démontrer les étapes d’optimisation WebPerf, étape par étape, avec à chaque fois, ce que l’on souhaite obtenir, ce que l’on obtient réellement, et une comparaison vidéo du changement.\n\nDes points ont été approfondis comme la gestion du cache, via LocalStorage, du fonctionnement des CDN (pour le mobile), du Pipellining HTTP, de la congestion TCP etc …\n\nBeaucoup d’outils ont aussi été mentionnés pour la WebPerf mobile :\n\n\n  Chrome remote debugging : Http://developers.google.com/chrome/mobile/docs/debugging/\n  iWebInspector for IOS simulator : www.iwebinspector.com\n  Weinre : Remote debugging from the desktop for what the phone is doing : https://people.apache.org/~pmuellr/weinre/\n  Aardwolf : Remote js debugging lexandera.com/aardwolf\n  Mobile Perf Bookmarklet : stevesouders.com/mobileperf/mobileperfbkm.php\n  Pcap2har : Turn packet captures to waterfalls https://pcapperf.appspot.com\n  …\n\n\nBref, un très bon panorama pour la WebPerf mobile avec deux cas concret d’étude, chacun avec deux approches différentes :\n\n\n  Améliorer les métriques de WebPerf pour le site d’Oreilly\n  Améliorer la perception utilisateur sans regarder les métriques pour le site mobile de la Vélocityconf\n\n\nSlides dispo ici : https://www.strangeloopnetworks.com/blog/the-90-minute-mobile-optimization-life-cycle/\n\nJe vous invite aussi à regarder son interview ci dessous.\n\n\n\n\n\n[Event] Akamai Pool Party\n\nCette journée touche à sa fin avec une Pool Party extérieure par Akamai avec un orchestre (qui nous joué notamment le thème de Mario ! Très fun), beaucoup à boire, et beaucoup à manger (légumes tremper dans du brie chaud, WTF ?). L’occasion de rencontrer quelques sponsors et 2 autres français. :-)\n\n\nDevOps drinking session / @jstinson\n\n[Event] Ignite Sessions\n\nA 19h30 avait lieu les Ignite sessions, des confs “lightning talks” de 5 minutes sur des sujets divers, certains très intéressant comme :\n\n\n  le “Perceptual Diff” par un ingénieur de chez Google, pour être alerté (par l’intégration continue) lorsque la page du Service customers de chez Google change. Photos de la présentation ici + https://pdiff.sourceforge.net/\n  les #lolops, avec une série de Twitt orientée Devops à mourir de rire : Voir ici https://www.slideshare.net/cwestin63/lolops-a-years-worth-of-humorous-engineering-tweets\n  …\n\n\nLe concept est vraiment efficace avec des speakers ultra dynamique et pour la majorité très drôle.\n\nMention spéciale pour la partie centrale, où 11 personnes de la salle (dont certains speakers, Allspaw et Souders en tête), avait 1 à 2 minutes pour improviser sur des slides plutôt très drôle qu’ils n’avaient jamais vu.\n\nJ’espère que les vidéos seront disponible car c’était juste hilarant au possible. Je ne m’attendais pas à pleurer de rire non stop ici :-)\n\n\n\nConclusion\n\nExcellente première journée, déjà des tonnes d’infos à condenser / retenir, et ce n’était que le premier jour !\n\nSinon l’organisation est impeccable, lieu exceptionnel, wifi public qui fonctionne, repas de très bonne qualité (et table qui plus est), pas mal de multiprises dans les salles, à boire à volonté … La grande classe !\n\nLes comptes rendus des prochaines journées et des sessions orientés Ops suivre ;-)\n\nN’hésitez pas à faire un maximum de retour sur ce compte rendu, cela nous aidera et nous motivera pour les prochains ;-)\n\nP.S: Retrouvez moi sur Twitter : @kenny_dee\n\n"
} ,
  
  {
    "title"    : "Monitoring applicatif : Pourquoi et comment ?",
    "category" : "",
    "tags"     : " monitoring, graphite, statsd, conference",
    "url"      : "/monitoring-applicatif-pourquoi-et-comment",
    "date"     : "June 26, 2012",
    "excerpt"  : "Voici les slides de la présentation que j’ai donnée au Forum PHP 2012, et au WebEvent 4 :\n\nVous êtes développeur, chef de projet technique ou même responsable et vous souhaitez avoir de la visibilité sur le fonctionnement de vos applicatifs, ou su...",
  "content"  : "Voici les slides de la présentation que j’ai donnée au Forum PHP 2012, et au WebEvent 4 :\n\nVous êtes développeur, chef de projet technique ou même responsable et vous souhaitez avoir de la visibilité sur le fonctionnement de vos applicatifs, ou sur la plateforme sur laquelle ils sont hébérgés ?\n\nNous étudierons comment, grâce à des outils simples (StatD / Graphite / Log BDD) et nos expériences chez M6Web, mettre en place un monitoring applicatif ultra complet.\n\nCe monitoring vous permettra de retrouver la vue sur vos projets, pour mieux anticiper la charge, detecter la root cause en cas d’incident et connaitre l’état de chacun de vos services ..\n\nMonitoring applicatif : Pourquoi et comment ?\n\n \n"
} ,
  
  {
    "title"    : "M6Web au Web Event Lyon #4",
    "category" : "",
    "tags"     : " webevent",
    "url"      : "/m6web-au-web-event-lyon-4",
    "date"     : "June 20, 2012",
    "excerpt"  : "\nUne partie de l’équipe de M6 Web au webevent de La ferme du Web.\n\n",
  "content"  : "\nUne partie de l’équipe de M6 Web au webevent de La ferme du Web.\n\n"
} ,
  
  {
    "title"    : "M6 Web était présent au Forum PHP 2012",
    "category" : "",
    "tags"     : " afup, forumphp, conference",
    "url"      : "/post/24732185644/m6-web-tait-pr-sent-au-forum-php-2012",
    "date"     : "June 9, 2012",
    "excerpt"  : "Voici quelques photos des membres d’M6Web prises lors du Forum PHP 2012.\n\nKenny a animé une conférence sur le monitoring applicatif.\n\n\n\nMartin, notre CTO, a participé une table ronde.\n\n\n\nOlivier était également présent en tant que membre de l’Afup...",
  "content"  : "Voici quelques photos des membres d’M6Web prises lors du Forum PHP 2012.\n\nKenny a animé une conférence sur le monitoring applicatif.\n\n\n\nMartin, notre CTO, a participé une table ronde.\n\n\n\nOlivier était également présent en tant que membre de l’Afup\n\n\n\nAinsi que Didier et Julien, développeurs dans nos équipes R&amp;amp;D, et trop préssés d’assiter toutes les conférences pour être photographiés.\n\n"
} ,
  
  {
    "title"    : "M6Web au Forum PHP 2012 et au WebEvent #4",
    "category" : "",
    "tags"     : " php, afup, monitoring, conference",
    "url"      : "/post/24184111542/m6web-au-forum-php-2012-et-au-webevent-4",
    "date"     : "June 1, 2012",
    "excerpt"  : "Cette année, M6Web sponsorise deux événements français majeurs dans le monde du Web :\n\n\n  le Web Event à Lyon #4 (https://event.lafermeduweb.net/ au centre de congrès de Lyon le 15 juin 2012)\n  et le forumPHP 2012 (https://afup.org/pages/forumphp2...",
  "content"  : "Cette année, M6Web sponsorise deux événements français majeurs dans le monde du Web :\n\n\n  le Web Event à Lyon #4 (https://event.lafermeduweb.net/ au centre de congrès de Lyon le 15 juin 2012)\n  et le forumPHP 2012 (https://afup.org/pages/forumphp2012/ la Cité universitaire à Paris le 5 &amp;amp; 6 juin 2012) !\n\n\nA cette occasion, M6Web sera bien représenté :\n\n\n  je (Kenny Dits) présenterais chaque évènement des sessions sur le monitoring applicatif en regard de ce que nous faisons au quotidien chez M6Web. https://afup.org/pages/forumphp2012/sessions.php#632\nhttps://event.lafermeduweb.net/les-sessions#c6\n  Olivier Mansour en tant que Vice Président de l’Afup parlera lors de la Keynote de cloture du Forum Php le 6 juin. https://afup.org/pages/forumphp2012/sessions.php#732\n  Martin Boronski, notre directeur technique participera également à la table ronde DSI organisée par l’AFUP Paris le 6 Juin.\n\n\nRendez-vous là bas ? ;-)\n\nBannière du Forum PHP 2012\n\n[\n\nBannière du Web Event Lyon\n\n[\n\n"
} ,
  
  {
    "title"    : "Performances PHP chez M6Web",
    "category" : "",
    "tags"     : " graphite, monitoring, nodejs, php, varnish, webperf, conference",
    "url"      : "/post/23671071384/performances-php-chez-m6web",
    "date"     : "May 24, 2012",
    "excerpt"  : "Voici les slides de la présentation du 23 mai réalisée à l’Epitech de Lyon.\n\nC’est un retour d’expérience, qui survole un peu tous les axes sur lesquels nous travaillons chez m6web, ayant trait aux optimisations de nos sites.\n\nJ’espère que certain...",
  "content"  : "Voici les slides de la présentation du 23 mai réalisée à l’Epitech de Lyon.\n\nC’est un retour d’expérience, qui survole un peu tous les axes sur lesquels nous travaillons chez m6web, ayant trait aux optimisations de nos sites.\n\nJ’espère que certains points feront l’objet d’autres articles dans le futur ;-)\n\nPerformances php chez M6Web\n&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;\n\n"
} ,
  
  {
    "title"    : "Lancement du blog technique d&#39;M6Web",
    "category" : "",
    "tags"     : " ",
    "url"      : "/post/23664141031/lancement-du-blog-technique-dm6web",
    "date"     : "May 24, 2012",
    "excerpt"  : "Bienvenue sur le blog de la direction technique de M6 Web.\n\nVous retrouverez ici, une fréquence qu’on espère des plus régulières, quelques articles et autres retours d’expérience de nos équipes technique.\n\nAttendez vous à manger du PHP, Mysql, Nod...",
  "content"  : "Bienvenue sur le blog de la direction technique de M6 Web.\n\nVous retrouverez ici, une fréquence qu’on espère des plus régulières, quelques articles et autres retours d’expérience de nos équipes technique.\n\nAttendez vous à manger du PHP, Mysql, Node.js, entendre parler de performance, monitoring, vidéo, html5 etc ;-)\n\nBonne lecture à tous.\n\n"
} 
  
  
  
]
