[
  
  {
    "title"    : "QCon London 2024",
    "category" : "",
    "tags"     : " cloud, devops, conference, 2024",
    "url"      : "/2024/04/17/qcon-london-2024.html",
    "date"     : "April 17, 2024",
    "excerpt"  : "We really enjoyed this QCon conference in London. It’s based on 5 tracks a day, covering resilience, scalability, architecture, monitoring, performance, management and all the subjects that speak to tech companies. None of these tracks are sponsor...",
  "content"  : "We really enjoyed this QCon conference in London. It’s based on 5 tracks a day, covering resilience, scalability, architecture, monitoring, performance, management and all the subjects that speak to tech companies. None of these tracks are sponsored: they’re all about feedback, with no marketing whatsoever. There is a 6th track where sponsorship is permitted, and it’s the only one. There aren’t many sponsorship stands, and you go to this conference to talk to your peers first and foremost.\n\nWe were able to focus on feedback from teams facing similar problems to those we face at Bedrock Streaming. It was an inspiring conference, which reinforced some of our choices and gave us ideas for others.\n\nThis article will summarise what we have learned from each theme.\n\nPerformance\n\nNot all optimisations are necessarily relevant, and it is important to measure their benefits and compare their costs. Some make the code more complex, while others lead to weaknesses in resilience. It’s all about compromise.\n\nThink about the main case, rather than anticipating all the possible exceptions, which can complicate the use in very isolated cases.\n\nProfiling is the key to your journey to optimization. However with one tool you are sure where problems lie but with a second one (or more) doubt creeps in… It is interesting to associate benchmarks before and after each optimization. And beware as the famous Pareto Principle,-aka the 80-20 rule-, as it turns out easy fixes are often the most efficient.\n\nYou can’t perform profiling? No problem just follow metrics and identify when a code change impacts your performances. What again? I can’t hear you with all this noisy data! Oh yes you are right, noise in your data is indeed a pain in the backend but you have to deal with it as it is commonplace. I know, this is scary, because you don’t come from a data engineering world but not worries, statistics are not so hard to tame. Go take a look at Change Point Detection, a non-parametric test which helps you test if these changes you saw in your metrics are relevant.\n\nLet’s now deep-dive in a more opaque layer of our system composed by the kernel. Here we have to admit that the heuristics at stake while for example choosing on which CPUs or GPUs to execute are not easily tweakables, despite the fact that it was shown they indeed impact overall performances.\n\nOne of the promising solutions is the famous tool cilium/eBPF which allows you to directly reprogram certain behaviours of your kernel, especially the network (for the cilium part), without having to recompile the kernel.\n\nHowever, some think that regarding these kernel issues we are stuck in a local maximum and it will require a mindset shift to fully overcome these hardware performance issues.\n\nScaling\n\nAround scaling, we attended both technical and organisational talks.\nOn the technical side, there are a number of relevant ideas. Always contain your costs - especially in the Cloud, with all its hidden expenses. Aiming for the highest possible SLO is a higher priority than cutting costs.\n\nSome incidents are an accumulation of many changes and are not linked to one team/one change/one regression. Those problems lead to long investigations and cannot be resolved by a simple rollback. This counterpart comes naturally with innovation.\n\nArchitects or equivalent must always coordinate internal exchanges: timeout durations, number and relevance of retries, etc.\n\nAlso, standardising tools brings more benefits than choosing a highly optimised tool that is different for each team. Try to reuse code and don’t reinvent the wheel.\n\nSeveral presenters spoke of asynchronous and ‘real-time’ scalability (meaning millions of users in a matter of seconds). The bottom line for all of them is that there is no way of scaling the necessary resources so quickly: everyone pre-scales. The pre-scaling itself can take a long time, so for a particular event (such as a sports match), resources need to be launched several hours in advance. Asynchronous processing can help to spread out all the non-critical calculations over time.\n\nDon’t forget to be patient! It really seems to go against the tide, yet designing long-time running APIs (meaning APIs that can handle waiting) really seems to be the key of success when trying to address scalability issues. Likewise try as much as possible not to overlap your APIs field of expertise.\n\nOn the organisational level, architects of all kinds must identify or create links between all elements in the ecosystem.\n\nThe architect’s vision is to understand the choices and make compromises between the teams so that the whole is coherent. There is no lock-in. Every choice brings both positives and negatives.\n\nSustainability\n\nIncluding sustainability in our business processes is a recent but no less important necessity. And do you know what? As it turns out, it often means cutting costs, increasing performances and resiliency!\n\n\n\nWhat we can sum up is that we have the power to act at our own level. Choose services that auto-scale, prefer ARM instances, select partners who measure and try to improve their Carbon emissions. In many cases, Serverless or Compute@Edge can be a very good choice for applications that can take huge load spikes as much as a few requests (quickly scalable and extremely elastic). When possible, try to run jobs during the night, when load spikes are over.\n\nThe aim is to use energy when it is really needed: switching off dev and staging environments when employees are resting. Limit over-provisioning of resources and pre-scaling times.\nAll cloud providers include a sustainability section in their ‘well-architected’ programs, a section that is independent of performance and costs, even though it provides co-benefits with those two subjects.\n\nIt really is in companies’ interests to move in this direction.\n\nManagement, People and  Process\n\nMaybe first keep in mind that decisions are not right or bad, they just are taken by a person or a quorum based on the context they had on a given time. And let’s face it, your organisation will need to change over time especially if you begin as a start-up and now need to scale-up (oh an interesting topic of scaling coming back!).\n\nAs for scaling your infrastructure, you need to realise that you will need to make compromises. For example, Trainline shared with us that their three pillars are: Alignment, Productivity and Code Quality. You can’t perform in the three at the same time, you need to choose which one(s) to let go a little in order to achieve the company’s goal. Importantly what is working at a given time in a certain situation could not be appropriate anymore months later.\n\nPlatform Engineering\n\nThe last day of the conference offered a dedicated track on platform engineering, where we listened to the insights shared by the speakers. As they delved into their own experiences and learnings, we found striking parallels with our own journey at Bedrock over the past few years.\n\n\n\nOne of the most resonant themes was the importance of treating the platform as a product: a perspective shift that has significantly influenced our approach. By emphasising the need for a smoother experience that balances consistency, performance, security, and development speed, the speakers reinforced the notion that our platform isn’t just a set of tools; it’s a crucial enabler for our developers’ success.\n\nThe concept of providing a golden path for developers stood out as particularly impactful. We’ve come to realise that by streamlining the onboarding process and offering clear guidelines, we can empower developers to navigate the platform with confidence and efficiency. This approach not only accelerates time-to-market for new features but also fosters a sense of trust between the platform team and its users.\n\nAnother key takeaway was the importance of communication and trust-building within the organisation. We were reminded that trust is the currency of collaboration, and effective communication is essential for maintaining it. Clear documentation, transparent processes, and accessible points of contact were highlighted as critical components of this effort.\n\nSurveys emerged as a valuable tool for gathering feedback and gauging user satisfaction, although the speakers acknowledged the challenges in ensuring high engagement and confidence in the results. However, they emphasised that even imperfect data can provide valuable insights when interpreted thoughtfully and supplemented with other forms of feedback, such as user interviews and proactive communication.\n\nThe speakers also shared their experiences with empowering developers to take ownership of certain tasks, such as fixing issues surfaced by conformity checks and contributing features that solved their own challenges back up to the general platform to benefit the whole organisation. This aligns closely with our own DevOps principles, emphasising collaboration and shared responsibility across teams.\n\nAs we reflect on the insights shared by the conference speakers, we’re reminded of the importance of continuous improvement and iteration in our platform engineering efforts. By learning from our mistakes, embracing new perspectives, and staying attuned to the needs of our users, we can continue to evolve our platform into a truly indispensable resource for our development community.\n\nConclusion\n\nIt was our first QCon for most of us. We appreciated the transparency of the speakers and the commitment of the organisers, who made the event a success on every level: the intensity of the conferences, the quality of the talks, the really good and varied food and the discreet presence of the sponsors.\n\nWithout being very technical, this conference is a source of inspiration for us, as it focuses on feedback and encourages discussion with our peers.\n\n"
} ,
  
  {
    "title"    : "KubeCon Europe 2024, Paris",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, 2024",
    "url"      : "/2024/04/09/kubecon-europe-2024.html",
    "date"     : "April 9, 2024",
    "excerpt"  : "Two weeks ago, six of us were at KubeCon in Paris. For a few, it was their first KubeCon! For one, it was the fourth one. Since Copenhagen in 2018, which was before we started moving to AWS and Kubernetes, so much has changed – and so much has not...",
  "content"  : "Two weeks ago, six of us were at KubeCon in Paris. For a few, it was their first KubeCon! For one, it was the fourth one. Since Copenhagen in 2018, which was before we started moving to AWS and Kubernetes, so much has changed – and so much has not ;-)\n\nBig ideas\n\nFirst of all, these four days clearly highlighted some wide topics.\n\nAI and GPUs everywhere\n\nSeems like “AI” was the impossible-to-miss subject vendors were trying to push, this year! Still, while many are running experiments, not many have deployed and maintained workloads in production yet.\n\nLike many, we have started using a few “AI” tools and are working on some ideas (not hosting them ourselves yet, though), but we can’t help but feel “AI” and especially the “Generative AI” term is a bit over-used and some solutions we’ve seen were closer to good-old-ML.\n\nAs we are not planning on running our GenAI workloads on Kubernetes anytime soon, we haven’t been to many talks on this matter. Still, between re:Invent last year and KubeCon this year, you’ll find plenty of contents if that’s your thing 💪.\n\nPlatform engineering is the new DevOps?\n\nNext up, looks like DevOps is dead and Platform Engineering is the new DevOps!\n\nOn one hand, we clearly see having everything related to all your projects in one place brings tremendous value (and we have a couple dashboards that go this way). Same with providing a common basis (Terraform, Golang Skeleton, Helm chart) most projects build upon instead of re-inventing it again and again.\n\nOn the other hand, we don’t think, today, “being able to create a new service in one click and 15 seconds” is a goal we should aim for. We’d rather rationalize micro-services and reduce network overhead. Remember, distributed systems are “easy”.\n\nStill, Backstage has gained in traction over the past two years, and it would be the first tool we’d try if (or when) we’d wanted to invest on this path.\n\nGreen Computing\n\n⏩ Heating Pools with Cloud Power: A New Wave in Green Computing - Saiyam Pathak, Civo &amp;amp; Mark Bjornsgaard, Deep Green Technologies\n\nNowadays, data centers’ electricity consumption uses 2% of the world’s electricity and the demand is growing exponentially due to more and more data centers being built. A very large percentage of future consumption will be dedicated to AI through high consumption GPUs.\n\nIn this context, Deep Green is embracing a generational shift as a company building a new kind of data centers. They aim to build small edge data centers as close as possible to heat consumers in order to recapture and distribute the heat. Indeed, in classic data centers, a huge chunk of energy is wasted into heat not being used. Chance with datacenter is that a very large amount of heat is produced in the same place (in comparison with other kind of industry). Deep Green’s computers are immersed into biodegradable oil which captures generated heat. Then, heat is transferred through heat exchangers to targets which can be swimming pools, private houses, offices… The process is simple and known for decades, but the smart move is to associate small edge data centers to heat consumers.\n\n\n\nAnd what about CNCF and kubernetes? CNCF tools are allowing them to make this work as efficient as possible by:\n\n\n  monitoring heat consumption and carbon emissions with tools like Kepler\n  adapting workloads sizes\n  managing intelligent workload scheduling: scheduling the workload to the datacenter which is actually requiring to heat up a particular consumer (using ArgoCD deployment, for instance)\n\n\nScaling depending on energy usage of instances\n\nIn recent years, sustainable IT has become a major issue for companies. At Bedrock streaming, we are committed to reducing the environmental impact of our platforms. One of the first steps in this transition is to understand the impact of our applications.\n\nAt kubeCon 2024, we were able to discover a number of interesting tools that we’ll be testing in the near future.\n\nKepler\n\n⏩ Sustainable Computing: Measuring Application Energy Consumption in Kubernetes Environments with Kepler\n\nKepler is a CNCF Sandbox project that uses eBPF to collect performance counters and other system statistics. Kepler estimates energy consumption per pod based on this data and exports it as Prometheus metrics.\n\n\n\nPEAKS\n\n⏩ Empowering Efficiency: PEAKS - Orchestrating Power-Aware Kubernetes Scheduling\n\nPeaks aims to optimize the energy consumption of a kubernetes cluster during scheduling. Peaks relies on pre-trained ML models vs energy consumption to predict the most suitable nodes. The tool is not yet available, but development is progressing and a Kubernetes Enhancement Proposal will be opened in the sig-scheduling project.\n\nBoth tools represent significant advances towards more sustainable computing, and reflect the Kubernetes community’s commitment to responsible innovation.\n\nFinops\n\nReducing – or optimizing – infrastructure costs has been a recurring topic at KubeCon for as long as we remember. Every year, there are talks about basic tools like HPA or ideas like using Spot instances (like we do, for 100% our worker nodes!). This time, Karpenter has clearly become a leader.\n\nThe basics being covered, costs are still too high, and reducing them is getting even more complex than before as more stuff is deployed to Kubernetes. Thinks GPU-based or long-running workloads, for example.\n\nWe are beginning to see more and more approaches using machine learning to forecast how much resources will be needed at a given time, depending on past data. We, like others, have implemented our own pre-scaling mechanism – not based on ML yet, though.\n\nKubernetes and Developer experience\n\nDevOps and Ops, who manipulate Kubernetes and its objects everyday, we tend to forget: Kubernetes is not (perceived as) easy for our developers colleagues, who only write manifests every once in a while or use kubectl when their app fails.\n\nDuring “Developers Demand UX for K8s”, Mairin Duffy and Conor Cowman presented the results of an UX research they did amongst Kubernetes users and identified the main issues developers encounter with it. Debugging network issues, YAML (indentation, lack of validation, clean export), the infamous Crash Loop (and disappering logs), CLI vs GUI… Yeah, we’ve had all of those!\n\nWe have partially solved some of these issues, both with technical solutions (Helm + Helmfile, reusable Github Action workflows) and training sessions, but debugging is still a pain :-/\n\nThings we are currently working on\n\nKeda\n\n⏩ Scaling New Heights with KEDA: Performance, Extensions, and Beyond - Jorge Turrado, SCRM Lidl International Hub &amp;amp; Zbynek Roubalik, Kedify\n\nIn our ongoing quest to improve the efficiency of our Kubernetes clusters, we recently opted for Kubernetes Event-driven Autoscaling (KEDA), a project supported by the Cloud Native Computing Foundation (CNCF) and enjoying growing adoption by the community. This choice is in line with our desire to go further than the capabilities offered by our previous tool, the Prometheus adapter, thanks in particular to more refined scaling management based on various external metrics.\n\nOne of KEDA’s real strengths is its ability to adjust scaling via ‘scaling modifiers’, allowing precise adjustment of the number of pods as required based on a calculation. At Bedrock Streaming this will enable us to refine our prescaling strategy, previously based on an external metric provided by a go service, we’ll be able to upgrade our system to something simpler and more precise. Maybe evolve from a simple calculation of minimum pods and multipliers, to a more sophisticated approach.\n\nThe presentation by Zbynek Roubalik and Jorge Turrado, KEDA maintainers, highlighted KEDA’s effectiveness in meeting various load requirements through the use of a variety of external metrics, making scaling more accurate.\n\nCilium\n\nCilium (and eBPF) were well represented at KubeCon, with a number of conferences on the subject.\n\nThis came at just the right time, as we are currently in the process of migrating to this CNI. This one, based on eBPF (with all that that implies) seemed to us to be a good replacement for our current CNI.\n\nWith cilium: no more KubeProxy, and no more IPtables, which for large clusters is a major performance factor.\n\nCilium also enables network traffic to be filtered with NetworkPolicies inbound and outbound.\n\nAnother feature we particularly like is “cluster-mesh”, which allows you to communicate between several clusters in a fairly simple way, at least on paper 😊! (conference available ⏩ here)\n\nWe were also able to see new features such as GatewayAPI support from version 1.15, and the use of stateDB from version 1.16 to manage the state of cilium objects (and therefore reconciliation in the event of inconsistent state (conference available ⏩ here)).\n\nTools and idea we want to try 💪\n\nKubeCon is also a great way to discover or re-discover new technologies or tools. We won’t share an exhaustive list (see you there next year!), but here are a few we are excited about:\n\n\n  WebAssembly (especially SpinKube): we are running some workloads at-edge on WASM and see a huge potential there. WASM in Kubernetes is intriguing, maybe as a way to reduce pods startup time?\n  OpenFeature: for feature-flipping, A/B testing… Glad to see the emergence of an open standard!\n  Working with multiple Kubernetes clusters: not something we think we need for now, but after hearing about Federation years ago, Karmada seems to be the current tool for this.\n  mirrord: to develop locally as-if inside a Kubernetes cluster, forwarding network and file accesses.\n\n\nCrik &amp;amp; Criu: creating snapshots for interrupted pods\n\n⏩ The Party Must Go on - Resume Pods After Spot Instance Shut Down - Muvaffak Onuş, QA Wolf\n\nDuring the conference held by Muaffak Onuş of the CNCF, an innovative solution was highlighted to mitigate the instability of spot instances: the CRIK project, which leverages CRIU to provide backup and restore functionality for Kubernetes pods. By leveraging CRIU, CRIK can capture the state of processes within a pod before a spot instance shutdown, and then restore them, making it easier to resume work after a spot reclaim for exemple.\n\nA kubernetes controller is deployed, which monitors the state of nodes. When a pod with the required criu configuration is going to be deleted it consults the controller to check if the node is going to be deleted to dump all it’s processes, file descriptors etc… Before being restored on another one.\n\nThis technology is particularly relevant for Bedrock Streaming, where our entire Kubernetes clusters are based on EC2 spot instances. The use of CRIK could transform the way we manage extended jobs, particularly for our machine learning / AI teams, by ensuring the continuity of jobs lasting several hours despite instance interruptions. The integration of CRIK promises to substantially improve the resilience and efficiency of our cloud operations. We plan to test Crik at one of our future r&amp;amp;d days.\n\nCilium as cluster-mesh\n\n⏩ Simplifying Multi-Cluster and Multi-Cloud Deployments with Cilium - Liz Rice, Isovalent\n\nOne thing that caught our attention was the native Cluster-Mesh feature which could allow us to connect multiple clusters between them with little effort and potentially multiple cloud providers between them.\n\nTo allow each Cilium’s components to communicate between them an internal load-balancer is deployed in each cluster.\n\nThere are some prerequisites to allow cluster mesh though:\n\n\n  CIDR block must not overlapped\n  Each cluster needs to have their own unique ID and name\n  All clusters need to use Cilium as their CNI\n  Cluster Mesh option has to be enabled on each cluster\n\n\nLiz Rice did a live presentation of cluster mesh capabilities with some use cases and functionalities like:\n\n\n  Creation of global services\n  The ability to setup affinity rules for local cluster and remote one, to prefer the usage of local cluster pods and fallback to remote if needed.\n  Setup of network policies to cluster level with the Cilium’s ability to add custom labels to pods to identify the cluster they’re running on.\n\n\nEven if for now the usage of multi-cluster through different cloud providers is not a common use case the fact that Isovalent succeeded to make it that simple is very impressive\n\nCilium is now the new standard and we are glad to have chosen it on our clusters.\n\nOpenFGA\n\n⏩ Federated IAM for Kubernetes with OpenFGA - Jonathan Whitaker, Okta\n\nAmong the presentations that stood out for us, the one from OpenFGA really grabbed our attention. OpenFGA, or Fine Grained Authorization, is an open source project that promises to transform the way we manage authorization and identity federation in modern applications.\n\nOpenFGA is a universal authorization solution that enables complex authorizations to be modeled in a granular way. Inspired by Google’s Zanzibar project, OpenFGA offers a developer-friendly API, while guaranteeing performance and security.\n\nOpenFGA gives us the ability to define detailed access policies, which is crucial for identity and access management in dynamic environments like Kubernetes.\n\nThe OpenFGA tool represents a major advance in the field of IT security. Its flexibility, performance and open approach make it an ideal candidate for companies looking to implement robust identity federation. The presentation at KubeCon 2024 only confirmed the immense potential of this tool.\n\nThings that are no longer “problems”\n\n\n  Basic reactive auto-scaling. Still challenges: using &amp;gt;50% of resources, predictive pre-scaling, long-running tasks.\n  Basic Kubernetes observability. Still challenges: observability at large-scale and/or for large distributed systems that go beyond Kubernetes.\n  Using EC2 Spot instances – we’ve been doing it for years for 100% our worker nodes. Now, with Karpenter, others seem to be doing it more and more.\n\n\nConclusion\n\nWhy do we keep going to KubeCon – and other conferences? Well, three goals:\n\n\n  discovering new approaches and tools that might help us better serve our customers in the future;\n  confirming the solutions we are working with are used by many others in the community, which (probably) means they are (still) the right tools;\n  and learning how to do some things better every day.\n\n\nAnd it’s funny how each of us noticed different matters, depending on which parts of our platform we work on and depending on our experience and knowledge in Cloud Native solutions!\n\nIf we were to suggest a few things to KubeCon’s organizers:\n\n\n  30 minutes talks is sometimes a bit short and longer talks could go deeper into technical details;\n  rooms were not big enough for popular talks – and we’re talking about “tech” talks ;-)\n  food 😵. I mean, we’re French 😅\n\n\nOnce again, we come back from KubeCon full of energy and ideas, ready for an exciting new year!\n"
} ,
  
  {
    "title"    : "Mon premier jeu sur BGA #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/mon-premier-jeu-sur-bga",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Introduction au jeu de société Velonimo et à la plateforme de jeux en ligne Board Game Arena, suivie d’un retour d’expérience de l’implémentation de ce jeu sur cette plateforme en tant que développeur Web.\n\n\nPar Oliver Thébault\n",
  "content"  : "\n  Introduction au jeu de société Velonimo et à la plateforme de jeux en ligne Board Game Arena, suivie d’un retour d’expérience de l’implémentation de ce jeu sur cette plateforme en tant que développeur Web.\n\n\nPar Oliver Thébault\n"
} ,
  
  {
    "title"    : "Le LFT du LFT - PUB LFT #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/le-lft-du-lft",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Comment aider les LFTs ?\n\n\nPar la LFTeam\n",
  "content"  : "\n  Comment aider les LFTs ?\n\n\nPar la LFTeam\n"
} ,
  
  {
    "title"    : "Le leader imposteur #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/le-leader-imposteur",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Je ne suis pas expert de mon métier et ce n’est pas parce que j’en parle facilement que je pense le devenir un jour.\nJe n’ai jamais su être un leader, et tous les leaders m’impressionnent.\nJe pense que les autres sont meilleurs que moi et que j...",
  "content"  : "\n  Je ne suis pas expert de mon métier et ce n’est pas parce que j’en parle facilement que je pense le devenir un jour.\nJe n’ai jamais su être un leader, et tous les leaders m’impressionnent.\nJe pense que les autres sont meilleurs que moi et que je ne pourrai pas être leur égal.\nJ’ai été nommé par chance à un poste de leader alors que je n’en avais pas la légitimité.\nEt…je n’ai jamais eu confiance en moi (j’aurais peut-être du commencer par ça)\n\n\n\n  Ceci est une histoire, MON histoire, et je vais vous parler de mon syndrome de l’imposteur.\nComment vivre avec, mes conseils, mes craintes et mes peurs…mais surtout comment vous pouvez vous aider et aider ceux qui le possède également.\n\n\nPar Mathieu Mure\n"
} ,
  
  {
    "title"    : "Du code à l&#39;image : Aller et Retour #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/du-code-a-l-image-aller-et-retour",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Pourquoi l’intelligence artificielle doit-elle s’entraîner pour devenir meilleure, et comment fait-elle ? C’est quoi un réseau de neurones ? Comment Dall-E arrive-t-il à dessiner de si belles images ?\n\n\n\n  Depuis peu, l’intelligence artificiell...",
  "content"  : "\n  Pourquoi l’intelligence artificielle doit-elle s’entraîner pour devenir meilleure, et comment fait-elle ? C’est quoi un réseau de neurones ? Comment Dall-E arrive-t-il à dessiner de si belles images ?\n\n\n\n  Depuis peu, l’intelligence artificielle fait énormément parler d’elle. Pourtant, derrière les termes de “Deep learning” et de “Réseau de neurones”, il est rare de comprendre exactement ce qui se passe dans la machine. Et si on explorait ça ?\n\n\n\n  Découvrons ensemble, dans une présentation ne demandant strictement aucune compétence technique, les bases de l’apprentissage de ces intelligences, afin de comprendre un peu plus ce qui se passe derrière les rouages et toute cette magie !\n\n\n\n  N’hésitez surtout pas à venir même sans la moindre connaissance technique, le but de cette conférence est que même votre grand-mère puisse y venir, et en ressortir en étant capable d’expliquer ce qu’est une IA !\n\n\nPar Etienne Doyon\n"
} ,
  
  {
    "title"    : "Comment j&#39;ai retrouvé le sens de la vie grâce à WebAssembly #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-j-ai-retrouve-le-sens-de-la-vie-grace-a-web-assembly",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Le jeu de la vie est un drôle de jeu qui a la particularité de n’avoir pas de joueur. Il se joue de lui-même et produit des configurations qui semblent évoluer de manière autonome, sans intervention extérieure. A partir de règles très basiques,...",
  "content"  : "\n  Le jeu de la vie est un drôle de jeu qui a la particularité de n’avoir pas de joueur. Il se joue de lui-même et produit des configurations qui semblent évoluer de manière autonome, sans intervention extérieure. A partir de règles très basiques, des structures d’une très grande complexité peuvent émerger d’une manière qui évoque l’apparition de la vie sur Terre à partir d’éléments inertes, d’où le nom mystérieux de jeu de la vie.\n\n\n\n  Comme ce jeu se joue idéalement sur des plateaux sans limite de taille, la question des performances de l’implémentation est capitale. Pour du développement web, cet exemple permet à la fois de voir les limites de JavaScript pour effectuer un grand nombre de calculs et de présenter une solution complémentaire à la réduction de la complexité algorithmique : une implémentation en WebAssembly qui permet de déléguer la charge de calcul à un langage compilé plus performant, Rust.\n\n\nPar Théo Gianella\n"
} ,
  
  {
    "title"    : "Au-delà des industries : Le pouvoir de l&#39;état d&#39;esprit #LFT 02/02/24",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/au-dela-des-industries-le-pouvoir-de-l-etat-d-esprit",
    "date"     : "February 2, 2024",
    "excerpt"  : "\n  Je souhaite explorer l’idée selon laquelle le succès n’est pas défini par une industrie, une discipline ou un secteur spécifique, mais repose plutôt sur notre état d’esprit. À travers mes expériences personnelles variées - en cosmétique, market...",
  "content"  : "\n  Je souhaite explorer l’idée selon laquelle le succès n’est pas défini par une industrie, une discipline ou un secteur spécifique, mais repose plutôt sur notre état d’esprit. À travers mes expériences personnelles variées - en cosmétique, marketing, e-commerce, ainsi que dans le domaine des bars, restaurants et de l’industrie - je mets en avant l’idée que les principes du succès sont universels. Cette présentation souligne l’importance de la mentalité, de la persévérance, des stratégies adoptées et d’une méthode, indépendamment du secteur professionnel. \nL’objectif est d’inspirer et de redéfinir la perception du succès, en dépassant les barrières spécifiques à chaque domaine.\n\n\n\n  En résumé, c’est une approche universelle vers la réussite qui est essentielle, quel que soit le secteur d’activité\n\n\nPar Serge Menassa\n"
} ,
  
  {
    "title"    : "Journal de l&#39;alternant - Comment j&#39;ai perdu mes dépendances pnpm",
    "category" : "",
    "tags"     : " pnpm, hoisting, node_modules, alternant",
    "url"      : "/2024/01/10/journal-de-l-alternant-comment-jai-perdu-mes-dependences-pnpm.html",
    "date"     : "January 10, 2024",
    "excerpt"  : "À Bedrock, on m’a chargé de faire un POC (proof of concept) pour tester les avantages et les limites d’un double run entre notre app côté web (sur une base maison React Server Side Rendering) en déléguant des pages progressivement vers une app Nex...",
  "content"  : "À Bedrock, on m’a chargé de faire un POC (proof of concept) pour tester les avantages et les limites d’un double run entre notre app côté web (sur une base maison React Server Side Rendering) en déléguant des pages progressivement vers une app Next.js. Étant tout nouveau dans le dev et encore plus nouveau sur le projet, ma vie ces derniers temps est une suite d’obstacles, d’essais, d’erreurs et de triomphes (pas toujours, mais souvent) bien mérités. Je suis habitué à faire des erreurs plus lunaires les unes que les autres, mais je vais m’attarder dans cet article sur une erreur qui m’a retourné le cerveau. Au menu : erreurs soudaines, dépendances disparues et désespoir… Bonne lecture.\n\nJe suis alternant depuis un an à Bedrock et je travaille pour la première fois sur notre projet web interne. C’est un projet qui est très complexe, avec lequel vient énormément d’historique (premières briques écrite en 2014) et dont la lecture du code relève parfois autant de l’histoire que du développement.\n\nEn route pour l’aventure\n\n\n  D’ailleurs à Bedrock, si on arrive à maintenir notre application web dans la durée, c’est grâce aux bonnes pratiques qu’on essaie de respecter au mieux.\n\n\nEn bref, je n’ai qu’une connaissance très superficielle du projet et des outils qu’il intègre.\n\nDans mes habitudes de code, il peut parfois m’arriver d’oublier de vérifier que le code que j’écris ne vienne pas casser les tests en place dans le code. Heureusement, notre CI qui nous est chère ne manque jamais de me rappeler mon manque de rigueur. Cette fois-là, je casse un test à cause d’une erreur tellement anodine que je ne parviens pas à m’en rappeler. Je peux juste vous dire que j’ai eu le réflexe d’aller dans mon terminal de lancer le runner de test jest à l’aide de notre package manager pnpm dans une commande qui ressemble à : pnpm test TEST_QUI_CASSE. Le test est rouge pour une raison qui me semble venir d’un problème de dépendances. Ayant beaucoup trituré mes node_modules, je me dis que repartir sur des bases propres ne devrait pas faire de mal au projet. Je décide donc, sans savoir ce qui m’attend, de lancer l’innocente commande : pnpm install\n\nJ’observe que pnpm fait son travail, met à jour des dépendances, je devais effectivement avoir joué un peu trop avec mes node_modules.\n\nJe relance le test et là quelle ne fut pas ma surprise quand mon terminal, sans trembler, m’a affiché Command: &quot;jest&quot; not found.\n\nJe commence à penser que je ne viens pas seulement de casser un test, mais j’ai également cassé jest. À ce moment-là, je venais de ressortir d’une bataille avec des dépendances et donc je venais de me familiariser avec le node_modules .pnpm et autre .bin . C’est dans ce dernier dossier que je me rends compte qu’effectivement, il y manque l’exécutable jest.\n\nEn fait, il y manque également d’autres outils que je m’attendais à trouver comme prettier et eslint.\n\nJe me dis que la portée de mon problème vient de s’étendre de jest à mes node_modules. 🫠\n\nDésespéré, je tente une recherche globale des mots clés : prettier et eslint. Je finis par trouver une correspondance intéressante dans le fichier .npmrc.\n\nVoilà à quoi ressemble le fichier à ce moment-là :\npublic-hoist-pattern[]=*@testing-library/jest-dom*\npublic-hoist-pattern[]=*@testing-library/react*\npublic-hoist-pattern[]=*@testing-library/user-event*\npublic-hoist-pattern[]=*enzyme*\npublic-hoist-pattern[]=*jest*\npublic-hoist-pattern[]=*redux-mock-store*\npublic-hoist-pattern[]=*eslint*\npublic-hoist-pattern[]=*prettier*\n\nJe peux sentir qu’il s’agit d’une véritable piste parce que dans ce fichier sont listées toutes les dépendances qui sont cassées sur ma machine.\n\nComprendre la configuration de pnpm\nHoisting des dépendances\nPour comprendre la configuration public-hoist-pattern il faut d’abord comprendre comment sont formés les node_modules par pnpm. Il ne va mettre dans le dossier node_modules en racine uniquement les dépendances directes du projet, toutes les sous-dépendances seront placées dans un dossier caché .pnpm et un lien symbolique sera créé.\n\n\n  Je vous invite à lire la documentation écrite par pnpm afin de comprendre leur système de dépendances.\n\n\nCela peut parfois poser des problèmes avec des libraries qui utilisent des dépendances fantômes. C’est pourquoi pnpm laisse quand même du contrôle sur ce comportement.\n\n\n  On parle de dépendance fantôme pour désigner toutes les dépendances qui ne sont pas désignées dans le package.json root mais qui sont quand même nécessaire pour le bon fonctionnement de l’application.\n\n\npublic-hoist-pattern permet d’indiquer les dépendances qu’on veut forcer à être dans le dossier node_modules racine plutôt que dansnode_modules/.pnpm.\n\nLa ligne public-hoist-pattern[]=*jest* veut donc dire qu’on ajoute jest aux dépendances qui sont accessibles depuis la racine et ainsi l’exécutable dans node_modules/.bin . Cela permet par exemple de déléguer la configuration et l’import de jest dans un package enfant du repository.\n\nRetour à l’histoire… let’s debug\nA cet instant je suis convaincu que c’est le fichier .npmrc qui est responsable de l’erreur Command: &quot;jest&quot; not found. Je ne vois rien d’anormal dans ce fichier qui pourrait me mettre la puce à l’oreille, c’est alors que je me dis que peut être pnpm ne lit pas la bonne configuration. En lisant la documentation, je tombe sur la commande parfaite : pnpm config get. Cette commande permet d’afficher la configuration que résout pnpm. La sortie de cette commande m’a mis sur une nouvelle piste puisque c’est là que j’ai vu apparaître la ligne problématique : shamefully-hoist=false.\n\nJe tente de chercher dans le projet où est écrite cette ligne. Aucune trace de cette maudite ligne. Je retourne tout le projet à la recherche d’une ligne de code qui pourrait ajouter cette ligne de configuration. Je me mets à lire toute la documentation pnpm pour pouvoir comprendre d’où cette ligne peut venir. Après avoir désinstallé et réinstallé pnpm, node et redémarré mon PC, je tente dans un dernier espoir de créer un dossier test-a-laide dans lequel je reclone le projet. Malheureusement, rien n’y fait.\n\nC’est à ce moment que je me dis que si le problème ne vient pas de mes outils ni de la configuration locale, il faut peut-être que j’aille chercher dans ma configuration globale. En effet, en ouvrant cette dite configuration ~/.npmrc, je m’aperçois que c’est de là que vient la ligne shamefully-hoist=false. C’est un soulagement, j’ai enfin trouvé d’où cette ligne mystique venait.\n\n\n  Je suis encore à la recherche de la réponse à la question : pourquoi diable, ai-je mis cette configuration dans mon .npmrc global. Je pense me souvenir l’avoir fait en me disant que je voulais m’assurer que pnpm se comporte en faisant des symlinks (l’intention n’était pas mauvaise, mais la conséquence pas joyeuse).\n\n\nOn peut lire dans la documentation de pnpm que : Setting shamefully-hoist to true is the same as setting public-hoist-pattern to *.\n\nEn d’autres termes shamefully-hoist à une influence sur le hoisting de toutes les dépendances du projet.\n\nJ’ai deux problèmes avec la documentation à ce sujet :\n\n\n  Tout d’abord, il n’est pas explicité le cas inverse à savoir si on met shamefully-hoist=false alors ça revient à écraser toutes les configurations de public-hoist-pattern\n  Le comportement, qu’il soit un bug ou un cas à la marge, de la configuration globale de shamefully-hoist qui écrase la configuration locale de public-hoist-pattern n’est pas spécifié\n\n\nBref, après avoir déduit que c’était cette ligne qui cassait mon hoisting, je retire la ligne et je lance un pnpm install. Bingo ! Je récupère toutes mes dépendances perdues.\n\nEnseignements\nJ’essaie a posteriori de déchiffrer pourquoi j’ai eu ce problème et comment faire en sorte que cela n’arrive pas. Je pense être tombé sur un comportement étrange de pnpm. Je ne sais pas s’il s’agit d’un bug ou d’une feature. En effet, intuitivement, j’aurais tendance à dire qu’une configuration globale de shamefully-hoist ne devrait pas override la configuration locale de public-hoist-pattern. Je suis prêt à entendre que le comportement est attendu et voulu, mais dans ce cas je pense qu’un peu plus de documentation à ce sujet ne peinera personne. À cet égard j’ai ouvert une issue sur le Github de pnpm.\n\nJe retire plusieurs enseignements de cette aventure :\n\n  Douter de la configuration qui est lue par les outils\n  La documentation ne contient pas toujours tous les comportements\n  Il faut penser à voir plus loin que son fichier local de config et penser aux potentielles surcharges…\n\n"
} ,
  
  {
    "title"    : "Bedrock aux API Days Paris (2023)",
    "category" : "",
    "tags"     : " conference, paris, tech, api, eda",
    "url"      : "/api-days-paris-2023",
    "date"     : "December 20, 2023",
    "excerpt"  : "Cette année Bedrock a envoyé 7 de ses collaborateurs et collaboratrices (i.e. nous, les auteurs de cet article) à l’édition 2023 des conférences “API Days” à Paris.\n\nL’événement a eu lieu au CNIT à La Défense (juste en face du marché de Noël) et a...",
  "content"  : "Cette année Bedrock a envoyé 7 de ses collaborateurs et collaboratrices (i.e. nous, les auteurs de cet article) à l’édition 2023 des conférences “API Days” à Paris.\n\nL’événement a eu lieu au CNIT à La Défense (juste en face du marché de Noël) et a duré 3 jours, du Mercredi 06/12/23 au Vendredi 08/12/23.\n\n\n\nEn plus des 11 paires de chaussettes différentes 🧦 que nous avons réussi à débusquer en parlant aux différents partenaires sur place… En tout, ce sont plus de 100 talks, répartis dans 9 salles, qui nous ont été présentés.\n\nVoici, un résumé de quelques-uns des talks que nous souhaitions mentionner sur ce blog 👇\n\nMaking the Most of Your OpenAPI Spec\n\n\n  Conférence présentée par Alexander Broadbent (Principal Engineer - SAPI)\n\n\nCette conférence expliquait, en détail, une technique “design-first” permettant d’éradiquer les erreurs de “désynchronisation” entre la documentation d’une API et son comportement réel, tout en générant une partie du code.\n\n\n\n\n\nCette technique peut se résumer en quelques points :\n\n  la documentation OpenAPI est la source de vérité et décrit l’intégralité des endpoints de l’application (celle-ci peut être fragmentée en plusieurs fichiers)\n  un outil (redocly) regroupe tous les documents OpenAPI dans un même fichier\n  un outil (openapi-typescript) génère le typage Typescript correspondant au fichier OpenAPI\n  un outil (fastify-openapi-glue) applique le typage Typescript généré sur le code des différents endpoints de l’API Fastify afin de s’assurer que le code produit par les développeurs est conforme à la documentation\n\n\nEt pour les détails d’implémentation, le repository GitHub de la démo d’Alexander est disponible ici : https://github.com/AlexBroadbent/openapi-demo\n\nForget TypeScript, Choose Rust to build Robust, Fast and Cheap APIs\n\n\n  Conférence présentée par Zacaria Chtatar (Backend Software Engineer - HaveSomeCode)\n\n\nCette conférence au titre subversif expliquait pourquoi Zacaria, développeur Typescript/NodeJS à ClubMed, en est arrivé à s’intéresser très fortement au langage de programmation Rust… Au point d’en faire la promotion, en anglais (respect), aux API Days.\n\nLa première partie de sa conférence parlait du langage de programmation Typescript, en dressant une liste de ses qualités (fullstack, très largement déployé en entreprise, écosystème riche, …) et de ses défauts (gestion d’erreur optionnelle, typage éphémère, runtime principal peu performant, …). Cette première partie s’est achevée par un message clair : “Typescript is not enough”.\n\n\n\nLa suite et fin de la présentation, quant à elle, était une introduction à Rust.\n\n\n\nBien que nous étions surpris de voir que le langage de programmation préféré des développeurs de ces 8 dernières années, et sur lequel les plus grosses entreprises tech du monde misent aujourd’hui, avait encore besoin d’être mis en avant en 2023… Zacaria a effectivement eu raison d’en remettre une couche, car encore trop peu d’entreprises françaises ont pris conscience des avantages qu’offre Rust.\n\nCependant, cette 2ème partie de conférence avait un goût de déjà vu pour nous car un de nos collaborateurs a déjà donné le même genre de conférence à Bedrock durant nos LFT 2 années auparavant (en partant de PHP plutôt que de Typescript)… Ceci étant, grâce à nos 2 ans d’expérience de Rust en production et pour les raisons évoquées par Zacaria durant son talk, Bedrock est en mesure de témoigner sa satisfaction d’avoir pris le temps de réécrire certaines des API les plus critiques de Bedrock en Rust. Et nous espérons que de nombreuses entreprises oseront suivre notre exemple, afin de permettre aux développeurs soucieux de la qualité de leur travail comme Zacaria, de disposer de ce merveilleux langage de programmation.\n\nEt si vous ne comprenez toujours pas l’intérêt de Rust, le talk de Zacaria est disponible en replay sur Youtube.\n\nReal-Life REST API Versioning: Strategies and Best Practices\n\n\n  Conférence présentée par Alexandre Touret (Senior Software Architect - Worldline)\n\n\nLa gestion des versions dans le contexte des API au sein d’une entreprise peut souvent s’avérer être un défi complexe et délicat. En effet, les interfaces de programmation applicative (API) constituent le cœur des interactions entre différents services et applications au sein d’un écosystème numérique. La nécessité de versionner ces API devient impérative pour garantir une évolution harmonieuse tout en préservant la compatibilité avec les systèmes existants.\n\nWorldline fait partie des entreprises avec une architecture technique complexe, exposant à des clients finaux un modèle métier en constante évolution.\nCe problème fait très fortement écho avec le business model de Bedrock.\n\nDans sa conférence, Alexandre a présenté son retour d’expérience sur comment versionner une API. Et comment l’exercice est loin d’être un long fleuve tranquille. Voici les points clés que nous avons retenus :\n\nToutes les APIs ne nécessitent pas d’être versionnées\n\nLe versioning c’est compliqué. C’est un fait. Autant l’appliquer sur des APIs où ce principe est vraiment utile. Il est effectivement peut-être moins utile de versionner une API interne, non exposée sur internet ou avec un périmètre fonctionnel très limité ou stable.\nVoici les questions proposées par Alexandre qui peuvent nous aider dans notre décision : ai-je besoin de versionner ? Combien de versions dois-je gérer en parallèle ? Est-ce que ma plateforme est compatible ? Quels sont les impacts sur ma configuration, mon modèle de données, mon système d’authentification (Alexandre a bien insisté sur ce point. Il ne faut pas négliger l’authentification dans la problématique du versioning) ?\n\nLe versioning s’applique aussi sur une architecture en micro service\n\nOn a tendance à croire que seules “les grosses API” sont concernées par le versioning mais Alexandre nous a montré qu’il n’en est rien. Un micro service, aussi micro (voir macro) qu’il soit, gère à lui tout seul un périmètre fonctionnel. Il est donc légitime qu’il puisse évoluer au fil des versions.\n\nComment gérer le versioning\n\nPlusieurs solutions, directement dans l’URL, via header (plus facile avec une API existante)\nAlexandre a fortement déconseillé d’utiliser le versioning par content type. À la fois peu lisible et difficilement maintenable.\n\n\n\nL’impact du versioning\n\nVersionner une API change en profondeur nos manières de travailler. Et ce à plusieurs niveaux :\n\n  Le code source, la technique. Il est indéniable que le code source ainsi que l’architecture technique de vos projets s’en verra impactés. L’impact ne s’arrête pas au code en lui-même, mais sur tout ce qui gravite autour. Nos bases de données, nos scripts, nos configurations serveur ou nos images docker par exemple\n  Le produit. Nos API exposent le métier de notre produit. L’impact n’est donc pas que technique, mais aussi fonctionnel / produit. Une évolution de version technique entraine des breaking change et inversement. Il est donc très important, les équipes et produit travaillent de pair pour évoluer ensemble\n  La livraison. Avoir plusieurs versions d’un API complexifie la mise en production de cette dernière. Il faudra très certainement revoir nos pipelines d’intégration et de déploiement. Ce point est, lui aussi, à ne pas négliger et nécessitera un travail commun au sein des équipes techniques\n\n\nL’observabilité\n\nAlexandre nous a aussi parlé de l’importance d’avoir de la visibilité sur ce qu’il se passe en production, et tout particulièrement à la maille des différentes versions de nos API.\n\nL’observabilité est quelque chose de plus en plus répandu dans notre métier. Mais le versioning porte le concept encore un cran au-dessus.\n\nIl est primordial d’être capable d’assurer une bonne observabilité de nos APIs à la maille :\n\n  De la version\n  Des clients (via des API Keys dédiées)\n\n\nUne bonne observabilité est la clé pour définir une bonne stratégie et être capable d’anticiper le dé commissionnement de versions obsolètes. Ce point est selon moi très important.\n\nIl est très (très) compliqué de maintenir un nombre trop élevé de versions pour une même API. Sans parler d’obsolescence programmée, il faut trouver le bon niveau de maintenance pour éviter de tomber dans le piège de la dette technique. Pas facile comme sujet !\n\nPour finir, je tiens à remercier Alexandre pour sa conférence vraiment intéressante. Bourrée d’exemples concrets. J’ai vraiment senti une vraie maîtrise du sujet. Bravo ! Cette conférence est mon coup de cœur de l’API Days Paris 2023.\n\nOur Ongoing Journey From REST To GraphQL On Android\n\n\n  Conférence présentée par Julien Salvi (Lead Android Engineer - Aircall)\n\n\nDurant cette présentation, Julien Salvi, Lead Android Engineer chez Aircall nous fait un retour d’expérience sur la migration de l’équipe Android d’une API REST à une API GraphQL. Ce choix a été motivé par plusieurs raisons :\n\n  Une problématique globale sur le scaling de leur API REST\n  L’efficacité et l’agrégation des données des API GraphQL\n  La recherche d’une alternative Serverless\n  L’objectif de limiter les perturbations pour les clients\n\n\nLeur aventure débute mi 2020 et est toujours en cours.\n\n\n\nPour répondre à ces demandes, les équipes ont dirigé leur choix vers GraphQL pour créer leur nouvelles API, qui a plusieurs avantages selon Julien notamment:\n\n  La possibilité pour les clients de récupérer seulement les données dont ils ont besoin, cela évite l’over-fetching et l’under-fetching\n  Les clients peuvent récupérer de multiples ressources dans une seule requête\n  GraphQL propose un moyen d’établir une connection constante entre le client et le serveur, ce qui augmente la scalabilité en temps réel\n  Le fort typage de GraphQL permet une communication claire, réduisant ainsi les erreurs\n  L’amélioration de la performance globale grâce au batching des requêtes\n\n\nL’équipe Android utilise le service AppSync d’AWS, facilitant le filtrage, permetttant de faire du realtime, assurer une scalabilité et une bonne intégration avec ElasticSearch et DynamoDB.\n\nAprès les premières migrations vers les API GraphQL, le conférencier insiste sur l’importance du monitoring, qui chez eux est présent que ce soit pour les queries ou les mutations.\n\nVoici les points à retenir de leur expérience\n\n\n\nPour finir, revenons sur un de leur point à surveiller, Julien nous évoque l’importance de la collaboration entre les équipes front et backend qui est également selon nous très importante, notamment pour optimiser l’efficacité des API. On peut citer comme actions par exemple, se mettre d’accord sur les meilleurs timeout à adopter sur les API ou aussi créer les schémas OpenApi ensemble.\n\nWhy API Contracts matters\n\n\n  Conférence présentée par Stéve Sfartz (Principal Architect - Cisco)\n\n\nCette présentation par le Principal Architect de Cisco nous explique pourquoi, dans une stratégie API First, le besoin d’avoir des contrats ainsi qu’un cycle de vie de l’API est primordial pour la cohérence du système.\n\nIls formalisent leur contrats d’API via OpenAPI Spécification, un standard pour les contrats d’API REST, en complément de documents OpenAPI, pour former la définition de l’API. A côté de cette définition, on trouve la gestion du cycle de vie (lifecycle) de l’API, pour informer des deprecated, du changelog et des Breaking Changes lors des versions majeures (semantic versionning).\n\n\n\nLors de la mise en place de ces contrats pour les API à cisco, une qualité (qu’ils appellent aussi Health Contract) y a été associée pour avoir une vue d’ensemble de la documentation des API. Ayant environ 2000 API, cette qualité ne peut pas être évaluée à la main au cas par cas, et passe donc par des outils d’analyse tels qu’un linter Spectral, pour éviter les erreurs et automatiser la génération de ce statut.\n\n\n\nVient ensuite la gestion du drift entre la documentation et le code (par exemple si une annotation est oubliée, une route non documentée) : la vérification du drift doit être faite lors de la CI/CD.\n\nLa conférence se conclut sur une question simple : “Quelle est la source de vérité pour vos API ?”. La réponse est bien évidemment le code, mais une documentation générée automatiquement permet justement de s’en rapprocher grandement. A Bedrock, nos API GraphQL ont leur documentation (schéma) généré automatiquement à partir du code lors du merge d’une PR, ce qui permet d’éviter les oublis de mise à jour.\n\nLet’s bring science into API docs\n\n\n  Conférence présentée par Lana Novikova (Technical Writer - JetBrains)\n\n\nAu cours de cette conférence, Lana Novikova explore comment fusionner les principes scientifiques avec une communication technique efficace dans la documentation des API. \nElle partage également ses connaissances sur la façon dont les développeurs et développeuses consomment l’information en ligne, en mettant en lumière les liens avec différents styles cognitifs, le tout appuyé par des articles scientifiques. Elle explique de manière concrète comment ces principes peuvent être appliqués dans la documentation des API et comment ils peuvent contribuer à améliorer l’expérience des développeurs et développeuses.\nSa conférence est une extension d’un précédent talk qu’elle a donné en 2022 à la “Write The Docs Australia”\n\nLa première étude que nous présente Lana s’intitule “Patterns of Knowledge in API Reference Documentation”. \nElle parle de la nature et de l’organisation des connaissances contenues dans la documentation de référence de centaines d’API au sein de deux plateformes technologiques : Java SDK 6 et .NET 4.0. L’étude a, entre autres, consisté à élaborer une taxonomie des types de connaissances et a pu dresser la liste de 12 types de connaissances distinctes dans la documentation de l’API :\n\n\n\nÀ travers cette étude, nous pouvons donc évaluer le contenu de la documentation de notre API en fonction des types de connaissances et ainsi développer des modèles de documentation adaptés aux connaissances communément associées aux différents types de composants de l’api. De plus, aujourd’hui, des projets comme the good docs project existent et proposent des templates de documentation basés sur ces données scientifiques.\n\nLa deuxième étude exposée dans cette conférence a comme titre “How Developers Use API Documentation: An Observation Study”. Sa méthodologie consiste à l’observation active, via des screencasts et des protocoles verbaux, des activités des personnes participantes pendant le test. Les chercheurs et chercheuses ont évalué le taux de réussite, le temps passé sur les tâches et l’utilisation de la documentation et des catégories de contenu. L’objectif principal est d’observer comment les développeurs et développeuses abordent les tâches avec une API qu’elles ne connaissent pas. Il s’agit également d’analyser comment les développeurs et développeuses utilisent les ressources d’information proposées par la documentation de l’API. Cela permet de caractériser les stratégies adoptées par les développeurs et développeuses lorsqu’elles commencent à travailler avec une nouvelle API. La conclusion que Lana nous partage est qu’en moyenne, les personnes participantes ont utilisé la documentation de l’API environ 49 % du temps (Min : 31 %, Max : 68 %). La catégorie de contenu à laquelle il est fait référence le plus souvent est “API reference”, suivie de “Recipes page”.\n\n\n\n\n\nIl se dégage que le temps que les personnes participantes consacrent aux différentes catégories de contenu varie considérablement d’une personne à l’autre. Sur la base de ces données, les chercheurs et chercheuses ont défini trois types de personnages de développeurs logiciels à la recherche d’informations ainsi que leurs approches lorsqu’ils opèrent celles-ci: Systematic learners, Opportunistic learners et Pragmatic learners. Pour les personnes curieuses d’approfondir le sujet, ces personae sont basés sur une autre étude intitulée “What is an end user software engineer?”.\n\nLana Novikova conclut en mettant l’accent sur le fait qu’il faut respecter les différentes stratégies adoptées par les développeurs et les développeuses lorsqu’elles abordent une nouvelle API et nous propose d’appliquer ces conseils :\n\n  Pour les “opportunistic learners”, donner des exemples de code complets et exhaustifs en donnant la possibilité de masquer tout le reste et de relier le texte au code.\n  Pour les “systematic learners”, fournir des informations importantes de manière redondante et donner des connaissances de base pertinentes.\n  Pour les “pragmatic learners” (et les autres), donner un moyen technique pour commencer à utiliser une API.\n\n\nJe tiens personnellement à souligner qu’il est rare d’assister à une conférence aussi complète se basant sur autant de données scientifiques. Je ressors de cette conférence agréablement surpris de la qualité de tout ce qui a été proposé et des ressources mises à disposition. Je vous laisse avec un lien contenant toutes les slides de la présentation de Lana Novikova qui expliquera bien mieux le propos que mon résumé. Bravo à elle et à son travail, en espérant voir de plus en plus de conférences de ce genre à l’avenir.\n\nLe mot de la fin\n\nSi on vous a donné envie d’en savoir plus :\n\n  le site officiel de la conférence\n  la majorité des talks sont disponibles sur Youtube\n  certains speakers ont mis à disposition les slides de leur talk\n\n\nBonnes fêtes de fin d’année !\n\n\n\n"
} ,
  
  {
    "title"    : "Bedrock at 2023 AWS re:Invent Las Vegas",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, aws, re:invent, las vegas, 2023",
    "url"      : "/2023/12/18/aws-reinvent-lasvegas-2023.html",
    "date"     : "December 18, 2023",
    "excerpt"  : "AWS re:Invent 2023 was a showcase for GenAI. It was the announcement of Amazon Q, Amazon’s new AI assistant, that attracted the most interest, designed to meet the specific needs of businesses.\n\nAlongside these announcements, we had the opportunit...",
  "content"  : "AWS re:Invent 2023 was a showcase for GenAI. It was the announcement of Amazon Q, Amazon’s new AI assistant, that attracted the most interest, designed to meet the specific needs of businesses.\n\nAlongside these announcements, we had the opportunity to attend talks by some of the major players in streaming, such as Prime Video, Peacock, Netflix and Spotify. Their presentations offered valuable insights into their challenges, successes and lessons learned, enriching our own understanding of the sector.\n\nHow Amazon scales resilience to new heights\n\n\n\nOlga Hall, Director of Live Events Availability &amp;amp; Resilience at Amazon Prime Video, and Lauren Domb, Chief Technologist, WWP Federal Financial Services, WW Chaos Engineering Lead at AWS, discussed the importance of resilience in the streaming industry.\n\nThey highlighted the high cost of downtime and the impact on companies’ ability to serve their customers. We’re talking about an average of $300,000 per hour across the industry.\n\nIn their view, preparing engineering teams for peak loads and streaming is very similar to preparing sports teams for major events. So they created a “resilience playbook”, a series of strategies and tactics inspired by the most successful sports teams, to help their teams become resilient in the face of the unpredictable.\n\nThey also shared their experience of broadcasting Thursday night soccer matches on Prime Video, highlighting how they had to manage unpredictable conditions, such as weather-related match delays, which extended the duration of the peak workload.\n\nAvailability is considered the number 1 feature at Amazon Prime Video. All projects and workflows are listed, budgeted and included in the list of expected features, with availability always at the top of the list.\nTo guarantee the availability of their services, they run load tests three times a week in each region, scaled for peak usage. This enables them to detect bugs before they affect users, and ensure that their systems are ready to handle the highest loads.\n\nAt Bedrock Streaming, we share this focus on resilience. Our applications incorporate circuit breakers, we run load tests very regularly and practice chaos engineering. \nThese practices at Amazon Prime Video offer an interesting perspective on how we might further improve our resilience at Bedrock Streaming. We were particularly impressed by the resilience score per application approach. This is definitely something we’ll be exploring in conjunction with a feature flipping by service approach.\n\nSurviving overloads: How Amazon Prime Day avoids congestion collapse\n\nJim Roskind, Distinguished Engineer at Amazon.com, and Ankit Chadha Solution Architect at AWS shared their experiences and knowledge at the conference. Formerly of Google, where he oversaw Google Chrome metrics and performance, Jim now works for Amazon with a singular mission: to buy less of AWS products. This approach, aimed at reducing IT costs, is supported by AWS, which wants to teach all its customers how to optimize their spending.\n\nHowever, this quest for efficiency and cost reduction is not without risks. One of Jim’s main concerns is congestion collapse, a phenomenon that can lead to a drop in productivity and even paralyze a system. To help us understand this phenomenon, Jim presented a series of pragmatic examples and theories on congestion collapse.\n\nWhat is congestion collapse?\n\nCongestion collapse occurs when demand exceeds a system’s capacity. This leads to the build-up of queues, reduced productivity and, in extreme cases, the complete cessation of productive work. This phenomenon is not uncommon and can occur in a variety of situations, from highways to web servers.\n\nJim shared some examples to illustrate this phenomenon, the main one was about \nAmazon Prime Day 2018. Even giants like Amazon aren’t immune to congestion collapse. \nHe hadn’t anticipated customer interest in a particular product. They had a massive demand for a product display and the service ended up being unavailable. At this point, traffic increased again sharply as customers began reloading the page. Above all, it highlights the fact that, as their services are slow to respond, the answers are no longer relevant, leaving us with a system that is 100% loaded and no longer doing anything useful.\n\nThese examples show that congestion collapse is a real problem that requires constant attention and planning.\n\nAt bedrock we’ve also encountered congestion collapse. For example, during busy events such as soccer matches, we’ve already encountered the case where a massive influx of users would cause the platform to become unavailable, at which point all the users would press F5 at the same time, drastically increasing the traffic on an already struggling platform. Moreover, requests are queued up and we end up answering requests issued 1 or 2 min earlier, and therefore answering people who have surely already left.\n\nJim mentions this issue, which has happened at amazon.com.\n\nThey were able to implement 2 solutions in particular.\n\n\n  \n    The first, when the servers can no longer respond, is to display a page with a message warning the user to wait a while and try again in a while. This had a surprisingly noticeable effect. Users were no longer repeatedly pressing the F5 button.\n  \n  \n    Secondly, they have implemented mechanisms to detect massive retries and thus avoid transmitting traffic to their backend. In particular, they have implemented this in the WAF service.\n  \n\n\nAt Bedrock Streaming we already display a page in case of trouble, but we can improve it to suggest to the user to wait before retrying. Moreover, we use Cloudfront and WAF on almost all our services. We have a few rules on WAF that allow us to deny traffic that seems illegitimate, but we’re going to work on a new rule to avoid transmitting untimely user retries in the event of an overloaded system.\n\nNetflix caching\n“Who in this room is a netflix user?” (90% of the room raises its hand): no doubt Prudhviraj Karumanchi, software engineer &amp;amp; Sriram Rangarajan, Senior Distributed Systems Engineer at Netflix, conference speakers, know how to introduce their talk and remind us that they are the market giants. During their conference, they presented how Netflix uses the EVCache solution for multi-region cache replication.\n\nNetflix likes to say that one of its missions is to spread joy. This involves two aspects: offering users a fully personalized homepage and \nbenefiting from a scalable, low-cost architecture (so that Netflix’s techies are happy too).\n\n\n\nThis conference addressed many of the issues we are familiar with: a home page that calls on a large number of microservices to display content to the user, scaling issues, cost management…\n\nThe heart of the conference detailed the architecture implemented by Netflix teams to replicate cache across multiple regions using EVCache, Kafka and SQS.\n\nIt was also interesting to see that at Netflix, as with us, it’s important to build with costs in mind: after analyzing the costs of their inter-region traffic, they finally decided to remove their network load-balancer to make their architecture more cost-efficient.\n\nA long part of the conference was dedicated to the observability of the replication stack: our teams have also done a lot of work on this issue in recent years, so it was interesting to compare our practices on the subject of observability.\n\nWhile we don’t yet work on the same scale as Netflix, attending this conference allowed us to reinforce our idea that caching is essential in the architecture of a platform such as Bedrock Streaming. And it gives us new ways for reflection…\n\nTakeaways from Reinvent2023. The newcomers’ point of view\n\nIt’s often said that “what happens in Vegas stays in Vegas”, but for some of us, this edition of Re:invent was our first time at a conference of this magnitude. So it’s hard not to share some of our feedback with you!\n\nFirst of all, a word about Vegas, the city of excess which hosts Re:invent every year, and which has been transformed for us into an open-air R&amp;amp;D ground! From Fremont Street to the brand-new Sphere, there are screens EVERYWHERE in this city, which has aroused our curiosity as video streaming professionals… \n\n\nOn the eve of Reinvent, the city is transformed: tourists give way to speakers from all over the world, advertising screens talk of nothing but cloud solutions, scalability and artificial intelligence… It’s an incredible phenomenon to behold!\n\nWith over 70,000 participants, nothing can be left to chance in the organization of Reinvent. It’s impressive to see (Vegas traffic jams aside) how smoothly the whole conference runs. A sort of “human load-balancing” has even been arranged so that each speaker can have lunch in record time!\n\nThe keynotes organized during Reinvent (5 in all) are events within events! The expected crowds are so huge that several overflow rooms are planned in addition to the main ballroom where the speaker is based.\nWe were able to attend two of them: Adam Zelipsky’s (CEO of aws) - for which the speakers were greeted by a fantastic rock band at 7:30 a.m., a guaranteed wake-up call - and Werner Vogels’ (CPO &amp;amp; VP of amazon.com), where this time the welcome was provided by a fantastic string quartet.\n\nWhile Adam Zelisky’s conference was undoubtedly a masterpiece (production, new services announced, influential clients such as Lidia Fonseca, chief digital and technology officer at Pfizer, speaking), it was Dr. Vogels’ conference that impressed us the most.\n\nBased on the laws of the “frugal architect”, this keynote spoke to everyone: technicians, business people, product managers… It took up the elementary concepts of what should motivate our design of IT solutions: cost awareness, the indispensable balance between commercial and technical needs, or the danger of never questioning oneself, quoting Grace Hopper: “One of the most dangerous phrases in the English language is: ‘We’ve always done it this way’”\nWe encourage you to watch the replay of this keynote, a must-see!\n\n\n\nBeyond the gigantism of the event, let’s look at what we’ve taken away from our participation in Reinvent. Attending a conference is always an opportunity to take a step back on our current work themes: we attended many conferences on resilience, scalability and new architectures:\n\n  it enabled us to compare our practices with those of major players in the sector (Prime, Spotify, Peacock…),\n  to transpose these themes into a context completely different from our own (“1.5 million requests per second—a story from the Brazilian elections”)\n  or to appreciate the work we’ve done over the year, which sometimes goes beyond what’s presented at conferences (“Use new IAM Access Analyzer features on your journey to least privilege”)\n\n\nFor all these reasons, no doubt Bedrock Streaming teams will be betting on Reinvent again next year!\n"
} ,
  
  {
    "title"    : "Android 14 is out",
    "category" : "",
    "tags"     : " android, mobile, google, 14",
    "url"      : "/2023/12/05/android-14.html",
    "date"     : "December 5, 2023",
    "excerpt"  : "Here’s what it means for users and developers.\n\nWith each new OS version, new things, upgrades, deprecations and changes are introduced, affecting the way we use and develop our apps.\nGoogle keeps going in the direction of more privacy, more acces...",
  "content"  : "Here’s what it means for users and developers.\n\nWith each new OS version, new things, upgrades, deprecations and changes are introduced, affecting the way we use and develop our apps.\nGoogle keeps going in the direction of more privacy, more accessibility and more control over what the apps can do to maximize security and integrity.\nAndroid 14 is no exception and here’s what I compiled on different topics that I will try to vulgarize to keep everyone on board.\n\nTechnical\n\nTechnical changes build over features and APIs already introduced in previous versions, mostly Android 12 and 13.\nThey tend to modernize tools by catching up with some Java features and semantics, helping manufacturers and improving the developers’ IDE to embrace those changes.\nDue to the nature of the changes, this is the topic that has to remain…technical, sorry for that.\n\n\n  Mobile screens are getting bigger with more ratios to support, we’re moving further and further away from the binary world of phone vs tablet. To ensure the best experience on this wide range of devices, Android 14 introduces the Large Screen Compatibility Mode to help manufacturers improve the experience on their devices.\n  Updates to OpenJDK17 may require a bit of attention from apps using Regex that are not close enough to openJDK’s new semantics, throwing exceptions when confronted to an invalid groupe reference.\n  Generating a UUID from a string sees the validation become stricter and will now lead to exceptions due to deserialization issues. More than ever, it’s time to unit test UUID generation.\n  A bit of additional ruling may be needed to fix Proguard issues when shrinking / obfuscating code involving the ClassValue class coming with API34.\n  The new Back APIs are now strengthened by built-in animations and support for custom ones.\n  Making the ForegroundService type explicit is now mandatory, if the implementation was already properly done back in the Android 10 days when it was introduced, congratulations, nothing to do here.\n  Foreground services are also encouraged to be migrated to user-initiated jobs. A new RUN_USER_INITIATED_JOBS permission is introduced and new methods on the JobInfo builder allow to set the userInitiated() status along with the estimated amount of bytes the job will expect from the network. Scheduling the job is now done with the app foregrounded and the notification icon system remains the same so the user knows something is going on even if the app is backgrounded post launch.\n\n\nBattery and performance\n\nWithout a single ounce of surprise, Google continues its effort to improve battery life and takes steps towards sanctioning bad actors that publish battery-draining or unstable apps.\nToday, not crashing is no longer enough, developers should take steps to push their app to their full potential and that means power management and performance monitoring.\n\n\n  Bad behaviours like ANRs (screen freezes) or background crashes now more aggressively flag the guilty apps and put them at the bottom of the priority list where apps are fighting for resources, meaning they’ll also be the first to go if the system needs some. No more filtering out ANRs and non-fatal crashes on Crashlytics, everything matters now.\n  While on the subject of fighting for resources, let’s also note that now, context-registered broadcasts are now queued when the app is backgrounded and the system will deliver them when the app is awake or system conditions allow it.\n  Another change to the cached state (aka when the app is backgrounded) impacts background tasks that can no longer be triggered unless one of the app components is awake. This change pushes devs to use framework’s JobScheduler and WorkManager more as they aren’t impacted by this change.\n  Still with Jobscheduler, jobs don’t just fail silently anymore if they don’t respond in time but trigger an ANR, it is advised to move to WorkManager with its out of the box async support.\n  If a job requires a special network state to be triggered, the ACCESS_NETWORK_STATE permission is now mandatory. Without it, a SecurityException will be raised.\n  Intents keep getting more and more headache prone as the implicit and pending intents now can only be delivered to exported components. If you need to reach an unexported component, explicit intent is your go-to solution. Note that mutable pending intents now need to specify a component or it will throw an exception.\n\n\nNotifications\n\nFinding the right balance between informative presence and in-your-face nuisance has always been a challenge for notifications and it seems Google keeps pushing to make them less invasive and easier for the user to dismiss or delay them.\n\n\n  The Fullscreen Intent notifications that we see when our clock rings or when we receive a phone call are luckily already rarely used.\nThey are now more restricted and available only to apps declaring Call or Alarm features, meaning we shouldn’t see bad actors abusing this feature that would allow them to bypass the lock screen amongst other things.\n  Non-dismissible foreground notifications are now dismissible in some cases but will remain non-dismissible\n    \n      on top of the Lock Screen to prevent it from being swiped by anyone accessing a device behind the owner’s back.\n      from the Clear all feature to prevent misclicks.\n    \n  \n\n\nPrivacy and security\n\nThis is, once again without surprise, where a lot of the changes happen and it is aligned with Google’s vision and goals when it comes to give users back the control of their data and permissions.\nSome of them seem so obvious that it’s surprising to see them in action only now. Maybe the EU pressure with GDPR starts to pay off? Maybe…\n\n\n  Android 14 introduces new places where the data sharing purposes are displayed. Until now, we could only check them from the PlayStore app page. \nNow, it will also be displayed in the runtime permission popups, starting with those related to location to remind why the data is necessary and with whom it will be shared.\n  It will now be impossible to install apps that don’t target at least the API 23 to prevent bad actors from exploiting security breaches discovered inside older Android versions.\nBe aware that installed apps won’t be removed and the system won’t warn you when starting one of those apps, maybe a new feature for Android 15?\n  Dynamic Code Loading now requires to flag the file as read-only to avoid any tampering or code injection. In any case, DCL should be avoided when possible and only trusted files should obviously be loaded this way.\n  When saving a file inside the app storage, the system attributes to the file an owner id, this id being the app package name that saved it. \nThis feature allows apps to know which file they can open without requesting the external read permission. The issue was that by querying this id, other apps could access the owner ids that weren’t them and deducting the owner’s installed apps list. \nTo fix this, the name is now redacted, increasing again a little bit the user data protection, the list of the installed apps being considered a sensitive data by Google.\n  If an app features screen or audio recording, it is now required to be granted the user consent to do so before each session start and therefore be able to handle permission denied scenarios.\n  Zip files are also impacted as a fixed vulnerability with the path transversal reading now triggers an exception if some characters are found inside it. (Contains .. Or starts with /)\n  Even though already required, the BLUETOOTH_CONNECT permission was not yet enforced to access the profile state, it is now the case.\n  Users are no more required to grant access to all images or videos to share or display a single media, Android 14 now upgrades the permission popup with an option to select only the media the app is allowed to access.\n  Apps can now react to a user screenshot event, they can’t manipulate the content but developers can now add a callback bound to the activity lifecycle. \nSensitive screens should still be protected with the secure flag.\n  Starting activities from the background with a pending intent or through another app in the foreground now requires the app to opt-in to this feature inside said activity and is no longer a default behaviour.\n\n\nAccessibility\n\nIt is no secret that mobile devices are now owned by more and more people every year, which includes people with a range of disabilities or personalities that may make an app usage more challenging.\nAndroid 14 helps them with new and upgraded features to ease their journey with a mobile device.\n\n\n  A step is taken towards low-vision users’ direction, the changes and impacts to the font scaling should be negligible to developers already properly using SP as their size units but a full testing pass with the scaling enabled should be scheduled to be safe and tweak improvable screens.\n  New tools inside Android studio are added to help developers handling per-app language more efficiently and easily.\n  Grammatical Inflection API is introduced, offering developers working on apps with gendered languages new tools. It adds a layer of complexity to the strings files by having three gender-files by gendered language. In those files are added only the strings affected by gender inflections like Vous êtes déconnecté for masculine, Vous êtes déconnectée for feminine or La déconnection est effective for neutral in french. More work for developers and translators but an overall better experience for users.\n\n\n\n\nAll in all, Android 14 is an update faithful to the Google roadmap. \nUsers today are very different than users 10 years ago. They care more about their data and their privacy; the Mobile ecosystem and business is also a lot more professional.\nIt’s important for us developers to be aware of those changes in order to continuously improve the experience, be it related to our core business or simply to keep the user engaged in a safe environment.\n\nWhen this article is released, Android 14 should be freshly out and developer teams hands deep in the migration tasks.\nI hope you enjoyed the information and see you soon for more Android related articles!\n\n\n  Changes potentially affecting all apps\n  Changes affecting apps targetting Android 14\n  New features introduced by Android 14\n  APIs changelog\n  Overview\n\n"
} ,
  
  {
    "title"    : "STOP à l&#39;espionnage ! Comment disparaître d&#39;internet ? #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/stop-a-l-espionnage-comment-disparaitre-d-internet",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  Vous en avez marre de vous sentir traqué sur Internet ? 😒 Nous aussi ! Mais est-il réellement possible de nos jours de devenir un véritable ninja digital ? 🥷🏻 Et si pour protéger votre vie privée en ligne, il fallait tout d’abord comprendre qui...",
  "content"  : "\n  Vous en avez marre de vous sentir traqué sur Internet ? 😒 Nous aussi ! Mais est-il réellement possible de nos jours de devenir un véritable ninja digital ? 🥷🏻 Et si pour protéger votre vie privée en ligne, il fallait tout d’abord comprendre qui a accès à vos données et ce qu’ils peuvent faire avec ?\n\n\n\n  Nous allons vous apprendre comment être un vrai pro de la confidentialité et de la sécurité des données, mais surtout comment devenir anonyme en ligne et éviter les curieux… 👀 Sortez vos loupes et suivez-nous dans cette enquête pour reprendre le contrôle de votre vie numérique !\n\n\n\n  Et si vous êtes chanceux, nous vous dévoilerons peut-être quelques secrets de ninja pour échapper aux espions ! Venez nous rejoindre et apprenez comment devenir le maître du camouflage numérique ! 😶‍🌫️\n\n\nPar Etienne Idoux &amp;amp; Mickaël Alves\n"
} ,
  
  {
    "title"    : "Le futur du web est sur la périphérie du réseau #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/le-futur-du-web-est-sur-la-peripherie-du-reseau",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  Le mot est apparu sur toutes les documentations ces derniers mois : les cloud providers comme Vercel, Netlify ou AWS proposent leur version des “Edge Functions”, alors que les frameworks Javascript comme Next, Nuxt ou encore Astro mettent en av...",
  "content"  : "\n  Le mot est apparu sur toutes les documentations ces derniers mois : les cloud providers comme Vercel, Netlify ou AWS proposent leur version des “Edge Functions”, alors que les frameworks Javascript comme Next, Nuxt ou encore Astro mettent en avant leur support des “Edge API Routes” ou du “SSR on the Edge”. Mais qu’est-ce donc que tout cela ?\n\n\n\n  À l’instar des CDNs pour les fichiers statiques, ce nouveau paradigme consiste à exécuter le code serveur au plus près des utilisateurs (“the Edge”). On peut ainsi obtenir du contenu dynamique à la vitesse du statique, avec des usages comme le SSR, l’authentification ou l’A/B Testing à la périphérie du réseau.\n\n\n\n  Je vous propose de découvrir, chiffres à l’appui, les performances que l’on peut atteindre sur certains cas d’utilisation, et les situations dans lesquelles il n’est au contraire pas intéressant de l’utiliser. Avec ça, plus d’excuses si votre site n’est pas “blazing fast”.\n\n\nPar Julien Sulpis\n"
} ,
  
  {
    "title"    : "Jeux vidéo, websocket et binaire: temps réel efficace pour navigateur #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/jeux-video-websocket-et-binaire-temps-reel-efficace-pour-navigateur",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  Connaissez vous les websockets ? La techno idéale pour coder des expériences temps réel dans le navigateur et que j’utilise dans tout mes projets collaboratifs et jeux multijoueur ?\n\n\n\n  Je vous propose de nous intéresser à son fonctionnement, ...",
  "content"  : "\n  Connaissez vous les websockets ? La techno idéale pour coder des expériences temps réel dans le navigateur et que j’utilise dans tout mes projets collaboratifs et jeux multijoueur ?\n\n\n\n  Je vous propose de nous intéresser à son fonctionnement, et à la façon dont on peut mettre en place cette communication client/serveur performante, en échangeant directement en binaire.\n\n\n\n  On s’amusera à les voir prendre vie tous ensemble dans une démo live…en GO !\n\n\nPar Thomas Jarrand\n"
} ,
  
  {
    "title"    : "Comment ne pas jeter son application Frontend tout les deux ans ? #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-ne-pas-jeter-son-application-frontend-tout-les-deux-ans",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  Bonnes pratiques pour la maintenance d’une application web\n\n\n\n  Refaire son front tous les 2 ans, c’est devenu une pratique plutôt courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la même codeb...",
  "content"  : "\n  Bonnes pratiques pour la maintenance d’une application web\n\n\n\n  Refaire son front tous les 2 ans, c’est devenu une pratique plutôt courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la même codebase et ça depuis plus de 7 ans! En plus, ce n’est pas une petite application puisqu’il s’agit de 6play et de salto.\n\n\n\n  Vous pourriez vous dire: “Oh les pauvres, maintenir une application vieille de presque 10 ans ça doit être un enfer !”\n\n\n\n  Rassurez-vous, ce n’est pas le cas ! Nous avons tous travaillé sur des projets bien moins vieux mais sur lesquels le développement de nouvelles fonctionnalités était bien plus pénible. Quel est notre secret ? C’est ce que vous allez découvrir pendant ce talk.\n\n\n\n  Automatisation des tâches courantes, gestion de la dette, testing et architecture seront des sujets abordés.\n\n\nPar Florent Dubost &amp;amp; Antoine Caron\n"
} ,
  
  {
    "title"    : "Comment faire de votre vie un BlackFriday permanent #LFT 24/11/23",
    "category" : "",
    "tags"     : " lft",
    "url"      : "/comment-faire-de-votre-vie-un-blackfriday-permanent",
    "date"     : "November 24, 2023",
    "excerpt"  : "\n  A travers ce talk, j’aimerai vous initier à l’intérêt de la négociation et les différentes techniques que l’on peut utiliser pour bien négocier.\nL’idée est de vous donner des clés simples et efficaces pour aborder une situation fréquente qui pe...",
  "content"  : "\n  A travers ce talk, j’aimerai vous initier à l’intérêt de la négociation et les différentes techniques que l’on peut utiliser pour bien négocier.\nL’idée est de vous donner des clés simples et efficaces pour aborder une situation fréquente qui peut parfois être source de stress ou de tensions.\n\n\nPar Thomas Sontag\n"
} ,
  
  {
    "title"    : "Swift Concurrency in a Nutshell",
    "category" : "",
    "tags"     : " swift, apple, xcode",
    "url"      : "/2023/11/14/swift-concurrency-in-a-nutshell.html",
    "date"     : "November 14, 2023",
    "excerpt"  : "As modern apps grow in complexity and features, the need for multitasking to enhance the user experience becomes evident. Whether processing large datasets or querying multiple systems over the network, concurrency is essential.\n\nThis article pres...",
  "content"  : "As modern apps grow in complexity and features, the need for multitasking to enhance the user experience becomes evident. Whether processing large datasets or querying multiple systems over the network, concurrency is essential.\n\nThis article presents a concise, yet comprehensive overview of Swift’s Concurrency, highlighting its key features and core concepts. Swift’s approach to concurrency provides several benefits:\n\n\n  Simplified code that’s easier to reason about and maintain\n  A noticeable reduction in bugs and performance issues\n  Ensured app responsiveness\n\n\nBefore delving into Swift’s concurrency paradigms,  let’s familiarize ourselves with foundational terminology.\n\nConcurrency\n\nConcurrency is about structuring your code so that tasks can be executed independently. It provides mechanisms for synchronization, communication, and coordination between units of work to avoid race conditions and ensure proper execution. However, concurrency doesn’t imply parallel execution; the actual mode of execution is determined separately.\n\nDesigning your code effectively for concurrency makes adding parallelism nearly free.\n\nParallelism\n\nParallelism is the simultaneous execution of tasks across multiple processing units, guaranteeing genuine concurrent progression of operations. It’s a specific form of concurrency where tasks are actually executed at the same time.\n\nStructured Concurrency\n\nTraditionally, developers had to manually manage threads, locks, and callbacks, leading to code that is difficult to manage and error prone. Even with a lot of discipline, it was really hard to get right as the cognitive load was so high.\n\nStructured concurrency is a programming paradigm providing a higher level of abstraction, allowing you to manage concurrency in a structured and organized way. It simplifies the task management and their dependencies, making it easier to write correct and efficient concurrent code.\n\nSwift Concurrency\n\nOne prime objective of Swift is safety, by removing undefined behaviors such as null pointer, array out-of-bounds, and integer overflows. Until recently, multithreading remained a weak spot in Swift’s safety features. Developers had to rely on Grand Central Dispatch, which wasn’t inherently designed to help with concurrency-related pitfalls like thread explosion.\n\nSwift Concurrency fills this gap, enhancing the language’s overall safety by integrating the Task abstraction from Structured Concurrency, the async/await pattern and Actors for data isolation.\n\nTask\n\nWith Swift Concurrency, Tasks become the primary unit of work and offer three core functionalities:\n\n\n  Carry scheduling information such as priority\n  Act as handles for task management\n  Hold user-defined and task-local data\n\n\nThese attributes make tasks the cornerstone that guides the execution model in running, prioritizing, and suspending or canceling jobs. Every asynchronous function operates within a task. Tasks also serve as the entry point for synchronous functions to execute asynchronous code.\n\nChild Tasks\n\nA child task is a task spawned by another task, known as the parent task. Child tasks inherit some properties from their parent, such as priority levels, but are their own individual units of work that can be scheduled independently. One important characteristic of child tasks is their lifetime is tied to their parent task; if the parent task is cancelled, all its child tasks are also cancelled. This ensures a structured way to manage and reason about concurrent tasks in your code. However, cancellations do not propagate upward, requiring parent tasks to manually check the status of their child tasks.\n\nChild Tasks are created using Task Groups as we will see later.\n\nasync / await\n\nThe async/await pattern simplifies asynchronous code development, allowing a sequential-like structure, akin to traditional synchronous functions.\n\nUse the async keyword to mark functions that perform asynchronous work.\n\nfunc performRemoteOperation(_ url: URL) async throws -&amp;gt; ResultType\n\n\nThe await keyword indicates potential suspension points in your code, which are necessary for running async functions. These markers also offer developers insight into the behavior and control flow of asynchronous operations. At these suspension points, the system can pause the current task to await the completion of an asynchronous operation.\n\nfunc processRemoteData() async throws -&amp;gt; Resource {\n    let data = try await performRemoteOperation() // waiting for performRemoteOperation() to complete\n    let resource = await process(data)\n    return resource\n}\n\n\nError propagation\n\nAs you may have noticed in the previous examples, Swift’s concurrency model seamlessly integrates with the language’s native error-handling mechanism. This brings several advantages over the old completion-based concurrency:\n\n\n  Clarity: Errors are propagated in a way that is consistent with how they are handled in synchronous Swift code. This means you don’t have to learn a new error-handling paradigm when moving to concurrent code.\n  Safety: Because errors can be propagated and caught, you can handle exceptional conditions gracefully, making your concurrent code more robust.\n  Maintainability: With explicit error types and propagation, debugging and maintaining concurrent code becomes easier. You can clearly understand what types of errors your asynchronous functions can throw and handle them appropriately.\n\n\nActors\n\nSwift’s Structured Concurrency is designed to address data races in concurrency for functions and closures. However, working concurrently usually involve dealing with shared mutable state, requiring tedious manual synchronization.\n\nTo address this, Swift introduces Actors, a new reference type designed to encapsulate states within a specific concurrency domain, ensuring data isolation and thread-safe operations. Actors not only enhances safety and efficiency but also align with Swift’s established patterns and features.\n\nTo create an Actor, just use the keyword actor.\n\nactor MessageThread {\n    let playerTag: String\n    var messages: [String]\n\n    init(playerTag: String, previousMessages: [String]) {\n        self.playerTag = playerTag\n        self.messages = previousMessages\n    }\n}\n\n\nActors are similar to class, the main difference is that they protect their mutable data from data races by implementing Actor Isolation.\n\nActor Isolation\n\nActor Isolation enforces that any mutable properties managed by an actor can only be modified using self.\n\nextension MessageThread {\n    func send(_ message: String, to other: MessageThread) {}\n        messages.append(message)\n        other.messages.append(message) ... // error: trying to access another actor mutable property\n        print(other.playerTag) // works fine as read only\n    }\n}\n\n\nIn the example above, the compiler complains when trying to modify the mutable property of another actor (cross-actor reference). However, accessing read-only properties poses no issue.\n\nTo address this, you can introduce another function allowing the other MessageThread actor to modify its own state.\n\nextension MessageThread {\n    func send(_ message: String, to other: MessageThread) async {\n        messages.append(message)\n        await other.receive(message)\n    }\n    \n    func receive(_ message: String) {\n        messages.append(message)\n    } \n}\n\n\nWith these modifications:\n\n\n  The send function is now async, because of the await suspension point required to call the receive function in the other actor’s asynchronous context.\n  While the receive function isn’t explicitly marked as async (since it doesn’t have suspension points and operates synchronously), actor isolation in Swift ensures functions behave as implicitly async when invoked from outside their own actor’s context.\n\n\nActors ensure safe execution by maintaining their own dedicated serial executor internally. Messages sent to an actor are termed partial tasks. While processing these tasks, the order of their execution is not strictly guaranteed, as priorities of partial tasks influence the sequence in which they are tackled.\n\nLastly, you can do a cross-actor reference on a mutable property with an asynchronous call as long as it’s read only.\n\nfunc getMessages(thread: MessageThread) async {\n    print(await thread.messages) // works\n}\n\n\nSendable\n\nFinally, to make Actors truly isolated we need to prevent cross-actor references from inadvertently sharing mutable state. The Sendable protocol was introduced to ensure that types shared across actor boundaries don’t introduce data races. This protocol doesn’t provide or dictate specific code behavior, but is leveraged by the compiler to ensure the safety of the concurrent code.\n\nHere are types that can conform to Sendable (some implicitely do):\n\n\n  Value types\n  Actors\n  final classes with immutable and sendable properties (and without superclass).\n  Functions and closures when using the @Sendable attribute.\n\n\nFor a detailed explanation, please refer to the official Apple documentation.\n\nGlobal Actor\n\nGlobal actors are Actors providing a way to extend actor isolation to global and static variables, safeguarding them from concurrent access issues. Global actor can be referenced from anywhere in the program. A common global actor is the MainActor which allows you to execute your code on the main thread.\n\nIn Practice\n\nTheory covered, let’s dive into practical use-cases.\n\nCall Async Functions Sequentially\n\nWhile calling functions sequentially is straightforward in synchronous code, achieving the same in asynchronous code used to be cumbersome, often leading to the Pyramid of doom. Swift’s concurrency model radically simplifies this by using the async/await paradigm.\n\nfunc fetchInfo() async throws -&amp;gt; UserInfo {...}\nfunc fetchImg() async -&amp;gt; ProfileImage {...}\nfunc fetchAct() async -&amp;gt; UserActivity {...}\nfunc saveDB(_ info: UserInfo, _ img: ProfileImage, _ act: UserActivity) async throws {...}\n\nfunc backupUserProfile() async throws {\n    let info = try await fetchInfo()\n    let img = await fetchImg()\n    let act = await fetchAct()\n    try await saveDB(info, img, act)\n}\n\n\nThe use of await ensures each async function completes before the next starts. This sequential execution offers the readability of synchronous code while retaining the benefits of asynchronicity.\n\nCall Async Functions in Parallel\n\nWhen async functions are independent, running them in parallel can save time.  async let allows you to achieve this with minimal code changes. Consider the previous example, modified to execute tasks concurrently:\n\nfunc backupUserProfile() async throws {\n    async let info = try fetchInfo()\n    async let img = fetchImg()\n    async let act = fetchAct()\n    try await saveDB(info, img, act) // Await the results of async let tasks\n}\n\n\nasync let spawns child tasks, sets placeholders on the variables, and allows the code to continue running until it needs the results, which are obtained using await at the end of the function.\n\nCall Async Functions from a Synchronous Function\n\nTask serves as a bridge between synchronous and asynchronous code, enabling you to use async-await without requiring the entire function chain to be asynchronous.\n\nfunc onSavePressed() {\n    Task {\n        do {\n            try await backupUserProfile()\n        } catch {\n            print(&quot;Error backing up profile: \\(error.localizedDescription)&quot;)\n        }\n    }\n}\n\n\nAn alternative is Task.detached. This creates a new top-level task and decouples it from its originating context, allowing it to operate on a different Actor and with a different priority. A typical scenario involves initiating a task from the main thread to execute it on a different thread.\n\nTerminology: Unstructured Concurrency\n\nCreating a standalone Task is known as an Unstructured Task, as it lacks both a parent task and child tasks.\n\nUnstructured Tasks are useful for:\n\n\n  Calling a task from a non-async context\n  Tasks that must persist beyond a specific scope\n\n\nNote: Swift’s use of the terms Structured and Unstructured Concurrency relates only to the hierarchy of Tasks and should not be confused with the broader concept of Structured Concurrency described in the introduction.\n\nQuoting the swift documentation.\n\n\n  Structured concurrency: Tasks arranged in a hierarchy. Each task in a task group has the same parent task, and each task can have child tasks. Although you take on some of the responsibility for correctness, the explicit parent-child relationships between tasks let Swift handle some behaviors like propagating cancellation for you, and lets Swift detect some errors at compile time.\n\n  Unstructured concurrency: Unlike tasks that are part of a task group, an unstructured task doesn’t have a parent task. You have complete flexibility to manage unstructured tasks in whatever way your program needs, but you’re also completely responsible for their correctness.\n\n\nParallel Processing with Task Groups\n\nWhile async let may suffice for handling a limited number of tasks, Task Groups are recommended when a structured approach to parallelism is desired. Here’s an example that employs Task Groups along with an accumulator to safely process an array of data in parallel.\n\nlet processedData = await withTaskGroup(of: Data.self, returning: [Data].self) { taskGroup in \t\n    // Create a new Task within the Task Group for each item   \n    for item in items {\n        taskGroup.addTask(priority: .background) { // Create a new Task within the Task Group\n            await process(item)\n        }\n    }\n\n    var allData: [Data] = []\n    // Asynchronously collect the task results as they complete\n    for await result in taskGroup {\n        allData.append(result)\n    }\n\n    return allData\n}\n\n\nThis code initializes a Task Group and spawns a child task for each item with .background priority. Then an AsyncSequence for await loop asynchronously collects and stores the task results in the allData accumulator as they complete.\n\nCooperative Cancellation\n\nTo enable cancellation within Task Groups, tasks must be built for Cooperative Cancellation, which means the task periodically checks whether it should terminate early. Two methods can be used to check if a task has been cancelled:\n\n\n  \n    try Task.checkCancellation() throws an error if the current Task is cancelled..\n  \n  \n    if Task.isCancelled { break } returns true if the Task is cancelled. Note that this approach might produce partial outputs, which should be documented.\n  \n\n\ntaskGroup.addTask(priority: .background) {\n    if Task.isCancelled { return nil } // Return empty or default Data\n    await process(item)\n}\n\n\nReference and Cancel a Task\n\nUntil now, we’ve only used tasks for running isolated asynchronous operations. However, there are scenarios where maintaining a task reference for potential cancellation is beneficial, as shown in the following static sales dashboard example.\n\nclass SalesDataViewController: UIViewController {\n    private var processingTask: Task&amp;lt;Void, Never&amp;gt;?\n\n    override func viewWillAppear(_ animated: Bool) {\n        super.viewWillAppear(animated)\n\n        guard processingTask == nil else { return }\n\n        processingTask = Task {\n            do {\n                let rawData = try await fetchSales()\n              \tlet chartData = await process(rawData)\n                await showChartData(chartData)\n            } catch {\n                handleError(error)\n            }\n\n            processingTask = nil\n        }\n    }\n\n    override func viewDidDisappear(_ animated: Bool) {\n        super.viewDidDisappear(animated)\n        processingTask?.cancel()\n        processingTask = nil\n    }\n}\n\n\nIn this SalesDataViewController class, we create and keep a reference to a new Task for fetching and processing sales data. If the user exits the view before the task completes, the task is canceled, preventing task accumulation during repeated view transitions.\n\nConvert completion based API to async functions with Continuation\n\nSometimes you encounter legacy APIs not designed to work with Swift’s Concurrency model, often the case with Objective-C-based APIs. Swift offers a solution via Continuation.\n\nContinuation wraps old-style block-based code and adapts it for use in an async function. This enables you to return values or throw errors within that function. Here’s how to apply this with HealthKit as an example:\n\nimport HealthKit\n\nfunc getWorkouts() async throws -&amp;gt; [HKWorkout] {\n    return try await withCheckedThrowingContinuation { continuation in\n        let query = HKSampleQuery(\n            sampleType: HKObjectType.workoutType(),\n            predicate: nil,\n            limit: HKObjectQueryNoLimit,\n            sortDescriptors: nil\n        ) { query, results, error in\n            if let error = error {\n                continuation.resume(throwing: HealthError.myError)\n            } else {\n                guard let results = results as? [HKWorkout] else {\n                    continuation.resume(throwing: HealthError.wrongType)\n                    return\n                }\n                continuation.resume(returning: results)\n            }\n        }\n        HKHealthStore().execute(query)\n    }\n}\n\nAlways ensure to resume a Continuation exactly once; failing to do so can lead to indefinite suspension of the task, resulting in a memory leak, as per Apple’s guidelines. Resuming multiple times is considered undefined behavior and should be avoided.\n\nExecuting Async Code on Main Thread with MainActor\n\nYou can use the MainActor to execute code on the main thread via three ways:\n\nAnnotate your code with @MainActor\n\nApply the @MainActor attribute to properties, functions and classes.\n\nclass MyClass {\n    @MainActor var image: Data // Update occurs on the main thread\n  \n    @MainActor func updateUI() async {\n        // this is now called on the main thread\n    }\n}\n\n// class properties and functions are now run on the MainActor\n@MainActor class MyClass {\n    var image: Data\n  \n    func updateUI() async { }\n}\n\n\nUse @MainActor in Task closures\n\nIncorporate @MainActor within a Task to switch its execution context to the main thread.\n\nTask { @MainActor in \n    // Code runs on the main thread\n}\n\n\nUse MainActor.run\n\nUse MainActor.run within any Task or asynchronous function to force main-thread execution.\n\nTask {\n    let data = await fetchAndProcessData()\n    await MainActor.run {\n        // Executed on main thread\n        await updateUI(with: data)\n    }\n}\n\n\nTips and pitfalls\n\nTask Cheat sheet\n\nFor quick reference, here’s a table taken from Explore structured concurrency in Swift WWDC session.\n\n\n  \n    \n       \n      Launched by\n      Launchable from\n      Lifetime\n      Cancellation\n      Inherits from origin\n    \n  \n  \n    \n      async-let tasks\n      async let x\n      async functions\n      scoped to statement\n      automatic\n      priority, task-local values\n    \n    \n      Group tasks\n      group.async\n      withTaskGroup\n      scoped to task group\n      automatic\n      priority, task-local values\n    \n    \n      Unstructured tasks\n      Task\n      anywhere\n      unscoped\n      via Task\n      priority, task-local values, actor\n    \n    \n      Detached tasks\n      Task.detached\n      anywhere\n      unscoped\n      via Task\n      nothing\n    \n  \n\n\nAsync Protocol Conformance\n\nWhen defining a protocol with async functions, you can conform to the protocol by implementing a synchronous function too.\n\nprotocol MyProtocol {\n    func processData() async\n}\n\nstruct TypeA: MyProtocol {\n    func processData() async\n}\n\nstruct TypeB: MyProtocol {\n    func processData() // also valid\n}\n\n\nReentrancy\n\nIn Swift concurrency, Reentrancy refers to the situation where a suspended block of code resumes execution at a later time. Upon resumption, the mutable state of your code is not guaranteed to remain the same as it was before suspension, posing potential risks of unintended side effects.\n\nTask Suspension and Unowned References\n\nIn Swift’s concurrency model, a Task strongly retains any reference to self, potentially extending the object’s lifecycle unexpectedly, especially if tasks remain active after their parent objects have been deallocated. To mitigate this, developers often employ weak self. However, introducing a suspension point using await within a Task can reintroduce issues associated with unowned references.\n\nclass MyClass {\n    unowned var dataStorage: DataStorage!\n    \n    func refreshData() {\n        Task { [weak self] in\n            guard let self = self else { return } // temporarily retains self\n            \n            let newData = loadDataFromDisk()\n            self.dataStorage = newData // Safe\n        }\n    }\n}\n\n\nIn this example, the code behaves as expected because it executes atomically. If self is available, it is temporarily retained, and newData is updated synchronously.\n\nHowever, introducing a suspension point can lead to issues similar to those encountered when neglecting to check for a weak self.\n\nclass MyClass {\n    unowned var dataStorage: DataStorage!\n    \n    func refreshData() {\n        Task { [weak self] in\n            guard let self = self else { return } // temporarily retains self\n            \n            let newData = await downloadData() // suspension point\n            self.dataStorage = newData // random crash\n        }\n    }\n}\n\n\nHere, if the task suspends during the await, nothing prevents dataStorage’s owner from being deallocated. When the task resumes, attempting to access the unowned property can result in a fatal error since dataStorage is no longer in memory.\n\nActor Reentrancy\n\nActor Reentrancy is a complex behavior that occurs when an actor method makes an asynchronous call, and while waiting for that call to complete, the actor processes other tasks. This can lead to unexpected states within the actor due to interleaved execution of its methods.\n\nactor Counter {\n    var value = 0\n\n    func increment() {\n        value += 1\n    }\n\n    func process() async {\n        increment()\n        print(value) // 1\n        await doLongProcessing() // suspension point\n        print(value) // Unpredictable output (1?)\n    }\n}\n\n\nIn this example, while process() is awaiting the completion of doLongProcessing(), there’s an opportunity for another task to call increment(). This undermines the expectation that an actor’s state remains consistent within a given method. So, the second print(value) may output an unpredictable result, illustrating the challenge of managing mutable state in an actor with reentrant behavior.\n\nAsync Function Execution Contexts\n\nContrary to the behavior in Grand Central Dispatch (GCD), where all code executed within the scope of a block is performed on the same thread, Swift’s concurrency model executes any async function on a global executor unless explicitly specified otherwise, such as with the @MainActor annotation.\n\nNote: a .task{} in SwiftUI runs implicitely on the MainActor when set within the body of a SwiftUI View.\n\nstruct MyView: View {\n  var body: some View {\n    ...\n    .task {\n      // Code within this block is executed on the Main Actor.\n      print(&quot;hello&quot;)\n      // Executed on a Global Executor despite being called from the Main Actor.\n      await fetchData()\n      // Executed on the Main Actor because we explicitly used @MainActor below.\n      await updateUI() \n    }\n  }\n\n\n  func fetchData() async { ... }\n  @MainActor func updateUI() async { ... }\n}\n\n\nConclusion\n\nAs we have seen, Swift Concurrency is a huge step forward in terms of safety and code maintainability. I hope you enjoyed reading this article and learned a few tricks. Dive in, experiment, and harness the power of Swift concurrency. Happy coding!\n\nFurther Reading &amp;amp; References\n\n\n  How async/await works internally in Swift\n  The Bleeding Edge of Swift Concurrency\n  Structured concurrency\n  Async/await\n  Async let\n  Actors\n  Global Actors\n  Concurrency is not Parallelism\n  How to determine where code runs in Swift Concurrency\n  Your Brain 🧠 on Swift Concurrency - iOS Conf SG 2023\n  Where View.task gets its main-actor isolation from\n\n"
} ,
  
  {
    "title"    : "Bedrock au Forum PHP 2023",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2023/10/20/forum-php-afup-2023.html",
    "date"     : "October 20, 2023",
    "excerpt"  : "Cette année encore, Bedrock était présent au Forum PHP pour cette édition 2023.\n\nNous étions sept au total : trois organisateurs, trois visiteurs et une conférencière, Pauline Rambaud.\n\nLes conférences que l’on retient\n\nWhy is PHP still awesome in...",
  "content"  : "Cette année encore, Bedrock était présent au Forum PHP pour cette édition 2023.\n\nNous étions sept au total : trois organisateurs, trois visiteurs et une conférencière, Pauline Rambaud.\n\nLes conférences que l’on retient\n\nWhy is PHP still awesome in 2023 ?\n\n\n  Conférence présentée par Frank Karlitschek, co-fondateur de NextCloud\n\n\nLa conférence s’est divisée en deux parties que l’on pourrait résumer de la\nmanière suivante :\n\n  Une présentation assez détaillée de ce qu’est NextCloud\n  Pourquoi le choix de PHP🐘 pour NextCloud ?\n\n\nNextCloud\n\nNextCloud est un outil comparable à Google Workspace, car il permet la gestion\nde fichiers partagés, de documents de type Office, et propose aussi des outils\nde conversations textuelles et d’appels vidéos.\n\nIl existe en version desktop bien sûr, mais aussi en version mobile.\nOn peut noter qu’il est utilisé par le Gouvernement Français, ou encore\nl’Union Européenne.\n\nEn somme, il constitue aujourd’hui une alternative crédible à ses concurrents\nAméricains ou Chinois.\n\nUne de ses différences majeure toutefois est qu’il est open-source.\n\nPourquoi choisir PHP pour NextCloud ?\n\nLes raisons que Frank nous donne sont les suivantes :\n\n\n  PHP🐘 est facile à déployer\n  Il est indépendant (il n’appartient pas à Google)\n  Son isolation des process garantie une bonne scalabilité\n  Il propose une bonne courbe d’apprentissage\n  Il évolue depuis longtemps et est donc robuste\n  Il a derrière lui une grosse communauté de développeur\n  Un important écosystème d’intégration et de frameworks\n\n\nIl évoque toutefois quelques limites qui subsistent à son sens :\n\n\n  il reste (de moins en moins) facile d’écrire du code non sécurisé, malgré\nle système de typehinting ou les enums\n  il reste des incohérences, par exemple dans les array functions\n  il reste très limité dans la programmation fonctionnelle\n\n\nToutefois, Franck conclura sur le fait que sans PHP, NextCloud n’aurait\njamais été possible.\n\nAugmentez votre couverture : supprimez des tests\n\n\n\n\n  Conférence présentée par Baptiste Langlade\n\n\nAvez-vous déjà été confronté à la problématique de l’augmentation exponentielle\ndu nombre de tests de votre projet, et par conséquent, de l’augmentation du temps\nd’exécution de vos tests ?\n\nCe fut le cas de Baptiste dans l’Application de gestion de Documents à laquelle\nil a participé.\n\nVive le hasard\n\nSa réponse à ce problème fut la suivante : jouer des tests au hasard, autrement\ndit faire du Property Based Testing.\n\nL’idée est de générer aléatoirement des données pour couvrir le plus de cas\npossible.\n\nEnsuite, on écrit un test si un bug survient.\n\nCette approche se base sur la fameuse loi de Murphy qui veut que lorsqu’un problème\ndoit survenir, il arrive toujours trop tôt.\n\nOn mise donc sur l’aléatoire pour faire remonter les bugs plus rapidement.\nAinsi, on détermine que pour tout ensemble de donnée X, l’ensemble des tests\ndoit être vrai.\n\nBlackbox\n\nDe cette approche est née Blackbox, une\nlibrairie compatible PHPUnit permettant de faciliter et d’automatiser la mise\nen place de tests basée sur le Property Based Testing.\n\nElle permet notamment de faire en sorte que les jeux de données que l’on va\ninsérer en input peuvent être vraiment divers et variés, et provoquer des\ncas critiques non répertoriés.\n\nRésultat : plus on joue les tests, plus notre confiance grandit dans l’application.\n\nComment contribuer à PHP en 2023 ? Georges Banyard\n\n\n\n\n  Conférence présentée par Georges Banyard\n\n\nTout d’abord Georges Banyard a introduit la notion de compilation minimale de PHP en C, pour cela il a utilisé son blog\net nous a présenté les choses importantes à connaitre.\n\nPuis, il a fait la présentation du code source de PHP et du moteur de recherche qu’il utilise.\n\nEnsuite, il a commencé à nous montrer quelques ressources auxquelles se réferer si nous désirons contribuer à PHP.\n\nRappelons que PHP est un langage de programmation open-source et que toute personne désireuse de l’améliorer peut\nproposer une implémentation.\n\nLes ressources que nous pourrions utiliser si notre souhait était de créer une nouvelle fonction dans PHP seraient :\n\n\n  des articles sur Zend (https://www.zend.com/resources/writing-php-extensions)\n  le PHP internal books (https://www.phpinternalsbook.com/)\n  ou bien encore la “Room 11” sur Stackoverflow où se retrouve de nombreux contributeurs PHP.\n\n\nGeorges Banyard est ensuite passé à la pratique et a codé en direct une nouvelle fonction de tri dans un\ntableau array_search. Le but de cette conférence était de démystifier la contribution à PHP et de montrer aux gens que\nfinalement ce n’est pas si compliqué, il suffit de plonger dedans !\n\nUtilisez la bibliothèque standard PHP (SPL) au quotidien\n\n\n\n\n  Conférence présentée par Florian Merle\n\n\nLa bibliothèque SPL ça vous dit quelque\nchose ? En réalité, vous l’utilisez déjà au quotidien à travers les Exceptions\nou bien la fonction spl_autoload_register par exemple, mais Florian est venu\nnous parler de certains aspects moins connus et pourtant très utiles.\n\nLes structures de données\n\nLa plus évidente et la plus connue, qui a sans doute contribué à la facilité\nd’accès de PHP🐘 est bien évidemment l’array. Mais si son avantage majeur\nest le fait qu’il soit multi-usage, il se révèle en réalité assez peu optimisé\npour les gros volumes de données.\n\nD’autres structures telles que les listes doublements chaînées\n(SplDoublyLinkedList),\nou les Heaps (SplHeap,\nSplPriorityQueue ont\nété abordées, mais on constate rapidement qu’elles présentent l’inconvénient majeur\nd’avoir de mauvaises performances par rapport à array et des nommages de méthode\npeu intuitif.\n\nPHP DS\n\nUne alternative intéressante est DS,\nqui ne présente pas de problème de gestion de priorité et de meilleures performances.\n\nFlorian nous a présenté notamment DS\\PriorityQueue\net DS\\Vector, qui constitue une\nalternative intéressante à array.\n\nLes iterators\n\nEnfin, nous avons vu les iterators de la SPl avec :\n\n\n  IteratorAggregate\nqui permet d’appliquer un traitement éventuel sur le tableau\n  AppendIterator\nqui permet d’insérer d’autres itérateurs\n  IteratorIterator\nqui renvoie un autre Iterator\n  InfiniteIterator\nqui permet de boucler à l’infini (pensez à la fonction lecture en boucle de votre player audio)\n  CallbackFilterIterator\nqui permet de filtrer les données à l’aide d’une callback.\n\n\nApprendre à apprendre : petit dev deviendra grand - Aline Leroy\n\n\n  Conférence présentée par Aline Leroy\n\n\nAline Leroy nous parle ici de sa reconversion, et des différentes expériences qu’elle a vécues au cours de son\napprentissage du code. Elle nous donne plein d’astuces et de clés pour améliorer notre façon d’apprendre et de\ntravailler notre plasticité cérébrale. Tout d’abord, il faut gérer son temps, alterner concentration et dispersion, et\ncomprendre qu’apprendre c’est créer des liens et des images mentales.\n\nAline Leroy nous conseille de commencer par faire une introspection afin de déterminer quels sont nos points forts et\nfaibles, quels sont nos objectifs et puis pour comprendre son propre fonctionnement. Ensuite, il faut faire preuve de\ncuriosité et ne pas se limiter à un domaine (pensez création de lien, plus le champ est grand, plus vous allez faire\ndes connexions). Nous pourrons aussi choisir de travailler par découpage, une partie des choses à apprendre, nous allons\ndevoir travailler notre mémoire. Il est aussi important que l’apprentissage soit une démarche active, prise de notes,\nrecherche, ne pas catégoriser et surtout, c’est un processus qui demande de la régularité.\n\nElle insiste sur l’aspect bien-être qui est très important pour apprendre, se mettre au calme, couper ses notifications\net puis on prend des pauses, car c’est là que tout le travail de liaison neuronale se fait. Une pause sportive et encore\nplus recommandée, bien sûr ces temps de dispersion doivent être sur un temps maitrisé.\n\nYou Build It, You Run It, l’observabilité pour les devs\n\n\n\n\n  Conférence présentée par Smaine Milianni, développeur chez Yousign\n\n\nL’observabilité est quelque chose de très important chez Bedrock, c’est pourquoi cette conférence était très\nintéressante.\n\nSmaine a découpé sa conférence en posant trois questions : pourquoi, comment et qui ?\n\nPourquoi faire de l’observabilité ?\n\nUne application va forcément planter à un moment, nous dit Smaine pour répondre à sa première question.\n\nL’observabilité permet non seulement de diminuer les risques que cela arrive, mais aussi d’anticiper les problèmes en\namont.\n\nAu-delà des plantages, l’observabilité ouvre la porte à d’autres formes d’analyses :\n\n  détecter les problèmes/changements de performance\n  capter les comportements inhabituels\n  ou simplement s’assurer qu’un système fonctionne correctement\n\n\nComment fait-on ?\n\nHabituellement, l’observabilité s’appuie sur trois piliers : les logs, les métriques et les traces, auxquels Smaine\najoute un quatrième : les alertes.\n\nPour résumer ces différents piliers :\n\n  les logs doivent être lus, et donc être disponibles dans un outil dédié\n  les métriques permettent d’observer l’état de santé d’une application\n    \n      elles peuvent être techniques (CPU, temps de réponse d’une URL, etc)\n      ou métier (nombre de ventes par jour, nombre d’inscriptions, etc)\n    \n  \n  les traces permettent de suivre le comportement d’une application\n    \n      par exemple pour une requête HTTP, on aurait le temps passé dans chaque couche technique (base de données, un\ncontroller PHP, lecture de cache, etc)\n    \n  \n  Et enfin, les alertes\n    \n      elles ne doivent être envoyées qu’en cas de problème (pour éviter de lasser les lecteurs)\n      elles peuvent se baser sur les métriques (même métier)\n      elles peuvent être liées aux logs et aux traces pour aider à les comprendre\n    \n  \n\n\nSmaine nous a également rapidement parlé des post mortems, qui sont essentiels après un incident pour apprendre des\nerreurs et ne pas les reproduire.\n\nQui est responsable de la mise en place ?\n\nL’entreprise de Smaine, Yousign, a une organisation sous forme de squads.\nUn squad est composé de plusieurs métiers : dev, devops, PO, designer, chef de projet, etc.\n\nIls sont alors responsables à la fois de leur périmètre, mais aussi de son observabilité. En gardant cette\nresponsabilité au sein d’un squad, les équipes sont plus attentives et réactives en cas d’incident.\n\nLes femmes et le numérique\n\n\n\n\n  Conférence présentée par Isabelle Collet\n\n\nIsabelle Collet est ancienne développeuse et sociologue épanouie comme elle le dit elle-même. Ce qui lui donne matière à\nnous expliquer beaucoup de choses sur “le choix des femmes de ne pas venir dans la tech”. Et bien, oui, pourquoi\nsont-elles réticentes ?\n\nTout d’abord, Isabelle Collet nous parle de pays qui sont des contre-exemples, comme en Malaisie où les femmes sont la\nmajorité des postes informatiques et de développement, elles sont aussi responsables d’Université. Globalement, dans\ncertains pays d’Asie, les femmes sont fortement représentées dans le secteur tertiaire, et pourquoi ?\nCar ces métiers sont considérés comme des “métiers de femmes”, non salissant, pas physique et surtout possibilité de\ntélétravail ce qui leur permet de s’occuper de leur famille.\n\nPuis grâce à un petit retour dans le passé, nous découvrons que les femmes au départ étaient très présentes dans\nl’informatique et plus précisément dans le développement. Les hommes étaient chargés de la conception des ordinateurs,\nce qui était bien vu dans la société, mais la programmation était un poste considéré comme inférieur. Ce qui va changer\ncela est l’apparition du micro-ordinateur qui va inverser la tendance et les hommes vont devenir plus nombreux à coder\net les femmes vont progressivement “disparaître” du paysage informatique.\n\nPour remédier à cela, Isabelle Collet nous présente deux initiatives universitaires où pour rééquilibrer le taux\nhommes/femmes des quotas ont été imposés pendant une dizaine d’années. Aujourd’hui l’équilibre de candidats se fait\nnaturellement.\n\nEn conclusion, les choses évoluent si on veut bien les faire évoluer. Il faut continuer les efforts d’inclusion et la\nlutte pour que les femmes soient plus nombreuses dans la tech.\n\nConclusion\n\nCette année, la fresque LEGO a célébré la diversité des membres de l’AFUP et bien sûr le PHP.\n\nCe forum fût l’occasion d’échanger avec de nombreuses personnes, de découvrir des sujets aussi bien techniques que\nsociétaux. On ne peut que féliciter les conférencier·e·s et les bénévoles pour un Forum PHP encore très réussi !\n\n\n"
} ,
  
  {
    "title"    : "Siteswap: jongler avec les maths #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/siteswap-jongler-avec-les-maths",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  Présentation des concepts de base du Siteswap, la notation mathématique qui permet de décrire les patterns/figures de jonglerie. Définition, exemples basiques, limites, comment déterminer si une séquence est valide… avec démonstration en direct...",
  "content"  : "\n  Présentation des concepts de base du Siteswap, la notation mathématique qui permet de décrire les patterns/figures de jonglerie. Définition, exemples basiques, limites, comment déterminer si une séquence est valide… avec démonstration en direct de certaines figures.\n\n\nPar Damien Krieger\n"
} ,
  
  {
    "title"    : "Nourrir nos IA #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/nourrir-nos-ia",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  ChatGPT, c’est magique. L’IA, c’est futur de l’homme. Les réseaux neuronaux, c’est la vie. Vraiment ?\n\n\n\n  Je vous propose de voir un peu ce qui se cache derrière l’IA par le biais de celleux qui la construisent : les travailleur·se·s du digita...",
  "content"  : "\n  ChatGPT, c’est magique. L’IA, c’est futur de l’homme. Les réseaux neuronaux, c’est la vie. Vraiment ?\n\n\n\n  Je vous propose de voir un peu ce qui se cache derrière l’IA par le biais de celleux qui la construisent : les travailleur·se·s du digital. Oui, j’ai bien digital et vous verrez que ce terme n’a rien d’abusif.\n\n\n\n  À la fin, vous aurez une vision plus précise de ce qui se trame dans le monde pas si magique des IA.\n\n\nPar Nastasia Saby\n"
} ,
  
  {
    "title"    : "Mentors: super-héros ou super-vilains ? #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/mentors-super-heros-ou-super-vilains",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  Développeuse junior, j’ai eu du mal à trouver ma place et faire mes preuves. Étudiante, vous m’aviez dit : “Deviens PO ou chef de projet !”. Je ne voulais pas d’un métier fonctionnel. Je me suis fixée un but, réussir là où vous, profs, maitres ...",
  "content"  : "\n  Développeuse junior, j’ai eu du mal à trouver ma place et faire mes preuves. Étudiante, vous m’aviez dit : “Deviens PO ou chef de projet !”. Je ne voulais pas d’un métier fonctionnel. Je me suis fixée un but, réussir là où vous, profs, maitres de stage, ne vouliez pas que j’aille. Des rencontres, cauchemardesques comme merveilleuses, tout au long de mon apprentissage, m’ont aidée à confirmer mon choix professionnel : être développeuse ! Comment mes mentors m’ont guidée ? Je partagerai des actions et des comportements qui m’ont incitée à lutter pour atteindre mes objectifs.\n\n\n\n  Vous aussi, vous avez le pouvoir de changer une vie !\n\n\nPar Pauline Rambaud\n"
} ,
  
  {
    "title"    : "Le Don du Sang (et +) #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/le-don-du-sang",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  Donner son sang, c’est unanimement considéré comme une bonne action\n\n\n\n  Mais pourquoi en fait ? Ça vient d’où ? Ça sert à quoi ? C’est dangereux ?? \nEt concrètement, ça se passe comment ?\n\n\n\n  Dans ce talk, on va raconter un peu l’histoire du ...",
  "content"  : "\n  Donner son sang, c’est unanimement considéré comme une bonne action\n\n\n\n  Mais pourquoi en fait ? Ça vient d’où ? Ça sert à quoi ? C’est dangereux ?? \nEt concrètement, ça se passe comment ?\n\n\n\n  Dans ce talk, on va raconter un peu l’histoire du don du sang dans le monde, puis rentrer dans le concret avec le processus de don et les différents types de dons. Et surtout, on va voir à quoi ça sert de donner (pour soi, et pour les autres). Enfin on verra un peu à travers le monde comment ça se passe aussi !\n\n\nPar Quentin Nambot\n"
} ,
  
  {
    "title"    : "La traversée du Finnmarksvidda: carnet de bord d’une aventure glaciale #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-traversee-du-finnmarksvidda-carnet-de-bord-d-une-aventure-glaciale",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  Carnet de voyage de ma traversée en solitaire du plateau du Finnmarksvidda, en Norvège, entre Alta et Karasjok, en mars 2023.\n\n\nPar Sylvain Guyon\n",
  "content"  : "\n  Carnet de voyage de ma traversée en solitaire du plateau du Finnmarksvidda, en Norvège, entre Alta et Karasjok, en mars 2023.\n\n\nPar Sylvain Guyon\n"
} ,
  
  {
    "title"    : "Json au service des devs #LFT 29/09/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/json-au-service-des-devs",
    "date"     : "September 29, 2023",
    "excerpt"  : "\n  Éditer un CV, imprimer des fiches de recette, écrire une lettre de motivation, rédiger un rapport … Tant de choses fastidieuses à écrire et chiantes à mettre en page (surtout quand on ne sait pas utiliser Word). C’est pourquoi j’ai développé un...",
  "content"  : "\n  Éditer un CV, imprimer des fiches de recette, écrire une lettre de motivation, rédiger un rapport … Tant de choses fastidieuses à écrire et chiantes à mettre en page (surtout quand on ne sait pas utiliser Word). C’est pourquoi j’ai développé un outil pour m’aider à faire tout ça en utilisant uniquement du JSON !\n\n\nPar Julie Nginn\n"
} ,
  
  {
    "title"    : "GopherCon UK 2023 highlights",
    "category" : "",
    "tags"     : " conference, london, tech, golang, go",
    "url"      : "/2023/08/12/gophercon-uk-london-2023.html",
    "date"     : "August 12, 2023",
    "excerpt"  : "Hello there! I’m Pierre-Alain, a senior back-end developer at Bedrock and I had the opportunity to go to London for the GopherCon UK.\n\nI travelled by train from Lyon to London, TGV and Eurostar. Train journey not only aligned with my commitment to...",
  "content"  : "Hello there! I’m Pierre-Alain, a senior back-end developer at Bedrock and I had the opportunity to go to London for the GopherCon UK.\n\nI travelled by train from Lyon to London, TGV and Eurostar. Train journey not only aligned with my commitment to minimize my carbon footprint for this trip, but is also a enjoyable travel choice. While the cost was higher than taking a plane, I would like to thank Bedrock for enabling me to take this option.\n\n\n\nWhat is GopherCon UK ?\n\nGopherCon UK 2023 spans three days, commencing with a workshop day on August 16th, followed by two conference days featuring multiple tracks on August 17th and 18th. The event takes place at The Brewery, at Barbican, situated in the heart of the City of London.\nWe were over 500 attendees, including delegates, speakers, and sponsors. This event is all about sharing the latest in Go programming with lots of networking, good food and drinks.\n\nWorkshop day (Practical GO for developers)\nIt was a first time for me to attend a workshop at an event like GopherCon. As a new Gopher, my exposure to writing Go code had been limited. However, I was looking forward to it.\n\nIn the following lines, I’ll be sharing the insights I gained during the workshop — knowledge I’m excited to share to both you and my colleagues:\n\n  \n    Because of some unicode characters, you should not use len() on a string in case you have emojis, hieroglyph or symbol (£世界). It is better to use tf8.RuneCountInString()\n  \n  use fmt.Printf(...) for debugging\n      package main\n\n  import &quot;fmt&quot;\n\n  func main() {\n      s := struct {\n          E int\n          A string\n      }{1, &quot;string&quot;}\n\n      fmt.Printf(&quot;%#v, %+v, (%T)&quot;, s, s, s)\n      // output : struct { E int; A string }{E:1, A:&quot;string&quot;},\n      //          {E:1 A:string},\n      //          (struct { E int; A string })\n  }\n    \n  \n  \n    `http://site.name`  gives a raw string which can be useful when you have slashes in your string, to avoid having to escape them.\n  \n  I liked this image which gives an idea of computer latency compared to a human.\n    Computer Latency at a Human ScaleSource: @ProwessConsult (2017) https://t.co/5Uhw5nCzUJ pic.twitter.com/YlVYnm3nGH&amp;mdash; Josh Jordan (@jordancurve) March 20, 2019\n    \n  \n  a well known bug also known as the for-loop gotcha where the i in the loop is not incrementing correctly.\n      for i := 0; i &amp;lt; 5; i++ {\n      go func() {\n          fmt.Printf(&quot;gr:%d\\n&quot;, i)\n      }()\n\n      //fix 1\n      go func(n int) {\n          fmt.Printf(&quot;gr:%d\\n&quot;, n)\n      }(i)\n\n      // fix 2\n      i := i\n      go func() {\n          fmt.Printf(&quot;gr:%d\\n&quot;, i)\n      }()\n  }\n    \n  \n  Goroutine channel tricks\n    \n      \n        send/receive will block until opposite operation (*)\n          \n            buffered channel of cap n has n non-blocking sends\n          \n        \n        receive from a closed channel will return the zero value without blocking\n        you can use val,ok := &amp;lt;- ch as second left variable to know if value comes from channel or not\n        send or close to a closed channel will panic\n        send or receive on a nil channel will block forever…\n      \n    \n  \n  \n    doc.go is used to add documentation to packages, example below:\n\n      /*\n  Package nope does nothing as Go package.\n\n  ...\n  */\n  package nope\n    \n  \n  example_test.go on a package, gives example tests to other developers\n\n\npackage nope_test\n\nimport (\n  &quot;fmt&quot;\n\n  &quot;github.com/shipt/nlp&quot;\n)\n\nfunc ExampleTokenize() {\n  nothing := nope.SayNothing()\n  fmt.Println(nothing)\n\n  // Output:\n  // &quot;nothing&quot;\n}\n\n\n\n  \n    you can create a testdata folder to store fixtures which would be avoided by compiler\n  \n  \n    we should use testify/require, testify/mock and testify/suites evil laugh\n  \n\n\nWorkshop given by Miki Tebeka, Ardan Labs was really great and gave the opportunity to compare myself to what is expected from a new Gopher, and so far I would say I’m doing OK. I am now eager to tackle more projects in Go at Bedrock. \\o/\n\nDay one\n\nUpon arrival, I was greeted with a delightful British-style Gopher plush, serving as a warm and wonderful welcome gift.\n\n\n\t\n\n\nThen, I enjoyed a truly good breakfast (kudos to @Formal for the exceptional coffee baristas) and engaged in conversations with the sponsor booths positioned within the venue, and even entered their raffle giveaways.\n\nScaling Coffee with Goroutines (workshop tutorial)\n\nSadie Freeman’s enlightening tutorial walks us through the process of taking advantages from goroutines to achieve scalable codebase and platform effectively.\n\nThe coffee shop challenge\n\nImagine a coffee shop aiming to do:\n\n  a lot of coffee\n  for a lot of people\n  as fast as possible\n\n\nFor a coffee to be considered complete, three essential actions were required:\n\n  Accept payment\n  Steam milk\n  Make espresso\n\n\nEach of these actions takes approximately 2 seconds to execute.\n\nTo break it down:\n\n  Serving one customer requires 2 seconds per action, totaling 6 seconds.\n  Serving three customers extends the total time to 18 seconds.\n\n\nScaling for Speed\n\nInitial attempts to introduce goroutines seem straightforward, by merely appending “go” before a method:\n\n go MakeCoffee()\n\n\nHowever, for three customers, this results in a staggeringly brief 49 microseconds, practically an impossible feat. The solution wasn’t as straightforward as it may seem.\n\nEnter the sync.WaitGroup\nsync.WaitGroup()\n\ndefer wg.Done()\n\nwg.Wait()\n\n\nNonetheless, even with this approach, we still find ourselves waiting for each coffee to complete before initiating the next one. Consequently, serving three customers still demands 6 seconds.\n\nOptimization beckons; each action could potentially transform into its own goroutine. Ultimately, with this approach, serving three customers takes a mere 2 seconds, finally achieving efficient customer service.\n\nScaling for Load\n\nEnter containerization! Deployment onto Kubernetes through Docker containers ensues. The coffee shop pods are allocated finite CPU and Memory resources.\n\nConsider a scenario where 300 customers arrive simultaneously on our webservice. This influx of customers causes the pod to become memory-intensive, ultimately leading to an Out of Memory (OOM) kill.\n\nBut what happens when the challenge escalates to 3000 customers? This needs vertical scaling: allocating more resources. However, this approach can become prohibitively expensive.\n\nEnter the pragmatic solution of horizontal scaling - opening more pods (coffee shops). With 10 pods, each handling 300 customers, accommodating 3000 customers in about 2 seconds becomes feasible.\n\nWhile this tutorial doesn’t delve into the topic, an intriguing possibility would be to split responsibilities in different pod(payment, milk, espresso) for further optimization and efficiency.\n\nThe 7 Deadly Sins for Gophers\n\nJohn Gregory provided invaluable insights into the pitfalls to avoid for Gophers, whether seasoned or new. Here’s a condensed overview of the key takeaways:\n\nLush\nThe urge to rush into production without due consideration. Concurrency might entice you with the “go” keyword, but remember:\n\n  Use goroutines judiciously, only when necessary.\n\n\nWrath\nSubstituting “panic” for proper error handling:\n\n  “Using a wall to stop a car instead of the brakes.”\n\n  Embrace defensive coding practices.\n\n  Reserved methods should be utilized outside user runtime; employ them at startup.\n\n\nGreed\nThe desire to future-proof everything:\n\n  Avoid over-engineering; simplicity often prevails.\n\n\nSloth\nFocusing on “what” rather than “why” in comments:\n\n  Enhance error handling with contextual information. Seek semantic context.\n\n  Utilize fmt.Errorf(&quot;this did not work: %w&quot;, err) for enhanced error messages.\n\n\nGluttony\nUnnecessary reliance on frameworks:\n\n  Beware of vulnerabilities.\n\n  Begin with simplicity.\n\n  Embrace libraries with robust support.\n\n\nEnvy\nForcing patterns where they aren’t required:\n\n  Interfaces aren’t always necessary.\n\n  Explore “Effective Go” principles.\n\n\nPride\nAssuming you possess the ultimate knowledge:\n\n  Avoid artificially restricting API access.\n\n\nJohn Gregory’s wisdom shines a light on the potential pitfalls Gophers may encounter. Steering clear of these seven deadly sins can lead to more effective and resilient Go programming practices.\n\n\nSocial event with food and drinks (responsibly)\nI (also) enjoyed video game, table tennis, a magician and a DJ !\n\n\n\nDay two\n\nI started this second day of conferences by having a Cup of tea (with milk) to get myself in the shoes of an English Gopher :)\n\nState of the Go Nation!\nCameron Balahan, Product Manager of the Go team, took us on a journey through the past, present, and future of Go. He shared insights into the language’s evolution, growth, and its roadmap ahead.\n\nGoing Back in Time\n\n  2007: Go’s inception within Google.\n  2009: The pivotal moment when Go was opensourced.\n  2012: The groundbreaking release of Go 1.0, marking its maturity.\n\n\nThe Stability of Go 1.0\nBalahan emphasized the continuity of Go 1.0, ensuring backward compatibility with all Go 1.* versions.\n\nRefinement and Progress\n\n  2015: Go’s strides in version 1.5:\n    \n      Advancements in the compiler.\n      Introduction of a new garbage collector.\n      Adoption of semi-annual releases.\n    \n  \n\n\nThe Flourishing Ecosystem of 2018\n\n  2018: Go’s ecosystem flourished, introducing key enhancements:\n    \n      Go modules with caching and checksum features.\n      A focus on creating a secure platform.\n    \n  \n\n\nForever Go 1.*\nBalahan said Go 2 “will never happen”, as keeping Go 1 program working with all Go 1.* version is their aim from the start.\n\nToday’s Triumphs\nIn the present:\n\n  Go’s user base has multiplied by four since 2018.\n  High levels of user satisfaction persist among the Go community.\n\n\nThe Path Ahead\nLooking forward:\n\n  Addressing the loop variable issue. (mostly done in Go 1.21)\n  Enhancing onboarding experiences.\n  Strengthening vulnerability management.\n\n\nA Bonus for the Dev Community\nIn the spirit of improving security, Balahan encourages developers to integrate the new Govulncheck tool into their CI pipelines. This tool developed by Go team stands as a sentinel, safeguarding against potential vulnerabilities within Go packages.\n\nEfficient Debugging and Logging with OpenTelemetry in Go\nIn the past, our debugging approach was straightforward: Log everything, locally or remotely. However, the landscape has evolved, favoring a combination of logs, traces, and metrics to illuminate the path.\n\nA quote I really liked:\n\n\n  “Logs are for your future self, left by your past self :)”\n\n\nWhile logs remain indispensable, their role can now be enhanced.\n\n\n  \n    Absence of Request Context - Log entries often contain detailed information about microservices, functions, or applications, yet they can fall short in providing the broader request context.\n  \n  \n    Scale Demands Complexity - To grasp an application’s normal functioning, collating and analyzing numerous log entries becomes vital. This can require extensive indexing and complex tools to achieve at scale.\n  \n  \n    Disparity Across Teams and Services - Uniformity in logs isn’t guaranteed across teams, adding complexity when attempting to link them cohesively.\n  \n\n\nEnter Distributed Tracing:\nA game-changer in the debugging realm, distributed tracing offers a comprehensive view of a request’s lifecycle. It stands as a potent tool to combat production issues effectively. With end-to-end visibility, distributed tracing is meant to level up your debugging power!\nWe, Bedrock, already been using distributed tracings for a while in our PHP codebase, which allow us to rapidly debug and understand problems in production when incidents happen ;)\n\nThe Hacker’s Guide to JWT Security\nJSON Web Tokens (JWTs) come with their own set of caveats that need to be carefully considered to ensure a robust security level.\n\nThe Pronunciation Puzzle: How to Pronounce JWT?\nBefore we dive into the security aspects of JWTs, let’s clarify a common query: how do you pronounce JWT? According to the standards outlined in RFC 7519, JWT is pronounced as “jot”. So, the next time you discuss JWTs, remember it’s neither three separate letters nor “JWT Token” as some say in France…, but a concise “jot”.\n\nThe Encoding vs. Encryption Distinction\nOne fundamental fact about JWTs is that they are designed for encoding, not encryption. This means that the data contained within a JWT is base64url encoded, which allows easy transmission between parties, but it’s not encrypted in a way that prevents unauthorized access to the actual content.\n\n1. The “none” Algorithm\nIn scenarios where the backend fails to verify the algorithm before checking the claims within the JWT, an attacker can exploit this weakness and craft a forged token. This issue lies not just in the hands of the RFC specifications but also in implementation.\n\nTo address this concern, it’s imperative to implement rigorous checks at the backend level. Verification of the algorithm should precede the verification of claims. This approach ensures that only tokens with valid algorithms are processed, mitigating the risk of unauthorized access.\n\n2. HS256: Password/Key Cracking Vulnerability\nThis algorithm employs a shared secret key for both encoding and decoding, making it susceptible to password or key cracking attacks. What’s particularly concerning is that even a single compromised token can lead to offline attacks, where no communication with the server is necessary while craking.\n\nTo counter this threat, regularly update and rotate the secret keys, ensuring that even if one key is compromised, the potential damage is limited.\n\n3. Man in the Middle on Internal Networks\nIn a man-in-the-middle attack scenario, an attacker can intercept and manipulate JWTs exchanged within an unsecured network, potentially gaining unauthorized access.\n\nTo minimise this risk, it’s recommended to adopt secure communication protocols such as HTTPS. Additionally, implementing strict network segmentation and proper access controls can minimize the attack surface within internal networks.\n\n4. XSS Vulnerabilities: Safeguarding Tokens\nStoring JWTs in local storage, which can be accessed by scripts, worsen this risk to counter XSS threats: consider using cookies to store JWTs. Cookies are less susceptible to XSS attacks, as their content cannot be directly accessed by Javascript scripts.\n\nBy adopting this practice, the risk of token theft through XSS is substantially reduced.\n\nTo sum up, Bedrock already adapted these principles for a while, but nevertheless it was great to remind myself to think about security and vulnerabilities around JWTs.\n\nHow NOT to Write a Test in Go\n\nAmir Malka stated that adhering to these practices elevates Go testing from trivial to masterful. Effective testing enhances code reliability and development predictability. Embracing these guidelines lays the foundation for successful software projects.\n\nWhy Testing?\n\n  Tests uncover bugs and validate code changes.\n  Tests build confidence in code correctness.\n  Tests serve as living documentation, illustrating code behavior.\n\n\nKey Testing Principles\n\n  Fixture Placement: Organize test fixtures in a dedicated testdata directory, avoided by compiler.\n  Structured Tests: Embrace table-driven tests for readability and comprehensive coverage.\n  Parallelism: Use caution with parallel tests to avoid unintended interactions.\n  Test Suites: Employ testing.M for related tests and control setup/teardown with TestMain.\n  Categorization: Use build tags or environment variables to categorize and skip tests.\n  Concurrency Safety: Detect data races with the -race flag during testing.\n  Effective Mocking: Benefit from embedded interfaces for accurate mocks.\n  Randomizing Tests: Introduce randomness with -shuffle flag to uncover hidden issues.\n  Benchmarks: Gauge code performance with testing.B, setup/teardown carefully.\n  Test Automation: Automate tests in CI pipelines to maintain consistent quality.\n  Code Coverage: Use -cover flag to monitor and improve test coverage.\n\n\nComing from PHP, I never thought about parallelism or Benchmarks while writing tests on a daily basis. All of these testing principles will help me and hopefully my team (when they would have read this post!) to write test the “Go way”.\n\nConclusion\nIn closing, my journey through GopherCon UK 2023 has been as exciting as tiring for my first Golang Conference. I thank a lot Bedrock for affording me the opportunity to partake in this experience. Participating in this event helped me to dive deeper into the world of Go and will enhance my professional growth.\n\nTo the organizers of GopherCon UK 2023, your planning and dedication have resulted in a seamless event. Every aspect has been orchestrated to perfection. Congrats to you ! Also, thanks to the sponsors, for continuing to take part in these events during financially hard times.\n\nAs I return to my daily routine armed with the knowledge gained, I am excited to share these learnings with my colleagues and hope to contribute even more effectively to my team. GopherCon UK 2023 has not only expanded my knowledge but has also strengthened my enthusiasm for Go.\n\nGopherCon UK 2024, hopefully, here I come (with other of my colleagues) ! Cheers!\n"
} ,
  
  {
    "title"    : "Bedrock at We Love Speed 2023",
    "category" : "",
    "tags"     : " conference, paris, tech, webperf",
    "url"      : "/2023/08/11/we-love-speed-2023.html",
    "date"     : "August 11, 2023",
    "excerpt"  : "The Frontend Bedrock teams were present at the 2023 edition of the We Love Speed conference in Paris on May 10. Its aim is to share as widely as possible knowledge and experience in the field of web performance.\n\nWebperf, or web performance, refer...",
  "content"  : "The Frontend Bedrock teams were present at the 2023 edition of the We Love Speed conference in Paris on May 10. Its aim is to share as widely as possible knowledge and experience in the field of web performance.\n\nWebperf, or web performance, refers to the set of techniques and best practices aimed at optimizing the loading speed and user experience (UX) of websites. It covers various aspects such as page loading time, interface responsiveness, animation fluidity, server request processing capacity, etc.\n\nWebperf is still a major challenge in today’s web industry, influencing UX, website visibility and overall commercial success. This article summarizes the topics discussed at this year’s We Love Speed conference and gives an overview of the 2023 approach to web performance.\n\nTable of contents\n\n\n  Why optimize your sites’ webperf?\n    \n      It’s all about money 🤑\n      What about SEO?\n    \n  \n  How do you take up performance projects?\n    \n      Build a long-term strategy based on usages\n      Take cognitive biases into account to prioritise optimisations\n      Performance must be a company-wide concern\n    \n  \n  What are the tools to measure and improve performance?\n    \n      Use the right tools to understand performance issues…\n      …and the right approach to fix them\n      MPA vs SPA\n    \n  \n  Implement performance improvements\n    \n      Using bfcache\n      Preloading, Preconnecting and HTML\n      Lazy…\n      Taking care of the Consent Management Platform (CMP)\n      Self-host blocking thirdparties\n      Use Server-side Rendering (SSR) and optimize it!\n      103 Early hints\n    \n  \n  Webperf at Bedrock\n\n\nWhy optimize your sites’ webperf?\n\nIt’s all about money 🤑\n\nImproving the webperf just for the technical challenge is not interesting in a company’s perspective and can’t make the approach durable. Boris Schapira in his talk “Parlons de valeur” 🇫🇷 shows how important it is to talk about value to make optimisation projects part of an organisation’s strategy.\n\nWeb performance is essential, as it has a direct impact on user satisfaction, engagement and the overall success of a website. Fast performance enables users to access information quickly, interact without delay and navigate smoothly. On the other hand, slow loading times, frozen interfaces or high response times can lead to user frustration, lower engagement and higher bounce rates.\n\nA lot of articles on the net have proved the impact of the webperf on users’ engagement.\n\n\n  Here is a great article from Google or hundreds of case studies from WPO stats, a site listing case studies demonstrating the impact of web performance optimization.\n\n  Reducing perceived waiting time by 40% increased search engine traffic and sign-ups by 15% at Pinterest. For Mobify, each 100ms reduction in homepage loading speed yields an average increase in annual sales of almost $380,000!\n\n\nSo, depending on the site’s field, it’s relatively easy to link the webperf approach to an objective of user views or loyalty, but also with the brand image. And the company’s governance will be able to quickly measure the benefits in terms of return on investment.\n\nWhat about SEO?\n\nMoreover, search engines such as Google also give a relative importance on website performance in their ranking algorithms. More optimized sites often benefit from better positioning in search results, which can have an impact on their visibility and traffic.\n\nHowever, Philippe Yonnet’s talk “Quel est le véritable impact des problèmes de web performance sur le SEO ?” 🇫🇷 reveals that actually the Google webperf scoring is in reality quite low in all the parameters taken into account in the ranking algorithm. Relevant content is always the first factor in top positioning. As it was the case with responsive design a few years ago, the Google’s communication around webperf is aimed more at encouraging good practices in the web community than at penalizing slow sites.\n\nHow do you take up performance projects?\n\nBuild a long-term strategy based on usages\n\nYou need a strategy and a speech! Boris Schapira 🇫🇷 defines 5 steps:\n\n\n  Identify the sticking point\n  Associate a value to the project for the business\n  Invest time for the optimizations\n  Evaluate the ROI\n  Sustain the approach over time (it should never be a one shot project)\n\n\n\n  We now universally use the Google’s Core Web Vitals KPI (CWV) to model the UX:\n\n  \n    Do users have a stable visual response? ➡️ it’s measured through the Cumulative Layout Shift (CLS)\n    Do users quickly see content they can trust? ➡️ it’s measured through the Largest Contentful Paint (LCP)\n    Can users quickly interact with the page in a qualitative way? ➡️ it’s measured through the First Input Delay (FID)\n  \n\n\nBut these KPI are a limited reflection of the UX, and it is hard to link it to a representative value. It’s possible to have a super-fast LCP and still have a feeling of slowness or discomfort on the user’s side. For example, a page that quickly displays the thumbnail of a film (LCP criterion) but takes a long time to display the film’s cast list (which is the content the user wants) won’t be great for the UX.\n\nYou need to correlate these measures with usages. There is no magic solution for this but segmenting sessions is a good practice to focus on relevant optimizations:\n\n\n  take a period of time where the measure is representative (e.g. on our streaming sites, the activity period is mainly in the evening)\n  take into account the fact that different actions or events on your site may not involve the same users (so not the same devices, etc.)\n  take “short” indicators to measure the impact of the performance (e.g. if users are on a product list, monitor the viewing of the product page and not the purchase of the product)\n  take into account the market you’re addressing (e.g. food click &amp;amp; collect users are more likely to wait the loading of the next page than users on a retail site)\n\n\nTake cognitive biases into account to prioritise optimisations\n\nWhen it comes to prioritising performance subjects, there are several strategies to consider. Philip Tellis in his talk “Understanding Cognitive Biases in Performance Measurement” 🇬🇧 shed light on the psychological aspect of performance, focusing on cognitive biases.\n\nPhilip made a powerful statement: “If you have a brain, you have a bias”. Let’s explore three examples that demonstrate how biases can impact your work and how you can leverage them to your advantage.\n\nSlowest is the norm\n\nThe negativity bias reveals that negative experiences tend to leave a stronger impression than positive ones, even if their intensity is the same. This is crucial to keep in mind when optimizing your app’s performance. Ideally, the slowest page should not be more than 15 times slower than the fastest page. Users might perceive the slower page as the norm for your app.\n\nTo counteract this bias, it’s important to be transparent with your users. You can display a message indicating that the page is slow and assure them that efforts are underway to resolve the issue. Interestingly, slow pages have been associated with a 38% increase in heart rate, similar to watching a horror movie. By prioritizing performance and being honest with your users, you can mitigate stress and anxiety.\n\nTake care at the beginning and end\n\nThe serial position effect highlights that people tend to remember the beginning and end of an experience more vividly. This is particularly relevant for our work in developing applications for Smart TVs, where performance challenges arise in two areas: page loading and remote control navigation. Taking this bias into account, we’ve learned that investing more time in optimizing page load times can significantly increase end-user satisfaction. Furthermore, statistics show that a 500ms increase in delay leads to a 26% rise in frustration.\n\nTake care at the beginning 😅\n\nThe escalation of commitment bias refers to the inclination to persist in an endeavor once a significant investment of money, effort, or time has been made. Applying this bias to our app development, we recognize the importance of ensuring that the initial pages users encounter are fast, as this positively influences user retention. There is a direct correlation between a smooth initial experience and increased user engagement.\n\nThese are just three examples among many others that Philip presented in his thought-provoking talk. It was an insightful perspective on performance, viewed through the lens of our cognitive biases. We all possess biases, and as developers, our objective is to acknowledge them and harness their power to inform the future updates of our apps.\n\nPerformance must be a company-wide concern\n\nWhile optimizing specific parts of your app is essential, it’s important to remember that you work in a company composed of diverse individuals with varying roles, perspectives, and concerns. Therefore, it’s crucial to extend performance awareness across all departments of your organization.\n\nLa Redoute, an international clothing retailer, provides an excellent example of addressing this issue by creating a performance community within their company through the talk “Comment construire une communauté Web Perf dans son orga ?” 🇫🇷.\n\nTo provide some context, La Redoute faces not only performance issues but also challenges related to synchronization and contributions on their platform. To tackle these challenges, they established a community that brings together individuals from different departments, including technical, marketing, finance, design, product, SEO, and more.\n\nIt’s vital to ensure that performance is not solely the responsibility of one department, as each department can contribute valuable insights and support that ultimately benefit the entire company.\n\nThis community meets weekly to exchange knowledge and share updates on performance-related topics. After six months, they began witnessing the initial benefits, and within one year, they deemed the community mature. They continue to hold these meetings, recognizing that the objective is not to prevent all issues but rather to minimize their impact and respond swiftly.\n\nLa Redoute’s approach is truly inspiring and serves as a great example of fostering collaboration and cross-departmental engagement to address performance challenges.\n\nWhat are the tools to measure and improve performance?\n\nWhen analyzing a website’s performance, it’s essential to take several key criteria into account in order to become the “Sherlock Holmes of Web Performance” as would say Ludovic Lefebvre 🇫🇷. In addition of the CWV mentioned above, there are other commonly used performance analysis criteria:\n\n\n\nTo measure/evaluate your site’s performance according to these criteria, there are 3 types of sources: Local (devtools), Synthetic (CI/CD / online scan tools), RUM (Real User Monitoring).\n\n\n\nUse the right tools to understand performance issues…\n\nJean-Pierre Vincent gives more details about how to measure performance in his talk “Mesure ou meurs : diagnostics rapides” 🇫🇷, but here is a summary of the key tools:\n\n\n  Local development tools - devtools\n    \n      Chrome &amp;amp; Firefox DevTools: a set of development tools integrated into the browser, for analyzing performance, debugging JavaScript code, inspecting DOM elements, etc.\n      Lighthouse DevTools (Chrome): a Chrome DevTools extension that provides automated audits to evaluate performance, accessibility, SEO optimization, etc.\n    \n  \n  Synthetic testing - CI/CD and online analysis tools\n    \n      WebPageTest: an online tool for evaluating website performance by running tests on different browser configurations, network connections and geographical locations.\n      Lighthouse CI: a Lighthouse-based continuous integration (CI) solution that automates web performance and quality audits with every deployment.\n    \n  \n  Real User Monitoring\n    \n      New Relic: an application performance monitoring platform that provides real-time visibility into the performance of your web applications, including usage metrics, load times, etc.\n      Grafana: a data visualization platform that can be used to display and analyze performance monitoring data collected from a variety of sources, including RUM tools.\n    \n  \n\n\n\n\nIt’s important to note that each type of analysis data has its own advantages and is used in specific contexts. Local data is useful for real-time development and debugging, synthetic data enables performance to be compared and optimized in a reproducible way, while RUM data, just as explained by Tim Vereecke in his “Noise Canceling RUM” talk 🇬🇧, provides a true view of UX. It is generally recommended to use a combination of these different data sources to identify/target problems or opportunities for performance improvement.\n\n…and the right approach to fix them\n\nOnce you’ve identified the relevant performance criteria and used the analysis tools to measure your website’s current performance, it’s essential to stabilize the various criteria before setting improvement targets and finally implement said improvements. Here are the steps to follow:\n\n\n  Stabilize criteria\n    \n      Analyze the results of the performance measurements for each criterion you’ve identified. Identify areas where improvements are needed, and problems that affect performance stability. For example, LCP depends on the appearance of the widest element on the page. It’s important that this element is always the same on the same page type, otherwise there will be no consistent result and optimization will be impossible.\n      Examine performance metrics over a sufficiently long period of time to detect trends and variations. This will help you understand whether performance problems are constant or sporadic.\n      Identify factors that could influence performance, such as code changes, server updates, new features, content contributions, etc.\n    \n  \n  Setting improvement targets\n    \n      Determine the performance levels you want to achieve for each criterion, based on benchmarks, best practices or objectives specific to your application.\n      Define measurable and realistic objectives for each performance criterion. For example, you could aim to reduce page load time by 20%, improve server response time by less than 200 ms, or reduce bounce rate by 15%.\n      Prioritize goals according to their impact on your website’s UX and business objectives (see above section about cognitive biases).\n    \n  \n  Plan and implement improvements\n    \n      Identify the specific actions to be taken to achieve each improvement objective. This may include code optimizations, server configuration adjustments, resource optimizations, caching, etc.\n      Establish an action plan for each objective, defining the necessary steps, responsibilities, deadlines and resources required.\n      Implement improvements iteratively, and measure performance regularly to assess the impact of changes.\n      Repeat the process of analyzing, stabilizing and improving performance iteratively to continue optimizing your website.\n    \n  \n\n\n\n  ⚠️ It’s important to bear in mind that website performance is often an ongoing process, as needs, functionalities and conditions change over time. It is therefore advisable to regularly monitor performance and continue to identify new opportunities for improvement to deliver a superior UX.\n\n\nMPA vs SPA\n\nThe performance possibilities are not the same between Single Pages Applications (SPA) and Multiple Pages Applications (MPA).\n\nMPAs are simple to measure: each page is independent. The first page loads resources and then caches certain resources. It’s therefore important to be able to distinguish between measurements based on a user’s first visit and those based on browsing (which is much faster).\n\nSPAs are more difficult, because apart from the initial page load, there is other page load. Initial loading is often heavier (because of JS framework), and therefore slower. Page changes can’t be measured simply since only the initial load is taken into account. Research is currently underway in browsers to improve the collection of information on page changes in SPAs. This new metric is called soft-navigation and is currently being tested on Chrome and specified by web organizations. Yoav Weiss explains this in his talk “Soft Navigations are hard!!” 🇬🇧.\n\nImplement performance improvements\n\nThere are many technical tips for improving performance. While the list is far from exhaustive, we can go here through some of those mentioned at the conference.\n\nUsing bfcache\n\nAt the time of writing, around 10% of a given user’s navigation consists of clicking on the “Go back” button of their browser. More often than not, however, these users will then need to wait for the page to load again, despite the fact that the browser already did that work a few seconds beforehand.\nIn order to help reduce that waiting time, browsers recently began to release what’s called the Back/Forward cache (Also known as bfcache).\n\nLike its name suggests, it caches the page the user is leaving, so that if they go back using their browser features, they’ll see the cached page instead. Barry Pollard talks about the bfcache in the “Top Core Web Vitals Recommendations for 2023” talk 🇬🇧.\n\nPreloading, Preconnecting and HTML\n\nAnother possible improvement explained by Barry Pollard is to use HTML properties to preload and preconnect resources. He explains here that there is one important rule with HTML and the way it’s processed by web browsers: it is parsed line by line, and it can be paused.\n\nWhen a browser is trying to render a web page, if it runs across something trying to call an url (such as loading a CSS file), then it’ll wait until said CSS file is fully loaded, before being able to keep going.\nHowever this behavior can be somewhat altered by using the rel HTML property to prepare some of that stuff ahead. For example, rel=”preload” will tell the browser to start loading said CSS asynchronously, as soon as a user opens the page. For an API, it will be the rel=”preconnect” that will allow the website to start the handshake with an API as soon as possible.\n\nIf a resource needs to be even higher-priority, there is the fetchpriority attribute which will make the browser fetch that resource as soon as possible. If used correctly, it can have immense benefit on LCP.\n\nLazy…\n\nImages, and especially high-quality ones, can take up a lot of a website’s loading time, and are often considered the Largest Content of a website, dictating the LCP value. However, usually a user isn’t seeing every image of a website at the same time. That’s where we can use lazy-loading.\n\nThe goal of lazy-loading is to make sure that a user is only loading the images they are seeing. Here again exists an HTML attribute that can take care of everything, named loading. It can receive the lazy value in order to only load an image when approaching the screen.\n\nHowever, that lazy-loading property is not limited to images only. A component can be lazyloaded as well, such as a chat component that doesn’t need to load until the user clicks on the chat icon.\n\nTaking care of the Consent Management Platform (CMP)\n\nA lot of websites, us included, are using a lot of third parties. We are not judging their usefulness here, we need them. This is why Andy Davies dedicates an entire talk about how to tame the speed impact of 3rd-party tags 🇬🇧.\n\nAs he explains, each new thirdparty comes with its own compounding issues. Adding another domain and thus another need to connect to a distant API, reheating the connection… A lot of these actions are taking network time, time that is actively needed to load a lot more data for a website.\n\nIt is important to understand how our thirdparties work, and the limitations they inherently have. For example, we CANNOT, and should NEVER try to preconnect to them. Preconnecting to a thirdparty discloses a user IP address, which according to GDPR laws at the time of writing, is considered a personnel data. If our user did not consent to share their data, we should never try to connect in the first place.\n\nTherefore, a big hurdle is that we need to collect the user’s consent to be able to connect to our thirdparties API. The first improvement to make is therefore to optimize the CMP, to make sure it loads fast and it shares the user’s consent with us as soon as possible.\n\nSelf-host blocking thirdparties\n\nSometimes, thirdparties can be needed to display anything at all to the user (e.g. for A/B testing). Since our user’s browser has limited network capabilities, it will try to prioritize its requests, and requests that are going to another domain are far down the line. For this exact reason, Andy Davies, in his talk 🇬🇧, recommends to self-host these thirdparties, so that they are on the same domain as our website and are prioritized by the browser.\n\nUse Server-Side Rendering (SSR) and optimize it!\n\n\n  ℹ️ This is far from an easy to implement solution, and may require you to change your framework altogether.\n\n\nImplementing SSR is a solution that is so powerful that it had a dedicated talk, where Kévin Raynel and Marting Guiller explained how Lazy Hydrate, Never Hydrate, and Resumable JS 🇫🇷 can be used to improve Web Performance.\n\nBy allowing the code of an application to be pre-rendered in a server before being sent to a browser, you can dramatically decrease the time it takes for a user to be able to interact with the page. At Bedrock, we understood the power of SSR very early on, when we implemented it with React in 2015 for the new JS version of 6play.fr! Since then, we haven’t looked back, but we will need to fine-tune the hydratation to keep the best.\n\n103 Early Hints\n\n\n  Do note that this is an experimental feature ⚠️\n\n\nIn his excellent Web Protocols for Frontend Developers talk 🇬🇧, Robin Marx details extensively what is called the 103 Early Hints feature and how it can be used to improve your performance.\n\n103 Early Hints is a status code used to send preliminary headers in a response, providing the client with potential resource information before the main response is sent. This enables the browser to optimize its call flow.\n\nThere are many other implementation tips covered by the conference, such as the new Speculation Rules API, but we can’t talk about everything here 😉\n\nWebperf at Bedrock\n\nBedrock’s technical teams have long been aware of the importance of web performance. Since we set up our first full JS frontend in 2015, we have been implementing best practices (SSR, lazy loading, prefetching, code spitting, image optimization, etc.) and we try to carry out regular optimization projects.\n\nBut we know that we still have a lot to do in this area, particularly in terms of long-term monitoring and company-wide concern.\n\nFollowing this conference, there are lots of things we’d like to try out, such as using the fetchpriority attribute to improve our LCP, optimizing the hydration of our SSR (perhaps with the use of React Server Components) or studying the new experimental features.\n\nOn our TV platforms developed in JS, hardware constraints are forcing us to go even further in optimizing performance, particularly in terms of memory usage. For example, a project is underway to use a global lazy loading to improve the management of rows of programs and videos. To be continued…\n\nKeep at it! 💪\n\nAll in all, performance is definitely anything but an easy task to do. There is no magic formula that will make all your problems go away. Even if you manage to hold a long workshop of several weeks (or even months!) that will finally fix your performance issues, if it does not become a habit in your developer teams, it will never be over. Performance is a habit, built through practice and good communication. But it can be achieved.\n\nAuthors: Etienne Doyon, Alexandre Gory, Maxime Blanc, Florent Dubost\n"
} ,
  
  {
    "title"    : "Bedrock à la GopherCon EU (2023)",
    "category" : "",
    "tags"     : " conference, berlin, tech, go",
    "url"      : "/2023/07/10/gophercon-eu-2023-a-berlin.html",
    "date"     : "July 10, 2023",
    "excerpt"  : "Depuis maintenant presque 1 an, la verticale Backend de Bedrock, s’ouvre à d’autres langages de programmation que PHP, à\nsavoir Golang et Rust.\nC’est pourquoi cette année, pour la 1ère fois, 6 de nos collègues ont participé (2 en présentiel, 4 à\nd...",
  "content"  : "Depuis maintenant presque 1 an, la verticale Backend de Bedrock, s’ouvre à d’autres langages de programmation que PHP, à\nsavoir Golang et Rust.\nC’est pourquoi cette année, pour la 1ère fois, 6 de nos collègues ont participé (2 en présentiel, 4 à\ndistance) à la GopherCon EU ayant lieu à Berlin.\n\n\n\nLa GopherCon EU, c’est un peu comme le Forum PHP, mais pour le Go et à un niveau international. À cette édition, environ\n600 participants étaient présents sur place ou à distance depuis les 4 coins du monde (Brésil, États-Unis, Afrique du\nNord, Europe, Asie, Australie…) et bien sûr d’autres français.es.\n\nElle se déroule sur 1 semaine entière :\n\n\n  Jour 1 : Visite de Berlin et table ronde\n  Jour 2 : Atelier\n  Jour 3 et 4 : Conférences\n  Jour 5 : Interview avec les UX de l’équipe Go\n\n\nL’évènement avait lieu dans un espace assez typique de l’Allemagne, un “Biergarten” que l’on pourrait traduire par\n“Brasserie en plein air”. On vous rassure tout de suite pas de bière pendant les conférences ;)\n\nLes conférences se déroulaient principalement dans la salle de concert du Biergarten, cependant l’après-midi, il y avait\nune 2ᵉ track qui se tenait dans un entrepôt de ventes aux enchères.\n\n\n\nLieux atypiques, pour nous, bonne ambiance, des gophers.euses très sympas, amicaux, respecteux.ses, tout pour assister à\ndes conférences fantastiques.\n\nNous ne pouvons vous présenter l’ensemble des conférences, mais en voici quelques-unes nous ayant marqués pour vous\ndonner envie d’en voir davantage :\n\nKeynote - State of the Go Nation\n\nLes deux jours de conférences ont démarré par une Keynote donnée par Cameron Balahan,\nle Product Lead pour le Go chez Google.\n\nCette keynote a été l’occasion de revenir sur l’historique de Go en tant que plateforme depuis sa création en 2007\njusqu’à aujourd’hui avec l’arrivée prochaine de la version 1.21 en août :\n\n\n  2007 : Création de go par Google (utilisé à 20% sur les projets Google)\n  2009 : Go devient open source\n  2012 : Sortie de Go 1.0, avec pour objectif de construire une plateforme stable et compatible dans le temps\n  2015 : Sortie de Go 1.5, qui apporte une augmentation des performances (via la mise en place du « low latency garbage collection »), un compilateur et runtime écrits en Go\n  2018 : Introduction des modules. Amélioration de la sécurité. Nouveautés : SBOM, fuzzing\n  2022 : Introduction des generics sans breaking changes\n\n\nUne fois cet historique présenté, Cameron a parlé de l’avenir du Go et de son écosystème dans les années à venir. Pour\nl’équipe de développement (dont plusieurs membres étaient présents), le Go n’est pas juste un langage de programmation\nmais tout un écosystème : des outils pour les IDE (le plugin Go pour VS Code est maintenu par la team), la gestion des\ndépendances, les systèmes de test, le formatting, le profiling, la CLI, la rétro-compatibilité, la documentation web et\nbien sûr le langage lui-même.\n\nLa rétro-compatibilité est un point sur lequel il a beaucoup insisté, en parlant notamment de l’ajout des generics\ndans la version 1.18, et ce, sans aucun breaking change. Cet ajout est pour la Core\nTeam la modification la plus complexe ayant eu lieu sur le langage, et la plus complexe qu’il n’y aura jamais. Tout ça\npour dire qu’ils ne voient pas de raison à ce qu’il existe un jour un Go 2.0 et que le langage restera donc toujours\nrétro-compatible dans ses futures versions.\n\nUseful Functional-Options Tricks For Better Libraries\n\n\n\nJulien Cretel nous a présenté le pattern functional options à travers l’exemple d’une\npetite librairie de gestion de CORS.\n\nPour faire cette première librairie, une première approche “classique” serait d’utiliser une fonction de création de\ncors avec des paramètres pour chacune des options.\n\nIl note plusieurs inconvénients :\n\n  Pas très ergonomique\n  Pas assez expressif\n  Et pas extensible\n\n\nfcors.NewCORS([]string{&quot;https://example.com&quot;}, 0, nil)\n\n\nUne première alternative est l’utilisation d’une struct Config qui contiendrait toutes les options possibles et de passer\ncette struct en paramètre :\n\ntype Config struct {\n  Origins []string\n  MaxAgeInSeconds uint\n  RequestHeaders []string\n}\n\nfunc NewCORS(cfg Config) *Middleware\n\n\nC’est plus extensible, mais certains inconvénients restent :\n\n\n  pas de possibilité de ne pas passer un paramètre\n  peu expressif\n\n\nIl propose alors une autre possibilité : le functional options :\n\nfunc NewCORS(opts ...Option) *Middleware\n\nfunc FromOrigins(origins ...string) Option\nfunc MaxAgeInSeconds(delta uint) Option\nfunc WithRequestHeaders(names ...string) Option\n\n\nL’idée est d’utiliser des constructeurs nommés pour instancier des options selon les besoins et de passer ses options en\nparamètre !\n\nLes avantages qu’il y voit sont :\n\n  le système est facilement extensible\n  c’est beaucoup plus expressif\n\n\nIl a ensuite détaillé certaines astuces pour aller plus loin avec ce pattern :\n\n  « When order doesn’t matter, users are happier »\n  « Multiple calls to the same option »\n  « Immutability »\n\n\nVous pouvez retrouver les slides de sa présentation ici\net cerise sur le gâteau, une librairie qui sert d’exemple existe : https://github.com/jub0bs/fcors\n\nTowards modern development of cloud applications with service weaver\n\n\n\nCette conférence, présentée par Robert Grandl, est intéressante pour une entreprise\nqui commence à utiliser Go. On peut se poser la question : Faut-il partir tout de suite sur des micro-services ou bien\ncommencer par un monolithe ?\n\nL’idée générale de Service Weaver est de permettre aux développeurs de se concentrer sur le\ndéveloppement de leur application sans se soucier de cette question d’architecture.\n\nGrâce à ce framework, une application peut être développée comme une sorte de monolithe via des modules Go qui\ncommuniquent via des interfaces. Le framework permet ensuite de déployer cette application de deux façons différentes :\n\n\n  un monolithe, où les modules communiquent via des appels directs dans un seul fichier binaire final\n  des micro-services, où les modules communiquent via des appels réseaux (gRPC) et sont déployés dans des containers\nséparés\n\n\nDans le cas d’un déploiement en Micro-services, le framework prend entièrement en charge la communication entre les\nmodules, sans aucun impact sur le code ou le développeur.\n\nGo Sync or Go Home: Advanced Concurrency Techniques for Better Go Programs\n\nYarden Laifenfeld, Software Engineer chez Rookout, nous a présentée les fonctionnalités moins connues des packages\nsync et x/sync.\n\nVoici les différents sous packages présentés lors de cette conférence :\n\nsync - Wait Group\n\nWaitGroup va permettre d’attendre que toutes les tâches soient terminées.\nSon utilisation simplifie l’implémentation, la lecture du code, mais aussi la performance comparée au couple\ngoroutines/channels.\n\nx/sync - Error Group\n\nErrorGroup permet de prendre en charge la propagation des erreurs, si une tâche renvoie une erreur, la tâche\nprincipale peut agir en fonction.\n\nx/sync - Single Flight (Do)\n\nDo de singleflight permet de ne pas exécuter deux fois la même tâche avec la même valeur d’entrée. Si une entrée\nsimilaire arrive avant la fin de la précédente, cette entrée va attendre le résultat de la précédente et retourner le\nmême résultat faisant ainsi gagner en performance.\n\nYarden nous a indiqué que Kubernetes utilise ce mécanisme dans le cached token authenticator depuis maintenant\nquelque temps.\n\nNous allons regarder dans les prochaines semaines si nous avons la possibilité d’utiliser ces patterns dans notre\ncodebase actuelle et l’avoir à l’esprit pour nos futurs développements.\n\nPour conclure\n\nBedrock participe depuis longtemps directement ou indirectement à des événements liés aux différentes technologies\nutilisées en interne.\n\nPar exemple en 2023 : Android Makers,\nl’AFUP Day,\nle MiXiT,\nl’AWS Summit,\nKubernetes Community Days\net Vue Amsterdam.\n\nC’est donc dans la même logique que Bedrock a choisi de faire participer certains membres du backend à la GopherCon.\n\nParmi les 6 collègues présents à la conférence, certains viennent de commencer à utiliser le langage de programmation Go\ntandis que d’autres commencent à avoir une certaine expérience. Ce fût donc l’occasion d’échanger, de partager et de\ndécouvrir, aussi bien entre nous qu’avec les autres participants, différents sujets au sein de l’écosystème Go, qui\nprend une part croissante dans notre plateforme.\n\n\n"
} ,
  
  {
    "title"    : "You Need a Custom Gradle Plugin, and Here’s Why",
    "category" : "",
    "tags"     : " android, gradle, plugin",
    "url"      : "/2023/07/07/gradle-convention-plugins.html",
    "date"     : "July 7, 2023",
    "excerpt"  : "In the last couple of years, Gradle has been encouraging developers to work towards modularizing their projects. Of course, when effectively implemented, this approach offers several advantages, with build parallelization being a significant facto...",
  "content"  : "In the last couple of years, Gradle has been encouraging developers to work towards modularizing their projects. Of course, when effectively implemented, this approach offers several advantages, with build parallelization being a significant factor.\n\nBut splitting your Android project into many modules has a major drawback, at first: you need to write a build file for each of them.\n\nThe naive approach\n\nOne might be tempted to create “common” Groovy files (also known as “script plugins”) and import them into each module. We can also define some properties in the root project, which can then be used in each subproject.\n\napply plugin: &#39;com.android.library&#39;\napply plugin: &#39;kotlin-android&#39;\napply plugin: &#39;kotlin-kapt&#39;\n\n// This imports a Gradle file which we can use everywhere\napply from: rootDir.path + &#39;/lib-common.gradle&#39;\n\nandroid {\n    // compileSdkVersion is defined in the root project\n    compileSdkVersion rootProject.ext.compileSdkVersion\n\n    defaultConfig {\n        minSdkVersion rootProject.ext.minSdkVersion\n        targetSdkVersion rootProject.ext.targetSdkVersion\n\n        consumerProguardFiles &#39;proguard-rules.pro&#39;\n        testInstrumentationRunner &quot;androidx.test.runner.AndroidJUnitRunner&quot;\n    }\n\n    compileOptions {\n        sourceCompatibility rootProject.ext.sourceCompatibility\n        targetCompatibility rootProject.ext.targetCompatibility\n    }\n\n    kotlinOptions {\n        jvmTarget = rootProject.ext.kotlinJvmTarget\n        freeCompilerArgs += rootProject.ext.kotlinCompilerArgs\n    }\n}\n\ndependencies {\n    // Dependencies are defined in a map in the root project\n    def dep = rootProject.ext.dependencies\n    implementation dep.&#39;androidx.core:core-ktx&#39;\n    implementation dep.&#39;androidx.paging:paging-runtime-ktx&#39;\n    implementation dep.&#39;com.squareup.okhttp3:okhttp&#39;\n\n    rootProject.applyTestDependenciesOn(dependencies)\n    rootProject.applyToothpickDependenciesOn(dependencies)\n}\n\n\nThis is the approach we were using before moving to a better system. These few following drawbacks made it obsolete and not recommended by Gradle maintainers.\n\n\n  Script plugins need to be imported individually for each module in your project. This means that your heap will grow a lot, and this approach will scale terribly on a project with many modules.\n  Relying on the rootProject in your modules−or relying on subprojects from your root project, for that matter−will add unwanted dependencies between your modules, which will in turn defeat optimization mechanisms designed by Gradle, such as configuration-on-demand or configuration cache. These are made to help bring down the time Gradle spends configuring your project (i.e. reading the configuration and building the task graph) each time you build; it goes without saying that getting this time to decrease will make for happier and more productive developers.\n\n\nIn addition to these issues, we wanted to start modularizing much of our project. We already had about 150 modules, but we planned on making many more soon, so this would be a good time to find a future-proof architecture. Plus, this was a good opportunity to clear some tech debt: cleaning unused dependencies, moving to a version catalog…\n\nModern problems call for modern solutions\n\nCentralizing version management\n\nA significant challenge we faced, which is also common in the industry, is managing dependencies and versions across the entire project. Hard-coding the version of okhttp for every module is not recommended, as it can be tedious and error-prone.\n\nThere are several known solutions to this problem, such as storing versions in the root project or using a buildSrc script. But not only are some solutions bad for your build performance (see: reliance on the root project), almost all of them share an insoluble issue: tooling support.\n\nThere are multiple ways to be informed when your dependencies can be upgraded. You can rely on your IDE to highlight your outdated dependencies, which it does by trying to look for some string that… looks like a Gradle . You can also rely on a tool like Renovate, which does the same thing on your CI. In either case, you probably could use a standard solution, where there is some kind of standard to declare your centralized dependencies, which both humans and machines can rely on consistently.\n\nTo solve this problem, Gradle introduced the version catalog:\n\n[versions]\nandroidCompileSdk = &quot;33&quot;\nandroidGradlePlugin = &quot;7.4.2&quot;\njvm = &quot;17&quot;\n\n[libraries]\nandroid-billingclient-core = { module = &quot;com.android.billingclient:billing&quot;, version.ref = &quot;billing&quot; }\nandroid-billingclient-ktx = { module = &quot;com.android.billingclient:billing-ktx&quot;, version.ref = &quot;billing&quot; }\nandroid-gradle = { module = &quot;com.android.tools.build:gradle&quot;, version.ref = &quot;androidGradlePlugin&quot; }\nandroid-installreferrer = { module = &quot;com.android.installreferrer:installreferrer&quot;, version = &quot;2.2&quot; }\nandroid-tools-desugar-jdk-libs = { module = &quot;com.android.tools:desugar_jdk_libs&quot;, version = &quot;1.1.5&quot; }\nandroid-tools-lint-api = { module = &quot;com.android.tools.lint:lint-api&quot;, version.ref = &quot;lint&quot; }\n\n[plugins]\nandroid-app = { id = &quot;com.android.application&quot;, version.ref = &quot;androidGradlePlugin&quot; }\nandroid-library = { id = &quot;com.android.library&quot;, version.ref = &quot;androidGradlePlugin&quot; }\n\n\nThis format has been great, even for a project as big as ours. It’s flexible: you can now store library versions, but plugin versions as well, and even just plain versions, which you can get from your custom plugin later on!\n\nAnd it’s a standard format, so it works out of the box with tools like Renovate or Android Studio.\n\nCode reuse\n\nThe direction of Gradle best practices in our industry is evident, with numerous talks and blog posts from big tech companies and even Gradle itself emphasizing the use of convention plugins.\n\nWhile the name may sound intimidating, convention plugins are actually pretty straightforward. They are Gradle plugins that can be applied to each module, ensuring consistent configuration across all of them.\n\nConvention plugins offer the advantages of build scripts and the elimination of duplicate configuration, all without the need for a dependency on the root project. The convention plugin is an isolated project, which could be stored in your monorepo, but could very well be stored in a completely different place. Unlike build scripts, it’s compiled and instantiated only once, and is then ×*called** once for each module.\n\nCreating a convention plugin is similar to creating any custom Gradle plugin. If you haven’t had to do this yet, it looks like this:\n\n// settings.gradle\n// …\nincludeBuild &#39;gradle-plugins/convention-plugin&#39;\n\n\nThis will include your convention plugin alongside your main project at build time, so you will be able to use its result for your main project’s build system.\n\nYou’ll need a simple settings.gradle(.kts) file for your plugin. If your plugin is located in your monorepo, it will be very useful to be able to access its Version Catalog, so you can even share your dependency versions in the build files of your plugin.\n\ndependencyResolutionManagement {\n    versionCatalogs {\n        libs {\n            from(files(&quot;../../gradle/libs.versions.toml&quot;))\n        }\n    }\n\n    repositories {\n        google()\n        mavenCentral()\n        gradlePluginPortal()\n    }\n}\n\nrootProject.name = &#39;gradle-plugin-convention&#39;\n\n\nThen, you need a build.gradle(.kts) configuration script for your custom plugin. In order to configure other modules with the Android Gradle Plugin (AGP), for example, you will need access to the AGP’s classpath at build time in your plugin. You might be tempted to apply the AGP as a plugin, but you actually need to import it as an implementation.\n\ngroup = &quot;com.bedrockstreaming&quot;\nversion = &quot;1.0-SNAPSHOT&quot;\n\nplugins {\n    // This is a Gradle plugin written in Kotlin, import the Gradle Kotlin DSL\n    `kotlin-dsl`\n}\n\njava {\n    toolchain {\n        // This sets the JVM version needed to build this project.\n        // Notice that we set this version in the Version Catalog, and we can use it here!\n        languageVersion.set(JavaLanguageVersion.of(libs.versions.jvm.get()))\n    }\n}\n\ngradlePlugin {\n    plugins {\n        // Your custom plugin&#39;s module can actually contain many plugins.\n        // Create as many as you need - if you have multiple application modules, \n        // it might be useful to at least create one for library modules,\n        // and one for application modules.\n\n        create(&quot;androidMobileAppPlugin&quot;) {\n            id = &quot;com.bedrockstreaming.convention.application.mobile&quot;\n            implementationClass = &quot;com.bedrockstreaming.gradle.convention.android.application.AndroidMobileApplicationPlugin&quot;\n        }\n\n        create(&quot;androidLibraryPlugin&quot;) {\n            id = &quot;com.bedrockstreaming.convention.library.android&quot;\n            implementationClass = &quot;com.bedrockstreaming.gradle.convention.android.library.AndroidLibraryPlugin&quot;\n        }\n\n        create(&quot;jvmLibraryPlugin&quot;) {\n            id = &quot;com.bedrockstreaming.convention.library.jvm&quot;\n            implementationClass = &quot;com.bedrockstreaming.gradle.convention.jvm.JvmLibraryPlugin&quot;\n        }\n    }\n}\n\ndependencies {\n    // Note that we add the AGP and Kotlin plugin as implementations, which is unusual.\n    implementation(libs.android.gradle)\n    implementation(libs.kotlin.gradle)\n}\n\n\nThen, you’ll need an extension, which is Gradle speak to describe a configuration interface. Each option you will add to your extension will be usable from your module’s build.gradle(.kts). This is one of the most powerful advantages of custom plugins: you can reuse code and still make it configurable!\n\nabstract class BaseConventionPluginExtension {\n\n    internal abstract val enableCompose: Property&amp;lt;Boolean&amp;gt;\n\n    /**\n     * Enable Jetpack Compose on this module, and add core libraries.\n     */\n    fun composeToolkit() {\n        enableCompose.set(true)\n    }\n\n    // …\n}\n\n\nThen, it’s time to create the actual plugin class, the entry point for Gradle (specified in implementationClass above).\n\npackage com.bedrockstreaming.gradle.convention.android.library\n\nimport org.gradle.api.Plugin\nimport org.gradle.api.Project\nimport org.gradle.kotlin.dsl.create\n\nclass AndroidLibraryPlugin : Plugin&amp;lt;Project&amp;gt; {\n\n    override fun apply(target: Project) {\n        // This is where we declare that our extension will be available in a bedrock {} block.\n        val extension = target.extensions.create&amp;lt;AndroidLibraryExtension&amp;gt;(&quot;bedrock&quot;)\n        // …\n    }\n}\n\n\nThat’s it for boilerplate! You’re free to architect the internals of your Gradle plugin however you want, but this Plugin::apply method will be the entry point for your configuration code. It will be called for each module on which your plugin has been applied.\n\nFor example, here’s how you might apply the com.android.library plugin to your module, and configure it:\n\nfun apply(target: Project) = with(target) {\n    // getPluginId is an extension function that reads the plugin ID from the version catalog\n    apply(plugin = getPluginId(&quot;android.library&quot;))\n\n    configure&amp;lt;LibraryExtension&amp;gt; {\n        compileSdk = getVersion(&quot;androidCompileSdk&quot;).toInt()\n    }\n\n    androidComponents.finalizeDsl {\n        configure&amp;lt;LibraryExtension&amp;gt; {\n            defaultConfig {\n                minSdk = getVersion(&quot;androidMinSdk&quot;).toInt()\n                consumerProguardFiles(&quot;proguard-rules.pro&quot;)\n            }\n        }\n    }\n}\n\n\nYou can reuse this principle and apply it to all your common configuration blocks. You can automatically add dependencies, add some unit testing configuration, set the correct JDK toolchain, build flags, and even configure other third-party plugins with the same mechanism. The sky is the limit!\n\nEnd result\n\nRemember our old build file, with its included Groovy scripts, referenced root project, custom extension functions? Here’s what it looks like now!\n\nplugins {\n    alias(libs.plugins.bedrock.library.android)\n}\n\nbedrock {\n    moshi(codegen: true)\n    composeToolkit()\n    unitTests()\n}\n\ndependencies {\n    implementation(libs.androidx.core.ktx)\n    implementation(libs.androidx.paging.runtime.ktx)\n}\n\n\nMuch nicer, isn’t it? 🤩\n\nIn summary\n\nThe scalability of our project has been significantly improved through the migration from included build scripts and root project dependencies. Although writing custom Gradle plugins can initially pose challenges due to the potential for frustrating errors resulting from a minor misunderstanding of the Gradle API, once you are set up, the maintenance becomes much easier. It feels more rewarding to work in harmony with Gradle, rather than working against the optimizations introduced with each Gradle update, knowing that we can automatically benefit from them. The version catalogs provide a convenient method for organizing dependencies, and the fact that our tooling recognizes the format is a significant advantage.\n\nIn conclusion, for developers working on medium-to-large Gradle projects, whether in the Android realm or elsewhere, I highly recommend exploring the use of convention plugins. Mastering them is not as difficult as it may seem, and they provide effective solutions to address real challenges that we all face day-to-day.\n\nCover image © Isis Petroni\n"
} ,
  
  {
    "title"    : "Deux jours à Android Makers by Droidcon 2023",
    "category" : "",
    "tags"     : " android, mobile, conference, makers",
    "url"      : "/2023/06/19/android-makers-23.html",
    "date"     : "June 19, 2023",
    "excerpt"  : "Il y a quelques semaines déjà, nous avons pu nous rendre à LA conférence annuelle Android en France : Android Makers. Conférence qui s’associe tout juste avec une initiative un peu plus internationale qui est DroidCon (cf. notre précédent article,...",
  "content"  : "Il y a quelques semaines déjà, nous avons pu nous rendre à LA conférence annuelle Android en France : Android Makers. Conférence qui s’associe tout juste avec une initiative un peu plus internationale qui est DroidCon (cf. notre précédent article, par exemple).\nL’occasion d’assister à des conférences de speakers du monde entier mais également de networker et revoir avec plaisir beaucoup de têtes connues !\n\nVoici pêle-mêle nos retours et les apprentissages que nous avons pu collecter durant ces 2 jours intenses !\n\nPar @Antoine Pitel\n\nSi on devait retenir une chose de la conférence The Rise and Fall of Feature Teams de Danny Preussler, on pourrait dire en résumé que “Les développeurs ont besoin d’autres développeurs de la même technologie pour s’épanouir”. Le format en Feature Team peut facilement tomber dans le piège de l’isolement du développeur sur sa technologie. Il est fondamentale de s’assurer soit de multiplier les profils de même technos dans une team, soit d’organiser un partage de connaissance et du pair programming très récurrent.\n\n\n\n\n\nLa conférence Practical ADB usage to enhance your life! de Benjamin Kadel était, pour moi, définitivement la plus passionnante (au sens propre du terme passionné). L’usage d’ADB pour optimiser le travail quotidien en développement comme en test me parait ultra efficace. On a pu y découvrir notamment une astuce particulièrement utile : il est possible via ADB de nettoyer, refuser ou accepter les permissions demandées au framework par l’app. Un gros gain de temps quand on doit développer sur une feature qui nécessite une permission spécifique (et le statut de celle-ci) .\n\n\n\n\n\nJe ne me suis jamais vraiment penché sur Android Auto, shame on me 😉 Mais la conférence “Going on a road trip with Android Auto” de Carlos Mota nous a ouvert un tout nouvel univers de jeu que j’ai hâte d’explorer chez Bedrock ! En effet grâce aux dernières évolutions d’Android Auto il est désormais possible de publier, comme avant, des services audio (radio, podcast,..), mais désormais aussi des services vidéo ! L’accès vidéo n’étant disponible qu’à l’arrêt du véhicule, information que le framework Android Automotive fourni à travers la classe CarUxRestrictions du package android.car.drivingstate et sa méthode isRequiresDistractionOptimization().\n\n\n\n\n\nIl est tellement plaisant, mais rare, d’assister à des conférences qui parlent de CD et de publication ! En cela la conférence How to ship apps better, faster, stronger de Fabien Devos était passionnante et pleine d’apprentissage à diffuser au plus grand nombre ! J’en retiendrai 2 de mon côté :\n\n  La notion de “release Train” et cette métaphore du train qui part à heure fixe de manière régulière. Avec de plus l’info que la périodicité hebdomadaire semble être adaptée aux projets qui collaborent avec des stores comme App Store et Google Play Store ;\n  L’absolue nécessité de disposer d’un système de feature flipping robuste et couvrant le maximum du fonctionnel du service. Un vaste chantier !\n\n\n\n\nPar @Baptiste Candellier\n\nComme tous les ans, Android Makers nous propose des talks variés, des dernières nouveautés de Jetpack Compose au management d’équipe, en passant par l’habituelle keynote humoristique de Romain Guy et Chet Haase. J’ai sélectionné pour vous mes talks préférés.\n\nLe talk 90s Website … in 2023 on mobile in Compose … for science de Maia Grotepass a été pour moi le plus original, intriguant et intéressant de la conférence. Maia nous a guidé à travers son projet de cœur : reproduire, grâce à Jetpack Compose, le look-and-feel d’un site web des années 90. Un mélange de technologique moderne, qui est écrit pour tourner à la fois sur Android mais également sur desktop, grâce aux efforts de Jetbrains sur le multi-plateforme. Un projet qui pourrait sembler futile au premier abord, mais Maia nous plonge dans son parcours nostalgique tout en nous expliquant de manière pédagogique les APIs d’animation et de dessin de Compose, que nous avons assez peu souvent l’occasion d’utiliser dans des projets professionnels, qui se reposent souvent sur des composants pré-conçus. À voir, que vous soyez amateur·ice de web old-school ou de Canvas.\n\n\n\n\n\nForging the path from monolith to multi-module app, par Marco Gomiero, séduira les adeptes d’architecture. Ce talk vante non seulement les avantages d’un projet Gradle multi-modules, mais nous détaille tous les choix architecturaux qui en découlent, de façon subjective. Un retour d’expérience sur ce sujet, qui est une trend relativement récente dans l’écosystème Android, est très intéressant et permet non seulement de guider nos décisions d’avenir, mais aussi de regarder d’un nouvel œil nos propres décisions architecturales. Marco nous propose un apercu de sujets tels que les types de modules, la gestion de la navigation, les version catalogs, les convention plugins — autant de sujets qui sont à l’état de l’art des projets Gradle, et qui mérient bien un partage d’expérience !\n\nMarco apporte un bon équilibre en nous montrant le travail de son équipe sur l’app Tier, tout en nuançant sur le fait que l’architecture n’est pas une science exacte. On retiendra cette citation frappante :\n\n\n  Sometimes the “best decision™” is not the best one\n\n\n\n\n\n\nOn termine pour ma part avec un talk qui parle encore d’architecture, mais cette fois en ce qui concerne la migration vers Compose. Un Design System, ça se Compose !, avec Jean-Baptiste Vincey et Julie Gléonnec, nous présente la direction prise par les équipes de Deezer en ce qui concerne la migration de leur design system vers Jetpack Compose. C’est un sujet d’actualité et qui aura très certainement des ramifications sur les années à venir — Bedrock a entamé cette année, j’étais donc curieux de connaître les approches prises par d’autres équipes travaillant sur de grosses applications avec des design systems basés sur des vues XML.\n\nLes équipes de Deezer ont choisi de réécrire entièrement leur implémentation du design system en Compose, et de migrer leur app écran par écran. Un choix qui a des avantages - la facilité de migration des nouveaux écrans, l’absence de legacy dans les nouveaux composants, mais aussi des inconvénients comme la nécessité de garder à jour deux versions des composants jusqu’à la migration complète de l’app. Deezer nous expose dans cette présentation passionnante leurs choix et leur chemin vers Compose.\n\n\n"
} ,
  
  {
    "title"    : "Bedrock à l&#39;AFUP Day Lyon (2023)",
    "category" : "",
    "tags"     : " conference, lyon, tech, php, afup",
    "url"      : "/2023/06/06/afup-day-lyon-2023.html",
    "date"     : "June 6, 2023",
    "excerpt"  : "Comme à l’accoutumée, les équipes backend de Bedrock étaient présentes le 12 mai 2023 à CPE (au Campus Lyontech) pour une nouvelle édition de l’AFUP Day.\n\nPour suivre les conférences qui gravitaient autour de PHP bien sûr, mais aussi pour soutenir...",
  "content"  : "Comme à l’accoutumée, les équipes backend de Bedrock étaient présentes le 12 mai 2023 à CPE (au Campus Lyontech) pour une nouvelle édition de l’AFUP Day.\n\nPour suivre les conférences qui gravitaient autour de PHP bien sûr, mais aussi pour soutenir notre collègue Pauline Rambaud qui était la star du jour (pour Bedrock du moins) 🤩 !\n\nEt nous n’oublions pas non plus, les âmes braves de Bedrock qui ont revêti le maillot bleu de l’AFUP et qui ont donné de leur personne pour nous accueillir dans d’excellentes conditions.\n\nEnfin bref (🐘), voici le petit récap habituel des différentes conférences de cette journée 👇\n\nTirer parti du composant ExpressionLanguage de Symfony, laissez les utilisateurs finaux être créatifs !\n\n\n  Conférence présentée par Florian MERLE et Mathias ARLAUD\n\n\nDans leur talk, Florian et Mathias nous parlent du composant Symfony Expression Language. \nCe composant fournit au développeur un moteur d’expressions et permet l’utilisation de ces dernières dans la configuration d’un projet ou bien encore comme fondation d’un moteur de règles métiers. \nSi ce composant vous est inconnu, sachez qu’il est lui-même utilisé par Symfony : par exemple pour la sécurité sur les règles de validation des routes.\n\nFlorian et Mathias nous familiarisent avec le fonctionnement du composant : on peut définir des expressions simples qui ne requièrent pas d’être compilées en PHP (ex : “1+1”) et à l’inverse des expressions compilées en PHP qui vont correspondre à des règles personnalisées définies au sein de notre projet (“est_eligible_a_une_promo(client)”). \nPour mieux comprendre comment tout cela fonctionne en interne, nous avons ensuite une présentation de comment les expressions sont interprétées grâce à de l’analyse lexicale et le principe d’arbre syntaxique.\n\nPour finir ce talk, nous avons le droit à une démonstration du composant comme moteur de règles métiers. \nOn nous présente un petit site e-commerce où les administrateurs peuvent, via un formulaire, utiliser les règles métiers définies dans le projet et les ajuster pour correspondre au mieux à leurs besoins.\n\nMentors : super-héros ou super-vilains ?\n\n\n  Conférence présentée par Pauline RAMBAUD\n\n\nOn a choisi de ne rien vous dévoiler sur cette conférence donnée par notre collègue Pauline, il fallait être présent pour savourer l’instant 😉… \nCeci dit, Pauline sera sûrement ravie d’en parler avec vous sur Twitter.\n\nMonades : paradigme unique pour la programmation\n\n\n  Conférence présentée par Baptiste LANGLADE\n\n\nDans cette conférence, Baptiste nous a fait un rapide tour d’horizon des solutions existantes pour faire de l’asynchrone en PHP. \nLe conférencier nous a expliqué qu’il était difficile aujourd’hui de faire du synchrone et de l’asynchrone de la même manière, facilement.\n\nC’est pour cela que le speaker a développé le composant Sequence. \nGrâce à ce projet et sa démo, nous avons pu voir comment, sans changer le code (hormis une dépendance), nous pouvons faire des appels synchrones ou asynchrones.\n\nNous avons, au passage, eu la présentation du composant Filesystem qui apporte des outils intéressants pour la manipulation de fichiers.\n\nLe Zéro Downtime Deployment en pratique\n\n\n  Conférence présentée par Smaïne MILIANNI\n\n\nSmaïne nous a présenté la logique à avoir lorsque l’on veut faire du ZDD avec deux exemples et les limites de cette pratique.\n\nLes deux règles d’or à respecter concernant les changements sont :\n\n  être rétrocompatibles, c’est-à-dire que les nouvelles modifications doivent fonctionner avec l’existant. \nSi le déploiement fail cela ne doit pas empêcher la version actuelle de fonctionner.\n  être livré par release\n\n\nCela implique de repenser la façon de construire une nouveauté et pour illustrer cela, Smaïne a donné l’exemple de l’ajout d’une colonne not null dans une base de données.\n\nObjectif : ajouter une colonne not null dans une table\n\nDécoupage en deux releases :\n\n  Release 1\n    \n      ajout d’une colonne A null dans la table\n      mise à jour de l’entité concernée dans le code initialisé à null\n    \n  \n  Release 2\n    \n      mettre à jour les lignes sans valeur dans la colonne A avec une valeur par défaut\n      ajouter la contrainte NOT NULL sur la colonne A\n      mettre à jour l’entité en spécifiant l’attribut comme non nullable\n    \n  \n\n\nAfin de s’assurer que chaque release peut fonctionner avec l’existant et anticiper les impacts que pourraient avoir les modifications, il ne faut pas hésiter à tester de façon automatique ou manuelle.\n\nSuite à l’exemple, Smaïne a présenté les limites du ZDD qui sont :\n\n  cette pratique a du sens si vous faites des releases fréquemment\n  un changement majeur nécessitera toujours une maintenance\n  tant que l’ensemble des releases prévues pour faire une modification ne sont pas en production, le système est considéré comme instable\n  cette pratique nécessite de penser et de concevoir différemment les releases.\n\n\nPour conclure, le speaker a donné quelques clés pour se lancer dans le ZDD :\n\n  former ses équipes\n  documenter les processus mis en place\n  itérer et s’améliorer avec chaque expérience\n  tester et encore tester\n\n\nDémystifions les pratiques du Software craftsmanship !\n\n\n  Conférence présentée par Thomas BOILEAU\n\n\nAprès une rapide présentation des différentes pratiques autour du craftsmanship (Test Driven Development, Domain Driven Development, Clean Architecture, Architecture Hexagonale …), Thomas nous raconte comment il est tombé dans le piège du Gatekeeping.\n\nQu’est-ce que le Gatekeeping ? D’après lui (cf. photo), c’est l’art de marquer son appartenance à un groupe en excluant les autres.\n\n\n\nTout commence avec une remarque lue sur Internet, du style “Si tu ne fais pas de TDD, alors tu n’es pas un vrai développeur”, qui le complexera au point de le pousser à étudier et appliquer cette pratique jusqu’à son tour devenir l’auteur de ce genre de remarque.\n\nEn considérant le craftsmanship comme la solution universelle et en l’appliquant de manière dogmatique, il se retrouve à proposer des solutions inadaptées à ses projets et donc à nuire à ses clients.\n\nL’objet de cette conférence sera donc de nous parler de comment il a su se remettre en question et sortir de l’impasse.\n\nComment faire pour ne plus être un gatekeeper ? \nThomas nous parle alors de prendre du recul sur soi, d’être pragmatique et bien sûr de savoir faire preuve d’humilité. \nMais il existe aussi des pratiques reconnues dans notre milieu telles que l’Egoless programming, le Pair Programming, ou tout simplement reconnaître à chacun le droit à l’erreur et connaître ses propres limites.\n\nComment refondre un legacy sans cris et sans larmes - Retour d’expérience et bonnes pratiques\n\n\n  Conférence présentée par Kevin BALICOT\n\n\nÀ travers son retour d’expérience d’une refonte d’un très vieux projet PHP, Kevin BALICOT nous a offert sa recette d’une refonte progressive sans cris ni larmes :\n\n\n  Lister tous les problèmes de l’application\n  Définir une stratégie et des objectifs\n  Faire un inventaire de l’application\n  Mettre en place un Golden Master\n  Mettre des outils d’analyse de code\n  Implémenter des Design Pattern et des architectures\n  Consolider les choix avec des ADR et du Pair Programming\n  Tester !\n\n\nSi vous souhaitez approfondir un de ces points, vous pouvez sans doute lui demander directement sur Twitter.\n\nLe travail invisible en entreprise : le cas du glue work\n\n\n  Conférence présentée par Camille CASTILLO\n\n\nNous avons découvert le concept du “glue work” lors de la première conférence de Camille.\n\nEnfin, nous avons un terme pour décrire cette idée que nous avions tous.tes en tête, mais qui manquait d’une définition concrète.\n\nLe “glue work” représente toutes ces tâches accomplies par les employé.e.s, notamment les développeur.euse.s, lors de leur travail quotidien, qui ne sont généralement pas facilement quantifiables et rarement valorisées par l’entreprise.\n\nCamille a identifié trois catégories de “glue work” : social, managérial et technique.\n\nPar exemple, organiser une sortie au restaurant favorise les liens sociaux. Planifier une réunion avec des clients renforce les relations professionnelles. Et effectuer une veille et proposer de nouveaux outils de développement relève de la dimension technique.\n\nMalheureusement, en effectuant ces tâches essentielles à la vie de l’entreprise et même à sa productivité, les employé.e.s consacrent logiquement moins de temps à leurs tâches principales, comme le développement.\n\nCela peut devenir problématique si l’entreprise ne reconnaît pas la valeur de ces activités.\n\nAlors quelles solutions pour prendre en considération le glue work ?\n\nTout d’abord, le repérer et se porter volontaire.\n\nUn manager peut aussi veiller à répartir ces tâches.\n\nCamille conclut en indiquant que le glue work est nécessaire à l’entreprise, qu’il faut l’identifier et être acteur.ice.s chacun à son niveau pour le faire reconnaître.\n\nTransformer efficacement du JSON en structure PHP fortement typée\n\n\n  Conférence présentée par Romain CANON\n\n\nUne chouette conférence qui présentait la librairie d’Object Mapping pour PHP Valinor, permettant de tirer parti au maximum des types PHP au runtime.\n\nD’ailleurs suite à ça, certaines de nos équipes ont commencé à l’utiliser à Bedrock… Peut-être un prochain article de REX à prévoir 😉\n\nLes instruments des devs augmenté·e·s\n\n\n  Conférence présentée par Gabriel PILLET\n\n\nOn a fini cette journée en beauté, par une vue d’ensemble des différents outils permettant dès aujourd’hui d’épauler les développeurs dans leur travail quotidien.\n\nDe PHPStan à GPT-4 en passant par GitHub Copilot, cette conférence, dont les slides étaient habillées d’images générées par une IA, nous a bien fait comprendre qu’on a tout intérêt à accueillir ces nouveaux outils, si on souhaite décupler notre productivité 🤞.\n\nÀ l’année prochaine !\n\n\n"
} ,
  
  {
    "title"    : "Et si vos prochaines vacances se passaient à vélo ? #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/vacances-a-velo",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  La randonnée à vélo, c’est économique, écologique, relaxant, dépaysant et c’est plus facile d’accès que ce que vous pourriez pensez. Alors on y va ? Un talk pour bien démarrer, faire découvrir, chasser les idées reçues, partager des astuces et ...",
  "content"  : "\n  La randonnée à vélo, c’est économique, écologique, relaxant, dépaysant et c’est plus facile d’accès que ce que vous pourriez pensez. Alors on y va ? Un talk pour bien démarrer, faire découvrir, chasser les idées reçues, partager des astuces et donner l’envie de pédaler !\n\n\nPar Thomas Jarrand\n"
} ,
  
  {
    "title"    : "Situations conflictuelles : et si vous sortiez de la spirale infernale ? #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/situations-conflictuelles",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  Persécuteur, victime, sauveur… lequel êtes-vous ?\n\n  La situation suivante vous parle ?\n\n  “J’en ai marre ! Quand je demande de la revue sur mon travail, les gens de mon équipe ne me donnent jamais de retours et je dois toujours les relancer au...",
  "content"  : "\n  Persécuteur, victime, sauveur… lequel êtes-vous ?\n\n  La situation suivante vous parle ?\n\n  “J’en ai marre ! Quand je demande de la revue sur mon travail, les gens de mon équipe ne me donnent jamais de retours et je dois toujours les relancer au moins 10 fois avant d’obtenir une réponse… Heureusement que Michel, mon ancien collègue super sympa qui est maintenant dans l’équipe Warrior continue à me répondre, lui !”\n\n  Oui ? C’est normal. Inconsciemment, nous jouons les rôles de persécuteur, victime ou sauveur. Et nos interactions en souffrent : rien de constructif ne peut émerger de cette spirale infernale.\n\n  Apprenons à identifier ces rôles pour comprendre leurs effets sur nos relations avec nos collègues, puis nous explorerons comment en sortir pour des interactions plus constructives et positives. Mettons fin à cette spirale infernale !\n\n\nPar Elodie Perrin\n"
} ,
  
  {
    "title"    : "Du CSS aux shaders WebGL : panorama des techniques d&#39;animation en 2023 #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/panorama-css-animations",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  Parfois subtiles, parfois proéminentes, les animations sont un élément essentiel pour une expérience utilisateur agréable. Mais une fois qu’on a imaginé des animations plus ou moins folles, vient la question fatidique du “comment fait-on ça ?”,...",
  "content"  : "\n  Parfois subtiles, parfois proéminentes, les animations sont un élément essentiel pour une expérience utilisateur agréable. Mais une fois qu’on a imaginé des animations plus ou moins folles, vient la question fatidique du “comment fait-on ça ?”, à laquelle j’aimerais vous aider à répondre.\n\n  Pour pouvoir choisir la technique la mieux adaptée à chaque animation, il vaut mieux avoir une boîte à outils la plus complète possible. J’aimerais vous aider à construire la vôtre en vous présentant un panel le plus large possible de techniques d’animation sur le web : des APIs natives classiques (CSS, Web Animation API) aux plus complexes (Canvas API) en passant par les librairies spécialisées (FLIP, Lottie, Framer Motion, Rive…). Nous finirons avec WebGL et ses shaders GLSL, qui feraient trembler plus d’un développeur mais dont on retrouve les effets impressionnants sur tous les sites récompensés aux Awwwards.\n\n  Je passerai rapidement sur toutes ces techniques, en comparant leurs performances et utilisations possibles, afin de vous laisser la liberté d’explorer plus en profondeur celles qui vous intéressent.\n\n\nPrésenté par Julien Sulpis.\n"
} ,
  
  {
    "title"    : "OZINT - Vos traces vous trahissent ! #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/ozint-lft",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  L’Open Source Intelligence (OSINT), ou le renseignement en sources ouvertes (ROSO) en français, est un ensemble de méthodologies et de pratiques destinés à la collecte et l’analyse d’informations publiques et légales en ayant pour objectif de r...",
  "content"  : "\n  L’Open Source Intelligence (OSINT), ou le renseignement en sources ouvertes (ROSO) en français, est un ensemble de méthodologies et de pratiques destinés à la collecte et l’analyse d’informations publiques et légales en ayant pour objectif de répondre à des questions ou de faire des choix. L’OSINT est utilisé dans le monde du public comme dans le monde du privé, et dans plusieurs domaines, incluant : l’intelligence économique, le journalisme, les services de renseignement, la recherche scientifique, la cybersécurité, ou encore la lutte contre la criminalité.\n\n  L’objectif de cette conférence serait de mettre en lumière cette discipline mal connue, mais également d’utiliser les méthodes OSINT pour effectuer une démonstration de sensibilisation sur le sujet des données personnelles.\n\n\nPar Alexis Martins\n"
} ,
  
  {
    "title"    : "Créer son association #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/creer-son-association",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  Ça fait maintenant plus de deux ans que j’ai créé mon association sportive. J’aimerais à travers ce talk, faire un retour d’expérience sur mon aventure associative.\n\n  Le format serait axé autour d’un fil rouge concernant ma propre expérience a...",
  "content"  : "\n  Ça fait maintenant plus de deux ans que j’ai créé mon association sportive. J’aimerais à travers ce talk, faire un retour d’expérience sur mon aventure associative.\n\n  Le format serait axé autour d’un fil rouge concernant ma propre expérience avec mon association “Fit for All” Tout en généralisant des tips et des “choses à savoir/connaitre” pour monter un association.\n\n  Je fais ce talk sur toutes les choses que j’aurais aimé savoir avant de me lancer “dans le grand bassin”\n\n\nPrésenté par Guillaume Trémé.\n"
} ,
  
  {
    "title"    : "Célébrons nos réussites grâce au Brag Document ! #LFT 02/06/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/brag-document",
    "date"     : "June 2, 2023",
    "excerpt"  : "\n  « Quelles sont tes réussites du dernier sprint ? Qu’as-tu aimé faire? » Si, comme pour moi, vos réussites s’évaporent de votre mémoire comme un mojito au soleil, alors le Brag Document est fait pour vous.\n\n  Grâce au Brag Document, listez vos r...",
  "content"  : "\n  « Quelles sont tes réussites du dernier sprint ? Qu’as-tu aimé faire? » Si, comme pour moi, vos réussites s’évaporent de votre mémoire comme un mojito au soleil, alors le Brag Document est fait pour vous.\n\n  Grâce au Brag Document, listez vos réussites, ce que vous avez appris, apprécié faire, bref, tout ce qui vous a semblé important. Il peut être partagé avec votre manager pour faire le point sur votre progression, ou tout simplement pour répondre à ces questions : « De quoi êtes-vous fier(e) ? Qu’est ce que vous aimeriez faire plus ? »\n\n  Regardons un exemple de template pour initier son propre Brag Document. Puis je vous proposerai un exemple de mise en place du Brag Document au sein d’une équipe (Picsou), auprès de chaque collègue mais aussi pour la team en elle-même. N’oublions plus nos réussites !\n\n\nPar Anne-Laure de Boissieu.\n"
} ,
  
  {
    "title"    : "Bedrock à MiXiT Lyon (2023)",
    "category" : "",
    "tags"     : " conference, lyon, tech, agilité",
    "url"      : "/2023/04/25/mixit-lyon-2023.html",
    "date"     : "April 25, 2023",
    "excerpt"  : "Nous étions présent les 13 et 14 avril 2023 à CPE pour l’édition de MiXiT 2023, pour suivre les \nconférences bien sûr et pour soutenir les collègues qui donnaient une conférence !\n\nEt si vos prochaines vacances se passaient à vélo ?\n\nConférence pr...",
  "content"  : "Nous étions présent les 13 et 14 avril 2023 à CPE pour l’édition de MiXiT 2023, pour suivre les \nconférences bien sûr et pour soutenir les collègues qui donnaient une conférence !\n\nEt si vos prochaines vacances se passaient à vélo ?\n\nConférence présentée par Thomas Jarrand\n\n\nJay-Z, Maths and Signals ! How to clone Shazam 🎧\n\nConférence présentée par Moustapha Agack\n\n\nBienvenue dans le monde merveilleux des systèmes distribués !\n\nConférence présentée par Pascal Martin\n\n\nRemotion : le 7ème art à portée de composants web et d’API 🎬\n\nConférence présentée par Antoine Caron et Mickael Alves\n\n\n\nConversations avec ChatGPT: illusion ou réalité?\n\nConférence présentée par Marie-Alice Blete\n\nMarie-Alice a pris le temps de réexpliquer ce qu’est ChatGPT et pourquoi ce générateur de texte \nne garantira jamais la véracité des informations fournies. En effet, si les réponses de ChatGPT \npeuvent être très crédibles, le contenu proposé n’est en aucun cas vérifié \npuisqu’il s’agit d’une suite de mots les plus probables. \nEn guise d’illustration, Marie-Alice a demandé à ChatGPT de se rendre sur son terminal Linux, et \nde taper des commandes afin de cloner le projet chatGPT 4. Celui-ci s’exécute. Mais la \nretranscription n’est pas la réalité, c’est ce qu’il y aurait probablement eu dans le terminal. Un \nexemple bluffant !\nPour Marie-Alice, tout le monde peut être trompé. De nouveau à titre d’exemple, elle cite le cas \nd’un expert en IT qui a demandé à ChatGPT de résumer un article d’une revue scientifique. \nChatGPT a proposé un résumé imaginé à partir du titre - il n’a pas accès à l’article. Ce résumé \nétait si convaincant que l’expert a cru que ChatGPT avait accès à l’article.\n\nCette conférence a remis en place les attentes qu’on pouvait avoir à propos de ChatGPT, qui n’est qu’un outil avec ses limites.\n\nVoir les slides\n\nFresque anti-sexisme\n\nAtelier présenté par Sara Dufour\n\nCet atelier, à l’image de la fresque du climat, nous a permis de construire, en plusieurs étapes,\net par groupe de 5-6 personnes, un tableau dépeignant la vie actuelle.\nLes débats étaient constructifs et les faits, appuyés par les statistiques de l’INSEE, étaient effrayants.\nL’atelier était très intéressant, mais un peu démoralisant de voir qu’en 2023,\nil y a encore beaucoup à faire pour diminuer le sexisme aussi bien dans le milieu professionnel que privé.\n\nLego Flow Game : le Waterfall, le Scrum et le Kanban tu différencieras !\n\nAtelier présenté par Fanny Klauk\n\nCet atelier nous a permis de (re)voir les différentes organisations suivantes : le Waterfall, le Scrum, le Kanban.\nNous avons formé des équipes de 5 personnes et utilisé des Lego :heart_eyes: pour illustrer les taches de constructions.\nChaque personne jouait un rôle particulier qui représente les différents rôles d’une équipe de \ndéveloppement standard.\n\nL’objectif était de construire le plus de modèles d’un calendrier de l’avent Lego, donc des \nmodèles petits et (en général) assez simples, nous avions 6 minutes par type d’organisation. Les \nrôles étaient les suivants :\n\n  Analyste : Prend une porte (du calendrier de l’avent), note le numéro de la porte (jour où le calendrier sera ouvert) sur une carte et accroche avec un trombone la carte sur le trombone. Il gère la priorité en fonction du numéro de la carte.\n  Fournisseur : Prend la carte et va chercher le sachet correspondant. Il n’y a aucune indication sur le sachet qui permet de savoir lequel prendre\n  Réalisateur : Construit le modèle\n  Testeur : Vérifie que tout est ok\n  Pilote : Remplit un tableau de suivi des différents postes\n\n\nPremière étape, le Waterfall : chaque rôle devait finir son travail pour 5 cartes avant de passer au suivant.\nBilan, au bout des 6 minutes, l’analyste a fini son travail, le fournisseur est toujours en train de chercher des sachets, et les autres attendent et sont frustrés, car ils ne peuvent rien faire d’autre.\n\nDeuxième étape, le Scrum : chaque rôle peut faire passer une carte au prochain dès que son \ntravail sur celle-ci est finie.\n3 itérations de 2 minutes seront faites pour illustrer les sprints et une estimation est faite avant chaque itération.\nBilan : entre chaque itération, nous avions une petite rétrospective, celle-ci permettait de nous améliorer au tour d’après.\nAu final, nous avons pu terminer 4 cartes sur 5.\n\nDernière étape, le Kanban : comme pour le Scrum, une carte peut passer à l’étape d’après, dès \nqu’elle est terminée.\nPas d’estimation par contre, pas le droit d’avoir plus de 2 cartes à la même étape (pas de stock).\nBilan : là aussi, nous avons fait des petites rétrospectives pour nous améliorer entre chaque itération.\nAu final, nous avons pu terminer 6 cartes (un bug est survenu en cours de route). Le Kanban a \npermis une avancée plus importante, mais le suivi par le pilote était plus compliqué, car les cartes bougeaient très vite.\n\nPersonnellement, j’ai apprécié travailler en Kanban et suis content de travailler avec la méthode ScrumBan chez Bedrock, cette dernière mélangeant le Scrum et le Kanban.\n\nA l’année prochaine !\n\n\n\n"
} ,
  
  {
    "title"    : "Bedrock à l&#39;AWS Summit Paris 2023",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, aws, summit, paris, 2023",
    "url"      : "/2023/04/20/aws-summit-paris-2023.html",
    "date"     : "April 20, 2023",
    "excerpt"  : "L’AWS Summit Paris 2023 s’est déroulé le 4 avril. C’était pour nous l’occasion de découvrir les dernières innovations au cœur des services AWS, comme la solution d’IA d’aide au développement nommée CodeWhisperer. De plus, Pascal Martin, Principal ...",
  "content"  : "L’AWS Summit Paris 2023 s’est déroulé le 4 avril. C’était pour nous l’occasion de découvrir les dernières innovations au cœur des services AWS, comme la solution d’IA d’aide au développement nommée CodeWhisperer. De plus, Pascal Martin, Principal Engineer, y assistait aussi en tant que speaker pour partager notre expérience en conception et maintenance de Systèmes Distribués.\nEn plus des deux points précédemment cités, nous verrons aussi comment eTF1 s’est préparé pour la Coupe du Monde de la FIFA 2022, ou de souveraineté et de son application chez AWS.\n\nÀ vos côtés pour les grands moments : AWS, TF1 et la Coupe du Monde de la FIFA 2022\n\nConférence présentée par :\n\n  Imane Zeroual - Senior Technical Account Manager, AWS\n  Djamel Arichi, Head of Managed Services and Support, eTF1\n  Ali Oubabiz, Head of Digital Infrastructure, eTF1\n  Remy Pinsonneau, Architecte, eTF1\n\n\n\n\neTF1 partage son retour d’expérience sur la Coupe du Monde de foot 2022 et les défis surmontés pour que leur plateforme de replay myTF1 propose une parfaite expérience utilisateur tout au long de l’événement.\n\nLa présentation, coanimée par Imane, Senior Technical Manager de chez AWS, permet aussi d’en apprendre un peu plus sur le programme IEM d’accompagnement de clients AWS lors d’événements critiques. Nous avons d’ailleurs déjà exploité ce programme chez Bedrock Streaming.\nChallenge technique pour les équipes eTF1, la Coupe du Monde de football 2022 a battu plusieurs records de la plateforme, dont des pics à plus de 2,4 Millions d’utilisateurs simultanés. L’événement a été préparé en collaboration avec les équipes d’AWS pour adapter les infrastructures à recevoir de fortes charges.\n\nTrois points critiques identifiés :\n\n  Authent/backend, les millions d’utilisateurs vont s’authentifier dans une fenêtre de 15 minutes.\n  Delivery vidéo, tout au long de l’événement une forte charge, constante, est attendue.\n  Publicité, pic de charge très important mais ponctuel.\n\n\nDes scénarios de tests de performances ont été effectués à l’aide de K6, pour chacun des points. La préproduction a servi d’environnement de test, avant d’effectuer une validation finale sur la production. Du travail a été également réalisé sur les services AWS : par exemple, les tables DynamoDB ont été basculées en OnDemand afin de profiter de l’élasticité plus rapide du service, malgré les coûts supplémentaires, comparé au mode provisionné.\nAu niveau des clusters Kubernetes, les applications ont été redimensionnées à la hausse (mémoire, cpu, HPA) pour anticiper les pics de charge et ne pas seulement se reposer sur du scaling réactif.\n\nLors de la compétition, une War Room était ouverte suivant l’importance des matchs. Elle était composée d’intervenants AWS grâce au programme IEM, de personnels techniques eTF1 et de membres du service management pour pouvoir réagir en cas d’imprévus. \nLa War Room a d’ailleurs été mise à contribution puisque la plateforme à subi des attaques DDOS pendant certains matchs. Le CDN Cloudfront et WAF ont permis de les contenir.\n\nChez Bedrock Streaming, nous étions curieux de ce retour d’expérience : nous avons préparé ce même type d’événement lors de l’Euro de football 2020. Les défis à surmonter sont les mêmes que ceux que nous avions rencontrés et nous sommes arrivés à des conclusions similaires dans nos choix techniques. Nous avions d’ailleurs développé un outil pour répondre au problème de scalabilité dans kubernetes durant l’Euro 2020 et que nous utilisons toujours aujourd’hui, un article de blog à ce sujet est disponible ici.\n\nComment bien débuter avec Amazon CodeWhisperer\n\nConférence présentée par :\n\n  Sébastien Butreau, Senior Partner Solutions Architect, AWS\n  Sébastien Grazzini, Principal Solutions Architect, AWS\n\n\n\n\nAmazon annonce une sortie grand public, prochaine, de son assistant de développement par IA CodeWhisperer (update: depuis, CodeWhisperer est passé GA).\nNous avons eu droit à une démonstration de l’outil. En quelques minutes et seulement à l’aide de quelques commentaires, les deux présentateurs ont produit un script python capable de prendre en entrée un répertoire de photos et donner en sortie un JSON qui, pour chaque photo, donnait le nom de la célébrité présente dessus.\n\nChez Bedrock Streaming, nous pensons qu’il est très important de suivre ce nouveau tournant que prend l’aide au développement via l’IA depuis quelques mois. Nous prévoyons de tester lors de nos journées R&amp;amp;D Github Copilot et Amazon CodeWhisperer.\n\nL’outil d’Amazon a quelques atouts, notamment la fonctionnalité de suivi des références qui permet de savoir si du code proposé est similaire à du code utilisé pour l’apprentissage et peut-être protégé par une licence incompatible avec notre usage. De plus, l’intégration du SDK Amazon est assez poussée et cela prend tout son sens, notamment lors du développement pour des lambdas AWS où l’outil semble très performant.\n\nBienvenue dans le Monde Merveilleux des Systèmes Distribués\n\nCette année encore, nous avons eu la chance de pouvoir partager notre expérience, lors d’une conférence donnée par Pascal, Principal Engineer et AWS Hero : « Bienvenue dans le Monde Merveilleux des Systèmes Distribués »\n\n\n\nPourquoi s’embêter avec des Systèmes distribués ? Comment en tirer profit ? Quels dangers ? Scalabilité, coordination et résilience : trois grands axes pour ce talk, basé sur l’expérience acquise par les équipes Bedrock, tant infra que devs, depuis plusieurs années.\n\nEn tant que speaker, pouvoir partager avec notre communauté est toujours aussi agréable ! Et, dans le public, il était assez intéressant d’entendre les réactions de nos voisins lorsque Pascal racontait certaines anecdotes ou présentait certains concepts. Les problématiques que nous rencontrons dans nos métiers, nous sommes nombreux à les rencontrer, et c’est tout l’intérêt des événements comme AWS Summit : apprendre les uns des autres !\n\nCette présentation n’a malheureusement pas été enregistrée lors du Summit, mais Pascal l’a redonnée depuis à MixIT, où elle a été enregistrée – et les vidéos devraient être bientôt publiées ;-)\n\nLa souveraineté des données chez AWS\n\nUne des conférences portait sur les thèmes de la Souveraineté dans le Cloud AWS et du Règlement européen Général sur la Protection des Données (RGPD). Lors de cette présentation, Stephan Hadinger (Directeur de la Technologie chez AWS) a exposé le cadre de ce règlement et sa mise en application au sein de l’infrastructure AWS. C’est cette partie qui était, d’après nous, la plus intéressante, étant donnée sa dimension technique.\n\nRGPD est un regroupement de règles qui régissent et protègent les droits des résidents d’Union Européenne. Il porte sur le respect de la confidentialité et la protection des données personnelles. Toute entreprise exerçant dans l’UE y est soumise. Dans le cas présent, la RGPD couvre à la fois les clients AWS (comme Bedrock) et les utilisateurs finaux (comme les utilisateurs des services Bedrock).\n\nChez AWS, la Souveraineté est synonyme d’autonomie stratégique et s’exprime de la façon suivante :\n\n  la possession des données clients : tous les clients AWS ont le contrôle de leurs données et applications, et nous verrons comment ;\n  le choix de la localisation des données, via la possibilité d’héberger l’intégralité des données sur le territoire de son choix, en France par exemple ;\n  l’accès au meilleur de la technologie, qui favorise l’innovation ;\n  et la possibilité de changer de solution (pas de lock-in).\n\n\nLes clients sont les seuls possesseurs de leurs données, ils en ont le contrôle total : AWS n’a aucun droit d’usage des données de leurs clients. De plus, AWS n’a pas accès aux données et ne déplace pas (géographiquement) les données de ses clients.\n\nL’implémentation technique de ces concepts repose, entre autres, sur le chiffrement systématique des données. AWS Nitro est une des briques d’architecture qui en est responsable pour les EC2 (depuis 2013 pour la partie réseau). Nitro permet le chiffrement de toute la chaîne de données (réseau, volumes de stockage) et comprend plusieurs composants :\n\n  Carte Nitro dédiée au échanges externes (réseau + accès aux EBS, stockage persistant)\n  Carte Nitro pour le stockage local (stockage temporaire attaché à l’hôte)\n  Hyperviseur Nitro (il s’agit d’un hyperviseur basé sur linux KVM, mais grandement modifié pour les besoins, pas de sshd, pas de systemd, pas de couche réseau)\n  Puce de sécurité Nitro (qui empêche le client d’avoir accès aux composants de l’hôte, procède à la mise à jour des firmwares des composants du serveur et gère le sécure boot afin de contrôler l’état des firmwares des composants avant de démarrer l’hôte).\n\n\n\n\nAu delà du chiffrement dont il est principalement question ici, Nitro permet aussi de grandement augmenter les performances des EC2 en limitant l’impact de l’hyperviseur sur le CPU utilisé par les clients. Dans le cas d’une virtualisation classique, toutes les tâches listées ci-dessus sont effectuées par le processeur lui-même, grignotant ainsi de la puissance des machines. Ici, Nitro permet de décharger le CPU de ces tâches en le rendant ainsi dédié aux EC2.\n\nAWS utilise aussi des solutions telles que Key Management Service (KMS) pour chiffrer les données de plus d’une centaine de ses services. Il s’agit là aussi d’un système de protection des données des utilisateurs : seul l’opérateur possédant la clé de chiffrement est capable de lire les données de ces services. \nUne version étendue de KMS est même disponible pour les clients les plus soucieux de la protection de leurs données : External Key Stores. XKS est un dispositif physique pouvant être hébergé en dehors des locaux d’AWS. Il est même capable de se “défendre” contre les attaques physiques en procédant à l’effacement des clés lors d’une tentative d’intrusion physique. Il s’agit probablement de l’ultime implémentation de sécurité et de souveraineté chez AWS.\n\n\n\nTout au long de cette conférence, on a bien senti que le but d’AWS, afin de respecter les données de ses usagers, était de faire en sorte de ne pas pouvoir accéder aux données de ses clients.\n\nLe mot de la fin\n\nL’AWS Summit comportait plus d’une centaine de sessions et nous avons juste eu l’occasion d’effleurer le contenu proposé lors de cette journée.\n\nNous avons commencé à migrer vers Le Cloud en 2018 et notre premier AWS Summit Paris était en 2019 – nous y avions d’ailleurs déjà parlé de cette migration au cours d’un autre événement.\n\nDepuis, que de chemin parcouru ! Cette année, nous pensions moins à Kubernetes, à DynamoDB ou aux optimisations de coûts, sur lesquels nous avons bien bossé depuis 2019. Notre attention était plus attirée vers des sujets que nous avons commencé à travailler plus récemment et où nous avons encore des challenges majeurs, comme les approches pleinement serverless ;-)\n"
} ,
  
  {
    "title"    : "Bedrock au Kubernetes Community Days France 2023",
    "category" : "",
    "tags"     : " kubernetes, cloud, devops, opensource, community, conference, rex",
    "url"      : "/2023/04/03/kubernetes-community-days.html",
    "date"     : "April 3, 2023",
    "excerpt"  : "La première édition de KCD (Kubernetes Community Days) en France s’est déroulée le 7 mars au Centre Pompidou et rassemblant près de 1000 participants pour une belle journée de conférences.\n\n\n\nKCD a rassemblé les communautés tech françaises pour ce...",
  "content"  : "La première édition de KCD (Kubernetes Community Days) en France s’est déroulée le 7 mars au Centre Pompidou et rassemblant près de 1000 participants pour une belle journée de conférences.\n\n\n\nKCD a rassemblé les communautés tech françaises pour cette journée de partage d’expertise et de retours d’expérience autour de Kubernetes et des technologies Cloud Native et DevOps.\n\nSolomon Hykes, son acolyte Jérome Petazzoni et l’Éducation Nationale ont présenté la keynote d’ouverture.\nCette première keynote a permis d’introduire le projet Santorin du Ministère de l’Éducation. C’est un système d’aide à la correction et à la notation pour lequel ils utilisent 3 clusters afin d’analyser 5 millions de copies.\n\n\n  \n\nSolomon Hykes &amp;amp; Jérôme Petazzoni\n\n\nDes grands acteurs de la tech en France tels que Scaleway, OVHCloud, Shadow, eTF1, Back Market, vpTech, Doctolib, Deezer, Carrefour et l’Éducation Nationale étaient présents pour rapporter leurs expériences. \nLes trois salles nommées aux couleurs du drapeau français étaient disponibles tout au long de la journée pour accueillir la quarantaine de conférences organisées par KCD.\n\nLa plus-value d’un portail développeur chez Back Market \n\nConférence présentée par :\n\n\n  Sami Farhat - Backend Engineer\n\n\nBack Market, entreprise française de commerce électronique, est venue nous parler de leur implémentation \nd’un “DevPortal” basé sur le projet Backstage.io.\n\n\n\nLa création de ce portail développeur à été initié en 2021 suite au projet de mise à l’échelle et de passage en microservices de leur applications.\n\nInitialement, chaque nouveau service était créé manuellement et nécessitait du travail dans les équipes d’infrastructure.\nEn plus de demander du travail lors de leur création, les services n’étaient donc pas systématiquement créés avec les mêmes bases de codes et pouvaient différer dans leurs implémentation.\n\nLe but était donc d’obtenir une vue centralisée sur les projets et de permettre aux développeurs de créer de nouveaux services eux-mêmes.\n\n\n\nLa création de ce portail à également permis à Back Market d’initier l’utilisation d’un modèle pour la création de services, ainsi d’uniformiser les architectures et de faciliter le passage en microservices.\n\nIls ont également implémenté une vue relationnelle concernant les projets et les équipes qui y sont associées.\n\n\n\nEnfin, pour trouver les projets prioritaires pour la migration en microservices, ils ont créé une vue nommée Coupling scores :\n\n\n\nC’est une vue qui permet d’obtenir la liste des applications monolithiques avec le taux de couplage le plus élevé.\n\nLe replay de cette conférence est disponible ici.\n\nVPC dans k8s : Pas aussi simple que ça en a l’air \n\nConférence présentée par :\n\n\n  Louis Portay - Ingénieur DevOps Kapsule Scaleway\n\n\nAu tour de Scaleway qui nous ont présenté comment ils ont implémenté les VPC privés dans le service Kaspule, leur Kubernetes managé.\nC’est un besoin qui s’est présenté afin d’éviter que les échanges inter-noeuds transitent via le réseau public.\n\n\n\nPour utiliser le réseau privé dans Kaspule, ils ont ajouté une interface nommée “kapsule0” sur les instances utilisées dans la création du cluster. Cette interface est ensuite attachée à Cilium dans le cluster.\n\n\n\nCette fonctionnalité est actuellement en bêta, elle sera bientôt disponible de manière régionale.\n\nParmi les implémentations futures, Scaleway prévoit de proposer la possibilité de retirer l’interface réseau publique afin que tous les échanges entre Kubelet et le Control Plane passent également via le réseau privé.\n\nLe replay de cette conférence est disponible ici.\n\nKubernetes the not so hard Veepee way \n\nConférence présentée par :\n\n\n  Loïc Blot - Lead SRE Veepee\n  Mickaël Todorovic - Tribe Lead SRE Veepee\n\n\nLoïc et Mickaël de Veepee sont venus présenter l’évolution des infrastructures utilisées par l’entreprise.\nInitialement, avant 2019, ils comptaient plus de 10 000 machines virtuelles dans leur parc.\nCes machines hébergaient les applications de Veepee via diverses technologies :\n\n  Swarm\n  Rancher\n  Hashicorp Nomad\n  Docker compose\n  LXC\n\n\nEn 2019, pour anticiper la gestion de la vente des tickets pour le concert de Céline Dion, ils ont choisi de migrer leurs services sur des clusters Kubernetes.\nLa première infrastructure était managée via Ansible, ils utilisaient Traefik et un cert manager home made.\n\n\n\nAujourd’hui, ils fournissent un produit Container as a Service nommé Starfish. C’est un outil qu’ils ont écrit en Go et qui permet de gérer les applications des équipes Veepee. Ils utilisent également Gitlab et ArgoCD.\n\nLe replay de cette conférence est disponible ici.\n\nConclusion\n\nLa majorité de conférences auxquelles j’ai assisté étaient des retours d’expérience. C’était particulièrement intéressant car en plus de la présentation d’une technologie, nous avons un retour détaillé sur l’usage de cette dernière.\n\nMerci à tout les speakers pour leur partage de connaissances et aux organisateurs de KCD France.\n"
} ,
  
  {
    "title"    : "Twitch: du streaming mais pas en lit de pierre #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/twitch-streaming",
    "date"     : "March 31, 2023",
    "excerpt"  : "Twitch: du streaming mais pas en lit de pierre.\nPrésenté par Quentin GILLIE.\n",
  "content"  : "Twitch: du streaming mais pas en lit de pierre.\nPrésenté par Quentin GILLIE.\n"
} ,
  
  {
    "title"    : "REX-Shape Up, un LFT dont vous êtes les héros #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/rex-shape-up",
    "date"     : "March 31, 2023",
    "excerpt"  : "REX-Shape Up, un LFT dont vous êtes les héros. \nPrésenté par Pierre-Thomas GUILLOT.\n",
  "content"  : "REX-Shape Up, un LFT dont vous êtes les héros. \nPrésenté par Pierre-Thomas GUILLOT.\n"
} ,
  
  {
    "title"    : "Retour Conférence Vue Amsterdam 2023",
    "category" : "",
    "tags"     : " node, Node, vue, vuex, pinia, vite, Vitest, TypeScript, developer, javascript",
    "url"      : "/2023/03/31/retour-vue-amsterdam-2023.html",
    "date"     : "March 31, 2023",
    "excerpt"  : "C’est dans le Theater Amsterdam que se sont déroulés ces deux jours de VueJS Amsterdam, événement faisant partie de la JSWorld Conference, durant toute la semaine.\n\nDe nombreux sponsors étaient là pour l’occasion, ainsi que des écoles comme VueMas...",
  "content"  : "C’est dans le Theater Amsterdam que se sont déroulés ces deux jours de VueJS Amsterdam, événement faisant partie de la JSWorld Conference, durant toute la semaine.\n\nDe nombreux sponsors étaient là pour l’occasion, ainsi que des écoles comme VueMastery ou VueSchool (proposant une toute nouvelle certification Vue), et des partenaires plus connus comme Storyblok ou Nuxt Labs.\n\n\n\nL’ambiance était au rendez-vous dès le début avec une conférence en musique, avec Tim Benniks guitare en main !\n\nNous avons ensuite pu profiter de conférences aussi nombreuses que variées. De l’accessibilité à la gestion de l’interface en Vue du leader mondial du transport de marchandises Maersk, en passant par les tests et le guide ultime pour publier un package NPM !\n\nSans oublier la grande famille Nuxt qui était (presque) au complet !\n\nL’ensemble des conférences est visible sur la chaîne youtube du JSWorld Conference.\n\nState of Vuenion\n\nEvan You (créateur de Vue et Vite) a présenté un état des lieux et des dernières avancées de Vue, épaulé par Alex Kyriakidis (fondateur de VueSchool).\n\n\n\nCommençant par un retour sur les nombreuses nouveautés de ces trois dernières années, pour les plus connues Vue 3, Vite, Vitest et Pinia, Evan a d’abord fait un focus sur la position de Vue 3 en tant que version par défaut depuis le 7 février 2022.\n\nIl a aussi évoqué les avancées relatives à Vue 2.7 (dont l’intégration de la Composition API, du v-bind CSS, etc.) permettant de rapprocher les expériences développeurs entre Vue 2 et Vue 3.\n\nEvan a ensuite présenté les derniers travaux sur la version core de Vue en version 3. Principalement orientés sur des améliorations concernant la facilité d’utilisation, l’accélération des tests (avec Vitest) ainsi que la vitesse de build (avec Rollup).\n\nEnfin, Evan a présenté les projets à venir pour le core. La disparition de la Reactivity Transform jugée trop risquée, l’amélioration du server side rendering et la création d’un “vapor mode”, une nouvelle manière de compiler les Single Files Components en optimisant l’utilisation d’un Virtual DOM. Ce dernier permet d’approcher la vitesse de compilation d’une application en JS vanilla, en gardant la puissance du framework !\n\n\n  📺 Conférence sur Youtube\n\n\nLa fin de vie de Vue 2\n\n\n\nAprès avoir présenté l’état de l’écosystème qui gravite autour de Vue et Vite lors de sa conférence “State of the Vuenion 2023”, Evan You a terminé en dévoilant la date de fin de vie de Vue 2 au 31 décembre 2023.\n\nAprès cette date, la version arrêtera de recevoir des mises à jour ; il sera néanmoins possible de bénéficier de mise à jour payante, bien qu’il soit vivement conseillé de migrer vers la dernière version du framework.\n\nUne page dans la documentation a été mise en place, présentant les options qui s’offrent aux développeurs et aux organismes qui n’ont pas encore migré leurs applications.\n\nIl était une fois… Histoire\n\n\n\nGuillaume Chau, un des membres de l’équipe de développement de Vue, a présenté l’outil Histoire durant une petite demi-heure.\n\nL’outil est dans la même veine que Storybook. La plupart du temps, il sert à afficher et documenter des composants d’un design system en complète isolation.\n\nContrairement à d’autres outils, Histoire est pensé pour s’intégrer parfaitement dans son environnement de développement, de façon à ce que l’écriture des “stories” s’apparente le plus possible à l’écriture et à l’utilisation native de composants Vue.\n\nHistoire utilise Vite ce qui lui permet de s’intégrer dans un projet qui l’utilise déjà avec très peu de configurations supplémentaires.\n\n\n  📺 Conférence sur Youtube\n\n\nUne autre histoire de… migration !\n\n\n\nLa société Maersk, spécialiste dans la logistique des transports, nous a présenté son application destinée entre autres, à la gestion des conteneurs maritimes. Une occasion pour nous faire part de leur processus de migration de Vue 2 vers Vue 3 !\n\nRéalisant la même migration de version à Bedrock Streaming sur la partie backoffice, nous constatons que nous partageons beaucoup de similitudes !\n\nVous trouverez un article dédié sur la migration Vue 2 vers Vue 3 à Bedrock en suivant ce lien ! 🎉\n\n\n  📺 Conférence sur Youtube\n\n\n“Let’s Build A Virtual DOM”\n\n\n\nCertaines conférences étaient aussi l’occasion de rappeler des fondamentaux. Beaucoup de développeurs sont conscients que Vue utilise un système de DOM virtuel pour générer ses pages mais peu savent vraiment ce que cela signifie.\n\nLa conférence de Marc Backes a permis de démystifier cela en développant sur scène un DOM virtuel simple à travers plusieurs cas pratiques.\n\nCe dernier a d’ailleurs publié le code écrit sur son Github : https://github.com/themarcba/vue-vdom.\n\n\n  📺 Conférence sur Youtube\n\n\nLe guide complet du packaging des librairies\n\n\n\nCette conférence était présentée par Bjorn Lu (core team member de Astro, Vite et Svelte), et expliquait comment créer un package d’une librairie et le publier “presque” sans peine.\n\nEn prenant l’exemple d’une librairie extrêmement simple, proposant une fonction d’addition, Bjorn a parcouru les étapes de la création de ce package progressivement, en prenant en compte le fonctionnement d’ESModules, l’ajout du typage, puis le support de CommonJS pour les utilisations sur des anciennes versions de node et de l’export en parallèle des version ESM et CJS.\n\nIl présente ensuite les outils de build les plus courants ainsi que quelques outsiders, puis prend l’exemple de Tsup pour montrer une commande de build.\n\nUne solution intéressante pour typer utilisant JSDoc et simplifiant beaucoup les étapes du build completera sa conférence, \npour enfin terminer par une série d’outils et de “do and don’t” très pratiques dont publint.dev fait parti.\n\n\n  📺 Conférence sur Youtube\n\n\nConclusion\n\n\n\nCe séjour à Amsterdam pour assister à cette conférence de deux jours a été enrichissant à bien des égards.\nNon seulement la ville est belle et agréable à visiter, mais la découverte d’outils prometteurs répondant à nos besoins a également été un véritable apport pour notre entreprise.\n\nPour en découvrir plus\n\n\n  Site VueJS Amsterdam\n  Gallerie photos\n  Replay des conférences sur Youtube\n\n"
} ,
  
  {
    "title"    : "Montres bracelets, le guide pratique de l&#39;amateur d&#39;horlogerie #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/guide-pratique-amateur-horlogerie",
    "date"     : "March 31, 2023",
    "excerpt"  : "Montres bracelets, le guide pratique de l’amateur d’horlogerie.\nPrésenté par Rafi PANOYAN.\n",
  "content"  : "Montres bracelets, le guide pratique de l’amateur d’horlogerie.\nPrésenté par Rafi PANOYAN.\n"
} ,
  
  {
    "title"    : "Comment gérer des journées de 35h #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-gerer-des-journees-de-35h",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment gérer des journées de 35h\nPrésenté par Sylvain GOUGOUZIAN.\n",
  "content"  : "Comment gérer des journées de 35h\nPrésenté par Sylvain GOUGOUZIAN.\n"
} ,
  
  {
    "title"    : "Comment (enfin) sortir vos side projects #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-enfin-sortir-vos-side-projects",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment (enfin) sortir vos side projects.\nPrésenté par Thomas JARRAND.\n",
  "content"  : "Comment (enfin) sortir vos side projects.\nPrésenté par Thomas JARRAND.\n"
} ,
  
  {
    "title"    : "Comment j&#39;ai réussi à capturer la couleur et quelle est sa signification ? #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/capture-et-signification-de-la-couleur",
    "date"     : "March 31, 2023",
    "excerpt"  : "Comment j’ai réussi à capturer la couleur et quelle est sa signification ?\nPrésenté par Hugo DETANG.\n",
  "content"  : "Comment j’ai réussi à capturer la couleur et quelle est sa signification ?\nPrésenté par Hugo DETANG.\n"
} ,
  
  {
    "title"    : "Couleur, Typographie, Logo: Analyse dune charte graphique #LFT 31/03/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/analyse-charte-graphique",
    "date"     : "March 31, 2023",
    "excerpt"  : "Couleur, Typographie, Logo: Analyse dune charte graphique. \nPrésenté par Sylvain MASSON.\n",
  "content"  : "Couleur, Typographie, Logo: Analyse dune charte graphique. \nPrésenté par Sylvain MASSON.\n"
} ,
  
  {
    "title"    : "De Node.js 10 à Node.js 18, nous avons rattrapé 8 ans de retard et de dette technique",
    "category" : "",
    "tags"     : " node, Node.js, vue, Vue.js, vuex, pinia, vite, Vite.js, Vitest, TypeScript, developer retention, migration",
    "url"      : "/2023/03/25/de-node-js-10-a-node-js-18-nous-avons-rattrape-8-ans-de-retard-et-de-dette-technique-et-seule-une-approche-progressive-et-incrementale-etait-viable.html",
    "date"     : "March 25, 2023",
    "excerpt"  : "Difficile de faire évoluer des applications et améliorer une stack si l’ensemble est basé sur une version obsolète de Node.js… Dans cet article, nous verrons comment nous avons réussi à migrer vers une version récente et maintenue de Node.js grâce...",
  "content"  : "Difficile de faire évoluer des applications et améliorer une stack si l’ensemble est basé sur une version obsolète de Node.js… Dans cet article, nous verrons comment nous avons réussi à migrer vers une version récente et maintenue de Node.js grâce à une approche progressive et incrémentale.\n\n\n  Contexte général et fonctionnel\n  Contexte technique\n  Objectif\n  Une première stratégie problématique : la méthode “rhinocéros” 🦏\n  La stratégie gagnante : une migration progressive 📶    \n      Motivation\n      Plan d’action\n    \n  \n  Difficultés rencontrées    \n      Non découpage des étapes de migration\n      Méconnaissance de Typescript\n      Suppression précipitée de librairies obsolètes\n      Non anticipation de la complexité liée à certaines dépendances\n      Entretien des applications legacy en même temps\n    \n  \n  Autres avantages    \n      Uniformisation des technologies au sein de la société\n      Attractivité et rétention des développeurs\n    \n  \n  Conclusion\n\n\nContexte général et fonctionnel\n\nBedrock streaming est une co-entreprise (joint-venture) créée en 2020 par M6 Group et RTL Group, permettant à 7 diffuseurs et sociétés de médias dans 5 pays d’Europe de divertir 45 millions d’utilisateurs chaque jour, sur tous les écrans.\n\nPour gérer tous leurs utilisateurs ainsi que leurs contenus, notamment vidéos, les clients de Bedrock Streaming accèdent chacun à une constellation d’applications au sein d’un back-office centralisé (appelé BO par la suite).\n\nContexte technique\n\nDe part sa conception initiale, le BO est une application monorepo. Elle fournit (à elle-même donc), des données via une API Symfony 4 (PHP 7.4), consommées uniquement par :\n\n\n  des applications Vue.js 1 et Vue.js 2 gérées par la team backend (qui historiquement maintient le frontend de quelques applications) ;\n  des applications Vue.js 2 gérées par la team frontend.\n\n\nLe tout, dans un environnement Node.js 10.\n\nObjectif\n\nNode.js 10 est arrivé en fin de vie le 30 avril 2021. Il n’est donc plus maintenu, que ce soit en terme de fonctionnalités ou en terme de sécurité. Naturellement, toutes les dépendances JS migrent progressivement vers un support des versions de Node.js supérieures, et abandonnent le support de cette version 10 devenue obsolète.\n\nIl s’agit donc de migrer la version de Node.js vers une version supérieure, dans l’idéal LTS afin de se prémunir d’une obsolescence prématurée. Dans un premier temps, Node.js 12.\n\nVoici plusieurs raisons qui poussent à migrer Node.js :\n\n\n  Nouvelles fonctionnalités (e.g. nouvelle implémentation pour l’ES6 Module Support expérimental, source : https://nodejs.medium.com/announcing-a-new-experimental-modules-1be8d2d6c2ff ) ;\n  Abandon de fonctionnalités défaillantes (e.g. via dépréciation) ;\n  Performance (e.g. mise à jour de V8 engine, source : https://nodejs.medium.com/introducing-node-js-12-76c41a1b3f3f ) ;\n  Sécurité (e.g. mise à jour de TLS, source : https://nodejs.medium.com/introducing-node-js-12-76c41a1b3f3f ) ;\n  Évolutions des dépendances externes. (e.g. Cypress qui abandonne les versions de Node.js non maintenues et qui requiert Node.js 14, 16 ou 18+, source : https://docs.cypress.io/guides/references/changelog#12-0-0).\n\n\nUne première stratégie problématique : la méthode “rhinocéros” 🦏\n\nLa décision a été prise de migrer le repository de Node.js 10 vers Node.js 12 en début d’année 2021.\n\nEmpiriquement, cette méthode a montré plusieurs limites :\n\n\n  même si la compilation semblait bien se dérouler, des erreurs apparaissaient au moment de l’affichage de l’UI ➡ Il semblait donc nécessaire de parcourir l’intégralité des écrans afin de déceler toutes les anomalies possibles ➡ Le travail de la QA était alors conséquent ;\n  même lorsqu’une anomalie est corrigée, une nouvelle peut apparaitre ➡ Il fallait re-parcourir les écrans concernés (par exemple, après avoir corrigé une anomalie qui empêche l’apparition d’une modale, de nouvelles anomalies peuvent être décelées au niveau des fonctionnalités que permet cette modale) ➡ Le travail de la QA augmentait de façon exponentielle au fil des corrections d’anomalies ;\n  des dizaines voire centaines de dépendances dans le projet étaient dépendantes de Node.js 10 sans être encore compatibles avec Node.js 12 ➡ Il s’agissait donc de faire le point sur celles-ci, pour trouver des équivalents compatibles.\n\n\nAprès plusieurs mois, bien que bon nombre d’anomalies avaient pu être corrigées, la situation stagnait et la fin ne semblait pas plus proche qu’au début.\n\nLes raisons de l’échec :\n\n\n  L’ancienneté de certaines applications. Certaines d’entre elles avaient plus de 8 ans d’existence. En n’ayant subi que quelques corrections seulement. Les connaissances fonctionnelles et techniques s’étaient donc estompées naturellement, en raison d’une absence de documentation (autant fonctionnelle que technique). Il s’agit là des dettes fonctionnelle et technique. Lorsqu’elles sont là, elles sont relativement simples à identifier. Mais c’est déjà trop tard… ;\n  L’absence de mise à jour des technologies. Certaines technologies devenues obsolètes (jQuery 1.9, Vue.js 1, Bootstrap 2.3) imposait non plus un refactor lié à une migration, mais une véritable refonte ;\n  L’absence de tests. La couverture de tests était alors faible voire nulle. Migrer sans régression relevait alors d’une chance non maitrisable ;\n  La façon dont la migration a été lancée était trop téméraire : c’est la méthode rhinocéros.\n    \n      création d’une nouvelle branche (et d’une PR pour cette branche)\n      suppression de Node.js 10 et installation de Node.js 12\n      correction de toutes les anomalies qui apparaissent !\n    \n  \n\n\nCe fonctionnement peut marcher pour des périmètres techniques plus petits ou du moins dont les contours sont précisément marqués ;\n\nL’organisation en équipe devenait compliquée. Au fur et à mesure des découvertes des anomalies au sein d’une seule et unique PR, il devenait difficile de suivre tous les sujets, sans découpage précis et rigoureux.\n\nFace à cette situation, dont les développeurs et testeurs ne semblaient plus voir le bout, il a été décidé d’employer une autre stratégie.\n\nLa stratégie gagnante : une migration progressive 📶\n\nDe part un essoufflement des développeurs et une nouvelle énergie insufflée par des départs et arrivées dans l’équipe, une nouvelle stratégie a émergé. Face à l’échec de la première, il a été proposé plus simplement de partir sur des bases saines, afin de migrer les applications sur des fondations plus solides car maitrisées.\n\nPlus techniquement, cela s’est traduit par :\n\n\n  Création d’un nouveau répertoire modern-apps/ dans le monorepo.\n  Mise en place d’une architecture basée sur Node.js 16 (Oui oui, Node.js 16 directement ! Il s’agissait de la version LTS en cours en date de début 2022.) dans ce répertoire seulement.\n  Migration des applications du BO, une par une, vers une stack plus moderne. En date de début 2023, cette migration est toujours en cours.\n\n\nMotivation\n\nLa motivation était principalement portée par :\n\n\n  une volonté forte d’abandonner des outils et technologies vieillissantes voire obsolètes ;\n  une pression engendrée par l’évolution rapide des technologies :\n    \n      Node.js sort une version LTS tous les ans ;\n      Vue.js 3 venait de sortir et l’effort des développeurs du framework allait se porter plutôt sur cette version que sur la version 2.\n    \n  \n  une pression engendrée par les autres équipes de la société qui, elles, étaient à jour (pour certaines), dont celle qui proposait des outils JS et TS dont l’équipe pourrait avoir l’usage, comme par exemple une librairie de configuration pour eslint couplé à vue ;\n  une excitation liée à l’utilisation d’une stack récente et de cutting-edge tools.\n\n\nPlan d’action\n\nCette page blanche a nécessité un plan d’action que voici :\n\n\n  Création d’une application simplissime en guise de PoC, afin de montrer la viabilité d’un travail sous Node.js 16 dans une sous-partie du projet en parallèle d’un travail toujours actif sous Node.js 10 dans le reste du projet.\n  Mise en place d’une certaines DX vis-à-vis des linters et formatters notamment (ainsi que d’extensions d’IDE), par l’application de règles simples mais strictes, qui évitent aux développeurs les tâches sans plus-value, comme ajuster manuellement l’indentation ou ajouter les points-virgules.\n  Migration des librairies internes au monorepo.\n  Migration du design system, ainsi que des outils afférents (Storybook).\n  Migration d’une première application, la plus simple possible. L’objectif était alors de se rendre compte très concrètement des étapes de migration d’une application, afin d’en tirer une documentation exploitable pour les futures applications. Il en est ressorti que la majeure partie du travail consistait à refactor le code avec les nouvelles technologies choisies, en l’occurrence :\n    \n      Vue.js 3 et sa Composition API (framework JS),\n      Vite (serveur de dev et de build),\n      Pinia (global state management),\n      Vitest (framework de test unitaire),\n      Cypress dans ses dernières versions (framework de test end-to-end)\n      aussi et surtout Typescript (langage de programmation, sur-couche à JS).\n    \n  \n  Migration du processus de build et d’intégration aux templates backend (via notamment une extension Twig implémentée par nos soins, ViteAppExtension.php)\n  Mise en place d’une CI pour ces nouvelles applications, calquée sur celle des anciennes applications : linting, tests pour celles qui en avaient, déploiement en preview, etc.\n\n\nEn quelques mois seulement, il a été possible d’obtenir un résultat concret. Le répertoire modern-apps/ a été initié en février 2022, et dès avril de la même année, une première application migrée était livrée en production. Et cela, avec un seul développeur à plein temps sur le sujet.\n\nDifficultés rencontrées\n\nCette seconde stratégie n’a bien sûr pas été sans encombre. Voici les principales difficultés rencontrées, dont l’équipe a su se prémunir au fil du temps.\n\nNon découpage des étapes de migration\n\nLors de la migration d’une des premières applications dont la complexité était légèrement supérieure aux précédentes, nous nous sommes retrouvés embourbés dans une multitude de bugs techniques et fonctionnels. En effet, migrer implique plusieurs changements qui n’ont pas nécessairement de rapport les uns avec les autres :\n\n\n  ajouter des types TS\n  migrer la librairie de Global State Management de vuex vers pinia\n  migrer la Global API de Vue (de new Vue() vers createApp())\n  migrer de l’Options API vers la Composition API de Vue\n  etc.\n\n\nSi tous ces changements sont opérés en même temps, comment réagir lors de l’apparition d’une anomalie ? Comment traquer efficacement cette anomalie ?\n\n\n  Solution adoptée\n\n  Nous avons décidé de découper plus finement nos développements. Une PR doit concerner un périmètre réduit et bien défini. Par exemple, la PR de migration de la librairie de Global State Management ne doit comporter que des modifications à ce sujet, et doit fournir une application fonctionnelle dont les tests passent.\n\n\nMéconnaissance de Typescript\n\n\n  TypeScript is a strongly typed programming language that builds on JavaScript, giving you better tooling at any scale.\n\n\nSource : https://www.typescriptlang.org/\n\nCe langage de programmation, bien que son adoption parmi les développeurs JS explose, s’est avéré une complète nouveauté dans l’équipe. Il peut être tentant d’écrire des any partout, ou de supprimer le strict mode…\n\n\n  Solution adoptée\n\n  Nous avons décidé d’intégrer TS progressivement sans se mettre trop de pression quant à l’intégralité du typage de nos applications. Typescript permet justement cette intégration progressive aux projets.\n\n  Un très gros progrès a aussi été réalisé grâce à la génération automatique des types TS à partir de l’API (grâce à l’introspection system de GraphQL). Les données reçues du backend se voyaient alors avoir une structure directement exploitable dans le frontend.\n\n\nSuppression précipitée de librairies obsolètes\n\nLors du découpage des étapes de migration, une problématique est apparue. Par exemple, si nous souhaitons migrer de vuex vers pinia dans un second temps, comment faire pour que l’application reste fonctionnelle avec vuex dans le premier temps ?\n\n\n  Solution adoptée\n\n  Nous avons décidé de conserver certaines librairies, le temps de la migration des applications. Il peut être tentant de vouloir supprimer immédiatement ce qui nous semble obsolète, mais ces éléments ne seront vraiment obsolètes que lorsque toutes les applications seront migrées ; mais pas le temps qu’elles le soient.\n\n\nNon anticipation de la complexité liée à certaines dépendances\n\nBien que cet aspect n’était pas une surprise, certaines librairies ont apporté plus de difficultés que d’autres lors de la migration. Par exemple, l’intégration de Vue 3 et la Composition API impliquait la montée de version de vee-validate, un librairie de validation de formulaire. Il s’est avéré que l’implémentation imposée était radicalement différente de la version précédente (compatible avec Vue 2 et l’Options API), moins intuitive et plus complexe.\n\n\n  Solution adoptée\n\n  Ce cas de figure n’est pas vraiment impressionnant car nous nous y attendions. Nous avons décidé dans un premier temps d’effectuer une certaine veille technique, afin de remettre en cause le choix initial de cette librairie. Il s’est avéré que nous l’avons conservée, ce qui amenait dans un second temps une montée en compétence quant à l’utilisation de celle-ci, en vue de son intégration.\n\n\nEntretien des applications legacy en même temps\n\nUne application donnée pouvait se retrouver d’une part en cours de migration, et d’autre part devoir recevoir une évolution ou une correction d’anomalie.\n\n\n  Solution adoptée\n\n  Le choix et l’ordre des applications à migrer a été choisi en fonction des priorités en cours. Nous avons choisi de migrer en premier les applications qui ne subissaient que très peu de modifications. Par la suite, et encore aujourd’hui, nous livrons en production rapidement chaque application migrée, afin de ne pas avoir à maintenir plusieurs versions en même temps (la version legacy étant tout de même conservée le temps de s’assurer que la version moderne tourne correctement en production auprès des clients). Dans les très rares cas où une application en cours de migration devait recevoir une évolution ou une correction d’anomalie, nous la traitions dans les 2 versions.\n\n\nAutres avantages\n\nUniformisation des technologies au sein de la société\n\nAu sein de Bedrock, le back-office n’est pas la seule application. Il existe aussi des applications frontend sur les mêmes technologies pour adresser l’écran web ou les télévisions connectées. Bien que le framework utilisé pour celles-ci soit React.js et non Vue.js, l’outillage peut être uniformisé entre les projets et les équipes. La migration a permis de préparer le terrain pour mettre en place ces outils : TypeScript, PNPM, etc.\n\nAttractivité et rétention des développeurs\n\nCette migration générale permet de mettre en place une stack résolument plus moderne et d’utiliser des outils et technologies plus récents. N’est-ce pas là un argument fort pour attirer des nouveaux développeurs et retenir ceux déjà en place ? Dans l’équipe, plusieurs personnes ont émis des doutes sur leur volonté de rester dans la société si la décision de migrer, et donc d’intégrer des technologies plus à jour, n’avait pas été prise. En date de début 2023, il fait peu de doutes que les projets en Vue 3 sont plus attractifs que les projets en Vue 2…\n\nConclusion\n\nEn fin de compte, cette approche progressive et incrémentale, toujours en cours, permet de maintenir dans un répertoire bien défini une stack récente dont les mises à jour sont simples car petites. Par exemple, nous avons récemment migré de Node.js 16 vers Node.js 18… en quelques jours !\n\nCette grande aventure, toujours en cours, nous a permis de vraiment prendre conscience qu’il faut entretenir certes les applications mais aussi les versions des frameworks et outils ! Utiliser un nouvel outil ou une nouvelle technologie est un choix fort qu’il faut être capable d’assumer dans le temps.\n\nIl peut paraitre frustrant d’entretenir des outils, sans gagner en performance ni en productivité mais seulement pour ne pas devenir obsolète. Mettre l’accent sur ces points, tout en sachant bien jauger jusqu’où doivent aller ces upgrades, est la marque d’un certain professionnalisme.\n\nIl est vrai que dans l’immédiat, la valeur ajoutée pour le client est modérée : les gains restent très techniques, notamment en termes de stabilité et de performances. Ce n’est que plus tard que les gains se feront concrètement sentir : plus d’efficacité et de productivité pour les évolutions, et plus de fiabilité.\n\nIl est aussi important de savoir reconnaitre qu’une technologie utilisée (parfois avec fierté à ses débuts) est devenue obsolète, et qu’il faut s’en débarrasser pendant qu’il est encore temps.\n"
} ,
  
  {
    "title"    : "La gamification contre le legacy",
    "category" : "",
    "tags"     : " infra, legacy, retour d'expérience",
    "url"      : "/2023/03/20/La-gamification-contre-le-legacy.html",
    "date"     : "March 20, 2023",
    "excerpt"  : "Ce que vous ne voulez pas voir dans vos backlogs…\n\nElles sont là, tapies dans l’ombre de la colonne “To do” de vos backlogs, attendant que leur heure vienne. À chaque backlog refinement, vous vous demandez s’il ne faut pas tout simplement les annu...",
  "content"  : "Ce que vous ne voulez pas voir dans vos backlogs…\n\nElles sont là, tapies dans l’ombre de la colonne “To do” de vos backlogs, attendant que leur heure vienne. À chaque backlog refinement, vous vous demandez s’il ne faut pas tout simplement les annuler, puisque personne ne les prend en charge… De quoi parle-t-on ? De ces user stories qui existent dans le backlog de chaque équipe technique, pour traiter “un jour” un sujet legacy. Ces petits aides-mémoire de sujets “à ne pas oublier” qui nous poursuivent mais ne sont que peu souvent traités, faute de priorisation.\n\n\nUn exemple de backlog legacy\n\n\nClean de code mort, montées de versions de layers Terraform, projets de refactoring jamais débutés… autant de sujets pénibles à traiter qui nécessitent du temps… et de la résilience. Parce que bien souvent, débuter l’un de ces sujets revient à s’attaquer à toutes les dépendances liées, à gérer tous les impacts. Et parce qu’il s’agit aussi de tâches redondantes, non-automatisables, n’apportant quasiment aucune valeur business immédiatement mesurable.. Du “run”, pur et simple. Dans le Slack de Bedrock, il y a un emoji tout trouvé pour ce type de tâche : \n\nBien sûr, on parvient parfois à dégager du temps pour s’atteler à ces user stories. Mais il faut souvent plus d’un sprint pour en venir à bout, et l’équipe en charge de leur réalisation peut rapidement se décourager devant l’ampleur et le caractère répétitif de la tâche.\n\nNos équipes Ops et DevOps sont responsables de 23 repositories Terraform. Lorsqu’il a été nécessaire d’upgrader tous nos layers en version 1.x, nous nous sommes d’abord donné pour consigne que chaque personne qui tombait sur un layer obsolète devait le mettre à jour avant de poursuivre son travail. Oui mais voilà, mettre à jour un layer ça ne se fait pas en deux minutes, et bien souvent on refuse d’abandonner ce sur quoi on travaillait jusqu’alors pour mettre à jour sa version de Terraform. La consigne a alors évolué : pour chaque layer à mettre à jour, on créé une US en colonne “to do”… Vous voyez où l’on veut en venir ? 😏\n\nPour tenter de venir à bout de ces sujets legacy que l’on traîne comme des boulets, nous avons mis en place depuis octobre 2022 les “Jeudis du fun”, dont l’organisation est prise en charge par la facilitatrice agile et la Project Manager Officer (PMO) du service Infrastructure (autrices de cet article).\n\n\nLogo de la 1ère édition du “jeudi du fun”\n\n\nÉradiquer en gamifiant, challengeant, s’entraidant.\n\nL’idée est simple : faire travailler ensemble, sur une journée, les cinq équipes de la verticale (trois équipes de SysAdmins et deux équipes DevOps) pour faire avancer un sujet legacy. Au cours de cette journée, les profils et les membres d’équipe seront mixés, afin de ne pas travailler avec les mêmes collègues qu’au quotidien. Leads, principal engineer, seniors et juniors : tout le monde participe à la corvée !\n\nIl est difficile de convoquer 25 personnes sur une journée en leur disant que la journée est banalisée pour traiter des tâches pénibles. Elles viendraient à reculons. Deux axes ont été choisis pour faire de ces journées des journées “particulières” :\n\n\n  Gamifier certains moments clés de la journée : la découverte du sujet, la composition des équipes, la remise en jambe du début d’après-midi…\n  Challenger les participants pour ne pas simplement leur demander de traiter du legacy, mais bien d’être la meilleure équipe pour traiter du legacy. Celle qui ira le plus loin, qui en fera le plus.\n  (Un troisième axe, plus convivial, est choisi pour la fin de journée : partager un verre tous ensemble.)\n\n\n\nLe jeu de découverte du sujet de la 3ème édition : Ansible\n\n\nLors de la 1ère édition, en octobre 2022, nous avons proposé aux équipes un grand thème : le repository “SysAdmin/Terraform”, centre névralgique du travail de l’Infra. Il y a beaucoup à faire : les fameux upgrades de layers, du refactoring de code pour industrialiser nos process, des PRs ouvertes et restées en suspens depuis de nombreux mois… chacun peut y trouver son compte. Chacune des six équipes composées ce jour-là disposait de dix minutes pour définir à quel chantier elle s’attaquerait durant la journée. A l’issue de ces dix minutes, le représentant de l’équipe devait présenter aux autres le sujet choisi et l’indicateur qui permettrait de juger si le travail a été accompli ou non, en fin de journée. L’équipe ayant proposé le sujet le plus ambitieux s’est vue attribuer des points bonus, rentrant en compte pour le calcul du score final.\n\nPour la seconde édition le mois suivant, le sujet était imposé : toutes les équipes avaient pour objectif de mieux sécuriser les secrets contenus dans le code Bedrock. L’équipe qui en traiterait le plus grand nombre l’emporterait.\n\nLors de la dernière édition, en février dernier, la compétition reposait également sur le nombre de points gagnés par chaque équipe en fin de journée. Nous avons attribué un nombre de points à chaque tâche pouvant être traitée dans la journée, en fonction de sa complexité et/ou de sa priorité. Chaque équipe pouvait s’organiser librement : choisir plusieurs petites tâches ou deux plus importantes…\n\nBien sûr, pour que la compétition soit totale, chaque édition du jeudi du fun se termine par une remise de prix : distribution de goodies, de cartes “bonus” ou “malus” valables dans nos “vrais” sprints, de gourmandises… il faut que la récompense soit réelle pour que les participants se prennent au jeu.\n\n\nExemple de lot pouvant être remporté lors du “Jeudi du fun”\n\n\nLe risque avec la compétition, c’est de se laisser déborder : gagner coûte que coûte, ajouter des points à son compteur en faisant du “quick &amp;amp; dirty”. Jusqu’à présent, la compétition dans la verticale Infra est restée bon enfant : les équipes se défient entre elles tout au long de la journée, des points “bonus” sont réclamés aux organisatrices au moindre prétexte… mais personne ne perd de vue l’objectif principal : venir à bout du sujet.\n\nLes Jeudis du Fun reposent donc sur le challenge et le jeu. Mais nous avions sous-estimé un autre axe nous permettant de faire de ces journées un succès : l’entraide. A chaque édition, les retours les plus enthousiastes portent sur le fait de passer une journée à travailler en cross-team. SysAdmins et DevOps apprennent les uns des autres, les juniors ont l’occasion de former des leads… et chacun élargit son spectre de compétences. Au-delà du fait de venir à bout de sujets legacy, l’émulation engendrée par ces journées justifie à elle-seule leur organisation.\n\nEt puis, quitte à faire des jeudis du fun des journées particulières, autant y aller franchement : certains membres de nos équipes n’hésitent pas à venir déguisés pour ajouter une dose de fun. Vous avez croisé une licorne, Pikachu ou un plombier dans l’open space de Bedrock ? Aucun doute, c’était un jeudi ! Un dress code a même été défini lors de l’édition de février 2023.\n\nItérer, et corriger nos erreurs à chaque édition\n\nTrois éditions du “jeudi du fun” ont été organisées jusqu’à présent. À la fin de chaque édition, les organisatrices recueillent le feed-back des participantes et participants, afin de corriger ce qui doit l’être et de capitaliser sur ce qui a marché. Voici le premier bilan que nous pouvons en tirer.\n\nDe l’importance du choix du sujet\n\nLe succès de la journée repose sur le choix du sujet. En choisissant un sujet fédérateur, comme lors de notre première édition, et en laissant le soin à chaque équipe de définir quel chantier elle souhaitait mener, nous partions gagnantes. Le repo Sysadmin/Terraform sur lequel nous avons travaillé lors de cette journée est un point de douleur pour l’ensemble de nos équipes : chacun des participants a compris l’intérêt de jouer le jeu et de retrousser ses manches. Les équipes ont même eu du mal à clôturer la journée, car elles voulaient finir ce qu’elles avaient commencé.\n\n\nAu cours de la 1ère journée du “Jeudi du fun”\n\n\nLors de la seconde édition en revanche, le sujet de cette édition a mis la journée en péril. Nous avions demandé aux équipes d’ajouter un niveau de sécurité à l’ensemble des secrets contenus dans la codebase de Bedrock. Cela a suscité quelques difficultés :\n\n  Tout d’abord, il s’agissait de trouver une méthode pour identifier tous les secrets concernés. Toutes les équipes du jeudi du fun ont alors planché sur ce sujet, en utilisant des méthodes et outils différents. Au final, nous ne sommes parvenus que tardivement (2h après le lancement de la journée) à nous mettre d’accord sur une méthodologie. Autant de temps perdu que nous aurions pu consacrer au cœur du sujet, la sécurisation des secrets.\n  En nous attaquant à l’ensemble des secrets de Bedrock, nous touchions forcément à des repositories projets dont nous ne sommes pas les code owners. Ce n’est pas une véritable difficulté en soi, puisqu’au quotidien, nous intervenons fréquemment dans ces repos projets pour accompagner les équipes devs. En revanche, l’ajout d’un niveau de sécurité supplémentaire sur des secrets implique de pouvoir tester, puis de merger nos modifications. Impossible de réaliser ces actions sans les équipes back et front responsables des projets, ou sans impacter leur travail. Notre périmètre d’intervention lors de cette journée à été considérablement limité.\n\n\nLa complexité du sujet et le constat de notre incapacité à avancer lors de cette journée ont rapidement conduit à un découragement des troupes. Nous sommes tout de même ressortis de cette édition avec des points positifs :\n\n  Une meilleure visibilité sur le périmètre de sécurisation à couvrir, en définissant le nombre de secrets concernés,\n  Un workflow visant à détecter à l’avenir tout nouveau secret concerné\n  … et la nécessité de mieux définir les guidelines pour le choix du sujet !\n\n\nEntendu pendant la 2nde édition du jeudi du fun 😅\n\n\n  👧🏻 : “Alors, qu’est-ce que tu fais de beau ?”\n\n  👦 : “Je souffre”\n\n\nCes guidelines nous ont aidé à définir le choix de la thématique de la 3ème édition du jeudi du fun. Le sujet devait répondre à ces critères :\n\n  Être réalisable en une journée,\n  Permettre de terminer / accélérer un projet ou d’éradiquer du legacy,\n  Être dans le périmètre dont l’infra est le code owner,\n  Et être “morcelable” en sous-périmètres, un pour chaque équipe.\n  Enfin, l’avancée du sujet doit être mesurable.\n\n\nPour l’édition de février 2023, nous avons donc “joué” avec la migration Ansible en cours de réalisation dans l’une de nos équipes de SysAdmins. 45 rôles Ansible restaient à migrer vers notre nouveau template Ansible, utilisé pour déployer nos machines on-prem : il y a du travail pour tout le monde, c’est parti !\n\nEt finalement, est-ce que ça marche ?\n\nAprès trois éditions, il nous semble nécessaire de prendre un peu de recul pour analyser si ces journées portent leur fruit. Les équipes sont ravies de travailler ensemble, certes, mais l’objectif principal est-il rempli ? Les jeudis du fun permettent-ils de venir à bout de sujets legacy ?\n\nLa première édition a fortement contribué à éradiquer du legacy : nous avons mis à jour la quasi-totalité des layers Terraform, nous avons mergé ou fermé l’entièreté des PRs, et nous avons initié des travaux de rework. Cependant, nous n’avions pas défini d’indicateurs de réussite assez fiables lors de cette première itération pour quantifier réellement le travail accompli. Si toute la Verticale partage le sentiment d’avoir avancé lors de cette journée, nous ne savons pas le mesurer finement.\n\n\nCapture d’écran du repo sysadmin/terraform au cours de la 1ère édition du “Jeudi du fun”\n\n\nPour pallier cette difficulté, nous avions défini un indicateur de suivi très simple pour la seconde édition du jeudi du fun : nombre de secrets à traiter / nombre de secrets traités. Ainsi, nous savons que, lors de cette (difficile) journée, nous avons traité environ un quart du périmètre.\n\nAu lancement de la 3ème édition du jeudi du fun, nous avions 45 rôles à migrer vers notre nouveau template Ansible. À l’issue de cette journée, l’équipe responsable du sujet n’en avait plus que 10 à traiter. La mutualisation de nos forces a porté ses fruits !\n\nInsuffisants lors de la première édition, les indicateurs de suivi mis en place dans les éditions suivantes sont cruciaux pour évaluer le ROI de ces journées de travail “particulières”.\n\nLes coulisses du jeudi du fun\n\nLes jeudis du fun sont organisés par deux personnes au sein de la verticale infra. Si les séances de préparation de cette journée (qui débutent environ 3 semaines avant la tenue de l’événement) sont source de beaucoup de rires, il n’empêche qu’elles doivent également répondre à certaines problématiques.\n\nS’adapter aux habitudes de travail de chacun\n\nEn premier lieu, nous devons organiser une journée à laquelle tous les membres de nos équipes puissent prendre part, qu’ils soient au bureau ou en télétravail. Tous les moments de la journée doivent tenir compte de cet élément, qu’il s’agisse des phases de travail en petits groupes, des sessions en plénière (25 personnes) comme le lancement de la journée, la remise des prix ou les différents jeux qui ponctuent ces jeudis.\n\nLes phases de travail en équipe sont les plus simples à gérer : nos équipes ont déjà l’habitude au quotidien de travailler avec des collègues à distance. Tout le monde se connecte sur une room de visioconférence, et le tour est joué.\n\n\nTeam mixte présentiel / distanciel lors du 1er “jeudi du fun”\n\n\nLes moments en plénière sont en revanche plus délicats à gérer, car le brouhaha d’une vingtaine de personnes rassemblées dans une même pièce reste difficilement audible pour les personnes à distance. Un prochain challenge pourrait être d’organiser un jeudi du fun 100% distanciel.\n\nIl est également nécessaire de tenir compte de la façon de travailler de chacun : si certaines personnes sont capables de travailler en faisant fi du bruit d’un open space, d’autres ont besoin de plus de calme. À chaque édition, nous tentons d’organiser le jeudi du fun sous différentes formes, pour tenir compte des besoins de chacun, mais nous n’avons pas encore trouvé la solution idéale.\n\nLors de la première édition, nous étions tous rassemblés dans le même open space, sans dispositif particulier pour les personnes ayant besoin d’un environnement silencieux, et cette journée leur a été difficile à supporter. De nombreuses autres équipes de Bedrock avec qui nous partageons d’habitude cet open space étaient en déplacement ce jour-là, ce qui a néanmoins permis de limiter nos nuisances sonores à notre seule verticale.\n\nPour la seconde édition, nous avions réservé un open space dans les locaux de Bedrock pour ne pas prendre le risque de déranger les autres équipes : l’ambiance y a été d’autant plus conviviale mais n’a apporté aucun mieux aux personnes ayant besoin de tranquillité pour travailler.\n\nLors de notre dernière édition, nous avons tenté une approche hybride : la plupart des équipes étaient rassemblées dans un même open space, et pour les personnes ayant besoin de s’isoler, une salle de réunion avait été réservée pour l’occasion. Il semble que cette organisation a apporté un mieux pour les personnes souffrant du bruit avec un écueil cependant : elles étaient isolées des autres équipes tout au long de la journée, et le jeudi du fun repose (aussi) sur l’émulation collective…\n\nLes autres limites de l’organisation\n\nAu fil des éditions, nous avons rencontré, en tant qu’organisatrices, deux autres limites.\n\nLa première touche au choix du sujet. Si la définition de la thématique de la première journée a été évidente car le repository sysadmin/terraform est source de complaintes quotidiennes, très vite, nous avons eu besoin d’aide pour définir les sujets des éditions suivantes.  \nEn effet, il est difficile pour nous d’appréhender un sujet dans sa globalité : y aura-t’il du travail pour chaque équipe ? Le sujet est-il accessible pour tous nos profils, sans montée en compétence préalable ? Quelles sont concrètement les actions à conduire pour venir à bout d’un sujet ? Pour pallier à ce problème, nous avons réalisé un tour de passe-passe : l’équipe qui remporte le jeudi du fun gagne le droit de définir avec nous le sujet de l’édition suivante. Et ça fonctionne ! Les gagnants participent avec plaisir au choix du prochain sujet de torture de fun !\n\nLa seconde limite concerne la récurrence de l’événement. Initialement, nous avions prévu d’organiser un jeudi du fun par mois, pour venir à bout rapidement de nos sujets legacy. Après les deux premières éditions (organisées en octobre et novembre 2022), nous nous sommes aperçues que nous perdrions le fun de cette journée si elle revenait trop fréquemment. Pour que cet événement reste une journée de travail particulière à laquelle les personnes participent avec plaisir, nous avons fait le choix d’opter pour un format trimestriel.\n\nNext steps et prochains défis\n\nD’autres améliorations restent à apporter, notamment autour de la gestion du reste à faire. Comment finir correctement les travaux initiés dans cette journée, afin de ne pas créer de nouvelles user stories legacy ? Ce point est tout aussi important que celui sur le travail accompli au cours de ces journées. Entamer un rework et le laisser en chantier génère au moins autant de frustration que le manque de temps pour traiter du legacy.\n\nNéanmoins, après trois éditions du jeudi du fun, il nous semblait important de partager notre expérience, ne serait-ce que pour convaincre des équipes de devs de Bedrock de venir jouer avec nous lors d’une prochaine édition !\n\n\nLes participants du Jeudi du fun\n\n\n\n\nPour vous donner un aperçu de comment se déroulent ces fameux jeudis, voici grosso modo le programme d’une journée :\n\n\n  \n    ⏰ 9h00 Petit déjeuner convivial (car c’est très important de commencer une telle journée en prenant des forces)\n  \n  \n    ⏰ 9h30 Début officiel de la journée : on se retrouve en plénière, dans une grande salle de réunion, avec tous les participants et on (ré)explique le contexte de la journée ainsi que le programme. \nOn commence avec un petit jeu (5 minutes maximum) qui sert à deviner le sujet du jour. Les sujets sont toujours gardés secrets jusqu’au lancement de la journée, ce qui donne lieu à toutes sortes d’hypothèses les jours qui précèdent (“Oui, oui, bien sûr on va recoder toute notre plateforme dans un autre langage jeudi”).On fait monter la pression !  \nL’objectif de ce premier jeu est d’énergiser un maximum nos collègues et de leur permettre de commencer à se projeter sur ce qu’ils vont pouvoir y faire. Le jeu change à chaque fois, pour garder un effet de surprise. \nEnsuite, vient le temps de révéler la constitution des équipes qui changent elles aussi à chaque édition afin de permettre à chaque personne de côtoyer de nouveaux collègues.\n  \n  \n    ⏰ 10h00 Les équipes partent travailler sur le sujet du jour, à leurs postes de travail\n  \n  \n    ⏰ 12h30 - 13h30 Déjeuner\n  \n\n\n\n\n\n  \n    ⏰ 13h30 Jeu de reprise (facultatif) : on se retrouve autour d’un blind test ou un gartic phone, histoire de passer un bon moment et de se remettre en jambe pour l’après-midi. C’est un court moment de team building qui est très apprécié la plupart du temps (sauf lorsque les équipes ne veulent pas perdre un minute pour venir à bout de leur objectif !)\n  \n  \n    ⏰ 14h00 Les équipes reprennent le travail initié le matin et essayent de finir un maximum de choses\n  \n  \n    ⏰ 17h30 On se retrouve en plénière pour le débrief de la journée : on fait le point sur le travail accompli, le décompte des points gagnés par chaque équipe et on fait le fameux podium ainsi que la remise des prix. \nOn récupère à chaud les premiers retours des participants.\n  \n  \n    ⏰ 18h00 Le verre de l’amitié\n  \n\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #19",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2023/03/13/bedrock-dev-facts-19.html",
    "date"     : "March 13, 2023",
    "excerpt"  : "La fin de l’hiver approche, il est temps de faire un bilan ! Quelles bêtises le froid aura-t-il apportées parmi les devs ? ❄️\n\nLa confiance 🤞\n\n\n\nMieux qu’un readme\n\n\n  Quand le mec t’explique la solution, et finit par :\n\n  “Enfin ça c’est si mon c...",
  "content"  : "La fin de l’hiver approche, il est temps de faire un bilan ! Quelles bêtises le froid aura-t-il apportées parmi les devs ? ❄️\n\nLa confiance 🤞\n\n\n\nMieux qu’un readme\n\n\n  Quand le mec t’explique la solution, et finit par :\n\n  “Enfin ça c’est si mon code a bien continué d’être copié collé partout”\n\n\nUne éthique, moi ?\n\n\n  Moi je peux mettre du code dégueulasse un peu partout, c’est pas un problème !\n\n\nLe Socrate des temps modernes\n\n\n  La vie est un Spike\n\n\nOn apprend de ses erreurs\n\n\n  Set up a reminder “@myself ne jamais dire ‘je finis aujourd’hui’” in this channel at 9h45AM every day.\n\n\nEn tout bien tout honneur ❤️\n\n\n  Ah bah go, mets-moi en dur si tu veux.\n\n\nGIT 101\n\n\n  J’en ai connu certains, à chaque fois qu’ils avaient un conflit sur leur branche, ils supprimaient le repo avant de le re-cloner\n\n\nTen seconds before disaster\n\n\n  Le cache, c’est nul !\n\n\nshoeHasHole: boolean 👟\n\n\n  A : ‘tin j’ai un trou dans ma chaussure\n\n  B : Tu es sûr que c’est pas un false ?\n\n\nOn a tous un env de test. Certains ont aussi un env de prod.\n\n\n\nPeur de rien, sauf d’une chose…\n\n\n  A : Ça finit en devs qui se reconvertissent Boulanger ça.\n\n  B : Certes, mais l’inverse est vrai aussi, il arrive que des Boulangers se reconvertissent après être devenus allergiques à la Farine.\n\n  A : C’est pour ça que je ne me reconvertirai pas en Barman… trop peur…\n\n\nLa confiance, second épisode 🤞\n\n\n  C’est pas n’importe quoi, juste un peu yolo !\n\n\nComme de l’eau de roche trouble !\n\n\n  Franchement, je trouve ça clair ! Mais je comprends pas..\n\n\nDe rares génies 💡\n\n\n  On est peut-être des lumières, mais ça ne veut pas dire qu’on est tous allumés !\n\n\nCassage de prod dans 3… 2… 1…\n\n\n  J’le sens bien là. J’le sens bien bien bien.\n\n\nIt’s not a bug, it’s a feature\n\n\n  J’ai vérifié, le bug marchait bien.\n\n\nToujours lire les petites lignes 🔎\n\n\n  Tout* marche du coup !\n\n  (*pour l’instant)\n\n\nMiaou 🐱📈\n\n\n  A : Je théorise que le chat ne miaule devant la porte que pour savoir s’il pourrait passer quand il aura envie.\n\n  B : Ouah ton chat il fait du monitoring de la porte !\n\n\nFacile comme tout !\n\n\n  TKT ! tu mets ton JSON dans le yaml et ça ira !\n\n\nUne grande histoire d’amour, épisode 1\n\n\n  Moi, j’adore le JSON\n\n\nUne grande histoire d’amour, épisode 2\n\n\n  Le mail et le DNS c’est ma grande passion\n\n\nPartir comme un roi 👑\n\n\n\nLes grandes questions de la vie 🥐\n\n\n  D’ailleurs c’est LinkedIn ou pain au linked ?\n\n\nL’humour pour les nuls\n\n\n  A : Pouffe de rire\n\n  B : Tout va bien ?\n\n  A : Désolé, je viens de relire ma vanne\n\n\nLes progrès de l’IA 🤖\n\n\n  A : Alors B, c’est quoi le format de date php de la constante de format ‘c’ ?\n\n  B : Tu m’as pris pour chatGPT ou quoi ?\n\n\nUn stagiaire en détresse\n\n\n  TLDR: À l’aide svp\n\n\nLa sécurité pour les nuls\n\n\n  Brian is in the Keychain.\n\n\nPromis dans le contexte c’est vrai\n\n\n  On doit afficher des ronds, alors c’est mieux s’ils nous envoient des carrés.\n\n\n😳\n\n\n  (Au pire, si on a la main sur une regexp, c’est déjà plus qu’il n’en faut pour me faire rêver)\n\n\nLa sélection naturelle\n\n\n  Je suis d’accord que là il y a un bug, mais c’est un bug parce que je suis con !\n\n\n🍵\n\n\n  Je suis en train de me rappeler de mon weekend, et spoiler mettre du rhum dans son thé ce n’est pas une bonne idée.\n\n\nError : Task completed successfully\n\n\n\nLa confiance, 3.0 🤞\n\n\n  Coucou, aujourd’hui, je pète la reco (en prod), mais c’est sous contrôle.\n\n\nUn instant de réalisme\n\n\n  Personnellement, je sais pas ce que je fous en développeur !\n\n\nThomas the train 🚆\n\n\n\nTurlututu chapeau pointu !\n\n\n  On aurait dû dire c’est “chapeau perché”.\n\n\nMieux qu’un rappel automatique 🤯\n\n\n  A : Du coup, tu as envoyé un mail ?\n\n  B : Pas encore non ! J’attendais d’y penser !\n\n\n"
} ,
  
  {
    "title"    : "Why is Transit Gateway service not right for us?",
    "category" : "",
    "tags"     : " on-premise, cloud, aws, network",
    "url"      : "/2023/03/02/aws_transit_gateway.html",
    "date"     : "March 2, 2023",
    "excerpt"  : "Managing the network of many interconnected AWS accounts can quickly lead to having a messy network architecture.\nTransit Gateway (TGW) service seems to be the way out of this. So how do you know if TGW is right for you?\n\nThis blog post will intro...",
  "content"  : "Managing the network of many interconnected AWS accounts can quickly lead to having a messy network architecture.\nTransit Gateway (TGW) service seems to be the way out of this. So how do you know if TGW is right for you?\n\nThis blog post will introduce how the service works and explain why we chose not to carry on with our migration to AWS Transit Gateway.\n\nTransit Gateway’s backstory\n\nTransit Gateway is a network transit hub that connects multiple VPCs and On-Premises sites to allows control traffic between them.\nIt was created to provide a new approach of network implementation on AWS and to make network administration smoother.\n\nVPC peering is a point-to-point connection between 2 VPCs.\nIt is a great example of complex network management because it adds a new topology to the network architecture.\nOn this diagram you can see an example of VPC peering usage. It’s not that messy yet but at scale it will be.\n\n\n\nBy acting as a “cloud router”, TGW centralizes network connections and takes control of packet forwarding between VPCs.\n\n\n\nVPC peerings are not required anymore, we go back to a simpler star network topology thanks to Transit Gateway which really does address the complexity and restrictions of VPC peerings.\nAt that point, TGW seems to be the perfect answer for a simpler network architecture.\n\nWhat about TGW at Bedrock?\n\nWe operate more than 20 different AWS accounts for our customers’ platforms. Each account has a VPC with at least 3 private and 3 public subnets. We also manage AWS accounts for internal tools like ECR repositories, monitoring tools and shared s3 buckets. We configured Site-to-Site VPNs from On-Premises infrastructure to all the VPCs in these accounts.\n\nFrom the creation of new AWS accounts to deploying the tenants’ platform, onboarding a new customer requires a lot of work and time.\n\nConfiguring VPCs Site-to-Site VPN is one of the steps that requires a lot of work. This is why we were interested in Transit Gateway at first.\n\nProof of concept\n\nWe created a production like Proof of Concept infrastructure using three AWS accounts, two different regions, multiple VPCs and a single Site-to-Site VPN from TGW to On-Premises firewall.\n\nHow did we test TGW?\n\nWe started by trying to split routing domains.\nCentralizing network connections also means (with correct ACLs or Security Groups) that VPCs can reach all other VPCs. We want to control that.\nTransit Gateway attachments read their routes in the TGW route table they are associated to. This is how we manage routing domains.\nWe create a Transit Gateway routing table and create routes for target networks.\nTGW attachments are able to propagate routes in a route table if we want to. But because of routing domains, we can’t use that option and we have to add routes manually (attachments only read routes in the route table).\n\nThen we tested Transit Gateway peering.\nTGW is a regional service, this means that we need to have a TGW for each active AWS region. We use TGW peering to interconnect them.\nWe expected to have some way to propagate routes dynamically in the Transit Gateway peering route table. But it is not possible.\n\nThe last thing we tested is migrating from VPC Site-to-Site VPN to TGW VPN.\nBecause of the amount of VPC Site-to-Site VPN we have, it was important for us to know if we could get a minimal down time on On-Premises to VPC connections when migrating to the Transit Gateway VPN.\nThis process requires a lot of time because routes have to be deleted and created manually on each side.\n\nEven if we noticed some pain points, tests went well. So we decided to initiate the migration to the Transit Gateway service.\n\nWhy did we choose to rollback?\n\nEverything was okay at first, we successfully migrated two VPC Site-to-Site VPN to our Transit Gateway VPN.\n\nBut then previous pain points became barriers:\n\n  creating and managing routing domains is possible, but makes it impossible to use dynamic route propagation\n  there is not option to propagate routes in VPC route table, they all have to be created manually\n  data transfer cost is too high (and multiplied by the number of region on which you deployed TGW if your packets go through all these regions)\n  migrating to Transit Gateway requires a planned maintenance because there is a network downtime\n\n\nWe took some time to talk about what to do next and concluded that migrating to Transit Gateway will just move the complexity of configuring VPC Site-to-Site VPNs to configuring TGW attachments and routes.\n\nAWS support did not suggest enough solutions to the problems we faced, so we decided to rollback to VPC Site-to-Site VPNs.\n"
} ,
  
  {
    "title"    : "Projet XState",
    "category" : "",
    "tags"     : " xstate, lyonjs, meetup, react, javascript, conference",
    "url"      : "/2023/02/08/projet-xstate.html",
    "date"     : "February 8, 2023",
    "excerpt"  : "Dans une application frontend moderne, la gestion d’état est un élément central de son bon fonctionnement. Malgré les nombreuses librairies disponibles (Redux, MobX, Recoil…), cette tache reste complexe à réaliser et il est facile de perdre le con...",
  "content"  : "Dans une application frontend moderne, la gestion d’état est un élément central de son bon fonctionnement. Malgré les nombreuses librairies disponibles (Redux, MobX, Recoil…), cette tache reste complexe à réaliser et il est facile de perdre le contrôle.\n\nDans l’objectif de rester maitre de son application, je vous propose de découvrir XState, une librairie reposant sur le concept de machine à états. Si l’outil ne fait pas tout, le concept de machine à état aide grandement à concevoir une application résiliente.\n\nPour présenter au mieux les concepts, la théorie sera suivie de pratique au travers d’un live coding.\n"
} ,
  
  {
    "title"    : "The time I tried to build a Second Brain #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/the-time-i-tried-to-build-a-second-brain",
    "date"     : "January 27, 2023",
    "excerpt"  : "The time I tried to build a Second Brain\nPrésenté par Sylvain ZOCCARATO.\n",
  "content"  : "The time I tried to build a Second Brain\nPrésenté par Sylvain ZOCCARATO.\n"
} ,
  
  {
    "title"    : "Projet XState #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/projet-xstate",
    "date"     : "January 27, 2023",
    "excerpt"  : "Projet XState.\nPrésenté par Maxime BLANC.\n",
  "content"  : "Projet XState.\nPrésenté par Maxime BLANC.\n"
} ,
  
  {
    "title"    : "Qu’est-ce que l’oignon dans le Web ? #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/loignon-dans-le-web",
    "date"     : "January 27, 2023",
    "excerpt"  : "Qu’est-ce que l’oignon dans le Web ?\nPrésenté par Etienne Doyon.\n",
  "content"  : "Qu’est-ce que l’oignon dans le Web ?\nPrésenté par Etienne Doyon.\n"
} ,
  
  {
    "title"    : "La philo et les livres : mes compagnons de route pour les défis sportifs #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-philo-et-les-livres",
    "date"     : "January 27, 2023",
    "excerpt"  : "La philo et les livres : mes compagnons de route pour les défis sportifs\nPrésenté par Chiara PETTINELLI.\n",
  "content"  : "La philo et les livres : mes compagnons de route pour les défis sportifs\nPrésenté par Chiara PETTINELLI.\n"
} ,
  
  {
    "title"    : "La culture Hip-Hop #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-culture-hip-hop",
    "date"     : "January 27, 2023",
    "excerpt"  : "La culture Hip-Hop\nPrésenté par Pascal HALTER.\n",
  "content"  : "La culture Hip-Hop\nPrésenté par Pascal HALTER.\n"
} ,
  
  {
    "title"    : "La photographie de paysages nocturnes #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/photographie-de-paysages-nocturnes",
    "date"     : "January 27, 2023",
    "excerpt"  : "La photographie de paysages nocturnes\nPrésenté par Camille NIEL.\n",
  "content"  : "La photographie de paysages nocturnes\nPrésenté par Camille NIEL.\n"
} ,
  
  {
    "title"    : "Le festival de cannes de sa naissance à aujourd’hui #LFT 27/01/23",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/festival-de-cannes-de-sa-naissance-a-aujourdhui",
    "date"     : "January 27, 2023",
    "excerpt"  : "Le festival de cannes de sa naissance à aujourd’hui\nPrésenté par Maxime LE MOAL.\n",
  "content"  : "Le festival de cannes de sa naissance à aujourd’hui\nPrésenté par Maxime LE MOAL.\n"
} ,
  
  {
    "title"    : "A journey into connected TVs industrialisation process, Part 1",
    "category" : "",
    "tags"     : " ",
    "url"      : "/2023/01/10/bedrock-app-launcher.html",
    "date"     : "January 10, 2023",
    "excerpt"  : "At Bedrock, we build and run streaming applications on a wide variety of OTT devices (more than 60 different ecosystems). While testing and experimenting is easy on web and mobile devices, even for non-developers, it’s not as easy for Connected TV...",
  "content"  : "At Bedrock, we build and run streaming applications on a wide variety of OTT devices (more than 60 different ecosystems). While testing and experimenting is easy on web and mobile devices, even for non-developers, it’s not as easy for Connected TV (CTV). In this article, you’ll discover how all of our employees can now access testing and pre-release environments on TV devices, with ease and without any technical knowledge.\n\nBedrock TvJS Project\n\nHow does it work ?\n\nTo address the growing number of CTVs vendors in the market, we have a one-and-only monorepo project named “TVJS”. It is a React application which we can deploy almost everywhere almost anywhere with the same code, UI and UX. The magic part? There isn’t much manufacturer-specific code in that application, most of those particularities are handled by our homemade JS library named PELO (Platform Easy Life Officer). For non-French readers, “pélo” is a Lyon/Grenoble city slang to designate “someone”.\n\n\n\nIn a few words, PELO is a set of libraries showing a unified front API for TV developers, so they don’t have to keep in mind every TV specific details and custom APIs (like lifecycle, keyboard, storage handling, and more). PELO also provides several CLI tools allowing the use of proprietary manufacturer SDKs, with a common shared API.\n\nThere are at least two ways to deploy a TV application:\n\n  A fully packaged solution, where all application files and resources are stored on the TV. Everytime you want to update it, application you have to go through the manufacturer QA process. Doing so, you can develop either a web application that will run through the TV’s Web Engine, or a native TV application.\n  The hosted solution, where the TV packaged application only redirects to a web application that you are responsible for. It grants much more flexibility, and delivery speed, as deployment and propagation of a new version are almost instantaneous.\n\n\nWe chose the second way as we are addressing a big number of devices and need all the flexibility we can have for deployments – and, sadly, for rollbacks too. Therefore, we host and deploy our CTV applications like any other website and we control the TV Browser Engine to navigate to specific domain names.\n\nThree teams are working on this project, on the same repository:\n\n  a team dedicated to Core features (like catalog, user lifecycle)\n  a team dedicated to Player features (video playback and advertising)\n  and a team supporting legacy devices\n\n\nDeveloper teams are supported by a QA team. It is responsible for functional quality assurance on Pull Requests and pre-releases. Quality assurance designates any processes to ensure a service meets its quality requirements in terms of experience, stability, …\n\nDevelop &amp;amp; release process\n\nWe do our maximum to ensure the best quality of service and experience of what we deliver to our customers and their end-users. We have a strong culture of automated testing &amp;amp; tech reviewing which allows us to deliver almost without a sweat… Still, at our scale, missing a bug means a bad experience for thousands or millions of people! And that’s something we won’t accept without a fight!\n\n\n  One of Bedrock’s Values is: ROCK-SOLID, ALWAYS\n\n\nAs a consequence, we also have dedicated QA teams testing our work for a subset of device models and versions, before it is being merged to the main codebase, and before going to production as part of a release. They are doing so by connecting TVs to specific environments that are deployed on-demand: previews and staging.\n\nLet’s show off a little bit: at the beginning of 2022, thanks to the TVJS project, we were able to deploy production code to 7 manufacturers, and 38 device versions, meaning 266 combinations to check before launching a release into production! And these numbers are ever increasing!\n\nMy wish: make testing environments easily accessible\n\nWe love showing-off a bit over the applications we deliver on such a huge number of device models, but that doesn’t go with ease nor without pain.\n\nTesting a specific environment on a device was not possible for non-project members (other teams, support, business &amp;amp; product teams, managers …). Starting a preview or a staging application requires a deep understanding of the project, proprietary SDKs (even with our PELO CLI), shell, Git commands and advanced knowledge of how devices work in Developer Mode. This was a major issue: it causes interruptions for developers, slows delivery down, reduces our Time To Market.\n\nQA teams assigned to the project know its basics, they can use PELO CLI and proprietary SDKs, but cannot debug issues they may encounter with such tools: they have to ask developers to take actions for them (as this is not their core job). Using those tools is also time-consuming and time is of the essence when running quality checks while preparing a release.\n\nMany teams also want to start environments by themselves, to test their own developments on back-end services, to investigate when a customer creates a support ticket, …\nThe most important of them are Video teams, responsible for video encoding, transcoding and packaging: they are constantly testing new streams and features, and need a way to test their content by themselves, without asking around for a TVJS developer.\n\nOur answer: The Launcher App !\n\nWhat does it do ?\n\nI’ve developed a TV application to quickly and efficiently start a specific environment. Using the TV remote, people can select the wanted environment and be redirected to it instantly, having the app like they would with the specific app installed.\n\nTyping long texts is painful for TV users. So, when selecting the preview environment, it shows another set of options where users can input a specific PR number. A background process will ask our Github if it knows the PR number, if it is deployed on the selected customer/manufacturer and will pre-fill the branch name. If not specified, it will default back to our master preview that is updated whenever we merge code to the master branch.\n\n\n\nTechnical Architecture\n\nThe launcher is part of the TVJS monorepo, developed using React and re-using modules and packages for UI and Navigation allowing it to have minimum maintenance cost.\n\nFor the first iterations of development of the launcher, I hosted it on AWS Amplify, but the Core team quickly integrated it back to a regular production deployment process we have at Bedrock.\n\nAn automatic process builds the javascript bundle and assets and sends everything to AWS S3. The launcher will then be served through Fastly CDN. We build and deploy a unique launcher per compatible manufacturer on their own domain names (as-of-writing, Samsung Tizen, LG webOS and Hisense). For security reasons, those Fastly services are only accessible from our office networks.\n\n\n\nUnreliability of launcher app installation\n\nI’m proud of this launcher and it is already saving loads of time for our QA teams ! They love it, as it helps them focus on their primary role: ensuring service &amp;amp; experience quality. Still, installing the launcher application on every device in our office is a huge amount of work! And, unfortunately, not a persistent one.\n\nTo develop and test apps on live devices, we need to set them in “Developer Mode”. And each manufacturer has its own way, more or less time-consuming. Worse, whenever Developer Mode expires, all applications installed during this time are uninstalled from the device! Which means we have to install the launcher again after a brief period of time.\n\n  Tizen Developer Mode\n  LG webOS Developer Mode\n\n\nThat period of time varies. For Samsung Tizen, we’re not absolutely sure, but it’s almost a month. For LG webOS, it is 50 hours if you don’t extend the Developer Mode or if you connect another TV with the same Developer Account.\n\nSpecifically for LG, I did set up a CRON that automatically extends the Developer Mode, but sometimes it is being disconnected without reason… Or a mishandling by team members can cause the CRON to fail.\n\n\n\n\n\nTherefore, we aren’t 100% sure the launcher application will be up and ready on all the office devices when work begins in the morning, which means developers will have to manually re-install the launcher when asked by another Bedrock employee. It generates frustration for both QA and developers as they are wasting precious time to re-install the launcher.\n\nDon’t worry though, I already have a couple of ideas to ensure the installation becomes reliable! I’ll talk more about these it in a future article.\n\nConclusion\n\nAny Bedrock employee can now start an office CTV, use the launcher app, select customer and environment, hit Let’s Go and access the environment they need to work!\n\nWhat we started to measure, and hopefully we’ll have more refined metrics over the next months, is the time QA teams are gaining per day. They needed an average of 15 minutes to start up a TV, set up the Developer Mode, and install the wanted app through CLI. They are validating 5 PRs per day, on 2 different devices at minimum, they almost gain one hour per day. That means our Time To Market is faster, and our QA teams have more time to do exploratory testing as well as refining their tests and writing more automated tests. Something that is not as measurable as time, is the enhanced peace of mind for them to go to work every morning knowing they have a tool designed for them to focus on their core work.\n\nThis has improved the QA team overall velocity! And it makes the whole project more accessible for any employee. However, there is still room for improvement regarding launcher deployment and stability over time, and this is something I will cover in our next article.\n\nI hope you liked this article and it helped you if you’re trying to achieve something similar!\n"
} ,
  
  {
    "title"    : "How Micro-Services changed our caching architecture",
    "category" : "",
    "tags"     : " on-premise, cloud, cdn, varnish, aws, cloud, fastly, varnish-operator, cloudfront, alb",
    "url"      : "/2022/12/23/varnish-operator.html",
    "date"     : "December 23, 2022",
    "excerpt"  : "At Bedrock we use Cloudfront or Fastly for two different reason. To protect our applications from potential Distributed Denial of Service Attack. And to provide a layer of cache in front of our applications. No need to go down to the app for an ea...",
  "content"  : "At Bedrock we use Cloudfront or Fastly for two different reason. To protect our applications from potential Distributed Denial of Service Attack. And to provide a layer of cache in front of our applications. No need to go down to the app for an easily cacheable response.\n\n\n\n\n\nAt least that is what we thought in 2018 when we were migrating from on premise to the Cloud.\n\nAt that time we had a Varnish instance caching everything at the border  of our on premise infrastructure. All the applications were running either on virtual machines or on bare metal servers. Those applications were mostly called by the end-user’s browser. Whenever an application called another application it did it through Varnish.\n\nThis is ideal if applications are mostly called from the outside world. The Varnish instance caches all cacheable content, and it does not cost too much time as it was in the same Data Center.\n\n\n\n\nIn 2023, we think otherwise. We have now a KOps managed Kubernetes cluster running on EC2 spot instances in private subnets at AWS. As we migrated to the cloud we also embarked on the journey of splitting monolith into smaller more manageable microservices.\n\nWith less monoliths the Bedrock product is more resilient and easier to scale but it changes the topologies of network calls. Before there were far more calls coming from the internet from end-users browsers. Now with the new architecture coming into place inter-app requests have increased.\n\nOne solution would be to directly call the ingress of the applications, staying inside the cluster but without the benefit of caching as it is handled by the CDN. This would lead to unsustainable increase in CPU usage, and probably very little gain in terms of response time.\n\nA better solution for us would be to have the caching of CDN inside the cluster. This would enable us to have fast response time and little to no increase in CPU usage.\n\nEnter Varnish-Operator\n\nWe tested the project IBM/Varnish-Operator. This project allows us to create Custom Resources for Kubernetes handled by the Varnish-Operator. This object is called a VarnishCluster, the configuration is pretty simple to get started. This enables us to have a caching layer, between the Ingress-Controller and the Application.\n\n\n\n\n\nVarnishCluster also uses Varnish Configuration Language (VCL) which we are pretty familiar with since we use Varnish On-Premise since 2015, and developers use it regularly to configure Fastly distribution.\n\nBy adding cache using VarnishCluster to an application that is not fully cacheable, we almost divided it’s average response time by two. It is not a surprise as inter api calls used to look like the following graph:\n\n\n\n\nWe changed parameters in the application after adding VarnishCluster so that it calls other app inside the cluster like in the following graph:\n\n\n\n\nA few details\n\nBefore I wrap this up, here are a few details about the implementations.\n\nAs you will be able to read in the Varnish documentation:\n\n  “By default Varnish will use 100 megabytes of malloc(3) storage for caching objects, if you want to cache more than that, you should look at the -s argument.”\n\n\nSo if you give many Gigs of memory to your Varnish container it won’t be attributed to the Varnish process. You can set it with the argument -s storage=malloc,&amp;lt;Number&amp;gt;.\n\nAs we use only Spot nodes that can be terminated by AWS at any moment with only 2 minutes notice, we want to give more resilience to our Varnish Clusters pod as cache is stored in RAM memory.\nYou lose all your cache at each restart of the Varnish Container.\n\nWe configured podAntiAffinity between application pods and VarnishClusters’ to avoid scheduling those pods on the same node and be vulnerable to reclaims.\n\nWe added a podDisruptionBudget to avoid losing all our pods at the same time. We also customized the VCL a bit to make Varnish serve stale content in case our application is unreachable.\n\nWe also added a Prometheus Service Monitor to make sure all Varnish metrics would be scraped by Victoria Metrics.\n\nIn the Future\n\nIn next versions we would like to add the possibility to configure PriorityClass of VarnishClusters pod. PriorityClasses are used to order workloads priority.\nIn a context of scaling and of scarcity of resources, the scheduler will evict pods of lower priority to make room for the pod it is trying to schedule.\n\nFor now our VarnishCluster’s pods have the PriorityClass by default but it is more critical than any other applications as it holds a cache in its memory.\n\nAlso we do not have logs of Varnish. We would like to be able to stream VarnishLog content into Loki. This would be super useful to debug and to investigate if we ever encounter bugs or unexpected behaviors.\n\nConclusion\n\nAverage response time going down, red bar is when we pushed it in production\n\n\n\nWith the generalization of microservices, Bedrock needed to rethink its architecture to optimize not only for browser to API calls but also for more API to API usage. By adding VarnishCluster in front of our applications and calling them directly from inside the cluster we improved significantly the Bedrock product.\n\nThe Github project is still young and lacks important features, we hope with this article to help draw attention to this project and potential contributors.\n\n\n\n"
} ,
  
  {
    "title"    : "Nos retours sur l&#39;HAProxyConf Paris 2022",
    "category" : "",
    "tags"     : " haproxy, haproxyconf, conference",
    "url"      : "/2022/12/23/haproxyconf-paris-2022.html",
    "date"     : "December 23, 2022",
    "excerpt"  : "Bedrock était présent lors de la Conférence HAProxy qui se déroulait à Paris en novembre 2022 : en tant que speaker, avec la présentation de Vincent Gallissot, mais aussi en tant que spectateur. Cet article relate les points forts qui nous ont mar...",
  "content"  : "Bedrock était présent lors de la Conférence HAProxy qui se déroulait à Paris en novembre 2022 : en tant que speaker, avec la présentation de Vincent Gallissot, mais aussi en tant que spectateur. Cet article relate les points forts qui nous ont marqués.\n\nLa présentation de Vincent Gallissot, Lead Cloud Architect chez Bedrock, mettait en valeur l’usage d’HAProxy en tant que brique essentielle de notre infrastructure. Chez Bedrock, nous développons et maintenons une plateforme de streaming qui a été migrée dans le Cloud en 2019. Cette présentation était grandement inspirée de l’article intitulé “Scaling Bedrock video delivery to 50 million users”, dans lequel vous trouverez pléthore d’informations concernant nos utilisations d’HAProxy.\n\n\n\nSommaire\n\n\n  Ce que des millions de requêtes par seconde signifient en termes de coût et d’économie d’énergie\n  Un outil pour les gouverner tous\n  Vous reprendrez bien un peu de pétaoctets?\n\n\nCe que des millions de requêtes par seconde signifient en termes de coût et d’économie d’énergie.\n\nLa keynote d’ouverture avait pour orateur Willy Tarreau, le Lead Developer d’HAProxy.\nAu travers d’une démonstration concrète mélangeant software et hardware, l’objectif était de :\n\n  transmettre l’idée qu’ajouter une brique logicielle dans un système ne le dégrade pas pour autant, bien au contraire\n  sensibiliser l’audience quant à la consommation d’énergie de nos systèmes\n\n\nContexte technique et premières améliorations\n\nPour ce premier cas d’étude, Willy Tarreau nous présente le cas d’un service de vente en ligne.\n\nLa stack technique est composée de PHP / pgSQL (NodeJS + Symfony) et les images sont stockées en base de données. C’est cette architecture qui sera mise à l’épreuve lors des tests de charge à venir.\n\nDans un premier temps, plusieurs améliorations (sans HAProxy) sont proposées. Il peut s’agir d’un simple rappel, voir d’un pro-tip d’architecture pour les plus novices : Les images en base de données, c’est une mauvaise idée.\n\nEn les déplaçant vers un CDN, le système peut rapidement et simplement doubler ses performances, la base de données étant un goulot d’étranglement. La taille des pages peut être optimisée via l’activation de l’option http “gzip”. Les informations de sessions sont elles aussi enregistrées en base de données. Afin d’améliorer les performances, il est possible d’ajouter du caching via des outils tels que Memcache.\n\nSuite à cela, une première amélioration d’architecture serait d’ajouter un NLB (Network Load Balancer) en amont du système qui distribuerait les requêtes entrantes vers plusieurs unités de calculs.\n\n\n\nSchéma d’architecture, première version\n\nDans le cas présent, les requêtes entrantes sont distribuées de façon aléatoire entre les différentes unités de traitement. Chacun de ces backends se connectant à la même et unique base de données.\nLe benchmark ci-dessous (efficacité, au sens nombre de requêtes traitées en fonction du nombre d’unités de calcul), ne montre pas une croissance linéaire. Il s’agit d’une courbe tendant vers une pente nulle (voir négative pour les plus grosses architectures).\n\n\n\nGraphique représentant l’efficacité du système en fonction du nombre de backends\n\nComment expliquer que cette architecture ne scale pas linéairement ?\n\nMalgré les améliorations apportées pour les sessions grâce au cache, il subsiste encore un problème.\n\nLe NLB est un composant qui ne fait que répartir la charge sans tenir compte de l’historique des requêtes. En effet, celui-ci va distribuer la charge d’entrée aléatoirement vers les backends.\nChaque backend reçoit des requêtes provenant de n’importe quel utilisateur impliquant alors un cache-miss très élevé : l’utilisateur est rarement trouvé dans le cache, ce qui génère une requête supplémentaire en base de données et dégrade les performances en plus de consommer inutilement des ressources.\n\nEt si nous ajoutons HAProxy à notre système ?\n\nC’est ici qu’entre en jeu HAProxy en remplaçant le NLB. Pour cela, pas besoin d’un foudre de guerre en termes de ressources.\n\nLes tests ont été effectués sur une machine ARM Breadbee cadencée à 1 GHz et possédant 64 Mo de RAM. Nous verrons également par la suite qu’on pourrait même se passer d’une machine supplémentaire.\n\nLe but d’HAProxy est de spécialiser les caches des backends et plus globalement de forcer les sessions utilisateurs vers les mêmes backends.\n\nPour cela, HAProxy effectue une inspection de la couche 7 du trafic et renvoie toutes les requêtes d’un même utilisateur sur une même machine en réduisant ainsi les cache-miss aux seuls cas des nouveaux clients se connectant à la plateforme. Ainsi, le nombre d’appels à la base de données pour récupérer les informations de session est drastiquement réduit, la majorité d’entre elles étant stockées en cache.\n\nAutre fonctionnalité de taille : HAProxy limite le nombre de requêtes faites en parallèle sur un même backend, ce qui limite les locks de processus et les temps d’attente. Ceci a pour conséquence directe de réduire la consommation CPU.\n\nCes deux améliorations permettent à l’application de scaler de façon beaucoup plus linéaire, tout en réduisant les consommations CPU et énergétiques inutiles. Globalement, les performances initiales sont largement dépassées avec deux fois moins de backends.\n\nA partir de quand est-il intéressant de franchir le pas ?\n\nMaintenant que les bénéfices d’HAProxy ont été présentés, la prochaine étape est de se demander : quand est-ce qu’on se lance ? La question est considérée en termes de performance, mais aussi sous un angle pécunier.\nSi HAProxy peut être intégré sans augmenter les coûts du système, c’est encore mieux.\n\nAjouter HAProxy dans un système composé d’un seul backend n’apporte pas de bénéfice : il n’y a pas de load-balancing possible. Avec deux backends, si on divise le besoin de processing par deux, nous n’avons plus qu’un seul backend et donc pas de load-balancing possible.\nC’est en fait à partir de 4 backends que l’ajout d’un HAProxy en entrée devient intéressant :\n\n  en retirant 2 serveurs de nos backends en conservant une puissance équivalente (cf les tests ci-dessus)\n  et en recyclant un des deux backends retirés en hôte pour HAProxy\nEn fin de compte, pour une même puissance de traitement, un backend est retiré ce qui permet de réduire les coûts de fonctionnement. Ce principe s’applique également sur un grand nombre de backends.\n\n\nC’est là que prend tout son sens l’expression qui avait été utilisée pour conclure cette keynote : “HAProxy is a free software running on free hardware”.\n\nChez Bedrock, nous appliquons aussi ces différentes techniques de Consistent Hashing en entrée de notre CDN vidéo. Nos caches vidéos sont spécialisés et chaque utilisateur est redirigé vers un unique backend lors de la lecture d’une vidéo.\nPour en savoir plus, vous pouvez consulter notre article au sujet du Consistent Hashing.\n\nUn outil pour les gouverner tous\n\nDans notre activité en informatique, nous sommes amenés à délivrer de plus en plus rapidement des applications, des mises à jour, etc… Nous avons donc adopté la philosophie DevOps et tout un panel d’outils autour de celle-ci afin de sécuriser, monitorer et automatiser chaque étape de nos pipelines de livraison.\n\nLe cas de figure du load balancing est intéressant dans ce type d’organisation, il est essentiel d’exposer de nouvelles applications sur les environnements de production mais étant donné que la maîtrise de cet outil requiert une compréhension du réseau, la responsabilité incombe souvent à l’équipe Ops de le gérer.\n\nVous souhaitez mieux gérer votre flotte HAProxy ?\n\nAnjelko Iharos, directeur de l’ingénierie à HAProxy Technologies nous a présenté leur nouvel outil d’automatisation : HAProxy Fusion Control Plane, packagé dans la version entreprise de HAProxy.\n\nCelui-ci va amener une nouvelle interface enrichie afin de gérer toutes les instances HAProxy et les outils gravitant autour de ces dernières.\n\nOn peut citer :\n\n  La possibilité pour les développeurs de router eux-même leurs applications sans avoir besoin d’un Ops dans leurs pipelines de CI via l’API Fusion.\n  Gérer les WAF de HAProxy de manière centralisée et répercuter cette configuration sur un ensemble de clusters/instances.\n  Permettre aux Ops de gérer la structure de leurs load balancers, ajouter de nouvelles instances, gérer les certificats SSL, le tuning des performances depuis un seul point d’entrée.\n\n\nEst-ce résilient ?\n\nFusion Control Plane est livré avec tout un set de features intéressantes pour assurer sa maintenabilité et sa résilience :\n\n  Une pleine observabilité avec une application unifiée de récupération de logs, métriques et rapports dans la même interface. L’export de ces data est possible, notamment pour les transposer dans un dashboard tiers (Grafana, par exemple).\n  Un système de RBAC permettant de mieux gérer les périmètres de chacune des équipes dans le control plane.\n  La gestion centralisée de la configuration, la validation des configurations et le bot management. La partie WAF est packagée avec OWASP (communauté publiant des recommandations pour la sécurisation des applications web) ModSecurity Core Rule Set (CRS) pour la détection des vulnérabilités. Dans le cadre d’un cluster un système de failover automatique avec auto-élection du leader (à la manière de GOSSIP avec Consul).\n\n\nUne vue de l’avenir ?\n\nAujourd’hui, Fusion Control Plane limite son scope à HAProxy Entreprise et Community Edition, les IngressController ne sont pour le moment pas encore supportés.\n\nIl n’est pas encore pleinement compatible avec les features offertes par AWS (Gestion des ASG et de Route53) mais c’est en cours de développement chez HAProxy Technologies.\n\nLe produit semble prometteur et intéressant. Les possibilités qu’il nous offre pour laisser la main aux développeurs sur la mise en place de routes vers leurs applications côté on-premise est vraiment un gros plus, mais il nous manque pour le moment le support de l’IngressController HAProxy utilisé sur nos cluster Kubernetes, ce qui nous empêche d’en profiter au maximum.\n\nVous reprendrez bien un peu de pétaoctets ?\n\nChez Bedrock, un élément central de notre métier est de fournir de la vidéo à nos utilisateurs. (Incroyable pour une boite qui fait de la VOD hein? 😀).\n\nPour ce faire nous avons nos propres serveurs CDN hébergés sur Paris, en complément des CDN publics comme Cloudfront ou Fastly. Cette année nous avons servis plusieurs centaines de PB de données via nos serveurs et nous espérons pouvoir au moins doubler ce trafic l’année prochaine !\n\nNotre architecture CDN est constituée d’un logiciel appelé LBCDN qui “load-balance” la charge sur les CDN, on-prem et publics, en redirigeant un utilisateur vers un serveur CDN spécifique.\nNos serveurs en eux-mêmes sont basés sur Nginx avec une configuration assez simple en direct IO sur de gros SSD.\n\nLa HAproxy conf 2022 nous a pas mal inspirés pour répondre à nos problématiques avec ces deux conférences :\n\n  Boost your web apps with HAProxy and Varnish, by Jérémy Lecour CTO of Evolix:Video\n  Was That really HAProxy, by Ricardo Nabinger Sanchez performance engineer at Taghos: Video\n\n\nCes deux présentations font état d’une architecture sur les CDN intéressante où HAProxy est utilisé pour mettre “en sandwich” l’outil (ou les outils) faisant fonction de CDN.\nL’architecture présentée semble permettre une configuration bien plus fine que ce que nous avons actuellement avec seulement Nginx.\n\nPar exemple, sur nos CDN on-prem nous devons aujourd’hui utiliser une astuce pour que Nginx puisse dynamiquement aller résoudre le nom de domaine du backend sur lequel il source ses fichiers. Cela est déjà un peu dommage de ne pas avoir de mécanisme disponible nativement. De plus, ce mécanisme est difficile à coupler avec d’autres permettant d’avoir du fail-over par exemple.\n\nC’est ici qu’HAProxy pourrait intervenir pour résoudre notre problématique car il nous permet d’avoir du fail over et des tests plus fins sur l’état de santé des backends.\n\nDe plus, nous sommes en train de tester une solution de second-tier de CDN qui, du fait de la complexité ajoutée à notre architecture de CDN, profiterait beaucoup d’une plus grande finesse de configuration.\n\n“Mais attends, tu n’as parlé que de HAProxy en backend là, tu triches un peu non? C’est pas un sandwich c’est une tartine de HAProxy là!”\nTout à fait, notre cas d’usage actuel n’a pas forcément besoin d’un HAProxy en frontal de Nginx.\n\nMAIS!\n\nC’est là que les conférences sont intéressantes car elles montrent que l’on peut mixer les backends.\nDans la conférence présentée par Ricardo, l’utilisation de deux backends (Varnish et hyper-cache) sur un même serveur est permise par un HAProxy. Cela permet de profiter de la complémentarité de ces services.\nDans notre cas, nous n’avons pas besoin de cela mais une autre conférence nous a mis la puce à l’oreille : Writing HAProxy Filters in Rust, by Aleksandr Orlenko.\nCela pourrait nous permettre, avec un HAProxy en frontal, d’agréger plus finement les mesures de performances du serveur afin d’optimiser l’usage de ses ressources, ou déporter une partie du trafic sur un serveur moins chargé, ou encore de récupérer une partie des traitements actuellement effectués par le LBCDN.\n\nAjouter cette fonctionnalité serait la belle cerise au kirsch au sommet de ce sandwich de HAProxy.\n\n\n\n“Il est bizarre ton sandwich”\n\n“Bon d’accord, c’est plutôt un gâteau à étages.”\n\n“Ok c’est mieux, mais je préfère les macarons de la HAProxy Conf 2022 quand même.”\n\nA une prochaine fois !\n\nLa HAProxyConf, c’était deux jours de conférences avec des orateurs venus de tous les coins du globe.\nUne belle occasion pour nous d’en apprendre plus sur un outil que nous utilisons quotidiennement chez Bedrock.\nDans cet article, nous n’avons pas pu faire mention de tout ce qui nous a intéressé. Nous pourrions notamment citer les très intéressantes conférences au sujet de :\n\n  Docker et leur utilisation de l’outil Keda\n  Ou encore de SoundCloud et leurs mesures anti-DDOS\n\n\nCette conférence était aussi l’occasion d’échanger avec l’équipe HAProxy autour de sujets techniques qui nous concernent, de voir que nous utilisions déjà certaines bonnes pratiques, mais aussi que nous avions de quoi nous améliorer.\n\nSuite à cette conférence, c’est HAProxy Fusion que nous attendons le plus. Fusion s’annonce comme l’outil idéal pour manager une flotte d’HAProxy. Jusqu’à présent, nous devions utiliser une solution maison HSDO, fonctionnelle, mais très probablement moins bien intégrée qu’un outil directement fourni par HAProxy.\n"
} ,
  
  {
    "title"    : "Trophy Hunter #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/trophy-hunter",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Les chasseurs qui ne font de mal à personne d’autres qu’à eux-mêmes 😻\n\n\nPrésenté par Oliver Thébault.\n",
  "content"  : "\n  Les chasseurs qui ne font de mal à personne d’autres qu’à eux-mêmes 😻\n\n\nPrésenté par Oliver Thébault.\n"
} ,
  
  {
    "title"    : "Trois patterns avancés pour améliorer la résilience d’une application #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/trois-patterns-avances-pour-ameliorer-la-resilience-une-application",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Après la présentation générale d’il y a quelques mois (vidéo publique), plongeons plus en profondeur dans trois approches qui aident à améliorer la résilience d’une plateforme.\n\n  Nous parlerons, aujourd’hui, de :\n\n  Random Jitter ;\n\n  de Cell-...",
  "content"  : "\n  Après la présentation générale d’il y a quelques mois (vidéo publique), plongeons plus en profondeur dans trois approches qui aident à améliorer la résilience d’une plateforme.\n\n  Nous parlerons, aujourd’hui, de :\n\n  Random Jitter ;\n\n  de Cell-based architecture ;\n\n  et de Shuffle Sharding.\n\n  Avec quelques schémas à ma façon 😍\n\n\nPrésenté par Pascal Martin.\n"
} ,
  
  {
    "title"    : "To be or not to be, ou quelques réflexions sur la dette technique et humaine #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/to-be-or-not-to-be",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Et si on parlait dette technique ? Oui, vous savez bien, tous les trucs qu’on laisse un peu de côté pour de bonnes ou de mauvaises raisons. Parceque on aura toujours le temps de voir ça plus tard, parcequ’il n’y a pas de temps pour s’y consacre...",
  "content"  : "\n  Et si on parlait dette technique ? Oui, vous savez bien, tous les trucs qu’on laisse un peu de côté pour de bonnes ou de mauvaises raisons. Parceque on aura toujours le temps de voir ça plus tard, parcequ’il n’y a pas de temps pour s’y consacrer, parceque qu’après tout, le produit ne fonctionne pas si mal que ça au fond, parceque cela coûte cher ! Mais au fond de vous …. En bon professionnel êtes vous vraiment en accord avec cette vision ? Etes vous ok pour sacrifier le produit ?\n\n  Maintenant, imaginons que “le produit”, ce soit vous et transposons tout cela à l’Humain je vous propose de découvrir ce que pourrait être la ou les différentes dettes technique qui peuvent nous freiner. La dette qui peut nous faire “bugger” et nous faire dysfonctionner. Et comment on peut connaitre ou reconnaitre quelques schémas, avoir quelques pistes à mettre en oeuvre pour au final être une meilleure version de soi même et pour mieux travailler avec soi et les autres.\n\n\nPrésenté par Emmanuel Herve.\n"
} ,
  
  {
    "title"    : "git log --since=486-09-16T00:00:00-00:00 --until=1453-05-29T00:00:00 #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/reconstitution-medieval",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Reconstitution medieval et autres joyeusetés 🏹\n\n\nPrésenté par Olivier Janin et Thomas Briset.\n",
  "content"  : "\n  Reconstitution medieval et autres joyeusetés 🏹\n\n\nPrésenté par Olivier Janin et Thomas Briset.\n"
} ,
  
  {
    "title"    : "40 min pour (tenter de) comprendre l&#39;informatique quantique #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/quarante-min-pour-comprendre-informatique-quantique",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  C’est une technologie qui défie l’imagination 🤯\n\n  Elle exploite des propriétés étranges de la matière et serait capable de résoudre en quelques minutes des calculs qui prennent aujourd’hui des années ⚡️ Elle fait rêver les physiciens, les gran...",
  "content"  : "\n  C’est une technologie qui défie l’imagination 🤯\n\n  Elle exploite des propriétés étranges de la matière et serait capable de résoudre en quelques minutes des calculs qui prennent aujourd’hui des années ⚡️ Elle fait rêver les physiciens, les grands groupes et même les gouvernements qui investissent massivement dans la recherche quantique 💸\n\n  Mais qu’est-ce que l’informatique quantique ? Pourquoi serait-elle révolutionnaire ?\n\n  Je vous propose une introduction aux enjeux de l’ordinateur quantique, un peu de théorie et un exemple d’algorithme quantique Préparez-vous à voir le monde en 12 dimensions, et c’est parti !\n\n\nPrésenté par Gabriel Forien.\n"
} ,
  
  {
    "title"    : "Fabriquer sa table avec plateau live edge : tout ce qu&#39;il faut faire... ou pas ! #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/fabriquer-sa-table-avec-plateau-live-edge-tout-ce-quil-faut-faire-ou-pas",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Fabriquer sa table avec plateau live edge : tout ce qu’il faut faire… ou pas !\n\n\nPrésenté par Timothé Crespy.\n",
  "content"  : "\n  Fabriquer sa table avec plateau live edge : tout ce qu’il faut faire… ou pas !\n\n\nPrésenté par Timothé Crespy.\n"
} ,
  
  {
    "title"    : "Écrire un livre, mais pourquoi faire ? #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/ecrire-un-livre-mais-pourquoi-faire",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Écrire un livre vous rendra-t-il riche et célèbre ? Peut-être ou peut-être pas.\n\n  En janvier dernier, j’ai publié un ouvrage sur Spark, framework data, aux Éditions ENI. Je vous propose de vous raconter cette aventure.\n\n  De cette manière-là, ...",
  "content"  : "\n  Écrire un livre vous rendra-t-il riche et célèbre ? Peut-être ou peut-être pas.\n\n  En janvier dernier, j’ai publié un ouvrage sur Spark, framework data, aux Éditions ENI. Je vous propose de vous raconter cette aventure.\n\n  De cette manière-là, vous aurez une vision des bénéfices (ou pas) que vous pouvez tirer en écrivant un livre.\n\n\nPrésenté par Nastasia Saby.\n"
} ,
  
  {
    "title"    : "Docteur qui ? #LFT 25/11/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/docteur-qui",
    "date"     : "November 25, 2022",
    "excerpt"  : "\n  Présentation de la série TV iconique, geek et britannique qu’est Doctor Who.\n\n  Découvrez :\n\n  De quoi parle Doctor Who ?\n\n  Pourquoi cette vieille série est culte ?\n\n  Par où commencer si on veut regarder\n\n\nPrésenté par Sarah Haïm-Lubczanski.\n",
  "content"  : "\n  Présentation de la série TV iconique, geek et britannique qu’est Doctor Who.\n\n  Découvrez :\n\n  De quoi parle Doctor Who ?\n\n  Pourquoi cette vieille série est culte ?\n\n  Par où commencer si on veut regarder\n\n\nPrésenté par Sarah Haïm-Lubczanski.\n"
} ,
  
  {
    "title"    : "How many DynamoDB RCU and WCU should we reserve to achieve maximum cost reductions, when our workloads are changing all the time?",
    "category" : "",
    "tags"     : " aws, dynamodb, finops",
    "url"      : "/2022/11/22/dynamodb-reservations.html",
    "date"     : "November 22, 2022",
    "excerpt"  : "Many of the microservices in our VOD and Replay platform use DynamoDB as their database.\nPerformance is very good if the data is architected for it, scalability is reasonably fast, and the serverless aspect offloads a lot of the administration and...",
  "content"  : "Many of the microservices in our VOD and Replay platform use DynamoDB as their database.\nPerformance is very good if the data is architected for it, scalability is reasonably fast, and the serverless aspect offloads a lot of the administration and hosting work. Whether it’s performance, resilience or time-to-market, DynamoDB helps us achieve our business goals.\n\nThat said, when we spend several hundred thousand dollars on DynamoDB every year, any optimization is good for us!\n\nWith DynamoDB, committing to a certain capacity for a year can help reduce costs – up to 50% savings on that capacity. But how do we know how much to reserve when traffic on our platform varies throughout the day?\n\n\n\nTable of Contents\n\n\n  DynamoDB: a not always obvious cost model!\n  How many WCUs and RCUs do we consume?\n  In theory: how much should we reserve, to achieve maximum savings?\n  In practice: let’s calculate how much to reserve!\n  Finally, let’s create those reservations!\n  After reserving, viewing the costs\n  Conclusion\n\n\nDynamoDB: a not always obvious cost model!\n\nTo skip all the theory about how DynamoDB is priced and WCUs, RCU, on-demand and provisionned billing modes, click here…\n\n\n  DynamoDB is serverless1!\n\n\nBut, as with many AWS services, you have to think for a while before you really understand DynamoDB costs…\n\nOut of scope costs\n\nWe pay for the volume of data stored, the volume of data backed up. These costs are outside the scope of this article and I won’t talk about them again today. They are not zero, however, and can even be a significant part of your bill – for example, if you store large data for a long time2 in DynamoDB. Something you probably shouldn’t do!\n\nWCUs and RCUs\n\nEach DynamoDB table can be configured in either on-demand or provisioned billing mode.\n\nIn the second case, we pay for RCUs (Read Capacity Units) and WCUs (Write Capacity Units), depending on the capacity we provision for each table.\nReservations only matter for these RCUs and WCUs, in purple in the screenshot below.\n\n\n\nOver the past year, our WCU and RCU costs in provisioned mode represent about half of our DynamoDB costs.\nStorage and backups have costs that we consider negligible today.\n\nAnd, from a financial standpoint, we work with far too many pay-per-request tables3 for my taste.\n\nThe documentation will tell you more, but in very broad terms:\n\n\n  One WCU is consumed when writing one line of data. Or for each 1 KB block written.\n  One RCU is consumed to read one line of data. Or for each 4 KB block read.\n  In eventually-consistent read mode, only 1/2 RCU is consumed to read one line of data. Or for each 4 KB block.\n  Transactional mode costs twice as much.\n\n\nAs you can imagine, the first optimization is to store only what is necessary and to request DynamoDB in the way that best meets the needs of the application, including consistency and costs. Developing a data schema that efficiently meets the needs of the application is crucial. I highly recommend you read Alex DeBrie’s very good “The DynamoDB Book”! Financial optimization based on reservations should – and can – only come afterwards, when usage patterns have been dealt with.\n\nThe on-demand / pay-per-request mode\n\nIn on-demand mode, we theoretically don’t have to worry about scalability, DynamoDB handles it for us4.\n\nIn this mode, we pay for each RCU and WCU we consume. If we don’t use DynamoDB, we don’t pay. If we use DynamoDB, we pay.\nThe counterpart is that RCUs and WCUs are more expensive in this mode than in the one presented below.\n\nThis mode is therefore very practical, in my opinion, in two cases:\n\n\n  In an environment where we only perform a few queries from time to time (dev, staging).\n  For tables that are usually not used much, but receive large and sudden peaks of requests at certain times.\n\n\nThis mode is not adapted, especially because costs are too high:\n\n\n  For tables where consumption is stable or varies slowly. Typically, tables for which usage follows our daily traffic wave, which is gentle enough on most applications for a reactive auto-scaling mechanism to meet our needs.\n\n\nProvisioned mode\n\nIn provisioned mode, we configure how many RCUs and WCUs we want and we pay for that number of RCUs and WCUs – no matter if we consume them or not.\nThis billing mode is therefore less flexible than on-demand. On the other hand, RCUs and WCUs are less expensive.\n\nIn provisioned mode, we can set up an auto-scaler on the RCUs and WCUs of the tables that need it. It will dynamically reconfigure the provisioned RCUs and WCUs for those tables, to approximate the actual usage. With an auto-scaler, we can pay as close as possible to our actual consumption, at the provisioned price, which is lower than the on-demand one.\nHowever, scale-out is not instantaneous: it takes several minutes to detect it needs to act, and then up to several minutes (especially on a large table) to do so. Also, scale-in can only be triggered around once per hour. For more detailed information, read the documentation and the quota page.\n\nThis mode is especially recommended, in my opinion and considering our workloads:\n\n\n  As often as possible, since each RCU and WCU costs much less than in on-demand mode.\n\n\nThis mode is not suitable:\n\n\n  On tables where consumption varies very abruptly.\n\n\nIn provisioned mode, reservations\n\nBy agreeing to pay for a certain amount of RCU and WCU for one year (or even three years in some regions), these RCU and WCU become even cheaper: up to ~50%5 cheaper than in default-provisioned mode.\nReserving capacity is a great way to considerably reduce the cost of read/write operations on DynamoDB!\n\nReservations lock us for one year. We will pay for the reserved RCUs and WCUs, whether we use them or not.\nIt is therefore important to calculate correctly the reservations to be made.\n\nAlso, we pay a part of the total yearly amount at the beginning of the commitment (= “upfront”), which means we must be able to invest a certain amount in advance.\nThe other part is spread over all the months of the commitment period.\n\nAs a consequence, the big question, to which the rest of this document tries to answer, is: “how many RCU and WCU should we reserve to keep our costs as low as possible?”\nWhen our consumption varies throughout the day, this calculation is pretty fun ;-)\n\nReservations are global to an AWS account, or even to all accounts on a consolidated bill6.\n\n→ Reserved pricing is documented on the page of “provisioned” pricing.\n→ You can also read this whitepaper.\n\nHow many WCUs and RCUs do we consume?\n\nFor the rest of our reasoning and this article, we only count the consumption in provisioned mode (and exclude on-demand), since that’s where we can play with reservations.\nAlso, we count provisioned WCU and RCU and not what is actually consumed – so beware of any potential waste.\n\nOn the DynamoDB Web Console home screen, we can see, for an account and a region, how many WCUs and RCUs are provisioned at the current time:\n\n\n\nBut these numbers only give a view at a given instant, in a single AWS account and in a single region.\nWe deploy our platform across dozens of accounts and multiple regions, with traffic that changes throughout the day, so this is not enough.\n\nTable WCUs/RCUs\n\nFor a global view of all tables in an account in a region, we can query Cloudwatch Metrics, analyzing the ProvisionedWriteCapacityUnits or ProvisionedReadCapacityUnits metrics:\n\n\n\nThe Stacked Area view shows, at any given time, the total WCUs (or RCUs) provisioned for all of our tables, in an account and a region.\n\nWCU/RCU of GSI\n\nWe also need to count the WCUs/RCUs of the Global Secondary Indexes – and these are different metrics! Or, at least, the metrics are shown in a different category in the Cloudwatch web console.\n\n\n\nSo, in total…\n\nTo get the total, you have to consider this metric for the tables and for the Global Secondary Indexes! In the Cloudwatch console, you have to search in two categories.\nGraphing it all :\n\n\n\nOf course, this is to be looked at for WCUs, but also for RCUs, following exactly the same principle.\nAnd, again, we’re working in multiple accounts and regions.\n\nIn theory: how much should we reserve, to achieve maximum savings?\n\nOnce we know how much capacity we’re actually using, we can move on to reservations.\n\nBut the calculation would be far too easy if our usage was flat!\nIn reality, thanks to auto-scaling, our provisioned capacity follows our usual traffic pattern: a wave.\n\nAnd, two things:\n\n\n  if we reserve more than we provision, we’ll waste money.\n  if we reserve less than we provision, we won’t save as much as we could.\n\n\nReserve at the bottom of the wave\n\nA first idea is to reserve the lowest value we provision throughout the day: what we provision at the bottom of our traffic wave, at night.\n\nIn this case, we are not wasting money, as we always provision 100% or more of our reservation.\nBut we are probably minimizing our savings, since we are provisioning more than the reservation, all day long.\n\nReserve at the top of the wave\n\nA second idea, kind of the opposite, is to reserve the highest value we provision throughout the day.\nThis way, we will never pay the full rate for any WCU/RCU.\n\nBut, in this case, we will be wasting a lot of money, since all day long we will be provisioning less than our reservation.\nThis is a bad idea.\n\nReserve “in the middle”, thanks to careful calculations\n\nNow, the real solution: calculate the right value:\n\n\n  Less than the highest value, to minimize waste.\n  And more than the lowest value, to optimize savings.\n\n\nIn practice: let’s calculate how much to reserve!\n\nManipulating metrics in Cloudwatch, for visualization, may be acceptable, although we rarely do it since we use other stacks for our metrics. And aggregating metrics from multiple accounts should be feasible (we haven’t tried it).\nBut for calculations, it is not enough.\n\nExporting metrics\n\nAs a first step, we exported the metrics visualized above, to be able to manipulate them in another tool – in a spreadsheet, for example.\nTo export these metrics from Cloudwatch, we can query its API. We need to do this for all accounts and for each table, which is complicated to do manually.\n\nTo simplify the task, we started working with a script that exports this data to a CSV file.\nSpecifically, this script exports one data point per hour: the number of WCUs or RCUs actually provisioned during that hour.\n\nRunning this script for a representative week, we have enough data to calculate the ideal reservations.\n\n\n  🗓️ Representative week?\nOf course, we have to be careful to choose the week we focus on.\nIf we work with data from a week with a huge unexplained peak of traffic, the results of our calculation will fit that week, but not so much to the rest of the year!\n\n\nA Google Spreadsheet calculation\n\nImporting this data into a Google Spreadsheet, we get two columns: a date+time and a number of WCUs.\nAnd this is for each one-hour range during an entire week:\n\n\n\n\n  ℹ️ Only twelve hours\nHere, I only reproduce twelve rows corresponding to twelve hours, but keep in mind that there are actually 168 rows in my spreadsheet: one row per hour, 24 hours per day, for 7 days.\nAlso, the values used for this article are all simulated, to avoid sharing sensitive information, but they scrupulously respect the shape of our traffic and usage wave.\n\n\nThe next step is to integrate the cost of these WCUs.\nEasy anough, we multiply the number of WCUs by the cost of a WCU in Paris, i.e. $0.000772.\nAnd the sum of the cost of each line gives us the total cost, without reservation:\n\n\n\nThe calculations, on an assumption\n\nNow, let’s assume, for the time being, that we reserve 25,000 WCUs:\n\n\n  The upfront, each hour, is $5.07991.\n  And, each hour, we also have to pay $3.82500 for this capacity, since the upfront is only partial.\n\n\nIn addition:\n\n\n  During some hours, when we consume less than 25,000 WCU, we will not pay anything extra.\n  During some other hours, when we consume more than 25,000 WCU, we will have to pay a supplement, at the full provisioned rate.\n\n\nAdding these data, we obtain a different hourly cost, often lower than the one determined above.\nAnd, therefore, we get a lower total cost as well:\n\n\n\nWith this hypothesis of a 25,000 WCU reservation, over these twelve hours, we would pay 135 dollars instead of 229 dollars without reservation.\nWe would then realize 40.96% savings!\n\nThe calculations, until we find the right value\n\nOf course, during the hours when we consume less than 25,000 WCU, we are wasting capacity: we are paying for it, without using it.\n\n\n\nThe goal of the game is to find the right number of WCUs to reserve: we want to reduce the total cost as much as possible, maximizing the percentage of savings.\n\nTo do so, we try different values for the number of WCUs reserved, until we find the one that maximizes the percentage of savings:\n\n\n\nHere’s the same thing as a graph:\n\n\n\nHere, over these twelve hours, the optimal approach would be to reserve 23,000 WCU.\n\n\n  💪 Getting real: an entire week\nIn reality, we perform exactly the same calculation and we follow this very same logic, on 168 lines of data, corresponding to a representative week.\n\n\nEasier calculations?\n\nThe first year we tried to reserve capacity, we quickly wrote a script to collect the data from Cloudwatch and export it as CSV.\n\nWe still haven’t, after three or four years now, written a program that would perform the calculations based on this data to come up with the right value for the number of WCUs or RCUs to reserve.\nAs a matter of facts, copying and pasting data from the CSV export to a spreadsheet only takes a minute, we reuse the same year after year, and its visual aspect is nice!\n\nAlso, we only do these calculations and reservations twice a year, so we don’t spend too much time working on this, while still refining more often than once each year.\nEach time, the process takes two of us7 about two hours, or one day per year in total… And the most time-consuming part is talking to our colleagues who are heavy DynamoDB users, and asking them “are you planning to reduce the consumption of your project over the coming year?”\n\nFinally, let’s create those reservations!\n\nWe calculated how many WCUs and how many RCUs we should reserve to achieve the best possible savings, hoping the week we chose to base our calculations on was actually a representative week.\n\nA commitment: be careful…\n\nA reservation commits us to pay for a year, whether we use this capacity or not.\n\nSo, it’s always a good idea to take a moment to validate with our colleagues that they are not planning to use less DynamoDB in the near future.\nOf course, the answer is often partly “it depends”, since usage depends on new projects as well as on the traffic on our platforms, but if we can already anticipate the next planned optimizations, it’s always a good thing.\n\nIn November 2022, we can only open DynamoDB reservations for one year if we work in the AWS Paris region.\nOther regions (us-east-1 for example) allow reservations for three years, which means more substantial savings. On the other hand, would we be willing to commit for three years and lose a major advantage of The Cloud, its flexibility?\n\nWhich account to reserve on?\n\nThe documentation says (emphasis mine):\n\n\n  If you have multiple accounts linked with consolidated billing, reserved capacity units purchased either at the payer account level or linked account level are shared with all accounts connected to the payer account.\nReserved capacity is applied first to the account that purchased it and then any unused capacity is applied to other linked accounts.\n\n\nWe have configured our AWS accounts to have a single payer account.\nWe have decided to make all our reservations in this account and they are applied to the child accounts without discrimination.\nThis applies to DynamoDB but also to RDS, EC2, Elasticache…\n\nReserving!\n\nTo reserve, we go through the AWS DynamoDB Web console, in our payer account, in the region where these reservations will be used.\n\nOn this screen, you can see how many WCUs and RCUs we have already reserved.\nSince we make several reservations during the year, the reservations already in progress are to be subtracted from the values calculated above!\n\n\n\nTo create a new reservation, click on “Purchase reserved capacity” and fill in the form ;-)\n\n\n\nAfter reserving, viewing the costs\n\nOnce the reservations are made, in AWS Cost Explorer, the upfront cost is clearly visible.\nIt is charged at once, on the day we opened the reservation:\n\n\n\nTo have a daily view of WCU/RCU costs (reserved + provisioned in addition to reservations), remember to fill in “Show costs as: Amortized costs” to smooth the monthly price of reservations over all days of the month:\n\n\n\n\n  Reservations and one payer account\nSince reservations, which cover the bulk of our DynamoDB costs, are made on our payer account, the bulk of our DynamoDB costs go back to this account… And not to the tenant/environment accounts.\nGood luck tracking costs and allocating them to projects and teams 💪\n\n\nConclusion\n\nWe work with DynamoDB a lot, for several dozen microservices, and we face several types of infrastructure costs: on-demand reads/writes, provisioned reads/writes, storage, backups.\nIn exchange for a loss of flexibility and through reservations that commit us for a year, AWS allows us to reduce the cost of provisioned reads/writes.\n\nDetermining how much to reserve, in the face of a constantly changing load, is not easy.\nWe need to have a certain vision on the evolution of usage, over a year, and must accept to lose flexibility.\nAnd we need to find the right values to reserve for read and write capacity.\n\nWith three or four years of hindsight, by making reservations twice a year and by following the method detailed in this article, we realize savings of about 30% to 35% on our read and write capacity in provisioned mode.\nOn our scale, this saving represents several tens of thousands of dollars per year – which is great, considering we only spend a few hours working on this every six months!\n\n\n\n  \n    \n      DynamoDB is one of the most serverless services we use and I like it a lot. Still, there are a few admin tasks left in our hands. Typically, we have to specify the capacity we need and configure an auto-scaler. We also have to enable encryption, backups, to setup permissions – and to check all this is done, for all tables, managed by many teams. &amp;#8617;\n    \n    \n      If you do store a lot of data for a long time in DynamoDB, take a look at Standard-IA, it might help you reduce costs. &amp;#8617;\n    \n    \n      Why do we use pay-per-request so much? Well, in short, because this mode is more flexible than the provisioned one, and several of our projects are willing to pay much more in exchange for this flexibility. &amp;#8617;\n    \n    \n      DynamoDB in on-demand mode and scalability: in practice, AWS hides what’s going on, but doesn’t scale to infinity instantly either. &amp;#8617;\n    \n    \n      50% is kind of the maximum possible saving we can achieve if our usage is flat and we reserve exactly what we provision. Flat usage might be what you see on your applications, but it’s not how our platform works! &amp;#8617;\n    \n    \n      At Bedrock, we have a dedicated billing account – a “payer account” – that aggregates costs from all our other accounts. Reservations are also shared amongst all (whitelisted) accounts that have a shared payer account. &amp;#8617;\n    \n    \n      For these kind of calculations and reservations, we usually work in pair, as this involves large amounts of money. Lowering risk of doing a costly mistake is quite a good idea. &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Ce que nous retenons de la droidcon London 2022",
    "category" : "",
    "tags"     : " android, droidcon, conference",
    "url"      : "/2022/11/22/droidcon-london-2022.html",
    "date"     : "November 22, 2022",
    "excerpt"  : "La communauté Android a apporté le soleil sur Londres les 27 et 28 octobre 2022. La droidcon London a réuni plus de 1400 développeurs autour de l’écosystème Android, de ses outils et enjeux actuels. Jetpack Compose, évidemment, mais aussi Gradle, ...",
  "content"  : "La communauté Android a apporté le soleil sur Londres les 27 et 28 octobre 2022. La droidcon London a réuni plus de 1400 développeurs autour de l’écosystème Android, de ses outils et enjeux actuels. Jetpack Compose, évidemment, mais aussi Gradle, modularisation, optimisation et autres sujets plus divers ont été abordés lors de ce rendez-vous incontournable pour la communauté.\n\n\n\n\n  Ça compile ? - Rafi Panoyan    \n      Vous reprendrez bien un peu de Gradle Enterprise ?\n      Dessine-moi un module\n      Trucs et astuces\n    \n  \n  Design the world - Damien Cuny    \n      To Compose\n      Design System\n      Vers l’infini et au-delà\n    \n  \n  La gestion des erreurs - David Yim    \n      Vérification des entrées\n      Le type Either\n      Kotlin Result\n      Conclusion\n    \n  \n  À la prochaine !\n\n\nÇa compile ? - Rafi Panoyan\n\nLes sujets de compilation ont tenu une place très importante lors de cette édition de la droidcon Londres 2022. \nQu’il s’agisse d’optimiser ses temps de compilation, de repenser la création de modules et des dépendances entre eux, de factoriser les logiques des scripts de compilation, \nnous avons eu une emphase claire sur l’importance d’adresser ces sujets.\n\nVous reprendrez bien un peu de Gradle Enterprise ?\n\nNelson Osacky, qui travaille chez Gradle, a présenté tous les outils que la formule Gradle Entreprise met à disposition des développeurs pour analyser en détail les compilations.\n\nVous voulez vérifier que la compilation incrémentale est bien appliquée partout où cela est possible ? Un script permet de comparer, dans des conditions reproductibles, \nles entrées et sorties de vos builds, et analyse les tâches empêchant ce mécanisme central dans la réduction des temps de compilation.\n\nVous voulez vous assurer que Gradle est bien capable de retrouver le cache de vos tâches sur un même poste ou bien depuis le cloud ? \nLà aussi des outils vous permettent d’identifier précisemment les points qui ne tirent pas parti de ces mécanismes.\n\nOn regrettera que ces outils soient disponibles uniquement pour la formule payante de Gradle. Cependant, les scans Gradle sont, eux,\ngratuits et illimités, et permettent tout de même de mesurer et comparer des compilations et ainsi suivre l’impact des différentes optimisations que vous pourriez apporter.\n\nDessine-moi un module\n\nLa modularisation ayant un impact sur les temps de compilation, plusieurs conférences ont abordé ce sujet très en vogue dans la communauté Android.\n\nUn point de vue intéressant de Josef Raska nous invite à nous poser la question de la pertinence de modulariser selon le contexte. \nNe pas suivre une tendance mais se poser la question de l’utilité d’un nouveau module, et encore plus de ses dépendances avec les autres modules. \nVoilà des propos qui invitent à mesurer concrètement l’impact du chantier de la modularisation dans nos applications.\n\nAinsi, si on peut penser que modulariser permet de réduire les temps de compilation (en tirant parti de la parallélisation des tâches par exemple), \nun chemin de dépendances trop long entre le module initial et la dépendance la plus profonde va entraîner une augmentation du temps de compilation.\n\nVigilance, donc, sur les “hubs de dépendances” (ces dépendances dont beaucoup de modules ont besoin, et qui ont besoin de beaucoup de modules).\n\n\n  \n  1. Hub de dépendances\n\n\n\nDe la même manière, un chemin de dépendances de trop grande profondeur ne permettra pas de tirer parti de la parallélisation des tâches de compilation.\nSur le schéma ci-dessous, on peut voir qu’un chemin de profondeur 4 existe pour aller du module applicatif vers le module le plus bas dans la hiérarchie.\n\n\n  \n  2. Profondeur de dépendances\n\n\n\nJosef Raska propose le schéma suivant avec un découpage API/implémentation afin de réduire au maximum cette profondeur, et ainsi compiler plus rapidement.\n\n\n  \n  3. Profondeur de dépendances réduite\n\n\n\nAndroid Studio et son analyse de dépendances peut être très utile pour vérifier et mesurer cela.\nJosef Raska a d’ailleurs créé un plugin Gradle afin de spécifier ces règles à l’echelle d’un projet et de s’assurer qu’elles soient respectées : modules-graph-assert.\n\nTrucs et astuces\n\nAprès ces conseils très avisés mais structurellement chronophages à mettre en place (surtout sur de gros projets déjà créés), d’autres conférenciers se sont plutôt tournés vers les “quick-win”. Des changements peu coûteux, aux gains plus modestes mais qui s’additionnent, il en existe quelques-uns.\n\nAinsi, si Gradle nous permet d’activer des fonctionnalités de caching (org.gradle.unsafe.configuration-cache=true pour gagner du temps lors de la phase de configuration par exemple), il est aussi possible de désactiver des fonctionnalités du plugin Android si elles ne nous sont pas utiles.\n\nVoici une petite liste des propriétés qui sont activées par défaut, même lorsqu’elles ne sont pas utilisées dans les modules :\nandroid {\n  buildFeatures {\n    buildConfig false\n    aidl false\n    renderScript false\n    resValues false\n    shaders false\n  }\n}\n\n\nSi vous n’utilisez pas les valeurs liées à la configuration de votre compilation, ne générez pas de BuildConfig.\nSi vous n’avez pas de resources dans votre module, désactivez la génération de resValue !\n\nRetrouvez ici la liste de ces fonctionnalités, leur utilité et leurs valeurs par défaut : BuildFeatures.\n\nDesign the world - Damien Cuny\n\nIl y a un peu plus d’un an sortait la version 1.0 de Jetpack Compose, le nouveau toolkit déclaratif pour la création d’interfaces Android. D’autre part, le design system Material Design 3 vient de sortir en version stable et son implémentation Compose Material sont également disponibles.\nAvec tout cela, le design a, cette année encore, tenu une place de choix dans l’agenda de cette droidcon 2022 à Londres.\nMais comment utiliser tout cela correctement ? Comment s’en servir pour implémenter un design system personnalisé ? Jusqu’où peut-on aller ?\nAutant de questions auxquelles ont tenté de répondre les nombreuses présentations sur le sujet.\n\nTo Compose\n\nCompose facilite beaucoup de choses dans l’implémentation et le maintien d’interfaces sur Android. Cependant, cela nécessite de réapprendre à faire certaines choses que l’on maîtrise déjà avec le système de View.\nDessiner dans un canvas en est une, et Himanshu Singh dans sa présentation “Composing in your canvas”, nous montre les pièges à éviter pour réaliser cela avec Compose.\n\nLa recomposition peut également être source de problèmes et de latences si Compose est mal utilisé. Dans sa présentation “Understanding recomposition performance pitfalls”, Jossi Wolf et Andrei Shikov nous donnent, à partir d’un exemple concret, les meilleures astuces pour l’utiliser à bon escient.\n\nDesign System\n\nEn faisant le parallèle avec la saga épique de JRR Tolkien, Daniel Beleza, dans sa présentation “One design system to rule them all”, nous explique comment il a réussi, tout en se passant de Material design, à unifier et automatiser son propre design system.\nCela demande, évidemment, une collaboration totale de la part de l’équipe de design, mais une fois cette intégration faite, les bénéfices et l’autonomie se ressentent de part et d’autre.\nDes outils tel que Figma, Kotlin Poet ou des plugins Android Studio custom lui ont permis d’automatiser ensuite ce processus.\n\nMaterial Design est un design system. Il a l’avantage d’être bien documenté, uniforme et régulièrement enrichi. De plus, il est déjà implémenté dans l’ancien système de View Android et plus récemment dans Jetpack Compose avec Compose Material.\n\nUne des différences majeures entre Compose et le système de View sur Android est son découpage. Dans Compose, Material n’est implémenté et n’apparaît que dans la partie la plus hautes alors que dans le système de View, son implémentation est répartie dans toutes les couches de la librairie.\nIl est donc assez complexe de se passer de Material avec le système de View mais cela est complétement envisageable, voire recommandé, dans certains cas avec Compose.\n\n\n\nPour illustrer cela Sebastiano Poggi (la moitié de Coding with the italians) est venu nous présenter, dans “Compose beyond Material”, les questions à se poser avant de se lancer dans son design system et comment le package Foundation de Compose peut nous aider.\n\nPour terminer il nous donne de nombreux conseils concrets sur l’implémentation de composants sans Material. Le principal, rejoint la présentation d’introduction de cette droidcon, “The Silver Bullet Syndrome Director’s Cut - Complexity Strikes Back!”, un bon design system est un design system qui correspond à nos besoins et qui y répond le plus simplement possible.\n\nVers l’infini et au-delà\n\nChris Bane et Nacho Lopez dans leur présentation “Branching out Jetpack Compose”, nous ont raconté comment l’aventure du passage à Compose s’est déroulée chez Twitter, une des premières apps à l’adopter.\nAvec une code base aussi conséquente (plus de 1000 modules, dont 300 pour le design, répartis sur plus de 30 équipes), ils ont dû progressivement convaincre les équipes, les former et les accompagner.\nLa question de continuer à utiliser Material Design s’est également posée chez eux. Ils l’ont dans un premier temps conservé pour faciliter le passage sur Compose, pour finalement le retirer complètement en se basant, eux aussi, sur le package Foundation.\nLeur présentation résume bien l’ensemble des étapes et des questions par lesquelles ils sont passés pour accomplir cette transition.\n\nAfin de remettre les choses en perspective, Ash Davies nous rappelle que Compose est un simple pattern de développement multiplateforme. De ce fait, il peux être appliqué à autre chose qu’à de l’UI comme le propose Jetpack Compose. Il nous explique dans “Demystifying Molecule: Running Your Own Compositions For Fun And Profit”, comment l’appliquer à la couche domaine d’un projet pour le “Fun”.\n\nLa gestion des erreurs - David Yim\n\nLa gestion des erreurs a été le sujet de plusieurs présentations à la droidcon. Ces présentations avaient pour objectif de servir de piqûre de rappel sur l’importance de bien prendre en compte ce problème concernant tous les développeurs. Aujourd’hui, nous avons tous les outils pour gérer facilement nos erreurs. Cependant, par paresse et comme nous préférons penser de manière positive, nous ne pensons souvent qu’aux cas de succès et les cas d’erreurs sont souvent brouillons voire ne sont même pas spécifiés.\n\nLes speakers m’ont marqué avec un exemple de mauvaise gestion d’erreur qui a coûté plusieurs centaines de milliers de dollars. L’exemple parlait d’une faille sur le site japonais de 7-Eleven, une chaîne de supérettes, dont elle a été victime. Dans la base de données de ce projet, les développeurs ont ajouté un champ “date de naissance” comme nullable. Plus tard, ce champ est devenu non-nullable. Par paresse, le développeur qui a rendu ce champ non-nullable a mis par défaut un 1er janvier 2019 sur cette date lorsqu’elle n’était pas renseignée, simplement pour satisfaire son compilateur. Le problème est que ce champ fut plus tard utilisé dans la fonctionnalité de mot de passe oublié du site. En utilisant la date par défaut du 1er janvier 2019, un hacker a pu récupérer des comptes utilisateurs et voler des informations bancaires. Cet exemple m’a marqué par l’habitude que nous avons en tant que développeur de nous soucier que de satisfaire notre compilateur plutôt que de vraiment discuter de solutions réfléchies à nos problèmes techniques.\n\nPlusieurs méthodes de gestion des erreurs existent et les speakers en ont présentés quelques-unes.\n\nVérification des entrées\n\nL’une des méthode pour être certain de ne pas avoir de problème est de vérifier les données que l’on reçoit. Prenons un exemple simple :\n\ndata class User(val email: String)\n\n\nRien n’empêche d’instancier cette classe de la manière suivante :\n\nval user = User(email = &quot;&quot;)\n\n\nCela peut créer des problèmes par le futur, alors qu’il y a un moyen d’éviter cela\n\nclass Email(value: String) {\n    val value: String =\n        if (value.isEmpty() || !Regex(EMAIL_FORMAT).matches(value)) {\n            throw Exception(&quot;Email format is not correct&quot;)\n        } else {\n            value\n        }\n}\n\ndata class User(private val email: Email)\n\n\nCette méthode peut paraître un peu exagérée dans cet exemple. Mais dans un contexte où la classe User serait utilisée par un grand nombre d’équipes et que les règles métier de l’Email serait complexe, cette méthode prendrait tout son sens pour éviter d’avoir de mauvaises surprises !\n\nLe type Either\n\nLe type Either est un moyen de différencier les cas de succès des cas d’erreurs. Il est disponible dans la librairie Arrow ou facilement reproduisible :\n\nsealed class Either&amp;lt;out A, out B&amp;gt; {\n    class Left&amp;lt;A&amp;gt;(val value: A): Either&amp;lt;A, Nothing&amp;gt;()\n    class Right&amp;lt;B&amp;gt;(val value: B): Either&amp;lt;Nothing, B&amp;gt;()\n}\n\n\nL’utilisation de ce type est qu’il est soit un type A, soit un type B. On peut ainsi définir par exemple que le A est un cas de succès et que le type B est un cas d’erreur.\n\ndata class User(val name: String)\n\nvar either: Either&amp;lt;User, Exception&amp;gt;\n\nwhen (either) {\n    is Either.Left -&amp;gt; {\n        println(&quot;The user is called ${(either as Either.Left&amp;lt;User&amp;gt;).value.name}&quot;)\n    }\n    is Either.Right -&amp;gt; {\n        (either as Either.Right&amp;lt;Exception&amp;gt;).value.printStackTrace()\n    }\n}\n\n\nGrâce à ce type, on peut par exemple savoir si un appel à une API a réussi ou non, ce qui nous permet de gérer plus facilement nos cas d’erreurs.\n\nKotlin Result\n\nLa classe Result est similaire au type Either et a pour avantage d’être directement inclue dans Kotlin et que l’on n’a pas à se synchroniser pour savoir si le côté gauche est le cas de succès ou d’erreur.\n\ndata class User(val name: String)\n\nvar result: Result&amp;lt;User&amp;gt;\n\nwhen {\n    result.isSuccess -&amp;gt; {\n        println(&quot;The user is called ${result.getOrNull()?.name}&quot;)\n    }\n    result.isFailure -&amp;gt; {\n        result.exceptionOrNull()?.printStackTrace()\n    }\n}\n\nOU\n\nresult.onSuccess { user -&amp;gt;\n    println(&quot;The user is called ${user.name}&quot;)\n}.onFailure { throwable -&amp;gt;\n    throwable.printStackTrace()\n}\n\n\nConclusion\n\nPlusieurs méthodes existent pour prendre en compte nos cas d’erreurs. Laquelle est la meilleure ? Eh bien vous vous y attendez sûrement, mais ça dépend ! On choisira une méthode ou une autre selon ce qui nous arrange par rapport à la situation, nos choix d’outils techniques ou nos effectifs. L’important étant de prendre en compte ces cas d’erreurs et de ne pas laisser leur résolution au hasard. Les cas d’erreurs ne sont en fait que d’autres usecases de l’utilisateur et souvent ne sont pas des edgecase. Ils méritent donc d’être tout autant réfléchis et spécifiés que les cas de succès !\n\nÀ la prochaine !\n\nIl est toujours intéressant de mesurer l’engouement pour tel ou tel sujet dans la communauté Android en analysant les présentations lors des différentes conférences technologiques.\n\nSans aucun doute, cette droidcon était sous le signe de Jetpack Compose, qui bénéficie d’un suivi et d’un engagement fort de Google et de toute la communauté.\nTout l’enjeu ici est de rester au contact des innovations et de l’évolution de la plateforme Android, et Jetpack Compose offre un défi que nous avons commencé à relever chez Bedrock.\n\nNous attendons avec impatience de voir où va Android, et avons à coeur de participer à cette aventure qui nous lie tous !\n\n\n"
} ,
  
  {
    "title"    : "Ce que nous avons retenu de la SymfonyCon - Disneyland Paris 2022",
    "category" : "",
    "tags"     : " conferences, backend, symfony, php",
    "url"      : "/2022/11/17/symfonycon.html",
    "date"     : "November 17, 2022",
    "excerpt"  : "En cette fin d’année, une petite équipe de chez Bedrock a assisté au grand retour de la SymfonyCon 2022 après 3 ans d’absence. \nNous avons eu la chance de découvrir les nouveautés liées à Symfony 6.2 et d’assister aux conférences sur de nombreux s...",
  "content"  : "En cette fin d’année, une petite équipe de chez Bedrock a assisté au grand retour de la SymfonyCon 2022 après 3 ans d’absence. \nNous avons eu la chance de découvrir les nouveautés liées à Symfony 6.2 et d’assister aux conférences sur de nombreux sujets techs à Disneyland.\nLa keynote présentée par Fabien Potencier, le créateur de Symfony, nous donne un avant-goût des nouvelles fonctionnalités qui seront présentes dans la future version de Symfony.\nAu programme, un nouveau composant Webhooks ainsi que l’évolution du composant Mailer.\n\nUnleashing the power of lazy objects in PHP\n\nNicolas Grekas, habitué de la scène PHP et membre de la Symfony Core Team, nous a présenté les différentes façons d’utiliser les lazy objects en PHP et plus spécifiquement à l’aide de Symfony. \n\nLes lazy objects sont des objets instanciés vides qui peuplent leurs propriétés eux-mêmes seulement quand ils sont utilisés.\nIls sont utiles lors de l’instanciation de lourds objets peu appelés, cela permet de faire du lazy loading.\n\nNicolas nous a aussi présenté deux nouveaux traits prochainement disponibles sur Symfony 6.2 permettant de travailler avec ces objets.\nLes VirtualInheritanceProxies et les GhostsObjects sont deux nouvelles possibilités pour implémenter plus facilement des lazy objects en plus du ValueHolder.\n\nAdvanced Git magic\n\nPauline Vos nous représente GIT en dehors de son usage quotidien, comment aller plus loin que les pull, commit, push et merge habituels, elle nous livre donc une présentation sur une méthode de debug en utilisant GIT.\n\nRetour dans un premier temps sur l’importance des commits dit “atomic” avec comme règles :\n\n  Chaque commit se résume à un fix ou une feature\n  Chaque commit doit fonctionner (tous les tests doivent passer)\n  Chaque message et description doivent être clairs et concis\n\n\nPauline introduit ensuite la commande git rebase -i qui permet un rebase interactif servant notamment à réécrire notre historique.\n\n\n\nVient ensuite l’utilisation de la commande git reflog, commande avec laquelle nous pouvons obtenir le détail des commandes lancées sur la branche, elle peut de ce fait être utile pour réparer une erreur.\n\nComment utiliser toutes ces commandes GIT pour debugger ?\nUne démonstration de la commande git bisect et de toutes ses options qui permettent d’identifier le commit qui a introduit le bug en faisant une recherche dichotomique.\n\nPauline pousse la réflexion plus loin en alliant la commande git bisect avec un script de debug ou un test unitaire.\n\n“Write it, push it and find where it breaks”\n\nAfin de tirer parti de cette méthode, il est important que chaque commit soit fonctionnel, tous les tests doivent donc passer.\n\nVous pouvez également retrouver un article de Pauline à ce sujet sur son blog personnel.\n\nSchrödinger’s SQL - The SQL inside the Doctrine box\n\nAu début de sa conférence Claudio Zizza insiste sur le fait de bien connaître nos bases de données et le langage utilisé pour les requêter, il évoque notamment les différences entre MySQL et PostgreSQL. Ensuite, il nous a parlé de Doctrine ORM, des fonctions que nous apprécions tant, car elles nous facilitent la vie. \nPuis, il a rappelé l’importance de savoir faire des requêtes SQL même si nous utilisons Doctrine. Comprendre le SQL peut nous permettre d’optimiser notre utilisation de Doctrine et donc de mieux appréhender son fonctionnement. \nIl insiste aussi sur le fait que SQL “seul” est bien plus puissant que DQL (Doctrine Query Language). \nPour terminer, Claudio nous a donné quelques recommandations de lecture pour apprendre le SQL ainsi qu’un site permettant de tester nos requêtes.\n\nAdvanced Test Driven Development\n\nDurant cette conférence, Diego Aguiar, développeur de chez SymfonyCasts nous rappelle ce qu’est le TDD (Test Driven Development), son histoire, les différentes techniques ainsi que des astuces afin de nous débloquer et bien sûr les cas où il ne semble pas utile d’utiliser le TDD.\n\nÀ retenir, le TDD est une discipline, il faut beaucoup d’entraînement et répéter continuellement ces exercices.\n\n\n\n“Fake it till you make it”, il s’agit d’abord d’écrire son code de test en utilisant par exemple des assertions, des tests avec différentes sorties et ensuite de produire le code qui va résoudre ces tests.\n\n\n\n“ Write your test code, produce it and repeat.”\n\nPourquoi nous retrouvons nous parfois bloqués ?\n\n  Les tests écrits sont peut-être faux\n  Les tests ne sont pas assez segmentés\n  Le code écrit est peut-être trop spécifique\n\n\nComment se débloquer ?\n\n  Continuer et trouver un test plus simple\n  Refactoriser le code en production qui met en difficulté\n  Écrire les différents use-cases\n  Passer outre les tests un instant\n\n\nLes cas les plus favorables au TDD sont les nouvelles fonctionnalités qui n’ont pas de lien avec du code legacy. En ce qui concerne les cas non pertinent au TDD, nous retrouvons les cas de configuration, de découverte de code et de requêtes.\n\nPour conclure, Diego nous rappelle que le TDD est bien évidemment plus un outil qu’une règle.\n\nDynamic Validation With Symfony\n\nTout en se basant sur les évolutions des Pokémon, Marion Hurteau a introduit le principe de validation dynamique. Par exemple, vérifier que le nom de notre Pokémon contient bien 10 caractères, ou encore les différentes règles d’évolution en fonction du type de Pokémon. \nÀ l’aide d’exemples de code qui sont disponibles sur son repo Git, elle a passé en revue les façons d’implémenter des validations à l’aide du composant Symfony Validator.\nAu fil de sa présentation, la complexité des contraintes croît ce qui permet de voir un éventail de possibilités.\n\nFrom monolith to decoupled…wait, why is that one getting bigger?!?\nLors de cette conférence, Shawna Spoor est venue nous parler de comment découper un monolithe en une multitude de microservices grâce au “Strangler Fig Pat”. Elle a commencé par nous rappeler les avantages et les inconvénients des microservices comparé à un monolithe.\n\nSuite à cela, elle nous a donné les différentes étapes pour découper une application monolithe en micro-services et cela sans jamais arrêter le développement de nouvelles features:\n\n  Choisir une fonctionnalité qui peut être découpée\n  Créé le nouveau Service\n  Déplacer le trafic vers le nouveau service\n  Recommencer jusqu’à la disparition du monolithe\n\n\nOn peut résumer ce pattern via l’image ci-dessous, le tronc représente le monolithe et les branches qui l’étranglent lentement correspondent aux micro-services.\n\n\nFrom a legacy Monolith to a Symfony Service Oriented Architecture with zero downtime\nLors de la conférence présentée par Clément Bertillon, nous avons pu voir comment son équipe a transformé leur ancienne application monolithe composée de milliers de fichiers PHP en un monorepo décomposé en micro-services en utilisant le Strangler Fig pattern et cela sans aucune rupture de service ni arrêter le développement de nouvelles features.\n\nDe manière très simplifiée, ils ont installé Symfony, mis le code legacy dans un dossier à la racine du projet, le routeur symfony permet d’accéder au nouveau micro-service tout en redirigeant vers le legacy si aucun contrôleur n’a été trouvé. Il a conclu avec les règles d’or et comment analyser les performances via Blackfire.\nCes deux conférences sur le Strangler fig paterne, mon permis de mettre en place un micro projet dans un de nos projets, tout en le cloisonnant du code parent (règles d’or vérifié grâce à l’outil présenté deptrac). Ce principe nous permettra de le transformer en micro service très facilement.\n\nPHPStan: Advanced Types\nCette conférence centrée sur l’outil d’analyse statique de code : PHPStan, a été présentée par son créateur Ondřej Mirtes.\nIl a commencé par nous rappeler quelle est la différence entre un langage compilé et un langage interprété, le premier ne se compile pas s’il y a des erreurs alors que le second ne plante qu’à l’exécution. Le but de PHPStan est de nous aider à identifier toutes les erreurs sans avoir besoin d’exécuter le code.\n\n\n\nCet outil analyse toutes les fonctions, les propriétés, le typage PHP, mais aussi la PHPDoc. Ondřej nous a ensuite parlé de tous les types PHPStan avec des exemples, en voici quelques un qui ont marqué notre attention :\n\n  non-empty-array, non-empty-string\n  literal-string\n  integer-range, integer-mask, integer-maskof\n  conditional return types, union types, intersection types …\n\n\nIl finit en nous rappelant que l’utilisation de @var est une mauvaise pratique et qu’il vaut mieux renforcer le typage quitte à modifier la documentation des vendor via les Stub files.\n\nGNAP: The future of OAuth\n[Slides]\n\nRobin Chalas @chalas_r nous a présenté GNAP (Grant Negotiation and Authorization Protocol) : une initiative pour développer la prochaine génération de protocoles d’autorisation.\nPour mieux comprendre les enjeux, nous sommes repartis de l’historique d’oauth, ses évolutions et ses écueils. Le constat étant que même si de nombreux problèmes connus ont été résolus, aujourd’hui, pour bien utiliser OAuth 2, il faut lire une douzaine de RFC et s’assurer qu’elles soient pertinentes pour les différents cas d’utilisation.\n\nL’augmentation de la complexité du protocole dégrade l’expérience du développeur, ce qui va à l’encontre de son objectif principal qui est la simplicité pour les développeurs de clients.\n\nGNAP (prononcé “nap”) est une complète réécriture afin de répondre aux besoins en sécurité des applications modernes :\n\n  Pensé pour tous les clients, plateformes (pas uniquement web, possibilités de deeplinking mobile par exemple)\n  les interactions sont un concept clé\n  Du chiffrement partout et des mécanismes de rotation extensibles\n  Plusieurs Access Tokens / grant request\n  Gestion de l’identité intégrée\n  Plus developer friendly\n  Pas rétrocompatible avec OAUTH2\n\n\nCe protocole est toujours à l’état de brouillon, le groupe de travail a été monté en octobre 2020 et lors du dernier rassemblement (nov. 2022), aucune modification du protocole n’a été actée. Le speaker conclut sur la nécessité de commencer à travailler sur l’implémentation de ce protocole dans l’écosystème PHP afin de supporter ce nouveau standard dont la finalisation ne devrait plus tarder. \nPour aller plus loin https://oauth.xyz/\n\nA self-training journey to the certification Symfony\nCette conférence traite de la méthodologie et des bonnes pratiques pour obtenir la fameuse certification Symfony.\nEn effet, la conférencière, Camille Jouan, nous présente sa manière de préparer l’examen.\nElle commence par énoncer son plan d’action :\n\n  Rassembler un maximum d’informations (Symfony doc, site pour la préparation à la certification, etc)\n  Organiser un plan autour du quoi/comment/pourquoi\n  Faire une timeline avec les étapes prévues\n  Se fixer un objectif dans le temps\n\n\nL’idée est d’accepter que cela ne sera pas parfait et que des retouches vont y être apportées\n\nPour Camille, l’idéal serait de pouvoir faire un test blanc au bout d’un certain temps de préparation sans “grandes convictions” : le but étant de se familiariser avec l’exercice.\nSi les fonds sont disponibles, un training est proposé par Sensiolabs.\nEn fonction de cet examen blanc, ajuster son plan, se concentrer sur des parties qui doivent être approfondies.\nUn autre point sur lequel la conférencière a insisté est le monitoring : régulièrement faire un bilan sur son avancée pour s’adapter.\nDes outils comme Trello, Excel, Google permettent d’en avoir une vision globale.\n\nIl est important de parler de ce projet autour de soi, notamment auprès de ses proches pour avoir du soutien, mais également auprès de son entreprise qui peut éventuellement proposer une subvention et ou un aménagement du temps de travail.\n\nElle conclut son intervention par un dernier conseil : cette méthodologie est adaptable à la vie quotidienne et peut être utile dans d’autres situations.\n\nNotre retour d’expérience\nCette nouvelle édition de la SymfonyCon nous a permis de découvrir ou d’approfondir certaines connaissances. \nNous pouvons aussi nous rendre compte de notre travail quotidien et prendre du recul sur celui-ci.\nCette expérience anglophone était très enrichissante et les conférences proposées étaient variées.\nIl y avait de la résolution de problèmes techniques, des retours d’expériences ou encore de la télémétrie.\n\n\n"
} ,
  
  {
    "title"    : "Un onboarding facilité grâce à la revue de code!",
    "category" : "",
    "tags"     : " team",
    "url"      : "/2022/11/15/onboarding-revue-code.html",
    "date"     : "November 15, 2022",
    "excerpt"  : "Au sein des équipes de développement, une activité bien connue est celle de la revue de code, et \nplus \nprécisément de la revue du delta du code. Il s’agit de l’inspection par nos \npairs du code proposé par nos soins, qui se trouve ainsi commenté ...",
  "content"  : "Au sein des équipes de développement, une activité bien connue est celle de la revue de code, et \nplus \nprécisément de la revue du delta du code. Il s’agit de l’inspection par nos \npairs du code proposé par nos soins, qui se trouve ainsi commenté pour répondre aux \nexigences de qualité de l’équipe et du projet.\n\nLes risques d’incompréhensions inhérents à la communication écrite, de malentendus\nou encore les remarques malheureuses peuvent rendre cet exercice redouté tant par les \nrelecteurs et relectrices \nque \npar celles et ceux dont le code est relu.\n\nAvant d’arriver chez Bedrock, j’étais un peu \ninquiète. Je savais déjà que les 9 personnes de ma future équipe font tou(te)s de la revue de code. \nComment échangerons-nous? Saurai-je faire “bonne impression” via mes commentaires \nsur leur code ?\n\nEn arrivant, j’ai été très \nagréablement surprise de découvrir que l’équipe applique un \nstandard, celui des conventions de commentaires, ou conventional comments. Grâce à cela, mon \nonboarding a été grandement facilité et j’ai découvert une façon plus efficace d’écrire mes \ncommentaires !\n\nDisclaimer : cet article est inspiré de ma conférence “Revue de code : on n’est pas \nvenu-e-s pour\nsouffrir !” donnée à l’occasion du meet-up anniversaire Duchess chez Dataiku en 2022 et au Forum\nPHP 2022.\n\n\n\nPetit rappel : pourquoi faisons-nous des revues de code ?\n\nPasser en revue le code proposé par ses co-équipier(e)s est largement répandu dans les \néquipes de développeurs et développeuses. Bien sûr, la qualité du code en elle-même se trouve \naméliorée \ncar chacun apporte un regard neuf sur ce qui est proposé, mais ce n’est pas tout. La \nrevue de code est également une façon de nous tenir informé(e) de \nl’implémentation de nouvelles features, d’apprendre autant du métier que de la \ntechnique et enfin, d’apprendre à travailler ensemble.\n\n\n\nVoici une \npetite liste non exhaustive de l’intérêt de la revue de code :\n\n\n  Améliorer la qualité et la lisibilité du code grâce aux remarques de toutes les\npersonnes de l’équipe\n  Appliquer les standards adoptés par l’équipe (et les apprendre !)\n  Détecter et corriger les éventuels bugs fonctionnels\n  Favoriser la collaboration en équipe\n  Former les développeurs et développeuses au fur et à mesure des remarques\n  Partager les responsabilités : en approuvant une pull request ou une merge request, nous\nsommes responsables en tant qu’équipe du code ajouté/modifié au tronc commun !\n\n\n…Et parfois, on souffre\n\n\n\nMais parfois, ce n’est pas tout rose. Les commentaires qu’on laisse peuvent vexer. On \npeut nous-même être vexé. Car certains jours, on peut manquer d’empathie. On peut avoir \nl’impression d’être plus compétent(e) en \ncritiquant les autres, on veut se rassurer en se montrant plus qualifié(e). On peut également \nêtre habitué(e) à une culture de la compétition, nous poussant ainsi à faire des remarques désagréables à nos pairs.\n\nComment le formatage de commentaire a-t-il amélioré mon arrivée dans l’équipe ?\n\nLa standardisation des commentaires a énormément amélioré mon intégration dans \nl’équipe. En effet, grâce à cela :\n\n\n  J’ai pu rapidement me rendre compte de ce qui était bloquant / non bloquant et ainsi me \nconcentrer sur les actions essentielles et prioritaires à mener;\n  je n’ai pas eu à me poser de questions sur le ton employé par mes collègues ni sur leur \nintention;\n  j’ai pu rapidement faire moi-même des revues de code sans craindre d’être mal comprise;\n  j’ai eu des retours qui m’ont permis de progresser sur la connaissance du fonctionnel et \ndes standards de la team.\n\n\nAméliorer sa posture\n\nAvant de parler du standard, je vous propose de nous interroger sur \nnotre posture en tant que développeur et développeuse. Recevoir ou donner des commentaires, ce \nn’est pas aisé pour tous. Notre ego peut interférer et dégrader la qualité de nos \néchanges avec nos collègues. Aussi, avant de chercher à formater nos commentaires, nous pouvons \nnous interroger sur leur contenu.\n\n\n\nL’Egoless Programming, proposé par Gerald Weinberg en 1971 dans son livre The Psychology of \nComputer Programming, présente une dizaine de commandements pour nous \naider à progresser.\n\nLe principe est le suivant : réduire au minimum les facteurs personnels lors des interactions \navec ses pairs, pour favoriser le travail en équipe et produire le maximum de qualité.\n\n\n  Critiquez le code au lieu des personnes,\n  Soyez factuels sur le code,\n  N’attaquez jamais les personnes.\n\n\nJe vous recommande de regarder cette excellente conférence sur l’egoless programming, où Olivier \nThelu prend le temps de revenir sur tous les concepts :\n\n\nLes 10 commandements de la programmation sans &amp;eacute;go - Olivier Thelu - MiXiT 2022 from MiXiT on Vimeo.\n\nUne autre excellente conférence de Kim Laï Trinh, lead développeur, et son “Auto-critique de la \nrevue de code\nbienveillante”.\n\n\n\nFormatez vos commentaires !\n\nUne fois qu’on a adopté une posture qui nous aide à mieux recevoir et donner des commentaires dans le cadre de nos revues de code, on peut réfléchir à la façon dont on les formate.\n\nGrâce au standard des conventional comments, nous disposons d’une convention pour écrire des \ncommentaires clairs et visuels et limiter les incompréhensions. Chacun de nous est invité \nà réfléchir à l’intention de son commentaire avant de l’écrire.\n\nPar exemple, avec ce commentaire qui peut prêter à confusion (le OMG qui signifie “Oh my god” \npeut être autant interprété comme quelque chose de négatif que de positif, notamment ici puisque \nnous n’avons pas le contexte 😈) :\n\n\n\nCe commentaire peut être préfixé par praise, ce qui signifie éloge. Cela change radicalement \nle ton du commentaire.\n\n\n\nVoici un autre exemple laconique : Poubelle.\n\n\n\nCelui-ci peut être amélioré en étant préfixé par l’étiquette nitpick, qui signifie \n“tatillonner”, ce qui diminue également son ton dramatique. De plus, l’urgence peut être \nindiquée (ici, non-bloquant) et le contexte est décrit et peut être exploité grâce à un patch \nproposant un code de remplacement.\n\n\n\nLa compréhension du commentaire est facilitée par l’effort fourni pour ajouter le maximum de\ncontexte possible. On gagne en lisibilité grâce à la catégorisation (étiquette), qui nous permet\négalement d’immédiatement savoir de quoi on parle. Par exemple, une question ne sera pas lue\nde la même façon qu’une suggestion !\n\nLa contextualisation permet de savoir si on traite le retour immédiatement ou si on ouvre une\nnouvelle pull request plus tard, pour rémedier au point soulevé. On limite ainsi les\nquiproquos ou les pertes de temps sur des actions non prioritaires.\nEt surtout, on limite les mauvaises impressions sur le ton employé.\n\nDescription du standard\n\n&amp;lt;label&amp;gt; [decorations]: &amp;lt;subject&amp;gt;\n\n[discussion]\n\n\n\n  étiquette (label) : “étiquette” pour signifier de quel genre de commentaire il s’agit\n  sujet (subject) : le commentaire en lui-même\n  contexte supplémentaire (decorations) (optionnel)  : labels supplémentaires pour donner plus d’indications (entre parenthèses, séparés par des virgules).\nExemple : non-blocking, blocking, test …\n  discussion (optional) : contexte, raisonnement ou tout autre élément pour aider à \ncomprendre le « pourquoi » et les « prochaines étapes » pour résoudre le commentaire\n\n\nLes labels\n\nVoici la liste de labels ou étiquettes, extraits du standard :\n\n\n  praise\n  nitpick\n  suggestion\n  issue\n  todo\n  question\n  thought\n  chore\n  typo\n  polish\n  quibble\n\n\nL’équipe est, bien entendu, libre de choisir ou d’inventer ses labels ! Chez nous, le choix a été \nfait de respecter le standard tel qu’il est proposé, mais cela pourrait être rediscuté si besoin.\n\nVoici quelques définitions (pour le reste, se référer au site du standard):\n\nPraise (éloge)\n\nGrâce à ce commentaire, on souligne quelque chose de positif, on encourage la personne. Bien \nentendu, pas de second degré !\n\nSuggestion (suggestion)\n\nLes suggestions sont la majorité des commentaires qu’on laisse, en général. Il s’agit \nd’améliorations à apporter au sujet actuel. On cherchera à être explicite et clair,\n\n\n  expliquer en quoi il s’agit d’une amélioration;\n  utiliser des patchs;\n  utiliser des décorations blocking ou non-blocking.\n\n\nIssue (problème)\n\nGrâce aux issues, on met en évidence des problèmes spécifiques. Idéalement, on couple ce \ncommentaire avec une Suggestion.\n\nThought (pensée)\n\nLes pensées sont des idées qui surgissent lors de la relecture du code. Celles-ci ne sont pas \nbloquantes par nature, mais sont extrêmement précieuses, car elles peuvent conduire à des \npossibilités de mentorat.\n\nAppropriez-vous la méthode !\n\nBien entendu, vous n’êtes pas obligé d’utiliser toute la liste de labels proposée par le \nstandard. Vous pouvez en choisir quelques uns, ou bien carrément vous en inspirer et créer les \nvôtres. C’est le choix qu’ont fait Camille et son équipe, qu’elle décrit dans cet excellent \narticle \nsur les conventional Comments et l’utilisation des emojis.\n\nAinsi, l’équipe a porté son choix sur une liste d’étiquettes illustrées par des emojis, qui \ntraduisent à fois l’intention du commentaire et son urgence.\nVoici quelques exemples, tirés de l’article :\n\n🥜 peanuts\n❓ question\n💬 discussion\n🚨 alerte\n🚫 no-go\n👏 bravo\n⚠️ warning\n☠️ bad idea\n✨ magic\n🔥 burn-it-all\n\nQuelques autres bonnes pratiques d’onboarding\n\nBien entendu, il y a plein d’autres façons d’aider vos nouveaux développeurs ou\nnouvelles développeuses à découvrir le code. Voici quelques autres idées :\n\n\n  \n    On peut se familiariser avec le workflow d’une publication de pull request ou de merge \nrequest en faisant une petite modification (ajouter son nom dans un fichier, par exemple ?);\n  \n  \n    on peut être accompagné(e) d’un “buddy” qui nous est assigné à l’arrivée dans l’entreprise avec \nqui on fait les premières revues de code en direct, et pas par écrit.\n  \n\n\nUne dernière bonne pratique très largement répandue : si les échanges par commentaires sont trop \nnombreux sur une même pull request, pourquoi ne pas se retrouver directement et résoudre en pair \nles \npoints discutés ?\n"
} ,
  
  {
    "title"    : "Forum PHP 2022 - L’éléphant bleu n’a pas peur de la souris aux grandes oreilles",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2022/10/28/forum-php-afup-2022.html",
    "date"     : "October 28, 2022",
    "excerpt"  : "Pour cette édition du Forum PHP qui s’est déroulée à Disneyland Paris, Bedrock a vu les choses en grand : première fois\nsponsor Or sur un forum et pas moins de 32 Bedrockien(ne)s présent(e)s dont 4 en tant que conférencier(e)s !\nCette année encore...",
  "content"  : "Pour cette édition du Forum PHP qui s’est déroulée à Disneyland Paris, Bedrock a vu les choses en grand : première fois\nsponsor Or sur un forum et pas moins de 32 Bedrockien(ne)s présent(e)s dont 4 en tant que conférencier(e)s !\nCette année encore, le forum a été un moment privilégié pour les échanges, le partage et vous avez été nombreux(ses) à\nvenir nous rencontrer sur notre stand et nous avons été ravis de pouvoir échanger avec vous.\n\nLa richesse et la diversité des conférences ont fait de cette édition une grande réussite. Nous ne pouvons\nmalheureusement pas aborder toutes les conférences dans cet article, mais voici une sélection de 10 d’entre elles.\n\n\n\nBedrock au Forum PHP\n\nGrande première pour nous cette fois-ci : Bedrock avait un stand sur le forum PHP !\nNotre équipe technique, ainsi que des membres de l’équipe RH a participé à la présentation de\nl’entreprise. Nous avons pu, tous ensemble, vous accueillir sur le stand pour répondre aux questions sur notre activité,\nnotre organisation et la vie dans l’entreprise.\n\nDe leurs propres mots :\n\n\n  “Notre présence au forum fut une réussite collective, qui nous a permis de nous faire connaître et mettre en lumière BEDROCK en tant que structure à part entière, et non plus en tant que « M6 Web ».\nNos équipes étaient ravies que Bedrock soit sponsor et ont pu se challenger sur de réels “casses-têtes” et non pas sur leur code.\nDurant ces deux journées, le stand Bedrock a été un point de rendez-vous agréable pour nos équipes et pour les autres participants curieux d’apprendre à nous connaître.\nCette participation, au-delà d’être très formatrice pour la communauté Bedrock, a même parfois été assimilée à un « team building » pour reprendre les termes de nos collaborateurs, leur permettant de se retrouver dans un autre environnement et de participer à l’attractivité de leur entreprise.\nLes équipes Backend &amp;amp; RH étaient fières de représenter Bedrock en participant à la vie du stand et de répondre aux questions des passants.\n\n\n\n\nLes conférences qu’on retient\n\nJour 1\n\nThe PHP Foundation: The past, the present, and the future - Sebastian BERGMANN, Roman PRONSKIY\n\n\n  “PHP c’est simple, suffit de dégommer un mec, et ça n’existe plus…”\n                                  \t- Chuck Norris -\n\n\nCette vision à peine exagérée de l’ami Chuck était assez proche de la réalité jusqu’à l’année dernière. En effet,\njusque-là, l’écrasante majorité de la connaissance internet et des implémentations des features de PHP reposaient sur\nles épaules de Nikita Popov et Dmitry Stogov. Lesquels faisaient cela de manière non-professionnelle, comme un hobby,\nalors que PHP est aujourd’hui utilisé pour environ 70% des sites internets.\n\nTout ceci est résumé dans cet article (en anglais).\n\nCe qui nous amène à un principe bien connu et très problématique de bon nombre d’entreprises : le Bus Factor.\n\nBus Factor\n\nCe principe identifie le nombre de personnes qui doivent être renversées par un bus pour que la connaissance d’un\nprojet disparaisse et que ce dernier soit réellement mis en péril. Appliqué à PHP, cela signifie que si deux\npersonnes avaient été renversées par un bus ou avaient croisé le chemin de Chuck, l’ensemble de la communauté PHP\naurait bien transpiré.\n\nLa PHP Foundation\n\nEn 2021, Nikita Popov annonce son souhait de réduire drastiquement sa contribution au langage, et c’est là que\nla PHP Fundation est fondée autour de personnalités comme Sebastian BERGMAN et\nRoman PRONSKIY qui ont animé cette conférence à laquelle nous avons eu la chance d’assister.\n\nL’objectif est simple : récolter des fonds pour pouvoir rémunérer des personnes qui font évoluer le langage et\nrépartir la connaissance dudit langage entre un maximum de personnes pour minimiser le Bus Factor\n\nL’aspect juridique et financier de la chose est délégué à une structure\nnommée Open Collective, dont l’activité est 100% transparente.\nLes derniers chiffres font ainsi valoir qu’après déductions des charges, il reste cette année environ 580 000$ pour la\nfondation.\n\nLes fonds proviennent majoritairement de donations répartis comme suit :\n\n  77% provenant d’entreprises (JetBrains, Livesport, Symfony…)\n  23% provenant d’individus\n\n\nParmi les actuels core contributeurs on retrouve :\n\n  Arnaud Le Blanc\n  Derick Rethans\n  George P. Banyard\n  Ilija Tovilo\n  Jakub Zelenka\n  Máté Kocsis\n\n\nLe futur de PHP ?\n\nAvec cette stratégie de contractualisation des core-contributeurs et le souci de partager la connaissance, on est en\ndroit d’espérer que le risque de mise en péril du langage soit fortement réduit. Qui plus est, l’arrivée prochaine de\nla version 8.2 de PHP semble montrer que la nouvelle équipe a su faire avancer le projet, pour notre plus grand plaisir.\n\nWatch the clock - Andreas HEIGHL\n\nDans cette conférence, Andreas aborde la douloureuse problématique du temps en PHP\net plus précisément le cauchemar que sont les tests sur la notion de temps réel.\n\nAfin de répondre à cette problématique, il évoque et explique en détail tout le cheminement intellectuel pour la mise en\nplace de la PSR-20. Elle donnera la possibilité, grâce à une “simple” interface, de\ngérer plus facilement le temps réel et les tests associés. Malheureusement, la PSR-20 est actuellement encore à l’état\nde brouillon et ne sera probablement pas mise à disposition avant longtemps.\n\nAfin de nous faire patienter, Andreas a développé sa propre implémentation de l’interface Clock qui sera proposée dans\nla PSR-20 : https://packagist.org/packages/stella-maris/clock.\n\nComment être bien onboardé en tant que développeuse junior reconvertie ? - Amélie ABDALLAH\n\nAmélie, alternante et en reconversion, a donné sa première conférence pour faire un retour d’expérience des onboardings\nqu’elle a vécu dans ses 2 premières entreprises.\n\nPour sa première expérience en tant que développeuse elle se retrouve à devoir coder dès le premier jour. Une fois\npassée l’euphorie, je rappelle qu’elle est en reconversion et sur-motivée, elle se rend vite compte de tout ce qui ne\nva pas :\n\n\n  personne pour la former que ce soit techniquement ou concernant le métier\n  aucune présentation des autres équipes\n  aucun accompagnement de la part des supérieurs hiérarchiques\n  seule sur un projet où les règles métiers semblent complexes\n\n\nElle se sent rapidement perdue et son moral et sa motivation tombent en flèche.\n\nConcernant sa seconde entreprise, c’est tout l’inverse :\n\n\n  pas de code pour les alternants/reconverti(e)s avant 2 semaines\n  un système de marrainage/parrainage\n  un repo avec des exercices et bonnes pratiques pour progresser\n  une rencontre avec toutes les équipes pour se connaître mais surtout pour bien comprendre les différents métiers de ses collègues\n\n\nAmélie conclut en incitant toutes les entreprises à envisager des candidat(e)s en reconversion qui, à la condition\nd’être bien accompagné(e)s, seront sur-motivé(e)s et plein(e)s d’énergie.\n\nCela nous permet de faire l’état des lieux de notre processus d’onboarding. Même si on peut trouver des pistes\nd’amélioration, comme par exemple avoir une présentation des autres équipes et différents métiers bien plus tôt, on\nconstate que nous avons déjà mis en place beaucoup de bonnes pratiques :\n\n\n  calendrier d’onboarding clair avec installation de la machine, présentation de l’équipe rejoint par la nouvelle personne, des locaux, des projets, etc.\n  système de marrainage/parrainage,\n  des formations disponibles sur les différentes technologies utilisées\n  ce qu’on appelle la Bedrock Academy, qui donne une vision globale des métiers de l’entreprise\n  un rapport d’étonnement présenté par le nouvel arrivant qui nous permet d’améliorer notre onboarding en continu\n\n\nFFI : De nouveaux horizons pour PHP - Pierre PELISSET\n\nChez Karafun, Pierre Pelisset nous présente un moyen de dépasser les frontières habituelles de PHP.\n\nLa mise en place de bar à karaoké nécessite en effet de manipuler du matériel spécifique. La transmission de données par\nUSB reste le moyen le plus simple de faire communiquer des systèmes, mais nativement rien n’existe en PHP. Jusqu’à\nprésent les équipes de Karafun utilisaient un script Python avec PySerial pour répondre à ce besoin.\n\nDepuis PHP 7.4, il est possible d’utiliser les Foreign Function Interface (FFI)\npour appeler directement des libraires C compilées depuis le code PHP. En se basant sur les bonnes librairies, il a été\nainsi possible pour les équipes de Karafun d’utiliser PHP dans la totalité de leur stack, et de remplacer leurs scripts\nPython.\n\nPierre nous présente ensuite la librairie php-termios qu’il a implémentée\npour permettre d’utiliser Termios, une librairie de manipulation de terminal POSIX écrite en C, depuis PHP.\n\nL’interfaçage reste facile tant qu’il s’agit de types de données simples (comme des entiers), mais devient beaucoup plus\ncomplexe lorsqu’il s’agit de manipuler des structures de données avancées (chaînes de caractères, objets, …).\n\nIdéalement, il faut créer des classes PHP ressemblant à la librairie C (mêmes noms de fonction, de constante, …), afin\nde faciliter sa manipulation.\n\nL’utilisation des FFI introduit de nouvelles difficultés liées généralement aux programmes compilés. Ainsi, dans le cas\nde Termios, les constantes utilisées varient selon la plateforme (Linux, MacOS, …). Il a donc fallu utiliser quelques\nastuces pour les copier dynamiquement dans le code PHP. De même, pour distribuer le binaire compilé, il faut prendre en\ncompte l’architecture CPU de la machine exécutant le code.\n\nOpenTelemetry : vers un standard pour surveiller vos applications - Benoit Viguier\n\nLors de sa conférence, Benoit Viguier (développeur chez Platform.sh au sein de l’équipe de Blackfire.io) est venu parler\nd’un standard de monitoring soutenu par la Cloud Native Computing Foundation : OpenTelemetry. Il a d’abord commencé par\nnous rappeler pourquoi nous faisons du monitoring.\n\nLe monitoring nous permet de savoir si nos services fonctionnent correctement (conforme au SLA|SLO), et si cela n’est\npas le cas, de savoir pourquoi cela ne fonctionne pas. Pour faire cela, il nous a présenté les différentes solutions de\nmonitoring qui existent, de la plus simple à mettre en place (Analytics), au plus|trop détaillé (logs) mais aussi via\ndes metrics qui eux demandent une plus grande intégration.\n\nEnsuite vient la présentation des promesses de ce nouveau standard (OpenTelemetry) et sa volonté d’uniformiser les trois\npiliers du monitoring : logs, metrics et les traces, avec la volonté de rendre interopérable ces données collectées avec\nn’importe quel service et ce qu’importe le langage.\n\n\n\nAfin de permettre cette interopérabilité pour la collecte, le traitement et l’envoi des données entre nos applications\net nos APMs, OpenTelemetry propose l’utilisation d’un collecteur. Ce collecteur possède trois composants par lesquels\nnotre donnée va transiter :\n\n\n  Receiver - Gère la récupération des données dans le collecteur. Il fonctionne aussi bien avec une mécanique de push que de pull et supporte nativement les protocoles HTTP et gRPC.\n  Processor - Permet le traitement des données avant l’envoi aux différents outils de monitoring.\n  Exporter - Envoie les données (via push ou pull) aux outils de monitoring.\n\n\nConcernant l’utilisation du standard OpenTelemetry en PHP, il existe aujourd’hui un SDK qui, cependant, ne possède\npas beaucoup de fonctionnalités.\n\nMalgré son jeune âge (release v1.0 en 2021), OpenTelemetry a encore de beaux jours devant lui si la contribution et le\nsoutien de la communauté continuent. Il est important de rappeler que le standard n’est pas encore finalisé, par exemple\nla fonctionnalité “logging” est encore en cours de développement et n’est disponible qu’en “draft”.\n\nJour 2\n\nTypage en PHP, comment ça fonctionne ? - George BANYARD\n\nLors de cette dissection du typage PHP, George Banyard, développeur salarié de la PHP Foundation, nous a expliqué (ou du\nmoins tenté) comment fonctionne le typage PHP grâce à des formules mathématiques. Mais avant de nous faire peur avec les\nformules, il nous d’abord rappelé les différents types existants et futurs (PHP 8.1 et 8.2) et comment ils sont\nreprésentés en C (zend_type, _zval_struct, _zend_class_entry)\n\n\n  Types Primitifs\n  Types définis en espace utilisateur (classe, interfaces, enum)\n  Types Littéraux (false, true)\n  Type callable\n  \n    \n      \n        \n          Types Composés (A&amp;amp;B, A\n          B, Forme Normale Disjonctive)\n        \n      \n    \n  \n  Alias de Type (PHP 8.2)\n\n\nEnsuite, il nous a expliqué le principe de substitution de Liskov de différente manière.\n\n  Si φ(x) est une propriété démontrable pour tout objet x de\ntype T, alors φ(y) est vraie pour tout objet y de type S tel que\nS est un sous-type de T\n\n\nMathématique, en C mais aussi en image :\n\n\n\nEn version très simplifiée, la substitution de Liskov permet de substituer un type par un autre type s’il est\nmieux-disant. C’est par exemple sur ce principe que sont fondées la co-variance et la contra-variance en PHP.\n\nAprès toutes ces informations, George nous a fait rêver avec les nouveautés qui pourraient arriver dans notre langage préféré :\n\n\n  La possibilité de définir nos alias de type (numeric qui serait int ou float)\n  Pouvoir définir un typage pour les paramètres et le retour des callable directement dans la fonction : foo(fn&amp;lt;int,string&amp;gt;:bool $callable) {}\n  Le paramètre in-out qui permet de vérifier que le type ne change pas entre l’entrée et la sortie d’une fonction, utile notamment dans le cas de passage par référence\n  Pouvoir créer des types génériques (par exemple class Collection&amp;lt;string&amp;gt;)\n\n\nVous pouvez retrouver les slides de la conférence ici\n\nProtéger votre application avec l’en-tête HTTP de sécurité “Content Security Policy” - Laurent BRUNET\n\nDurant ce talk, Laurent BRUNET nous a rappelé l’importance de sécuriser les sites internet en nous parlant des attaques\nles plus utilisées et dont on pourrait se défendre en exploitant correctement le header de réponse Content-Security-Policy (aka. CSP).\n\nConcrètement, le header CSP est délivré par le serveur en même temps que le HTML d’une page web et contraint les\nnavigateurs à s’assurer que le contenu de la page respecte les règles de sécurité définies grâce à de nombreuses directives.\n\nIl existe 2 variantes de ce header qui peuvent être utilisées conjointement :\n\n\n  une forme bloquante qui empêchera tous contenus illicites d’être chargés par le navigateur\nContent-Security-Policy: &amp;lt;directive1&amp;gt; ;  &amp;lt;directive2&amp;gt; ;  &amp;lt;directiveN&amp;gt;\n  une forme non-bloquante qui permet uniquement aux développeurs d’être avertis si du contenu illicite est présent sur la page\nContent-Security-Policy-Report-Only: &amp;lt;directive1&amp;gt; ;  &amp;lt;directive2&amp;gt; ;  &amp;lt;directiveN&amp;gt;\n\n\nEt voici quelques unes des failles de sécurité évoquées durant ce talk, chacune accompagnée d’une des directives permettant de s’en protéger :\n\n\n  on peut se protéger des attaques XSS (e.g. injection de scripts dans une page web permettant d’exploiter les accès d’un utilisateur) en whitelistant les scripts autorisés à s’exécuter dans votre application avec script-src &#39;nonce-MonIDDeScript&#39; &#39;sha256-MonHashDeScript&#39; &#39;strict-dynamic&#39; https: &#39;unsafe-inline&#39; *.example.com ;\n  le Clickjacking (e.g. injection d’iframe invisible dans laquelle l’utilisateur va cliquer sans le savoir) peut être facilement contré en whitelistant vos iframes et celles de vos partenaires avec frame-ancestors &#39;self&#39; https://example.com ;\n  la faille Magecart (e.g. récupération des données bancaires en les envoyant vers un nom de domaine pirate difficile à détecter) disparaîtra en whitelistant les noms de domaines utilisés dans des appels AJAX avec connect-src &#39;self&#39; https://example.com ;\n\n\nSi le sujet vous intéresse, n’hésitez pas à consulter le talk de Laurent dès qu’il sera disponible en replay. En\nattendant, profitez de la documentation du MDN et sachez\nque Google fournit un service en ligne pour inspecter et valider les CSP de vos\npages web.\n\nTester à travers OpenAPI, ou comment valider votre documentation ! - Stéphane Hulard\n\nStéphane Hulard a commencé sa conférence par nous rappeler ce qu’est OpenAPI : il s’agit\nd’une initiative qui vise à normaliser et standardiser la description d’APIs. Cela sert à l’interopérabilité,\nl’automatisation et la fiabilité.\n\nIl faut voir la documentation comme une spécification de notre API. “Une documentation n’a de sens que si elle reflète\nl’état actuel de l’application.” Rien de mieux donc que d’intégrer la validation de notre documentation par rapport à\nnotre code, et inversement : que notre documentation valide notre code !\n\n\n\nthephpleague nous propose une solution pour faire ça : openapi-psr7-validator.\nCe paquet peut valider les messages PSR-7 par rapport aux spécifications OpenAPI (3.0.x) exprimées en YAML ou JSON.\n\nCe paquet se base sur les PHP Standards Recommendations (PSR) qui sont des textes décrivant une manière commune de\nrésoudre un problème spécifique. De cette façon, les projets qui suivent ces recommandations auront une excellente\ninteropérabilité en suivant les mêmes recommandations et contrats.\n\nStéphane nous parle ensuite de la librairie Raven qu’il vient de publier. Raven a\npour but de faciliter la validation au travers de la documentation, de s’appuyer sur de vraies données pour valider les\nrequêtes et réponses ainsi que valider le comportement de l’API testée. La librairie n’en est qu’à ses débuts, quelques\nissues sont remontées sur le repository, n’hésitez pas à contribuer !\n\nStéphane a mis à disposition les slides de sa conférence ici\n\nFrankenPHP, dans les entrailles de l’interpréteur PHP, de machines virtuelles et des threads - Kévin DUNGLAS\n\nChez Bedrock, nous utilisons majoritairement nginx et php-fpm pour servir nos applications. Nous avons aussi fait des\nessais avec Road Runner comme alternative.\n\nDurant cette conférence, Kévin DUNGLAS nous a présenté une nouvelle alternative écrite en Go et nommée frankenphp.\nChouette effet d’annonce au passage, FrankenPHP a été ouvert au public en direct durant la conférence.\n\nEn plus d’avoir un logo très mignon, ce nouveau serveur d’application PHP promet un gain de performance en gardant en\nmémoire l’application chargée. Autre argument intéressant, la facilité d’installation et d’utilisation.\n\nBien qu’il a plusieurs fois précisé que ce n’était pas prêt pour la production, cette nouvelle approche semble\nprometteuse. Il est probable que nous ne tardions pas à l’essayer pour voir s’il est possible de gagner en performance.\n\nLes conférenciers Bedrock\n\n\n\nCette année, ce ne sont pas moins de 4 présentations qui étaient données par des personnes de chez Bedrock.\n\nVous pourrez les trouver très bientôt en replay. Les liens seront partagés sur ce blog et les vidéos disponibles\nsur la chaîne youtube de l’afup.\n\nEn attendant, et pour rappel, il s’agissait des conférences suivantes :\n\nComprenez comment PHP fonctionne, vos applications marcheront mieux - Pascal MARTIN\n\nÀ l’échelle à laquelle nous travaillons, avec des millions de personnes sur nos plateformes tous les jours, nous découvrons, atteignons, voire dépassons régulièrement des limites de l’approche traditionnelle de PHP et de php-fpm. Au cours de ce talk, Pascal souhaitait partager notre expérience de travail avec PHP, sur des sujets souvent peu connus par les développeurs et développeuses, pour aider le public à créer des applications qui répondent mieux aux attentes de leur public.\n\n\n  Pour exécuter du code, PHP consomme du processeur et de la mémoire. Quand une requête HTTP arrive, un processus php-fpm lui est dédié. Mais ces ressources sont limitées. Et, même dans le Cloud ou en serverless, scaler prend du temps et les coûts s’envolent !\n\n\n\n  Savez-vous combien de CPU et de RAM votre application réclame ? Et pendant quelle durée ? Si non ou sans comprendre « pourquoi », difficile de développer efficacement et de dimensionner un hébergement pérenne ! Peut-être que ça marche… Sur votre poste. Ou pendant un moment, en gaspillant de l’argent et des ressources. Mais l’expérience prouve que, tôt ou tard, ces questions vous rattraperont.\n\n\n\n  Cycle de vie de PHP, communication entre nginx et php-fpm, approche shared-nothing, compilation et cache d’opcodes, gestion interne de la mémoire ou même architecture logicielle et debugging… Pour qu’une application réponde aux attentes de son public, nous devons comprendre comment PHP fonctionne !\n\n\nSauve un-e dév, écris une doc ! - Sarah HAÏM-LUBCZANSKI\n\n\n  Vous êtes développeur ou développeuse PHP : vous aimez programmer, réfléchir. Vous aimez créer des applications ou des bibliothèques de qualité. Mais pourquoi personne ne les utilise ? Parce que votre documentation n’est pas à la hauteur !\n\n\n\n  Justement : je suis Technical Writer et mon métier est de vous aider à valoriser votre logiciel auprès de ses utilisateurs et utilisatrices, à travers une bonne doc. Comprenons comment architecturer, concevoir et rédiger votre contenu. Découvrons les outils qui vous procureront une aide précieuse. Enfin, facilitons sa mise à jour pour qu’elle soit pérenne.\n\n\n\n  Dorénavant, vous saurez identifier les passages obligés et ceux où vous pouvez gagner du temps.\n\n\nRevue de code : on n’est pas venu pour souffrir ! - Anne-Laure DE BOISSIEU\n\n\n  J’ai rejoint ma nouvelle équipe il y a 6 mois, avec une appréhension. Comment allais-je vivre les revues de code par des collègues que je ne connais pas encore ? Incompréhensions, malentendus : la communication écrite rend cet exercice très délicat. Vous avez été blessé-e par un commentaire ? Etait-il vraiment mal intentionné ? Vous avez blessé quelqu’un sans le vouloir, à cause d’une tournure maladroite ?\n\n\n\n  Dans mon équipe, j’ai découvert un cadre qui m’a permis de me sentir bien accueillie dès mon arrivée. En adoptant une posture et une convention bien adaptée, on peut largement diminuer le risque de mal se comprendre. Non seulement on communique mieux, mais on améliore la qualité globale du projet.\n\n\n\n  Vous n’aurez plus aucune raison de souffrir !\n\n\nBFF, notre best friend forever pour faire plein d’applications frontend ? - Valentin CLARAS\n\n\n  Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application étant déployée sur de nombreux appareils (ordinateur, mobile, set top box, tv connectée, consoles de jeux, tv stick etc …). Il était devenu très difficile de gérer la création et l’évolution de ces nombreuses applications qui requêtaient et formataient chacune elles-mêmes les données dont elles avaient besoin.\n\n\n\n  Pour cela, en 2018, nous avons décidé de nous lancer dans la création d’un Back For Front afin d’unifier et faciliter les interactions backend et frontend. Cette conférence fut l’occasion de passer en revue :\n  \n    les concepts du Back For Front\n    l’architecture api-gateway et micro service mise en place\n    les gains fonctionnels et la vélocité gagnée\n    les différents mécanismes développés pour absorber les importants pics de charge (résilience, circuit breaker, fallbacks etc.)\n    les impacts techniques et organisationnels d’une telle architecture\n  \n\n\n\n  Aujourd’hui notre API Gateway BFF opère 92 frontends délivrant 1.5 milliards de vidéos par an pour 45 millions d’utilisateurs actifs (MaU).\nVous pourrez trouver un complément d’informations au sujet de notre BFF dans la suite d’articles dédié.\n\n\nPour conclure\n\nNous sommes revenus la tête pleine de nouvelles idées. Ces deux jours de conférences nous ont permis de montrer le savoir faire présent chez Bedrock et nous avons aussi pu nous inspirer des connaissances d’autres personnes. Les formats variés répondaient aux goûts de chacun(e) et ont rendu ce forum unique.\n\nLes nombreuses activités proposées entre chaque conférence permettaient d’échanger entre pairs et comme toujours la communauté a été mise à l’honneur avec la construction d’une fresque LEGO représentant tous les logos des antennes de l’AFUP.\n\nMerci à tou(te)s les conférencier(e)s pour leur travail incroyable et merci l’AFUP pour l’organisation de ce superbe évènement !\n\n\n\nVivement l’année prochaine !\n"
} ,
  
  {
    "title"    : "Sauve un-e dév, écris une doc !",
    "category" : "",
    "tags"     : " conference, afup, forumphp, doc",
    "url"      : "/2022/10/13/sauve-une-dev-ecris-une-doc.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Vous êtes développeur ou développeuse PHP : vous aimez programmer, réfléchir. Vous aimez créer des applications ou des bibliothèques de qualité. Mais pourquoi personne ne les utilise ? Parce que votre documentation n’est pas à la hauteur !\n\nJustem...",
  "content"  : "Vous êtes développeur ou développeuse PHP : vous aimez programmer, réfléchir. Vous aimez créer des applications ou des bibliothèques de qualité. Mais pourquoi personne ne les utilise ? Parce que votre documentation n’est pas à la hauteur !\n\nJustement : je suis Technical Writer et mon métier est de vous aider à valoriser votre logiciel auprès de ses utilisateurs et utilisatrices, à travers une bonne doc. Comprenons comment architecturer, concevoir et rédiger votre contenu. Découvrons les outils qui vous procurerons une aide précieuse. Enfin, facilitons sa mise à jour pour qu’elle soit pérenne.\n\nDorénavant, vous saurez identifier les passages obligés et ceux où vous pouvez gagner du temps.\n"
} ,
  
  {
    "title"    : "Revue de code : on n’est pas venu pour souffrir !",
    "category" : "",
    "tags"     : " conference, afup, forumphp, revue",
    "url"      : "/2022/10/13/revue-de-code-on-n-est-pas-venu-pour-souffrir.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "J’ai rejoint ma nouvelle équipe il y a 6 mois, avec une appréhension. Comment allais-je vivre les revues de code par des collègues que je ne connais pas encore ? Incompréhensions, malentendus : la communication écrite rend cet exercice très délica...",
  "content"  : "J’ai rejoint ma nouvelle équipe il y a 6 mois, avec une appréhension. Comment allais-je vivre les revues de code par des collègues que je ne connais pas encore ? Incompréhensions, malentendus : la communication écrite rend cet exercice très délicat. Vous avez été blessé-e par un commentaire ? Etait-il vraiment mal intentionné ? Vous avez blessé quelqu’un sans le vouloir, à cause d’une tournure maladroite ?\n\nDans mon équipe, j’ai découvert un cadre qui m’a permis de me sentir bien accueillie dès mon arrivée. En adoptant une posture et une convention bien adaptée, on peut largement diminuer le risque de mal se comprendre. Non seulement on communique mieux, mais on améliore la qualité globale du projet.\n\nVous n’aurez plus aucune raison de souffrir !\n"
} ,
  
  {
    "title"    : "Comprenez comment PHP fonctionne, vos applications marcheront mieux, Forum PHP 2022",
    "category" : "",
    "tags"     : " conference, afup, forumphp, php",
    "url"      : "/2022/10/13/comprenez-comment-php-fonctionne-vos-applications-marcheront-mieux.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Pour exécuter du code, PHP consomme du processeur et de la mémoire. Quand une requête HTTP arrive, un processus php-fpm lui est dédié. Mais ces ressources sont limitées. Et, même dans Le Cloud ou en serverless, scaler prend du temps et les coûts s...",
  "content"  : "Pour exécuter du code, PHP consomme du processeur et de la mémoire. Quand une requête HTTP arrive, un processus php-fpm lui est dédié. Mais ces ressources sont limitées. Et, même dans Le Cloud ou en serverless, scaler prend du temps et les coûts s’envolent !\n\nSavez-vous combien de CPU et de RAM votre application réclame ? Et pendant quelle durée ? Si non ou sans comprendre « pourquoi », difficile de développer efficacement et de dimensionner un hébergement pérenne ! Peut-être que ça marche… Sur votre poste. Ou pendant un moment, en gaspillant de l’argent et des ressources. Mais l’expérience prouve que, tôt ou tard, ces questions vous rattraperont.\n\nCycle de vie de PHP, communication entre nginx et php-fpm, approche shared-nothing, compilation et cache d’opcodes, gestion interne de la mémoire ou même architecture logicielle et debugging… Pour qu’une application réponde aux attentes de son public, nous devons comprendre comment PHP fonctionne !\n"
} ,
  
  {
    "title"    : "BFF, notre best friend forever pour faire plein d’applications frontend ?",
    "category" : "",
    "tags"     : " conference, afup, forumphp, php, bff",
    "url"      : "/2022/10/13/bff-notre-best-friend-forever-pour-faire-plein-d-applications-frontend.html",
    "date"     : "October 13, 2022",
    "excerpt"  : "Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application étant déployée sur de nombreux appareils (ordinateur, mobile, set top box, tv connecté, consoles de jeux, t...",
  "content"  : "Chez Bedrock nous fournissons des applications de streaming (ASVOD, AVOD) pour plusieurs clients en France et en Europe, chaque application étant déployée sur de nombreux appareils (ordinateur, mobile, set top box, tv connecté, consoles de jeux, tv stick etc …). Il était devenu très difficile de gérer la création et l’évolution de ces nombreuses applications qui requêtaient et formataient chacune elles-mêmes les données dont elles avaient besoin.\n\nPour cela, en 2018, nous avons décidé de nous lancer dans la création d’un Back For Front afin d’unifier et faciliter les interactions backend et frontend. Au cours de cette conférence nous passerons en revue :\n\n\n  les concepts du back for front\n  l’architecture api-gateway et micro service mise en place\n  Les gains fonctionnels et la vélocités gagnée\n  les différents mécanismes développés pour absorber les importants pic de charge (résilience, circuit breaker, fallbacks etc.)\n  les impacts techniques et organisationnels d’une telle architecture\n\n\nAujourd’hui notre API Gateway BFF opère 92 frontends délivrant 1.5 milliards de vidéos par an pour 45 millions d’utilisateurs actifs (MaU). Venez découvrir notre retour d’expérience sur la mise en place d’un tel projet.\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #18",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/10/07/bedrock-dev-facts-18.html",
    "date"     : "October 7, 2022",
    "excerpt"  : "C’est maintenant l’automne 🍁🎃, on vous propose les devfacts de cette fin d’été et il y a du lourd !\n\nCogito ergo sum\n\n\n  Le seul truc que je “sais” c’est “on paye 200 par mois pour avoir un direct connect” ; mais peut-être que je sais que je ne sa...",
  "content"  : "C’est maintenant l’automne 🍁🎃, on vous propose les devfacts de cette fin d’été et il y a du lourd !\n\nCogito ergo sum\n\n\n  Le seul truc que je “sais” c’est “on paye 200 par mois pour avoir un direct connect” ; mais peut-être que je sais que je ne sais rien.\n\n\n👋🔥🙊🎉\n\n\n  Je te fais confiance, je te laisserai mettre des emoji sur ma tombe\n\n\nLa priorité c’est important\n\n\n  C’est plus important que des trucs moins importants.\n\n\nBiafine nécessaire\n\nUn QA voulant faire une vanne au morning\n\n  Le QA: “Qu’est-ce qui a deux lettres et qui marche pas?”\n\n  Réponse du dev: “La QA ?”\n\n\n3615 croissants\n\nParlant d’une évolution majeure\n\n  Est-ce que c’est OK pour vous pour qu’on MEP aujourd’hui ou pas ?\n\n  À priori pas de contre-indications. Peut-être acheter directement une boulangerie ?\n\n\nSugar !\n\n\n  C’est du sucre syntaxique\n\n  Quand il y a trop de sucre, tu risques du diabète\n\n  Surtout que le diabète syntaxique, c’est le pire\n\n\nLA metric\n\n\n  Je crois qu’on est bon, on a pas plus d’erreur que d’user.\n\n\nLe verre à moitié plein\n\n\n  Du coup ça a cassé je suppose?\n\n  Ouais, enfin pas totalement, presque un succès.\n\n\nLa montagne ça vous gagne\n\nEn parlant de son ascension du Ventoux à vélo:\n\n\n  “Moi je pensais vraiment que j’étais prêt”\n\n\nMais pourquoi ?\n\nEssayant de comprendre pourquoi une commande met beaucoup de temps sur sa machine.\n\n\n  Je suis sur master, branche à jour, yarn réalisé juste avant, et je pense pas que mon PC soit particulièrement atteint de maladie congénitale\n(Enfin, j’espère pas?)\n\n\nRead me please\n\n\n  Tout est dans le README (que les gens liront pas de toute façon mais bon).\n\n\nLes étoiles ⭐️\n\n\n  Être responsable d’un incident, c’est pas marrant c’est sûr. \nMais ça forme !\nCasser la prod et la réparer ça donne une étoile sur le maillot, et je peux te dire qu’on est plusieurs ici à avoir plus d’étoiles que le drapeau américain.\n\n\nReview main dans la main\n\n\n  La peer review, c’est la meilleure des reviews.\n\n\nPatterns\n\n\n  A : “Je pense que je vais devoir faire un singleton pour ça…”\n\n  B : “Sinon, tu devrais pouvoir aussi faire des multitons ?”\n\n  C : “On appelle ça une boîte de conserve.”\n\n\nම\n\n\n  Ça devait être en base64 des Maya\n\n\nTime is relative\n\n\n  Je pense que la première fois ce sera long …et après ce ne sera pas long, mais ce sera long quand même.\n\n\n:bim:\n\n\n  Alors l’intelligence elle existe déjà, elle s’appelle vous-même… mais actuellement elle n’est pas disponible.\n\n\n☕️\n\n\n  Tu peux mettre autant de code brouette dans tes pull request que de sucre dans ton café : L’important, c’est que ça soit bien dilué. \nMoi, je ne sucre pas mon café.\n\n\n🔍\n\n\n  Normalement j’utilise un vrai IDE. Mais je ne sais pas monter le zoom sur PHPStorm\n\n\nEncore une histoire de date\n\n\n  C’est une fois par semaine, les releases mensuelles ?\n\n"
} ,
  
  {
    "title"    : "API Platform Conference 2022",
    "category" : "",
    "tags"     : " conferences, backend, api, php",
    "url"      : "/2022/10/07/api-platform-conference-2022.html",
    "date"     : "October 7, 2022",
    "excerpt"  : "En cette période de rentrée, Bedrock participait à l’API Platform Conference 2022, où nous avons eu le plaisir d’assister à une partie des conférences proposées. Un grand merci à toutes les personnes chez Les-Tilleuls.coop pour l’organisation de c...",
  "content"  : "En cette période de rentrée, Bedrock participait à l’API Platform Conference 2022, où nous avons eu le plaisir d’assister à une partie des conférences proposées. Un grand merci à toutes les personnes chez Les-Tilleuls.coop pour l’organisation de cet évènement !\nPour cette seconde édition, le programme était réparti sur deux jours, les 15 &amp;amp; 16 septembre 2022.\n\nEn introduction à cette conférence, Kévin Dunglas, créateur d’API Platform, a mis en ligne la version 3.0.0 du framework en nous présentant certaines nouvelles fonctionnalités développées telles que le support natif de XDebug. Il a profité de l’occasion pour présenter un petit historique d’API Platform.\n\nDomain-driven design with API Platform 3\n\nLors de cette conférence, Robin Chalas et Mathias Arlaud nous ont parlé de l’utilisation d’API Platform dans le cadre du Domain Driven Development et de l’Architecture Hexagonale.\nLes présentateurs ont commencé cette présentation par plusieurs rappels et présentations sur des sujets comme :\n\n  Domain Driven Design\n  Structures hexagonales\n  CQRS\n\n\nCes rappels ont permis d’enchainer sur l’utilisation du framework dans ce contexte à travers un exemple de projet DDD utilisant API Platform 3 et suivant l’architecture hexagonale.\n\nIls expliquent comment implémenter API Platform dans notre code en détaillant plusieurs points :\n\n  L’implémentation des providers côté query\n  L’implémentation des processors côté command\n  Le système d’opération\n  Les providers/processors qui appellent l’app via les bus\n\n\nComment Alice Garden’s gère-t-elle son code métier via les évènements\n\nNous avons pu assister à la conférence “Comment Alice Garden’s gère-t-elle le code métier via des évènements ?” proposée par leur technical architect Nicolas Lemahieu. Tout d’abord, il nous a présenté le contexte de son entreprise Alice Garden’s qui fait de la vente de mobilier d’extérieur. En se basant sur la stack technique déjà présente : Symfony, RabbitMQ, MariaDB et sans tout refondre, comment faire pour mieux gérer le code métier actuellement éparpillé un peu partout dans le code.\n\nIls utilisaient beaucoup de subscribers Doctrine, ce qui entraîne plusieurs problèmes :\n\n  Des subscribers nombreux = plus de logique au flush\n  Code plus difficile à maintenir et à comprendre\n  L’augmentation du risque de boucles infinies implique que chaque changement entraîne des boucles sur le UnitOfWork\n  Duplication de code\n  Code fortement couplé à Doctrine et manque de typage\n  Du côté du profiler Symfony, cela devient compliqué aussi dès qu’on commence à en avoir beaucoup\n  Les tests sont compliqués :\n    \n      Unitaires quasi impossibles,\n      Fonctionnels possibles, mais demandent beaucoup de ressources en temps et donc d’argent\n    \n  \n\n\nNicolas Lemahieu a donc présenté les différentes solutions envisagées ainsi que leurs avantages et inconvénients :\n\n  Domain Driven Development : séparation très nette du métier et de l’infra, mais demande beaucoup trop d’effort à mettre en place, car aucune correspondance avec les bundles déjà existants (risque de régression trop haut, coût de développement trop grand…)\n  Garder les évènements et s’affranchir de Doctrine : c’est la solution qui a été retenue parce qu’elle permettait de réutiliser un maximum de l’existant\n\n\nPuis, nous avons pu apprendre comment implémenter cette solution :\n\n  Création d’une abstraction supplémentaire “BusinessObject” : nouveau dossier Business dans src où :\n    \n      Entité = objet métier\n      Méthodes des entités = règles métier\n      Toutes les interfaces entité étendent BusinessObjectInterface\n    \n  \n  Classes abstraites dans Business\n  Implémentation d’events custom pour chaque objet métier et par type d’event\n  Event provider : fournit les évènements qui sont mis dans une collection puis tag dans les services\n\n\nEn conclusion, chez Alice Garden’s, ils ont réussi à n’avoir qu’un seul Doctrine Subscriber, donc une seule boucle de UnitOfWork et des tests facilités, car c’est seulement du PHP. La dépendance à Doctrine est éliminée et il suffira de déplacer le provider pour adapter le code à une autre infrastructure.\n\nRéutiliser et partager vos opérations personnalisées avec API Platform\n\nGrâce à Hubert Lenoir et Jérémy Jarrié de l’entreprise SensioLabs, nous avons pu apprendre à réutiliser et partager les opérations avec API Platform. Ils utilisent 3 API REST faites avec API Platform v3. Des opérations génériques comme “liker” peuvent s’appliquer sur des ressources différentes (articles, photos, pages, etc.), on peut donc réutiliser du code.\n\nLes principes pour faire cette utilisation générique de code sont simples :\n\n  Un seul contrôleur pour plusieurs ressources = un contrôleur de comportement\n  Une interface pour que les entités puissent adopter ce comportement\n  Trait pour les méthodes du comportement (1 à n comportements, donc classe abstraite ou interface impossible)\n  Ajouter des services intermédiaires\n\n\nIl survient un seul problème avec ce pattern : la duplication des annotations API Platform. La solution est d’ajouter des décorations (design pattern decorator) sur les métadatas d’API Platform. Il est donc simple de créer des comportements indépendants des ressources qui pourront être facilement réutilisés. Ce projet était encore à l’état de POC, mais Jérémy et Hubert allaient mettre à jour la version plus aboutie sur le repository GitHub.\n\nLa revue de code est un art\n\nSmaine Milianni a proposé une conférence sur les conseils à suivre afin d’effectuer une revue de code. Nous avons pu réaliser qu’au sein de Bedrock nous appliquions déjà de nombreux principes.\n\nNous appliquons déjà :\n\n  Un template de PR pour décrire les caractéristiques du bug ou de l’US :\n    \n      Quoi, pourquoi, comment, comment tester, etc.\n    \n  \n  Des noms de commits explicites\n  La bienveillance\n  Le challenge du code des autres personnes\n  La connaissance des nombreux concepts de code (SOLID, KISS…)\n  Faire du pair review ou mob review\n  Un request bot déjà en place\n  Tester et ne pas se fier uniquement à la lecture du code\n\n\nNous avons aussi pu prendre du recul et noter des conseils à appliquer. Chacun a pu transmettre ses idées à son équipe. Le rappel qu’une revue c’est aussi souligner le positif et pas seulement challenger le code. Cette conférence est très concrète et facilement applicable à Bedrock.\n\nFighting impostor syndrome: a practical handbook\n\nLors de cette conférence, Marine Gandy commence sa présentation par définir ce que le syndrome de l’imposteur est : une peur de l’échec, la crainte que quelqu’un dise que nous ne sommes pas capables, mais aussi le sentiment de ne pas mériter de réussir.\nElle nous explique que ce terme était tout d’abord attribué exclusivement aux femmes, mais qu’après une étude montrant que 70% de la population était touchée, il se serait généralisé à tous les genres.\nMarine Gandy nous énonce que la tech est très touchée par ce phénomène pour plusieurs raisons comme le fait qu’il y ait beaucoup de renouveau dans ce corps de métier et que nous avons vite l’impression de retourner à nos débuts lorsque nous changeons de techno, créant ainsi un sentiment d’instabilité. \nDans ce contexte, Marine nous parle de l’effet Julien Lepers en prenant pour exemple le fait de se trouver dans une équipe de personnes bien plus expérimentées que nous où la situation influe sur la personne.\nPour finir, la conférencière nous présente plusieurs pistes à suivre pour éviter ou minimiser ce genre de sentiment :\n\n  Arrêter de se comparer aux autres\n  Se challenger sur de nouveaux domaines pour se rendre compte qu’on peut toujours apprendre\n  Travailler sur ses faiblesses pour permettre de se sentir plus compétent\n\n\nMon combat contre l’arachnophobie\n\nJérôme Tanghe, par son arachnophobie, nous a expliqué comment il est arrivé à contribuer à API Platform afin d’ajouter une option pour cacher la mascotte Webby. Et c’est ce dont il nous a parlé, à savoir comment bien démarrer sa première pull request pour contribuer au logiciel libre. La première étape étant de trouver le bon repository qui nous conviendrait parmi les projets existants. Dans le but d’identifier un sujet sur lequel contribuer, il ne faut pas hésiter à utiliser la fonctionnalité des tags sur les issues, par exemple le tag hacktoberfest dans le cadre d’API Platform. Une fois le sujet trouvé, il faut maintenant identifier la branche de base à partir de laquelle faire sa pull request, cela peut s’agir d’une version spécifique choisie ou même de la branche principale. La contribution au logiciel libre ou à l’open source ne passe pas uniquement par des pull requests uniquement basés sur le code. Il est également possible de tester les préversions (release-candidate), signaler des bugs, faire des suggestions, améliorer la documentation ou encore rédiger des traductions. Enfin, il est conseillé de prendre en compte chaque retour sur d’autres pull requests, cela permet notamment de découvrir les principes et standards du projet.\n\nPourquoi je n’utilise pas API Platform\n\nFrédéric Bouchery, s’est décidément perdu en se retrouvant à présenter cette conférence à l’API Platform conference 2022. Malgré tout, cela lui a permis de nous partager son introspection : Mais au fait, pourquoi il ne s’en sert pas ?\nDans cette première partie de sa conférence, Frédéric n’hésite pas à utiliser beaucoup de sarcasme. Il nous explique qu’il ne s’en sert pas, car API Platform est écrit en PHP et pour lui, c’est une technologie vieillissante qui ne devrait pas tarder à rejoindre Cobolt. Également parce qu’API Platform utilise Symfony, alors que tout le monde le sait très bien, enfin surtout les Google Trend, Laravel est plus utilisé dans le monde. De plus, Frédéric n’aime pas la magie et API Platform en est rempli : sérialisation et désérialisation à tout va alors que lui est capable de faire une API en seulement quelques lignes avec du PHP pur sans artifice. Enfin, il reproche à API Platform de devenir compliqué à utiliser si le projet qui se base dessus est complexe, trop de personnalisation et de configurations doivent être effectuées. \nDans cette deuxième partie, Frédéric fait tomber son masque sarcastique et décide de revenir sur les points qu’il a abordés précédemment :\n\n  Les tendances concernant un langage ne sont pas des bons indicateurs, en effet imaginer le futur ou la mort de PHP via des statistiques dont certaines basées sur l’opinion publique n’est pas une bonne façon de faire\n  PHP, c’est aujourd’hui 75% des sites du monde entier et 54% parmi le top 1000 des sites internet fréquemment utilisés\n  Malgré tout, il nous conseille de ne pas être mono technologique non plus. S’intéresser à d’autres langages est une bonne chose\n  API Plateform a quand même de bonnes performances : 99% de réponses avec une moyenne de 91 ms sur 5 000 requêtes comparées à son code PHP pur avec une moyenne de 21 ms pour 5 000 requêtes également sachant que son code ne prend pas en compte la sécurité\n  Le framework Laravel utilisant des composants Symfony, il est dès lors difficile de les comparer. Même si factuellement Laravel est plus utilisé dans le monde, on n’utilise pas du Laravel ou du Symfony pour les mêmes raisons et c’est une bonne chose que les deux coexistent ensemble\n  Il pensait qu’avec la complexité de ses projets, il était trop dur de passer à API Platform, mais il s’est rendu compte que ce n’était pas nécessairement vrai\n\n\nEn conclusion de sa conférence et sans aucun sarcasme, Frédéric n’hésite pas à se livrer à nous et finit par nous dire qu’il va finalement utiliser API Platform 3 pour un projet.\n\nWhat’s New in Caddy, the webserver of API Platform\n\nFrancis Lavoie nous a présenté plusieurs nouveautés dans Caddy, un webserver écrit en Go ayant beaucoup de fonctionnalités activées par défaut et fourni avec API Platform dans l’installation de base.\n\nParmi les nouveautés présentées, il nous a notamment parlé d’améliorations au niveau des request matchers avec des matchers réutilisables, des expressions et des fonctions. Il a ensuite parlé d’une gestion native de Authelia permettant de déléguer facilement l’authentification depuis la configuration du serveur.\nEnfin, la directive file_server peut maintenant servir des fichiers provenant d’autres sources que le système de fichiers local, par exemple depuis un bucket S3.\n\nCertaines fonctionnalités sont désormais activées par défaut comme HTTP/3 et la suppression du log des headers d’authentification où il est également possible de créer des filtres pour retirer d’autres informations.\n\nWebAuthn : se débarrasser des mots de passe. Définitivement.\n\nFlorent Morselli nous fait une proposition : il est de plus en plus possible aujourd’hui de se passer complètement des mots de passe.\n\nIl a commencé par nous rappeler les problèmes récurrents : mots de passe trop faibles et/ou trop courts, réutilisation sur plusieurs sites, la multiplication des fuites de bases de données, etc.\n\nLa solution proposée : WebAuthn, un standard d’authentification multifacteur permettant d’identifier les utilisateurs via des données biométriques, des clés physiques ou sans aucune information après la première authentification sur un appareil.\n\nCôté implémentation, Florent nous a présenté deux projets : un bundle Symfony pour le côté backend et un composant Symfony UX pour le frontend.\n\nPHP WebSockets, or how to communicate with clients in real-time\n\nHabituellement connue pour faire des conférences sur Git, Pauline Vos nous a fait une démo en live de l’utilisation des WebSockets en PHP.\n\nElle a commencé par une rapide explication de différents protocoles de communication en temps réel existant : WebRTC chez Google, Mercure chez Symfony et Livewire chez Laravel.\nLes WebSockets étant de simples tunnels à données, ces protocoles permettent de les enrichir de diverses fonctionnalités : identification, structures de messages, reconnexion auto, etc.\n\nVient ensuite la démo qui consistait en une mini webapp de tombola en ligne. Elle a été découpée en différentes étapes (préparées dans des branches Git) avec, pour chaque étape, une présentation du code et des tests en live via l’outil WebSocketKing.\nPour l’étape finale, un QR code a été affiché à l’écran pour permettre aux spectateurs et spectatrices de participer en live. Le hasard a voulu que le nom tiré soit Antoine Bluchet, le contributeur principal d’API Platform !\n\nComment (re)mettre la tech au service du bien commun ?\n\nPour conclure ces deux jours de conférences intenses en savoir et en émotions, animé par Grégory Copin : Hélène Marchois, Paul Andrieux et Kévin Dunglas nous ont proposé une excellente table ronde riche en idées et porteuse (d’un peu) d’espoir. Le sujet étant de savoir s’il est possible de faire évoluer la tech dans le but de rejoindre les objectifs de départ du logiciel libre.\n\nL’apparition du mouvement du logiciel libre puis celle du web se sont bâties sur de grands espoirs et de beaux objectifs : émancipation des individus, partage des connaissances à l’échelle planétaire, liberté d’expression, constructions de bien commun appartenant à toutes et tous et maintenues collectivement. Malheureusement, force est de constater que le web comme le logiciel libre ont été détournés de leurs objectifs de base et que les idéaux qu’ils portaient ont été bien mis à mal : surveillance de masse, capitalisation de ces biens communs et précarisation des individus et des libertés.\n\nKévin nous explique ensuite la différence entre logiciel libre et open source : API Platform est un logiciel libre plus qu’open source, même si techniquement, c’est les deux. Historiquement, le logiciel libre est apparu dans le but de créer un bien commun pour l’humanité et s’est élargi avec, notamment, la notion de commons via Wikipédia. La différence avec l’open source est que si le code est disponible, ce n’est pas uniquement pour bâtir tout et n’importe quoi avec, mais c’est un code qui porte des valeurs et qui a pour but de faire en sorte que tout le monde sans distinction puisse facilement créer de nouveaux outils qui puissent être partagés, qui appartiennent à un ensemble de personnes et qui vont socialiser le travail qui est réalisé en commun là-dessus. Le but du logiciel libre à la base, c’est de faire en sorte que ces valeurs de transparence, de démocraties, de partage de connaissance s’étendent via le logiciel à l’ensemble de la société. Donc si vous aussi, vous voulez utiliser un logiciel libre, la condition est que, vous aussi, vous devez faire quelque chose qui sert l’humanité : créer un bien commun et mettre aussi à disposition le code source du logiciel. En l’occurrence, API Platform, est une licence permissive, c’est-à-dire qu’il est possible de faire tout et n’importe quoi avec, mais ce n’est pas le cas pour le logiciel Mercure par exemple, où si vous l’utilisez et le modifiez, vous êtes obligé de redistribuer les éléments.\n\nQuant à l’open source, c’est une initiative qui est arrivée bien après le logiciel libre et est une offensive de multinationale de la technologie qui veut dépolitiser le mouvement du logiciel libre. Le point de départ étant que, techniquement, c’est très intéressant de créer du logiciel ensemble, de partager les coûts de maintenance entre différentes entreprises ou personnes et c’est surtout très intéressant d’avoir accès au secret de fabrication pour les choses qui ont peu de valeur ajoutée. Mais l’objectif final étant de capitaliser, faire du business et capter la valeur sur ce qui a une très forte valeur ajoutée. Par exemple, pour macOS, toutes les briques de bases sont complètement libres, développées par une communauté de personne, d’entreprise et essentiellement beaucoup de bénévoles et d’hobbyiste. Et dans ce cas-là, ce qui a une extrême valeur ajoutée, c’est l’UI au-dessus du matériel ou encore les jolis outils qui coûtent une fortune. Ce qui permet à Apple d’être la boite la plus riche du monde en réutilisant le travail de personnes qui n’ont pas fait ça pour macOS à la base.\n\nLes trois personnes intervenantes représentant chacune une SCOP, la table ronde s’est ensuite naturellement tournée vers le lien entre le logiciel libre et le mouvement coopératif. Le lien étant la vision politique du logiciel libre via son socle de valeur : liberté, transparence, gouvernance partagée et coopération. On retrouve cet esprit de transparence, de fonctionnement démocratique et de fonctionnement par coopération à l’intérieur de la SCOP et entre les différentes SCOP.\nS’en est ajoutée la question du sens par rapport à son travail. Effectivement, le logiciel libre, comme le mouvement coopératif, redonne du sens, principalement car cela ouvre le champ des possibles en vue des enjeux climatiques et sociaux actuels. Même si l’on vit dans une société qui est régie par le profit, la compétition féroce et le pouvoir, il existe des possibilités de s’organiser autrement et qui fonctionne quand même à une échelle conséquente, bien qu’encore insuffisante. Des actions individuelles existent et sont possibles. Pour cela, nous vous recommandons de regarder la conférence d’Hélène à l’API Platform Conference de l’année dernière qu’elle résume et étoffe lors de cette table ronde.\n\nEt bien sûr, quand cela sera possible, nous vous encourageons fortement de regarder le replay de cette conférence (sur la chaine des Tilleuls) qui redonne un peu d’espoir quant aux futurs des organisations démocratiques de nos métiers.\n\nConclusion\nMerci à toutes et tous les speakers, à API Platform ainsi qu’aux Tilleuls-coop pour cet évènement ! Nous avons pu en apprendre plus sur API Platform et revenir la tête pleine d’idées pour nos projets futurs et présents ! À l’année prochaine peut-être !\n"
} ,
  
  {
    "title"    : "Turn off your fracking notifications #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/turn-off-your-notifications",
    "date"     : "September 30, 2022",
    "excerpt"  : "Turn off your fracking notifications.\nPrésenté par Fabien Dumas.\n",
  "content"  : "Turn off your fracking notifications.\nPrésenté par Fabien Dumas.\n"
} ,
  
  {
    "title"    : "Nearby interaction, Airtags or how your iPhone shares your location #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/nearby-interaction-airtags",
    "date"     : "September 30, 2022",
    "excerpt"  : "Découvrez comment votre iPhone partage votre localisation.\nPrésenté par Oleksandr Balystky.\n",
  "content"  : "Découvrez comment votre iPhone partage votre localisation.\nPrésenté par Oleksandr Balystky.\n"
} ,
  
  {
    "title"    : "La facilitation spectacle : Entre artifices et intention #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/la-facilitation-spectacle",
    "date"     : "September 30, 2022",
    "excerpt"  : "La facilitation spectacle : Entre artifices et intention.\nPrésenté par Camille Cousin &amp;amp; Marie-Andrée Jolibois.\n",
  "content"  : "La facilitation spectacle : Entre artifices et intention.\nPrésenté par Camille Cousin &amp;amp; Marie-Andrée Jolibois.\n"
} ,
  
  {
    "title"    : "Courir: la voi(e/x) du fondeur #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/courir-la-voie-du-fondeur",
    "date"     : "September 30, 2022",
    "excerpt"  : "Courir: la voi(e/x) du fondeur\nPrésenté par Thomas Sontag.\n",
  "content"  : "Courir: la voi(e/x) du fondeur\nPrésenté par Thomas Sontag.\n"
} ,
  
  {
    "title"    : "Comment cloner Shazam ! #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/comment-cloner-shazam",
    "date"     : "September 30, 2022",
    "excerpt"  : "Découvrez comment cloner Shazam!\nPrésenté par Moustapha Agack.\n",
  "content"  : "Découvrez comment cloner Shazam!\nPrésenté par Moustapha Agack.\n"
} ,
  
  {
    "title"    : "Chaos engineering dans le frontend #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/chaos-engineering-frontend",
    "date"     : "September 30, 2022",
    "excerpt"  : "Découvrez comment faire du chaos engineering dans le frontend.\nPrésenté par Thibaud Courtoison.\n",
  "content"  : "Découvrez comment faire du chaos engineering dans le frontend.\nPrésenté par Thibaud Courtoison.\n"
} ,
  
  {
    "title"    : "Bullet journal #LFT 30/09/22",
    "category" : "",
    "tags"     : " lft, tech",
    "url"      : "/bullet-journal",
    "date"     : "September 30, 2022",
    "excerpt"  : "Bullet Journal.\nPrésenté par Bénédicte Garcia.\n",
  "content"  : "Bullet Journal.\nPrésenté par Bénédicte Garcia.\n"
} ,
  
  {
    "title"    : "Subtitles, open captions, closed captions, SDH, oh my!",
    "category" : "",
    "tags"     : " streaming, subtitles, captions, video, player",
    "url"      : "/2022/09/20/captioning-in-the-streaming-world.html",
    "date"     : "September 20, 2022",
    "excerpt"  : "Subtitles, open captions, closed captions, SDH, oh my!\n\nWait, what? Subtitles and captions are not the same? Have you noticed the popular « cc » logo in a player you use, standing for « Closed Captions » but you have never heard of Open Captions? ...",
  "content"  : "Subtitles, open captions, closed captions, SDH, oh my!\n\nWait, what? Subtitles and captions are not the same? Have you noticed the popular « cc » logo in a player you use, standing for « Closed Captions » but you have never heard of Open Captions? In this article we are going to dive into the different textual representations used in the streaming world.\n\nSubtitles vs Captions: a matter of accessibility\n\nSince both terms are often mixed up, in this post we are going to explain in details the different types of subtitles and captions.\n\nSubtitles exist in order to help the viewer understand the spoken language in the content being watched, assuming that the viewer can hear. This is the important part. You can think of subtitles as the closest translation of what is being said, textually represented on the screen.\n\nClosed Captions\n\nClosed Captions on the other hand, assume that the viewer is deaf (or hard of hearing) hence cannot understand what is being said, regardless of the spoken language. For this reason and contrary to subtitles, it will describe spoken dialogues as well as all important audio information such as music, sounds, speaker information when it makes sense (for example narrated content). In terms of appearance, closed captions are usually white text on a black background. An important note is that they are not supported through digital connections such as HDMI.\n\nSubtitles and closed captions are separate files that provide information for the receiver to decode. They’re not part of the stream and both can be turned off.\n\nSubtitles for the Deaf and Hard of Hearing\n\nSubtitles for the Deaf and Hard of Hearing, known as SDH, are a combination between subtitles and closed captions. They can be in the same language of the video original audio and bring some additional non-spoken information (speaker identification, sound effects, etc.) and/or be translated. This makes the content accessible for the deaf and hard of hearing who can read and understand foreign languages.\n\nOpen Captions\n\nThe most important difference between Closed and Open Captions is that Open Captions are always visible and cannot be turned off. Think of them as « burned » in the video stream: they are not a separate file. Because of this, quality and readability may be affected. Open captions are widely used on social media for retention. Since there is a high chance that the end user is scrolling through content without sound, ensuring the display of text on the video helps catch and retain attention. In other cases where closed captions cannot be used for example if you have no control on the media player that will play your file, you may provide open captions to be sure to display a textual translation. The downside might be that part of the audience dislikes the superfluous text burned in the stream.\n\nForced subtitles\n\nThere is often a misconception around forced subtitles (sometimes referred as forced narratives) as they are mistaken with open captions. The name « forced » might suggest that they are burned in the video stream like open captions but there is a difference. Forced subtitles are actually distributed in a separate file and, despite their name, are not necessarily displayed. On our platform, if a subtitle or closed captions track is selected by the user, forced subtitles will not show up. We will come back to this later.\n\nActually, forced subtitles are a text representation of a communication element like a spoken dialogue, specify a character ID that are not described in the original (or dubbed) audio stream. A common example would be to translate alien language. Despite watching a movie in your native (or any language that does not require you to activate subtitles), you would not be able to understand. That’s where forced subtitles come into play and ensure that you have a textual representation of what is being said even if you set subtitles to off, hence the « forced » attribute.\n\nHowever, imagine you are French and watch a Spanish show for instance. If you set the subtitles to French in order to be able to understand the content, forced subtitles won’t show up since you already have a textual representation of the content. Same goes for closed captions: if set to off, forced subtitles will display, if any. Otherwise, they won’t show up. To ensure better user experience, forced subtitles content should be included in all other tracks (regular subtitles, SDH, CC).\n\nIf available, forced subtitles should be displayed in the preferred language set up by the user.\n\nTo sum up, here is a table comparing the different technologies:\n\n\n  \n    \n       \n      Subtitles\n      SDH\n      Closed Captions\n      Open Captions\n      Forced Subtitles\n    \n  \n  \n    \n      Can be turned off\n      ✔️\n      ✔️\n      ✔️\n       \n      *\n    \n    \n      Appearance\n      Varies\n      Varies\n      Usually white text / black background\n      Varies\n      Varies\n    \n    \n      Position\n      Bottom third, centered\n      Bottom third, centered\n      Varies\n      Varies\n      Bottom third, centered\n    \n    \n      HDMI Supported\n      ✔️\n      ✔️\n       \n      ✔️\n      ✔️\n    \n    \n      Describes music and sounds\n       \n      ✔️\n      ✔️\n      ✔️\n       \n    \n    \n      Describes speaker ID\n       \n      ✔️\n      ✔️\n      ✔️\n       \n    \n    \n      Available in source language\n       \n      ✔️\n      ✔️\n      ✔️\n      ✔️\n    \n  \n\n\n*: Forced subtitles do not show up in the track selection tool making them impossible to turn on or off. However, the business rules of the media platform will take care of displaying them, if needed.\n"
} ,
  
  {
    "title"    : "Monitoring at scale with Victoria Metrics",
    "category" : "",
    "tags"     : " k8s, kubernetes, monitoring, prometheus, scaling, victoriametrics, cardinality",
    "url"      : "/2022/09/06/monitoring-at-scale-with-victoriametrics.html",
    "date"     : "September 6, 2022",
    "excerpt"  : "Monitoring at Bedrock :\nAt Bedrock Streaming, a large part of our applications are hosted on Kubernetes clusters, others use the EC2 service from AWS and a small part are hosted on “OnPremise” servers.\n\nFrom 2018 until January 2022, we used Promet...",
  "content"  : "Monitoring at Bedrock :\nAt Bedrock Streaming, a large part of our applications are hosted on Kubernetes clusters, others use the EC2 service from AWS and a small part are hosted on “OnPremise” servers.\n\nFrom 2018 until January 2022, we used Prometheus to monitor all these platforms, because Prometheus met all our needs: keeping control over our monitoring solution and supporting service discovery, which is essential in environments such as Kubernetes or AWS EC2. Prometheus also supports custom exporters we developed internally.\n\nOver the years, our business has grown significantly, so the load on our platforms has increased. Indirectly, the load on our Prometheus instances has also increased, to the point where certain limitations have become too much for us. This is why we have changed our monitoring/alerting stack.\n\nLimits of Prometheus\nPrometheus does not have a native High Availability mode: to have high availability, we had to duplicate our Prometheus instances. This implies that our targets were “scrapped” by all our Prometheus instances (same for our rules and records).\nTo avoid this, we had to use sharding, but this made the infrastructure more complex. More information on this subject in this documentation from the Prometheus operator\n\nPrometheus is not designed to store metrics on a long-term basis, as mentioned in the documentation :\n\n&amp;gt; Prometheus’s local storage is not intended to be durable long-term storage; external solutions offer extended retention and data durability.\n\nPrometheus’s local storage is not intended to be durable long-term storage; external solutions offer extended retention and data durability.\nWe worked around this limitation by using Victoria Metrics (VMCluster) as a LongTermStorage via the remote_write protocol\n\nAll processes (scrapping, ingest, storage, etc.) were, until now, managed in the same “prometheus” instance, which implied a less flexible and vertical scaling only (since recently a Prometheus agent is available for the “scrapping” part).\n\nThe RAM and CPU usage of a Prometheus instance is correlated to the number of metrics (and their cardinality) it has to manage. In our case, several Prometheus instances consumed more than 64 GB of RAM and 26 CPUs each, in order to absorb our peak loads. In a Kubernetes cluster, this high resources consumption can cause problems, especially for scheduling.\n\nThe Write-Ahead Log (WAL) system can cause rather slow restarts if the Prometheus instance runs out of RAM and can cause the Prometheus instance to hang for varying lengths of time. During the replay of the WAL, Prometheus doesn’t scrape anything, thus there is no alerting and no way of knowing if something is going on.\n\nThe cardinality of metrics\nWhen our Kubernetes clusters manage a large number of pods, a constraint quickly appears: cardinality.\n\n&amp;gt; The cardinality of a metric is the number of TimeSeries of that metric with single-valued labels.\n\n\n\nIn the example above, the status_code label has a cardinality of 5, app has a cardinality of 2 and the overall cardinality of the server_reponses metric is 10.\n\nIn this example, any Prometheus instance can handle this cardinality, but if you add for example the label pod_name or client_IP (or both) to the server_reponses metric, the cardinality increases for each different clients calls and for each pod.\n\nYou should read the excellent article from “Robust Perception” for more details on this subject.\n\nAt Bedrock the high cardinality metrics come from our HAProxy ingress. For our needs, we retrieve several labels like the name of the ingress pod as well as its IP address, but more importantly the name and IP address of the destination pod. In a cluster that can grow to more than 15,000 pods, the combination of unique labels (cardinality) is very significant for some of our ingress metrics.\n\nWe found that Prometheus performed poorly when we had multiple metrics with high cardinalities (&amp;gt; 100,000), and resulted in over-consumption of RAM.\n\nDuring a high load event, Prometheus could consume up to 200 GB of RAM before being OOMKilled. When this happened, we would go completely blind as we had no metrics or alerting.\nThis also impacts us on scalability in our Kubernetes clusters, as we use CustomMetrics very heavily in HPAs to scale the number of pods in our applications.\n\nRAM and CPU consumption of our prometheus instances (the red lines represent the reboots of our instances, we also see a loss of metrics)\n\n\n\nPrometheus is still a good solution, which has served its purpose well for several years, but we have reached its limits in our production environments.\n\nReplacing Prometheus?\n\nWe spent time optimizing Prometheus to absorb the amount of metrics and their cardinality, in particular by either directly removing high cardinality metrics if they were totally unused, or by removing the labels of certain metrics that caused high cardinalities.\nWe have also optimized the Prometheus configuration directly, as well as the maximum IOPS of our EBS. The RAM and CPU consumption of Prometheus is linked to the number of metrics to manage and their cardinality. But we always have more traffic and therefore always more pods in our clusters: we should have perpetually increased Prometheus instances resources. This was a problem for scalability and costs.\n\nCan we replace a critical tool like this? What are our short, medium and long term needs? How can we optimize costs? And especially in what timeframe?\nThe emergency of recent incidents forced us to exclude solutions such as Thanos and Cortex. Testing these solutions completely would have required too much time, which we did not have.\n\nIt is also important to consider that we were already using Victoria Metrics, but only for the Long Term Storage part, without any problems.\nCould replacing Prometheus with a stack based entirely on Victoria Metrics overcome the limitations we had with Prometheus?\nHigh availability and fault tolerance is well-supported, their documentation explains how to manage this.\n\nManaging long-term data is possible, as we were already doing it.\nVictoria Metrics is built around a set of microservices. Each one is built in order to serve a specific job, and each supports vertical and especially horizontal scaling (with sharding). A very important point when used in a Kubernetes environment.\n\nIn addition, Victoria Metrics seemed to handle high cardinality metrics better (see article on this subject). It is also possible to do rate limiting on the number of Time Series to be ingested:\n\nCPU and RAM consumption is lower with better performance than with Prometheus and even other TSDBs, several comparative articles on this subject have already been published:\n\n  Remote Write Storage Wars\n  Prometheus vs VictoriaMetrics benchmark on node_exporter metrics\n  When size matters — benchmarking VictoriaMetrics vs Timescale and InfluxDB\n  Comparing Thanos to VictoriaMetrics cluster\n\n\nWe also wanted to keep the Prometheus language: PromQL in order to keep our Grafana dashboards and all our Prometheus alerts. Even though Victoria Metrics offers its own MetricsQL language, it is perfectly compatible with PromQL.\n\nYou can see the main features of Victoria Metrics as well as various case studies in their documentation.\n\nPOC of Victoria Metrics\nWe wanted to validate the performance and consumption of a stack entirely based on Victoria Metrics, the results were really encouraging.\n\nTest environment :\n\n  1500 web app pods\n  250 Haproxy Ingress pods (metric with high cardinality enabled)\n  3700 scrapped targets\n\n\nComparative table between Prometheus and Victoria Metrics :\n\n\n  \n    \n       \n      Prometheus\n      Victoria Metrics\n    \n  \n  \n    \n      CPU consumption\n      26\n      8\n    \n    \n      RAM consumption\n      30Go\n      11Go\n    \n    \n      New TimeSeries / min\n      50K\n      6.5M\n    \n    \n      Max active TimeSeries\n      7M\n      91M\n    \n    \n      Max cardinality\n      4 metrics &amp;gt; 100K\n      10+ metrics &amp;gt; 1M\n    \n  \n\n\nGraph on the CPU consumption of Victoria Metrics components\n\n\nNumber of active “TimeSeries” in Victoria Metrics\n\n\nOur benchmark persuaded us to use Victoria Metrics as a replacement for Prometheus.\n\nImplementation of Victoria Metrics :\nWe used the official victoria-metrics-k8s-stack Helm chart which is based on an operator. This chart Helm permits to deploy a complete monitoring and alerting stack in a Kubernetes cluster.\n\nA VMCluster (Insert, Select, Storage) is deployed to manage access to metrics. The collection of metrics (push/pull) from exporters in Prometheus format is handled by the VMagent. Its configuration is done in the form of a Prometheus configuration file. It is able to :\n\n  Manage the relabeling of metrics.\n  Temporarily store the metrics it has collected if the VMCluster is unavailable or not able to send the metrics to the VMCluster.\n  Limit the cardinality of metrics.\n\n\nOne of the advantages of using this Helm chart is that it will deploy essential components to properly monitor a Kubernetes cluster such as Kube-state-metrics or prometheus-node-exporter, but also scraping configurations for services such as Kubelet, KubeApiServer, KubeControllerManager, KubeDNS, KubeEtcd, KubeScheduler, KubeProxy\n\nAlerting is also managed via a VMAlert component, which will execute the alerting and recording rules set by VictoriaMetrics. Notifications are managed by an Alertmanager which is also deployable via this chart.\n\nOne of the advantages of using this Helm chart is that it will deploy essential components to properly monitor a Kubernetes cluster such as Kube-state-metrics or prometheus-node-exporter, but also scraping configurations for services such as Kubelet, KubeApiServer, KubeControllerManager, KubeDNS, KubeEtcd, KubeScheduler, KubeProxy\n\nThis is what our monitoring and alerting stack based on this Helm chart looks like.\n\n\nResumption of the history\nWe wanted to keep historical metrics of our Kubernetes clusters. Victoria Metrics provides a tool to manage the export and import of data from different TSDB: vmctl.\n\nIn order not to overload our monitoring stack, we splitted the exports into smaller or larger time ranges, depending on the history of the cluster. For clusters with little activity and therefore few metrics, exports/imports were split day by day, for others we had to use smaller time slots.\nA home-made bash script launched several kubernetes jobs simultaneously and took care of restarting one of them as soon as another one ended.\n\nBelow an extract of the definition of our Kubernetes job with the arguments we used to do our history transfer by time range:\n\n      containers:\n      - name: vmctl\n        image: victoriametrics/vmctl\n        resources:\n          requests:\n            cpu: &quot;1&quot;\n        args:\n          - vm-native\n          - --vm-native-src-addr=http://victoria-metrics-cluster-vmselect.monitoring.svc.cluster.local.:8481/select/001/prometheus\n          - --vm-native-dst-addr=http://vminsert-victoria-metrics-k8s-stack.monitoring.svc.cluster.local.:8480/insert/000/prometheus\n          - --vm-native-filter-match={__name__!~&quot;_vm.*&quot;}\n          - --vm-native-filter-time-start=&quot;{ { start } }&quot;\n          - --vm-native-filter-time-end=&quot;{ { end } }&quot;\n      restartPolicy: Never\n\n\nFeedback after months of use\nSince we have been using our new monitoring stack, we have encountered a few bugs (as with all solutions).\nMost of the time, these were not impactful, except for one that caused us a production incident.\nWe had an overconsumption of RAM of VMStorage which was fixed in version 1.76. I would like to highlight the responsiveness of the VictoriaMetrics team, whether on slack or on GitHub: I have had several discussions with them on various subjects, and they have always been reactive\n\nVictoria Metrics regularly releases new versions, including performance improvements and new features. The changelog will give you an idea of the latest improvements and their frequency.\n\nVictoria Metrics has an Enterprise version that adds some features, including one that we are interested in but have not yet tested: downsampling.\nWe have configured a one-year retention for each of our Kubernetes clusters, and on some clusters that’s mean more than 7 TB of data per VMStorage.\n\nThe downsampling allows you to configure how many metrics you want to keep per time interval.\n\nIn this example: -downsampling.period=24h:10s,1w:30s,30d:1m,360d:5m, (assuming we collect metrics every 5 seconds) we only keep:\n\n  one measurement point every 10 seconds beyond 24 hours (instead of one point every 5 seconds)\n  one measurement point every 30 seconds beyond 7 days\n  one measurement point every minute beyond 30 days\n  one measurement point every 5 minutes beyond one year\n\n\nIt is rarely necessary to keep all the measurements of our metrics on such a long scale, when we want to retrieve measurements that are several months old, it is usually to see a trend and not all the measurements.\nWith this option, we could greatly reduce the storage used by our metrics.\n\nConclusion\nThrough this article, you have discovered why and how we migrated our monitoring stack of our Kubernetes clusters at Bedrock from Prometheus to Victoria Metrics.\n\nThis was an important and critical subject for us, as monitoring is a critical need.\nNow our monitoring stack, based entirely on Victoria Metrics, is robust and capable of absorbing large load peaks.\n\nHere are some indicators of the victoria metrics stack performance of one of our Kubernetes clusters during last 6 months:\n\n  active time series: up to 39 million (average 7.4M)\n  total number of datapoints: 12 trillion\n  ingestion rate : up to 1.3 million new samples per second (average 227K)\n  churn rate : up to 117 Million new time series per day (average 30.6 Million)\n  disk usage (data + index): 15 TB\n  sample rate : up to 4.99M (average 343K)\n  scrape target : up to 49K (average 4.4K)\n\n\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Is machine learning a unicorn hiding a series of if and else?",
    "category" : "",
    "tags"     : " machine learning, Data Science",
    "url"      : "/2022/09/05/machine-learning-if-else.html",
    "date"     : "September 5, 2022",
    "excerpt"  : "Recently, a colleague asked me:\n\n\n  All good with your if and else machine learning system?\n\n\nIt was a joke but this one made me think.\n\nThis is a running gag: machine learning is only a series of if and else.\n\n\n\nBeyond the joke, it is true?\n\nYes!...",
  "content"  : "Recently, a colleague asked me:\n\n\n  All good with your if and else machine learning system?\n\n\nIt was a joke but this one made me think.\n\nThis is a running gag: machine learning is only a series of if and else.\n\n\n\nBeyond the joke, it is true?\n\nYes! …and no. As always, it depends.\n\nQuick answer: Machine learning is a bunch of mathematical and statistical operations. Sometimes, the operations you use can be translated into if and else clauses, and sometimes not. But you never write the series of if and else yourself.\n\nA recap of machine learning\nThe idea of machine learning is: you have some data, and you apply an algorithm to these to detect a pattern. You put this pattern into a function.\n\n\n\nThen, you’ll be able to use this function on new data to extract new information.\n\nA decision tree with a series of if and else\n\nThere are different types of machine learning. If you decide to build a decision tree (a famous way to do machine learning) to know the form of a diamond, you’ll get something like that:\n\n\n\nIf you translate it with code, you’ll get something like that:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    if size is high:\n        return red plate\n    else:\n        return grey pentagon\n\n\nThen, yes, you can see that here, you have a series of if and else.\n\nAnd decision trees are used a lot in machine learning. Most of the time, you don’t use decision trees directly but forests of decision trees in the Random Forest algorithm or a series of decision trees in the Gradient Boosted Trees algorithm.\n\nBut, many algorithms in machine learning are just the generation of plain mathematical formulas\n\nLet’s take another famous way to do machine learning: a neural network. What you’ll get at the end is more something like that:\n\na*10+b*15+c*16+20…\n\n\nThen, the process doesn’t try to find a series of if and else, but a mathematical formula.\n\nI would like to finish with a last example: recommender systems. There are many ways to build a recommendation system. One which is well known is matrix factorization.\n\nMatrix factorization, what?\n\nI won’t explain deeply what it is about, but as a sum up, it’s a manipulation of matrices. It comes from linear algebra.\nHere is a definition: Matrix decomposition.\n\nThe result is something like that:\n\nVector A * Vector B\n\n\nAs a result, yes, you have types of machine learning that will generate a series of if and else. But, you have also plenty of algorithms that try to find the variables of an equation or vectors.\n\nYou never write the series of if and else yourself\n\nLet’s go back to the decision tree. As you’ve seen previously, the result could be translated as a series of if and else.\n\nBut, you don’t write directly this code. You generate it using… mathematical operations. Yes, again!\n\nAs an example, you can get the result of a decision tree using an optimisation algorithm with the Shannon Entropy formula:\n\n\n\nLet’s suppose you want to guess the form (pentagon or plate) of a diamond according to its attributes. You have three diamonds:\n\n\n  \n    \n      carat\n      size\n      form\n    \n  \n  \n    \n      high\n      small\n      plate\n    \n    \n      low\n      high\n      plate\n    \n    \n      low\n      small\n      pentagon\n    \n  \n\n\nThe process is the following:\n\n  The data is split randomly: a random if statement is created like if carat is high\n  The process checks if it helps to generate a more accurate view of the data: by doing this if, are the data separated correctly? Do we have pentagons mostly from one side and plates from another?\n\n\nTo be able to know if the data are separated correctly, the Shannon entropy formula is used\n\n\n  if yes, the process keeps the if carat is high\n  if not, it generates another one\n\n\nThen, by keeping the if you get something like that:\n\n\n\nThe translation with a code is:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    #The process doesn&#39;t know yet how to handle that\n\n\nNote that you have a branch (below low) with a plate and a pentagon. It corresponds to the else where the process doesn’t know what to put yet.\n\n\n  So, the data below low is split randomly: another if is created\n  The process checks if it helps to generate a more accurate view of the data\n\n\n\n  if yes, the process keeps the new if\n  if not, it generates another one\n\n\nBy keeping the new if, you get another branch:\n\n\n\nThe translation with a code is:\nif size is high:\n    return red plate\nelse:\n    return grey pentagon\n\n\nAt the end, you get a final tree decision:\n\n\nwith a final code:\n\nif carat (the weight of a diamond) is high:\n    return red plate\nelse:\n    if size is high:\n        return red plate\n    else:\n        return grey pentagon\n\n\nOf course, in real life, data are more complicated and the process must iterate a lot until getting the perfect tree. The process used is an optimisation algorithm. This is the part called learning in machine learning.\n\nMathematical optimization […] is the selection of a best element, with regard to some criterion, from some set of available alternatives (definition from Wikipedia)\n\nIf you want to know how the Shannon entropy works with mathematical formulas, you’ve got this article: Classification in machine learning - Example of Decision Tree with Shannon Entropy\n\nThen as a result, yes, you can have machine learning algorithms that will build a series of if and else. But to generate it, you’ll use mathematical operations.\n\nNote that for other algorithms such as the matrix factorisation or neural networks, you don’t use a process with the Shannon entropy formula, but other optimisation algorithms that don’t generate a series of if and else but, as previously seen, vectors or formulas.\n\nSo why do we sometimes say that machine learning is a bunch of if and else statements?\n\nTo my opinion, because of expert systems. They are the ancestors of machine learning in artificial intelligence.\n\nArtificial intelligence is a way to simulate human cognitive abilities. In the history of artificial intelligence, people thought that they would be able to target that with expert systems. These are big series of hardcoded rules and then… of if and else.\n\nConclusion\nTo conclude, most of the time, machine learning is not a series of if and else. It’s just mathematics and for some techniques, they are very old. I’m thinking of linear regressions or Bayesian probabilities. These were used long before the existence of computers.\n\nPhoto of the unicorn by Stephen Leonardi on Unsplash\n"
} ,
  
  {
    "title"    : "Using a circuit breaker to spare the API we are calling",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, resiliency, circuit-breaker",
    "url"      : "/2022/09/02/backend-circuit-breaker.html",
    "date"     : "September 2, 2022",
    "excerpt"  : "Hi! We’re going to start our fourth article about Bedrock’s API gateway.\nToday we will talk about the circuit breaker pattern, what it is, and how we’re using it.\n\nThe Circuit Breaker Pattern\n\nWith this pattern, our API Gateway detects errors when...",
  "content"  : "Hi! We’re going to start our fourth article about Bedrock’s API gateway.\nToday we will talk about the circuit breaker pattern, what it is, and how we’re using it.\n\nThe Circuit Breaker Pattern\n\nWith this pattern, our API Gateway detects errors when calling its dependencies.\nIt will stop calling them if a given threshold (ratio of errors) is crossed.\n\nThe circuit breaker allows us to spare the dependencies in difficulty, but also avoid taking time to do something that will most likely fail.\n\nYou’ll find a more detailed explanation about the circuit breaker on Martin FOWLER’s blog.\n\nWhere to use it?\n\nAs soon as a service call is not mandatory for our BFF to answer something that a frontend application can read, then we can use the circuit breaker pattern.\n\nIf an API cannot handle a sudden increase in traffic (for example: it’s not scaling fast enough or its database starts to throttle), it’s better to stop calling it temporarily.\nWhen the right timeouts are configured, an API throttling will result in an error, as seen in the previous article\n\nHere are some examples:\n\nVideo progress information\n\nDisplaying a video progress bar is useful for end users, but it’s better to not display this information instead of risking the entire page to not be displayed!\nIf the service that stores video viewing sessions is (slowing) down, we can stop asking for this information and stop displaying the video progress bar.\n\n\n\nUser geolocation\n\nThe geolocation service allows us to know where the end user is in the world. Based on this information we lock some area restricted contents.\nIf this service goes down for some reason, we will stop calling it, and instead use a default area matching the area of our customer as it’s the majority case.\n\nImplementation and configuration\n\nSo far we’re only using the circuit breaker pattern with HTTP calls.\nThis is made possible thanks to the Ganesha library, and its Guzzle middleware.\n\nThe Guzzle middleware is created as a service within the Symfony service definitions.\nIt’s then injected into our HttpClientFactory that will handle the creation of all the different clients.\nThe responsibility of using the circuit breaker falls on each service that will create a http client.\n\nAckintosh\\Ganesha\\GuzzleMiddleware:\n    factory: [&#39;@...Infrastructure\\HttpClient\\CircuitBreaker\\CircuitBreakerMiddlewareFactory&#39;, &#39;buildWithRateStrategy&#39;]\n    arguments:\n        $timeWindow: 60\n        $failureRateThreshold: 40\n        $minimumRequests: 10\n        $intervalToHalfOpen: 60\n\n\nMonitoring the circuit breaker\n\nAt Bedrock, we’re used to monitor everything. The circuit breaker makes no exception to this rule.\nUsually we store time spent and response code for every outgoing http call.\nTo see when the circuit breaker is open, we catch the ganesha’s RejectedException to save a dedicated 666 http status.\n\nThis allows us to look for the exact number of calls avoided.\nBelow lies an example of a monitoring chart showing some errors happening during a usual night.\n\n\n\nWe also have to query slower services that often trigger our circuit breaker because they cannot answer in the short timeout we impose.\nThereafter, the same monitoring chart including such services.\n\n\n\nGoing further\n\nSo far, we have identified two areas for improvements described below.\n\nDifferent configurations\n\nWe’re only using a single configuration for the circuit breaker.\nWe should allow each service to choose from a named list of configurations when creating a client, similarly to the different guzzle configuration we are using.\nThe main obstacle is a lack of hindsight which prevent us to have fine-tuned values.\nThis is something that will definitively be improved over time as we monitor over long period.\n\nStaled cache when the circuit breaker is open\n\nFor many editorial contents, we’re using a staled cache version of the data as a fallback.\nTo do so, we’re using another guzzle middleware.\n\nSadly, the two middlewares don’t work together. We have to chose which one to use based on the criticality of the content and the API behind. \nThis is something that we aim at solving with a bit of R&amp;amp;D.\n\nConclusion\n\nIn today’s post we’ve seen our usage of the circuit breaker pattern.\nIt allows us to spare the services we are calling, and avoid slowing us down in case of throttling.\n\nNext time, we will talk about our ultimate layer of protection to ensure the BFF always responds something readable to frontend applications.\n\nFrom the same series\n\n\n  What’s a BFF\n  Handling API failures in a gateway\n  What’s an error, and handling connexion to multiple APIs\n  Using a circuit breaker\n\n"
} ,
  
  {
    "title"    : "Prescaling pods in Kubernetes, we open source our solution",
    "category" : "",
    "tags"     : " k8s, kubernetes, pods, prometheus, scaling, hpa, resiliency, go, prescaling, opensource",
    "url"      : "/2022/09/01/kubernetes-prescaling-we-open-source-our-solution.html",
    "date"     : "September 1, 2022",
    "excerpt"  : "Previously we discussed how we manage the load of our Kubernetes clusters and how we can anticipate our needs with prescaling. Today, we are here to share our solution that we have reworked and open sourced! \n\n\nAt Bedrock Streaming, we provide str...",
  "content"  : "Previously we discussed how we manage the load of our Kubernetes clusters and how we can anticipate our needs with prescaling. Today, we are here to share our solution that we have reworked and open sourced! \n\n\nAt Bedrock Streaming, we provide streaming platforms to our customers (6play, Salto, Videoland and many others), we have a good knowledge of the daily load peaks and we know in advance the programs that are likely to generate a lot of traffic. We can therefore rely not only on reactive scaling, which has its limits (cf. prescaling article) but also on prescaling.\n\n&amp;gt; Prescaling consists of increasing the number of critical application pods in our clusters in advance in order to be ready to face a sudden traffic peak.\n\nInitially, we developed an in-house solution in Python for a simple reason: it was the language that most people in the team knew. Since we had time to test our solution, we thought it would be great to share it with everyone. But to do so, we had to make some adjustments.\n\nWe rewrote everything in go\n\nMany open source projects we use are written in Golang. In addition, the DevOps/Cloud world is mostly focused on Go. So, we decided to rewrite our prescaling solution in Go in order to make our teams more skilled in this language. The other goal was to make it cloud agnostic. In the Python version, we had an API part that stored prescaling events in a DynamoDB table, which made the solution dependent on AWS. Since prescaling is Kubernetes oriented, we had thought in the first versions in Python to store these events in Custom Resources (CRD) but due to lack of time, we did not implement it. We took advantage of the redesign to implement it and remove the dependency with AWS DynamoDB.\n\nWe also wanted to simplify the project. In the first versions, we had two bricks: one containing the exporter and another the API. We merged the two applications into one monolith. The API is CRUD and can handle CRD events.\n\nHere we go, we open source it\n\nThe great moment has come. Our prescaling solution is now available on GitHub in its alpha version: https://github.com/BedrockStreaming/prescaling-exporter.\n\nThis is the version we currently use in all our clusters. Let’s quickly see how to implement the solution (you can find more details in the repo README).\n\nThe prescaling-exporter is distributed with helm charts in order to install it in kubernetes cluster.\n\nPrerequisites\n\nThe following bricks must be installed in the k8s cluster:\n\n  Prometheus Stack or Victoria Metrics Stack\n  Prometheus Adapter\n\n\nIt is possible to use another metrics stack but we do not provide an example at this time.\n\nClone the repo and run the following command with Helm3:\n\nhelm install prescaling-exporter ./helm/prescaling-exporter -n prescaling-exporter --create-namespace\n\n\nIt’s required to add the following configuration to Prometheus adapter:\n\n- &quot;metricsQuery&quot;: &quot;avg(&amp;lt;&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;})&quot;\n    &quot;name&quot;:\n      &quot;as&quot;: &quot;prescale_metric&quot;\n    &quot;resources&quot;:\n      &quot;overrides&quot;:\n        &quot;namespace&quot;:\n          &quot;resource&quot;: &quot;namespace&quot;\n    &quot;seriesQuery&quot;: &quot;prescale_metric&quot;\n\n\nDaily prescaling event\n\nWe have chosen to manage the configuration of daily events directly on the HPA (HorizontalPodAutoscaler) of the applications. Here is how to activate it, through annotations:\n\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: &quot;&quot;\n  annotations:\n    annotations.scaling.exporter.replica.min: &quot;&quot;\n    annotations.scaling.exporter.time.end: &quot;&quot;\n    annotations.scaling.exporter.time.start: &quot;&quot;\nspec:\n  metrics:\n  - type: External\n    external:\n      metricName: &quot;prescaling_metric&quot;\n      metricSelector:\n          matchLabels:\n            deployment: &quot;&quot;\n      targetValue: 10\n\n\nWe are able to control the start and end time of the prescaling and the minimum number of pods we want during this window. Please note that if the number of pods we want for prescaling is less than the current number of pods, the solution will not downscale the application and the HPA will continue to behave as usual.\n\nOne-time events\n\nWe can also record one-off events. For example, at Bedrock Streaming, during an important soccer match, we will record a special event in a Custom Resource Definition. \nOne-time events allow to prescale all applications having annotations on their HPA by multiplying their prescaling minimum replicas (annotations.scaling.exporter.replica.min) by the multiplier of the event in question.\n\nTo record a one-time event, an OpenAPI UI (formerly known as Swagger) is exposed by the prescaling exporter at the url /swagger/index.html. We can also register a new event from here or directly by making an api call to the following address /api/v1/events/.\n\n\n\nWhat’s next?\n\nWe will continue to improve the solution. For example, we are thinking about removing annotations on HPAs and replacing them with a new dedicated CRD.\n\nAll contributions are welcome, don’t hesitate to come and exchange with us on GitHub if you want to use the solution, we would be delighted.\n\n\n\nAuthors: Jérémy Planckeel, Valentin Chabrier\n"
} ,
  
  {
    "title"    : "BFF&#39;s error definition, and handling connections to multiple API",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, error, timout, retry, slo, guzzle",
    "url"      : "/2022/08/25/backend-errors-connections.html",
    "date"     : "August 25, 2022",
    "excerpt"  : "A quick sidetrack in our series about Bedrock’s API gateway.\nThis piece defines what are we talking about when we say “an error”, and explains how we handle the numerous connections to services we are calling.\n\nDefinition\n\nIn the previous article,...",
  "content"  : "A quick sidetrack in our series about Bedrock’s API gateway.\nThis piece defines what are we talking about when we say “an error”, and explains how we handle the numerous connections to services we are calling.\n\nDefinition\n\nIn the previous article, we’ve seen how we handle errors.\nThis was mainly from a business point of view, and how it’s done in our domain.\n\nBut what is “an error”?\n\nThis term is a bit generic, and the definition will be too: an error is anything unexpected by the application.\n\nIn our context of an API Gateway, we are restricting this to the services we are calling.\n\nThis can be, but not exhaustively, a service not responding because:\n\n  it’s offline;\n  it’s taking too much time to answer;\n  it’s responding with a 5** error (when talking about an API);\n  it’s giving us an invalid or unexpected content.\n\n\nWhat are the consequences of those errors?\n\nThe first issue is: we won’t be able to display some part of the application as intended.\nWe’ve talked about this previously already.\n\nThe second error, more insidious, is that it can slow down our BFF terribly.\n\nThe BFF response time is, on average, equals to the slowest service the BFF is calling.\nIf a service that usually responds in 200ms starts slowing down to an average response time of 1s and also times out half the time, it will increase the BFF response time to 1,5s (1s average, and 50% retry).\n\nThat’s why we must be careful when configuring those timeouts.\nThe BFF exposes a response-time Service Level Objective (SLO), and frontend applications will cut any connection that takes too long.\nLosing some parts of the responses is better than slowing the BFF down to a point where frontend won’t get any response at all.\n\nHow are we mitigating the errors?\n\nFor any remote service, we configure short timeouts, and retry when we must.\nA short timeout is a timeout that usually match the SLO of the called services, and that will match 99% of our calls.\nWhen the SLO of the called service is higher than ours, we use a shorter timeout and accept that a larger parts of the calls will be cut.\nThe values are tailored according to our usages.\nWe use our monitoring to adapt those values in order to reduce the number of errors, while minimizing the impact on the BFF response time.\nWe are also constantly challenging our colleagues to improve the average response time of their services that we are calling.\n\nThe choice of using retries is based on the information criticality.\nFor example, retrieving the user’s previous viewing sessions, is important for his/her experience, so we’re using a retry here.\nOn the opposite, analytics are less important, so we don’t use any retry there.\n\n    app.http_client_configs.best_effort:\n        retry: 1\n        timeout: 0.6\n        connect_timeout: 0.1\n    app.http_client_configs.fast_fail:\n        retry: 0\n        timeout: 0.6\n        connect_timeout: 0.1\n    app.http_client_configs.long_fail:\n        retry: 0\n        timeout: 1\n        connect_timeout: 0.1\n    app.http_client_configs.reliant:\n        retry: 2\n        timeout: 60\n        connect_timeout: 0.1\n\nAbove, you can see the yaml configuration our Symfony application uses to build its Guzzle clients.\n\nEach configuration can cascade onto the clients, making variants available for our Symfony services.\n\nBelow lies a Symfony configuration example:\n\n  We have an interface BFF\\Domain\\Content\\Repository from the domain for a content repository.\n  The interface is linked to an implementation BFF\\Infra\\HttpContentClient inside the infrastructure.\n  The implementation is built with variants (best_effort and fast_fail) from a factory using the matching Guzzle configurations.\n  Other services use a chosen repository according to their needs and criticality.\n\n\n    # Service definition with its aliases.\n    BFF\\Domain\\Content\\Repository: &#39;@BFF\\Domain\\Content\\Repository.fast_fail&#39;\n    BFF\\Domain\\Content\\Repository.best_effort: &#39;@BFF\\Infra\\HttpContentClient.best_effort&#39;\n    BFF\\Domain\\Content\\Repository.fast_fail: &#39;@BFF\\Infra\\HttpContentClient.fast_fail&#39;\n\n    # Concrete implementations\n    BFF\\Infra\\HttpContentClient.best_effort:\n        class: &#39;BFF\\Infra\\HttpContentClient&#39;\n        factory: [&#39;@BFF\\Infra\\ContentClientFactory&#39;, &#39;create&#39;]\n        bind:\n            $clientConfig: &#39;%app.http_client_configs.best_effort%&#39;\n    BFF\\Infra\\HttpContentClient.fast_fail:\n        class: &#39;BFF\\Infra\\HttpContentClient&#39;\n        factory: [&#39;@BFF\\Infra\\ContentClientFactory&#39;, &#39;create&#39;]\n        bind:\n            $clientConfig: &#39;%app.http_client_configs.fast_fail%&#39;\n\n    # Other services using the Repository\n    BFF\\Domain\\Navigation\\NavBarResolver:\n        $content: &#39;@BFF\\Domain\\Content\\Repository.best_effort&#39;\n\n    BFF\\Domain\\Layout\\BlockResolver:\n        $content: &#39;@BFF\\Domain\\Content\\Repository.fast_fail&#39;\n\n\nThis is an over simplified example as we have more layers and wrappers used for things like caching, monitoring, logging, etc.\n\nConclusion\n\nIn this article, we’ve clarified what an error is, and explained that we cannot generalize the configuration and usage of our APIs. Timeouts and retries, especially, must be tailored depending on the criticality of each call.\n\nThis was a deviation on the road to our next article, where we will talk about monitoring the errors and stopping calls to failing APIs by implementing the circuit-breaker pattern.\n\nFrom the same series\n\n\n  What’s a BFF\n  Handling API failures in a gateway\n  What’s an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n"
} ,
  
  {
    "title"    : "Les spikes : quand, comment, pour quoi faire ?",
    "category" : "",
    "tags"     : " spike, methodologie, cytron, tech",
    "url"      : "/how-to-spike",
    "date"     : "August 23, 2022",
    "excerpt"  : "C’est une histoire bien connue, dans la vie de n’importe quel développeur : un ticket arrive dans le backlog, décrivant une problématique relativement complexe. C’est parfois une question de technologie inconnue, ou parfois simplement un chantier ...",
  "content"  : "C’est une histoire bien connue, dans la vie de n’importe quel développeur : un ticket arrive dans le backlog, décrivant une problématique relativement complexe. C’est parfois une question de technologie inconnue, ou parfois simplement un chantier un peu trapu. Je pense que toutes les équipes ont, au moins une fois dans leur vie, fait face à ce genre de tâche impossible : c’est l’occasion des regards désespérés, alors qu’un junior se lamente en disant « Mais par où est-ce qu’il faut commencer ? ». Et c’est là qu’on répond : « Essaye de faire un spike ».\n\nFaire un spike ? Quelle excellente idée ! Encore faudrait-il savoir ce qu’est un spike, comment ça marche, et à quoi ça sert.\n\nJe vous propose donc ensemble de voir dans cet article : qu’est-ce qu’un spike, quand l’utiliser, et comment considérer qu’il est réussi ?\n\nspike –help 📚\n\nSi je devais citer Wikipedia, je dirais qu’un Spike, c’est “une méthode de développement de produit, dérivée de l’extrême programming, et qui cherche à créer le code le plus simple possible pour obtenir des solutions potentielles”.\n\nEn gros, le but d’un spike, c’est de répondre à la question “Comment on fait ?” avec un prototype de code réalisé grâce à une série de petites étapes simples. Un spike n’est pas une formule magique qui va vous permettre de réaliser la tâche impossible que votre client vous a donné. En revanche, le spike va vous permettre de savoir si la tâche impossible ou compliquée à première vue est en fait possible, et si oui, comment.\nIl arrivera également que votre spike vous permette de constater qu’une tâche donnée peut être réalisée de plusieurs manières : que ce soit en passant par des librairies différentes, avec une implémentation changeante, ou autre chose encore. Dans ces cas, le spike va également vous servir à essayer ces différentes possibilités, et à choisir celle qui est la plus appropriée !\n\nLe moyen le plus simple est de procéder morceau par morceau. Alors je vous propose qu’on s’y mette maintenant, et qu’on regarde quoi faire !\n\n“kowalski, analysis !” 📊\n\nAvant toute chose, il faut savoir exactement ce que vous souhaitez faire. Rien ne sert de mettre la charrue avant les bœufs.\n\nSi ce n’est pas fait, écrivez noir sur blanc les lignes exactes qui vont définir votre tâche comme finie. Que ce soit connecter votre utilisateur de façon sécurisée, afficher une vidéo sans heurt, ou juste avoir une page qui clignote en blanc et bleu, il faut que vous ayez une liste de bullet points, qui définit précisément ce que vous voulez faire.\n\nVotre objectif final est de réaliser tout ce que vous avez sur cette liste : strictement rien de moins, mais aussi strictement rien de plus ! Pas de demande implicite de type “Ah mais je voulais aussi que l’image soit visible en noir et blanc” : si ce n’est pas sur la liste, ce n’est pas à faire.\n\nCette liste peut être écrite selon votre format favori : un cahier des charges, une série de directives Gherkin, l’important c’est qu’elle soit écrite, claire et précise. En d’autres termes, vous définissez ici votre propre cahier des charges.\n\nLe résultat final doit donc être quelque chose dans ce style :\n\nAs a client\nI want to see my product in 3 dimensions\nSo that I can know what it looks like\n\nAs a client\nI want to be able to rotate my product using the arrow keys\nSo that I can check it out entirely\n\nAs a client\nI want to be able to zoom on my product\nSo that I can see even the smallest details\n\n\nUne fois que vous savez quoi faire, on peut vraiment commencer à mettre la main dans le code !\n\n// TODO : make the code below work 💻\n\nStop. Lâchez tout.\n\nJe vous vois déjà, votre liste de points en main, à tenter de la faire rentrer dans votre gros projet à grands coups de burin, de vous gratter la tête à comprendre pourquoi ça ne rentre pas, et qu’est-ce qui a bien pu casser, cette fois.\n\nUn peu de calme : le but d’un spike n’est pas de faire tout fonctionner, pas du tout. Prenez de la distance, et on va y aller en douceur.\n\nPour commencer, isolez une partie de votre projet et de vos points objectifs. Il existe plusieurs moyens de s’y prendre : créer un nouveau projet, créer une nouvelle page avec seulement quelques composants, décharger votre backend… On veut un environnement le plus propre possible.\nBeaucoup de projets sont vieux, et si mal conçus qu’il aurait fallu les jeter au bout de deux ans. On cherche ici à se détacher au maximum de cette dette technique.\n\nN’hésitez pas à utiliser des mocks, des faux appels et résultats au reste de votre application :  en simulant comment se comporte le reste de votre projet sans véritablement y faire appel, vous diminuez au maximum votre marge d’erreur, et vous assurez que vous contrôlez la moindre information qui transite par votre code.\n\nMaintenant seulement, vous pouvez prendre votre clavier, et coder. Regardez comment implémenter chacun de ces points dans votre code propre de manière épurée.\nÇa fonctionne du premier coup ? Génial, notez comment vous avez fait ! Ça ne marche pas ? Dommage, mais ce n’est pas une raison pour Ctrl+Z et recommencer. Notez bien ce qui n’a pas marché, avant de retenter ! Si ça ne marche toujours pas au bout de 2/3 essais, pas de soucis, n’hésitez pas à laisser ce point de côté et passer à un autre. Mais écrivez tout, car cela va vous servir très bientôt !\n\nif(bug == true) { delete(bug); console.log(“It works !”); } 🤖\n\nIl peut cependant arriver que, parfois, tous vos efforts ne mènent à rien. Vous avez déjà passé plusieurs jours sur les différents sujets du spike, et vous n’avez pas encore identifié de solution pour faire fonctionner le tout.\nDans ce cas-là, pas de panique ! Il s’agit également d’un des objectifs du spike. Après tout, si vous n’avez pas pu réaliser votre objectif dans un cadre réduit, il est bien probable que vous n’auriez jamais pu le faire fonctionner dans votre projet lui-même.\n\nLes mêmes points qu’indiqués ci-dessus continuent de s’appliquer : notez ce que vous avez tenté et les soucis rencontrés avec chaque implémentation. Puis, continuez le processus détaillé ici : ce n’est pas parce que votre code n’as pas fonctionné qu’il ne doit surtout pas être présenté. Peut-être un de vos collègues trouvera-t-il la ligne qui vous manque, ou le point-virgule que vous avez oublié : mais peut-être aussi qu’il vous aidera à comprendre ensemble pourquoi la solution ne fonctionne pas dans votre cadre.\nEt puis, vous pourrez alors vous poser la question : est-ce qu’il faut bien faire comprendre que la tâche demandée est irréalisable, ou est-ce qu’il faut prévoir un chantier pour réussir à trouver un moyen de remplir la requête ?\n\nL’instant doc 📝\n\nUne fois que vous avez terminé de coder, il est temps pour vous de poser votre IDE, et de sortir votre outil de documentation favori : Confluence, Jira, que sais-je. \nPuis, écrivez un compte-rendu de votre aventure. Présentez l’origine de votre spike (Le Pourquoi), ce que vous avez tenté (Le Comment). Expliquez ce qui a marché et ce qui n’a pas marché : cela vous servira lorsque vous implémenterez vraiment la feature !\nEnfin, écrivez également les étapes qu’il faudrait suivre pour terminer la feature : ajoutez un maximum de détails techniques. Ce sera autant de problématiques en moins pour le pauvre dev qui va récupérer les US après vous.\n\nJe vous suggère donc de faire un plan de ce type :\n\n\n  Problème - Expliquez ici l’état initial. Qu’est-ce qui était demandé ? Pourquoi avoir choisi de faire un spike ? Quel en est l’objectif ?\n  Observations  - Indiquez là vos réflexions et le code que vous avez produit. Expliquez ce que vous avez tenté, les problèmes rencontrés et les solutions établies, vos pistes de réflexion. N’hésitez surtout pas à détailler !\n  Actions - Enfin, détaillez dans cette dernière partie ce qu’il restera à faire afin de transformer ce spike en une feature fonctionnelle. Quels bugs corriger ? Quels points n’ont pas encore été réalisés, et comment faire pour les réaliser ?\n\n\nPour la dernière étape, je vous conseille de réaliser un tableau d’actions SMART afin de définir au mieux les tâches à réaliser.\nLe principe SMART suppose qu’une tâche doit être composées des cinq caractéristiques suivantes afin d’être pertinente :\n\n  Elle doit être Spécifique, afin que l’objectif soit clair et concis (Qu’est-ce que je dois faire ? Exemple de réponse : « Il faut que l’image d’un objet soit en 3D »)\n  Elle doit être Mesurable, pour définir un objectif quantifiable (Quant est-ce que ma tâche sera finie ? Exemple de réponse : « Il faut que je puisse faire tourner l’image avec les flèches gauches et droites du clavier  »))\n  Elle doit être Atteignable, sans demander de décrocher les étoiles (Comment réaliser ma tâche ? Exemple de réponse : « Utiliser la méthode Get3D de la librairie Easy3D »))\n  Elle doit être Réaliste au sujet en cours, donc nécessaire à l’accomplissement final (Est-ce qu’il est pertinent de prendre du temps pour faire ça ? Exemple de réponse : « Afin que notre client puisse voir l’avant et l’arrière de nos produits »)\n  Elle doit être définie de façon Temporelle, afin de ne pas pouvoir s’éterniser (Pour quand ma tâche doit-elle être réalisée ? Exemple de réponse : « A réaliser avant que la feature soit considérée terminée »)\n\n\nDans le cas où une des tâches que vous avez devisé ne peut pas répondre à un de ces cinq points, alors il est probable qu’elle ne soit pas suffisamment précise : peut-être la tâche manque-t-elle de cadre ou de contexte, ou le temps nécessaire pour la réaliser ne peut que difficilement être justifié. Je vous invite alors à la supprimer, ou à la fusionner avec une autre jusqu’à enfin pouvoir répondre à ces cinq questions !\n\nBien entendu, n’hésitez pas à modifier le plan de cette documentation comme vous l’entendez : vous êtes celui qui allez l’utiliser, après tout !\n\nLa doc est finie ? Il ne reste plus que deux étapes, puis on pourra enfin considérer ce spike comme fini !\n\nPresentation_Spike.ppt 🎬\n\nAvant de pouvoir clôturer ce spike, il serait bien d’avoir des retours extérieurs. Pour ça, rien de mieux que de le présenter à votre équipe !\nOrganisez ensemble une réunion, pas très longue. Au sein de mon équipe, une demi-heure suffit. Il vous faudra peut-être un peu moins ou un peu plus de temps.\n\nUtilisez cette présentation afin de montrer, étape par étape, ce que vous avez réalisé. Rappelez tout d’abord les objectifs du spike, avant d’expliquer votre analyse du problème et les objectifs que vous avez identifiés. Puis, présentez les différentes implémentations que vous avez tentées, avant de conclure en montrant votre documentation et en expliquant les tâches qui restent à accomplir pour réaliser la feature objectif.\n\nIl est très important que vous ne présentiez pas uniquement le code que vous avez réussi à faire fonctionner, mais aussi vos tentatives échouées, et ce pour plusieurs raisons. Tout d’abord, il est tout à fait possible qu’un de vos collègues, en voyant votre présentation, réalise une de vos erreurs et vous l’indique. Mais surtout, si quelqu’un d’autre que vous récupère une des tâches restantes, il risque de tenter les mêmes pistes que vous, et rencontrer les mêmes problématiques que vous !\n\nUne fois votre présentation terminée, débattez avec le reste de votre équipe. S’ils sont d’accord avec vous sur le plan d’action que vous avez établi grâce à votre tableau SMART, il vous reste une toute dernière étape à accomplir !\n\n“Happily ever after…” 💭\n\nMaintenant que tous vos coéquipiers ont pu constater et valider votre travail, il ne vous reste plus qu’à acter la mise en place : et pour ça, rien de mieux que, aux côtés de votre Product Owner (Ou de l’équivalent dans votre équipe) de créer des tâches, User Story, post-its, ou quoi que ce soit, pour que les étapes restantes soient visibles et accessibles par tous !\n\nN’hésitez pas à le guider pour ajouter encore une fois des détails techniques dans ces US ou tâches : vous avez réalisé l’analyse, il serait dommage de ne pas l’utiliser, et ce sera autant de temps gagné pour votre équipe. Tant que vous y êtes, pensez aussi à ajouter un lien vers votre documentation, ou vers une vidéo de votre présentation… Plus il y aura de détails, mieux ça sera !\n\nIl est également possible, comme indiqué plus haut, que la tâche qui a entraîné la réalisation de ce spike se découvre être impossible à implémenter. Il s’agit là également d’un point à faire avec votre Product Owner, afin de décider ensemble de la procédure à suivre : peut-être faudra-t-il redéfinir les critères d’acceptation, ou bien laisser tomber complètement cette idée.\n\nreturn 0;\n\nVous avez fini votre spike ! Ce qui était à l’origine une tâche complexe, confuse ou impossible à prévoir, est désormais divisée en une série d’étapes, qui sera désormais bien plus aisée à réaliser pour votre équipe. Alors, satisfait ?\n"
} ,
  
  {
    "title"    : "Handling dependencies failures in an API gateway",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front, resiliency",
    "url"      : "/2022/08/12/backend-fallbacks.html",
    "date"     : "August 12, 2022",
    "excerpt"  : "Welcome to our second article about the backend architecture and its api gateway.\nIn the first part, we talked about the BFF and all services it depends on.\nToday we’re going to take a look at what to do when one of them (or many), fails to respon...",
  "content"  : "Welcome to our second article about the backend architecture and its api gateway.\nIn the first part, we talked about the BFF and all services it depends on.\nToday we’re going to take a look at what to do when one of them (or many), fails to respond.\n\nService dependencies\n\nAs seen previously, the BFF uses multiple data sources and services to create a full layout.\n\nThose services are used to gather the contents to be displayed in the application:\n\n  getting user personalisation data;\n  advertising and analytics configuration;\n  asking if the user has some authorizations.\n\n\nIf we don’t want our BFF to become one giant SPOF (1), we need to be resilient to the death (2) of those dependencies, any of them, at any time!\nYou must keep in mind that our top priority is to always be able to answer something readable to the frontend applications.\n\nDDD\n\nFirst thing first, we are using a DDD (3) approach for our modeling.\nThis means that we focus on the business, as described by our Product Owner. We try not to worry about the various implementation of our backend’s friends and their different services.\n\nA picture is always easier to understand.\n\n\n\nAbove, we can see that when a user ask for a layout A, we are looking to resolve who is A.\nFrom the domain point of view, the page collection is only an interface.\n\nIn the picture below, we see the “Page collection implem (Infra)”.\nIt’s a layer implementing the interface defined in the domain. It uses multiple clients that call the services behind.\nIt’s its responsibility to chose which service to look on for the page.\n\n\n\nDDD is a too large subjects to be perfectly defined in this article. If you want to dig deeper into it, there are multiple great reads, feel free to check them out!\nNow, how does this help us?\n\nHandling failures\n\nFailures handling is done by the middle layer seen in the previous example.\nIts goal is to catch error (4), and convert them to something expected and defined by the interface.\n\nThat said, its responsibility is not to know what the expected answer is. To do that, we use the domain.\n\nLet’s see with a small code sample.\n\nNote: The following example is not a real use-case, but it’s representative and simple enough to illustrate how it works.\n\nIn the code below, we see a class that represents the subscribing status of a user, which has two properties:\n\n  hasAccess controls whether the user can read protected contents;\n  isSubscribed is used in analytics, and to show subscription pages.\n\n\n&amp;lt;?php\nfinal class SubscribeStatus\n{\n    private function __construct(\n        public readonly bool $hasAccess,\n        public readonly bool $isSubscribed,\n    ) {\n    }\n\n    public static function createAnonymous(): self\n    {\n        return new self(false, false);\n    }\n\n    public static function createSubscribed(): self\n    {\n        return new self(true, true);\n    }\n}\n\n\nTo create such an object, we use either one of the two static functions, depending on the status we get from the subscriptions API.\nThis is done in the middle layer, but the business is kept in the domain.\n\nTo handle the failure, we add a new named constructor, dedicated to this specific case.\n\n    public static function createUnknown(): self\n    {\n        return new self(true, false);\n    }\n\n\nWhen an error happens and we can’t retrieve the user subscription status, we now have a fallback option.\nWith this fallback option, the user will:\n\n  be able to access any content, it’s better to let an anonymous user access a content it should not, that blocking a paying customer;\n  still be reported as not subscribed and will see all available offers.\n\n\nMost of the time, the answer is even simpler than this one.\n\nAnother example would be user’s viewing statuses. If we can’t retrieve them, we don’t display any progress bar.\nUsers won’t be able to tell if they have seen a content, but they will still be able to navigate the application.\n\nInfrastructure solution, the stale cache\n\nIn some cases, the above solution doesn’t work.\nFor example, contents information cannot be replaced by default values. If we don’t know about a video or a program, we cannot guess what it is.\n\nLuckily, we can rely on the stale cache.\nStale cache is an old cache entry which is expired. When the cache finds such entry, it usually ignores it and asks for a new version of the response.\nIn case of failure, we can use the available staled version.\n\n\n\nThe limitation is that a response must have been cached at least once, in order to have a staled version.\n\nWhen there is no stale cache, we don’t display the content (5).\n\nSo far, we are only using it with http implementation:\n\n  called API must answers with stale-if-error cache directive, it allows for the response to be used while stale when an error happens;\n  called API can answer with stale-while-revalidate cache directive, for better performances;\n  calling API can query with max-stale cache directive, to use stale response see the mdn for more on those headers;\n  on the client side, we are using the Kevinrob/guzzle-cache-middleware to do the job.\n\n\nFor an entry cached for up to 10 minutes (answered with max-age), we allow up to 4 hours of stale cache (with stale-if-error).\nSince we are using a shared cache, we are using max-stale when querying, with a random value up to 1 hour.\nThis makes most requests use the last stale response while one of them ask for a fresher response.\nThose values are chosen according to our platform usages where peak visitor last for about 2 to 3 hours at night.\n\nWe plan to expand its usage to other kinds of cached entries, such as manually saved data, and database queries.\n\nConclusion\n\nIn today’s post, we have seen how we handle the loss of our dependencies by anticipating their potential failures and preparing default acceptable behaviours.\n\nNext time, we will see how we can spare some traffic on those dependencies when they’re struggling with traffic.\n\nNotes\n\n  SPOF, as single point of failure since all frontend applications have to rely on the BFF, I cannot resist linking this excellent xkcd.\n  By “death”, we mean anything unexpected. It can be a 500 error code, a timeout, a wrong content. We will talk a bit more about this in the next article.\n  DDD, as domain driven design, you can read more about it on Martin FOWLER’s website.\n  Throwing errors is still allowed, but restricted to domain exceptions, and must be specified in the method’s declaration in the interface (i.e. via a comment).\n  There will be a dedicated article on partial rendering.\n\n\nFrom the same series\n\n\n  What’s a BFF\n  Handling API failures in a gateway\n  What’s an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  🇺🇸 Encrypt AWS AMIs: one way to do it wrong\n  🇫🇷 Bedrock à la kubecon 2022 (4 articles)\n\n"
} ,
  
  {
    "title"    : "How to ingest 400GB of logs per hour?",
    "category" : "",
    "tags"     : " onprem, cdn, logs, aws, cloud, nginx, vector, lambda, s3, glue, athena",
    "url"      : "/2022/08/08/private-cdn-logs.html",
    "date"     : "August 8, 2022",
    "excerpt"  : "Bedrock Streaming is a company that sells a white labeled streaming and live platform. Our customers are media groups, TV channels, and streaming companies. Our goal is to deliver a state-of-the-art streaming platform to our customers.\n\nTo achieve...",
  "content"  : "Bedrock Streaming is a company that sells a white labeled streaming and live platform. Our customers are media groups, TV channels, and streaming companies. Our goal is to deliver a state-of-the-art streaming platform to our customers.\n\nTo achieve this goal, we have our own Content Delivery Network (CDN), made of several bare metal servers racked in our Data Centers. Those servers run Nginx and are designed to output hundreds of Gbps (several tens of Pb per month) to end-users. We use them to cache video content at our infrastructure’s edge.\n\nThis increases efficiency of the platform 96 times out of 100, as video traffic doesn’t have to flow all the way through our infrastructure, and improves user experience as it serves video faster. Also, it diminishes the cost of our Video On Demand (VOD) infrastructure as we need less servers in VOD Stack.\n\nThis in-turn increases end-users (clients of our customers) satisfaction with the service.\n\nWho needs to ingest 400GB of logs per hour anyway?\nEvery time someone watches a video, it generates traffic on our CDN, resulting in a lot of access logs. Without filtering, it averages to 400GB uncompressed logs per hour.\n\nThis is why, at first, we chose to not log 2XX or 3XX HTTP codes. We had too many of them, and we considered them not as worth it as 4XX and 5XX. The 4XX and 5XX can be especially useful for debugging a particular situation or, from a broader perspective, improving the user experience.\n\nThis was the kind of Nginx configuration we had deployed:\nmap $status $loggable {\n    ~^[23]  0;\n    default 1;\n}\naccess_log /path/to/access.log combined if=$loggable;\n\n\nGiving autonomy for all teams on logs\n\nAt the end of 2021, the finance team approached us with a challenge: how to bill our customers based on their end-users CDN usage?\nThis was in fact a need we already anticipated, we tried the nginx module Traffic_accounting, but it did not satisfy us fully. This module calculates and exposes metrics on-the-fly, which is CPU and memory intensive, especially above 50Gbps of traffic per server.\n\nWe also had another objective that wasn’t addressed with the nginx module. We needed to give autonomy to QA, Video, Data, and Finance teams. We wanted to allow them to use CDN logs when they needed without having to ask for it, and ideally in a practical and unified way.\n\nThe company philosophy states that we are user obsessed and that we do not finger point. We work as a team to offer the best user experience, this is why we make all our logs available to all teams. We didn’t come around to do it for the CDN as the volume of logs was too much of a constraint.\n\nTechnical Solution\n\nAt Bedrock, we like to keep things simple. We think our CDN main mission is to serve video as efficiently as possible. Our CDN’s servers can’t keep PetaBytes of logs on their disks. This is why we chose to output logs to Amazon S3.\n\nThe real benefit to using S3 is that you can easily plug it into Glue and Athena which allows you to request TeraBytes of data easily.\n\n\n\nSending logs to S3: Vector\n\nTo send logs from our CDN servers to Amazon S3 bucket, we had many options, but chose to test two approaches: Fluentd and Vector. Fluentd is the legacy one, and Vector the new rusty one.\n\nAfter a quick evaluation, we decided to go with Vector as it seemed more memory efficient and output more Logs Per Second under heavy load than Fluentd.\n\n\nSource: Who is the winner — Comparing Vector, Fluent Bit, Fluentd performance from Ajay Gupta\n\n\nWe have Nginx and Vector installed on the CDN servers. Nginx now outputs all the access logs to a file. Vector reads the file, compresses logs to GZIP format and every 10Mb sends the logs to S3. Nginx may generate at peak 600GB of logs; we only send 10GB.\n\nThose logs are then locally cleaned by Logrotate.\n\nStoring logs: S3\nWe chose to store logs on an S3 bucket. We figured it was the most scalable and time efficient. S3 buckets can grow to PetaBytes easily. It is a few terraform lines away, this is convenient as we handle all our infrastructure with Terraform.\n\nWe configured our bucket to use several lifecycle policies. One to automatically clean logs after 365 days, another to remove incomplete uploads, and another one to immediately remove files with a delete marker. Also, we configured the storage class in intelligent tiering mode to store logs according to their access frequency.\n\nThis will permit us to diminish the cost of our S3 bucket and not have an ever-increasing S3 bill.\n\nPartitioning logs on S3: Lambda stack\n\nOnce logs are stored in S3 bucket, we need to classify and sort them in order to extract valuable intel. At Bedrock, we already use a modified version of a lambda stack, that does just that. Originally designed for Cloudfront, we have been using it also for Fastly and now for our Private CDN. You can find the original version at AWS Sample Github.\n\nWe have 2 different parts in this lambda stack.\n\n\nsource: moveAccessLogs\n\nThe first part is called by S3 Event when a new file is pushed to a specific path. This lambda moves the file to a path assigned per server and per hour. This way, logs are stored for each server, each month, each day and each hour in a separate prefix.\n\n\nsource: transformPartition\n\nThen, another lambda transforms logs into Parquet format. Parquet is an open source format from the Apache Foundation. It is commonly used in big data. It takes up little space and is very effective.\n\nWe chose to use AWS glue in order to create a database of our logs. The columns of the table are based on our log format. We can then request everything we want in Athena.\n\n\n\nWe are now capable of extracting the bytes sent from a particular virtual host and sum it over a month for all CDN servers to bill our customers.\nThose logs are now available for all the teams who may need them to improve their application or to debug an issue they are facing.\n\nConclusion\nWe chose Vector to transport our private CDN logs to an S3 Bucket. Then, we chose to reuse an AWS Stack using Lambda and Glue to extract information from these logs, asynchronously. This stack is used in production for several months on other projects.\nAll the teams that needed to extract value from our CDN logs are now autonomous to do so. We are now able to bill our customers based on their CDN usage.\n"
} ,
  
  {
    "title"    : "Retour sur la conférence MiXiT 2022",
    "category" : "",
    "tags"     : " conference, agile",
    "url"      : "/2022/07/28/retour-sur-mixit-2022.html",
    "date"     : "July 28, 2022",
    "excerpt"  : "\n\nMiXiT est une conférence “avec des crêpes et du cœur” qui se déroule à Lyon. Les sujets sont assez variés abordant autant l’agilité, que la programmation, le droit ou encore l’histoire de l’informatique.\n\nVoici un résumé des conférences de l’édi...",
  "content"  : "\n\nMiXiT est une conférence “avec des crêpes et du cœur” qui se déroule à Lyon. Les sujets sont assez variés abordant autant l’agilité, que la programmation, le droit ou encore l’histoire de l’informatique.\n\nVoici un résumé des conférences de l’édition 2022 qui nous ont le plus marquées.\n\nHow to build the alert system that France deserves?\n\nGaël Musquet nous a d’abord expliqué le rôle de Gustave Ferrié, qu’il considère comme le premier hacker, qui a installé des mâts de télégraphe sans fil en 1902, entre les émetteurs en Martinique, pour remplacer le câble télégraphique, détruit lors de la catastrophe de la montagne Pelée du 8 mai 1902. Cet homme avait saisi l’intérêt d’avoir un système de communication fiable.\n\nGaël Musquet nous explique ce qu’on est en droit d’attendre en 2022 d’un pays moderne, concernant les alertes sur les risques majeurs, qui varient selon notre emplacement (du tsunami à la rupture de barrage artificiel).\n\nIl nous incite à lire le DICRIM de notre ville (celui de Lyon) ainsi qu’à nous procurer un poste de radio à piles, car dans l’éventualité d’un moment catastrophique sans Internet et sans satellites, comment ferons-nous pour nous tenir au courant de ce qu’il faut faire pour rester en vie ?\n\nPage du talk sur le site de MiXiT et voir le replay\n\nMeet NULL the UNKNOWN\n\nDans cette conférence, Laëtiia Avrot entame un rappel de la norme SQL, que PostgresQL implémente au plus près, sur la valeur de NULL en SQL. Et la valeur UNKNOWN est également abordée. Notamment la complexité induite par le fait qu’un champ de type Boolean peut se retrouver avec comme valeurs possibles : True, False, UNKNOWN et NULL. Cela donne un système à quadruple valeur. Pour un champ typé.\n\nNULL est plus facile à définir par ce qu’il n’est pas qu’en expliquant ce qu’il est.\nUne option intéressante pour mettre en évidence la valeur NULL dans PostgresQL est d’en définir nous-même une valeur affichée.\n\nEnsuite, Laëtitia nous propose un Quizz. Sur une base de données qu’on connaît, chaque fois la même question est posée sur “Combien de lignes vont être retournées par la requête SQL ?”\n\nC’est intéressant, car chaque question comporte un degré de complexité élevé impliquant l’usage de la valeur NULL, tout en suivant la logique de la norme SQL. Cerise sur le gâteau, Laëtitia propose en “Réponse D”, le nom d’une scientifique célèbre et nous en donne une courte biographie à chaque question.\n\nLiens\n\n\n  Blog de l’oratrice, Laëtitia Avrot\n  Page du talk sur le site de MiXiT et voir le replay\n\n\nParlez de vous, faites des feedbacks\n\nLe feedback est un outil communicationnel qui permet de formuler un avis sur une situation passée dans le but de gérer les situations futures.\n\nOn peut trouver plusieurs formes de feedbacks :\nle feedback est à destination de la personne, pour l’aider à s’améliorer. Elle peut décider de le suivre ou non,\nla demande que l’on fait à quelqu’un est à notre bénéfice (on demande à la personne de changer un comportement qui nous gêne) en laissant la possibilité à la personne de décider si elle veut ou non répondre favorablement à cette demande,\nl’exigence qui est aussi à notre bénéfice, mais pour laquelle on ne laisse pas le choix (dans le cadre d’une relation hiérarchique)\n\nJulie Quillié propose un modèle de feedbacks basé sur la CNV (Communication Non Violente) et qui peut se résumer de la manière suivante.\n\nFeedback basé sur la CNV (Communication Non Violente)\n\n\n  On vérifie la disponibilité de la personne en lui demandant si elle est d’accord pour qu’on lui fasse des feedbacks et sous quelle forme.\n  On formule le feedback :\n  Décrire une Observation, les faits (= pas de jugement)\n  Exprimer le Sentiment que cette situation a engendré\n  Expliquer le Besoin qui est la source du sentiment ressenti\n  et finir par faire une Demande (= réalisable, formulée positivement, précise)\n\n\nUn exemple :\n  Nous avions rendez-vous à 12h et il est 12h30 = observation, factuel.\n  Je suis très fâché car je m’étais organisé pour être à l’heure = le sentiment\n  C’est important pour moi de ne pas perdre de temps et de pouvoir rester libre dans mon organisation = le besoin\n  La prochaine fois que tu sais que tu seras en retard, peux-tu stp m’appeler dès que possible pour me le signaler ? De cette manière, je peux me réorganiser facilement. = la demande\n\n\n  On vérifie ce qui a été reçu par la personne. On lui propose de nous reformuler ce qu’elle en a retenu. Cela permet de vérifier que le message que l’on voulait faire passer a bien été entendu.\n\n\n2ème possibilité pour faire un feedback : le feedback en 4 temps\n\n\n  On demande à la personne ce qu’elle a aimé dans ce qu’elle vient de faire\n  \n    On lui demande ensuite ce qu’elle aurait aimé faire différemment\n\n    On lui demande si elle veut qu’on lui donne notre feedback\n  \n  “Moi, j’ai aimé …, parce que … ” : on parle de ce que ça nous a apporté (clarté, motivation, inspiration, soutien, etc.)\n  “Et j’aurais aimé …  de différent, parce que … ” on parle de ce que ça nous apporterait (clarté, motivation, inspiration, soutien, etc.)\n\n\nEt en bonus : “Peux-tu me dire comment tu reçois ce que je te dis ?”\n\nPage du talk sur le site de MiXiT et voir le replay\n\nArrêtez l’auto-sabotage et sortez de la boucle (systémique)\n\nDans cet atelier, Albane Veyron nous explique que nous avons tous des croyances sur nous-mêmes et sur les autres. Les croyances sont des pensées qui sont des vérités, pour nous. Elles ont plusieurs origines : l’enfance, notre cercle social et notre expérience de vie.\n\nLes croyances peuvent être aidantes ou limitantes.\n\nL’atelier commence par une première phase qui consiste à reconnaître une de ses croyances limitantes :\n\n  les généralisations : personne, tout le monde, toujours, tout le temps, jamais, trop, je dois, il faut, pas assez\n  les barrières infinies aka les bonnes excuses pour ne pas passer à l’action : j’aimerais, mais … / je pourrais, mais …\n  les sensations de déjà vu : les blocages et les situations récurrentes\n\n\nUne fois qu’on a repéré une de ses croyances limitantes, on l’écrit sur une feuille et on va ensuite décomposer cette croyance et réfléchir à :\n\n  son origine : d’où nous vient cette croyance ? depuis combien de temps fait-elle partie de nous ? nous vient-elle de notre éducation ?\n  les bénéfices : quels bénéfices nous apporte cette croyance ? qu’est-ce qu’elle nous permet ?\n  les inconvénients / les freins : en quoi cette croyance nous gêne et quels sont les impacts sur notre vie (pro ou perso) ?\n  les contradictions : a-t-on déjà fait quelque chose ou été dans une situation qui vient contredire cette croyance ?\n\n\nOn va ensuite venir agrémenter notre croyance avec tous ces éléments puis, pour finir, transformer notre croyance limitante en une croyance aidante.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nComment fonctionne un gestionnaire de mots de passe\n\nLes mots de passe sont partout. Ils nous permettent d’accéder à nos photos, nos comptes bancaires, nos documents de santé et bien d’autres données sensibles que l’on ne souhaite pas voir aux mains d’individus que l’on ne connaît pas.\nTout le monde sait que l’on doit avoir des longs mots de passe mais comment tous les retenir ? C’est là que les gestionnaires de mot de passe entrent en jeu. Mais peut-on leur faire confiance ? Comment ça marche au juste ? C’est à cette question qu’a souhaité répondre Eric Daspet pendant sa conférence.\n\nLe rôle d’un gestionnaire de mots de passe est de permettre à son utilisateur d’utiliser qu’un seul mot de passe pour ensuite laisser l’outil générer et mémoriser tous les autres mots de passe. On a plus qu’à retenir un seul mot de passe qui peut donc être long et complexe. L’exercice de mémoire sera alors moins compliqué que si on en avait plusieurs à retenir.\n\nÀ travers son exposé, on découvre un peu plus tous les procédés de cryptographie utilisés afin de gérer les mots de passe que l’on va créer ou modifier en utilisant ces outils.\nGrâce à de nombreux schémas, il explique clairement les différentes étapes de chiffrements utilisées que ce soit pour la création du mot de passe maître, la création et le changement des mots de passe, l’affichage des mots de passe et même le fonctionnement du partage de mots de passe (lorsque celui-ci existe dans l’outil).\n\nOn découvre pendant cette heure que les gestionnaires de mots de passe ne cherchent pas à réinventer la roue en matière de cryptographie mais s’appuient sur des concepts déjà éprouvés et robustes. On apprend aussi que tout est chiffré de bout en bout et que seul celui qui détient le mot de passe maître (l’utilisateur donc, même l’outil ne le connaît pas et n’en a pas besoin) peut interagir avec les mots de passe créés. Rassurant, non ? En tout cas, me voilà maintenant prêt à expliquer autour de moi pourquoi il est grand temps de passer à un gestionnaire de mot de passe !\n\nPage du talk sur le site de MiXiT et voir le replay\n\nOptimiser votre revue de code avec le rebase interactif\n\nGIT est un outil bien connu des développeurs de nos jours, mais dès qu’on s’écarte des commandes traditionnelles (checkout, commit et push), on sait bien moins ce que l’on peut faire d’autre avec.\n\nSonia Seddiki nous explique ici comment rendre la revue de code, souvent longue et fastidieuse, plus simple et agréable pour nos collègues avec quelques astuces qu’elle a partagées avec nous lors d’un live coding.\nContrairement à l’idée que j’en avais, le rebase interactif n’est pas là que pour nettoyer les noms de commit sans aucun sens que j’avais mis dans la précipitation mais que c’est un outil bien plus puissant.\n\nElle nous a ainsi montré comment elle utilise cette commande afin d’organiser et de donner une chronologie à son travail rendant ainsi la revue de code plus facile. Elle a ainsi, devant nos yeux, changé des fichiers de commits, réorganisé l’ordre des commits et tout ça sans altérer le code produit.\n\nÉvidemment, c’est une habitude à prendre, elle-même le souligne que ce n’est pas facile d’exporter cette bonne pratique au sein des équipes avec qui elle travaille. Mais la démonstration m’a convaincu, je vais m’essayer à cette pratique et qui sait, un jour j’arriverai peut-être à mon tour à convaincre des gens de mon équipe à en faire de même.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nViolence Herméneutique - Comment éviter le malaise\n\nLe MiXiT est aussi un évènement nous permettant d’ouvrir notre esprit à des connaissances qui sortent de notre quotidien. Cette conférence animée par Romeu Moura et Sara Dufour en fait partie. Ce talk nous fait découvrir le concept d’herméneutique, défini en début de présentation comme étant “La connaissance d’un concept permettant l’interprétation”. Si vous n’avez rien compris à cette définition à ce stade, c’était également mon cas.\n\nMalgré cette introduction confuse, petit à petit, en allant de plus en plus dans le détail, des sujets apparaissent et donnent sens à ce concept. On y parle de systémisme, de charge mentale, de patriarcat et autres systèmes de notre société dont l’exercice de compréhension va plus loin que leur simple mot ou leur définition. L’herméneutique consiste à comprendre les fondements et rouages d’un système, qu’on y appartienne ou non.\n\nMais notre société, et l’humain, tend à compliquer cet exercice de compréhension de concept. C’est là qu’on arrive à la notion de violence herméneutique, à savoir tous les mécanismes conscients et inconscients, systémiques ou non, internes ou externes, qui vont venir entraver et contraindre l’herméneutique. De réels freins à la compréhension d’un système. Ils peuvent prendre plusieurs formes, comme la notion de norme, le fait de nier l’existence d’un système ou de réfuter un sujet du simple fait qu’il soit considéré tabou. On y retrouve également la déformation de mots, et le fameux “wokisme”.\n\nIl s’agit d’une conférence passionnante, dérangeante et éclairante que je conseille à tous. Le début piétine un peu, mais le voyage en vaut la peine.\n\nPage du talk sur le site de MiXiT et voir le replay\n\nDesigner pour le service public\n\nCela peut faire un peu peur dit comme ça, mais je suis allé sceptique à cette conférence d’Anne-Sophie Tranchet. J’étais rattaché à une image peu flatteuse des outils du service public, alors que ces derniers ont connu une vraie progression ces dernières années. Anne-Sophie fait partie du programme beta.gouv qui intervient auprès des administrations pour les services numériques.\n\nC’est armé des bonnes pratiques de nos métiers que Beta.gouv a la mission de transformer et d’accompagner les services publics. On y apprendra le parcours d’Anne-Sophie, ce que travailler pour le service public veut dire, ainsi que les projets et challenges qui en découlent. Leur méthodologie centrée utilisateur leur permet de travailler en itération, et de délivrer de la valeur, en incubation d’abord, puis jusqu’à un développement national en fonction des retours sur le service.\n\nOn peut citer quelques réalisations Beta.gouv comme la plateforme dossierfacile.com, qui facilite la création de dossier pour une location, ou 1000 Premiers Jours, qui délivre des informations et un accompagnement sur la grossesse et les 2 premières années de l’enfant\n\nPage du talk sur le site de MiXiT et voir le replay\n\nNos autres conférences coup de coeur\n\n\n  Ma vie est un ticket de Romain Couturier, une conférence raconté avec dessins légère et qui donne des idées pour lutter contre la mauvaise utilisation des outils de ticketing\n  Tout ce que l’on ne vous pas dit sur l’IA de Amélie Cordier, une conférence pleine d’humour sur ce qu’est et n’est pas une IA\n\n"
} ,
  
  {
    "title"    : "Comment appliquer automatiquement des modifications sur une codebase JS 🤖",
    "category" : "",
    "tags"     : " javascript, outil, cytron, frontend, react, refactor, js",
    "url"      : "/refactorer-avec-jscodeshift",
    "date"     : "July 26, 2022",
    "excerpt"  : "Dans cet article, je vais vous présenter JSCodeshift, une libraire qui va vous permettre d’analyser et appliquer automatiquement des modifications sur du code Javascript ou Typescript.\n\nCas d’école 👨‍🎓\n\nMaintenir à jour les dépendances de nos proj...",
  "content"  : "Dans cet article, je vais vous présenter JSCodeshift, une libraire qui va vous permettre d’analyser et appliquer automatiquement des modifications sur du code Javascript ou Typescript.\n\nCas d’école 👨‍🎓\n\nMaintenir à jour les dépendances de nos projets JS est l’une des règles primordiales que nous nous efforçons de bien respecter pour ne pas avoir à jeter nos applications tous les deux ans. 🗑\n\nCette tâche exige souvent d’un développeur plus de travail que de simplement changer les versions des libraires dans le package.json.\nSi une dépendance est utilisée dans différentes parties du code et qu’un breaking-change est introduit, on peut vite se retrouver avec des centaines de fichiers à modifier manuellement.\n\n\n\n\nℹ️ Exemple d&#39;un project Javascript qui ne respecte pas cette règle\n\n\n\nC’est un problème de ce genre que nous avons rencontré lors de la mise à jour de notre librairie d’internationalisation sur notre web app React en JS.\n\nAprès mise à jour, l’appel à l’API de la librairie change de forme :\n//Before\nconst t: (\n    translationKey: string,\n    // All options are passed as parameters\n    data?: object, // Data used for interpolation\n    number?: number, // Amount used for plural form\n    general?: boolean, // Use general plural form\n    renderers?: object // JSX renderers\n) =&amp;gt; string\n\n//After\nconst t: (\n    translationKey: string,\n    // Object containing all options\n    options?: {\n      data?: object, // Data used for interpolation\n      number?: number, // Amount used for plural form\n      general?: boolean, // Use general plural form\n      renderers?: object // JSX renderers\n    }\n) =&amp;gt; string\n\n\nPlus simplement, quelques exemples de transformations :\n// Before\nconst title1 = t(&#39;translationKeyExample&#39;)\nconst title2 = t(labelKey, { someData }, aNumber);\nconst title3 = t(&#39;translationKeyExample&#39;, undefined, 0);\n\n// After\nconst title1 = t(&#39;translationKeyExample&#39;); // Basic usecase with only one argument, nothing changed on this one\nconst title2 = t(labelKey, { data: { someData }, number: aNumber });\nconst title3 = t(&#39;translationKeyExample&#39;, { number: 0 });\n\n\nDans le cas le plus basique sans les arguments optionnels t(‘translationKey’) nous n’avons rien à modifier, mais dans les autres cas, il y a du changement à faire. 🧹\n\nLes solutions que nous avons écartées ❌\n\n\n  Avec un Find All, trouver toutes les utilisations de la librairie et modifier les appels problématiques à la main.\n    \n      Cette solution est la plus simple, mais peut être très répétitive, ce qui augmente la probabilité de faire une erreur. On aura du mal à uniquement filtrer les cas spécifiques qui nous intéressent.\n    \n  \n  Utiliser des RegExp pour mieux cibler les cas spécifiques\n    \n      Cela nous a permis de faire rapidement une estimation approximative du nombre de cas qu’il nous faudrait modifier, mais nous avons eu du mal à cibler correctement tous les appels et la modification se fait toujours à la main.\n    \n  \n  Créer un fichier de définition TypeScript pour la librairie, et laisser le Language Server Protocol ou son IDE trouver les appels problématiques\n    \n      La solution la plus rapide et la plus fiable pour la partie détection, mais qui demande toujours de faire les modifications à la main.\n    \n  \n\n\nMais il nous restait encore un Joker pour cette tâche. 🃏\n\nJSCodeshift 🪄\n\nCette librairie permet d’exposer facilement l’Abstract Syntax Tree, autrement dit la représentation du code après le parsing des fichiers.\nNous pouvons ainsi écrire des scripts qui nous permettent de parcourir cet arbre, de le modifier facilement, d’appliquer les modifications et de les formater.\nCes scripts s’appellent des codemods.\n\nPour en savoir un peu plus sur l’Abstract Syntax Tree, je vous conseille de jeter un coup d’œil à ASTExplorer qui vous permet de visualiser l’AST d’un fichier facilement pour en comprendre le fonctionnement.\n\nQuelques librairies ont proposé des codemods lors de leurs grosses mises à jour, par exemple React avec react-codemod.\n\n\n\n\nℹ️ Capture d&#39;écran du site ASTExplorer\n\n\n\nEn application 💪\n\nmodule.exports = function (file: FileInfo, api: API) {\n  const j = api.jscodeshift;\n\n  // If we don&#39;t find any &quot;Translate&quot; string inside our file, we can assume that it&#39;s safe to skip it\n  const regex = new RegExp(&#39;Translate[(]&#39;, &#39;i&#39;);\n  if (!regex.test(file.source)) {\n    return null;\n  }\n\n  return j(file.source)\n    .find(j.CallExpression, {\n      callee: {\n        type: &#39;Identifier&#39;,\n        name: &#39;t&#39;,\n      },\n    })\n    .filter(filterOutSimpleUsages)\n    .map(mutatePath(j))\n    .toSource();\n};\n\n\nDans la fonction principale du script, j’ai utilisé une expression régulière pour filtrer les fichiers qui ne possèdent pas la chaîne de caractères Translate(.\nCeci permet de gagner un peu de temps sur l’exécution. ⌛️\n\nEnsuite, je cherche dans le fichier une ou plusieurs variables t. Si aucune n’est présente, on peut passer au fichier suivant, sinon on continue le raffinage.\n\nOn passe dans un filtre qui va nous permettre d’enlever les usages de la fonction t avec un seul argument qui ne posent pas de problème.\n\nconst requiredPropertiesKeys = [&#39;data&#39;, &#39;number&#39;, &#39;general&#39;, &#39;renderers&#39;] as const;\n\n// Filter function to ensure that we enter the mutation function only if needed\nconst filterOutSimpleUsages = (p: ASTPath&amp;lt;CallExpression&amp;gt;) =&amp;gt; {\n  const args = p.value.arguments;\n\n  // If we only have the translation key, we don&#39;t need to refactor this usage\n  if (args.length === 1) {\n    return false;\n  }\n\n  // More than 2 arguments is an absolute sign of an old usage\n  // If second argument is not an object, we need to manually fix this case\n  if (args.length &amp;gt; 2 || args[1].type !== &#39;ObjectExpression&#39;) {\n    return true;\n  }\n\n  // If none of the above properties is found in second argument, we can say that this is an old usage\n  return requiredPropertiesKeys.every(\n    (requiredPropertyKey) =&amp;gt;\n      !(args[1] as ObjectExpression).properties.find(\n        // I needed to do some TS trickery to avoid getting warnings everywhere, sorry for that\n        (property) =&amp;gt; ((property as ObjectProperty).key as Identifier).name === requiredPropertyKey,\n      ),\n  );\n};\n\n\nFinalement, on peut passer dans la fonction de mutation, qui va nous permettre de modifier directement le code des fichiers.\n\n// Mutation function, we apply our modification to the AST\nconst mutatePath = (j: JSCodeshift) =&amp;gt; (p: ASTPath&amp;lt;CallExpression&amp;gt;) =&amp;gt; {\n  const objectProperties = requiredPropertiesKeys.reduce((acc, propertyKey, index) =&amp;gt; {\n    const argument = p.value.arguments[index + 1];\n    // If no argument or argument is a spread type, we don&#39;t take it in consideration\n    if (!argument || argument.type === &#39;SpreadElement&#39;) {\n      return acc;\n    }\n\n    // If argument is undefined, we skip it\n    if ((argument as Identifier).name &amp;amp;&amp;amp; (argument as Identifier).name === &#39;undefined&#39;) {\n      return acc;\n    }\n\n    // We create a new object property with an identifier (the object key) and put our argument inside\n    return [...acc, j.objectProperty(j.identifier(propertyKey), argument)];\n  }, [] as ObjectProperty[]);\n\n  // Finally, we keep our translation key in first position and our newly created object in second argument\n  p.value.arguments = [p.value.arguments[0], j.objectExpression(objectProperties)];\n\n  return p;\n};\n\n\nOn récupère les arguments déjà existants, on crée un nouvel objet et on y place nos arguments !\n\nRésultats ✨\n\n⏱ Pour à peu près 2900 fichiers, le script a mis moins de 5,9 secondes à s’exécuter (Macbook Pro 13” 2019).\n\nJSCodeshift nous a permis de cibler très rapidement 99 % des cas problématiques et de les corriger automatiquement.\n\nLe pourcentage restant concerne des cas où il était généralement difficile de cibler la fonction t (passée en props à un autre composant sous un autre nom). Ces quelques cas ont pu être corrigés rapidement à la main et détectés grâce à nos nombreux tests (heureusement qu’on a une règle de bonne pratique pour ça 😇).\n\ntl;dr &amp;amp; conclusion 🏃\n\nVous pouvez retrouver la source du codemod ici même.\n\nSi vous êtes mainteneur d’une librairie, il peut être très intéressant de livrer des codemods en même temps que les breaking-changes pour faciliter l’adoption des mises à jour par exemple !\n\nAvec une prise en main relativement facile pour un résultat très rapide, nous avons été très satisfaits de JSCodeshift et nous n’hésiterons pas à réutiliser cette librairie dans le futur. 👊\n\nMerci à tous pour la lecture de mon premier article et JSCodeshiftez bien. 😘\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #17",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/07/22/bedrock-dev-facts-17.html",
    "date"     : "July 22, 2022",
    "excerpt"  : "L’été arrive, les vacances, le repos.\nEt vu les derniers devfacts des équipes Bedrock, il semblerait qu’il soit temps.\n\nLes autres articles de cette série sont disponibles ici.\n\nC’est presque fini\n\n\n  C’est fait à 80%.\nJ’ai fait tout le code, mais...",
  "content"  : "L’été arrive, les vacances, le repos.\nEt vu les derniers devfacts des équipes Bedrock, il semblerait qu’il soit temps.\n\nLes autres articles de cette série sont disponibles ici.\n\nC’est presque fini\n\n\n  C’est fait à 80%.\nJ’ai fait tout le code, mais il ne marche pas.\n\n\nLa potion magique\n\n\n  Je n’ai plus de cerveau, je vais aller au bar\n\n\nC’est dommage, ils allaient le faire !\n\n\n  Notre stratégie, c’est d’être à la bourre pour que d’autres fassent le travail\n\n\nL’envers du décor du télétravail\n\n\n  Quand il y a du vent chez moi la connexion est instable\n\n\nAh !\n\n\n  J’ai tiré toute mon inspiration de manager des Shadocks\n\n\nLes conséquences du management à la Shadock\n\n\n  Je ne vais tout de même pas faire ma sieste pendant des heures de repos\n\n\nAuto-critique du manager Shadock. On est sauvé !\n\n\n  Tout le monde s’en fout de ce que je fais\n\n\nQuand tout le monde le fait, mais que personne ne comprend pourquoi\n\nEn parlant du TimeSheet, de manière naïve\n\n  “Mais il y a vraiment quelqu’un dans la boite qui fait ça correctement ?”\n\n\nEst-ce que je peux casser à moitié ?\n\n\n  Est-ce qu’on peut faire un semi BC-break ⁉️\n\n\nC’est pas faux\n\n\n  C’est connu et assumé, ça fonctionnera pas jusqu’au moment où ça fonctionnera\n\n\nQuand tu as délaissé la review toute la semaine\n\n\n  Allez bon vendREVIEW à tous\n\n\n🤔\n\n\n  Les tests passent mais ça plante\n\n\nLe détecteur à petit-dej\n\n\n  \n    “je cherche X …”\n    “va voir à la cafete, il y a quelqu’un qui a ramené de la bouffe”\n    “c’est un honeypot à X ça !”\n  \n\n\nC’est beau le mob programming\n\n\n\nMeurs un autre jour\n\nQuand tu veux vraiment être sûr que ton code est exécuté mais que tu oublies de nettoyer l’historique\n\n\nC’est pas compliqué l’informatique\n\n\n  \n    Comment tu as fait ça ?\n    J’ai appuyé sur des touches de mon clavier\n  \n\n\n\n\nGoogle translate n’est pas toujours ton ami\n\n\n  \n    “Tu peux traduire l’US en français quand tu la lis ?”\n    “Pas de souci, Alors… il faut ajouter un message de grille pain d’erreur lors du …\n  \n\n\nSi ça passait, c’était beau\n\nUne personne qui parle à son écran sur lequel il y a du code :\n\n  Dis moi que ça va marcher, s’il te plait 🙏\n\n\nSur un malentendu, il a été engagé\n\n\n  L’essentiel de mes connaissances, c’est du bluff\n\n\nEt on fini avec un instant poésie\n\n\n  Mes p’tits chats, demain c’est démo infra,\nPour l’instant il n’y a pas d’inscrits,\nN’hésitez pas à venir présenter votre travail accompli !\nD’ici à ce prochain rendez-vous,\nDes bisous 😘\n\n"
} ,
  
  {
    "title"    : "Encrypt AWS AMIs: one way to do it wrong",
    "category" : "",
    "tags"     : " cloud, aws",
    "url"      : "/2022/07/08/encrypt-aws-amis.html",
    "date"     : "July 8, 2022",
    "excerpt"  : "At Bedrock, we build our own privately shared AMIs (Amazon Machine Images) for different parts of our stack: kubernetes platform, vod platform, etc. We build those AMIs to optimize kernel parameters,to embed some tools, and more. We have been usin...",
  "content"  : "At Bedrock, we build our own privately shared AMIs (Amazon Machine Images) for different parts of our stack: kubernetes platform, vod platform, etc. We build those AMIs to optimize kernel parameters,to embed some tools, and more. We have been using Packer for a couple of years, and everything has been working just fine.\n\nConcerned about following AWS best-practices, we recently added encryption by default to all new EBS volumes in all our accounts.\n\nWe didn’t expect it, but this decision impacted our AMI creation process. We thus began to update our Packer workflow to integrate this new constraint. We were telling ourselves that more security was for the best and we didn’t take enough steps back to analyze drawbacks.\n\nYou will find in this blog post multiple tips that may help you handle your AMIs encryption, but also why you shouldn’t handle it our way.\n\nBuild an encrypted AMI\n\nTo build our AMI, Packer launches an EC2 in a “builder” account, then a snapshot is created and copied in needed regions. To use this AMI, “user” accounts are listed in the AMI allowed users.\n\nWith account EBS encryption enabled, snapshots are now encrypted. The default behavior is to use the account’s default KMS Key. Our first “easy” problem while trying to build new AMI with Packer was the following error message:\n\nError Copying AMI (ami-xxxxxx) to region (xx-xxx-x): InvalidRequest: Snapshot snap-xxxxxxx is encrypted. Creating an unencrypted copy from an encrypted snapshot is not supported.\n\n\nTo avoid that, we enabled AMI encryption with Packer, but it resulted in another error :\n\nError modify AMI attributes: InvalidParameter: Snapshots encrypted with the AWS Managed CMK can&#39;t be shared.\n\n\nAs our AMI has to be shared to other accounts, it was impossible to encrypt our AMI with the account default KMS Key. So we created a dedicated KMS Key for Packer encryption.\n\nAnd it worked! We had our beautiful encrypted AMI, ready to be used in all our accounts.\n\n\n\nHow we build our encrypted AMIs\n\nRun an encrypted AMI\n\nThis is where it gets complex.\n\nWhen we tried to launch an EC2 instance with our newly encrypted AMI, it failed with this error code :\n\nClient.InternalError: Client error on launch\n\n\nIt means that AWS can’t use this AMI because it is encrypted.\n\nFirst step was to authorize the KMS Key to be used for encryption in user (external) accounts.\n\nThere are two methods to do that, for two different needs.\n\n\n\nPolicy method\n\nTo authorize an external customer managed role (ours), we had to authorize our role in KMS Key dedicated policy to use it, then authorize KMS Key in our role policy to be used. It is some kind of symmetric reference hard to correctly maintain with IaC (Terraform). And we had to do the same for KMS Key replicas in other regions, because they have a dedicated policy.\n\n\n\nPolicy method\n\nOne important thing to know here: some KMS Key permissions aren’t available for external account sharing. It means that when we try to add the permission kms:* to our role policy (for debug purposes only, we follow least privileges principles), it failed. You can find which permission is accessible in cross account use and which is not here.\n\nGrant method\n\nTo authorize an AWS managed role, like AWSServiceRoleForAutoScaling (to launch our EC2), we also needed to allow it to use our key. It is impossible to add a new policy on an AWS Managed role. So instead of using a policy method like before, we had to create a grant on that role to use our key. We tried to create that grant from the source account (where the key is created), but it didn’t work. We had to create that grant from the destination account (where AWSServiceRoleForAutoScaling is), using a role in the destination account that is allowed to create a grant… So we had to allow a role from the destination account to create a grant with Policy method, then use the previous role to allow an AWS Managed Role to use our KMS Key with Grant method. Pretty fun, right?\n\n\n\nGrant method\n\n\n\nOnce all needed roles were allowed, we tried to launch an EC2 with the allowed role attached as an instance role. It failed again, because we needed to also use the AMI KMS Key on root volume of our instance. By default, it was the account KMS Key that was used.\n\nWe attached that key on our root volume, and it worked. We also could launch our EC2 with ASG. It was all good.\n\nBut there was a big security vulnerability: instead of using one KMS key per account to encrypt our EBS volume, we were now using the same KMS key on all our accounts because of our encrypted AMI.\n\nKMS Key rotation\n\nA short word about Key rotation: it can easily be enabled to automatically rotate key materials each year. All new AMIs will be encrypted with new key material and nothing has to be changed to run encrypted AMIs.\nBut in case of a manual rotation: if a key is leaked for example, you will need to recreate a new KMS Key, its replicas, and all permissions and grants seen before.\n\nConclusion\n\nUsing privately shared encrypted AMI caused us multiple problems:\n\n  higher complexity to maintain.\n  lower security in cross-account configuration.\n\n\nFurthermore, we checked all our AMIs to see if they contain sensitive data. It isn’t the case : all sensitive data is uploaded at startup by Launch Template. We had no interest in continuing to use encrypted AMI, and we would have spared so much time if we had seen that sooner.\n\nThis is why we decided to disable encryption for all new EBS volume on our builder account and stop building encrypted AMI.\n\nDoing all the previous configuration took us several weeks. We are now more aware that doing security just for the beauty of it can be really counterproductive.\n\nIf your AMIs contain sensitive data, a better way to handle encrypted AMI may be to stop creating privately shared AMIs. Instead, copy and encrypt a private AMI in each of your “user” accounts with a dedicated KMS Key per account. As a result, there will be a larger amount of AMI to handle (one AMI per account per region), KMS Key permissions will still be complex, but security should improved.\n\nLogo used in thumbnail\nDeath by Imogen Oh from NounProject.com\nKey by Baboon designs from NounProject.com\nGears by Aybige from NounProject.com\n"
} ,
  
  {
    "title"    : "Debugging and reviewing your Android dependencies with apktool",
    "category" : "",
    "tags"     : " android, apktool, instrumentation, debugging, productivity",
    "url"      : "/2022/06/20/android-apktool-decompiling.html",
    "date"     : "June 20, 2022",
    "excerpt"  : "If you maintain an Android application, you might be relying on performance monitoring SDKs like Firebase Performance or New Relic, to name a couple. These plugins usually have a light setup process—just apply a Gradle plugin, and they provide the...",
  "content"  : "If you maintain an Android application, you might be relying on performance monitoring SDKs like Firebase Performance or New Relic, to name a couple. These plugins usually have a light setup process—just apply a Gradle plugin, and they provide the ability to collect statistics about every network call and database query in your app automatically.\n\nThe usual way to achieve this is to rely on a process called instrumentation, which is supported via the Android Gradle Plugin’s Transform API, or its successor, the Instrumentation API. This feature is very powerful, and potentially dangerous; in our case, a minor patch of one of these SDKs caused a production bug that left one of our core features crippled.\n\nThe visible cause of our bug, from a developer’s point of view, was that the video player saw the network requests as always being extremely fast, no matter the network quality. Therefore, it assumed the device had access to a very high bandwidth, and tried loading video segments with a very high bit rate. This did not go well for users with slower network speeds.\n\nTo understand what was going on, what went wrong, how to fix it and how to take measures so that it never happens again, we had to do some investigation.\n\nDiving into the Android build process\n\nBefore we get to the topic of instrumentation, we first need to know a little about the Android app build process. Don’t worry, we won’t need to dive too deep into the details.\n\nTo put it simply, during the build process, your source files (Kotlin and Java) are compiled to Dalvik bytecode, which is stored in .dex files. These files are then packaged into an APK file, which is basically just a ZIP file with all your code and resources.\n\n\nflowchart LR\n    kt[.kt files] -- kotlinc --&amp;gt; dex[.dex files] --&amp;gt; packaging[[packaging]]\n    java[.java files] -- javac --&amp;gt; dex\n    res[resource files] -- aapt --&amp;gt; resc[compiled resource files] --&amp;gt; packaging --&amp;gt; APK\n    subgraph APK\n    direction TB\n    dex1[.dex] -.- dex2[.dex] -.- dex3[.dex] -.- dex4[.dex]\n    res1[res] -.- res2[res] -.- res3[res] -.- res4[res]\n    signature -.- manifest\nend\n\n\nUnderstanding bytecode instrumentation\n\nNow, let’s say you want to take an existing application with its untouched source code, and automatically inject calls to your SDK every time a network call is made, to log whether it was successful or not. How would you achieve this?\n\nThe easiest way is to plug yourself into the build, right after the code is compiled into bytecode, and modify the bytecode to your will.\n\n\nflowchart LR\n    kt[.kt files] -- kotlinc --&amp;gt; dex[.dex files] --&amp;gt; transform[[transform]] --&amp;gt; packaging[[packaging]]\n    java[.java files] -- javac --&amp;gt; dex\n    res[resource files] -- aapt --&amp;gt; resc[compiled resource files] --&amp;gt; packaging --&amp;gt; APK\n    classDef transformed fill:#ff0000\n    class transform transformed\n\n    subgraph APK\n    direction TB\n    dex1[.dex] -.- dex2[.dex] -.- dex3[.dex] -.- dex4[.dex]\n    res1[res] -.- res2[res] -.- res3[res] -.- res4[res]\n    signature -.- manifest\n    class dex1,dex2,dex3,dex4 transformed\nend\n\n\nThe Android Gradle Plugin (AGP) offers APIs to do this, so SDK vendors can just develop a Gradle plugin and ta-da! Once you apply it, your app is automatically instrumented.\n\nNote that there are other ways to achieve this without the AGP. Notably, Kotlin now uses an Intermediate Representation (IR), before it gets compiled down to a target-specific format. You can write a Kotlin IR compiler plugin to transform the IR code and add your own hooks in an Android-agnostic way, although this API is still experimental at the time of writing.\n\nReverse-engineering a built APK\n\nNow, this is great. But when you open an APK file, what do you get?\n\nLet’s unzip one and look inside.\n\n.\n├── META-INF\n├── assets\n├── google\n├── okhttp3\n├── res\n├── AndroidManifest.xml\n├── classes.dex\n├── classes2.dex\n├── classes3.dex\n├── classes4.dex\n├── firebase-common.properties\n├── firebase-crashlytics.properties\n├── play-services-base.properties\n├── ...\n└── resources.arsc\n\n\nA bunch of noise, and four interesting .dex files. That’s where the app’s code is stored, but unfortunately, these files are not human-readable.\n\nTo turn them into low-level but understandable code, some tooling will be necessary. The easiest to use for this task is apktool, which is free and open-source.\n\nLet’s run apktool on our APK, and see what happens:\n\n\n\n\n\n~/Downloads\n❯ apktool d bedrock-sample-release.apk\nI: Using Apktool 2.6.1 on bedrock-sample-release.apk\nI: Loading resource table...\nI: Decoding AndroidManifest.xml with resources...\nI: Loading resource table from file: /Users/bcandellier/Library/apktool/framework/1.apk\nI: Regular manifest package...\nI: Decoding file-resources...\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_text_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xhdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_icon_light_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-xxhdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-mdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_icon_dark_normal_background.9.png&quot;. Renaming it to *.png.\nW: Cant find 9patch chunk in file: &quot;drawable-hdpi-v4/common_google_signin_btn_text_dark_normal_background.9.png&quot;. Renaming it to *.png.\nI: Decoding values */* XMLs...\nI: Baksmaling classes.dex...\nI: Baksmaling classes2.dex...\nI: Baksmaling classes3.dex...\nI: Baksmaling classes4.dex...\nI: Copying assets and libs...\nI: Copying unknown files...\nI: Copying original files...\nI: Copying META-INF/services directory\n\n\n\nThere we go! In our case, we can ignore the warnings. apktool created a new directory with a bunch of .smali files, organized by package: one file per class, containing their Dalvik bytecode.\n\n.\n├── AndroidManifest.xml\n├── res\n│   ├── values\n│   │   ├── strings.xml\n│   │   └── ...\n│   ├── layout\n│   │   ├── layout_home.xml\n│   │   └── ...\n│   └── ...\n├── smali\n│   ├── com\n│       ├── bedrockstreaming\n│       │   ├── app\n│       │   │   ├── mobile\n│       │   │   │   ├── R$anim.smali\n│       │   │   │   ├── R$layout.smali\n│       │   │   │   ├── R$string.smali\n│       │   │   │   ├── R$style.smali\n│       │   │   │   └── ...\n│       │   │   └── ...\n│       │   └── ...\n│       └── google\n│           ├── android\n│           │   ├── exoplayer2\n│           │   │   ├── AbstractConcatenatedTimeline.smali\n│           │   │   ├── AudioBecomingNoisyManager.smali\n│           │   │   ├── AudioFocusManager$AudioFocusListener$$ExternalSyntheticLambda0.smali\n│           │   │   ├── AudioFocusManager$AudioFocusListener.smali\n│           │   │   ├── AudioFocusManager.smali\n│           │   │   ├── BasePlayer.smali\n│           │   │   ├── BaseRenderer.smali\n│           │   │   ├── BuildConfig.smali\n│           │   │   └── ...\n│           │   └── ...\n│           └── ...\n├── smali_classes2\n│   ├── com\n│   │   └── bedrockstreaming\n│   │       ├── app\n│   │       │   ├── mobile\n│   │       │   │   ├── MobileApplication.smali\n│   │       │   │   └── ...\n│   │       │   └── ...\n│   │       └── ...\n│   └── ...\n└── ...\n\n\nIf you see files with mangled names and contents, make sure that you run apktool on an APK with R8 obfuscation disabled, or you’ll have a hard time figuring things out.\n\nUnderstanding Dalvik bytecode\n\nNow, if you open one of these files, it will contain code that looks like the snippet below. It will look unfamiliar; that’s normal.\n\n.method private final getContent()Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    .locals 2\n    .line 119\n\n    iget-object v0, p0, Lcom/bedrockstreaming/example/HomeViewModel;-&amp;gt;state:Landroidx/lifecycle/LiveData;\n\n    invoke-virtual {v0}, Landroidx/lifecycle/LiveData;-&amp;gt;getValue()Ljava/lang/Object;\n\n    move-result-object v0\n\n    instance-of v1, v0, Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    if-eqz v1, :cond_0\n\n    check-cast v0, Lcom/bedrockstreaming/example/HomeViewModel$State$Content;\n\n    goto :goto_0\n\n    :cond_0\n\n    const/4 v0, 0x0\n\n    :goto_0\n\n    return-object v0\n  \n.end method\n\n\nIf you’ve ever worked with assembly code before, you might notice similarities in the way the code is written. Each line begins with an instruction, which can take comma-separated parameters. To work out what these instructions and their parameters mean, you will need to refer to the Dalvik bytecode documentation provided by Google.\n\nLet’s take an example line from the snippet and decode it together. Looking at the table in the documentation, we can see deduce this:\n\n# We&#39;ll decode this line:\ninvoke-virtual {v0}, Landroidx/lifecycle/LiveData;-&amp;gt;getValue()Ljava/lang/Object;\n\ninvoke-virtual                                                                   # We&#39;re calling a virtual method\n               {v0},                                                             # We&#39;re calling the method on the object referenced in register v0\n                     Landroidx/lifecycle/LiveData;                               # The method we&#39;re calling is defined by androidx.lifecycle.LiveData\n                                                  -&amp;gt;getValue()                   # We&#39;re calling a method called getValue()\n                                                              Ljava/lang/Object; # This method returns an Object\n\n\nWith some determination, we can figure out what the snippet does. Here, we’re defining a getContent() method that tries to cast a LiveData’s value to State.Content and returns it, or null otherwise.\n\nUsing a decompiled APK as a debugging tool\n\nInspecting suspicious code\n\nBefore doing anything else, we can already start looking at the generated code to identify patterns that could cause issues. Problem is… there can be a lot of code to look through.\n\nBefore going this deep in the rabbit hole, we already figured our issue was, somehow, related to instrumentation: disabling it fixed this issue; downgrading to the previous release of the SDK also fixed it. This means that if we want to get a clear look at what needs to change to go from a working APK from a broken one, we could just compare an APK instrumented by the previous SDK version with an APK instrumented by the current one!\n\nOf course, we want to do this on the human-readable smali files, not the raw dex files. We can generate a full diff with the help of the diff tool:\n\ndiff -bur normal/ instrumented/\n\n\nIn our case, it also proved useful to compare an APK that has been instrumented with one that hasn’t, to understand what that instrumentation is meant to achieve. Most of it was to notify the SDK of every HTTP request, along with its result.\n\nAs a simple example, the snippet below shows a class belonging to Picasso. We can see the HTTP calls it makes are being intercepted by the SDK.\n\n--- normal/smali/com/squareup/picasso/NetworkRequestHandler.smali\t2022-01-05 11:09:22.000000000 +0100\n+++ instrumented/smali/com/squareup/picasso/NetworkRequestHandler.smali\t2022-01-05 11:08:34.000000000 +0100\n@@ -128,10 +128,26 @@\n\n     .line 103\n     :cond_4\n+    instance-of v2, v1, Lokhttp3/Request$Builder;\n+\n+    if-nez v2, :cond_5\n+\n     invoke-virtual {v1}, Lokhttp3/Request$Builder;-&amp;gt;build()Lokhttp3/Request;\n\n     move-result-object v2\n\n+    goto :goto_1\n+\n+    :cond_5\n+    move-object v2, v1\n+\n+    check-cast v2, Lokhttp3/Request$Builder;\n+\n+    invoke-static {v2}, Lcom/vendor/instrumentation/okhttp3/OkHttp3Instrumentation;-&amp;gt;build(Lokhttp3/Request$Builder;)Lokhttp3/Request;\n+\n+    move-result-object v2\n+\n+    :goto_1\n     return-object v2\n .end method\n\n\nFinding the source of the issue by iteration\n\nWe haven’t talked about apktool’s greatest strength yet: its ability to recompile an APK from the smali sources it has decompiled! This means we can effectively decompile an APK, make modifications to its low-level code, recompile and run it.\n\nThis proved really useful during our investigation. Since we have one directory with our APK in a bad state, and one directory with our APK in a good state, we can process by elimination to point out exactly which single class, when modified, causes our bug.\n\nIn our case, a useful workflow was to start with a suspect—let’s say we think instrumenting the OkHttp classes might have caused the bug.\n\n\n  Copy the OkHttp classes from the “bad” APK, and only those, to our “good” APK.\n  Recompile and run the app.\n  Does the bug occur?\n    \n      If it does, then that means it is caused by the instrumentation of at least one of the OkHttp classes. We can go through this process again, this time by selecting only a subset of OkHttp’s classes, and check if the bug still occurs, etc.\n      If it doesn’t, revert the OkHttp classes and try again with another suspect.\n    \n  \n\n\nThis process can be accelerated with a very simple script, to iterate faster. The recompilation step occurs incrementally, and so only takes a few seconds.\n\n#!/bin/sh\n\n# rebuild-and-run.sh\n# Rebuild, sign and install an APK from its decompiled source.\n# (c) 2022 Bedrock Streaming\n\n# Inputs:\n# DECOMPILED_APK_PATH: path to your previously decompiled APK directory\n# KEYSTORE_PATH: path to your debug keystore\n# KEYSTORE_PASSWORD: your debug keystore password\n\napktool --use-aapt2 b &quot;$DECOMPILED_APK_PATH&quot; \\\n    &amp;amp;&amp;amp; apksigner sign -ks &quot;$KEYSTORE_PATH&quot; --ks-pass &quot;pass:$KEYSTORE_PASSWORD&quot; &quot;$DECOMPILED_APK_PATH/dist/*.apk&quot; \\\n    &amp;amp;&amp;amp; adb install &quot;$DECOMPILED_APK_PATH/dist/*.apk&quot;\n\n\nHere’s what it looks like in action:\n\n\n\n\n\n~/bytecode-playground\n❯ ./rebuild-and-run.sh\nI: Using Apktool 2.6.1\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether sources has changed...\nI: Checking whether resources has changed...\nI: Building apk file...\nI: Copying unknown files/dir...\nI: Built apk...\nPerforming Incremental Install\nServing...\nSuccess\nInstall command complete in 445 ms\n\n\n\nIn our case, we narrowed down the issue to the instrumentation of a single class: okhttp3.internal.http.CallServerInterceptor: once it was reverted, the bug disappeared.\n\nIn fact, we narrowed it down to a very small patch with which the app runs fine:\n\n .../okhttp3/internal/http/CallServerInterceptor.smali         | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali b/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\nindex c916149f..c26eab15 100644\n--- a/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\n+++ b/apk/smali_classes2/okhttp3/internal/http/CallServerInterceptor.smali\n@@ -510,7 +510,7 @@\n \n     instance-of v8, v14, Lokhttp3/Response$Builder;\n \n-    if-nez v8, :cond_b\n+    #if-nez v8, :cond_b\n \n     invoke-virtual {v14, v15}, Lokhttp3/Response$Builder;-&amp;gt;body(Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n \n@@ -574,7 +574,7 @@\n \n     instance-of v15, v8, Lokhttp3/Response$Builder;\n \n-    if-nez v15, :cond_e\n+    #if-nez v15, :cond_e\n \n     invoke-virtual {v8, v14}, Lokhttp3/Response$Builder;-&amp;gt;body(Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n \n-- \n\n\nBasically, when the code went through this if statement, our request got wrapped by com.vendor.instrumentation.okhttp3.OkHttp3Instrumentation:\n\ninvoke-static {v8, v14}, Lcom/vendor/instrumentation/okhttp3/OkHttp3Instrumentation;-&amp;gt;body(Lokhttp3/Response$Builder;Lokhttp3/ResponseBody;)Lokhttp3/Response$Builder;\n\n\nAnd what does this method do, you ask? Let’s take a look at the decompiled source in Android Studio, so that it’s a bit easier to read:\n\npublic Builder body(ResponseBody body) {\n    try {\n        if (body != null) {\n            BufferedSource source = body.source();\n            Buffer buffer = new Buffer();\n            source.readAll(buffer);\n            return this.impl.body(ResponseBody.create(body.contentType(), buffer.size(), buffer));\n        }\n    } catch (IOException var4) {\n        log.error(&quot;IOException reading from source: &quot;, var4);\n    } catch (IllegalStateException var5) {\n        log.error(&quot;IllegalStateException reading from source: &quot;, var5);\n    }\n\n    return this.impl.body(body);\n}\n\n\nThe body is being read into memory!\n\nsource.readAll(buffer);\n\n\nWhen correlating this discovery with the source code from ExoPlayer, we could verify that, indeed, our player was expecting that the time it takes reading the response body would be the time it took to download the entire video segment. Here’s what this flow looks like in a functional app:\n\n\nsequenceDiagram\n    participant exo as OkHttpDataSource\n    participant nr as OkHttp3Instrumentation\n    participant okhttp as OkHttpClient\n    participant server as Server Endpoint\n\n    exo-&amp;gt;&amp;gt;nr: body()\n    nr-&amp;gt;&amp;gt;okhttp: body()\n    activate server\n    okhttp-&amp;gt;&amp;gt;server: \n    server-&amp;gt;&amp;gt;okhttp: \n    okhttp-&amp;gt;&amp;gt;nr: ResponseBody (length=0)\n    activate exo\n    nr-&amp;gt;&amp;gt;exo: ResponseBody (length=0)\n    server-&amp;gt;&amp;gt;exo: length=512\n    server-&amp;gt;&amp;gt;exo: length=1024\n    server-&amp;gt;&amp;gt;exo: length=1536\n    server-&amp;gt;&amp;gt;exo: length=2048\n    server-&amp;gt;&amp;gt;exo: length=2560\n    note right of exo: OkHttpDataSource controls the body reads  and can measure the time it took  to read the whole response\n    deactivate server\n    deactivate exo\n\n\nBut with this bug in the SDK, since the HTTP response has been buffered into memory by some SDK, the read was always almost-instantaneous, no matter the speed of the connection. Additionally, it messed with the overall performance since requests were no longer properly streamed by their rightful users.\n\n\nsequenceDiagram\n    participant exo as OkHttpDataSource\n    participant nr as OkHttp3Instrumentation\n    participant okhttp as OkHttpClient\n    participant server as Server Endpoint\n\n    exo-&amp;gt;&amp;gt;nr: body()\n    nr-&amp;gt;&amp;gt;okhttp: body()\n    activate server\n    okhttp-&amp;gt;&amp;gt;server: \n    server-&amp;gt;&amp;gt;okhttp: \n    activate nr\n    okhttp-&amp;gt;&amp;gt;nr: \n    server-&amp;gt;&amp;gt;nr: length=512\n    server-&amp;gt;&amp;gt;nr: length=1024\n    server-&amp;gt;&amp;gt;nr: length=1536\n    server-&amp;gt;&amp;gt;nr: length=2048\n    server-&amp;gt;&amp;gt;nr: length=2560\n    deactivate nr\n    activate exo\n    note right of exo: OkHttpDataSource is only notified  after everything is downloaded\n    nr-&amp;gt;&amp;gt;exo: ResponseBody (length=2560)\n    deactivate server\n    deactivate exo\n\n\nUsing a decompiled APK as a review tool\n\nIt’s no secret to developers in any software ecosystem that library updates can be a source of problems - security vulnerabilities, bugs, incompatibilities, and so on. It’s hard to vet them properly, especially in compiled form, like libraries distributed in the Java ecosystem. Things get even harder when arbitrary Gradle plugins start rewriting our own code!\n\nThe tooling needed to decompile an APK is free, fast, and easy to automate. It’s a really helpful tool to investigate obscure bugs in places your debugger won’t let you place a breakpoint, and it’s also really useful to be able to see a human-readable diff between two binaries.\n\nGenerating a diff of the effects of a library upgrade can seem overkill and hard to do in practice, but at least in the case of bug-fix releases with hopefully few changes, it can be very helpful to have an actual report of what changed. It’s an accepted practice to review the code your team checks in; why not review the code of others, since it ends up in the exact same artifact?\n"
} ,
  
  {
    "title"    : "Bedrock à la Kubecon 2022, 4ème partie : chaos, résilience, ressenti global et conclusion générale…",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/16/kubecon-2022-part-4.html",
    "date"     : "June 16, 2022",
    "excerpt"  : "Pour terminer cette série, un ou deux sujets divers que nous n’avons pas regroupé dans les trois articles précédents\n(les performances applicatives et la scalabilité, \nles performances bas niveau, le système et le réseau,\nla dev XP, l’outillage, l...",
  "content"  : "Pour terminer cette série, un ou deux sujets divers que nous n’avons pas regroupé dans les trois articles précédents\n(les performances applicatives et la scalabilité, \nles performances bas niveau, le système et le réseau,\nla dev XP, l’outillage, la CI/CD et l’observabilité), \npuis une conclusion globale avec ce que nous avons retenu de cette KubeCon Europe 2022.\n\n\nLa conclusion, @ KubeCon 2022 !\n\nChaos Engineering / Chaos Testing pour une meilleur résilience aux pannes\n\nC’est un des sujets sur lesquels nous avons commencé à travailler activement cette année : casser des choses dans nos clusters, dans notre plateforme, entre nos microservices.\nL’idée sous-jacente est, bien sûr, que tout va casser un jour ou l’autre, donc autant provoquer du chaos nous-même, en environnement contrôlé. Nous identifierons ainsi des points sensibles de notre plateforme et pourrons les corriger, évitant ainsi des incidents, parfois majeurs, au mauvais moment.\n\nCe thème du Chaos Engineering est régulièrement abordé en conférences et nous étions contents de voir que nous ne sommes pas les seuls à nous interroger sur “comment” en mettre en place.\nNous sommes repartis avec quelques pistes d’outils, comme chaos mesh ou Litmus Chaos, que nous allons peut-être prototyper pour les comparer à chaos-controller que nous avons récemment expérimenté.\n\nAu cours de la conférence, “Case Study: Bringing Chaos Engineering to the Cloud Native Developers” (vidéo) par Uma Mukkara, Litmus et Ramiro Berelleza, Okteto, nous avons pu avoir un aperçu de l’outil de chaos Litmus, sa force semblant résider dans le partage des scripts de chaos au sein la communauté.\nPuis, il a été décrit une approche CI des tests de chaos visant à intégrer certains tests de chaos dans le flux de développement plutôt qu’à la fin.\n\nEnfin, toujours sur des questions de résilience en cas d’interruption de service, dans sa conférence “Building for the (inevitable) Next Cloud Outage” (vidéo), Pavel Nikolov de Section nous a questionnés sur la manière d’être plus robuste à une catastrophe.\nLa question n’est pas de savoir si une catastrophe se produira, mais quand elle se produira. C’est pourquoi il est aussi préférable de disposer d’un plan de reprise après sinistre mais surtout de prévoir en amont un système d’auto-guérison permettant d’être plus résilient aux catastrophes.\nIl nous a ensuite présenté un use case spécifique au réseau, nous invitant à préférer au traditionnel “DNS à la rescousse”, la mise en place de BGP (Border Gateway Protocol).\n\nQuelques sujets divers\n\nÀ travers quelques talks, nous avons jeté des coups d’œil sur des sujets sur lesquels nous ne travaillons pas réellement au quotidien – appelez ça de la curiosité intellectuelle si vous le voulez ;-)\n\n\n  Nous avons vu un ensemble de design patterns pour le développement de controllers Kubernetes (vidéo), approche qui devient petit à petit un moyen répandu de répondre à des problématiques, en codant directement dans Kubernetes.\n  La conférence “A treasure map of hacking (and defending) K8s” (vidéo) était très sympathique, elle montrait à quel point il peut être “facile” de prendre le contrôle d’une infrastructure. Une façon de montrer que patcher est obligatoire !\n  Et dans un registre hors-technique, “Composability is to software as compounding interest is to finance” (vidéo) mettait en évidence à quel point, en construisant des outils, puis des projets, puis un écosystème, les uns profitent aux autres, on construit donc plus grand et plus gros. Il suffit de voir le landscape CNCF aujourd’hui par rapport à 4 ans en arrière.\n\n\nConclusion, KubeCon Europe 2022\n\nNous avons commencé à migrer vers Le Cloud, vers AWS et Kubernetes, il y a plus de quatre ans. Notre première KubeCon était à Copenhague, en 2018. Que dire, en conclusion de cette conférence annuelle ? Comment conclure ces articles ?\n\nAujourd’hui, les grandes idées que nous avons retenues de cette KubeCon Europe 2022, en résumant, sont les suivantes :\n\n\n  Les problématiques d’auto-scaling sont bien cernées, les outils sont plutôt matures. Comme beaucoup d’autres entreprises, nous arrivons sur l’étape suivante, qui est de dimensionner en tenant mieux compte des couts et pas uniquement des performances et/ou de la disponibilité.\n  La gestion des coûts dans Kubernetes n’est toujours pas simple. À la fois pour les suivre et les répartir, mais aussi pour décider du bon compromis entre performances / disponibilité / souplesse / autonomie des équipes / couts.\n  Service Mesh : nous n’avons toujours pas franchi le pas et Istio, qui était un sujet très à la mode il y a quatre ans, nous semble désormais presque oublié. Aujourd’hui, Cilium semble être la nouvelle approche qui s’impose, et il se pourrait que nous jouions avec “pour voir” prochainement…\n  L’observabilité, plus vraiment un problème.\n  Le chaos engineering / chaos testing : toujours une idée séduisante, mais pas encore réellement industrialisée ?\n  L’outillage autour de la CI/CD, le déploiement progressif, le rollback (possiblement automatisé) progresse, et ça fait plaisir !\n  L’écosystème progresse, mûrit, et on parle de sujets de plus haut niveau que quelques années en arrière. Par exemple, nous avons entendu plusieurs fois parler de base de données magiquement scalable hébergée dans Kubernetes, alors que l’époque où nous évitions de stocker quelque état que ce soit dans un cluster ne nous semble pas si lointaine !\n\n\nEt, pour finir, quelques points dont nous n’avons pas du tout ou très peu entendu parler :\n\n\n  Nous avons peut-être loupé des choses en créant nos programmes, mais nous n’avons vu aucun talk autour de “comment nous développons des applications cloud-native”. Pourtant, la problématique de l’environnement de développement, avec des services managés, des déploiements vers Kubernetes et des plateformes distribuées, ne nous parait pas encore réglée !\n  L’approche “FaaS” (Function as a Service) nous paraît encore moins répandue que quelques années en arrière ?\n  Nous n’avons pas entendu parler une seule fois de Fédération, alors que le terme revenait encore et encore il y a quatre ans. Nous avons bien fait de ne même pas essayer, on dirait ;-)\n\n\n\nRejoignez-nos équipes et venez vivre les prochaines conférences avec nous l’an prochain\n"
} ,
  
  {
    "title"    : "Bedrock à la Kubecon 2022, 3ème partie : Dev XP, outillage, CI/CD, observabilité…",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/15/kubecon-2022-part-3.html",
    "date"     : "June 15, 2022",
    "excerpt"  : "Pour notre troisième article de cette série sur ce que nous avons retenu de la KubeCon Europe 2022, après \nles performances applicatives et la scalabilité et \nles performances bas niveau, le système et le réseau, \npassons à la Developper eXperienc...",
  "content"  : "Pour notre troisième article de cette série sur ce que nous avons retenu de la KubeCon Europe 2022, après \nles performances applicatives et la scalabilité et \nles performances bas niveau, le système et le réseau, \npassons à la Developper eXperience, à l’outillage, à la CI/CD, aux rollback, à l’observabilité et aux incidents !\n\n\nUne nouvelle journée commence, @ KubeCon 2022 !\n\nLa prod est tombée !\n\nDans notre secteur d’activité, nous avons tous subis des incidents de production et le retour d’expérience, qu’il soit interne ou public, est important et formateur.\nEn effet, même si les incidents de production sont malheureusement inéluctables dans nos métiers, il est important de les analyser afin de mieux les comprendre et demieux s’en prémunir.\n\nPreuve de l’importance de ces sujets : nous avons assisté à deux conférences très intéressantes sur ce thème dans des salles pleines à craquer !\nToutes deux portaient sur des incidents de production majeurs suite à une modification de code qui peut paraître anodine : la première était donnée par Influxdata, la seconde par Skyscanner.\nLes conférences étaient particulièrement joviales et bienveillantes : les réactions du public à certains slides montraient bien que ce genre de situations sentait le vécu pour certains !\n\nNous avons tous à apprendre de ces cas concrets d’incident, aussi, nous vous conseillons de visionner les vidéos de ces conférences : skyscanner et influxdata.\nMais si nous devions les résumer : l’automatisation de bout en bout demande une grande maturité, beaucoup (beaucoup !) de tests et des reviews de qualité !\n\nEt comme il est dit dans une des slides :\n\n\n\nDebugger, en production, avec des conteneurs éphémères\n\nNous utilisons régulièrement la commande kubectl exec pour entrer dans conteneur / pod et y lancer des commandes de débogage – parce que certains problèmes ne sont pas reproductibles ailleurs qu’en production, ou parce qu’il faut comprendre ce qu’il se passe avant de savoir reproduire en environnement de développement.\n\nCela dit, cette approche n’est pas géniale : si nous modifions des choses dans un conteneur, ces modifications persistent.\nAussi, il faut pouvoir installer des outils de débug dans un conteneur (ce qu’on ne peut pas facilement faire chez Bedrock, où nos conteneurs ne s’exécutent pas en root et ont souvent un filesystem read-only), ou les embarquer dans les images (ce qui les grossit considérablement, sans compter l’augmentation du risque de failles de sécurité).\n\nPour remédier à cette problématique, la fonctionnalité de conteneurs éphémères (vidéo) arrive en bêta dans Kubernetes 1.23 et ça semble absolument génial !\nL’outil parfait pour lancer des conteneurs temporaires à l’intérieur de pods existant et incroyablement puissant pour débugger !\nNous allons pouvoir réduire le nombre d’outils de debug intégrés à nos images et parvenir à débugger plus aisément des problèmes qui ne surviennent qu’en production !\n\nLes risques de l’observabilité / Observabilité piratée\n\n“How attackers use exposed Prometheus Server to Exploit Kubernetes Clusters” (vidéo) par David de Torres et  Miguel Hernandez, ou “comment obtenir l’empreinte de vos clusters k8s à travers vos données de monitoring”.\n\nSysdig est venu nous remémorer que le monitoring, c’est bien, mais que ne pas exposer ses données de monitoring, c’est mieux !\nEn effet, attention aux informations qui sont exposées à l’extérieur, elles pourraient être recueillies par des attaquants externes pour acquérir des connaissances sur votre plateforme (provider cloud, version de l’OS utilisé…) et s’en servir ensuite pour s’introduire dans votre infrastructure (fuite de données, cryptominage ou ransomware).\n\nÀ travers un cas d’utilisation fictif, ils nous ont démontré la facilité de récupération de ces informations et comment elles sont utilisées pour monter une attaque.\nEnfin, ils nous ont rappelé que pour se prémunir de ces attaques, il suffit de suivre les recommandations de sécurité ! CQFD.\nIl est toujours bon d’avoir ces piqures de rappel et de toujours bien penser aux données que l’on expose vers l’extérieur.\n\nCI/CD, déploiement progressif\n\nChez Bedrock, nous sommes en pleine refonte de notre chaîne de CI/CD : nous basculons tous nos projets du bon vieux Jenkins “temporaire”, que nous avions monté au début de notre migration vers Le Cloud, vers Github Actions.\nAu passage, nous nous demandons forcément comment nous pourrions améliorer nos déploiements et les rendre plus sécurisés, tant pour la santé de notre plateforme que pour la paix d’esprit de nos équipes et de nos utilisateurs.\n\nLa conférence “Automated progressive delivery using gitops and service mesh” (vidéo) parlait de déploiement progressif avec Argo CD, pour améliorer l’excellence opérationnelle, réduire le MTTR, accroître l’automatisation et la fiabilité des processus de déploiement. Bref, des idées qui nous parlent !\n\nReste des fonctionnalités, qui nous semblent primordiales avant de se lancer sur un autre outil, qui ne sont pas encore gérées, hélas : mirroring de traffic, routing basé sur des en-têtes (typiquement : pour faire du déploiement progressif à la maille “utilisateur” et pas à la maille “requête HTTP”), détection d’anomalie et rollback automatisé…\nUn projet à suivre, donc, qui pourrait mûrir dans les prochains mois.\n\nAu niveau des aspects moins sympathiques : cette approche de déploiement progressif passe par un service mesh (envoy, ici).\nOr nous n’en avons pas en place et depuis quatre ans n’avons toujours pas trouvé les bons arguments pour en introduire dans nos clusters, notamment à cause de la complexité ajoutée…\n\nUne autre conférence (vidéo) mentionnait l’outil Flagger pour des déploiements Canary.\n\nQuelques autres idées à retenir\n\nNous avons aussi vu quelques autres conférences dont nous avons tiré quelques idées, en plus bref :\n\n\n  Kubernetes 1.23 apporte (en alpha) une nouvelle commande kubectl events, qui retourne ses résultats dans l’ordre chronologique. Ce que l’actuel kubectl get events ne fait pas et ça peut être bien embêtant. Vue comme de la culture générale, la conférence “The soul of a new command: adding ‘events’ to kubectl” (vidéo) racontait comment cette fonctionnalité a été implémentée et était fort intéressante.\n  Un speaker parlait de la mise en place de Crossplane dans son entreprise (vidéo). Sujet potentiellement intéressant, mais qui ne correspond pas à notre approche actuelle. Nous avons toutefois retenu quelques points autour de comment il fournit des outils à ses collègues développeurs : documentation, composition de services, management d’attentes, utilisation de l’écosystème… Des problématiques auxquelles nous nous sommes confrontés de nombreuses fois, pour encourager nos équipes à adopter des évolutions ou de nouveaux outils !\n  Si vous commencez à mettre en place votre stack de logs, la conférence “Show me your labels and I’ll tell you who you are” (vidéo) est faite pour vous. L’idée d’utiliser les labels assignés aux pods pour aller jusqu’à filtrer l’accès aux logs via RBAC, terrible ! Aussi, la création de flux de logs avec Logging Operator a l’air fort sympathique. Si ce talk était venu trois ans plus tôt, c’est quelque chose que nous essayerons !\n\n\nConclusion\n\nCes conférences nous ont permis d’approfondir les questions que nous nous posons actuellement alors que notre changeons de CI/CD (déploiement progressif, rollback automatisé ou non…).\n\nPlus globalement, nous sommes contents de voir que l’outillage autour de Kubernetes continue à progresser et que la Developer eXperience est un sujet pris au sérieux dans notre communauté.\n\n\nRejoignez-nos équipes et venez vivre les prochaines conférences avec nous l’an prochain\n"
} ,
  
  {
    "title"    : "Bedrock à la Kubecon 2022, 2nde partie : performances, système et réseau",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/14/kubecon-2022-part-2.html",
    "date"     : "June 14, 2022",
    "excerpt"  : "Pour ce second article de synthèse de la KubeCon Europe 2022, continuons sur le thème des performances, peut-être plus bas niveau,\net plongeons aussi dans des outils pouvant être déployés au cœur de nos clusters !\n\n\nCa va commencer, @ KubeCon 2022...",
  "content"  : "Pour ce second article de synthèse de la KubeCon Europe 2022, continuons sur le thème des performances, peut-être plus bas niveau,\net plongeons aussi dans des outils pouvant être déployés au cœur de nos clusters !\n\n\nCa va commencer, @ KubeCon 2022 !\n\nL’autoscaling, autrement\n\nUne des conférences : “Autoscaling Kubernetes Deployments: A (Mostly) Practical Guide” (vidéo) présenté par NewRelic présentait le principe d’autoscaling dans Kubernetes, avec les trois principales ressources associées à ce concept : ClusterAutoscaler, HorizontalPodAutoscaler et VerticalPodAutoscler.\n\nCette conférence présentait :\n\n\n  le fonctionnement du scale up/down des pods avec les périodes de stabilisation ;\n  le calcul par rapport aux indicateurs utilisés ;\n  les types de métriques utilisables par les HPA et VPA.\n\n\nPas de grande découverte technique pour nous, mais cette conférence nous a surtout permis de confirmer que, chez BedRock, \nnous sommes de plus en plus matures sur la scalabilité de nos clusters Kubernetes.\n\nLa conférence donnée par AWS (vidéo) portait sur deux aspects :\n\n\n  Premièrement, l’utilisation d’instances Spot (une option à ne pas négliger si vous souhaitez fortement réduire vos coûts de compute) et les bonnes pratiques à mettre en place en utilisant ce type d’instances EC2.\n  Le second traitait de la scalabilité des nœuds avec ClusterAutoscaler mais présentait un nouvel outil de provisionnement de nœuds Kubernetes proposé par AWS : Karpenter.\n\n\nUne des différences notables par rapport à cluster-autoscaler est que Karpenter ne fonctionne pas avec des AutoScalingGroup AWS \nmais provisionne directement des instances EC2.\nOutre cette fonctionnalité, Karpenter est actuellement à l’étude chez BedRock, notamment car il permet l’utilisation de la \ndimension région, ce qui n’est pas possible avec cluster-autoscaler et nous pose des problèmes avec nos statefullsets dans des ASG multiAZ.\n\nRéseau, Bande passante et GPU\n\nAutre point abordé lors de la KubeCon : comment intégrer la bande passante comme une ressource limitante, de la même façon que le CPU et la RAM actuellement.\nNous avons pu suivre deux présentations à ce sujet : “Network-aware Scheduling in Kubernetes” de José Santos, Ghent University (video et “Better Bandwidth Management with eBPF” de Daniel Borkmann et Christopher M. Luciano, Isovalent (video).\n\nLa première session proposait un nouveau plugin (repo github) pour permettre l’orchestration du déploiement de nouveaux pods en fonction de leur charge et coût réseau, afin de réduire la latence des déploiements.\nUne nouvelle fonctionnalité de ce plugin est par ailleurs en développement et permettra d’éviter de déployer sur un nœud ou la bande passante est déjà saturée.\n\nLa seconde présentation exposait comment eBPF permet de mettre en place de nouveaux pods en prenant en compte la bande passante. Le replay de la conférence est disponible ici, nous vous conseillons son visionnage.\nCette approche pourrait être très intéressante pour Bedrock si nous décidions de migrer notre plateforme VOD sur un cluster Kubernetes : en effet, elle nous permettrait de mieux gérer les burst réseaux et le throttling de la bande passante qui se produisent sur nos instances.\n\nCôté GPU, Google, dans son exposé “Improving GPU Utilization using Kubernetes” de Maulin Patel et Pradeep Venkatachalam (video), nous a présenté deux façons de partager des ressources GPU dans un cluster kubernetes :\n\n\n  soit en partageant le temps d’utilisation (timesharing, temporal multiplexing) entre conteneurs sur un même nœud,\n  soit en multi-instance GPU (MIG, spatial multiplexing) permettant de partager les ressources en parallèle entre conteneur en allouant une partie des cœurs GPU et de sa mémoire pour chaque conteneur.\n\n\nCette conférence sur l’utilisation des GPU dans un cluster k8s nous incite à réfléchir aux optimisations que nous pourrions faire sur nos plateformes vidéo et data…\n\nService Mesh : Cilium\n\nAu cours de diverses conférences, nous avons plusieurs fois entendu le nom de “Cilium” associé au concept de Service Mesh.\nLa conférence “A guided tour of Cilium Service Mesh” (vidéo) nous a permis d’en apprendre plus sur ce nouveau service qui ne se base plus sur des sidecars, mais sur eBPF.\n\nUn outil peut-être encore un peu jeune, mais clairement prometteur – et très certainement quelque chose que nous allons étudier lors d’un POC dans le courant de l’année ;-)\n\nRécapitulatif\n\nIl n’existe toujours pas d’outils magique pour passer à l’échelle et supporter les pics de charges.\nToutefois, les solutions présentées au cours de cette KubeCon EU 2022 viennent répondre à des besoins qui sont apparus au fil des années et dont peu d’utilisateurs avaient mesuré l’impact au début de leur périple avec Kubernetes.\n\nAussi, eBPF continue à faire parler de lui et son utilisation semble se répandre.\nL’idée d’un service mesh plus léger que Istio, par exemple, a l’air fort intéressante !\n\n\nRejoignez-nos équipes et venez vivre les prochaines conférences avec nous l’an prochain\n"
} ,
  
  {
    "title"    : "Bedrock à la kubecon 2022, 1ere partie : performances applicatives et scalabilité",
    "category" : "",
    "tags"     : " kubecon, kubernetes, cloud, k8s, conference",
    "url"      : "/2022/06/13/kubecon-2022-part-1.html",
    "date"     : "June 13, 2022",
    "excerpt"  : "\n\nBEDROCK à la KubeCon 2022\n\nAprès 2018 à Copenhague et 2019 à Barcelone, cette année encore, nous étions trois, Coraline, Julien et Pascal, présents à la KubeCon CloudNativeCon Europe 2022, à Valencia !\n\nPlus de quatre ans après le début de notre...",
  "content"  : "\n\nBEDROCK à la KubeCon 2022\n\nAprès 2018 à Copenhague et 2019 à Barcelone, cette année encore, nous étions trois, Coraline, Julien et Pascal, présents à la KubeCon CloudNativeCon Europe 2022, à Valencia !\n\nPlus de quatre ans après le début de notre migration vers Le Cloud (AWS + Kubernetes) racontée dans Le Plan Copenhague, nous visions à découvrir de nouvelles idées, à confirmer certains de nos choix et à apprendre des retours d’expérience de nos pairs. Après tout, avec une communauté aussi large (plus de 7000 participants et participantes cette année), il serait dommage de rester seuls avec nos idées !\n\nSommaire\n\nÀ trois, nous avons assisté à une grosse quarantaine de conférences. Nous avons choisi d’organiser nos notes par thèmes, en quatre articles :\n\n  Un premier, celui-ci, centré sur les performances applicatives, sur la scalabilité des applications et la gestion des coûts.\n  Le second, consacré aux performances système, aux services mesh, aux fonctionnalités au niveau du cluster.\n  Le troisième, pour regrouper ce qui est Dev XP, outillage, CI/CD, rollback, observabilité…\n  Et un dernier, pour quelques sujets divers, dont le chaos engineering et la résilience, et pour conclure sur ce que nous avons retenu de cette édition de la KubeCon publication jeudi.\n\n\nAvec une plateforme de VOD et de replay déployée en marque blanche pour des broadcasters européen majeurs, des millions d’utilisateurs actifs, des milliers de CPU consommés, des centaines d’instances allumées et des dizaines de microservices, les performances sont au cœur de nos préoccupations.\n\nCet article reprend nos retours sur les nombreuses conférences consacrées à la scalabilité lors de cette KubeCon 2022. Cette fonctionnalité essentielle de Kubernetes est l’une des raisons de notre migration sur cette plateforme. En effet, notre activité nécessite que nous adaptions la taille de nos clusters en fonction du nombre d’utilisateurs connectés.\nNous avons donc assisté à la plupart des conférences consacrées à la performance et à l’adaptation de celle-ci en fonction de nos besoins.\n\nLe scaling vertical\nLa conférence “How Lombard Odier Deployed VPA to Increase Resource Usage Efficiency” (vidéo) nous présentait comment fonctionnent les requests et limits.\nUn sujet qui demande du temps pour être efficace afin de ne pas être en oversizing ou au contraire en undersizing.\n\nMais surtout, le conférencier nous a présenté son implémentation d’un composant Kubernetes assez rarement utilisé : Le VerticalPodAutoscaler. Le VPA à fait récemment l’objet de discussions au sein de nos équipes et cette présentation a confirmé notre ressenti : cette ressource est intéressante pour des cas d’usages spécifiques, notamment sur des “workloads” assez consommateurs en RAM et/ou en CPU et ne pouvant pas être découpés en multiples pods via un HorizontalPodAutoscaler.\n\nle VPA souffre toujours d’une limitation : l’ajout de RAM ou CPU à chaud n’est pas possible et nécessite la re-création du pod.\n\n\n\nAméliorer la scalabilité\nUne autre conférence, donnée cette fois-ci par Intel, présentait un projet récent : Telemetry Aware Scheduler (vidéo). Cet outil permet d’améliorer les choix du scheduler de Kubernetes en s’appuyant sur des métriques “customs”. Le projet est récent et en ALPHA, mais à surveiller dans l’avenir.\n\nLors d’une autre conférence intitulée “How Adobe is optimizing resource usage in K8s” (vidéo), Carlos Sanchez a présenté un outil interne permettant d’émettre des recommandations basées sur un historique de métriques, un peu comme fait VPA, mais au niveau d’un namespace ou du cluster entier. Il est également revenu sur comment ils parviennent à éteindre automatiquement des applications non utilisées par les clients pour réaliser des économies conséquentes.\n\nMais comment configurer les requests, limits et tout ça… sans y passer des mois ?\n\nNotre plateforme est composée de dizaines de services qui interagissent les uns avec les autres et sont soumis à un trafic qui varie au quotidien, avec des pics parfois impressionnants. Le paramétrage des requests et limits de chaque conteneur, ainsi que d’autres ressources, comme le nombre de processus php-fpm par conteneur, est un travail de fourmi, où nous devons itérer quotidiennement pendant une ou deux semaines, en travaillant application par application. Et tout ce travail est à refaire lorsque les applications ou leurs usages évoluent… un vrai casse-tête !.\nNous ne sommes pas les seuls à rencontrer ces problématiques et c’était le sujet de la conférence “Getting the optimal service efficiency that autoscaler won’t give you” (vidéo), où une approche basée sur de l’IA (ou, plutôt, sur du brute-force) était présentée.\n\nVoici les grandes lignes de la méthodologie présentée :\n\n  définition d’un scénario de load-test (ce qui reste difficile, il faut qu’il soit représentatif de la réalité)\n  Définition d’objectifs (les temps de réponses attendus, le pourcentage d’erreurs… en fait, des SLOs que chacun devrait déjà avoir pour ses services),\n  Lancer en boucle ces scenarios en retouchant request et limits (et configuration JVM) entre chaque itération.\n\n\nSur un cas réel, après la 34ᵉ itération (réalisées en 19 heures), environ 49% d’économies ont été réalisées. Mais surtout, cela a représenté un jour de travail grâce à cet outillage, au lieu de deux mois à la main.\n\nLe logiciel utilisé ne semble pas disponible en open-source, mais l’approche “automatiser les itérations” en retouchant les paramètres est très intéressante et nous saurions la reproduire. Elle nous permettrait de gagner beaucoup de temps, en supprimant beaucoup de tâches fastidieuses aujourd’hui. Reste à continuer à définir des SLOs, puis créer de nouveaux scénarios de load-testing représentatifs ! ;-)\n\nEt les coûts d’hébergement, alors ?\n\nNous avons aussi entendu parler plusieurs fois de coûts d’hébergement tout au long de cette KubeCon : comme l’illustrent les travaux de la FinOps Foundation, nous sommes de plus en plus nombreux à réaliser que si nous ne pensons pas à l’impact financier de nos infrastructures élastiques, où n’importe quel membre des équipes peut déployer des applications, la facture augmente vite et fort.\n\nLe talk “Why Kubernetes can’t get around FinOps - Cost Management best practices” (vidéo) était une bonne introduction aux principes de gestion de coûts sur Kubernetes. Rien de nouveau pour nous, sur la théorie… même s’il nous reste encore beaucoup de progrès à réaliser pour mieux maîtriser nos frais d’hébergement !\n\nConclusion\nSur ces sujets de scalabilité, les conférences auxquelles nous avons assisté confirment que bon nombre des choix que nous avons fait sont les bons, et que les problématiques qui nous font encore souffrir sont partagées par d’autres membres de la communauté.\n\nNous allons prochainement tenter de mettre en place VPA sur un de nos composants majeur, VictoriaMetrics, qui consomme beaucoup de ressources quelques heures par jour et pour lequel un scaling horizontal n’est pas adapté.\n\nNous n’en avons pas (ou peu) entendu parler pendant cette KubeCon, mais nous étudions en ce moment la solution Karpenter pour remplacer cluster-autoscaler, très utilisé dans la communauté, mais qui ne sait pas réellement tirer profit de spécificités liées à AWS.\n\nEnfin, sur les coûts… OK, il n’y a pas que chez nous que c’est compliqué. Et c’est clairement un sujet, dans Kubernetes comme au niveau d’AWS, sur lequel nous avons encore du boulot devant nous pour un an ou deux. Nous avons même un poste FinOps ouvert ;-)\n\n\n"
} ,
  
  {
    "title"    : "Bedrock&#39;s backend architecture and its front API Gateway",
    "category" : "",
    "tags"     : " backend, php, api, api-gateway, back-for-front",
    "url"      : "/2022/06/10/backend-bff-intro.html",
    "date"     : "June 10, 2022",
    "excerpt"  : "What is a BFF, and how does it simplify the development of frontend applications?\n\nIntroduction\n\nAhoy there o/\n\nThis article is the first in a series explaining the backend architecture we use at Bedrock.\nThis first piece is dedicated to the BFF A...",
  "content"  : "What is a BFF, and how does it simplify the development of frontend applications?\n\nIntroduction\n\nAhoy there o/\n\nThis article is the first in a series explaining the backend architecture we use at Bedrock.\nThis first piece is dedicated to the BFF API. Without further delay, let’s jump into the subject!\n\nSo, what’s a BFF?\n\nFor years, we have been using a microservices pattern (1). Each with their own responsibilities.\n\nBackend and frontend development have long been decoupled.\nEvery frontend applications had to know about all microservices, call them and know what to do with their data.\n\n\n\nThis approach had three main limitations from a frontend point of view:\n\n  It forced Bedrock to duplicate logic in each application.\n  It prevented us from deprecating legacy APIs.\n  New features implied a frontend development and deployment.\n\n\nThere were other downsides, which were mainly derivatives from those listed above.\n\n  As an example, updating an icon into the menu bar required us to deploy all applications. It is not always easy or doable, and cannot be forced onto users without losing some of them.\n\n\nThe BFF tries to answer those limitations!\n\nIt’s a single API (2) that handles all the frontend applications queries to display contents, navigation, or even start downloads.\nIn addition, this gateway (3) gathers all business logic. This is done in order to avoid repeating the logic in each application.\nThat’s what we call a Back For Front!\n\nAbstracting the microservices\n\n\n\nThe first main advantage is to abstract the backend complexity for the frontend teams.\nIn the previous model, each application had to know where each data came from, how to parse it, and what to do with it.\n\nIn the new model, we can easily deprecate an API, replace it, change how the data is stored or returned.\nTo do so, we only need the team leading the change, and the team handling the BFF to work together at their own pace.\nThey can decide, depending on the change, how to handle the migration.\nThey might decide to use a new endpoint to be switched at some point, or add a new attribute in the response, etc.\n\nAll those changes will happen without any frontend application noticing it.\n\nSimplification of the data structure\n\nAnother advantage is to simplify the data representation.\n\n\n\nBy taking all this responsibility in a single API, it now translates the data from the APIs to a single unified representation that all applications can use.\n\n\n\nThis representation is maintained by the BFF in a single openapi schema (4). It shares the same concepts between the multiple endpoints of the API.\n\nThe main usage of the BFF is to handle the navigation between the pages of the application.\nIn the pictures above and below, the central block shows the application screen. The application page is split into two parts.\n\nThe top is answered by the navigation endpoint which gives a list of groups and entries.\nEvery entry can have nested groups, and an action.\n\nThe second part is what we call the layout. It’s a representation of the page, composed of multiple blocks, each with a list of items.\nEach item has a title, a description, an image, and an action (the same type as in the entries).\n\nThis makes the BFF responsible for what to display in the page, and in which order and how to display it.\nHow to display things is described through template strings that tell how to display each block.\n\nIt’s important to understand that the BFF does not return HTML! It returns a JSON string that needs to be parsed and interpreted by the application.\n\nEvery application still has to care about its design system, what font to use, which iconography.\nThis means that a template Card might not be displayed exactly the same between a computer, a mobile phone or a television; even if the data are the same.\n\n\n\nThere are other usages to the BFF (5), such as handling downloads, and some others to come, but it shares the same concept by answering to the front something to display.\n\nKeeping all logic in one place\n\nThe last main gain with the BFF, is that we’re able to put all the logic in one place.\nThis allows us to update and change the business rules at any time.\n\nHere are a few examples\n\n\n  When a user tries to navigate the application, if he uses a new device while he has already reached the limit of allowed devices, we can display a layout asking him to delete a device first.\n\n\nThis limit can be removed or changed at any time in all applications.\n\n\n  In France, explicit contents must be filtered out during daytime\n\n\nIf this rule changes, we will do so directly in the BFF, and no application will ever notice it.\n\nConclusion\n\nThe model known as back for front or API Gateway is nothing new and other major services already use it.\nWe’ve been using this model for more than 3 years now. It has undergone some major updates (6) but this is a model we’re happy with.\n\nWe plan to expand this pattern to handle even more logic inside the BFF in the coming years and keep being frontend application’s best friend.\n\n\n\nThat’s all for today’s post!\n\nIn the next part we will talk about handling the failures of the dependencies the BFF is calling, and what to do to always answer something usable by the applications.\n\nNotes\n\n\n  For more details about microservices, you can read this piece from AWS.\n  There are some other APIs called by our applications, such as the authentication service, but let’s not get lost into details…\n  There’s a lot of resources about API Gateway, here is one from nginx.\n  Open API is used to define the communication standards between our BFF and the clients, more explanation on the dedicated website of the organization.\n  In addition to note 1, we are currently moving to the api gateway model, and some behaviors still require the application to call dedicated microservices.\n  ( in French 🇫🇷 ) An old conference from 2020 given by Benoit VIGUIER, previous Team Lead in charge of the BFF, about API gateway and asynchronous development.\n\n\nFrom the same series\n\n\n  What’s a BFF\n  Handling API failures in a gateway\n  What’s an error, and handling connection to multiple APIs\n  Using a circuit breaker\n\n\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  🇺🇸 Announcing BedrockStreaming/pr-size-labeler github action\n  🇫🇷 Retour sur l’AFUP Day Lille 2022\n\n"
} ,
  
  {
    "title"    : "Migration progressive vers Redux Toolkit",
    "category" : "",
    "tags"     : " redux, lyonjs, meetup, react, javascript, conference",
    "url"      : "/2022/06/08/migration-progressive-vers-redux-toolkit.html",
    "date"     : "June 8, 2022",
    "excerpt"  : "Redux est le gestionnaire d’état global le plus populaire au sein de la communauté JS.\nSes créateurs encouragent désormais l’utilisation de Redux Toolkit (RTK). Une suite d’utilitaires facilitant l’usage de Redux et réduisant notamment sa verbosit...",
  "content"  : "Redux est le gestionnaire d’état global le plus populaire au sein de la communauté JS.\nSes créateurs encouragent désormais l’utilisation de Redux Toolkit (RTK). Une suite d’utilitaires facilitant l’usage de Redux et réduisant notamment sa verbosité.\nDans cette présentation, je vous propose un live coding pour migrer pas-à-pas une application React/Redux vers RTK.\n"
} ,
  
  {
    "title"    : "Comment ne pas jeter son application Frontend tous les deux ans ?",
    "category" : "",
    "tags"     : " conference, js, react, lyonjs, meetup",
    "url"      : "/2022/06/08/comment-ne-pas-jeter-votre-application.html",
    "date"     : "June 8, 2022",
    "excerpt"  : "Bonnes pratiques pour la maintenance d’une application web\nRefaire son front tous les 2 ans, c’est devenu une pratique plutôt courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la même codebase et ç...",
  "content"  : "Bonnes pratiques pour la maintenance d’une application web\nRefaire son front tous les 2 ans, c’est devenu une pratique plutôt courante dans tous les projets. Tous ? Non, chez Bedrock streaming, nous avons choisi de rester sur la même codebase et ça depuis plus de 7 ans! En plus, ce n’est pas une petite application puisqu’il s’agit de 6play et de salto.\nVous pourriez vous dire: “Oh les pauvres, maintenir une application vieille de presque 10 ans ça doit être un enfer !”\nRassurez-vous, ce n’est pas le cas ! Nous avons tous travaillé sur des projets bien moins vieux mais sur lesquels le développement de nouvelles fonctionnalités était bien plus pénible.\n\nQuel est notre secret ? C’est ce que vous allez découvrir pendant ce talk !\nAutomatisation des tâches courantes, gestion de la dette, testing et architecture seront des sujets abordés.\nCe talk propose des thématiques qui ne concernent pas que le frontend !\n\nPlus de détails dans l’article suivant.\n\n"
} ,
  
  {
    "title"    : "🍪 It’s Cookie Jar Time 🍪 #LFT 03/06/22",
    "category" : "",
    "tags"     : " UX, lft, tech",
    "url"      : "/%F0%9F%8D%AA-its-cookie-jar-time-%F0%9F%8D%AA",
    "date"     : "June 3, 2022",
    "excerpt"  : "Découvrez “Cookie Jar”, la base de données de connaissances utilisateurs qui centralise et documente toute l’UX research produite chez Bedrock. \nPrésenté par Elise Carenau.\n",
  "content"  : "Découvrez “Cookie Jar”, la base de données de connaissances utilisateurs qui centralise et documente toute l’UX research produite chez Bedrock. \nPrésenté par Elise Carenau.\n"
} ,
  
  {
    "title"    : "La pression je ne la subis pas, je la fais #LFT 03/06/22",
    "category" : "",
    "tags"     : " homemade, diy, beer, brewing, lft, partage",
    "url"      : "/la-pression-je-ne-la-subis-pas-je-la-fais",
    "date"     : "June 3, 2022",
    "excerpt"  : "Dans cette présentation, Mathieu Lopez nous présente un retour d’expérience sur le brassage de bière.\nBrasser sa bière, comment ça marche ? Quelles sont les étapes clés ? \nAu final, c’est quoi une bière ?\n\n",
  "content"  : "Dans cette présentation, Mathieu Lopez nous présente un retour d’expérience sur le brassage de bière.\nBrasser sa bière, comment ça marche ? Quelles sont les étapes clés ? \nAu final, c’est quoi une bière ?\n\n"
} ,
  
  {
    "title"    : "Errances à Kiilopää #LFT 03/06/22",
    "category" : "",
    "tags"     : " voyage, lft, partage",
    "url"      : "/errances-a-kiilopaa",
    "date"     : "June 3, 2022",
    "excerpt"  : "Mode d’emploi et retour d’expérience d’un trek polaire hivernal en solitaire présenté par Sylvain Guyon.\n",
  "content"  : "Mode d’emploi et retour d’expérience d’un trek polaire hivernal en solitaire présenté par Sylvain Guyon.\n"
} ,
  
  {
    "title"    : "Créer un jeu vidéo en moins d’une heure sur Unity #LFT 03/06/22",
    "category" : "",
    "tags"     : " diy, livecoding, brewing, lft, tech",
    "url"      : "/creer-un-jeu-video-en-moins-dune-heure-sur-unity",
    "date"     : "June 3, 2022",
    "excerpt"  : "Julie Nginn nous présente une introduction au moteur de jeu Unity, en livecodant la construction d’un jeu video.\nPas besoin d’être développeur pour pouvoir créer un jeu vidéo, ce talk s’adresse à tout le monde 🙂\n",
  "content"  : "Julie Nginn nous présente une introduction au moteur de jeu Unity, en livecodant la construction d’un jeu video.\nPas besoin d’être développeur pour pouvoir créer un jeu vidéo, ce talk s’adresse à tout le monde 🙂\n"
} ,
  
  {
    "title"    : "Connaissez vous Cache°Cache ? #LFT 03/06/22",
    "category" : "",
    "tags"     : " swift, ios, lft, tech",
    "url"      : "/connaissez-vous-cache%C2%B0cache",
    "date"     : "June 3, 2022",
    "excerpt"  : "Petite découverte d’un pattern de composition en utilisant une librairie de cache comme exemple, présentée par notre expert Sebastien Drode.\n",
  "content"  : "Petite découverte d’un pattern de composition en utilisant une librairie de cache comme exemple, présentée par notre expert Sebastien Drode.\n"
} ,
  
  {
    "title"    : "Comment faire un trailer vidéo qui déchire avec les technos web ? #LFT 03/06/22",
    "category" : "",
    "tags"     : " video, react, js, remotion, ffmpeg, lft, tech",
    "url"      : "/comment-faire-un-trailer-video-qui-dechire-avec-les-technos-web",
    "date"     : "June 3, 2022",
    "excerpt"  : "Un jour, alors que Mickaël Alves était fraichement arrivé à Bedrock, il a eu le malheur de demander à Antoine Caron sur quoi il bossait entre midi et deux, qui semblait fort l’amuser. Quelle erreur du dev Franco-Portugais ! :scream:\nIl ne se douta...",
  "content"  : "Un jour, alors que Mickaël Alves était fraichement arrivé à Bedrock, il a eu le malheur de demander à Antoine Caron sur quoi il bossait entre midi et deux, qui semblait fort l’amuser. Quelle erreur du dev Franco-Portugais ! :scream:\nIl ne se doutait pas encore de la folie de son nouveau tech lead : « J’essaie de générer des vidéos en MP4 à partir de composants React, tu veux voir ? »\n"
} ,
  
  {
    "title"    : "Amateur de pression #LFT 03/06/22",
    "category" : "",
    "tags"     : " plongée, partage, lft, partage",
    "url"      : "/amateur-de-pression",
    "date"     : "June 3, 2022",
    "excerpt"  : "Ivresse des profondeurs, exploration, dépassement de soi, durant ce talk, Hugo Riffiod nous partage sa passion pour la plongée sous-marine.\n",
  "content"  : "Ivresse des profondeurs, exploration, dépassement de soi, durant ce talk, Hugo Riffiod nous partage sa passion pour la plongée sous-marine.\n"
} ,
  
  {
    "title"    : "Announcing BedrockStreaming/pr-size-labeler github action 🎉",
    "category" : "",
    "tags"     : " oss, github, devops",
    "url"      : "/2022/05/31/github-action-pr-size-labeler.html",
    "date"     : "May 31, 2022",
    "excerpt"  : "\n\nSmaller PR for a reduced mental load\n\nFor several years at Bedrock Streaming the technical teams have used the Pull Requests code review for each project. \nBetween collective ownership, quality improvement, regression detection, knowledge sharin...",
  "content"  : "\n\nSmaller PR for a reduced mental load\n\nFor several years at Bedrock Streaming the technical teams have used the Pull Requests code review for each project. \nBetween collective ownership, quality improvement, regression detection, knowledge sharing, learning, there is no question in this article to further legitimize the immense interest to implement this practice in your teams.\n\nThis practice can however lead to some problems, each developer who proposes Pull Requests for review by his colleagues can sometimes propose monstrous diffs.\nSometimes constrained by certain project mechanics or tools, but sometimes also by the “Wheelbarrow” effect.\n\n\n  While I was there, I took the opportunity to modify this too.\n\n\nIt always starts from a good will, however, to make PR that changes several intentions. \nBy creating his wheelbarrow, the developer is adding diff to a pull request that deviates from the original intent.\n\nLimiting the number of intentions of a pull request often simplifies the proofreading of it.\n\n\n  Has anyone ever had the pleasure of reviewing a Pull Request with more than 1000 lines of changes with more than 100 modified files?\n\n\nWe also forget that making a “big” Pull Request can also generate a mental load on the person or persons assigned to its development. \nWe have to remember the modified files, we are more likely to generate conflicts.\n\n\n  Ok, lets make smaller PR’s! We promise!\n\n\nYou can’t improve anything without measuring it\n\nSaying “from now on we do smaller PR1” is a pious hope.\nWe have been doing application monitoring for a long time, we know that thanks to these measurements we are able to understand if the evolution is rather positive or not.\nWhy not do it on our PR sizes?\nWhy not implement monitoring on our devs?\n\nThe idea is absolutely not to measure/comparison the performance of our developers. \nIt would not be positive for the engineering manager and the dev to compare the performance of one developer against another. \nWe are all different after all!\n\nThe size of a dev’s PRs does not reflect his productivity at all, it just allows to evaluate the personal and collective mental load produced.\nThere are other measures we would like to follow, but let’s start with the size of the PR.\n\nBe warned, the purpose of this metric is not to say “Oh! you made an XL size PR that’s not right” 😡.\nIt happens from time to time, and it’s not bad.\nYou should rather look at the distribution of PR sizes of a dev.\n\nLet’s take the example of a dev named Bob who would have this distribution over the last month:\n\n\nHere we see that Bob is globally making large PRs (taking arbitrary t-shirt sizes), seeing this we can say: As a TechLead, how can I best accompany Bob to make smaller PRs?\n\nNext, let’s look at Alice’s profile, which has a more centered distribution:\n\n\nHere, we can say that overall the majority of RPs are of moderate size (in this absolute scale), so the mental load should be lower than for Bob.\nThis remains an interpretation that will require some discussion to be sure.\n\nHow to set it up?\n\nIf you are interested in this measure and like us you use the Github Actions solution for your automation, it will be very easy for you to implement our brand new pr-size-labeler in your projects.\n\nTo do so, you can add a workflow to your Github repository:\n\nname: 🏷 PR size labeler\n\non: \n  pull_request:\n\njobs:\n  pr-labeler:\n    runs-on: ubuntu-latest\n    name: Label the PR size\n    steps:\n      - uses: BedrockStreaming/pr-size-labeler@v1.1.0\n        with:\n          token: $\n          exclude_files: .lock # RegExp of your excluded file pattern\n\n\nThe action will then put Size/S, Size/XL tags on your PRs automatically according to the number of modified files and the number of added or deleted lines.\n\n🧙‍ You can change the text of the labels used and even the thresholds for each size as you wish.\nTake a look at Github presentation page of this Github action.\n\nOnce set up, you should also notice the added labels can allow to evaluate the time needed for the review before starting it.\n\n\n  I’ve got 30 minutes to spare, I’m not going to start reviewing this PR XL.\n\n\nIt’s now your turn to play!\n\n  \n    \n      Alias for Pull Request &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Retour sur l&#39;AFUP Day Lille 2022",
    "category" : "",
    "tags"     : " backend, php, conference",
    "url"      : "/2022/05/30/afup-day-lille-2022.html",
    "date"     : "May 30, 2022",
    "excerpt"  : "\n\nCette année encore, Bedrock participait à l’AFUP Day 2022, nous avons eu la chance de profiter de conférences de qualité et aux sujets variés.\n\nPHP 8.1 en détail\n\nDamien Seguy nous a parlé des nouveautés de PHP 8.1 mais aussi de celles de PHP 8....",
  "content"  : "\n\nCette année encore, Bedrock participait à l’AFUP Day 2022, nous avons eu la chance de profiter de conférences de qualité et aux sujets variés.\n\nPHP 8.1 en détail\n\nDamien Seguy nous a parlé des nouveautés de PHP 8.1 mais aussi de celles de PHP 8.2 qui arriveront prochainement.\n\nDamien nous a parlé en vrac :\n\n\n  de l’ajout du format AVIF dans GD\n  l’ajout des fonctions fsync et fdatasync qui permettent de synchroniser les données d’un fichier sur l’OS. Cela veut dire que le fichier sera bien écrit sur le disque.\n  l’arrivée de la prise en compte du format ristreto255 avec le libsodium\n  de la fonction array_is_list\n\n\nLe conférencier a insisté sur le fait de ne pas hésiter à monter de version régulièrement. Il est possible d’utiliser les polyfills déjà existants (pour la 8.1 et même la 8.2) ou d’ajouter ses propres fonctions, mais de bien faire attention d’utiliser function_exists pour préparer les migrations.\n\nUne des nouveautés phare de la version 8.1 est l’ajout de la gestion des enums. Une énumération est un objet et non un type scalaire. Il est donc possible de l’étendre (via des interfaces, …). Cela veut aussi dire que nous ne pouvons pas les utiliser comme clefs de tableau par exemple. Il est par contre possible d’utiliser la propriété $myEnum-&amp;gt;value. Attention, les énumérations et les classes partagent le même espace de nom, nous ne pouvons donc pas avoir une enum et une classe s’appelant pareil dans le même namespace.\n\nLes Fibers, solution pour rendre les programmes interruptibles, a été rapidement introduite, mais nous vous conseillons de regarder la conférence sur ce sujet de notre cher et estimé collègue Benoit Viguier.\n\nDans la dernière version, à ce jour, a été introduit le type de retour never. Une fonction ou méthode qui retourne ce type ne pourra pas faire de return (même vide) ni même un yield. Il sera, par contre, possible d’appeler die, exit, throw ou suspend.\n\nLes constantes peuvent maintenant être finales, cela bloquera la possibilité de surcharger leurs valeurs par héritage.\n\nIl est désormais possible de faire des propriétés readonly. Cette propriété devra forcément être typée, et ne pourra pas avoir de valeur par défaut. readonly ne peut pas être utilisé avec static. Si la propriété est un objet, l’instance pourra être modifiée (par exemple avec des setters), mais pas remplacée.\n\nIl est maintenant possible d’instancier des valeurs par défaut. Par exemple :\n\nfunction serialize(\n    string \\$data, \n    Formatter \\$formatter = new DefaultFormatter()\n) { ... }\n\n\nCette instanciation est possible dans les arguments de fonction ou de méthode, les variables statiques ou encore les constantes globales. Par contre, ce n’est pas compatible avec les constantes de classes ou les propriétés de classes (sauf si ces dernières sont des propriétés promues).\n\nDans sa version 8.1, PHP apporte aussi les types d’intersections. Un exemple présenté serait de vouloir une instance de type Traversable ET Countable. Les types scalaires ne sont pas acceptés, c’est uniquement avec plusieurs classes.\n\nDe l’humain à l’ordinateur, ou découvrir le sens d’un texte avec ElasticSearch\n\nMathias ARLAUD nous a ensuite parlé d’ElasticSearch et de comment il est possible d&#39;attribuer un score de corrélation entre un texte donné et une multitude d&#39;autres.\n\n\nIl a décortiqué cette équation (déjà simplifiée) en nous expliquant les mécanismes en place pour calculer ce score.\nIl nous a parlé de Term Frequency (la fréquence à laquelle un mot apparaît dans un document), d’Inverse Document Frequency (la pertinence des mots) ainsi que de Coordination Factor (le fait de valoriser un document avec le plus haut pourcentage de mots présents dans la requête).\nAvec un exemple simple (Les Développeurs 🥰 développer avec,VIM&amp;amp;excl;), il nous a montré comment les filtres de caractères tels que html_strip, le mapping, les générateurs de tokens (whitespace -&amp;gt; 1 token = 1 mot) ou bien encore les filtres de tokens (phonetic, stopwords) permettent d’enlever le bruit des phrases humaines pour ne récupérer que les informations les plus pertinentes pour calculer ce score de corrélation. Après application de ces différentes étapes, son exemple se transforme en [developp] [aim] [developp] [vim], ce qui permet alors à ElasticSearch d’éliminer des documents non pertinents et qui seraient remontés si ces filtres n’avaient pas été appliqués.\nPour plus d’informations, Mathias a mis à disposition les slides de son\ntalk.\n\nGuide pratique d&#39;une méthodologie UX pour la conception de features\n\nJessica Martel nous a présenté une méthodologie UX pour la conception de features qu’elle a mise en place et suivie lors d’une expérience chez Decitre et maintenant chez Unow.\nElle nous a parlé de l’importance de la constitution d’une équipe projet regroupant tous les acteurs (PO, devs, le pôle Design et les équipes métier). Diversifier les acteurs permet d&#39;accroître l’adhésion du projet, d’apporter différentes cultures et de cibler le besoin.\n\nS&#39;ensuivent plusieurs étapes :\n\n\n  Product concept : évaluation du besoin, de la criticité\n  UX Research et audit : bench global, entretiens\n  User journey : identification des différentes étapes (déterminées suite au bench et entretiens), mise en place du workflow\n  Specs fonctionnelles et design : cas d’usages, règles métier, versions, KPI / création de wireframes, maquettes UI et prototypes\n\n\nCette méthodologie comprend cependant des limites ! Elle prend beaucoup de temps et est soumise au contexte, aux priorisations d’autres features, au downsizer…\n\nLe Prométhée moderne : embarquer PHP dans Go\n\nKévin Dunglas nous a parlé de comment embarquer PHP dans du Go. Après avoir listé les différentes SAPI (Module Apache, FPM, …) et nous avoir expliqué brièvement le langage Go et sa librairie standard net/http, Kévin nous a présenté FrankenPHP et toute la réflexion et les contraintes rencontrées pour le créer. Ce projet est un nouveau serveur Web en Go qui est capable d’appeler l’interpréteur PHP et donc de faire tourner nos applications Web. Le projet est bien avancé, mais pas terminé. N’hésitez pas à le contacter si vous voulez essayer avec vos applications, les retours lui seront utiles. En tout cas, chez Bedrock, on va suivre ça de près !\n\nLes subtilités du e-commerce à la française\n\nAprès nous avoir présenté les différentes taxes françaises et chez quelques-uns de nos voisins européens, David Buros nous a résenté les différents problèmes qu’il a rencontrés avec Sylius:\n\n\n  l’affichage du prix HT et du prix TTC\n  la gestion des réductions avec ce double affichage\n  la gestion des écotaxes\n  le paiement par mandat administratif\n\n\nComment on est passé de 1 800 emails à 70 000 par jour chez Trustt en 1 mois avec RabbitMQ\n\nCédric Driaux nous a expliqué comment ils ont mis en place RabbitMQ chez Trustt pour gérer l’envoi de plus de 70 000 mails par jour, afin de remplacer une ancienne solution qui lançait un CRON toutes les 15 minutes dans le but de faire les calculs et la distribution. Il y avait également des appels API à des outils externes, causant des ralentissements. De plus, certains reliquats de mail n&#39;étaient pas envoyés.\n\nPour répondre à ce problème, l’idée était de mettre les mails dans une file ou queue en anglais, permettant une mise en attente de l’envoi des messages, puis utiliser un consumer pour traiter les messages.\n\nCédric a décidé de choisir et de mettre en place (en à peine un mois !) RabbitMQ comme solution à cette problématique. Cela a permis : une baisse de charge des serveurs, notamment due à la suppression des CRON, une augmentation des mails envoyés, dont ces derniers peuvent désormais tous être traités dans la journée. Les erreurs sont mises de côté pour être traitées plus tard et ne pas ralentir le processus. Enfin, il est dorénavant possible pour eux d’ajouter d’autres mails dans la queue dans la journée.\n\nGrâce à RabbitMQ, ils ont pu fortement augmenter leurs capacités d’envoi de mails tout en soulageant les serveurs.\n\nCecil, mon générateur de site statique\n\nArnaud Ligny nous a présenté son projet perso de générateurs de sites statiques en archive phar : Cecil. Il voulait une solution rapide à prendre en main, intuitive et avec une séparation entre le contenu et la mise en forme. Ce side project avait pour but de remettre ses connaissances à jour en appliquant les bonnes pratiques. L’application est automatisée, le paquet .phar est automatiquement généré par GitHub Action lors de la création d’une release, scrutinizer qui fait des corrections, des previews sont réalisables avec netlify.\n\nEt si on étendait SQL avec du PHP\n\nAntoine BLUCHET nous a présenté plus en détail les projets Doctrine et principalement l’ORM. Cet outil est extensible, mais a quelques limites. Comment pouvons-nous faire des requêtes complexes avec Doctrine ? Peut-on utiliser des Common Table Expression ? La réponse à ces questions, proposée, est ESQL. Cet outil permet de construire des requêtes SQL complexes facilement sans se soucier des noms des tables ou des colonnes, car il permet d’utiliser ces métadonnées depuis Doctrine.\n\nPourquoi vous n’attirerez et ne retiendrez pas les femmes dans vos équipes tech.\n\nMarcy Charollois fait un constat sur le monde du travail dans le numérique, dominé largement par les hommes et ne laissant que trop peu de place aux femmes. Marcy commence par introduire la notion d’habitus, qui désigne un système de préférences, de style de vie particulier à chacun, qui influence les pratiques des individus au quotidien. Ces pratiques sont intériorisées inconsciemment, car l’individu s’adapte et s&#39;intègre à son environnement social. Cela crée un groupe majoritaire qui devient décisionnaire. Se met alors en place un statu quo qui va soit inclure ou exclure et qui est fortement dominé par la pensée masculine. Marcy nous dévoile que sur 100 % de freins ressentis par les femmes dans la tech, 30% proviennent des biais d&#39;oppression de groupe, une part donc assez conséquente.\n\nLe constat est réel, les femmes dans la tech vivent mal leur condition de femmes, il faut changer ce sentiment, mais les attitudes face au changement sont variées. 15% de personnes sont réfractaires, il sera donc difficile de faire évoluer les choses avec eux, 15% sont déjà partantes et 70% sont neutres, potentiellement pour ce changement, mais ne savent pas comment le faire.\n\nMarcy nous donne alors des clés qui permettront d’attirer les femmes dans nos équipes en mettant en avant les freins ressentis par celles-ci : une expression du genre, une légitimité face au métier exercé, des a priori sur la provenance des profils féminins qui sont souvent reconvertis et donc potentiellement juniors :\n\n\n  féminiser les postes et en particulier sur les offres d’emploi, une femme est développeuse, pas développeur.\n  mettre en avant les témoignages de femmes qui montent dans votre entreprise pour donner des exemples concrets de ce qu’elles pourraient trouver en venant chez vous\n  s’intéresser réellement à elles et non pas de voir en vous ce qu’elles mettent en avant\n\n\nLa conférence continue sur les actions à mener pour garder les femmes dans nos équipes :\n\n\n  parler d’égal à égal pour éviter la posture sachant(e)/ignorante\n  soyez clair, transparent sur les salaires, les évolutions de poste\n  mettez en place des moments conviviaux plus portés sur des préférences féminines\n  minimiser les interruptions pendant les prises de parole\n  écoutez des besoins spécifiques inhérents aux femmes et accommodez-les en offrant des ressources sans juger : parentalité, menstruation, assistance psychologique, adaptation à l’emploi du temps\n  encouragez les femmes à prendre la parole, à devenir un rôle modèle parce que compétente\n\n\nMarcy termine sa conférence par la présentation de quelques chiffres sur l’évolution de carrière des femmes et des enjeux psychosociaux résultant\nde cette évolution et conclut en montrant les bienfaits de l’inclusion des femmes au sein des entreprises et en nous donnant quelques noms de femmes célèbres dans ce combat.\n\nConclusion\n\nEncore une fois, l’AFUP a réussi à faire un événement chaleureux, intéressant et diversifié\n\nNous sommes ravis d’avoir pu participer à cette manifestation qui nous a permis de rencontrer les membres de la communauté ainsi que de visiter rapidement la ville de Lille et manger des Welsh.\n\n\n"
} ,
  
  {
    "title"    : "You Build it, you run it",
    "category" : "",
    "tags"     : " conference, docker, meetup, devops",
    "url"      : "/2022/05/13/docker-meetup-you-build-it-you-run-it.html",
    "date"     : "May 13, 2022",
    "excerpt"  : "L’une des grandes étapes de l’autonomie d’une équipe de développement dans la méthodologie DevOps est de s’intéresser à l’alerting lié à son infrastructure. Comment sommes-nous arrivé à proposer aux équipes de développement de s’intéresser et de m...",
  "content"  : "L’une des grandes étapes de l’autonomie d’une équipe de développement dans la méthodologie DevOps est de s’intéresser à l’alerting lié à son infrastructure. Comment sommes-nous arrivé à proposer aux équipes de développement de s’intéresser et de maîtriser cet alerting ?\n"
} ,
  
  {
    "title"    : "Retour sur l&#39;Android Makers 2022",
    "category" : "",
    "tags"     : " android, mobile, conference, makers",
    "url"      : "/2022/05/09/bedrock-android-makers-22.html",
    "date"     : "May 9, 2022",
    "excerpt"  : "Que c’est bon de se retrouver !\n\nAprès deux ans sans conférence en présentiel, l’Android Makers a fait son grand retour les 25 et 26 avril 2022, pour le plus grand bonheur de la communauté Android. \nL’équipe de développeurs Android de Bedrock (don...",
  "content"  : "Que c’est bon de se retrouver !\n\nAprès deux ans sans conférence en présentiel, l’Android Makers a fait son grand retour les 25 et 26 avril 2022, pour le plus grand bonheur de la communauté Android. \nL’équipe de développeurs Android de Bedrock (dont je fais partie) a partagé ce bonheur en assistant à ce rendez-vous incontournable. Jetpack Compose, accessibilité, optimisation de build et autres sont autant de sujets en maturation constante : essayons d’en faire le tour ensemble.\n\n\n\n\n  Que c’est bon de se retrouver !    \n      Penser         \n          Design System et Jetpack Compose \n          Accessibilité \n          Modularisation \n          Support de Chrome OS \n        \n      \n      Développer         \n          Splashscreen Android 12 \n          Tester les coroutines \n        \n      \n      Optimiser         \n          Toujours plus de CI \n          Optimisation du temps de build \n        \n      \n      Extras         \n          Création d’un UI Toolkit avec Romain Guy et Chet Haase \n        \n      \n      En conclusion \n    \n  \n\n\nPenser \n\nDesign System et Jetpack Compose \n\nPlusieurs conférences ont évoqué le sujet du Design et plus particulièrement l’implémentation d’un Design System (DS) avec Jetpack Compose. François Blavoet nous a partagé l’expérience d’Instacart à ce sujet, en dévoilant quelques détails d’implémentations de leurs API mises à disposition des features engineers, afin de leur faciliter l’intégration des éléments du DS.\nParallèlement, il nous a aussi invité à réflechir sur la nécessité d’intégrer Material Design dans notre implémentation du DS. En effet, si le cadre Material peut parfois s’avérer utile, il est quelques fois trop contraignant et pas toujours adapté aux besoins spécifiques de nos applications.\n\nAccessibilité \n\nVoilà un sujet qu’il est important d’évoquer, tant il est facile d’oublier d’adresser une application à tous. Cette édition de l’Android Makers a eu la chance d’accueillir une très belle conférence de Fanny Demey et Gerard Paligot sur le sujet de l’accessibilité. Dans une séance de Live Coding teintée d’un jeu de rôle sur le thème de l’émission C’est pas sorcier !, nous avons pu faire le tour de plusieurs points d’attention afin d’inclure au mieux nos utilisateurs porteurs de handicaps :\n\n  ne pas donner d’informations inutiles via TalkBack, comme les contentDescription des icônes décoratives\n  penser à la manière dont TalkBack va assembler les informations provenant de plusieurs vues distinctes\n  donner un retour d’action sur les clics de boutons et mieux placer ces actions lorsque le mode accessibilité est activé\n  et bien d’autres !\n\n\nLe Live Coding a pu également démontrer à quel point Jetpack Compose considère l’accessibilité comme une fonctionnalité cruciale grâce à des APIs très complètes (je pense ici à Modifier.sementics par exemple).\n\nModularisation \n\nJean-Baptiste Vincey, développeur chez Deezer, a partagé l’expérience de son équipe concernant la modularisation de leur code pour gérer le nombre grandissant d’applications dans leur catalogue. Basé sur la création de bibliothèques internes, plusieurs stratégies ont été explorées avec leurs bons et mauvais côtés. Lancé dans un chantier similaire, il est important pour Bedrock de voir comment d’autres acteurs du milieu ont répondu à ces questions, sans oublier que chaque entreprise a sa propre réponse qui doit s’adapter à ses process, son organisation et son produit.\n\nSupport de Chrome OS \n\nFrédéric Torcheux et Pierre Issartel, lors de leur conférence sur l’adaptation Chrome OS des applications Android, ont fait un constat intéressant : le nombre de Chromebook vendu a explosé récemment pour dépasser le nombre de Mac vendu. Sachant qu’un nombre grandissant de Chromebook a accès au Play Store, il est de plus en plus important d’adapter ses applications pour cet usage.\nEn vrac : exploiter le potentiel du curseur de la souris, naviguer dans l’application sans jamais quitter le clavier, supporter l’environnement multi-fenêtré et le redimensionnement de celles-ci, autant de points d’améliorations comportant pièges à éviter et bonnes pratiques.\n\nDévelopper \n\nSplashscreen Android 12 \n\nDeux développeurs de chez Google nous ont plongé dans les entrailles du WindowManager d’Android, ce composant qui est chargé d’orchestrer les applications que nous utilisons tous les jours : charger une application, la placer à l’écran puis la dessiner, la déplacer et gérer son cycle de vie, autant de responsabilités pour un WindowManager complexe à maitriser.\nÀ travers cette conférence pointue, Vadim Caen et Pablo Gamito ont rebondi sur le nouveau système de SplashScreen d’Android 12 pour nous expliquer quel problème il doit résoudre (essentiellement le ressenti de lenteur au lancement d’une application) et comment en tirer parti. À ce titre, la documentation de Google sur la migration vers le SplashScreen d’Android 12 est incontournable.\n\nLe nouveau Splashscreen pour Android 12 comporte son lot de challenges, notamment pour tenir compte des animations. Chez Bedrock, les reflexions à ce sujet ont démarré, et nous comptons partager un retour d’expérience sur notre propre migration !\n\nTester les coroutines \n\nMárton Braun, aussi développeur chez Google, a présenté les nouveautés de la bibliothèque kotlinx-coroutines-test en version 1.6+. Exit runBlockingTest, place au runTest qui permet, grâce à son TestCoroutineScheduler, de gérer les délais et l’ordre d’execution de toutes les coroutines lancées dans un test.\nL’ancienne version des API de test étant maintenant dépréciée, cette nouvelle version est encore experimentale mais vouée à passer en état stable, tant elle parait plus mature que la précédente.\nLes Flow et StateFlow n’ont pas été oubliés puisqu’ils ont aussi leurs spécifités en matière de tests.\n\nOptimiser \n\nToujours plus de CI \n\nChez Bedrock, la CI tient une place particulière dans nos process de release, et il est toujours intéressant de voir comment d’autres entreprises se saisissent de cet outil et améliorent leur process.\nAprès les rappels toujours pertinent sur l’importance de déléguer le maximum de tâches répétitives à nos environnement de CI, Xavier F. Gouchet, développeur chez Datadog, a présenté divers outils pour y parvenir.\n\nDetekt, un plugin Gradle permet d’aller encore plus loin qu’Android Lint en offrant l’analyse statique de n’importe quel code Kotlin. Son extensibilité nous est exposée via une API sur le pattern visiteur, redoutablement efficace pour parcourir le PSI (Program Structure Interface) de Kotlin. D’autres outils peuvent également être efficaces pour parcourir cette interface.\n\nXavier Gouchet présente également KSP (Kotlin Symbol Processor), le projet sponsorisé par Google voué à remplacer KAPT, son ancêtre basé sur Java. Combiné avec Kotlin Poet, cet outil permet d’automatiser la génération de code Kotlin à partir d’un code source annoté dans le projet.\n\nPour aider au développement autour du PSI Kotlin, Xavier Gouchet recommande l’excellent plugin PSIViewer pour IDE Jetbrains.\n\nOptimisation du temps de build \n\nZac Sweers est venu nous présenter la manière dont Slack, entreprise pour laquelle il travaille, optimise les builds Gradle. Les projets se complexifiant avec toujours plus de code et de modules, les temps de build ont tendance a augmenter.\n\nCette conférence a mis en lumière diverses optimisations pour tirer pleinement parti de Gradle et de ses nouvelles fonctionnalités :\n\n  désactiver les fonctionnalités non utilisées du plugin Android\n  profiter du cache, y compris sur serveur\n  éviter d’utiliser buildSrc pour factoriser du code\n  écrire son propre plugin de convention gradle\n  avoir un compte Gradle Entreprises pour profiter des Gradle Build Scans afin de déterminer quels sont les points de friction du projet, que ce soit pour l’utilisation du cache, la parallélisation, l’invalidation des builds, l’optimisation des arguments de la JVM…\n  parfois même, acheter du nouveau matériel ! (Apple Silicon)\n\n\nRéduire le temps de build est un enjeu constant, et participe au confort du développeur au quotidien.\n\nExtras \n\nCréation d’un UI Toolkit avec Romain Guy et Chet Haase \n\nUne conférence très intéressante a vu Romain Guy et Chet Haase nous présenter un projet experimental d’UI Toolkit maison, Apex, très proche de Jetpack Compose dans son API. \nCet exercice original a été un moyen de faire valoir le concept d’Entity component system, un pattern se basant sur la composition pour enrichir les comportements des entités d’un système.\n\nLeur présentation a mis en lumière la philosophie d’un UI Toolkit mais a aussi et surtout souligné la quantité de travail à accomplir pour passer d’un projet experimental à un toolkit utilisable en production. Enrichir sa boite à outils avec le maximum de widgets différents, permettre une personnalisation maximale aux développeurs, rendre le moteur de rendu multi-plateforme, autant de tâches nécessaires pour rendre votre Toolkit vraiment utile pour la communauté aujourd’hui.\n\nEn conclusion \n\n\n\nCette édition de l’Android Makers 2022 s’est conclue sur une conférence humoristique inédite de la part de Chet Haase et Romain Guy. Il a été question de tourner en dérision la communauté des développeurs sur le nombre de nouveaux patterns de développement qui sortent régulièrement et les discussions acharnées (et parfois virulentes) autour de ces derniers.\nCela a été une très bonne manière de prendre du recul sur notre communauté Android, et de se féliciter, tout de même, de la recherche constante d’amélioration des pratiques de développement au service de la qualité de nos produits et de leur accessibilité.\n"
} ,
  
  {
    "title"    : "How did we streamline the delivery of our internal conferences aka LFTs?",
    "category" : "",
    "tags"     : " lft, talks, live, stream, obs, streamyard, conference",
    "url"      : "/2022/04/29/how-we-simplify-our-lft-broadcast.html",
    "date"     : "April 29, 2022",
    "excerpt"  : "\n\nSome time ago, we shared with you an article explaining how we managed to capture and broadcast our conferences in the Bedrock auditorium. \nWe must admit, it worked great, but we wanted to make it simpler.\n\nAs a reminder, our internal conference...",
  "content"  : "\n\nSome time ago, we shared with you an article explaining how we managed to capture and broadcast our conferences in the Bedrock auditorium. \nWe must admit, it worked great, but we wanted to make it simpler.\n\nAs a reminder, our internal conferences or meetups are called LFT. \nIf you want to know more about our Last Friday Talk and the motivations that push us to do it, we invite you to read the above-mentioned article.\n\nAfter the success of this broadcasting, the (voluntary) organization team started thinking about how to make it simpler. \nIt was already a challenge in itself!\nIf you read this part of the article, you can see that we already had some ideas for improvements.\n\nImprovement areas\n\nIn the previous version, you could see that a very important quantity of material was necessary (meters of various cables, multi outlets, a camera, etc…)\nA large part of this material was kindly lent by Pascal (and we thank him for that), but we could not decently borrow it for each LFT.\n\nMoreover, this very specific equipment did not allow each Bedrock employee to manage the control room, quickly and with his or her own machine.\nFinally, as you can imagine, setting up and putting away such a large amount of equipment takes a lot of time. \nThere were no less than four of us setting up and tidying up.\n\nIn another topic, let’s talk about video quality. We used to use Google Live Stream before, but the 720p broadcast, with its very low bitrate and aggressive compression, sometimes made it difficult to follow.\nText and images were often very pixelated.\n\nAlso, we wanted to, easily, handle a hybrid mode to our LFTs.\nBecause of the pandemic, telecommuting and the fact that some of Bedrock’s employees are located in Paris, we need to broadcast and capture the LFT both in person and remotely.\nBy hybrid mode we also meant that during the same event, we need to host speakers from the location of their choice.\n\nWhat we did?\n\nAfter a few exchanges with other conference organizations, we decided to give Streamyard a chance.\nAfter some quick tests, we bought a license and started the new version of the LFT.\n\nWhat is Streamyard?\n\nStreamyard is a live streaming platform that runs directly in the browser.\n\nIt does not have all the customization and possibilities of OBS, but its huge advantage is its versatility.\nThis solution allows Bedrock employees to manage the LFT from any computer.\n\nHere are some sample images and overviews of the Streamyard UI:\n\n\n\n\n\nThe positive points of Streamyard:\n\n\n  Allows us to have several people in the control room at the same time\n  Streams in 1080p\n  Personalization: manage banners, chat questions, music on hold,…\n  Handles multi-speakers management\n  Re-stream to Youtube / Facebook / Twitter / Linkedin / Video recording\n  A free trial mode allows one to test it before taking out their credit card.\n\n\nNew setup\n\nAs a reminder, here is the organization of our auditorium during the live broadcast of our LFT.\nThe room is large enough to accommodate all Bedrockers who wish to attend in person the presentations of their colleagues. \nThe remote speakers have their conference broadcasted on the two screens of the room.\n\n\n\n\n\n\n\nWhat has changed since we switched to Streamyard is mainly related to the way we capture the image and sound of what is displayed on the screen.\nPreviously, we were using an Elgato HD60S+ device, to capture the HDMI output from the speaker broadcast. \nHowever, we had to try several times to get it to work each time we switched speakers. Not fun at all.\n\nNow, with Streamyard, every time we switch speaker, we just need to:\n\n  share the link of the “Streamyard broadcast”\n  The speaker joins the stream (and switches off her microphone and webcam)\n  Connect the HDMI cable of the speaker’s computer\n  The speaker displays her screen via the room’s projectors\n  The control room operator puts the RF microphone on the speaker(s)\n\n\nFor a remote speaker, it’s even easier.\nJust pass them the Streamyard link and they can use their webcam and their own microphone.\n\nThis is what our Stream setup now looks like with the cabling:\n\n\n\n\n\nWhat did we achieve?\n\nLFT is also a group of volunteers who give their energy to offer the best event possible, to offer to all Bedrock members a space where they can share their passion, technical subjects and others.\nFrom the proposal of their subject, through rehearsals and during the broadcast, the team is there to help speakers – either beginners or confirmed.\n\nThe switch to 720p and then to 1080p has been a real positive point for the quality of the live show, but also for the recording and the replay.\nMore than 200 participants during the last LFT on the day, the organizing team is delighted.\n\nThe simplification of the broadcasting setup since the switch to Streamyard has also made it easier to set up the room.\nSwitching from one speaker to another is less complex and can be done in just a few minutes.\n\nThe youtube lives allow participants to pause and rewind the broadcast.\nThis is really convenient for them.\nAlso, by going through the youtube chat, we can share questions directly on the screen and on the replay.\n\nThe LFT replays are also available on Bedrock’s Youtube channel in a private way accessible to all employees.\n\n\n\nNext steps\n\nWe don’t want to stop there.\nFor the next editions, we will try to do even better.\nWe are working on the matter of sharing some talks in public on our Youtube channel, so more people in our communities can learn from them.\nIn order to simplify the setup, we will try to put in place a more fixed table to avoid wiring and moving furniture.\nWe also wish to propose and train our employees to the use of this setup in order to allow us to host meetups and conferences in the best conditions.\n\nNow, it’s your turn: if your company or user-group does this kind of talks, how do you manage broadcasting and recording?\n"
} ,
  
  {
    "title"    : "⚡️ Vite ⚡️ the Webpack killer",
    "category" : "",
    "tags"     : " conference, js, webpack, vite, devoxx",
    "url"      : "/2022/04/21/vite-the-webpack-killer.html",
    "date"     : "April 21, 2022",
    "excerpt"  : "Toute application web a besoin d’être packagée afin d’être livrée en production. Pour répondre à cette problématique, de nombreux outils, connus sous le nom de modules bundler, sont apparus, et ces dernières années, c’est Webpack qui semble s’être...",
  "content"  : "Toute application web a besoin d’être packagée afin d’être livrée en production. Pour répondre à cette problématique, de nombreux outils, connus sous le nom de modules bundler, sont apparus, et ces dernières années, c’est Webpack qui semble s’être imposé comme l’outil incontournable.\n\nOn ne va pas se le cacher, si vous avez mis les mains dans une configuration webpack, c’est loin d’être un outil simple ni rapide.\n\nÇa n’a pas échappé à Evan You, le créateur de Vue.JS, qui voulait répondre à ces problématiques avec une nouvelle façon de procéder, avec des idées novatrices, reposant sur les dernières fonctionnalités des navigateurs : Vite.\n\nQuelles sont ces idées novatrices à la base de Vite ? En quoi concurrence-t-il Webpack ? C’est ce que nous allons voir dans ce talk et live coding!\n"
} ,
  
  {
    "title"    : "Bedrock à l&#39;AWS Summit 2022",
    "category" : "",
    "tags"     : " aws, summit, cloud, sysadmin, conference, kubernetes",
    "url"      : "/2022/04/20/aws-summit-2022-notre-retour-dexperience.html",
    "date"     : "April 20, 2022",
    "excerpt"  : "\n\nRetour à l’AWS Summit \n\nDeux années se sont écoulées depuis le dernier AWS Summit à Paris, il a fait son retour ce 12 avril !\nCet évènement, qui a lieu au printemps dans plusieurs pays, est l’occasion de rencontrer la communauté AWS française, d...",
  "content"  : "\n\nRetour à l’AWS Summit \n\nDeux années se sont écoulées depuis le dernier AWS Summit à Paris, il a fait son retour ce 12 avril !\nCet évènement, qui a lieu au printemps dans plusieurs pays, est l’occasion de rencontrer la communauté AWS française, d’assister à de nombreuses conférences et de bénéficier de retours d’expérience d’autres clients.\nC’était aussi pour nous, comme en 2019, l’occasion de partager les nôtres !\n\nDepuis notre migration vers le Cloud, AWS et Kubernetes entre 2018 et 2021 (plus d’informations dans Le Plan Copenhague), nous sommes plusieurs centaines à travailler au quotidien avec AWS.\nCette année, cinq de nos DevOps, SysOps et Développeurs ont eu la chance de se rendre à l’AWS Summit.\n\nNous partageons quelques notes que nous avons prises lors de cette journée, sur des sujets qui nous ont marqués et qui vont sans doute nous occuper une partie de l’année à venir.\n\nSommaire\n\n\n  Retour à l’AWS Summit\n    \n      Rendez vos équipes de Data Science 10x plus productives avec SageMaker\n      Comment le cloud permet à France Télévisions d’innover dans la diffusion de contenu live\n      Découvrez comment Treezor utilise AWS comme moteur de sa plateforme de Banking-as-a-service\n      Tracer votre chemin vers le Modern DevOps en utilisant les services AWS d’apprentissage machine\n      Innover plus rapidement en choisissant le bon service de stockage dans le cloud\n      Sécuriser vos données et optimiser leurs coûts de stockage avec Amazon S3\n      Minimiser vos efforts pour déployer et administrer vos cluster Kubernetes\n      Serverless et évènement, les nouvelles architectures\n    \n  \n  Nous étions aussi intervenants\n    \n      Transformer le load balancing pour optimiser le cache : objectif 50 millions d’utilisateurs\n      Etes-vous bien architecturé ?\n      Préparez et donnez votre premier talk\n    \n  \n  Conclusion de l’article\n\n\nRendez vos équipes de Data Science 10x plus productives avec SageMaker \n\nConférence présentée par :\n\n\n  Olivier Sutter - AWS Solution Architect\n  Yoann Grondin - IA Team Leader Canal+\n\n\n\n\nCette conférence présentait le produit Amazon SageMaker, d’abord dans sa globalité, puis appliqué au cas des équipes de Data Scientist chez Canal+.\n\nSageMaker a été lancé fin 2017 pour créer, entraîner et déployer des modèles de machine learning. C’est une solution tout-en-un, avec une interface graphique intuitive et un accent porté sur l’automatisation. Amazon promet également de gagner en performance sur SageMaker via l’implémentation de nombreux algorithmes d’apprentissage supervisés ou non-supervisés (XGBoost, kNN, PCA…).\n\nDe manière générale, les équipes de Canal+ utilisent des solutions d’apprentissage pour différents cas d’usages :\n\n\n  Personnaliser l’expérience utilisateur et proposer du contenu ciblé\n  Mieux connaître, labelliser, classifier leurs contenus vidéo\n  Anticiper les besoins des abonnés ou des prospects\n\n\nIls se sont tournés vers SageMaker pour diminuer le temps passé dans les étapes de preprocessing, data cleaning et déploiement en production.\n\nChez Bedrock, nous avons aussi rencontré ces problématiques, nous avons réalisé des PoC de différentes solutions (dont SageMaker) et nous avons retenu la plateforme Databricks.\n\nEn effet, Databricks répond à nos besoins de fine-tuning des paramètres des clusters Spark et d’intégration avec Terraform (ce qui est important pour nous car nous utilisons exclusivement de l’Infra-as-Code). Nous avons également automatisé le déploiement en production de nos modèles d’apprentissage, de la même manière que SageMaker.\n\nCette conférence nous a conforté dans l’approche et l’utilisation que nous avons de nos outils actuels, tout en nous confrontant à d’autres solutions techniques et d’autres cas d’usages au sein de notre industrie.\n\nRésumé par Gabriel FORIEN - DevOps\n\nComment le cloud permet à France Télévisions d’innover dans la diffusion de contenu live \n\nConférence présentée par :\n\n\n  Raphaël Goldwaser - AWS Solution Architect\n  Guillaume Postaire - Directeur de la Media Factory, France Télévisions\n  Matthieu Parmentier - Responsable de l’Al factory, France Télévisions\n  Nicolas Pierre - Al factory Lead Tech, France Télévisions\n\n\n\n\nNous avons assisté à une conférence présentée par les responsables Média et AI de France Télévisions et Raphaël Goldwaser Solutions Architect chez AWS. Ils nous ont parlé de l’évolution de leur usage du cloud dans la diffusion de vidéo en direct et des différentes étapes de la construction d’un système de sous-titrage automatique en direct.\n\nPour ce système de sous-titrage, ils utilisent les services Media &amp;amp; Entertainment fournis par AWS.\n\nLes flux vidéo sont envoyés directement dans le cloud via Elemental MediaConnect pour générer des sous-titres automatiquement en utilisant Media-Cloud AI et Speechmatics. Une fois les fichiers de sous-titres générés, ils sont insérés et synchronisés sur le flux en direct.\n\nCes outils peuvent également être utilisés pour analyser des vidéos afin de contextualiser les publicités affichées et/ou choisir le meilleur moment pour les afficher.\n\nChez Bedrock comme chez France Télévisions, nous challengeons régulièrement les solutions Médias proposées par AWS pour améliorer nos infrastructures et apporter de nouvelles fonctionnalités à nos produits.\n\nRésumé par Christian VAN DER ZWAARD - SysOps\n\nDécouvrez comment Treezor utilise AWS comme moteur de sa plateforme de Banking-as-a-service \n\nConférence présentée par :\n\n\n  Armel Negret - AWS Central Sales Representative\n  Nicolas Bordes - Technical Lead and AWS Sponsor, Société Générale\n\n\nTreezor, une filiale du groupe Société Générale, fournit une plateforme complètement APIsée qui permet aux fintechs et plus généralement aux acteurs de la finance d’accéder à leurs services bancaires. Cette plateforme est hébergée sur AWS et utilise une stack de services entièrement serverless : API Gateway, CloudWatch, Lambda, SNS et SQS entre autres. Les Lambdas sont développées en PHP grâce au framework Bref.\n\nA l’instar de Treezor, Bedrock possède également de nombreuses Lambda développées en PHP avec Bref. Ces Lambdas sont majoritairement déployées via le framework serverless et maintenues par le pôle backend. Au pôle infrastructure, nous essayons d’utiliser d’autres langages comme Python ou Go avec lesquels nous sommes plus à l’aise et qui sont nativement supportés par Lambda.\n\nL’approche “full serverless” est intéressante car elle permet de s’abstraire de la gestion de l’infrastructure sous-jacente et donc de se concentrer sur des problématiques intrinsèques au métier.\nEn sus, les services AWS serverless apportent souvent nativement de la haute disponibilité ainsi que de l’auto-scaling, deux problématiques très importantes pour garantir un service de qualité à nos utilisateurs finaux. C’est pour ces raisons que Bedrock utilise de nombreux services AWS serverless : Athena, CloudWatch, DynamoDB, Lambda, S3, SNS, SQS, Kinesis, …\nLe pôle infrastructure de Bedrock étant relativement “petit” par rapport au nombre total de développeurs (23 devops/sysops pour 250 fullstack en date du 15 avril 2022), l’utilisation du serverless est un réel enjeu business.\nServerless ne répond pas à tous les besoins non plus, particulièrement sur de très forts pics de charge où nous préférons utiliser Kubernetes.\n\nRésumé par Timothée AUFORT - DevOps\n\nTracer votre chemin vers le Modern DevOps en utilisant les services AWS d’apprentissage machine \n\nConférence présentée par :\n\n\n  Patrick Lamplé - AWS Principal Specialist SA\n\n\nLa conférence nous proposait d’en apprendre un peu plus sur les nouveaux produits AWS Code Guru et DevOps Guru.\n\nCode Guru\nse découpe en deux parties :\n\n\n  Reviewer, qui a pour ambition d’accélérer la revue de code ;\n  Profiler, qui peut aider à optimiser les performances d’une application.\nA ce jour, ces services ne supportent que les langages Python et Java.\n\n\nDevOps Guru\npermet d’identifier les comportements anormaux des applications au runtime.\nPar exemple, si une application utilise une table DynamoDB qui n’est pas suffisamment provisionnée, une alerte va être déclenchée. Cette dernière pourrait permettre d’identifier un souci de configuration avant même que l’application ne soit déployée en production.\n\nChez Bedrock, les langages utilisés étant principalement Javascript, PHP et Python, Code Guru ne sera pas une solution adéquate dans toutes les situations. Nous avons donc mis en place la solution KICS qui nous permet, à l’aide de règles Open Policy Agent, d’effectuer automatiquement de nombreuses validations sur le code infrastructure (Terraform, Docker, YAML, …). KICS est utilisé au travers de GitHub Actions pour ajouter des commentaires sur les pull requests comme Code Guru est capable de le faire.\n\nUne analyse au runtime effectuée par DevOps Guru pourrait permettre de venir compléter la liste de services AWS que nous utilisons déjà et qui vérifient la configuration de notre infrastructure comme : Config, Trusted Advisor, CloudWatch,  …\n\nLes outils du Modern DevOps d’AWS pourraient venir en complément d’outils de qualité actuellement utilisés chez nous. À tester en complément de KICS pendant une de nos journées R&amp;amp;D (journées organisées le dernier vendredi du mois, un mois sur deux).\n\nRésumé par Valentin CHABRIER &amp;amp; Mickaël VILLERS - DevOps\n\nInnover plus rapidement en choisissant le bon service de stockage dans le cloud \n\nConférence présentée par :\n\n\n  Thomas Barandon - AWS Enterprise Support Manager\n  Laurent Dirson - Directeur des Solutions Business et des Technologies, Nexity\n\n\nThomas Barandon a rappelé les solutions de stockage d’AWS : S3 pour du stockage objet, EBS pour le stockage bloc et EFS/FSx pour le stockage fichier.\nIl a par la suite présenté Storage Gateway, qui permet d’utiliser les services de stockage AWS dans une infrastructure on-prem via un montage NFS/Samba ou iSCSI.\n\nLaurent Dirson de chez Nexity a ensuite partagé la stratégie adoptée pour concevoir leur SI comme un service. Tous leurs documents sont désormais stockés dans un bucket S3 mis à disposition des agences via un montage NFS opéré par l’outil Storage Gateway. Une politique d’Object Lock permet d’utiliser le modèle WORM (write-once-read-many).\n\nChez Bedrock, nous stockons déjà la grande majorité de nos données dans des buckets S3. Nous aimerions aussi bénéficier des avantages de ce service pour le stockage de nos métriques. Mais, comme nous utilisons VictoriaMetrics, ce mode de stockage n’est pas disponible et ces données sont stockées dans EBS. Peut-être que Storage Gateway nous permettrait d’écrire nos métriques directement sur un bucket S3 ?\n\nRésumé par Coraline PETIT - SysOps\n\nSécuriser vos données et optimiser leurs coûts de stockage avec Amazon S3 \n\nConférence présentée par :\n\n\n  Meriem Belhadj - AWS Storage Specialist Solutions Architect\n\n\nPendant cette présentation, Meriem Belhadj est revenue sur les classes de stockage disponibles sur S3, en mettant une attention particulière à Glacier Instant Retrieval et à Intelligent-Tiering. Le second permet d’appliquer une politique de stockage basée sur la fréquence d’accès aux données au cours des 30 derniers jours.\nEn effet, pour déterminer la “bonne” classe à utiliser, il faut notamment connaître la disponibilité des données. Les autres points à prendre en compte sont la fréquence d’accès, les performances recherchées, la taille des objets à stocker et enfin la durée de rétention.\n\nNous appliquons ces pratiques chez Bedrock depuis plusieurs années. Toutefois, il serait judicieux de mettre en place des règles, type AWS Config, pour s’assurer que ces recommandations soient bien appliquées sur tous nos buckets S3.\n\nRésumé par Coraline PETIT - SysOps\n\nMinimiser vos efforts pour déployer et administrer vos cluster Kubernetes \n\nConférence présentée par :\n\n\n  Abass Safouatou - AWS Lead Solution Architect\n  Sébastien Allamand - AWS Solution Architect Specialist Container\n  Patrick Chatain - CTO Contentsquare\n\n\nContentsquare, analyste de l’expérience numérique, est venu nous parler de son utilisation d’EKS Blueprint avec AWS CDK (Cloud Development Kit) pour la configuration et le déploiement de leurs infrastructures Kubernetes. Cet outil leur a permis de rapidement migrer leurs infrastructures dans le Cloud.\n\nÀ l’occasion de cette conférence, un début de comparatif a été amorcé entre les solutions de passage à l’échelle automatique : Cluster Autoscaler et Karpenter.\n\nCette analyse a particulièrement attiré notre attention : nous souhaitons migrer nos clusters Kubernetes, actuellement déployé par Kops vers des clusters EKS, pour gagner en maintenabilité et en rapidité de scaling. Karpenter est l’une des solutions que nous étudions dans le cadre de ce projet, afin de tirer partie de cet outil qui a été développé par AWS et qui semble mieux tirer profit des fonctionnalités spécifiques d’AWS que Cluster Autoscaler.\n\nContentsquare a mentionné son besoin de développer un outil de passage à l’échelle basé non pas sur la consommation CPU et mémoire mais sur des métriques custom. C’est un besoin que nous avons également chez Bedrock, nous avons donc développé un outil pour y répondre, vous trouverez plus de détails dans l’article de blog dédié à cet outil. Cette remarque nous a conforté dans notre volonté de continuer à open-sourcer les outils que nous développons, pour qu’ils bénéficient à la communauté.\n\nRésumé par Coraline PETIT &amp;amp; Christian VAN DER ZWAARD - SysOps\n\nServerless et évènement, les nouvelles architectures \n\nMainframe, monolithe, système distribué, microservices, la conception d’un SI ou d’un projet est en constante évolution.\n\nCeci dit, depuis plusieurs années, beaucoup d’entreprises font la transition sur des architectures orientées évènements pour limiter les couplages forts entre les microservices.\nÀ cela s’ajoute l’essor du tout Serverless : fini le temps où on gérait nous-mêmes le dimensionnement de nos serveurs.\n\nCette année, plusieurs conférences étaient consacrées à ces sujets.\n\n3 designs patterns pour bien démarrer avec Serverless\n\nMatthieu Napoli, Hero AWS Serverless et créateur de la librairie Bref, a présenté trois designs patterns pour bien démarrer avec Serverless.\n\nApplication HTTP\n\nIl est maintenant très simple et peu coûteux de créer des applications HTTP en Serverless, en combinant différents services AWS :\n\n\n  Lambda function et Lambda function URL (nouvelle fonctionnalité sortie une semaine avant le Summit) ;\n  CloudFront CDN ;\n  API Gateway ;\n  S3.\n\n\nUn exemple concret :\n\n\n  CloudFront CDN délivre les assets JS/CSS/image depuis S3 ;\n  il transmet également les retours d’API Gateway ;\n  et API Gateway communique avec la/les Lambdas.\n\n\nAjoutons une base de données DynamoDB ou Aurora et nous voilà avec une application full Serverless.\n\nFile de messages avec worker\n\nLorsqu’on met en place ce type de pattern, nous déployons :\n\n\n  un projet qui pousse un message dans une file “producer” ;\n  une file de messages (SQS/SNS/MQ) ;\n  un second projet qui lit les messages depuis la file “consumer”.\n\n\nDans cette architecture, le “consumer” se connecte à la file de messages, lit les messages et se charge de toute la gestion d’erreur et de retry.\n\nEn utilisant une Lambda comme “consumer”, AWS a mis en place une intégration spécifique entre les files de messages et les Lambdas. C’est maintenant la file de messages qui appelle directement la Lambda en lui donnant le message et qui gère également le retry : votre code applicatif est déchargé d’autant de responsabilités sans valeur métier.\n\nCommunication entre microservices\n\nQuoi de plus contraignant que de gérer la communication de plusieurs services ? Il faut gérer :\n\n\n  les erreurs : que faire si plusieurs microservices partent en timeout ou échouent dans un workflow ?\n  le couplage : lors de la création d’un nouveau microservice, il doit être lui aussi appelé dans les chaines d’appels ;\n  l’authentification entre les différents services ;\n  et la latence : les appels de services en cascade augmentent la durée totale d’exécution.\n\n\nDe ce constat, Matthieu propose une solution que nous avons déjà mise en place chez Bedrock depuis plusieurs années : communiquer avec des évènements.\nPour cela, AWS fournit EventBridge : un service serverless de routage d’évènements sans stockage.\nAinsi, si un microservice doit en informer d’autres, il lui suffit d’envoyer un événement dans EventBridge. Les autres services n’auront qu’à “écouter” l’événement.\nSNS, plus ancien, permettait la même approche, mais EventBridge propose de créer des règles de filtrage sur la totalité du message d’un événement.\n\nConstruire des applications serverless orientées événements\n\nNicolas Moutschen, Solution Architect AWS et Guillaume Lannebere de chez Betclic ont fait un retour d’expérience sur la mise en place de différents services serverless AWS orientés événements.\n\nBetclic absorbe à chaque match/course, une quantité énorme de données (plusieurs millions d’événements) en quelques minutes.\nPar exemple, lors d’un match de football, les paris sont effectués à tout instant : avant le match, à la mi-temps, dans les dernières minutes…\nLeur SI est donc soumis, fréquemment, à de très forts pics de charge pendant des laps de temps très courts.\n\nAfin d’éviter de provisionner énormément de machines pour se mettre à l’échelle, Betclic à fait le choix du full serverless. Les applications de paris et de paiement communiquent par des messages d’événements envoyés dans le service AWS SNS : les Lambdas reçoivent les messages et les traitent avec une mise à l’échelle quasi immédiate en fonction du trafic.\n\nRésumé par Fabien LALANNE - Développeur\n\nNous étions aussi intervenants \n\nNous aimons tout particulièrement apprendre en lisant des articles écrits par d’autres membres de notre communauté ou en assistant à des conférences présentées par d’autres clients. Il est donc normal et important pour nous, de partager aussi notre expérience, ce que nous faisons régulièrement, y compris sur ce blog.\n\nCette année, nous avons eu la chance d’intervenir et de partager avec notre communauté lors de trois conférences. Merci à AWS pour la confiance qui nous a été accordée !\n\nTransformer le load balancing pour optimiser le cache : objectif 50 millions d’utilisateurs \n\nVincent Gallissot @vgallissot, Lead Cloud Architect, a expliqué comment Bedrock a amélioré le Load Balancing chez AWS, pour optimiser le cache de sa diffusion de vidéos, avec comme objectif 50 million d’utilisateurs :\n\nDémarrage des conférences à l’#AWSSummit. Et voici un REX intéressant pour partager l’une des problématiques intéressantes pic.twitter.com/kuKGZZyE2A&amp;mdash; Akram BLOUZA (@akram_Blouza) April 12, 2022\n\n\nGuillaume Marchand, Senior Solutions Architect chez AWS a débuté notre talk en parlant de Load Balancing chez AWS, des différentes solutions et des bonnes pratiques, ainsi que des exemples d’architectures possibles. J’ai ensuite expliqué notre besoin de scaler des serveurs de cache et comment nous avons relevé ce challenge, en développant notamment Haproxy Service Discovery Orchestrator. Ce talk n’a pas été enregistré, mais les slides sont disponibles sur ce lien.\n\nEtes-vous bien architecturé ? \n\nPascal Martin @pascal_martin, Principal Engineer, est intervenu pour partager notre retour d’expérience client pendant une conférence de présentation du Well-Architected Framework :\n\nPour cette conférence, Rémi Retureau, Partner Management SA Lead chez AWS, a commencé par présenter les pratiques Well-Architected. Un ensemble de recommandations basées sur 10 ans d’expertise de Solutions Architects AWS.\nJe suis ensuite intervenu pour partager un retour d’expérience : comment nous utilisons Well-Architected Framework chez Bedrock pour nous aider à valider l’architecture de composants de notre plateforme, à prioriser des évolutions ou même, à en identifier de nouvelles.\nEn quelques mots : nous passons une revue Well-Architected une fois par an et, si nous ne nous posons pas explicitement l’ensemble des questions du Framework à chaque nouveau projet, nous l’intégrons de plus en plus à nos pratiques et habitudes.\nSi vous commencez à travailler sur AWS, le Well-Architected Framework et ses recommandations, bien que peut-être effrayantes au premier abord, sont un ensemble de bonnes pratiques qui vous aideront à concevoir et à construire une plateforme plus solide, plus résiliente et moins coûteuse.\n\nPréparez et donnez votre premier talk \n\nPascal est aussi intervenu, cette fois en tant qu’AWS Hero pour guider la préparation de vos talks :\n\nPour cette seconde intervention, j’ai choisi de parler d’un sujet qui n’est pas lié à AWS.\nJ’aime assister à des conférences : je le fais depuis très longtemps et j’apprends beaucoup ainsi.\nJe suis aussi toujours très content de voir d’autres speakers monter sur scène et partager leur expérience. Je sais que beaucoup de personnes, dans notre communauté, ont des connaissances et des idées géniales et j’aimerais qu’elles les partagent plus souvent !\n\nJe sais toutefois que cet exercice est effrayant et que se lancer sur scène pour la première fois est difficile. J’espérais donc, à travers ce talk déjà donné chez Bedrock lors d’un Last Friday Talks (une journée de conférences internes le dernier vendredi du mois, un mois sur deux) aider de nouvelles personnes à se lancer.\nL’idée vous intéresse mais vous n’avez pas pu assister à cette conférence ? Et bien, j’ai aussi écrit un livre pour vous accompagner : « Préparez et donnez votre première conférence (quand ce n’est pas votre métier) »\nEt j’ai hâte, l’année prochaine, de vous voir monter sur scène et partager avec notre communauté !\n\nConclusion de l’article \n\nAvec des milliers de participants et participantes, l’AWS Summit est toujours une excellente occasion d’échanger et d’apprendre. Nous étions également très heureux de pouvoir, cette année encore, partager notre expérience lors de trois interventions.\nCet événement était aussi le premier pour certains et certaines d’entre nous, une très bonne découverte !\n\nComme beaucoup d’autres speakers et entreprises rencontrés mardi, nous recrutons : des SysOps, des DevOps, des développeurs et des développeuses, une ou un FinOps. Vous voulez nous aider à construire et à faire grandir notre plateforme ? Nous avons encore de super projets et challenges, faites-nous signe !\n"
} ,
  
  {
    "title"    : "Comment faire un trailer vidéo qui déchire avec des technos web ?",
    "category" : "",
    "tags"     : " remotion, react, video, js, frontend, conference, lyonjs",
    "url"      : "/2022/04/04/comment-faire-un-trailer-qui-dechire-avec-des-technos-web.html",
    "date"     : "April 4, 2022",
    "excerpt"  : "Avec Antoine Caron on est allé mettre des paillettes dans les yeux des participants du LyonJS en leur montrant comment créer des vidéos avec des technos web ! ✨\n\nIl était une fois … 📖\n\nUn jour, alors que j’arrivais fraichement à Bedrock, j’ai eu l...",
  "content"  : "Avec Antoine Caron on est allé mettre des paillettes dans les yeux des participants du LyonJS en leur montrant comment créer des vidéos avec des technos web ! ✨\n\nIl était une fois … 📖\n\nUn jour, alors que j’arrivais fraichement à Bedrock, j’ai eu le malheur de demander à Antoine Caron ce sur quoi il bossait entre midi et deux et qui semblait fort l’amuser.\n\nSa réponse : “J’essaie de générer des vidéos en MP4 à partir de composants React, tu veux voir ?”\n\nAprès des heures à tester chaque fonctionnalité de Remotion, il était temps de présenter ça à la communauté Javascript de Lyon lors du meetup n°71 du Lyon JS ! 🦁\n\n\n  \n\n\n\nPas le temps de regarder le replay ? ⏱\n\nPour vous donner une petite idée de ce que l’on a fait, on vous partage un site qui génère dynamiquement des trailers vidéo en fonction d’un programme et d’une couleur ! 🤯\n\n\n  \n\n\n\nℹ️ On vous conseille quand même de regarder le replay, même le créateur de Remotion a aimé 😉\n\n\n  A demo of Remotion in French at @LyonJS!Thanks for organizing this awesome talk @Slashgear_ @CruuzAzul 😃https://t.co/xujfC7tR6e&amp;mdash; Remotion (@remotion_dev) April 3, 2022 \n\n\n\nOne more thing… Petite surprise du chef ! 👨🏻‍🍳\n\nVoilà un petit aperçu d’une vidéo surprise que l’on a fait grâce à Remotion uniquement avec des composants React ! (Si des gens sont nés avant 2000, ça doit vous rappeler quelque chose 😉)\n\n\n  \n\n\n\nVous trouvez ça incroyable et vous voulez essayer ? N’hésitez pas à venir nous montrer vos vidéos, ou directement sur twitter avec @Slashgear_ et @CruuzAzul 🎞\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #16",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2022/03/26/bedrock-dev-facts-16.html",
    "date"     : "March 26, 2022",
    "excerpt"  : "Sur ce début 2022 les équipes de Bedrock se sont lachées ça promet pour le reste de l’année.\nLes autres articles de cette série sont disponibles ici.\n\nPromis !\n\n\n\nL’arbre qui cache la forêt\n\n\n  Quand c’est la couleur des noms de fichiers qui te pe...",
  "content"  : "Sur ce début 2022 les équipes de Bedrock se sont lachées ça promet pour le reste de l’année.\nLes autres articles de cette série sont disponibles ici.\n\nPromis !\n\n\n\nL’arbre qui cache la forêt\n\n\n  Quand c’est la couleur des noms de fichiers qui te permet de te repérer dans l’arborescence du projet…\n\n\n\n\nLa boucle a bouclé\n\nLe débat est et sera éternel.\n\n\n  A: La meilleure extension de VSCode c’est d’installer Webstorm.\n\n  B: Et la meilleure extension de Webstorm c’est d’installer Phpstorm.\n\n  C: Et la meilleure extension de Phpstorm c’est d’installer vim.\n\n  D: La meilleure extension de Vim c’est d’installer Neovim.\n\n  E: La meilleure extension de neovim est le bloc-note.\n\n  F: Mon bloc-note c’est VSCode.\n\n\nQuand on va à la pêche à l’exception\n\nException message (1) :\n------------------------\nAttempted to load class &quot;TruiteException&quot; from namespace &quot;Bedrock\\Stores\\Infra\\HttpClient&quot;.\nDid you forget a &quot;use&quot; statement for another namespace?\n\n\n“Ce n’est pas un échec… ça n’a pas marché”\n\n\n  Y’a pas de bug, y’a juste un truc qui fonctionne pas\n\n\nLes bottleneck c’est la vie\n\n\n  La vie est un bottleneck\n\n\nAtlas, le géant ?\n\n\n  Qui c’est qui a déplacé la terre ?\n\n\nTranquilou bilou\n\n\n  Le pair programming n’est pas du programming pépère\n\n\nTester c’est …\n\n\n  A: Faire des tests, c’est quand même moins chiant à faire à deux\n\n  B: Ouais, comme le sexe\n\n\nLabyrinthe\n\n\n  Pourquoi vous vous faites chier à mettre les secrets dans SOPS. \nVous l’auriez mis dans votre doc, personne ne les aurait trouvé.\n\n\nVoilà qui voilà !\n\n\n  A: Je crois que la branche master elle est pas protégée.\n\n  …\n\n  Ah bah heureusement que je suis là pour tester\n\n  B: Le plus étonnant c’est que ‘A’ ait le droit de push non ?\n\n  A: Le plus étonnant c’est que j’ai quelque chose à push non ?\n\n  …\n\n  On a résolu une faille de sécurité sur le projet, grâce à mon investigation.\n\n  C: Le mec c’est inspecteur gadget en fait, on sait pas comment mais il finit par être utile.\n\n\n🕯\n\n\n  The Domain is a sanctuary\n\n\nQuand il y a trop de viennoiserie chez Bedrock\n\n\n  A: Fait gaffes tu es en ordre croissant, l’ordre pain au chocolat est bien meilleur\n\n\nMais oui c’est clair !\n\n\n  On complexifie pour faire plus simple !\n\n\nLa PRO-crastination\n\n\n  Lead: C’était qui le chargé du monitoring hier ?\n\n  A: C’était moi mais j’ai rien fait, mais si vous voulez je peux le refaire aujourd’hui.\n\n  B: Si tu veux je peux même t’aider à rien refaire.\n\n\nÇa sent un peu la poussière ici\n\n\n  A: Eh eh, là on a effectivement sur-gonflé l’estim … (mais le côté pas dans l’inconnu l’explique)\n\n  B: Ouais c’était carrément la peur de toucher à un projet legacy de chez legacy, qu’on ne connait pas, pas dans notre scope…\n\n  A: Mais pas de souci majeur pour un développeur qui pratiquait Symfony sous Pompidou (le code date de cette époque)\n\n\nUn nouveau mot\n\n\n  Hybriditance\n\n\nDouble hit combo\n\n\n  “I want to identify pages where users hit”, le developper traduit alors “Je veux identifier la page où l’utilisateur se frappe”\n\n\nUne histoire de dev enrhumé\n\n\n\nQui s’appeleriot le CPU\n\n\n  Il fait des soustractions, des additions… il fait des trucs de fous\n\n\nUn nouveau KPI\n\n\n  Alors mon piffomètre du sprint…\n\n\nC’est simple non ?\n\n\n  I talked to A that said that B said that C said that it is possible to parallelize steps on Jenkins.\n\n  So I asked B how to do that, saying that A said that he said that C said that it was possible to do it!\n\n"
} ,
  
  {
    "title"    : "How AWS Cloudfront is helping us deliver our Web streaming platform? - Part 1",
    "category" : "",
    "tags"     : " cloudfront, aws, cdn, node.js, react, javascript, frontend",
    "url"      : "/2022/03/08/cloudfront-web-streaming-platform-part-1.html",
    "date"     : "March 8, 2022",
    "excerpt"  : "A bit of context\n\nThe web is a major platform for the distribution of our customers’ content at Bedrock.\nMillions of users connect every month to watch their live, replay or the series and movies of their choice.\nThe broadcasting of sports events ...",
  "content"  : "A bit of context\n\nThe web is a major platform for the distribution of our customers’ content at Bedrock.\nMillions of users connect every month to watch their live, replay or the series and movies of their choice.\nThe broadcasting of sports events such as the Euro 2020 soccer tournament represents a real technical challenge when it comes to maintaining the stability and performance of such a platform.\n\nThe web application works in SSR (Server Side Rendering) mode: we have NodeJS Express servers returning pre-rendered HTML pages.\nWe made this choice several years ago, for two reasons: SEO and to improve the first display time on slow devices.\nIn addition to the HTML pages, the web platform is also a huge collection of assets that allow the website to function: Javascript bundles, CSS, images, manifests.\n\nToday our customers have users distributed over a large part of the globe.\n\nTo meet these challenges, a CDN is a very good solution.\n\nWhat is a CDN then ?\n\nCDN for Content Delivery Network is a service delivering content to users across the internet (here our HTML pages and assets).\nWherever the user is in the world.\nTo all users, even if they are many.\n\nCloudfront is the CDN service of AWS.\nWith a large number of POPs (Point Of Presence) around the world, it helps us provide a response to each user as close as possible to their location.\nThis allows us to significantly reduce the time to first byte of our responses.\nDifferent price classes allow you to choose the global “area” in which your application should be available in order to achieve savings.\n\n\n\nBeing in Lyon (France), we sometimes get answers from the POP of Milan (Italia).\nIndeed, Lyon ↔ Milan is almost as closer as Lyon ↔ Paris.\n\nNote that it is very easy to know which Cloudfront POP answered you.\nEach POP is identified by a three letter code that corresponds to the code of the nearest international airport (here: CDG corresponds to Paris Charles de Gaulle airport).\n\nx-amz-cf-pop: CDG50-C1\n\n\nDelivering content as close to the user as possible is great, it theoretically reduces waiting time but it does not solve the problem of heavy load.\n\nThe best solution for load problems is caching.\n\n\n  You put 1 second of cache-control, and you already won!\n\n  Y. Verry, our Head of Infrastructure and Ops\n\n\nCloudfront service makes it easy to cache responses at the edge servers.\nIf we take the example of sports broadcasting, users arrive in large numbers in a very short period of time.\nCaching (telling Cloudfront to cache a web page) takes a lot of the load off our Node servers because they are not called.\n\nCaching objects in Cloudfront is also about improving response times.\nNo need to wait for our servers, the user receives the cached object directly.\nCloudfront even takes advantage of this to apply more powerful compression algorithms like Brotli on these cached objects.\nThese compressions, performed directly by the CDN, allow you to drastically reduce the size of your objects on the network.\nReducing objects size make our applications load even faster for our users.\n\nHere is our Cache hit ratio in production on 6play.fr website.\n\n\n\nCloudfront also allows us to do “Edge computing”: run code directly in Amazon edges and POPs instead of doing it in our applications.\n\nLambda at edge (on regional edges servers), Cloudfront function (function that runs on POP servers), Web Application Firewall, here are some very cool features that will allow you to do usual manipulations on your requests/responses.\n\nFinally, by using regional Pop, hundreds of end server edges do not contact your origin (your application) when the cache is invalidated or exceeded.\nYou can even activate the Origin Shield feature that allows you to further limit the load on your origins.\n\n\n\nGood per-level cache management even allowed us to completely invalidate the cache of a Cloudfront distribution a few minutes before the start of an event without generating huge traffic on our servers.\n\n\n\nAnd that’s it for this first article, in the next part (and normally the last one) you will discover how we have implemented some patterns on our sites.\n\nIn the meantime, feel free to have a look at other articles available on this blog:\n\n\n  More efficient Load Balancing and Caching at AWS, using Consistent Hashing and HAProxy\n  Scaling Bedrock video delivery to 50 million users\n\n"
} ,
  
  {
    "title"    : "Streaming recommendations at Bedrock",
    "category" : "",
    "tags"     : " recommender systems, machine learning, data, data science",
    "url"      : "/2022/02/27/streaming-recommendation.html",
    "date"     : "February 27, 2022",
    "excerpt"  : "Personalised recommendations are everywhere. No exception for the streaming world. To improve user experience, recommender systems with machine learning are uplifting.\nAt Bedrock, until recently, there was no recommendation shaped this way.\n\nBut w...",
  "content"  : "Personalised recommendations are everywhere. No exception for the streaming world. To improve user experience, recommender systems with machine learning are uplifting.\nAt Bedrock, until recently, there was no recommendation shaped this way.\n\nBut we are writing a new story.\n\nA quick win solution\n\nWe wanted to find a way to get a solution that would be quick to integrate.\n\nWe chose to use Amazon Personalize. This service aims to construct recommender systems with machine learning. The promise is to Create real-time personalized user experiences faster at scale. Perfect! It was exactly what we were looking for.\n\nRapidly, we encountered an obstacle. You can’t deploy Personalize with Terraform. Yet, Terraform is the tool we use to manage our infrastructure.\n\nHow to deploy Amazon Personalize?\n\nAs Personalize is supposed to be a temporary solution in our stack, for once, we accepted not using Terraform. We developed a Python script to interact with Personalize. Apache Airflow schedules and monitors the script.\n\nPersonalize is a black box. You can’t have access to explanations about the generated models. But, with Personalize, you have different ways to evaluate your recommendations.\n\nWith recommender systems, using the offline metrics to judge your model is not enough. It’s better than nothing! But to check that a recommender system works, you need to evaluate it online with real users.\n\nWe have millions of users. Releasing a recommender system to all our users is definitely not the best idea ever.\n\nHow to release a recommender system?\n\nFirst of all, check offline metrics. They’re still a valuable hint. Then, analyse the recommendations with people from the editorialist team.\n\nNote that this kind of analysis is very subjective.\n\nFinally, deliver the functionality to a small portion of your users.\n\nWe configured an AB test that gives recommendations to 5% of users. With dashboards, we study the impacts.\n\nPerfect! We have a way to check the success of a full broadcast.\n\nBut how to be sure that the new product will support the load? We have millions of users. It means that 5% of users still represent a lot of people.\n\nHow to assess the performance of a recommender system?\n\nLaunch load tests. Today you have a myriad of tools to do that. At Bedrock, we use Artillery.\n\nDuring the load tests, we had a bug. We discovered that by default, the limit of requests per second with Personalize is 500. In our context, that’s not acceptable.\n\nWe asked Amazon to help us and they changed the option for us.\n\nThe results\n\nIf you’re a big fan of reality TV shows, you will see that:\n\n\n\nIf you prefer reports, you will see that instead:\n\n\n\nWhat now?\n\nWe’ve deployed an AB test for our first recommender system built with machine learning.\n\nWe don’t have the results of the AB test yet. But, we’ve noticed that many users interact with the recommendations.\n\nAfter different challenges, we nailed it.\n\nBut, Personalize is expensive and a black box that we can’t integrate with Terraform easily (we’ll have to develop something for that, at least). It doesn’t suit our context. That’s why we’ve started to develop our first models.\n"
} ,
  
  {
    "title"    : "Tonight&#39;s football time, let&#39;s prescale Kubernetes to avoid a crash!",
    "category" : "",
    "tags"     : " kubernetes, scaling, high availability, aws, cloud",
    "url"      : "/2022/02/03/prescaling.html",
    "date"     : "February 3, 2022",
    "excerpt"  : "Are you experiencing peak loads on your Kubernetes-hosted platform? Rest assured, you are not alone.\nAt Bedrock, we have developed a prescaling solution. It allows us to handle sudden and abrupt, but predictable, \ntraffic spikes, like soccer games...",
  "content"  : "Are you experiencing peak loads on your Kubernetes-hosted platform? Rest assured, you are not alone.\nAt Bedrock, we have developed a prescaling solution. It allows us to handle sudden and abrupt, but predictable, \ntraffic spikes, like soccer games.\n\nKubernetes provides HorizontalPodAutoscalers to handle traffic variations. \nWe’ll look at their limitations in the case of meteoric spikes in load and how prescaling helps us deal with the \nsudden arrival of several hundred thousand users.\n\nTable of Contents\n\n\n  Load and traffic vary\n  Beginning of the scaling problems\n  How does reactive scaling work in Kubernetes?\n    \n      An HorizontalPodAutoscaler\n      What scale out looks like in a real case\n      How fast is reactive scaling?\n    \n  \n  Prescaling… What is this about?\n  Prescaling our applications\n    \n      Enabling and configuring prescaling on an HPA\n      The prescaling exporter\n      How can the HPAs prescale?\n      Prescaling works!\n    \n  \n  What about special, huge, events?\n    \n      The prescaling API\n      Let’s see how an application scales during a very special event\n    \n  \n  Prescaling external services: another challenge\n\n\nLoad and traffic vary\n\nLoad and traffic have always varied over time on our platform:\n\n\nCPU per instance over time\n\nTo deal with these load variations, several tools help us to automatically adapt our Kubernetes clusters’ capacity:\n\n  HorizontalPodAutoscaler \n(HPA): adds/removes Pods (= capacity) on a workload resource such as Deployment or a \nStatefulSet.\n  Cluster Autoscaler: \nadjusts the size of a Kubernetes cluster by adding/removing nodes.\n  Overprovisioning: starts “empty” \npods (and new “useless” nodes, as a consequence), so the cluster has available capacity that will be used to start \napplications pods quicker.\n\n\n\n  If you wish to know more about which tools we use and why we use them in our Kubernetes clusters, I advise you to \ncheck out another dedicated blog post named \n“Three years running Kubernetes on production at Bedrock”.\n\n\nUnfortunately, all those tools are not sufficient to deal with heavy and sudden traffic spikes on some special \nevenings such as the final of a football game or a successful show. During this kind of events, users arrive massively, \nall at the same time. On some evenings, some of our applications see their load rise by 5 in 2 minutes, others even \nsee theirs multiplied by 10 in 2 minutes!\n\nThe predictable aspect of those arrivals is very important because it means we are able to prepare our platform \nbeforehand. That’s why our prescaling solution was born.\n\nBeginning of the scaling problems\n\nOn an ordinary evening, when the multiple Kubernetes scaling tools were kicking in, here is basically what was \nhappening:\n\n\n\nAs the load increased, our capacity was increasing as well and we always had spare capacity. We were able to double our \ninitial capacity every 5 minutes thanks to reactive scaling when load started to rise.\n\nAfter a while and on some special evenings, we began to see this kind of behavior:\n\n\n\nSometimes, load was increasing faster than what reactive scaling could handle, that is to say about x2 in capacity \nevery 5 minutes. The consequence is that we could not serve everyone. For a while, our platform would fail for some \nusers, until autoscaling kicked in or until load stopped rising so fast.\n\nLet’s see how reactive scaling works to understand how we can leverage it to prepare the platform in advance.\n\nHow does reactive scaling work in Kubernetes?\n\nAn HorizontalPodAutoscaler\n\nFor a Deployment to be autoscaled (reactively, according to varying load), the number of replicas of a Kubernetes \nDeployment is reconfigured by an HorizontalPodAutoscaler:\n\n\n\nIts manifest usually looks like this:\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\n# …\nspec:\n  # …\n  minReplicas: 2\n  maxReplicas: 100\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 70\n  - type: Pods\n    pods:\n      metricName: phpfpm_active_process_percentage\n      targetAverageValue: &quot;40&quot;\n  # …\n\n\nHere:\n\n  minReplicas is the minimum number of Pods that will run (under normal conditions).\n  maxReplicas is the maximum number of Pods that will run.\n  metrics is a list of metrics the HPA will analyze to determine if it must scale or not. If usage gets higher \nthan the target specified for a metric, the HPA will add new Pods (= scale out). If usage gets lower than all targets \nspecified for all metrics, the HPA will remove Pods (= scale in).\n\n\nWhat scale out looks like in a real case\n\nAt Bedrock, we use three types of metrics to scale: Resource, Custom and External. Here is an example of \nan application scaling out on a Custom metric:\n\n\nProcess status (%) over time\n\nIn this graph, around 20:52:30 and 20:55:00, the percentage of active processes rises to go above \nthe target 40 we saw in the YAML example before. This triggers the scale out of the HPA of this application:\n\n\nPod status over time\n\nWe can see that around 20:53:00 (30 seconds after we first go above the target), the number of unavailable pods rises. \nThe scale-out has just started. A few moments after, the pods become available and are able to serve users.\n\nHow fast is reactive scaling?\n\nScaling in Kubernetes with HPA is not instantaneous:\n\n\n\n\n  Metrics are updated every 30 seconds (for CPU or memory) or every 60 seconds (for external and custom metrics) or so.\n  HPA analyses the metrics to know if scale-out is required every 30 seconds or more.\n  We might not have enough EC2 servers running to host the new Pods that are trying to start (we usually have between \n10% and 20% of spare capacity). If starting new EC2 servers is necessary, this takes between 2 and 4 minutes.\n  Pods (as our applications and Docker images are big) usually need between 30 and 60 seconds to start.\n\n\nThis means when a spike in load occurs, we need between 1’00’’ and 6’00’’ for new Pods to be able to handle that load. \nScaling is clearly not instantaneous.\n\nPrescaling… What is this about?\n\nMost our applications deployed in Kubernetes use an HorizontalPodAutoscaler. As we’ve seen, this autoscaling \nmechanism is reactive in nature: Pods are added when load (or another metric) gets higher than a target. \nIt can handle load that increases slowly (say, instant +20% or x2 in 5 minutes), but not huge instantaneous spikes \n(say, x10 in less than 5 minutes). It works fine for most of our usual workloads, but cannot absorb spikes we receive \nduring special events like Top Chef or when the France football team plays.\n\nThe only way we can handle a huge and sudden spike in traffic on an application is by pre-provisioning capacity. \nIn Kubernetes, this is done by running more Pods than necessary so that they are ready to handle the additional load. \nRunning additional, mostly superfluous, capacity has a cost…\n\nWe know our applications receive a sudden and brutal traffic spike once a day, between 20:50 and 21:00 Paris time. \nNot taking special events into account, that’s the only time load increases violently enough for reactive autoscaling \nto be unable to handle it – and, most days, it actually does quite fine. So, we could pre-provision more capacity \naround that time (only), and not pay for it the rest of the day…\n\nPrescaling our applications\n\nEnabling and configuring prescaling on an HPA\n\nTo enable and configure prescaling on an HorizontalPodAutoscaler, we add two sets of information:\n\n  Three annotations to define:\n    \n      Time when prescaling starts.\n      Time when prescaling stops.\n      The minimum number of Pods we want between those two times.\n    \n  \n  A new metric to scale on.\n\n\nThe annotations are set in the metadata block:\n# …\nmetadata:\n  # …\n  annotations:\n    annotations.scaling.exporter.replica.min: &quot;25&quot;\n    annotations.scaling.exporter.time.start: &quot;19:30:00&quot;\n    annotations.scaling.exporter.time.end: &quot;23:30:00&quot;\nspec:\n  scaleTargetRef:\n    # …\n    name: &quot;service-6play-images&quot;\n\n\nIn this example, we indicate we want at least 25 pods between 19:30:00 and 23:30:00. As a result, \nthe number of pods of this application will rise up to at least 25 pods during this period of time.\n\nTimes are expressed in the local timezone of the Kubernetes cluster as prescaling is linked to events on the platform. \nThose events are usually linked to events on live TV and we deploy one cluster per TV broadcaster.\n\nIn the metrics block, you need to configure a new External metric:\n# …\nmetrics:\n # …\n - type: External\n   external:\n     metricName: &quot;annotation_scaling_min_replica&quot;\n     metricSelector:\n       matchLabels:\n         deployment: &quot;service-6play-images&quot;\n     targetValue: &quot;10&quot;\n\n\nThe label used for deployment must be set to the value of scaleTargetRef.name (= the name of the Deployment the \nHPA reconfigures). The targetValue must always be set to 10.\n\nYou now know how we configure an HPA for prescaling. What you do not know yet is how the HPA annotations are used \nand which application exposes the metrics called annotation_scaling_min_replica. It’s now time to talk about the \nprescaling exporter.\n\nThe prescaling exporter\n\nThe prescaling exporter is a Prometheus exporter we developed (in Python). \nIt exposes metrics, used to scale Kubernetes applications on a day-to-day basis during a given time range.\n\n\n  Prometheus is one of the tools we use in our monitoring stack at Bedrock. One of its purposes, among others, is to \ncollect metrics from Pods. This article will not present our Prometheus stack in detail.\n\n\nHere is how the prescaling exporter works:\n\n\n\n\n  Every 15 seconds or so, Prometheus scrapes the prescaling exporter pod to get the metrics it exposes.\n  Scrapping triggers the generation of the metrics. Before they are exposed, the exporter first \ncalls the k8s API to list all HPAs in the cluster.\n  Then, it will:\n    \n      Filter those HPA with the required annotations (the annotations we added a bit earlier on an HPA).\n      Calculate the new annotation_scaling_min_replica metric for each HPA with the prescaling annotations.\n      The prescaling exporter can now expose the metrics.\n    \n  \n  And Prometheus can retrieve them.\n\n\nHow does the prescaling exporter calculate the metrics of the HPA subscribed to the prescaling? Well, it depends \non the content of the annotations you configured on the HPA.\n\nHere is how a Prometheus metric of the prescaling exporter looks like:\n\n\n\nIn each metric, you will find several labels. I chose to put only one here to simplify.\nThe metric can take one of three values, depending on the content of the annotations we saw earlier:\n\n  When we are not within the time range of the prescaling annotations of the HPA, it means that we do not \nneed to prescale. As a result, the metric is set to 0.\n  If we are within the time range of the prescaling annotations, it means we are in the prescaling time range. \nFrom there, two possibilities:\n    \n      If “Current number of replicas” &amp;lt; “Number of minimum replicas in the HPA annotation”, we need to add replicas so \nthe metric is set to 11.\n      If “Current number of replicas” &amp;gt;= “Number of minimum replicas in the HPA annotation”, we have enough replicas so \nthe metric is set to 10.\n    \n  \n\n\n\n  When the metric is set to 10 and we already have enough replicas running, the number of replicas will never go below \nthe minimum chosen in the prescaling annotation annotations.scaling.exporter.replica.min.\n\n\nHere is what it looks like on Grafana for the application service-6play-images during the evening:\n\n\nannotation_scaling_min_replica over time\n\nIn this example:\n\n  Until 19:30, annotation_scaling_min_replica is set to 0.\n  From 19:30 until about 19:35, annotation_scaling_min_replica is set to 11 (scale-out will happen).\n  From 19:35 until 00:00, annotation_scaling_min_replica is set to 10 (scale-out is done, we have enough Pods).\n  From 00:00 until the following evening, annotation_scaling_min_replica is set to 0 again.\n\n\nWe can guess two things from this example:\n\n  The prescaling period for this application was 19:30 until 00:00.\n  It took about 5 minutes to prescale (= add more Pods) the application.\n\n\nHow can the HPAs prescale?\n\nTo understand how the HPAs can prescale, we need to talk about \nthe prometheus adapter. It is an implementation \nof the Kubernetes metrics APIs. We use it to expose custom and external metrics for HPAs to use in order to scale:\n\n\n\n\n  Prometheus adapter collects metrics from prometheus once every minute and exposes them as External and\nCustom metrics.\n  The HPA controller manager fetches metrics provided by metrics-server and prometheus-adapter to scale out or \nscale in Deployment/Replica Set/Stateful Set resources. To do so, it uses k8s aggregated \nAPIs (metrics.k8s.io, custom.metrics.k8s.io and external.metrics.k8s.io). \nMetrics-server provides resource metrics (only CPU and memory). \nPrometheus adapter provides all non-resource metrics (external and custom).\n\n\nFor more information: HPA Kubernetes documentation.\n\nPrescaling works!\n\nAfter prescaling has been deployed into production and teams started to add annotations in their projects, \nthis is what happened:\n\n\nNumber of pods regarding their status over time\n\nAround 19:30, the number of pods for this application goes from 25 to a bit more than 55. It means its HPA was \nscaled out, based on the prescaling metric of that application. Mission accomplished!\n\nWhat about special, huge, events?\n\nSome days, during very special events, “normal” prescaling was not enough to handle the load that was rising \nway faster than what we usually see on our platform:\n\n\n\nAs you can see, even with prescaling doing its job before the start of the TV program (we can see capacity \nrising at the beginning of the graph), the traffic rises so quickly at 20:55 that we are still unable to scale fast \nenough to serve all users.\n\nFor these special events, we have developed an additional mechanism that allows us to set a multiplication coefficient \nto all prescaling. We use it to say “I want a 5x higher minimum number of Pods than what’s configured in the annotation \nwe’ve seen before, for all HPAs bearing this annotation”.\n\nTo deal with those very special events, we added another component in the prescaling stack: the prescaling API.\n\nThe prescaling API\n\nThe prescaling API is a backend application also developed in Python. It was designed to store prescaling \nsettings for future events on the platform in AWS DynamoDB. We chose DynamoDB because it’s a serverless database \neasily maintainable through Terraform code. By “event”, understand a football game or another big show such as \nTop Chef. Those settings define when and how we must enlarge the platform to sustain bigger \ntraffic spikes than on standard days (= on normal prescaling evenings).\n\n\n\nWith this API, our prescaling workflow has evolved:\n\n\n  Prometheus scrapes the prescaling exporter pod, same as before.\n  Scrapping triggers the exposition of the Prometheus metrics. Before the metrics are exposed, the exporter first \ncalls the Prescaling API server to get the current special event if there is one.\n  After calling the prescaling API, the exporter calls the k8s API (as before) to list all HPAs in the cluster.\n  Then, it:\n    \n      Filters those HPA with the required annotations.\n      Calculates the new annotation_scaling_min_replica metrics by merging information from the HPA annotations and \nthe special event from the prescaling-api server.\n    \n  \n  The prescaling exporter now exposes the metrics so that Prometheus can retrieve them.\n\n\nLet’s see how an application scales during a very special event\n\nHere is how the number of pods evolves with a special prescaling event configured:\n\n\nNumber of pods by status over time\n\nFor this specific application, we had around 25 pods during the day and standard prescaling was configured at \n40 pods in the HPA annotations. On normal days, we would have had about 40 pods throughout the evening. \nOn this particular day, we had around 125 pods: a ”x3” multiplier was applied, thanks to the prescaling API.\n\nPrescaling external services: another challenge\n\nReactive scaling still answers most of our needs. We are still able to do “x2 every 5 minutes” in Kubernetes. \nPrescaling is great, it can help critical applications to sustain sudden and expected traffic spikes we had \nproblems dealing with before. On top of that, prescaling for special events even allows us to deal with extreme cases.\n\nStill, the applications we prescale often depend on external services: a database, a cache, a search engine… \nMost of these external services will not prescale as easily as with our prescaling solution. \nSome services, like AWS DynamoDB or AWS Aurora serverless, come with a reactive autoscaling solution, \nbut not all of them. And still, even those autoscaling services have limits…\n\n\n\nVery special thanks to all my Bedrock Streaming colleagues who helped me improve this blog post.\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #15",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/12/20/bedrock-dev-facts-15.html",
    "date"     : "December 20, 2021",
    "excerpt"  : "Le père noël 🎅🏻 vous apporte en avance une hotte pleine de devfacts !\n\nTempora mori, tempora mundis recorda\n\n\n  L’autre jour, on allait au ski. C’était l’année dernière\n\n\nLe pass sanitaire de la prod\n\n\n  Un test de charge c’est comme un test PCR, ...",
  "content"  : "Le père noël 🎅🏻 vous apporte en avance une hotte pleine de devfacts !\n\nTempora mori, tempora mundis recorda\n\n\n  L’autre jour, on allait au ski. C’était l’année dernière\n\n\nLe pass sanitaire de la prod\n\n\n  Un test de charge c’est comme un test PCR, c’est valable 3h\n\n\nUn pair programming un peu trop random\n\n\n  _ “Tu peux aller sur un site web random pour vérifier un truc ?”\n\n  _ “P***hub ça te va ou ça te dérange ?”\n\n\nLa précision\n\n\n  Alors, pour X, tu as un gros tas d’éléments, pour Y un tas d’éléments, et pour Z, un petit tas d’éléments.\n\n\nOn n’est pas sorti du sable\n\n\n  “stdout n’existe plus”\n\n\nL’effet papillon\n\n\n  “Bon tu viens manger ?”\n\n  “Attends je suis d’astreinte et il y a des orages à Paris qui font sauter le live en Croatie”\n\n\nA new hope\n\nQuand tu envisages de refacto un bout de code, que tu regardes la PR à l’origine de ce code et que tu trouves un commentaire que tu avais laissé qui dit:\n\n\n  I really hope this will solve our problems and not introduce new ones.\n\n\nLe retour du first try\n\n\n\nUn bug ? Où ça un bug ?\n\n\n  Comme j’ai pas réussi à le résoudre, on peut le mettre en terminé\n\n\nAh la boulette !\n\n\n  Est-ce que vous savez si on peut abort un git rebase --abort ?\n\n\nRetour de congés compliqué\n\n\n  Possible de m’unlock mon compte Okta ?\n\n\nTODO\n\n\n\nUn bon conseil\n\n\n  Regardez le côté fonctionnel du succès\n\n\nLe re-retour du first try\n\n\n\nTout est planifié depuis le début\n\n\n  C’est prévu mais ça fait pas ce que je voulais.\n\n\nAh !\n\n\n  On lui a demandé “qu’est-ce qu’on en fait ?“, ils nous a répondu “rien”\n\n\nMon nom est Paco 🦜\n\n\n  il répète tout ce que tu dis … mais en faux !\n\n\nNe pas sous-estimer le miss-click\n\n\n  Tu peux pas miss-click trois fois de suite c’est pas possible, même en vacances tu peux pas.\n\n\nOn se la pète un peu\n\n\n  \n    \n      Ça montre bien qu’on est trop fort!\n    \n    \n      Ne confond pas le hasard avec une compétence!\n    \n  \n\n\nDifficulty Driven Design\n\n\n  Sommes-nous convaincus par le DDD ou des cons vaincus par le DDD ?\n\n\nL’importance de se relire\n\n\n  After more reflection I said bullshit.\n\n\nVers l’infini et l’au-delà\n\n\n  L’engineering manager m’a confirmé que l’on pourrait aller bien au delà, voire un peu plus…\n\n\nUn env de dev ? Pourquoi faire ?\n\n\n  \n    Tu as testé en local ?\n    Bien sûr que non. AH ça ne fonctionne pas en local\n  \n\n\nQuand on vient annoncer une bonne nouvelle\n\n\n  Bonjour ! Comment ça allait ?\n\n\nJ’y crois moyen\n\n\n  Pour une fois ça marcherait même mieux sous Windows.\n\n\nPlus simple que simple\n\n\n  Avec les nouvelles fonctionnalités, c’est compliqué de faire au plus simple.\n\n\nSa vision est basée sur le mouvement\n\n\n  Gris c’est bien parce que c’est ni vert ni rouge !\n\n\nLa concurrence est rude\n\n\n  \n    (P.O. Mobile) : “C’est moche”\n    (Tech Lead Android) : “C’est Android”\n    (Dev iOs) : “Si on peut même plus troller”\n  \n\n\nEddy malou en marque blanche\n\n\n  La customiseration\n\n\nLe crime parfait\n\n\n  Ah oui, y’a des voyous ici qui mettent des espaces insécables dans les noms de fonctions.\n\n\nOn l’avait pas vu venir celle-là\n\n\n  C’est hyper compliqué de prévoir des choses qu’on ne peut pas prévoir.\n\n\nC’est trop calme\n\n\n  C’est l’encéphalogramme de l’huitre ton dashboard là\n\n\nThree magic words\n\nÀ la cafétéria:\n\n\n  \n    👋 Il faut qu’on discute !\n    Si c’est pour parler boulot c’est même pas la peine !\n    Incident en prod\n    Ah oui j’ai vu passer une notif, je regarde ça de suite !\n  \n\n\nL’expérience ou l’age\n\nMontre un raccourci clavier dans un outil de gestion de tickets\n\n  T’inquiète pas, j’ai roulé ma bosse, je gère !\n\n\nLes geeks no-life\n\n\n  \n    tu peux bouger stp pour la lumière\n    pfff, j’ai bougé il y a pas 5mn\n  \n\n\nNe t’inquiète pas, on va trouver ça dans les log\n\n\n\nFatigue du soir\n\n\n  500 c’est pour succès, c’est ça ?\n\n\nEn rouge et noir !\n\n\n  Quand c’est rouge c’est que c’est OK.\n\n\nDEL\n\nQuand on supprime du code et qu’on explique comment on l’a fait dans la PR.\n\n\n  How?\n\n  Del key on the keyboard.\n\n"
} ,
  
  {
    "title"    : "Scaling Bedrock video delivery to 50 million users",
    "category" : "",
    "tags"     : " aws, cloud, sysadmin, HAProxy, video, high availability, Unified Streaming, VOD, OTT, video delivery",
    "url"      : "/2021/12/15/scaling-bedrock-video-delivery-to-50-million-users.html",
    "date"     : "December 15, 2021",
    "excerpt"  : "Here’s our journey to migrate tens of thousands of videos, accessed by millions of users, to the cloud. How we minimized our costs without losing the biggest benefit of the cloud: scaling.\n\nThe purpose of this article is to show you the evolution ...",
  "content"  : "Here’s our journey to migrate tens of thousands of videos, accessed by millions of users, to the cloud. How we minimized our costs without losing the biggest benefit of the cloud: scaling.\n\nThe purpose of this article is to show you the evolution of this cloud video delivery platform, from the first draft to the current version.\n\nTable of Contents\n\n\n  How we do streaming\n  Just In Time Packaging\n  Version 1: The quest for self\n    \n      Local cache with Nginx\n      Network Load Balancer: manages TLS and helps with scaling\n      Content Delivery Network: Keep it Simple and Stupid\n      A first conclusion: V2 needs Consistent Hashing\n    \n  \n  Version 2: Let the requests flow\n    \n      HAProxy to make Consistent Hashing\n      EC2 costs are reduced by using only Spot instances\n      Production launch on this V2\n      EC2-other: the financial abyss\n    \n  \n  Version 3: Cost Explorer Driven Development\n    \n      Be multi-AZ without inter-AZ traffic\n      Mono-AZ AutoScalingGroups\n    \n  \n  Optimizations\n    \n      Adapt HAProxy config for EC2 bandwidth throttling\n      Adjust the hash balance factor to correctly trigger scaling\n    \n  \n  Conclusion\n\n\nHow we do streaming \n\nTo stream video, we cut each video file in 6 seconds chunks. The video player loads the associated manifest, which lists these pieces and in which order it must read them. It then downloads the first video chunk, plays it, then loads the second chunk, etc.\n\n\nPictorial explanation of video streaming\n\nA video is composed of several chunks.\nFor example, a 90 minutes movie, with a duration of 6 seconds per chunk, means 90×60÷6=900 video chunks called from the player plus another 900 audio chunks. A total of 1800 different chunks for a single video.\n\nJust In Time Packaging \n\nA client calls a manifest and chunks to play a video.\nDepending on the format (Dash, HLS, Smooth) a client supports, it will request one of three kinds of manifests+chunks.\n\nThe Unified Streaming software handles these calls. Unified Origin (which we call USP) fetches the associated video from a AWS S3 bucket. It relies on a server manifest (.ism file), stored with the video, to respond with the video format the client requested: Dash, HLS, etc.\n\nSo, we store a complete video and its server manifest on S3, and USP provides the client with a client manifest and specific chunks: this is Just In Time Packaging (JITP).\n\nAnother way is to compute all the chunks and manifests in advance and write them to S3: this is offline packaging.\nIn this case, once packaging is done, there is no need to do these calculations anymore: it lightens the architecture and avoids the availability challenges of doing real-time computing.\n\n\nComparing Just-In-Time Packaging with Offline Packaging\n\nStill, this causes a big cost problem. On AWS S3, you pay for data access (GET requests), as well as storage. The more you store, the more you pay and the more you access, the more you pay.\n\ni.e, a 90mn video, played in Dash, is cut into 900 chunks plus a single Dash manifest. The same video in HLS, it’s 900 different chunks and another manifest: 1802 files written on S3. Add the Smooth Streaming format and you get 2703 files stored on S3, for a single video.\n\nOffline packaging is interesting, but incompatible with our need to manage a large number of equipments and vast catalogs: tens of thousands of program hours per customer.\n\nAnother approach, which uses the best of the two above solutions, is possible: the CMAF (Common Media Application Format) standard.\nThe player is able to chunk the video itself by adding the HTTP header Range: Bytes.\nMany devices, especially connected TVs or old Android versions, are not compatible with CMAF, which is why the rest of this article will focus on Dash/HLS in JITP.\n\nVersion 1: The quest for self \n\nWe were using USP on-prem. We decided to migrate it to the AWS cloud.\n\nThe goal of the V1 was to quickly provide a platform to our video teams to work on and certify the video players. For Bedrock Ops team, it was a first stepping stone building this platform.\n\nLet’s detail the components of this V1.\n\n\nv1 of our VOD platform\n\n\n  Players send their requests to a CDN.\n  The CDN uses a network load balancer as its origin.\n  The network load balancer forwards requests using a Round Robin algorithm, to multiple AWS EC2 instances we call “USP Origin”. These EC2 instances are controlled by an AutoScalingGroup and are dynamically scaled based on their network or CPU usage.\n  EC2s retrieve files from the S3 bucket.\n\n\nLocal cache with Nginx \n\nOn EC2 instances, USP runs as a module of Apache HTTPD.\n\nWhen a player requests a specific video chunk, it sends an HTTP request to HTTPD. The USP module it embeds will:\n\n\n  load the according .ism file from S3 (the server manifest)\n  load the video metadata, stored in the first 65KB and the last 15B of a .mp4 file on S3\n  load the specific chunk from the mp4 container, according to the player’s information: bitrate, language, etc. (still on S3)\n\n\nFor each video chunk called from a player, the USP module does another call to the S3 bucket, loading the same .ism manifest and the same metadata (first 65K and latest 15B).\nTo avoid these calls and reduce S3 costs by 60%, we added Nginx on these EC2s. It goes between HTTPD and S3, to cache the manifest .ism files and metadata of .mp4 video files.\nWe’re using LUA in the Nginx vhost, to cache these 65KB and 15B requests made by USP to the S3 bucket.\n\n\nDetails on the composition of a USP origin\n\nWe use Nginx for caching because we have a solid experience with it, under heavy load, on our on-prem edge servers, which each delivers up to 200Gbps of video traffic. We want to capitalize on this expertise and avoid spreading ourselves thin on multiple tools (e.g, Apache Cache module).\n\nI recommend reading the article published by unified streaming, which uses a similar method: caching via httpd directly.\n\nNetwork Load Balancer: manages TLS and helps with scaling \n\nWe’re using Network Load Balancers to offload TLS. They are cheaper than Application Load Balancers and we don’t need to interact with the HTTP layer: this is not the role of the load balancer, we prefer to keep a KISS principle.\n\nThe major advantage of NLBs is a single entry point (a CNAME domain name), which distributes the load over n EC2 instances. This is essential for auto-scaling: nothing to configure at the CDN level, the load balancer will distribute the load among all Ready instances, whether there are 2 or 1000.\n\nAWS managed load balancers are also interesting because certificates are auto-renewed. Another advantage is that they are distributed over all the availability zones, which was one of our prerequisites in our multi-AZ strategy.\n\nContent Delivery Network: Keep It Simple and Stupid \n\nWe’re using Cloudfront CDN with a basic configuration: we respect standards and use Cache-Control header.\n\nWe’re also using our on-prem Edge servers and other CDNs. Likewise, they all respect the HTTP protocol RFCs and we provide a valid Cache-Control header to be CDN agnostic.\n\nA first conclusion: V2 needs Consistent Hashing \n\nV1 of this platform allowed our video teams to work on new features and new software versions quicker, compared to on-prem. It was also new for Infra teams: we wanted to understand how to scale the platform on AWS to meet our load requirements, making the best use of managed services and auto-scaling, which we did not have on-prem.\n\nWe have identified the problem of version 1 during our tests: the cache is ineffective under heavy loads. The Round Robin algorithm (used by the NLB) is not adequate in front of cache servers because each server will try to cache all the data and will not be specialized to a part of the data. The more requests we have, the more servers we will add and the less each server will have a relevant cache.\n\n\nInefficiency of a Round Robin algorithm in front of cache servers\n\nTo use the cache as much as possible, we need an adapted load balancing method: Consistent Hashing.\n\n\nConsistent Hashing is an ideal method for caches\n\nLast two images are from our recent blog post about doing advanced load balancing at AWS.\n\nWith Consistent Hashing, we can send all requests for the same video to the same cache server. This would optimize the local Nginx cache and reduce S3 costs.\n\nVersion 2: Let the requests flow \n\nV2 will be used in production, with thousands of requests per second: we need it to handle the load, to be reliable and robust.\nAccording to our strong experience with HAProxy, we know that it is able to do Consistent Hashing with Bounded Loads, which is exactly what we need.\n\nWe started by adding HAProxy servers between the load balancer and the USP servers.\n\nHAProxy to make Consistent Hashing \n\nHAProxy is running on EC2 instances, in a dedicated AutoScalingGroup. As with the USP AutoScalingGroup, this one scales with AWS scaling policies: network bandwidth or CPU consumption. We can launch hundreds of HAProxy servers if we need to, and their scaling is independent from the number of USP servers (but they are often linked).\n\n\nv2 of our VOD platform\n\nTo send requests to USP origin, HAProxy needs to know all the healthy EC2 instances running in their AutoScalingGroup.\nWe started by using Consul, to automatically populate our HAProxy backend with these USP servers.\n\nSee the dedicated blog post to know why we preferred to develop a tool dedicated to this task, which we called HAProxy Service Discovery Orchestrator (HSDO).\n\nEC2 costs are reduced by using only Spot instances \n\nIn addition, HSDO is very responsive to movements in the AutoScalingGroup, which allowed us to replace all EC2 On Demand instances with Spot instances.\nAnd by all instances, I mean all USP servers (with cache), as well as HAProxy servers: 70% reduction in server costs.\n\nNote that replacing USP origins with Spot instances has almost no impact on the cache, as we follow the AWS best practices for Spot: use many different instance types, be multi-AZ and use the “Capacity Optimized” strategy. This way we observe very few reclaims, which translates to a longer cache life.\n\nProduction launch on this V2 \n\nThe production launch of this V2 has confirmed the stability and performance of the platform. We were happy to see that our objectives were met, so we started migrating all our VOD content from on-prem to the cloud, video by video, client by client.\n\n\nNginx cache hit ratio\n\nWith Consistent Hashing, the cache becomes quite efficient and we saved 62% of calls to S3.\n\nIn addition, the cache speeds up the video packaging and we have reduced the overall origin response time. Win-win.\n\nEC2-other: the financial abyss \n\nEC2-other, in this case, means network traffic between Availability Zones.\nThe private network between two data centers (AZs) at AWS is re-billed and accounted for 48% of the bill for our VOD platforms at the time.\n\n\nAWS Cost Explorer, October 2020\n\nWhen HAProxy servers sent/received traffic from USP servers, the latter might not be in the same Availability Zone and the traffic between the two was charged at full price.\n\nIt was necessary to quickly find a solution for these costs which torpedoed the project. We started the creation of version 3 as soon as we had these metrics from version 2, at the beginning of our VOD content migration.\n\nVersion 3: Cost Explorer Driven Development \n\nThe idea is to do as well for half the price.\n\nBe multi-AZ without inter-AZ traffic \n\nWe have updated HSDO so each HAProxy only sends requests to USP origins of the same AZ.\nAnd the Network Load Balancers still send traffic to the HAProxys, on multiple AZs.\n\nRemoving inter-AZ traffic was not that much work and we quickly saw the difference: 45% cost savings.\n\n\nThe costs of this platform, where v3 was deployed in mid-November 2020\n\nWe can see on the picture above that the EC2-other costs (in orange) have disappeared in December.\n\nThe work on version 3 began shortly after the start of our cloud migration.\nWe were still migrating on-prem to the cloud when we put V3 on prod. That’s why you can see all costs have increased from October to December: we’ve doubled the number of viewers during the period.\n\nMono-AZ AutoScalingGroups \n\nWe have also replaced the multi-AZ AutoScalingGroups by several mono-AZ ones. It gives us a finer scaling that corresponds to the real needs in each AZ. The randomness of the round robin and client requests means that, from time to time, one AZ receives a significantly higher load than another.\n\n\nv3 of our VOD platform\n\nSince the NLBs are using a Round Robin algorithm, each HAProxy can receive traffic for any video. Now that the HAProxy servers of an AZ only send traffic to the USP origins of the same AZ, everything that is cached exists in as many copies as we have configured of AZs.\nIt makes us all the more resilient to an AZ failure.\n\nOptimizations \n\nSince V3, we have not made any major architectural changes. However, some optimizations were necessary.\n\nAdapt HAProxy config for EC2 bandwidth throttling \n\nOn AWS, an EC2 instance has a baseline network capacity and a burst capacity (see UserGuide).\nBaseline capacity is the network bandwidth you can consume all the time.\n\nThe Burst capacity is what you may be able to consume temporarily before being throttled to the baseline capacity.\nIn the EC2 presentation, the value “Up to” refers to the burst.\n\nLess visible in the EC2 documentation, one can find the baseline capacity for each instance type (which is public knowledge since July 2021).\n\nFor example, c5.large instances have a network bandwidth Up to 10Gbps (burst) but only 0.75Gbps baseline bandwidth.\n\nTroubles start when HAProxy sends a little more traffic to one instance than to the others: USP origin’s bandwidth may be throttled at some point… And we will observe poor performance or even service interruptions of this server.\n\n\nA server whose bandwidth is throttled (seen from CloudWatch)\n\nWe added observe layer7 to the default-server in our HAProxy backends, to remove servers returning HTTP error codes (5xx) from its load-balancing.\n\nWe also added the retry and redispatch options, which allow to retry a request sent to an unhealthy server on a healthy server. It’s not optimal for the cache, but what matters is that a client’s request is successfully answered.\n\nWe observed that with throttled bandwidth, the connection time from HAProxy to a USP server increases dramatically.\nSo we’ve also reduced timeout connect to 20 milliseconds.\n\nHighlights of our HAProxy configuration:\n\ndefaults\n   timeout connect     20ms\n   retries     2\n    # We do not use &quot;all-retryable-errors&quot; because we don&#39;t want to retry on 500,\n    # which is an USP expected error code when it goes wrongly\n   retry-on     502 503 504 0rtt-rejected conn-failure empty-response response-timeout\n   option     redispatch\n   timeout server     2s\n   default-server     inter 1s fall 1 rise 10 observe layer7\n\n\nNow, if a USP origin throttles on its network bandwidth or if there is a degradation of service, HAProxy will immediately redispatch the request to another server.\n\nWe are working on adding an agent-check, so that the weight of the servers in HAProxy can be directly defined by an USP origin, if it detects that its bandwidth is throttled.\n\nAdjust the hash balance factor to correctly trigger scaling \n\nOur scaling depends on the average server utilization in an AutoScalingGroup. If a few servers are overloaded but the majority is not doing anything, we don&#39;t scale.\n\nBut all contents on our platforms are not equally popular. This affects Consistent Hashing which would result in few servers receiving way more traffic than others. Few servers would be overloaded and the majority would not do much.\n\nHere is an example in a load test:\n\n\nGraph showing few overloaded servers, using classic Consistent Hashing\n\nWe want to benefit from Consistent Hashing while being able to scale on average consumption.\nThis is what Consistent Hashing with Bounded Loads allows: to benefit from Consistent Hashing, while balancing load.\n\nThe Bounded Loads are controlled by the hash-balance-factor option in HAProxy.\nAccording to the doc:\n&amp;lt;factor&amp;gt; is the control for the maximum number of concurrent requests to\n         send to a server, expressed as a percentage of the average number\n         of concurrent requests across all of the active servers.\n\n\nWe played the same load test, once using classic Consistent Hashing, a second time using bounded loads:\n\n\nGraph showing the effects of Bounded Loads over Consistent Hashing\n\nWe did dozens of load tests before finding the best value for our use: 140.\nFor each load test, we looked at the evolution of:\n\n\n  Nginx Cache Hit Ratio\n  Number of requests to S3\n  HAProxy Backend retries and redispatches\n  HAProxy backend 5xx response codes\n  Free disk space on USP origins\n\n\nOur configuration of the Consistent Hashing with Bounded Loads remains simple:\n\nbackend usp-servers-AZ-C\n    balance hdr(X-LB)\n    hash-type consistent sdbm avalanche\n    hash-balance-factor 140\n\n\nThanks to Consistent Hashing with Bounded Loads, our cache is optimized without impacting our autoscaling.\nThere may be contents much more solicited than others, the load will be balanced and our autoscaling will be activated.\n\nConclusion \n\nWe migrated our video delivery to the cloud, moving from static servers to an end-to-end auto-scaling and multi-AZ infrastructure. We are now able to handle very high loads, which we could not do on-premise.\nWe had the opportunity to review our architecture three times, within a few weeks of each other, even though the migration had begun.\n\nThe v3 is not perfect, but it is quite well optimized, reliable and scalable.\n\nWe are thinking about V4 and saving 20% of the costs by removing the NLB. We also identified some possible improvements, adding cache on HAProxy for example, or using HAProxy Agent Check so that the weight of the servers in HAProxy is driven directly by the servers, using the Amazon metrics on network performances. Another promising performance improvement could be to use HAProxy on ARM as Graviton type instances offer significant discounts, it will be worth testing.\n\nIn parallel, we also invest time on CMAF which is for us, the long-term objective.\n\n\n\nSpecial thanks to all my colleagues at Bedrock Streaming and members of Unified-streaming, for re-re-re-and-rereading this blog post. ❤️\n"
} ,
  
  {
    "title"    : "More efficient Load Balancing and Caching at AWS, using Consistent Hashing and HAProxy",
    "category" : "",
    "tags"     : " aws, cloud, sysadmin, HAProxy, video, opensource, high availability",
    "url"      : "/2021/11/18/hsdo.html",
    "date"     : "November 18, 2021",
    "excerpt"  : "AWS ALB &amp;amp; NLB currently supports Round-Robin (RR) and Least Outstanding Requests (LOR) balancing algorithms. But what happens when you try to load balance cache servers with these algorithms? How to implement an effective cache in Cloud at sca...",
  "content"  : "AWS ALB &amp;amp; NLB currently supports Round-Robin (RR) and Least Outstanding Requests (LOR) balancing algorithms. But what happens when you try to load balance cache servers with these algorithms? How to implement an effective cache in Cloud at scale?\n\nContext\nAt Bedrock we use both ALB &amp;amp; NLB for different use-cases (like in front of our Kubernetes clusters) in our platforms. For our last VOD platform, we needed to be able to load balance heavy content (videos) to our cache servers. We knew that the balancing algorithm was a key factor for our cache in Cloud at scale, and that ALB &amp;amp; NLB won’t be sufficient to achieve our goals.\n\nBalancing algorithms\n\nRound-Robin is a widely used balancing algorithms.\n\n\nRound robin or Least Outstanding Requests algorithms\n\nThe load balancer cycles through cache servers sequentially, so each cache server should receive an equal share of requests. Each cache server has to possibly store every requested object (♠️, ♥️, ♦️, and ♣️ represent different objects).\n\nWith Least Outstanding Requests, load balancers send requests to the cache server with least awaiting requests. The cache server still has to store every requested object.\n\nConsistent Hashing is a very interesting balancing algorithm for caching purposes.\n\n\nConsistent Hashing algorithm\n\nThis algorithm allows to distribute the load so that all requests for the same object will always go to the same cache server. This way, each cache server has to store half of the objects.\n\nAnd more cache servers means more load balancing between the servers.\n\n\nConsistent Hashing at scale\n\nWhile at scale, Round-Robin or Least Outstanding Requests will look like this:\n\n\n\nRound Robin or Least Outstanding Requests at scale\n\nCache servers are not efficient at scale with these algorithms. There is a higher chance for the cache to miss, because all servers do not store all objects. Every time a cache expires or cache server boots, objects need to be cached again to be hit. You also need more resources, as the same object needs to be cached on all cache server disks.\n\nWith Consistent Hashing, once an object has been cached you have a greater chance for the cache to hit. And you save money by using smaller disks on cache servers and reducing network bandwidth.\n\nHAProxy implementation\n\nConsistent Hashing at AWS is not available with ALB, ELB and NLB. You need to implement it yourself.\n\nTo do this, we chose to use HAProxy.\n\nHAproxy is fast and reliable. We use it often, we know it well, and it can use consistent hashing.\n\nThis is how we architected it.\n\n\nLoad Balanced Cache Architecture\n\nHAProxy servers and Cache servers are deployed with Auto Scaling Groups (ASG). A Target Group is registering HAProxy ASG instances so NLB will load balance between them. Having separated ASG for HAProxy and Cache allows it to have dedicated automatic scaling and management.\n\nWe need to implement something for HAProxy so it could discover and register Cache ASG instances. And there is one thing important to do consistent hashing with HAProxy: a centralized and consistent configuration.\n\nCentralized configuration\n\nAll HAProxy instances need to have the same configuration with the same list of servers, or requests will be split differently depending on HAproxy instances.\n\n\nConsistent Hashing with different list of servers per HAProxy\n\nYou have a higher chance to have a miss on your cache request as a single object may be at different locations depending on the HAproxy instance.\nWith a centralized configuration, you can be sure that each HAProxy instance will request the same cache server for the same object.\n\nConsistent configuration\n\nHAProxy consistent hashing is based on backend server IDs. These IDs match the position of the server in the backend server list. For example, this HAProxy configuration:\n\nbackend cache\n   server CacheA 192.168.0.1:80 #ID = 1\n   server CacheB 192.168.0.2:80 #ID = 2\n   server CacheC 192.168.0.3:80 #ID = 3\n   server CacheD 192.168.0.4:80 #ID = 4\n\n\nwill be seen as the following:\n\n\n\nTo keep consistent hashing efficient, cache servers need to change ID rarely. HAProxy backend server list must be consistent across all HAProxy instances.\n\nIf CacheC is removed, configuration has to be like:\n\nbackend cache\n   server CacheA 192.168.0.1:80          #ID = 1\n   server CacheB 192.168.0.2:80          #ID = 2\n   server CacheC 192.168.0.3:80 disabled #ID = 3\n   server CacheD 192.168.0.4:80          #ID = 4\n\n\nCacheC backend server is now disabled until another Cache server takes its place.\n\n\n\nWith consistent configuration: ♠️, ♣️ and ♥️ requests are always balanced to the same cache servers, while ♦️ requests are balanced to another available cache server.\nWithout consistent configuration: all requests could be rebalanced to other cache servers. This would mean that for each cache server scale up or down, we no longer have the cached objects: we MISS the cache. This would be inefficient: we would lose the advantage of the cache.\n\nSolutions\n\nConsul\n\nAt first, we started to configure HAproxy through Consul. We already used it at BedRock, and an article gave us hope to quickly achieve what we wanted. \nConsul Service Discovery with DNS won’t provide sorted/consistent DNS records by design. We can’t have a consistent configuration with it.\n\nAnother way of doing so would be to use consul-template for generating backends and registering servers into an HAProxy configuration file. With this approach, we would reload systemd to add new servers to HAProxy.\n\nBut HAProxy Runtime API is the recommended way to make frequent changes on configuration, service reloads are not considered safe.\n\nAWS EC2 Service Discovery\n\nHAProxy also released a new functionality called AWS EC2 Service Discovery in July 2021. We haven’t tested it yet, but it lacks the possibility to keep a consistent list of servers between HAProxy instances, which isn’t good for consistent hashing as discussed before. We opened an issue on HAProxy dedicated Github repository.\n\nAdded to the fact that we were starting to think that Consul was overkill for our needs, we start to implement our own solution.\n\nHAProxy Service Discovery Orchestrator\n\nWhat we wanted to achieve was to use maximum managed service from AWS, meet our standard of stability and resilience, and keep things simple. We choose Python with boto3 to implement our solution as it is one of our team’s favorite languages.\n\nHAProxy Service Discovery Orchestrator (or HSDO) is open-source.\n\nHSDO is composed of a server and a client.\n\nHSDO Server\n\nHSDO Server runs in standalone. It could be a Lambda, but we were more comfortable with system processes when we designed it.\nIts job is to keep track EC2 instances of one or multiple Cache ASGs and update a list accordingly.\nHSDO server provides a consistent sorted list of instances. Every time a new cache server appears in the ASG, it is added to the list at a given ID that will never change.\nThis list is stored in DynamoDB.\n\n\nDynamoDB Items View\n\nHSDO Client\n\nHSDO client is reading the DynamoDB table to get the cache servers. The client run on the same instance as HAProxy and use the runtime API to update HAProxy config.\n\n\nHAProxy Status Page\n\nBrown lines are disabled servers, while green lines are servers stored in DynamoDB as seen above.\n\n\nHSDO in Load Balanced Cache Architecture Schema\n\nWith this architecture, we achieve a centralized and consistent configuration to make consistent hashing work at scale for cache servers.\n\nConclusion\n\nWe have been using HSDO since September 2020. We are distributing VOD content for Salto and 6play streaming platforms and are able to handle at least 10.000 requests/s. This wasn’t possible without a few improvements (on platform cost, timeout funnels, …) and this will be presented in another post, so keep in touch. ;)\n\nSpecial thanks to all Ops team members in BedRock Streaming for re-re-re-and-rereading this blog post.\n"
} ,
  
  {
    "title"    : "Forum PHP 2021 - L&#39;édition des retrouvailles",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2021/11/02/forum-php-2021.html",
    "date"     : "November 2, 2021",
    "excerpt"  : "Cette année encore, Bedrock participait au Forum PHP où était proposé une grande diversité de conférences.\nDes sujets techniques et d’autres, plus génériques, étaient abordés : Symfony 6, Git, environnement, sous-représentation des femmes dans l’i...",
  "content"  : "Cette année encore, Bedrock participait au Forum PHP où était proposé une grande diversité de conférences.\nDes sujets techniques et d’autres, plus génériques, étaient abordés : Symfony 6, Git, environnement, sous-représentation des femmes dans l’informatique…\nAvec Sofia LESCANO, Benoit VIGUIER sur les planches et une quinzaine de participantes et participants dans le publique, l’occasion de rencontrer à nouveau la communauté PHP en chair et en os a été saisie avec une certaine impatience.\n\nPlusieurs conférences ont retenu notre attention et auront un impact à court terme sur nos projets :\n\nSuite à la conférence “Les exceptions : le trou dans la raquette du typage” de Baptiste LANGLADE, la bonne gestion des exceptions nous semble primordiale. Nos équipes sont donc en train de tester le bundle Innmind/Immutable pour mettre en place le pattern Monad (Maybe et Either) afin d’améliorer la gestion d’absence de données à différents niveaux de nos outils.\n\nLa conférence “Des tests unitaires pour nos règles de conception” de Frédéric BOUCHERY mettait en lumière l’importance de documenter, expliciter et tester les règles de conception d’un projet. La mise en place d’ADR (Architectural Decision Records) et des tests unitaires associés est une bonne pratique que nous souhaitons développer au sein des équipes. Nous attendons avec impatience le bundle que Klaxoon devrait bientôt open-sourcer: il permettra de tester facilement nos règles de conceptions avec PHPUnit et d’automatiser une partie de la revue technique.\n\nAnne-Laure DE BOISSIEU et Amélie DEFRANCE ont rappelé quelques règles fondamentales d’accessibilité pour nos sites Internet pendant “Accessibilité et SEO : et si on relevait le niveau ?”. Nous espérons désormais améliorer l’accessibilité de notre back-office. Par exemple : retravailler le contraste couleur de certains écrans ou ajouter du contenu dans nos balises HTML pour faciliter la compréhension.\n\nPendant leur conférence “Kairoi, et PHP se réconcilie avec les tâches planifiées”,  Emeric KASBARIAN et Jérémy JAMES nous ont expliqué que leurs clients ont un même besoin : “Déclencher une action automatique à un moment précis, sans aucune limite dans le temps”. Par exemple : “supprimer, automatiquement, un panier d’achat au bout de 15 minutes”.\nAprès de longues recherches, il n’existe rien sur le marché pour répondre à un tel besoin. Et nous, BedRock, confirmons : nous avons le même besoin et n’avons rien trouvé non plus.\nKairoi est donc né. C’est une application serveur Rust de planification de tâches, avec son propre protocole (inspiré de celui de Redis) qui permet de :\n\n  Récupérer des évènements à planifier\n  Connaître l’état d’un l’évènement\n  Le déclencher au moment opportun\n    \n      soit sur un protocole AMQP\n      soit sur un shell\n    \n  \n\n\nLe Forum PHP est l’occasion de parler de sujets pointus techniquement, mais aussi une occasion d’échange et de partage autour de sujets plus transversaux.\nNotre domaine, l’IT, comme bien d’autres, est sensible au sujet de l’écologie. Deux axes de réflexion ont été évoqués pendant deux conférences.\n\nEn ouverture, François ZANIOTTO nous a partagé avec entrain sa recherche de mesures fiables des dépenses énergétiques. Elle l’a menée à développer  GreenFrame, un outil en cours de construction chez Marmelab, qui a pour but de cibler au plus près la consommation énergétique afin de tendre “Vers la sobriété numérique”.\n\nHélène MAITRE-MARCHOIS a sû mettre en perspective le rôle de chaque développeuse et développeur en insistant sur le fait que la responsabilité du dérèglement climatique n’est pas forcément là où on l’attend. Avec “Comment sauver la planète en ne faisant rien”, elle entend faire prendre conscience que si, la production et consommation de contenu représentent des pôles sur lesquels en tant que tech, nous pouvons agir. Le renouvellement du parc reste une cause prépondérante dans l’impact écologique.\nLe renouvellement accéléré par le foisonnement de nouvelles fonctionnalités, trop souvent inutiles, est une obsolescence programmée.\nUtile, Accessible, Durable. Voilà trois notions simples qui peuvent, pourtant, nous permettre de faire la différence.\n\nNicolas GREKAS, principal engineer Symfony, nous a parlé de l’écosystème de ce framework à travers de sa conférence “Symfony 6 : le choix de l’innovation et de la performance”. Il nous a présenté le calendrier de livraisons et de maintenance des différentes versions, avec la sortie d’une nouvelle majeure prévue tous les deux ans. Chaque majeure voit la suppression du code déprécié dans la version précédente. Par exemple, Symfony 6 est un Symfony 5.4 sans ses dépréciations. Les dernières versions avant une majeure (comme la 4.4 ou 5.4) sont assurées d’avoir un support à long terme.\nPour les prochaines versions de Symfony, l’accent est mis sur la compatibilité avec PHP8. Puisque la majorité du travail consiste à remplacer les annotations @return par un typage natif, Nicolas a parlé de l’outil patch-type-declarations qui automatise cette tâche.\n\nPour finir cette série de conférences, nous avons suivi l’incroyable histoire de WorkAdventure, lors de “WorkAdventure de la genèse à aujourd’hui : Retour d’expérience sur 1 an d’univers virtuels” présenté par David NÉGRIER) : le résultat d’un hackathon fait pour pallier l’ennui des confinements, qui est devenu le support d’événements majeurs l’année dernière.\n\n\n\nLes speakers Bedrock\nLors de cette édition, deux Bedrockers ont eu l’opportunité de présenter un sujet, l’occasion pour nous de demander à Sofia LESCANO “Faites confiance aux développeurs.euses de votre équipe : voyez plus loin que les fonctionnalités” et Benoit VIGUIER “Fiber : la porte ouverte sur l’asynchrone” comment ils ont vécu cet événement.\n\nComment vous est venue l’idée de soumettre un sujet de conférence, et comment avez-vous abordé sa préparation ?\n\nSofia: Cela faisait longtemps que j’avais ce sujet en tête, et l’idée de refaire des événements en présentiel m’a fait me lancer. Pour moi les tech meetings étaient une grande découverte et un rituel que j’apprécie vraiment et je voulais partager cela avec la communauté. Pour la préparation, j’ai été accompagnée par Matthieu Napoli avec le programme de mentoring de l’AFUP et par mes collègues de Bedrock.\n\nBenoit: Le PHP asynchrone est un sujet qui m’occupe beaucoup à Bedrock, j’ai donc suivi attentivement la RFC Fiber. L’idée d’en faire un sujet de conférence est venue en me rendant compte que, même au sein de nos équipes, il n’était pas évident pour tout le monde de comprendre tout ce que cet outil pouvait changer. Et puis, refaire un événement en présentiel me manquait vraiment ! La préparation de ce format court était nouveau pour moi, heureusement j’ai pu faire quelques répétitions à Bedrock et à l’AFUP Lyon pour bien ajuster mon timing.\n\nMaintenant que l’événement est derrière nous, que retenez-vous de cette expérience ?\n\nSofia: C’était une très belle expérience et les échanges que j’ai pu avoir suite à ma conférence ont été très intéressants. C’est très enrichissant d’échanger avec la communauté et de voir que des pratiques similaires ont lieu ailleurs et pouvoir les enrichir dans les deux sens.\n\nBenoit: C’était un vrai plaisir de pouvoir échanger avec de vraies personnes, sans écrans interposés ! Côté speaker, l’organisation était au top et j’ai eu pleins d’échanges prometteurs sur le potentiel des Fibers. Côté conférences, j’ai vu plein de choses intéressantes, ça donne toujours matière à réfléchir, que l’on partage le point de vue exposé ou non. Merci encore à l’AFUP pour avoir mis toute cette énergie au service d’un si bel événement.\n\nLe forum, particulièrement cette édition en présentiel, c’est retrouver toute une communauté qui partage la même passion. Encore merci aux conférencières et conférenciers, merci aux organisatrices et organisateurs… Et à l’année prochaine !\n"
} ,
  
  {
    "title"    : "Fiber: the open door to asynchronous",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2021/10/21/fiber-the-open-door-to-async.html",
    "date"     : "October 21, 2021",
    "excerpt"  : "Parmi les nouveautés apportées par Php 8.1, les Fibers tiennent une place particulière. Il s’agit certainement d’une fonctionnalité qui aura un impact majeur sur l’écosystème Php, tout en ayant un impact mineur sur le code que vous écrivez tous le...",
  "content"  : "Parmi les nouveautés apportées par Php 8.1, les Fibers tiennent une place particulière. Il s’agit certainement d’une fonctionnalité qui aura un impact majeur sur l’écosystème Php, tout en ayant un impact mineur sur le code que vous écrivez tous les jours. Les Fibers sont comme des générateurs améliorés, des fonctions interruptibles, mais qui peuvent s’imbriquer de manière transparente avec d’autres fonctions. Il est donc enfin possible de créer des fonctions similaires à await et async pour rendre la programmation asynchrone moins intrusive dans notre code et permettre la compatibilité avec les frameworks existants. Voici une introduction à ces nouveaux concepts, ainsi que des exemples concrets de ce que cela permettra dans l’écosystème Php.\n"
} ,
  
  {
    "title"    : "Faites confiance aux développeurs.euses de votre équipe : voyez plus loin que les fonctionnalités",
    "category" : "",
    "tags"     : " conference, afup, php",
    "url"      : "/2021/10/21/confiance-aux-devs-de-votre-team.html",
    "date"     : "October 21, 2021",
    "excerpt"  : "Deadlines, besoins produit, pression forte et fonctionnalités à livrer : nos projets ont besoin de nous ! L’amélioration du quotidien se perd dans un second plan, alors qu’elle a un impact majeur sur l’augmentation de notre productivité et la qual...",
  "content"  : "Deadlines, besoins produit, pression forte et fonctionnalités à livrer : nos projets ont besoin de nous ! L’amélioration du quotidien se perd dans un second plan, alors qu’elle a un impact majeur sur l’augmentation de notre productivité et la qualité et maintenabilité de notre code.\n\nConstatant que nous voulions augmenter notre confort de travail, nous avons, depuis plus d’un an, mis en place des réunions techniques bi-hebdomadaires pour prendre le temps de discuter de notre plateforme et nos outils, au-delà des fonctionnalités. Chaque membre de l’équipe contribue ainsi à améliorer son expérience de travail et notre produit.\n\nÀ travers notre vécu, nos erreurs et des exemples techniques concrets, repensez vous aussi au développement de votre produit.\n"
} ,
  
  {
    "title"    : "Increase performance and stability by adding an Egress Controller in a Kubernetes cluster at AWS",
    "category" : "",
    "tags"     : " php, aws, cloud, performance, sysadmin, kubernetes, HAProxy",
    "url"      : "/2021/10/18/increase-performance-and-stability-by-adding-an-egress-controller.html",
    "date"     : "October 18, 2021",
    "excerpt"  : "Introduction\n\nWe recently encountered issues with our PHP applications at scale in our Kubernetes clusters at AWS. We will explain the root cause of these issues, how we fixed them with Egress Controller, and overall improvements. We also added a ...",
  "content"  : "Introduction\n\nWe recently encountered issues with our PHP applications at scale in our Kubernetes clusters at AWS. We will explain the root cause of these issues, how we fixed them with Egress Controller, and overall improvements. We also added a detailed configuration to use HAProxy as Egress Controller.\n\nContext\n\nBedrock is using PHP for almost all of the backend API of our streaming platforms (6Play, RTLMost, Salto, …). We have deployed our applications in AWS on our kops-managed Kubernetes clusters. Each of our applications is behind a CDN for caching purposes (CloudFront, Fastly). This means every time an application needs to access another API, requests go on the internet to access the latter through CDN.\n\nDuring special events with huge loads on our platforms, we started to see TCP connection errors from our applications to the outside of our VPC.\n\nErrorPortAllocation source\n\nAfter a few investigations, we saw that TCP connection errors were correlated with NAT Gateways ErrorPortAllocation.\n\n\nSome loadtesting on our platform, which you may see as no traffic, huge traffic, then no traffic again\n\nIn AWS, NAT Gateways are endpoints allowing us to go outside our VPC. They have hard limits that can’t be modified:\n\n  A NAT gateway can support up to 55,000 simultaneous connections […]. If the destination IP address, the destination port, or the protocol (TCP/UDP/ICMP) changes, you can create an additional 55,000 connections. For more than 55,000 connections, there is an increased chance of connection errors due to port allocation errors. AWS Documentation\n\n\nOur applications always request the same endpoints: other APIs CDN. Destination port, IP or protocol doesn’t change that much, so we start hitting max connections, resulting in ErrorPortAllocation.\n\nAt the same time, we found a very interesting blog post: Impact of using HTTP connection pooling for PHP applications at scale, which was a very good coincidence.\n\nAs you can read in Wikimedia’s post, PHP applications aren’t able to reuse TCP connections, as PHP processes are not sharing information from a request to another. Recreating new connections on the same endpoints is inefficient: adds latency, wastes CPU (TLS negotiation and TCP connection lifecycle) but also overconsumes TCP connections.\n\n\nPHP application calls another API on internet through the NAT gateway\n\nOutgoing requests optimization\n\nEgress Controller\n\nHAproxy is fast and reliable. We use it often and know it well. We already have it as Ingress Controller in our clusters and we know service mesh needs time to be production-ready. So we thought a service mesh might be overkill in our case and we tried to add HAProxy as Kubernetes Egress Controller in our clusters.\n\n\nOutgoing requests go through the Egress Controller, which pools and maintains TCP and TLS connections\n\nWe configured some applications to send a few outgoing requests to Egress Controller. The latter was configured to do TCP re-use and to forward to desired endpoints.\n\nEffects\n\nWith this optimization, we don’t encounter ErrorPortAllocation anymore. Requests duration are reduced by 20 to 30%, and apps are consuming less CPU. Ressources were spent to instantiate a new TLS connection, which is now handled by Egress Controller.\n\n\nApplication consumes less CPU, because Egress Controller is responsible of TLS and TCP connections to the outside world, which consumes a lot of resources\n\nDetailed configuration\n\nWe generally prefer to use what already exists rather than starting from scratch, so we tried to see if HAProxy Kubernetes Ingress Controller could be used as egress.\n\nHAProxy Ingress Controller loads its frontend domains in Ingress resource, and loads backend servers in the associated Service resource. To inject an external domain as a backend server, we have to use Service ExternalName.\n\n\n\nTo use HAProxy Kubernetes Ingress Controller as an Egress Controller, we will use Ingress Kubernetes resource as Egress to define domains handled by the Controller.\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app1\nspec:\n  type: ExternalName\n  externalName: app1.example.com\n  ports:\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: app1\nannotations:\n    haproxy.org/server-ssl: &quot;true&quot;\n    haproxy.org/backend-config-snippet: |\n      # See this article for the deep reasons of both parameters: https://www.haproxy.com/fr/blog/http-keep-alive-pipelining-multiplexing-and-connection-pooling/\n      # enforce SNI with the Host string instead of the &#39;Host&#39; header, because HAProxy cannot reuse connections with a non-fixed Host SNI value.\n      default-server check-sni app1.example.com sni str(app1.example.com) resolvers mydns resolve-prefer ipv4\n      # make HAProxy reuse connections, because the default safe mode reuses connections only for the same source.ip\n      http-reuse always\n\nspec:\n  rules:\n  - host: app1.example.com\n    http:\n      paths:\n      - backend:\n          serviceName: app1\n          servicePort: 443\n\n\nWhen everything is ready, you will be able to send requests:\n\ncurl -H &quot;host: app1.example.com&quot; https://haproxy-egress.default.svc.cluster.local/health\n\n\nBy default, HAProxy resolves domain names only at bootime. But it can be configured to resolves during runtime by adding a config snippet to Egress Controller configuration:\n\nglobal-config-snippet: |\n  resolvers mydns\n    nameserver local &amp;lt;MY_DNS&amp;gt;:53\n\n\nConclusions\n\nIt seems surprising to reduce requests latency by adding a hop in a network. But it does really work, even if it has some limits.\n\nThe main problem with this approach is the fact that we are effectively creating a Single Point of Failure in our clusters if we choose to send all our egress traffic through it. Instead, we are carefully selecting applications that should use an Egress Controller to refine the configuration little by little. Some applications are tightly tied to external services and would massively gain from this and others would only be less resilient.\n"
} ,
  
  {
    "title"    : "How did we live stream our Last Friday Talks?",
    "category" : "",
    "tags"     : " lft, talks, live, stream, obs",
    "url"      : "/2021/10/14/live-streaming-lft.html",
    "date"     : "October 14, 2021",
    "excerpt"  : "At Bedrock, the last Friday of every other month1 is Last Friday Talk – or LFT.\n\nThis event encourages sharing: technical topics, less technical topics, cross-team topics…\nWe also take the opportunity to meet and talk with colleagues we don’t work...",
  "content"  : "At Bedrock, the last Friday of every other month1 is Last Friday Talk – or LFT.\n\nThis event encourages sharing: technical topics, less technical topics, cross-team topics…\nWe also take the opportunity to meet and talk with colleagues we don’t work with daily. And the post-LFT snack, put aside during COVID but which I hope will come back, is also meant for that!\nFinally, these LFTs are an excellent opportunity for beginner speakers to practice in front of a friendly audience. We also use them to rehearse and validate talks we will give in public later.\n\nDuring the COVID full-remote period, we moved our LFTs to Google Live Stream, where each speaker shared their screen and showed their face through the camera.\nWith the return to the office2 and partial telecommuting, a new problem arose: how can we broadcast the talks given in our auditorium to 150+ remote colleagues? In good quality, to encourage people to attend several talks in a row? While remaining enjoyable and lively on-site?\n\nThe goal: to broadcast live\n\nWe can seat up to 70 people in our amphitheater, where most of our speakers were giving their talks3. And we were aiming for 100 to 200 people remotely, for whom we wanted to broadcast live4.\n\nFor the broadcast, we worked with one of our usual tools: the organizers and speakers broadcast in a Google Meet, and the audience follows the stream via Google Live Stream5. This solution supports a large number of participants, and access is filtered through our SSO, ensuring that only our colleagues have access to the stream.\n\nWe thought we were going to do this in a bit of an ugly way, like when we were all telecommuting: each speaker joins the Google Meet, shares their screen, and speaks into their microphone, often the headset provided by the company. But, still… We decided to make an effort and try to provide a better experience for our colleagues attending LFT remotely!\n\nSo, how we did it, this time\n\nHow did we capture and stream slides (or even code and video!), audio, and video of each speaker?\n\nFirst step: our amphitheater\n\nLet’s start from the room where we broadcast: our amphitheater.\n\n\n\nYou can see the room below. This photo is taken6 from the first row of the bleachers, on the right on the picture above:\n\n\n\nSo, the configuration of the room:\n\n\n  a speaker;\n  a lectern to put his or her PC;\n  from the HDMI output of this PC (possibly via a USB-C to HDMI adapter), we connect an HDMI cable that goes to the two projectors in the room;\n  these two projectors project (the same thing) on two screens, on the left and on the right of the speaker.\n\n\nBehind a screen, we placed a table for the capture and broadcast computer7. This way, it is not too far from the speaker8, without being visible from the audience.\n\nEquipment:\n\n\n  speaker’s laptop, placed on the lectern;\n  USB-C to HDMI adapter;\n  HDMI cable;\n  two projectors (fixed to the ceiling of the room);\n  two screens (fixed to the ceiling of the room).\n\n\nThis setup allows for efficient presentation in the room… But we haven’t started working on the live stream yet.\n\nA PC to manage the video and audio\n\nTo broadcast live, the speaker could join the Google Meet and share their screen. That’s what we were doing during the COVID… But we can also do much better!\n\nLet’s install a PC on the table behind the screen. It will broadcast, to the Google Meet, the video generated by OBS’ virtual camera – a free and open-source video recording and streaming software.\n\nA second PC, also placed on this table, is used to watch the Google Live Stream watched by all our remote colleagues. This feedback helps us validate that everything is working well, even if we suffer from a few dozen seconds of lag.\n\nEquipment:\n\n\n  a table, not too visible from the room;\n  a PC (laptop) – the one used this time had two USB-A ports and one USB-C port, which affects the cables/adapters required later on;\n  its charger and, perhaps, an extension cord;\n  a good quality internet connection;\n  a second PC (laptop) to view the live stream;\n  and a headset to listen to the live stream.\n\n\nVideo capture of the speaker’s slides / screen\n\nWe want to broadcast to the live stream what the speaker is projecting in the room. To do this, we place an HDMI capture box between her PC and the projector:\n\n\n\nThis capture box has an HDMI input (= the cable coming out of the speaker’s PC), an HDMI output (= the cable going to the projector) and a USB-C output (= the cable going to the control / broadcast PC).\n\n\n\nOn the control / broadcast PC, the capture box connected via USB is recognized as a webcam. It is then used as a video input device in OBS.\n\nHardware:\n\n\n  additional HDMI cable;\n  capture device + passthrough: Elgato HD60 S+;\n  USB-C to USB-A cable;\n  USB-A extension cable (because the control desk is a bit far).\n\n\nThe video recording of the room\n\nIt would be even cooler if the remote audience could see the speaker! As a matter of fact, part of the message of each talk is transmitted by the speaker’s gestures.\n\nSo let’s position a camera in the room, at the level of the audience sitting in the stands, to give the impression the speakers are looking at the camera when they are looking at the audience.\n\n\n\nWe did not have a real camera at hand. So we used an iPhone 11 Pro Max9. It has only a Lightning port… And, with an adapter, we can connect an HDMI cable. And the Filmic Pro10 application knows how to stream a clean HDMI11 video.\n\nTo capture the HDMI signal from the iPhone to the PC, we used a second capture box, simpler than the previous one: it has an HDMI input (= to connect the iPhone) and a USB output (= connected to the PC):\n\n\n\nFearing the iPhone’s battery wouldn’t last all day while filming, we powered it via an external battery12. The Lightning-to-HDMI adapter happens to include a lightning plug for power.\n\nHardware:\n\n\n  iPhone 11 Pro Max (another high-end model less than four or five years old would have done the trick as well);\n  lightning to HDMI adapter, with Lightning input for power supply;\n  10m HDMI cable (to reach the control PC);\n  external battery;\n  HDMI capture box: Elgato Camlink 4K;\n  USB cable (between camlink and PC);\n  USB to lightning cable (between the battery and the adapter connected to the iPhone).\n\n\nThe capture box is recognized as a USB webcam. We use it as a video input device in OBS.\n\nWe first put the camera on the side of the room, to not disturb. But the speakers never looked in its direction, and it was not very nice for the remote audience, which felt less included. So we moved the camera almost in front of the speaker. This way, the remote audience feels more like the person speaking is looking in their direction.\nAlso, the camera that films the speaker also films a screen, including when the speaker wants to point to a part of what they are presenting.\n\nAudio recording of the speaker\n\nOur amphitheater is equipped with microphones and speakers, but we don’t yet know how to get the sound from this audio system to inject it into a live stream.\n\nWith the resources and time we had, the best we could do for the speakers was a wireless lapel mic:\n\n\n\nWe didn’t have a wireless mic, but we had something that could act as such, and we ended up with a 3.5” jack to plug into the microphone port of the control PC. This microphone is then used as an audio source in the Google Meet.\n\nEquipment:\n\n\n  Lapel microphone with TRRS Jack 3.5 port: RODE SmartLav+;\n  TRRS female to TRS male adapter (between the microphone and the transmitter box);\n  wireless transmitter + receiver kit (TRS 3.5 jack input on transmitter, TRS 3.5 jack output on receiver): RODE Wireless Go (1st generation);\n  TRS 3.5 male to male cable (between the receiver box and the PC).\n\n\nSome other points and nice “bonuses”\n\nOf course, we didn’t stop at these main points, and lots of other little things came into play throughout the day.\n\nRemote speakers?\n\nI wrote above that I wouldn’t talk about it, and I lied a bit: one of the talks that day was given by a colleague who was working from home.\nHe joined the organizers’ Google Meet, shared his screen, activated his camera, and presented, exactly as many of us have done during more than a year of forced telecommuting because of COVID.\nFor those in the auditorium, one of the organizers plugged in his computer to the two screens in the room and its sound system.\n\nTransitions between sessions\n\nAt the end of each talk, we quickly took over the speaker’s desk and microphone. We wanted to set up the next person, plug in their PC and position the mic so as not to interfere with them, explain how we were broadcasting and validate the setup and configuration.\nAn organizer was in charge of managing the transition, indicating at what time we would resume. But this transition, in the room, was done without a microphone – and therefore, without being broadcast to the live stream.\nFor the live stream, another organizer made the same transition by joining the Google Meet of organizers and speakers. This way, our remote colleagues were not left in the dark and knew when the next talk would resume.\n\nSome useful, or even essential, utilities\n\nOf course, in a large room with few electrical outlets that are not always well placed, you need to bring extension cords and power strips.\nAlso, to avoid someone getting their feet caught in the extension cords or cables (USB, HDMI), bring a roll of duct tape to secure everything to the floor.\nAnd depending on where the lapel microphone is attached (shirt collar, shirt buttonhole…) and where its cable goes (above the shirt in front, under the shirt / under the shirt, on the shoulder and in the back…), more delicate tape13 is very handy.\n\nAt OBS level\n\nOn the control PC, in OBS, we configured several scenes to highlight the speakers or their slides. The images below are screenshots from the live stream, so in 720p maximum and much too compressed…\n\nScreen only\n\nTo display large code examples, videos…\n\n\n\nLarge screen\n\nWith speaker overlay in the bottom right corner: for slides written small, while reflecting an idea of the gestures. And we also had the same thing with the speaker inlay in another corner, for slides with important texts in the bottom right of the screen.\n\n\n\nMosaiq\n\nScreen on the left half of the screen, video of the speaker on the right half: for large written slides and to allow people at a distance to see the speaker clearly. We used this view a lot when the camera was on the side of the room – and we didn’t use it anymore once the camera was in front of the speaker.\n\n\n\nSpeaker only\n\nOnly the speaker’s video, framed to include one of the two screens. We used this view “as if the person watching the stream was sitting in the stands” for much of the afternoon, when the camera was placed in the middle of the audience.\n\n\n\nFor the practical aspects of managing the live stream\n\nChanging scenes in OBS can be done with the mouse (but it’s not convenient) or with keyboard shortcuts (but you have to remember the shortcuts and the scenes they correspond to).\n\nBut it’s much more fun with a Stream Deck: a command pad with big buttons, which you can customize! Yes, it’s a bit gimmicky and not at all essential, but it’s also very cool to use ;-)\n\nHardware:\n\n\n  Elgato Stream Deck;\n  since it connects to USB-A: an adapter or a USB hub.\n\n\n\n\nThe control desk, in photo\n\nA lot of unorganized cables… Here is what the table looked like with the control PC14:\n\n\n\nSome things to improve\n\nOf course, not everything was perfect… Still, for less than an hour and a half of installation and testing that morning, we were quite happy with the result!\n\nFirst of all, Google Live Stream: the solution, integrated to Google Workspace, is very practical. Including the aspect of “limiting access to our employees”15. However, the video quality, in 720p too compressed, is not optimal :-/.\n\nOur auditorium has two cameras fixed to the ceiling. Today, they are not yet functional16, but we hope they will soon replace the iPhone – and allow us to get a view of the audience for questions.\n\nAlso, having only one microphone is problematic when two speakers are speaking for a talk. It happened once during the day, and we put the lapel mic on the lectern and asked the speakers to move closer to it when it was their turn to speak. The RODE SmartLav+ being omni-directional and of good quality, it was just about right… Without being optimal. Also, we didn’t have a microphone for the questions, so the speakers had to repeat them17.\nWe have two wireless microphones in the auditorium. Currently, they are used to amplify the sound in the room and we didn’t have time in the morning when we were setting up to figure out how to capture their output and integrate it into the live stream. Maybe an improvement for next time, because they seem to be pretty good ;-)\nAlso, we didn’t have the time18 to talk with our colleagues who were managing the sound of these events before the COVID: somewhere in a closet, we have a physical mixer, which could have been useful! Another area for improvement!\n\nThat said, now that we’ve seen that it is possible, we want to do even better next time, in two months ;-). And, clearly, a prototype in 1h30 of setup, which worked that well all day long and which improved the experience of 150 or more remote colleagues, is a big success which will push us to do better - and we know we will be able to do so!\nAnd then, with time and successive improvements, we may spend less time each time installing and configuring!\nAlso, a more turnkey solution – like Streamyard19 – might make our lives easier, compared to OBS…\n\nEvery two months, we organize a LFT day at Bedrock: our Last Friday Talks.\nAfter COVID and with the partial return to the office, we wanted our remote colleagues to experience this LFT as best as they could.\nFor this iteration, we did with what we had. It’s up to us to iterate and do even better next time!\nAnd you, how do you share these kinds of events with your remote colleagues?\n\n  \n    \n      Historically, it was the afternoon of the last Friday of every month. We revised the format in 2019 to make it a full day every other month. &amp;#8617;\n    \n    \n      In France, some companies – including ours – started returning to the office, full-time or not, in June 2021, following recommendations from the government. &amp;#8617;\n    \n    \n      One of the talks was given by a remote speaker, I will not talk much about it in this article: he joined the Google Meet of the organizers and shared his camera and screen. An organizer then projected the Google Meet into the auditorium for the live audience. &amp;#8617;\n    \n    \n      We went up to 150 people watching the stream simultaneously. &amp;#8617;\n    \n    \n      Google Live Stream. We use this solution because Google Meet alone does not support enough people in a call. &amp;#8617;\n    \n    \n      Photo taken while projecting talks of the Demuxed conference, for those who wanted to see them on a big screen and together… &amp;#8617;\n    \n    \n      In the photo above, we see the legs of the table and high chairs placed around it. Yes, this space also serves as a break room. &amp;#8617;\n    \n    \n      And the organizers can tap them on the shoulder, to tell them to start their talk or ask them to take a break in case of technical problems… &amp;#8617;\n    \n    \n      High-end smartphones have had very good quality cameras for several years. An iPhone 11 is more than enough to film and broadcast a live conference. &amp;#8617;\n    \n    \n      Filmic Pro: a not cheap application, but very good when it comes to filming with an iPhone. &amp;#8617;\n    \n    \n      Clean HDMI: the filmed video, in real time, without the decorations of the application interface. &amp;#8617;\n    \n    \n      External battery: because there are no electrical outlet in this part of the room and we didn’t want to add more cables and extension cords. &amp;#8617;\n    \n    \n      I use medical tape, which is easy to cut, sticks well and comes off easily too; and is not too aggressive with clothes and skin. &amp;#8617;\n    \n    \n      Yes, it’s a bit of a mess… And, no, you can’t see everything: when we took this picture, we didn’t think we would post it ^^ &amp;#8617;\n    \n    \n      We may broadcast some talks in public in the future… But it was not a topic for this time and we know that some topics will remain internal no matter what. &amp;#8617;\n    \n    \n      We moved in recently and other more important items were configured first. &amp;#8617;\n    \n    \n      A person in the control room tapped the speakers on the shoulder when they forgot to repeat the questions… &amp;#8617;\n    \n    \n      We should have done it a few days in advance, not the morning itself… &amp;#8617;\n    \n    \n      Which we’ve already used at conferences, both as organizers and speakers. &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Best practices for Web application maintenance",
    "category" : "",
    "tags"     : " js, react, web, frontend",
    "url"      : "/2021/09/06/web-best-practices.html",
    "date"     : "September 6, 2021",
    "excerpt"  : "\n  How not to throw away your application every two years?\n\n\nFeedback based on best practices applied to the web platform developed at Bedrock Streaming\n\nA bit of context\n\nAt Bedrock Streaming many teams develop and maintain frontend applications ...",
  "content"  : "\n  How not to throw away your application every two years?\n\n\nFeedback based on best practices applied to the web platform developed at Bedrock Streaming\n\nA bit of context\n\nAt Bedrock Streaming many teams develop and maintain frontend applications for our customers and users.\nSome of those applications are not very young.\nIn fact, the application I’m mainly working on is a website whose developments started in 2014.\nI have already mentioned it in different articles of this blog.\n\n\n\nYou might think: “Oh poor people, maintaining an almost 10 year old application must be hell!”\n\nDon’t worry, it’s not the case!\nI have worked on projects that are much less old but where the development of new features was much more painful.\n\nToday the project is technically up to date, we must be on the latest version of React while it had started on a version 0.x.x.\nIn this world of often criticized web technologies where tools and practices are constantly evolving (eg: the many articles on the Javascript Fatigue), to keep a project “up to date” remains a real challenge.\n\n\n\nMoreover, in the context of this project, in almost 10 years, we have had about 100 contributors.\nSome have only stayed a few months/years.\nHow can we keep the maximum knowledge on “How we do things and how it works?” in such a moving human context?\n\n\n\nThis is what I would like to demonstrate in this post.\n\nWith the help of my colleagues, I have collected the list of good practices that still allow us to maintain this project today.\nWith Florent Dubost, we have often thought that it would be interesting to publish it.\nWe hope you will find it useful.\n\nSet rules and automate them\n\nA project that stands the test of time is first and foremost a set of knowledge that is stacked one on top of the other.\nIt’s like the Kapla tower you used to build as a child, trying to get as high as possible.\nA solid base on which we hope to add as much as possible before a potential fall.\n\nFrom the beginning of a project, we have to make important decisions about “How do we want to do things?\nWe think for example about “What format for our files? How do we name this or that thing?”\nWriting accurate documentation of “How we do things” might seem like a good idea.\n\nHowever, documentation is cool, but it tends to get outdated very quickly.\nOur decisions evolve, but documentation does not.\n\n\n  “Times change but not READMEs.”\n\n  Olivier Mansour (deputy CTO at Bedrock)\n\n\nAutomating the checking of each of the rules we impose on ourselves (on our codebase or our processes) is much more durable.\nTo make it simple, we avoid as much as possible to say “We should do things like that”, and we prefer “we’ll code something that checks it for us”.\nOn top of that, on the JS side we are really well equipped with tools like Eslint that allow us to implement our own rules.\n\nSo the reflex we try to adopt is the following:\n\n\n  “We should try to do it like this now!”\n  “Ok that’s interesting, but how can we make sure we do it like that automatically with our CI (Continuous Integration)?”\n\n\nContinuous Integration of a project is the perfect solution to not miss anything on every Pull Request we provide.\nReviews are only easier because you don’t have to worry about all the rules that are already automated.\nIn this model, the review is more for knowledge sharing than for typo copying and other non-compliance with the project conventions.\n\nIn this principle, we must therefore try to banish oral rules.\nThe time of the druids is over, if all the good practices of a project have to be transmitted orally, it will only take longer to guide new developers into your team.\n\n\n\nA project is not set in stone. These rules evolve with time.\nIt is therefore preferable to add rules that have a script that will autofix the whole codebase intelligently.\nMany Eslint rules offer this, and it is a very important selection criteria when choosing new conventions.\n\neslint --fix\n\n\nA very strict rule that will force you to modify your code manually before each push is annoying in the long run and will annoy your teams.\nWhereas a rule (even a very strict one) that can auto-fix itself at commit time will not be seen as annoying.\n\nHow to decide to add new rules ?\n\nThis question may seem thorny, take for example the case of &amp;lt;tab&amp;gt; / &amp;lt;space&amp;gt; in files.\nFor this, we try to avoid the endless debates and follow the trend and rules of the community.\nFor example, our Eslint configuration base is based on Airbnb’s which seems to have some success in the JS community.\nBut if the rule we want to impose on ourselves is not available in Eslint or other tools, we sometimes prefer not to follow the rule rather than say “We’ll do it without a checking CI”.\n\nThe almost exhaustive list 🤞\n\n\n\n\n  The file format is managed by Editorconfig, prettier and Eslint.\nWe have opensourced our own configuration, if it is of any use to you.\n  We use a specific commit name to generate our changelog.\nTo make sure devs follow it, a simple step in our CI checks it.\n  We don’t want a dev to make our JS bundles very big in production, so we track and measure their size in the CI.\nWe use an in-house tool but we recommend to use the BuildTracker tool.\n  Test coverage is not an indicator for the team, not all lines have the same need for us to be tested.\nSome teams at Bedrock however follow this indicator which at least has the interest to give a trend.\n  Our unit tests obviously run on the CI, these must pass.\n  Our functional tests (End to end: E2E) run on Chrome Headless, they must be green.\n  The logs of our E2E tests are retrieved and parsed to avoid errors or React warnings (the parsing script is however complicated to maintain)\n  Functional tests run in a sandbox where the whole network is proxied.\nWe make sure that our tests do not depend on a non mocked API that could slow down their execution.\n  During the E2E tests we check that no image request has generated a 404.\n  We perform some accessibility checks with Axe during our E2E tests.\n  We check some rules on the CSS with Stylelint and bemlinter (we don’t use BEM anymore but there is still some style managed in SCSS that we migrate little by little in StyledComponent)\n  The project is a monorepo on which we try to maintain the same dependencies versions for each package.\nWe developed a tool which automates this check: monorepo-dependencies-check\n  We check that our yarn.lock file has not been inadvertently modified or that it has been updated with respect to the modifications of the package.json.\n  Terraform is used to manage our cloud resources, we check that the file format is correct.\n\n\nTest, test, test\n\nI hope that in 2021 it is no longer necessary to explain why automatic testing of your application is essential to make it sustainable.\nIn JS, we are rather well equipped in terms of testing tools today.\nHowever, the eternal question remains:\n\n\n  “What do we want to test?”\n\n\nGlobally if we search on the internet this question, we see that different needs make emerge very different practices and testing tools.\nIt would be very presumptuous to think that there is a good way to automatically test your application.\nThis is why it is preferable to define one or more test strategies that meet defined and limited needs.\n\nOur test strategies are based on two distinct goals:\n\n\n  To automate the verification of the functionalities proposed to the users by putting ourselves in their place.\n  To provide us with efficient solutions to specify the way we implement our technical solutions to allow us to make them evolve more easily.\n\n\nTo do this, we perform two “types of tests” that I propose to present here.\n\nOur E2E tests\n\nWe call them “functional tests”, they are End-to-end (E2E) tests on a very efficient technical stack composed of CucumberJS, WebdriverIO with ChromeHeadless\nThis is a technical stack set up at the beginning of the project (at the time with PhantomJS for the oldest among you)\n\nThis stack allows us to automate the piloting of tests that control a browser.\nThis browser will perform actions that are as close as possible to what our real users can do while checking how the site reacts.\n\nA few years ago, this technical stack was rather complicated to set up, but today it is rather simple to do.\nThe site that hosts this blog post is itself proof of this.\nIt only took me about ten minutes to set up this stack with the WebdriverIo CLI to verify that my blog is working as expected.\n\nI recently published an article presenting the implementation of this stack.\n\nSo here is an example of an E2E test file to give you an idea:\n\nFeature: Playground\n\n  Background: Playground context\n    Given I use &quot;playground&quot; test context\n\n  Scenario: Check if playground is reachable\n    When As user &quot;toto@toto.fr&quot; I visit the &quot;playground&quot; page\n    And I click on &quot;playground trigger&quot;\n    Then I should see a &quot;visible playground&quot;\n    And I should see 4 &quot;playground tab&quot; in &quot;playground&quot;\n\n    When I click on &quot;playground trigger&quot;\n    Then I should not see a &quot;visible playground&quot;\n\n    # ...\n\n\nAnd it looks like this in local with my Chrome browser!\n\n\n\nHere is a diagram that explains how this stack works:\n\n\n\nToday, Bedrock’s web application has over 800 E2E test cases running on each of our Pull Request and the master branch.\nThey assure us that we are not introducing any functional regression and that’s just great!\n\n👍 The positives\n\n\n  WebdriverIO also allows us to run these same tests on real devices on a daily basis through the paid SAAS service Browserstack.\nSo we have every day a job that makes sure that our site works correctly on a Chrome last version on Windows 10 and Safari on MacOs.\n  These tests allow us to easily document the functionality of the application using the Gherkin language.\n  They allow us to reproduce cases that are far from nominal.\nIn a TDD logic, they allow us to advance on the development without having to click for hours.\n  These tests allowed us not to break the old version of the site which is still in production for some customers while our efforts are concentrated on the new one.\n  They give us real confidence.\n  Thanks to our library superagent-mock, we can fixturer (plug, mock) all the APIs we depend on and thus even check the error cases.\nAlso, mocking the browser’s XHR layer allows for a significant improvement in test execution time. 🚀\n  They give us access to extended uses like:\n    \n      checking accessibility rules\n      check the browser console logs (to avoid introducing errors or React Warning for example)\n      monitoring all network calls of the site through a proxy\n      and so on…\n    \n  \n\n\n👎 The complications\n\n\n  Maintaining this stack is complicated and expensive.\nSince few resources are published on this domain, we sometimes find ourselves digging for days to fix them 😅.\nSometimes we feel quite alone in having these worries.\n  It is very easy to code a so-called flaky E2E test (ie: a test that can fail randomly).\nThey make us think that something is broken.\nThey sometimes take us a long time to stabilize.\nIt is still much better to remove a test that will not give you a stable result.\n  Running all the tests takes a lot of time on our continuous integration.\nWe must regularly work on their optimization so that the feedback they provide you is as fast as possible.\nThese important times also cost money, because we have to run these tests on machines.\nFor your information, the infrastructure of the website (just the hosting of our Node servers + static files + CDN) cost much less than our continuous integration.\nThis obviously makes our Ops team smile! 😊\n  The new recruits in our teams have often never done this kind of testing, so there is a struggle phase of learning…\n  Some features are sometimes too complicated to test with our E2E stack (for example, payment paths that depend on third parties).\nSo we sometimes fall back on other techniques with Jest, especially with a less unitary scope.\n\n\nOur “unit” tests\n\nTo complete our functional tests we also have a stack of tests written with Jest.\nWe call these tests unit tests because we have as a principle to try to always test our JS modules independently from the others.\n\nLet’s not debate here about “Are these real unit tests?”, there are enough articles on the internet about this topic.\n\nWe use these tests for different reasons that cover needs that our functional tests do not cover:\n\n\n  to help us develop our JS modules with TDD practices.\n  to document and describe how a JS module works.\n  test very/too complicated edge cases with our E2E tests.\n  facilitate the refactoring of our application by showing us the technical impacts of our modifications.\n\n\nWith these tests, we put ourselves at the level of a utility function, a Redux action, a reducer, a React component.\nWe rely mainly on the automock functionality of Jest which allows us to isolate our JS modules when we test.\n\n\n\nThe previous image represents the metaphor that allows us to explain our unit testing strategy to newcomers.\n\n\n  You have to imagine that the application is a wall made of unit bricks (our ecmascript modules), our unit tests must test one by one the bricks in total independence from the others.\nOur functional tests are there to test the cement between the bricks.\n\n\nTo summarize, we could say that our E2E tests test what our application should do, and our unit tests make sure to check how it works.\n\nToday there are more than 6000 unit tests that cover the application and allow to limit regressions.\n\n👍\n\n\n  Jest is really a great library, fast, complete, well documented.\n  Unit tests help us a lot to understand several years later how it all works.\n  We always manage to unit test our code, and it complements our E2E tests well.\n  The automock feature is really handy for breaking down tests by modules.\n\n\n👎\n\n\n  Sometimes we found ourselves limited by our E2E test stack and couldn’t rely solely on unit tests.\nWe were missing something to be able to make sure that the cement between the bricks worked as we wanted it to.\nFor this, a second test stack Jest was set up called “integration test” where the automock is disabled.\n  The abuse of Snapshot is dangerous for your health.\nThe use of “Snapshot testing” can save time on the implementation of your tests but can reduce the quality.\nHaving to review a 50 line object in Snapshot is neither easy nor relevant.\n  With the depreciation of EnzymeJS, we are forced to migrate to React Testing Library.\nIt is of course possible to unit test components with this new library.\nUnfortunately, this is not really the spirit and the way to do it.\nReact Testing Library pushes us not to play with shallow rendering.\n\n\nOur principles\n\nWe try to always follow the following rules when asking the question “Should I add tests?”.\n\n\n  If our Pull Request introduces new user features, we need to integrate E2E test scenarios.\nUnit tests with Jest can complete / replace them accordingly.\n  If our Pull Request aims to fix a bug, it means that we are missing a test case.\nWe must therefore try to add an E2E test or, failing that, a unit test.\n\n\nIt is while writing these lines that I think that these principles could very well be automated. 🤣\n\nThe project stays, the features don’t\n\n\n  “The second evolution of a feature is very often its removal.”\n\n\nAs a matter of principle, we want to make sure that every new feature in the application does not base its activation on simply being in the codebase.\nTypically, the lifecycle of a feature in a project can be as follows (in a Github Flow):\n\n\n  a person implements on a branch\n  the feature is merged on master\n  it is deployed in production\n  lives its feature life (sometimes with bugs and fixes)\n  the feature is not needed anymore\n  a person unravels the code and removes it\n  new deployment\n\n\nTo simplify some steps, we have implemented feature flipping on the project.\n\nHow does it work?\n\nIn our config there is a map key/value that lists all the features of the application associated with their activation status.\n\nconst featureFlipping = {\n  myAwesomeFeature: false,\n  anotherOne: true,\n}\n\n\nIn our code, we have implemented conditional treatments that say “If this feature is activated then…”.\nThis can change the rendering of a component, change the implementation of a Redux action or disable a route in our react-router.\n\nBut what’s the point?\n\n\n  We can develop new evolutions progressively by hiding them behind a configuration key.\nWe deliver features in production without activating them.\n  In a test environment, we can overload this config to test features that are not yet activated in production.\n  In the case of a white label site, we can propose these features to our customers as possible options.\n  Before deleting code of a feature, we deactivate it and clean it up without risk.\n  Thanks to an in-house tool called Applaunch, this feature flipping config can be overloaded on time in a GUI without deployment.\nThis allows us to activate features without putting the code into production.\nIn the event of an incident, we can deactivate features that have been degraded.\n\n\nTo give you a more concrete example, between 2018 and 2020 we completely overhauled the application’s interface.\nThis graphical evolution was just a featureFlipping key.\nThe graphical redesign was not a reset of the project, we still live with both versions (as long as the switchover of all our customers is not completed).\n\n\n\nA/B testing\n\nThanks to the great work of the backend and data teams, we were even able to extend the use of feature flipping by making this configuration modifiable for sub-groups of users.\n\nThis allows us to deploy new features on a smaller portion of users in order to compare our KPI.\n\nDecision making, technical or product performance improvement, experimentation, the possibilities are numerous and we exploit them more and more.\n\nThe future flipping.\n\n\n  Based on an original idea by Florent Lepretre.\n\n\nWe regularly had the need to activate features at very early hours in the future.\nFor that we had to be connected at a precise time on our computer to modify the configuration on the fly.\n\nTo avoid forgetting to do this, or doing it late, we made sure that a configuration key could be activated from a certain date.\nTo do this, we evolved our Redux selector which indicated if a feature was activated so that it could handle date formats and compare them to the current time.\n\nconst featureFlipping = {\n  myAwesomeFeature: {\n    offDate: &#39;2021-07-12 20:30:00&#39;,\n    onDate: &#39;2021-07-12 19:30:00&#39;,\n  },\n}\n\n\n\n  Many coffees ☕️ at 9am have been saved by future flipping.\n\n\nMonitor, Measure, Alert\n\nTo maintain a project as long as Bedrock’s web application, testing, documentation and rigor are not enough.\nYou also need visibility on what works in production.\n\n\n  “How do you know that the application you have in production right now is working as expected?”\n\n\nWe assume that no functionality works until it is monitored.\nToday, monitoring in Bedrock on the frontend side takes the form of different tools and different stacks.\nWe rely on NewRelic, Statsd, a ELK stack and even Youbora for the video streaming part.\n\nTo give you an example, each time a user starts a browsing session we send an anonymous monitoring Hit to increment a counter in Statsd.\nWe then have to define a dashboard that displays the evolution of this number in a graph.\nIf we observe a too important variation, it can allow us to detect an incident.\n\n\n\nMonitoring also offers us solutions to understand and analyze a bug that occurred in the past.\nUnderstanding an incident, explaining it, finding its root cause are the possibilities that are open to you if you monitor your application.\nMonitoring can also allow you to better communicate with your customers about the impact of an incident and also to estimate the number of impacted users.\n\nWith the multiplication of our customers, monitoring our platforms well is not enough.\nToo much data, too many dashboards to monitor, it becomes very easy to miss something.\nSo we started to complement our metrics monitoring with automatic alerting.\nOnce we have enough confidence in the metrics, we can easily set up alerts that will warn us if there is an inconsistent value.\n\nHowever, we try to always trigger alerts only when it is actionable.\nIn other words, if an alert sounds, we have something to do.\nSounding alerts that do not require immediate human action generates noise and wastes time.\n\n\n\nLimit, monitor and update your dependencies\n\nWhat goes out of date faster than your shadow in a web project based on Javascript technologies are your dependencies.\nThe ecosystem evolves rapidly and your dependencies can quickly become unmaintained, out of fashion or completely overhauled with big breaking changes.\n\nWe therefore try as much as possible to limit our dependencies and avoid adding them unnecessarily.\nA dependency is often very easy to add but it can become a real headache to remove.\n\nThe graphic component libraries (e.g. React bootstrap, Material Design) are a good example of dependencies that we do not want to introduce.\nThey can make integration easier at first, but they often freeze the version of your component library later on.\nYou don’t want to freeze the React version in your application for two form components.\n\nMonitoring is also part of our dependency management routines.\nSince the addition of reporting security flaws in an NPM package, it is possible to know if a project has a dependency that contains a known security flaw with a simple command.\nSo we have daily jobs on our projects that run the yarn audit command to force us to apply patches.\n\n\n  Dependency maintenance is greatly facilitated by our E2E test stack which sounds the alarm if the version upgrade generates a regression.\n\n\nToday, except for security flaws, we update our dependencies “when we have time”, often at the end of sprint.\nWe are not satisfied with this because some dependencies can be forgotten.\nI personally use tools like yarn outdated and Dependabot on my personal projects to automate the update of my dependencies.\n\nAccepting your technical debt\n\nA project will always accumulate technical debt.\nThis is a fact.\nWhether it is voluntary or involuntary debt, a project that resists the years will inevitably accumulate debt.\nEven more so, if during all these years you keep adding features.\n\nSince 2014, our best practices, our ways of doing things have evolved well.\nSometimes we decided these changes but sometimes we underwent them (an example, the arrival of functional components with React and the Hooks api).\n\nOur project is not completely “state of art” and we assume it.\n\n\n\nWe try to prioritize our refactoring topics on the parts of the application on which we have the most concern, the most pain.\nWe consider that a part of the application that we don’t like but on which we don’t need to work (bring evolutions) doesn’t deserve that we refactor it.\n\nI could name many features of our application that have not evolved functionally for several years.\nBut since we have covered these features with E2E tests since the beginning, we didn’t really have to touch them.\n\nAs said above, the next evolution of a code feature is sometimes its deactivation.\nSo why spend time rewriting the whole application?\n\n\n  In any case, the code becomes “legacy”.\n  As long as the features are tested, nothing obliges us to refactor everything permanently so that our entire codebase is state of art.\n  We focus on our pain points, we re-factor what we really need to evolve.\n\n\nTo summarize\n\nThe best practices presented here are obviously subjective and will not be perfectly/directly applicable in your contexts.\nHowever, I am convinced that they can probably help you identify what can make your project go from fun to stale.\nAt Bedrock we have other practices in place that I haven’t listed here but that will be the occasion for a new article sometime.\n\nFinally, if you want me to go into more detail on some of the chapters presented here, don’t hesitate to tell me, I could try to dedicate a specific article to it.\n\n"
} ,
  
  {
    "title"    : "Bonnes pratiques pour la maintenance d&#39;une application web",
    "category" : "",
    "tags"     : " js, react, web, frontend",
    "url"      : "/2021/09/01/bonnes-pratiques-web.html",
    "date"     : "September 1, 2021",
    "excerpt"  : "\n  Comment ne pas jeter son application tous les deux ans ?\n\n\nRetour d’expérience basé sur les bonnes pratiques appliquées à la plateforme web développée chez Bedrock Streaming\n\nUn peu de contexte\n\nChez Bedrock Streaming de nombreuses équipes déve...",
  "content"  : "\n  Comment ne pas jeter son application tous les deux ans ?\n\n\nRetour d’expérience basé sur les bonnes pratiques appliquées à la plateforme web développée chez Bedrock Streaming\n\nUn peu de contexte\n\nChez Bedrock Streaming de nombreuses équipes développent et maintiennent des applications frontend pour nos clients et utilisateurs.\nCertaines ne sont pas toute jeune.\nEn effet, l’application sur laquelle je travaille principalement est un site web dont les développements ont commencé en 2014.\nJe l’ai d’ailleurs déjà évoquée dans différents articles de ce blog.\n\n\n\nVous pourriez vous dire: “Oh les pauvres maintenir une application vieille de presque 10 ans ça doit être un enfer !”\n\nRassurez-vous, ce n’est pas le cas !\nJ’ai travaillé sur des projets bien moins vieux mais sur lesquels le développement de nouvelles fonctionnalités était bien plus pénible.\n\nAujourd’hui le projet reste à jour techniquement, on doit être sur la dernière version de React alors que celui-ci avait commencé sur une version 0.x.x.\nDans ce monde des technologies web souvent décrié (ex: les nombreux articles sur la Javascript Fatigue) dont les outils et les pratiques évoluent constamment, conserver un projet “à jour” reste un vrai challenge.\n\n\n\nDe plus, dans le contexte de ce projet, en presque 10 ans, nous avons connu une centaine de contributeurs.\nCertains ne sont restés que quelques mois/années.\nComment garder au maximum la connaissance sur “Comment on fait les choses et comment ça marche ?” dans un contexte humain si mouvant ?\n\n\n\nC’est ce que je vous propose de vous présenter.\n\nAvec l’aide de mes collègues, j’ai rassemblé la liste des bonnes pratiques qui nous permettent encore aujourd’hui de maintenir ce projet en état.\nAvec Florent Dubost, on s’est souvent dit qu’il serait intéressant de la publier.\nNous espèrons que cela vous sera utile.\n\nS’imposer des règles et les automatiser\n\nUn projet qui résiste au temps c’est tout d’abord un ensemble de connaissances qu’on empile les unes sur les autres.\nC’est en quelque sorte la tour de Kapla que vous assembliez petit en essayant d’aller le plus haut possible.\nUne base solide sur laquelle on espère pouvoir ajouter le plus possible avant une potentielle chute.\n\nDès le début d’un projet on est donc amené à prendre des décisions importantes sur “Comment on souhaite faire les choses ?”.\nOn pense par exemple à “Quel format pour nos fichiers ? Comment on nomme telle ou telle chose ?”\nÉcrire une documentation précise de “Comment on fait les choses” pourrait paraitre une bonne idée.\n\nCependant la documentation c’est cool, mais ça a tendance à périmer très vite.\nNos décisions évoluent mais pas la documentation.\n\n\n  “Les temps changent mais pas les README.”\n\n  Olivier Mansour (deputy CTO à Bedrock)\n\n\nAutomatiser la vérification de chacune des règles qu’on s’impose (sur notre codebase ou nos process) est bien plus pérenne.\nPour faire simple, on évite dans la mesure du possible de dire “On devrait faire les choses comme cela”, et on préfère “on va coder un truc qui nous le vérifie à notre place”.\nEn plus de ça, coté JS on est vraiment bien équipé avec des outils comme Eslint qui nous permettent d’implémenter nos propres règles.\n\nLe réflexe qu’on essaie donc d’adopter est donc le suivant:\n\n\n  “On devrait essayer de faire comme cela à présent !”\n  “Ok c’est intéressant, mais comment peut-on s’assurer qu’on fasse comme cela automatiquement avec notre CI (Intégration continue) ?”\n\n\nL’intégration continue d’un projet est la solution parfaite pour ne rien louper sur chacune des Pull Request que nous proposons.\nLes reviews n’en sont que plus simples car vous n’avez plus à vous soucier de l’ensemble des règles qui sont déjà automatisées.\nDans ce modèle, la review sert donc plus au partage de connaissance qu’au flicage de typo et autre non respect des conventions du projet.\n\nDans ce principe, il faut donc essayer de bannir les règles orales.\nLe temps des druides est terminé, s’il faut transmettre oralement toutes les bonnes pratiques d’un projet, l’accompagnement de nouveaux développeurs dans votre équipe n’en sera que plus long.\n\n\n\nUn projet n’est pas figé. Ces règles évoluent donc avec le temps.\nOn préfèrera alors l’ajout de règles qui possèdent un script qui autofixera toute la codebase intelligemment.\nDe nombreuses règles Eslint le proposent, et cela est vraiment un critère de sélection très important dans nos choix de nouvelles conventions.\n\neslint --fix\n\n\nUne règle très stricte qui vous obligera à modifier votre code manuellement avant chaque push est pénible à la longue et énervera vos équipes.\nAlors qu’une règle (même très stricte) qui peut s’autofixer automatiquement au moment du commit ne sera pas perçue comme gênante.\n\nComment décider d’ajouter de nouvelles règles ?\n\nCette question peut paraitre épineuse, prenons par exemple le cas des &amp;lt;tab&amp;gt; / &amp;lt;space&amp;gt; dans les fichiers.\nPour cela, on essaie d’éviter les débats sempiternels et on se plie à la tendance et aux règles de la communauté.\nPar exemple, notre base de configuration Eslint) est basée sur celle d’Airbnb qui semble avoir un certain succès dans la communauté JS.\nMais si la règle qu’on souhaite s’imposer n’est pas disponible dans Eslint ou d’autres outils, il nous arrive de préférer ne pas suivre la règle plutôt que de se dire “On le fait sans CI qui vérifie”.\n\nLa liste presque exhaustive 🤞\n\n\n\n\n  Le format des fichiers est suivi géré par Editorconfig, prettier et Eslint.\nNous avons opensourcé notre propre configuration, si jamais celle-ci peut vous être utile.\n  Nous utilisons un nommage de commit bien spécifique pour générer nos changelog.\nPour s’assurer que les devs le respectent, une simple étape de notre CI le vérifie.\n  On ne souhaite pas qu’un dev fasse grossir énormément nos bundles JS en production, c’est pourquoi nous suivons et mesurons leur taille dans la CI.\nOn utilise un outil maison mais on peut vous recommander l’outil BuildTracker.\n  La couverture de tests n’est pas un indicateur pour l’équipe, toutes les lignes n’ont pas la même nécessité pour nous d’être testées.\nCertaines équipes à Bedrock suivent cependant cet indicateur qui a au moins l’intérêt de donner une tendance.\n  Nos tests unitaires tournent bien évidemment sur la CI, ceux-ci doivent passer.\n  Nos tests fonctionnels (End to end: E2E) tournent sur Chrome Headless, ils doivent être au vert.\n  Les logs de nos tests E2E sont récupérés et parsés afin d’éviter l’introduction d’erreur ou de React warning (Le script de parsing est cependant compliqué à maintenir)\n  Les tests fonctionnels fonctionnent dans une sandbox où tout le réseau est proxyfié.\nNous surveillons que nos tests ne dépendent pas d’une API non mockée qui pourrait ralentir leur exécution.\n  Durant les tests E2E nous vérifions qu’aucune requête d’image n’a généré une 404.\n  On réalise quelques vérifications d’accessibilité avec Axe durant nos tests E2E.\n  On vérifie quelques règles sur le CSS avec Stylelint et bemlinter (on n’utilise plus BEM aujourd’hui mais il reste encore un peu de style géré en SCSS qu’on migre petit à petit en StyledComponent)\n  Le projet est un monorepo sur lequel nous essayons de maintenir les mêmes versions de dépendances pour chaque package.\nPour cela nous avons développé un outil qui permet de faire cette vérification monorepo-dependencies-check\n  On vérifie que notre fichier yarn.lock n’a pas été modifié par inadvertance ou bien qu’il a été mis à jour par rapport aux modifications du package.json.\n  Terraform est utilisé pour la gestion de nos ressources cloud, nous vérifions que le format des fichiers est correct.\n\n\nTester, tester, tester\n\nJ’espère qu’en 2021 il n’est plus nécessaire d’expliquer pourquoi tester automatiquement son application est indispensable pour la rendre pérenne.\nEn JS on est plutôt bien équipé en terme d’outils pour tester aujourd’hui.\nIl reste cependant l’éternelle question:\n\n\n  “Qu’est-ce qu’on veut tester ?”\n\n\nGlobalement si on recherche sur internet cette question, on voit que des besoins différents font émerger des pratiques et des outils de testing bien différents.\nCe serait très présomptueux de penser qu’il y a une bonne manière de tester automatiquement son application.\nC’est pourquoi il est préférable de définir une ou plusieurs stratégies de test qui répondent à des besoins définis et limités.\n\nNos stratégies de tests reposent sur deux volontés bien distinctes:\n\n\n  Automatiser la vérification des fonctionnalités proposées aux utilisateurs en se mettant à sa place.\n  Nous fournir des solutions efficaces pour specifier la manière dont nous implémentons nos solutions techniques pour nous permettre de les faire évoluer plus facilement.\n\n\nPour cela, nous réalisons deux “types de tests” que je propose de vous présenter ici.\n\nNos tests E2E\n\nOn les appelle “tests fonctionels”, ce sont des tests End-to-end (E2E) sur une stack technique très efficace composée de CucumberJS, WebdriverIO avec ChromeHeadless\nIl s’agit d’une stack technique mise en place au début du projet (à l’époque avec PhantomJS pour les plus anciens d’entre-vous)\n\nCette stack nous permet d’automatiser le pilotage de tests qui contrôlent un navigateur.\nCe navigateur va réaliser des actions qui se rapprochent le plus de celles que nos vrais utilisateurs peuvent faire tout en vérifiant comment le site réagit.\n\nIl y a quelques années, cette stack technique était plutôt compliquée à mettre en place, mais aujourd’hui il est plutôt simple de le faire.\nLe site qui héberge cet article de blog en est lui-même la preuve.\nIl ne m’a fallu qu’une dizaine de minutes pour mettre en place cette stack avec le WebdriverIo CLI pour vérifier que mon blog fonctionne comme prévu.\n\nJ’ai d’ailleurs récemment publié un article présentant la mise en place de cette stack.\n\nVoici donc un exemple de fichier de test E2E pour vous donner une idée:\n\nFeature: Playground\n\n  Background: Playground context\n    Given I use &quot;playground&quot; test context\n\n  Scenario: Check if playground is reachable\n    When As user &quot;toto@toto.fr&quot; I visit the &quot;playground&quot; page\n    And I click on &quot;playground trigger&quot;\n    Then I should see a &quot;visible playground&quot;\n    And I should see 4 &quot;playground tab&quot; in &quot;playground&quot;\n\n    When I click on &quot;playground trigger&quot;\n    Then I should not see a &quot;visible playground&quot;\n\n    # ...\n\n\nEt ça donne ça en local avec mon navigateur Chrome !\n\n\n\nVoilà un schéma qui explique comment cette stack fonctionne:\n\n\n\nAujourd’hui, l’application web de Bedrock possède plus de 800 scénarios de tests E2E qui tournent sur chacune de nos Pull Request et sur la branche master.\nIls nous assurent que nous n’introduisons pas de régression fonctionnelle et c’est juste génial !\n\n👍 Les points positifs\n\n\n  WebdriverIO nous permet également de lancer de manière journalière ces mêmes tests sur des vrais devices en passant par le service payant SAAS Browserstack.\nOn a donc tous les jours un job qui s’assure que notre site fonctionne correctement sur un Chrome dernière version sur Windows 10 et Safari sur MacOs.\n  Ces tests nous permettent de facilement documenter les fonctionnalités de l’application grâce au langage Gherkin.\n  Ils nous permettent de reproduire des cas qui sont loin d’être nominaux.\nDans une logique TDD, ils permettent d’avancer sur le développement sans avoir à cliquer pendant des heures.\n  Ces tests nous ont permis de ne pas casser l’ancienne version du site qui est toujours en production pour quelques clients alors que nos efforts se concentrent sur la nouvelle.\n  Ils nous apportent une vraie confiance.\n  Grâce notre librairie superagent-mock, nous pouvons fixturer (bouchonner, mocker) toutes les API dont on dépend et ainsi même vérifier les cas d’erreurs.\nDe plus, mocker la couche XHR du navigateur permet une amélioration significative du temps d’exécution des tests. 🚀\n  Ils nous donne accès à des usages étendus comme :\n    \n      vérification de règles d’accessibilité\n      check les logs de la console navigateur (pour ne pas introduire d’erreur ou de React Warning par exemple)\n      surveiller tous les appels réseaux du site grâce à un proxy\n      et j’en passe…\n    \n  \n\n\n👎 Les complications\n\n\n  Maintenir cette stack est compliqué et coûteux.\nÉtant donné que peu de ressources sont publiées sur ce domaine, on se retrouve parfois à devoir creuser pendant plusieurs jours pour les réparer 😅.\nIl nous arrive de nous sentir parfois bien seul à avoir ces soucis.\n  Il est très facile de coder un test E2E dit flaky (ie: un test qui peut échouer aléatoirement).\nIls nous font croire que quelque chose est cassé.\nIls nous prennent parfois du temps à les stabiliser.\nIl reste cependant bien meilleur de supprimer un test qui ne vous donnera pas un résultat stable.\n  Faire tourner tous les tests prend un temps important sur notre intégration continue.\nIl faut régulièrement travailler sur leur optimisation pour que le feedback qu’ils vous apportent soit le plus rapide possible.\nCes temps importants coutent également de l’argent, il faut en effet bien faire tourner ces tests sur des machines.\nPour information, l’infrastructure du site web (à lui seul, juste l’hébergement de nos servers Node + fichiers statiques + CDN) coutent bien moins cher que notre intégration continue.\nCela fait bien évidemment sourire nos Ops ! 😊\n  Les nouvelles recrues de nos équipes n’ont souvent jamais réalisé ce genre de tests, il y a donc une phase de galère d’apprentissage..\n  Certaines fonctionnalités sont parfois trop compliquées à tester avec notre stack E2E (par exemple, les parcours de paiement qui dépendent de tiers).\nIl nous arrive alors de nous rabattre sur d’autres techniques avec Jest notamment en ayant un scope moins unitaire.\n\n\nNos tests “unitaires”\n\nPour compléter nos tests fonctionnels nous avons également une stack de tests écrits avec Jest.\nOn qualifie ces tests d’unitaires car nous avons comme principe d’essayer de toujours tester nos modules JS en indépendance des autres.\n\nNe débattons pas ici sur “Est-ce que ce sont des vrais tests unitaires ?”, suffisamment d’articles sur internet traitent de ce sujet.\n\nOn utilise ces tests pour différentes raisons qui couvrent des besoins que nos tests fonctionnels ne couvrent pas:\n\n\n  nous aider à développer nos modules JS avec des pratiques TDD.\n  documenter et décrire comment fonctionne un module JS.\n  tester des cas limites très/trop compliqués à tester avec nos tests E2E.\n  faciliter le refactoring de notre application en nous montrant les impacts techniques de nos modifications.\n\n\nAvec ces tests, on se met au niveau d’une fonction utilitaire, d’une action Redux, d’un reducer, d’un composant React.\nOn se base essentiellement sur la fonctionnalité d’automock de Jest qui nous propose d’isoler nos modules JS lorsqu’on teste.\n\n\n\nL’image précédente représente la métaphore qui nous permet d’expliquer notre stratégie de tests unitaires aux nouveaux arrivant.\n\n\n  “Il faut s’imaginer que l’application est un mur composé de briques unitaires (nos modules ecmascript), nos tests unitaires doivent tester une à une les briques en indépendance totale des autres.\nNos tests fonctionnels sont là pour tester le ciment entre les briques.”\n\n\nPour résumer, on pourrait dire que nos tests E2E testent ce que notre application doit faire, et nos tests unitaires s’assurent eux de vérifier comment ça marche.\n\nAujourd’hui ce sont plus de 6000 tests unitaires qui couvrent l’application et permettent de limiter les régressions.\n\n👍\n\n\n  Jest est vraiment une librairie géniale, rapide, complète, bien documentée.\n  Les tests unitaires nous aident beaucoup à comprendre plusieurs années après comment tout cela fonctionne.\n  On arrive toujours à tester unitairement notre code, et cela complète bien nos tests E2E.\n  L’automock est vraiment pratique pour le découpage de tests par modules.\n\n\n👎\n\n\n  Parfois, nous nous sommes trouvés limités par notre stack de tests E2E et nous ne pouvions pas uniquement nous baser sur les tests unitaires.\nIl nous manquait quelque chose pour pouvoir s’assurer que le ciment entre les briques fonctionnait comme on le souhaitait.\nPour cela, il a été mis en place une deuxième stack de tests Jest nommé “test d’intégration” ou l’automock est désactivé.\n  L’abus de Snapshot est dangereux pour la santé.\nL’usage du “Snapshot testing” peut faire gagner du temps sur l’implémentation de vos tests mais peuvent en réduire la qualité.\nAvoir à review un object de 50 lignes en Snapshot est ni facile, ni pertinent.\n  Avec la dépréciation d’EnzymeJS, nous sommes contraints de migrer sur React Testing Library.\nIl est bien évidemment possible de tester unitairement des composants avec cette nouvelle librairie.\nMalheureusement, ce n’est pas vraiment l’esprit et la façon de faire.\nReact Testing Library nous pousse à ne pas jouer avec le shallow rendering.\n\n\nNos principes\n\nNous essayons de toujours respecter les règles suivantes lorsqu’on se pose la question “Dois-je ajouter des tests ?”.\n\n\n  Si notre Pull Request introduit des nouvelles fonctionnalités utilisateurs, il faut intégrer des scenarios de test E2E.\nDes tests unitaires avec Jest peuvent les compléter / remplacer en fonction.\n  Si notre Pull Request a pour but de corriger un bug, cela signifie qu’il nous manque un cas de test.\nOn doit donc essayer de rajouter un test E2E ou à défaut un test unitaire.\n\n\nC’est en écrivant ces lignes que je me dis que ces principes pourraient très bien faire l’objet d’une automatisation. 🤣\n\nLe projet reste, les fonctionnalités non\n\n\n  “La seconde évolution d’une fonctionnalité est très souvent sa suppression.”\n\n\nPar principe, nous souhaitons faire en sorte que chaque nouvelle fonctionnalité de l’application ne base pas son activation sur le simple fait d’être dans la codebase.\nClassiquement, le cycle de vie d’une “feature” dans un projet peut être le suivant (dans un Github Flow):\n\n\n  une personne implémente sur une branche\n  la fonctionnalité est mergée sur master\n  elle est déployée en production\n  vis sa vie de fonctionnalité (avec parfois des bugs et des correctifs)\n  la fonctionnalité n’est plus nécessaire\n  une personne détricote le code et l’enlève\n  nouveau déploiement\n\n\nPour simplifier certaines étapes, il a été mis en place du feature flipping sur le projet.\n\nComment ça marche ?\n\nDans notre config il y a une map clé/valeur qui liste toutes les fonctionnalités de l’application associées à leur statut d’activation.\n\nconst featureFlipping = {\n  myAwesomeFeature: false,\n  anotherOne: true,\n}\n\n\nDans notre code, nous avons donc implémenté des traitements conditionnels qui disent “Si cette feature est activée alors…”.\nCela peut changer le rendu d’un composant, changer l’implémentation d’une action Redux ou bien désactiver une route de notre react-router.\n\nMais à quoi ça sert ?\n\n\n  On peut développer des nouvelles évolutions progressivement en les cachant derrière une clé de configuration.\nOn livre des fonctionnalités en production sans les activer.\n  En environnement de test, on peut surcharger cette config pour tester des features qui ne sont pas encore activées en production.\n  Dans le cas d’un site en marque blanche, on peut proposer ces fonctionnalités à nos clients comme des options possibles.\n  Avant de supprimer le code d’une feature, on la désactive puis on fait le ménage sans risque.\n  Grâce à un outil maison nommé l’Applaunch, cette config de feature flipping est surchargeable dans une interface graphique à chaud sans déploiement.\nCela nous permet d’activer des fonctionnalités sans faire de mise en production du code.\nEn cas d’incident, on peut désactiver des fonctionnalités qui sont dégradées.\n\n\nPour vous donner un exemple plus concret, entre 2018 et 2020 nous avons complètement refondu l’interface de l’application.\nCette évolution graphique n’était qu’une clé de featureFlipping.\nLa refonte graphique n’a donc pas été la remise à zéro du projet, on continue encore aujourd’hui de vivre avec les deux versions (tant que la bascule de tous nos clients n’est pas terminée).\n\n\n\nL’A/B testing\n\nGrâce au super travail des équipes backend et data, on a pu même étendre l’usage du feature flipping en rendant cette configuration modifiable pour des sous groupes d’utilisateurs.\n\nCela permet de déployer des nouvelles fonctionnalités sur une portion plus réduite des utilisateurs afin de comparer nos KPI.\n\nPrise de décision, amélioration des performances techniques ou produit, expérimentations, les possibilités sont nombreuses et nous les exploitons de plus en plus.\n\nLe futur flipping\n\n\n  Sur une idée originale de Florent Lepretre.\n\n\nNous avions régulièrement le besoin d’activer des feature à des heures très trop matinales dans le futur.\nPour cela nous devions être connecté à une heure précise sur notre poste pour modifier la configuration à chaud.\n\nAfin d’éviter d’oublier de le faire, ou de le faire en retard, nous avons fait en sorte qu’une clé de configuration puisse être activée à partir d’une certaine date.\nPour cela, nous avons fait évoluer notre selector redux qui indiquait si une feature était activée pour qu’il puisse gérer des formats de date et les comparer à l’heure courante.\n\nconst featureFlipping = {\n  myAwesomeFeature: {\n    offDate: &#39;2021-07-12 20:30:00&#39;,\n    onDate: &#39;2021-07-12 19:30:00&#39;,\n  },\n}\n\n\n\n  De nombreux cafés ☕️ à 9h ont été sauvés grâce au futur flipping\n\n\nMonitorer, Mesurer, Alerter\n\nPour maintenir un projet aussi longtemps que l’application web de bedrock, des tests, de la documentation et de la rigueur ne suffisent pas.\nIl faut également de la visibilité sur ce qui marche en production.\n\n\n  “Comment sais-tu que l’application que tu as en production en ce moment même fonctionne comme prévu ?”\n\n\nOn part du principe qu’aucune fonctionnalité ne marche tant qu’elle n’est pas monitorée.\nAujourd’hui le monitoring à Bedrock coté Frontend se matérialise par différents outils et différentes stacks.\nJe pourrais vous citer NewRelic, un Statsd, une stack ELK ou bien encore Youbora pour la vidéo.\n\nPour vous donner un exemple, à chaque fois qu’un utilisateur commence une session de navigation on envoie un Hit de monitoring anonyme pour incrémenter un compteur dans Statsd.\nOn a alors plus qu’à définir un dashboard qui affiche dans un graphique l’évolution de ce nombre.\nSi on observe une variation trop importante, cela peut nous permettre de détecter un incident.\n\n\n\nLe monitoring nous offre aussi des solutions pour comprendre et analyser un bug qui s’est produit dans le passé.\nComprendre un incident, l’expliquer, en trouver sa root cause sont les possibilités qui s’offrent à vous si vous monitorez votre application.\nLe monitoring peut également permettre de mieux communiquer avec les clients sur les impacts d’un incident et également d’estimer le nombre d’utilisateurs impactés.\n\nAvec la multiplication de nos clients, bien monitorer nos plateformes n’est plus suffisant.\nTrop de données, trop de dashboards à surveiller, il devient très facile de louper quelque chose.\nNous avons donc commencé à compléter notre suivi des mesures par de l’alerting automatique.\nUne fois que les mesures nous apportent suffisamment de confiance, on peut facilement mettre en place des alertes qui vont nous prévenir en cas de valeur incohérente.\n\nNous essayons cependant de toujours déclencher des alertes uniquement quand celle-ci est actionnable.\nDans d’autres termes, si une alerte sonne, nous avons quelque chose à faire.\nFaire sonner des alertes qui ne nécessitent aucune action immédiate humaine génèrent du bruit et de la perte de temps.\n\n\n\nLimiter, surveiller et mettre à jour ses dépendances\n\nCe qui périme plus vite que votre ombre dans un projet web basé sur des technologies javascript, ce sont vos dépendances.\nL’écosystème évolue rapidement et vos dépendances peuvent vite se retrouver non maintenues, plus à la mode ou bien complètement refondues avec de gros breaking changes.\n\nOn essaye donc dans la mesure du possible de limiter nos dépendances et d’éviter d’en ajouter inutilement.\nUne dépendance, c’est souvent très facile à ajouter mais elle peut devenir un vrai casse-tête à enlever.\n\nLes librairies de composants graphiques (exemple React bootstrap, Material Design) sont un bel exemple de dépendance que nous tenons à ne pas introduire.\nElles peuvent faciliter l’intégration dans un premier temps mais celles-ci bloquent souvent la version de votre librairie de composant par la suite.\nVous ne voulez pas figer la version de React dans votre application pour deux composants de formulaires.\n\nLa surveillance fait aussi partie de nos routines de gestion de nos dépendances.\nDepuis l’ajout du signalement de failles de sécurité dans un package NPM, il est possible de savoir si un projet intègre une dépendance qui contient une faille de sécurité connue par une simple commande.\nNous avons donc des jobs journaliers sur nos projets qui lancent la commande yarn audit afin de nous forcer à appliquer les correctifs.\n\n\n  La maintenance de dépendances est grandement facilité par notre stack de tests E2E qui sonnent direcement si la montée de version génère une regression.\n\n\nAujourd’hui, hors failles de sécurité, nous mettons à jour nos dépendances “quand on a le temps”, souvent en fin de sprint.\nCela ne nous satisfait pas car certaines dépendances peuvent se retrouver oubliées.\nJ’ai personnellement l’habitude d’utiliser des outils comme yarn outdated et Dependabot sur mes projets personels pour automatiser la mise à jour de mes dépendances.\n\nAccepter sa dette technique\n\nUn projet accumulera toujours de la dette technique.\nC’est un fait.\nQue ce soit de la dette volontaire ou involontaire, un projet qui résiste aux années va forcément accumuler de la dette.\nD’autant plus, si pendant toutes ces années vous continuez d’ajouter des fonctionnalités.\n\nDepuis 2014, nos bonnes pratiques, nos façons de faire ont bien évolué.\nParfois nous avons décidé ces changements mais parfois nous les avons subi (un exemple, l’arrivée des composants fonctionnels avec React et l’api des Hooks).\n\nNotre projet n’est pas complètement “state of art” et on l’assume.\n\n\n\nNous essayons de prioriser nos sujets de refactoring sur les parties de l’application sur lequel on a le plus de souci, le plus de peine.\nOn considère qu’une partie de l’application qui ne nous plaît pas mais sur laquelle on n’a pas besoin de travailler (apporter des évolutions) ne mérite pas qu’on la refactorise.\n\nJe pourrais vous citer de nombreuses fonctionnalités de notre application qui n’ont pas évolué fonctionnellement depuis plusieurs années.\nMais comme nous avons couvert ces fonctionnalités de tests E2E depuis le début, nous n’avons pas vraiment eu à y retoucher.\n\nComme dit plus haut, la prochaine évolution d’une feature de code est parfois sa désactivation.\nAlors pourquoi passer son temps à ré-écrire toute l’application ?\n\n\n  Le code devient dans tous les cas du “legacy”.\n  Tant que les fonctionnalités sont testées, rien ne nous oblige à tout refactorer en permanence pour que toute notre codebase soit state of art.\n  On se focalise sur nos pain points, on re-factorise ce qu’on a vraiment besoin de faire évoluer.\n\n\nPour résumer\n\nLes bonnes pratiques présentées ici restent bien évidemment subjectives et ne s’appliqueront pas parfaitement/directement dans vos contextes.\nJe suis cependant convaincu qu’elles peuvent probablement vous aider à identifier ce qui peut faire passer votre projet de fun à périmé.\nÀ Bedrock nous avons mis en place d’autres pratiques que je n’ai pas listées ici mais ce sera l’occasion de faire un nouvel article un jour.\n\nEnfin, si vous souhaitez que je revienne plus en détail sur certains chapitres présentés ici, n’hésitez pas à me le dire, je pourrais essayer d’y dédier un article spécifique.\n\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #14",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/07/16/bedrock-dev-facts-14.html",
    "date"     : "July 16, 2021",
    "excerpt"  : "Apportez vos devfacts à la plage cet été!\n\nLes phases de test\n\n  Pour une fois qu’on est payé à faire des 500\n\n\nLe panier de crabes\n\n  _ On attend le GoNoGo\n\n  _ On ne pourrait pas attendre le RustNoRust ce serait plus performant\n\n\n1… 2… 3… extinc...",
  "content"  : "Apportez vos devfacts à la plage cet été!\n\nLes phases de test\n\n  Pour une fois qu’on est payé à faire des 500\n\n\nLe panier de crabes\n\n  _ On attend le GoNoGo\n\n  _ On ne pourrait pas attendre le RustNoRust ce serait plus performant\n\n\n1… 2… 3… extincteurs\n\n  Parlant à un hébergeur cloud\n\n  “Tu sais la petite boite française en trois lettres qui a eu chaud ce matin?”\n\n\nÉchec et mat\n\n  We’ve been hacked by our incompetence\n\n\nLe vrai du faux\n\n  C’est pas le bon true\n\n\nDarladidadada\nQuand tu fais du ménage dans ton compte AWS de tests et que tu te dis qu&amp;#39;un collègue doit avoir l&amp;#39;oreille musicale 🎶 pic.twitter.com/yIINlboQBy&amp;mdash; Pascal MARTIN (@pascal_martin) March 12, 2021\n\nReview in progress\n\n\nIPV6 c’est pas encore hyper clair\n\n  Oui mais c’est une grosse IP\n\n\nLa confiance\n\n  Normalement, si tout va bien, ça marche!\n\n\nAlors cette mise en production ?\n\n  Elle s’est bien passée mais il a fallu revert\n\n\nEt oui mon petit\n\n  L’Internet, c’était nous\n\n\nUne API c’est pas si mal en fait\n\n  Quand je vois l’UX de la console AWS, je me demande s’il n’y a pas une véritable volonté à nous pousser à faire de l’infrastructure as code\n\n\nDe la part d’un dev backend Php 😱\n\n  J’aime mettre du javascript partout\n\n\nLe retour du come-back de l’automatisation manuelle\n\n  On pourra faire un rattrapage à moitié manuel, mais quand même automatique\n\n\nDans les méandres des fonctionnalités de Airtable\n\n  Bon, on va faire simple, on va écrire du JS\n\n\nUn mercredi à 15h55\n\n  Cette phrase est trop complexe pour un lundi matin\n\n\nÀ bicyclette 🚲\n\n  Le client il s’en fout que tu mettes des petites roues sur ton vélo, il veut juste que t’avances sans te péter la gueule\n\n\nEntendu dans une réunion à distance\n\n  Rassurez-vous c’est pas moi qui ronfle c’est mon chat\n\n\nEn parlant de choix techniques\n\n  Attention le choix pris aujourd’hui fera jurisprudence sur la suite\n\n\nC’est géré !\n\n  Je crois que l’on gère cette erreur, même si on ne la comprend pas\n\n\n1 + 1 = 11\n\n  30 est un nombre énorme\n\n\nEntendu juste après une grosse MEP\n\n  Oh! Il faut que je mette en place un mail de vacances.\n\n\nDes frites 🍟\n\n  Mes critères de succès, c’est les grosses patates\n\n\nPO, pas un métier facile\n\n  Le dev: “Est-ce que les PO peuvent le faire ?”\n\n  Le PO: “On ne sait rien faire nous”\n\n\nBougez pas\n\n  Je déploie le deployer\n\n\nUne légende raconte qu’il faut déployer deux fois le deployer\n\nUne petite équipe\n\n  On était 80 stagiaires sur le projet\n\n\n🎰\n\n  La magie du chmod -R 777. Je te dirais pas que j’en ai rajouté dans mes 3 dernières PR pour fixer de la CI\n\n\nIterm 2 secret features\nMa recherche Google:\n\n  Iterm2 big sur blinking\n\n\nGitlab issue reponse :\n\n  Just turn on Prefs &amp;gt; Advanced &amp;gt; Work around Big Sur bug where a white line flashes at the top of the screen in full screen mode.\n\n\nLes quadricolors\n\n  “_ Pourquoi avoir choisi du vert pour la doc ?”\n\n  “_ Écoute je suis daltonien.”\n\n\nPourquoi douter ?\n\n  “_ Je ne sais pas comment valider fonctionnellement mon développement.”\n\n  “_ Ben, pareil que tu l’as testé.”\n\n  “_ Je ne l’ai pas testé.”\n\n  “_ 😅”\n\n\nTimber !!!!\n\n  Dans la vie comme dans git tu peux pas supprimer la branch sur laquelle t’es.\n\n\nC’est du propre\n\n  La refacto, c’est comme le ménage, il faut en faire régulièrement.\n\n\nLes conflits de canard\n\n  C’est git checkout verdun ton rebase\n\n\nReconversion\n\n  De toute manière avec le réchauffement climatique, je vais tout plaquer et monter un business de vente de djellaba.\n\n\nIncident sur la ligne B\n\n  Parfois les réunions c’est comme les transports, tu es coincé dedans et ça n’avance pas\n\n"
} ,
  
  {
    "title"    : "No-code, ou le développement d’applications ouvert à d’autres métiers !",
    "category" : "",
    "tags"     : " conference, nocode, lowcode, afup",
    "url"      : "/2021/06/11/no-code-developpement-applications-ouvert-autres-metiers-pascal-martin.html",
    "date"     : "June 11, 2021",
    "excerpt"  : "Construire une application sans coder ? C’est une idée que j’entends depuis le début de mes études… Et c’est la promesse de no-code !\nD’ailleurs, pendant que des discours déclarent que nos enfants doivent apprendre à coder à l’école, nous écrivons...",
  "content"  : "Construire une application sans coder ? C’est une idée que j’entends depuis le début de mes études… Et c’est la promesse de no-code !\nD’ailleurs, pendant que des discours déclarent que nos enfants doivent apprendre à coder à l’école, nous écrivons nous-même déjà des applications no-code ! N’avez-vous pas lancé Excel récemment ?\n\nCes dernières années, l’approche no-code a évolué et devient petit à petit un concept viable.\nDes entreprises, startups ou mastodontes, se lancent sur ce marché et publient des outils et solutions qui aident à rivaliser avec certaines des applications que nous aurions pu développer nous-même…\nJe ne parle bien sûr pas (encore?) de supprimer nos métiers… Mais est-ce que no-code ou low-code ne permettraient pas à d’autres profils que les nôtres d’avancer plus rapidement sur leurs projets ?\n\nÀ travers cette introduction, vous découvrirez un pan de l’approche no-code et j’espère vous montrer que le développement d’applications n’est plus réservé qu’aux développeurs… Et que nos langages préférés ne sont plus la réponse à toutes les questions !\n"
} ,
  
  {
    "title"    : "The organisational challenge of building a Data team: lessons learnt",
    "category" : "Data",
    "tags"     : " Data, Data Science, Data Engineering, Agile, BigData, Organization",
    "url"      : "/data/2021/05/19/organisational-challenge-building-data-team.html",
    "date"     : "May 19, 2021",
    "excerpt"  : "Disclaimer: this is a post I’ve been meaning to share ever since the Dailymotion team published this post, so it’s going way back into our history but updated to our current status. It’s also an attempt to bring a counterpart to the blog posts of ...",
  "content"  : "Disclaimer: this is a post I’ve been meaning to share ever since the Dailymotion team published this post, so it’s going way back into our history but updated to our current status. It’s also an attempt to bring a counterpart to the blog posts of many web leaders that have written about what worked but rarely detail the difficulties they met and the lessons they learnt.\nHere’s a very personal (therefore biased) feedback of our story and the lessons we learnt down the road.\n\nAt Bedrock we’re building the best streaming platform in Europe to help our customers be local streaming champions.\nWithin Bedrock (and before that within M6 where our team was born), we embraced Data in 2016. From a top management perspective the move was straight forward once the company had understood the strategic importance of Data and the major use cases we could address: staff a team and deliver massive value (since Data was definitely the new oil back in those days). But for the teams we built (now north of 40 people), it was another story.\n\nFirst iteration: the Dream Team\n\n\n\nThe first step M6 took was to hire an amazing, PhD qualified, Business focused, Data Scientist whose mission was to build a Data Science team and explore the company’s data to find valuable use cases. A few months later, we added an experienced Product owner (me) to help set the vision, the roadmap and lead our Data initiatives. And some months later we were joined by a rock star Data/Machine learning Engineer who was to create an engineering team and the data platform we could build upon.\n\nOur collective mission was very ambitious but also very unclear. Basically, we were in charge of using Data to have a major impact on the company’s business. Nobody in the management had a very clear idea of what that meant and how to get there, that was up to us.\nEach one of us had many ideas on how to make that happen, and quite different approaches of the way to get there. The caricatural (but quite realistic) outline was:\n\n  Our lead Data Scientist wanted data, tools and manpower to explore the data, find awesome machine learning use cases that he would build with the DS team he had staffed, onboard the business, have them buy in and go to production (instantly, in his view the step to production was the tiny part).\n  Our lead Data Engineer wanted to start with small use cases, build a production infrastructure and enrich the science step by step later down the road.\n  I wanted to align with our stakeholders and management to build a shared vision, prioritize ideas and then organize the team work to iterate and increase value on a regular basis.\n\n\n\n\nThis misalignment led to an early split between the Data Scientists who did a lot of exploration + POC’s and the Data Engineers who worked on an industrial production platform. The projects were of different nature and everything was pretty smooth except for some frustration on my side because I was playing little to no role in the Data Science part of things. Overall, everybody was happy, and we started to deliver value… until our first real business (beyond tooling) cases entered the roadmap.\n\nThe first business case where we needed to leverage both engineering and scientific skills was A/B testing. After having scanned the market solutions to find a nice solution we chose to build our own because we wanted deep integration into all our frontends (including mobile apps and set-top boxes) and backends, a lot of freedom to build KPI’s upon all of our Data Lake and a few other things. \nTo build the complete toolbox, we had to create a backend serving layer that would return the correct feature flags for the A/B user groups, calculate KPI’s with a large combination of filters, ensure that the calculation pipeline was run every day and display the results in a decent dashboard. From a project management perspective, it was a headache because the KPI part needed to bring statistical expertise to production, combining statistical excellence and production grade reliability. In our first iteration, a Data Scientist did a POC, but it was no way near production standards. On a separate track, the Data Engineers did a totally separate POC that delivered a decent framework, the KPI’s were calculated incorrectly. Our third iteration (that took 6 months to make happen) was a 2-day session of peer programming between a Data Scientist and a Data Engineer that (at last!) delivered something real: a POC that actually calculated the right metrics in a way that could be industrialized.\n\nThe second business case was just as messed up. Our Data Science team headed off solo to build a POC of a program recommendation engine (we stream videos that belong to TV programs). Within a couple of months, they built something that seemed nice, but didn’t scale when run for our millions of users. They then went into optimising their code, rewriting and optimising again without any Data Engineer help for 4 more months until the algorithm was ready for an A/B test against our long standing business rules. And the POC lost the test. After some rounds of tuning, we stopped the initiative altogether because nobody believed in any potential success.\n\nWe repeated this type of scenario a few times, building up more and more frustration within the 2 teams and the stakeholders. We tried to break it down and make the collaboration work quite a few times, but 2 points couldn’t be resolved:\n\n  The Data Scientists didn’t want to embrace any product/project management, agile or not (though I believe the underlying issue was more about control in a “us” versus “them” mindset).\n  The Data Engineering team refused to put any Python code (the Data Scientists language of choice) into production and imposed that everything would be Java or Scala, versionned on git, unit tested and monitored.\nThis looks like very usual trolls, but they kept us stuck for more than a year.\n\n\nAt the end, our organization literally cracked up. The tensions became conflicts, some people left the team and our top management had to dive in to pacify and rebuild something that would work.\nAlthough we had delivered value in several places, we were clearly under effective.\n\nMy personal take away on our collective difficulties boils down to 2 things:\n\n  Starting off with a bunch of rock stars made collaboration impossible because each one wanted to lead in a very personal way, without much compromise.\n  Letting things slip towards a comfortable separation of 2 expertise teams with 2 very different organisations and agenda’s instead of insisting on aligning them led to a form of cold war with no collaboration at all.\n\n\n\n\nSecond iteration: pluridisciplinary teams\n\nThe conclusion of the management’s deep dive was that we needed to split the historic teams in a more official and long term way.\nThe Data Scientists would form the Data Lab in which they would do research and produce POC’s and whitepapers + staff their own engineers if they needed any. And we would create the Data Factory that would be focused on delivery with the Data Engineers and some new Data Scientists that would work on the team’s backlog with the Data Engineers on a daily basis.\n\n\n\nFrom the Data Lab perspective, that relieved most of the stress because they were now officially free to set a very scientific agenda for themselves. At the start it was cool, but over time (this was 3 years back), their disconnection from “production” put them very far away from the real world. The internal stakeholders turned away from them because their target was to make real things. It ultimately led them to having small business impact and a poor dynamic, ultimately reducing the team down.\nFrom the Data Factory perspective, the new organisation fixed it! Over the past couple of years, we have been building the vision and roadmap within the team with no distinction between the Data Science and Data Engineering roadmap. Our work with the stakeholders feels like we’re now walking on our 2 feet, the solutions we imagine for their challenges mix plain data engineering solutions with algorithms and statistics seamlessly, and we’re definitely delivering more value. And of course, there is no debate on what goes to production, everything the team does will end up live! At the time I’m writing, the Data Factory has scaled from ~10 people to more than 40 members.\n\n\n\nOrganization is one thing, collaboration is another\n\nOur approach to overcome our challenges was totally around changing the organization, but I truly believe the solution was also within the evolution of our mindset.\nWhen we officially split the Data Science and Data Engineering teams, we also increased staffing bringing new people into the game. During that round of staffing, we focused very much on soft skills and mindset, considering that authentic team players would ultimately bring us more value than very strong individuals. Somehow, we moved away from building the dream team and aimed to create a real team. Our idea was that we weren’t doing rocket science, we were working on a bunch of features that needed to incorporate some data science properly in our pipelines.\nOnce the newcomers joined the team, we put a lot of attention into the way people work together, the team spirit they build, the collective dynamic. That went through team building events, defining team values, helping each team member know and understand the other individuals around, creating collaboration rituals, maximizing pair programming between Data Scientists and Data Engineers, etc. And more than anything, entering a more agile test and learn approach, including within the organisation and ways to collaborate.\nI truly believe that this was the key to our success and it’s my major learning today: build a team.\n\nPutting the pieces together\n\nIn the Dailymotion team post I quoted in the introduction, there’s a quite precise and documented blueprint of the organisation about who does what between Data Scientists, Data Analysts and Data Engineers. We’ve never been that documented and structured for now, we’ve given each team a lot of freedom about who does what in the process depending on individual capabilities and what the team likes best.\nOne thing we did do is to sort out the language &amp;amp; tooling trolls. At first, anything that went to production was written in Scala, deployed and reviewed via our Git &amp;amp; continuous deployment pipeline, unit tested and monitored. So if a Data Scientist wanted to work on the production code it was following those rules (that has made some of our Data Scientists need to learn Scala and other things). This is now evolving, thanks to strong local collaboration and we now deliver more and more Python to production :)\n\nConclusion\n\nOver the past 4 years, our data team has scaled from 2 to more than 40 people. In the early days we underestimated the fact that putting excellent individuals together wouldn’t just work. We learnt the hard way that there was specific attention to be put into the organisation and the way we build teams and successful collaboration patterns between people with different domains of expertise and backgrounds. In that, our conclusion is totally similar to the learnings of the Dailymotion team.\nNow we have fixed the mindset and collaboration part of things, we feel nothing can go wrong, even if we changed our organization pattern.\nIf you want to go further, we spoke about this and some other challenges we faced with Morgiane Meglouli in a conference, the slides (in French) are available here\n\nLast but not least, Bedrock is scaling fast to help build streaming champions around Europe. If you’d like to join us, check out our open positions\n\n"
} ,
  
  {
    "title"    : "Comment nous réduisons l’augmentation de nos coûts AWS",
    "category" : "",
    "tags"     : " conference, aws, costs",
    "url"      : "/2021/03/30/comment-nous-reduisons-augmentation-couts-aws-pascal-martin.html",
    "date"     : "March 30, 2021",
    "excerpt"  : "Malgré les promesses du Cloud, votre facture AWS vous fait peur ? Je vous comprends !\n\nVenez découvrir comment, chez Bedrock, nous suivons et catégorisons les coûts d’infrastructure de notre plateforme de VOD et de Replay. J’enchainerai avec un re...",
  "content"  : "Malgré les promesses du Cloud, votre facture AWS vous fait peur ? Je vous comprends !\n\nVenez découvrir comment, chez Bedrock, nous suivons et catégorisons les coûts d’infrastructure de notre plateforme de VOD et de Replay. J’enchainerai avec un retour d’expérience basé sur trois ans de réponses à la question que nous nous posons tous : « comment réduire nos coûts AWS ? »\n\nDiviser par deux une facture DynamoDB ? Exploiter des instances spot à -70% ? Effectuer moins d’appels d’APIs ? Réduire les transferts inter-AZ, massifs lorsque nous manipulons des vidéos ? Ce ne sont que quelques-unes des pistes que nous avons envisagées…\n"
} ,
  
  {
    "title"    : "Migration de 6play vers Le Cloud, retour d’expérience.",
    "category" : "",
    "tags"     : " conference, cloud, migration, cloudsud",
    "url"      : "/2021/03/11/migration-6play-vers-le-cloud-retour-experience-pascal-martin.html",
    "date"     : "March 11, 2021",
    "excerpt"  : "En 2018, nous avons entamé la migration de la plateforme 6play vers Le Cloud.\nÀ présent, nous pilotons notre infrastructure AWS avec Terraform, utilisons des services managés et déployons nos applications sous Kubernetes.\n\nPendant cette conférence...",
  "content"  : "En 2018, nous avons entamé la migration de la plateforme 6play vers Le Cloud.\nÀ présent, nous pilotons notre infrastructure AWS avec Terraform, utilisons des services managés et déployons nos applications sous Kubernetes.\n\nPendant cette conférence, vous découvrirez comment nous avons réalisé cette migration. Vous trouverez des réponses aux questions que vous vous posez si vous envisagez de revoir votre hébergement.\nComment avons-nous transformé notre infrastructure ? Quels impacts sur nos projets ? Comment nous sommes-nous organisés ? Quels choix avons-nous effectués tout au long du processus ? Qu’avons-nous appris, qu’avons-nous fait évoluer ? Comment nos équipes se répartissent-elles les tâches ? Avons-nous dû adapter nos applications PHP ? Quelles difficultés avons-nous rencontrées ?\n"
} ,
  
  {
    "title"    : "Bedrock Dev Facts #13",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2021/03/02/bedrock-dev-facts-13.html",
    "date"     : "March 2, 2021",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nDes branchements surprenants\n\n  Ah tu test ta branche en prod ? Tu peux y aller, si tu touches pas au staging pas de soucis\n\n\nAngular conventional commit\n\nTrouvé dans l’historique\n\n\n\nTime is relative\n\n\n ...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nDes branchements surprenants\n\n  Ah tu test ta branche en prod ? Tu peux y aller, si tu touches pas au staging pas de soucis\n\n\nAngular conventional commit\n\nTrouvé dans l’historique\n\n\n\nTime is relative\n\n\n  Non mais c’est normal, ce sont des mois de 5 minutes\n\n\nLe confinement à la campagne, c’est parfois compliqué\n\n\n  Dédicace a tous ceux qui comme moi, vont passer une bonne semaine à batailler avec internet ! (au moins, j’ai assez de débit pour un mp3).\n320 Kbps\n\n\nApple 💰\n\n\n  Quand ca parle d’argent, Apple ne se rate pas généralement\n\n\nLe courage\n\n\n  Une MEP le vendredi avec la CI KO, vous êtes mes héros 😍\n\n\nNote à moi-même\n\n\n  TODO: test if my code is the problem\n\n\nAucun abus\n\n\n  Bon, je crois que je vais m’auto-déclarer meilleur développeur de l’année!\n\n\nUne semaine qui commence bien\n\n\n  3 push master, lundi matin, 11h30. La semaine va être bonne\n\n\nSoit gentil, pas méchant, c’est pas gentil d’être méchant\n\n\n  A: “Je comprends pas l’API elle repond rien”\n\n\n\n  B: “T’as essayé de demander gentiment ?”\n\n\n{\n    &quot;error&quot;: {\n        &quot;status&quot;: 400,\n        &quot;message&quot;: &quot;Invalid parameters &#39;MAIS_TU_VAS_ME_REPONDRE_UN_TRUC_COHERENT_BORDEL_DE_MERDE_!?!?!?!?!?!?!?!?!!&#39; for route &#39;xxxxxxxxxxxxx&#39;&quot;\n    }\n}\n\n\nDes réponses magiques\n\n\n  “so this case is never expected, but obviously it happens” Ah 😅, nous voilà rassuré\n\n\nAlerte générale\n\n\n  Quand on crée une alerte pour pouvoir la mettre en silence juste après 👌\n\n\nLe consentement\n\n\n  A: “J’ai insisté et c’est passé”\n\n\n\n  B: “Tu as insisté mais est-ce que tu as pensé au consentement de GraphQL et de postgre ?”\n\n\n🦥, moi paresseux ?\n\n\n  Même ne rien faire c’est déjà pas mal de taf\n\n\n? Question ?\n\n\n\n🍗 Mnom Mnom\n\n\n  A: “Ah mince j’ai pas mis mon texte en escalope”\n\n\n\n  B: “?”\n\n\n\n  A: “Entre quote”\n\n\nLa documentation éternelle\n\n\n  Les temps changent mais pas les readme\n\n\nLes petits génies\n\n\n  Nan mais attendez vous à plein de commentaires disant que c’est codé avec les pieds, et que Jean-Kevin, 16 ans, aurait mieux fait en 2j pendant son week-end chez sa mamie Germaine, pendant qu’il faisait une pause dans le développement du nouveau Bitcoin.\n\n\nBlackhole sun\n\nEn parlant de lignes supprimées.\n\n\n\nTechnique de sioux\n\n\n  Nan mais réécrit les status 404 en 200 et ça marchera !\n\n\nOn peut revert une fois, mais pas quinze !\n\n\n\nDéfinition AWS\n\n\n  AZ = Ama Zones\n\n\nHeureusement pour nous\n\n\n  Les cas sans erreurs fonctionnent très bien\n\n\nManager === 🐐 ?\n\n\n  La formatrice : je vous donne un mot et vous me donnez le premier mot qui vous passe par la tête.\n\n\n\n  La formatrice : “Chef” ?\n\n\n\n  Un participant : Chèvre\n\n\nIl n’y a pas de “mais” !\n\n\n  Je ne veux pas remettre en cause toute l’architecture MAIS …\n\n\n🐒 Singe ?\n\n\n  Quand j’entends “Go/NoGo”, mon cerveau comprend “Bonobo”\n\n\nAvec du savon !\n\n\n  A: “Ils font du SOAP!”\n\n\n\n  A: “C’est pour être bien propre”\n\n\nL’automatisation DIY\n\n\n  C’est automatiquement fait à la main\n\n\n🦥 le retour\n\n\n  La paresse n’est pas un défaut, c’est une optimisation\n\n\nAstuce de pro!\n\n\n  Si vos collègues et votre conscience vous embêtent quand vous dites « je vais tester ce changement en prod », dites à la place « je vais valider ce changement en prod ». Vous verrez, ça ira tout de suite mieux !\n\n"
} ,
  
  {
    "title"    : "Machine Learning en production",
    "category" : "",
    "tags"     : " machine learning, Lyon Data Science, conference",
    "url"      : "/2021/01/21/machine-learning-en-production.html",
    "date"     : "January 21, 2021",
    "excerpt"  : "Une fois passée la phase de prototype, comment va-t-on en production quand on fait du machine learning ?\nComment s’assure-t-on que tout va bien une fois en production ?\nDéploiement, tests, monitoring, etc. Il y a beaucoup de choses à penser. Sur c...",
  "content"  : "Une fois passée la phase de prototype, comment va-t-on en production quand on fait du machine learning ?\nComment s’assure-t-on que tout va bien une fois en production ?\nDéploiement, tests, monitoring, etc. Il y a beaucoup de choses à penser. Sur ce long sujet, je vous propose ici une petite introduction basée sur mes expériences.\n"
} ,
  
  {
    "title"    : "Three years running Kubernetes on production at Bedrock",
    "category" : "",
    "tags"     : " Infrastructure, Cloud, Kubernetes, Kops, AWS, HAProxy",
    "url"      : "/2020/12/08/Three-years-running-kubernetes-on-production-at-Bedrock.html",
    "date"     : "December 8, 2020",
    "excerpt"  : "We migrated our first application to a Kubernetes cluster at AWS in 2018 (a colleague even wrote a book about it).\nThree years later, we manage a dozen clusters, to which we have added a lot of tools and we have a much better grasp of certain subt...",
  "content"  : "We migrated our first application to a Kubernetes cluster at AWS in 2018 (a colleague even wrote a book about it).\nThree years later, we manage a dozen clusters, to which we have added a lot of tools and we have a much better grasp of certain subtleties.\nEach cluster reaches, depending on the load, hundreds of nodes and thousands of pods.\n\nTable of Contents\n\n\n  Base\n    \n      Kops and templates\n      Tools we use\n      Keep tools up to date on all clusters\n    \n  \n  Resiliency\n    \n      DNS\n      Lots of AutoScalingGroups\n      Dedicated AutoScalingGroups by app\n      QOS Guaranteed Daemonsets\n    \n  \n  Scalability\n    \n      Cluster Autoscaler\n        \n          Expander Priority\n        \n      \n      Overprovisioning\n      PriorityClass\n      Low HPA targets\n      Long downscale durations\n    \n  \n  Observability\n    \n      Metrics\n      Logs\n      Alerting\n    \n  \n  Costs\n    \n      Spot instances\n        \n          Inter accounts reclaims\n          On-demand fallback\n          Draino and node-problem-detector\n          Spot Tips\n        \n      \n      Kube-downscaler\n      HAProxy Ingress Controller\n    \n  \n\n\n\n\nBase\n\nKops and templates\n\nEKS didn’t exist when we started to work on Kubernetes on AWS. So we use Kops which, by the way, works very well.\nKops creates, updates and deletes our clusters, but also associates resources on our AWS accounts: DNS zone + entries, AutoScalingGroups, SecurityGroups, etc.\nOur rolling updates and rolling upgrades are 100% handled by kops which never failed us.\n\nBecause we have several clusters, we use kops toolbox template instead of having a single YAML file per cluster. We have mutualized resources definitions, like AutoScalingGroups, DNS options or namespaces list, inside common files and use a dedicated template file per cluster, referencing mutualized configs through variables.\n\nFor example, the EC2 instance types will be defined as snippets:\n\n± cat snippets/spot_4x_32Gb_machine_type.yaml:\n- c5.4xlarge\n- c5d.4xlarge\n- c5n.4xlarge\n\nAnd used inside a generic template file:\n\n± cat templates/3_spot-nodes.yaml.tpl\n…\n  mixedInstancesPolicy:\n    instances:\n    { { if eq $index &quot;4x_32Gb&quot; } }\n    { { include &quot;spot_4x_32Gb_machine_type.yaml&quot; . | indent 4 } }\n    { { end } }\n…\n\nFinally, if the cluster requires an ASG with instances size 4x with 32GB RAM on Spot instances:\n\n± cat vars/prod-customer.k8s.foo.bar.yaml\n…\nspot_nodes:\n  4x_32Gb:\n    az:\n      - eu-west-3a\n      - eu-west-3b\n      - eu-west-3c\n    min: 1\n    max: 100\n\nA bash script orchestrates all this. It generates manifest files, creates/updates clusters and checks everything is operating normally.\n\nAll of the above lives as files in a git repository, ensuring we’re doing only Infrastructure as Code.\n\nWe never make any Infrastructure modification outside of code.\n\nTools we use\n\nWe add some tools to a raw Kubernetes cluster:\n\n\n  aws-iam-authenticator\n  cluster-autoscaler\n  cloudwatch-exporter-in-cluster\n  cni-metrics-helper\n  draino\n  elasticsearch-cerebro\n  fluentd\n  haproxy-ingress-controller\n  iam-role-for-serviceaccount\n  k8s-spot-termination-handler\n  kube-downscaler\n  logstash\n  loki\n  metrics-server\n  node-problem-detector\n  overprovisioning\n  prometheus\n  prometheus-dnsmasq-exporter\n  prometheus-pushgateway\n  statsd-exporter\n  statsd-proxy\n  victoria-metrics-cluster\n\n\nSome of those tools stand for compatibility reasons after our cloud migration, so our developers can still use our ELK stack, or a statsd format to generate metrics.\n\nWe need all these tools to have a production-ready cluster, so we can provide scaling, resilience, observability, security with controlled costs. This list isn’t even exhaustive.\n\nIt evolved a lot over the last two years and will surely evolve a lot in the near future, as both Kubernetes and AWS are moving playgrounds.\n\nKeep tools up to date on all clusters\n\nWe use a Jenkins job for that.\n\nWe deploy k8s-tools the same way we deploy our apis in the cluster: with bash scripts and a helm chart, dedicated per application.\n\n± tree app/loki/.cloud/       \napp/loki/.cloud/\n├── charts\n│   ├── Chart.yaml\n│   ├── templates/\n│   ├── values.yaml\n│   ├── values.customerX.yaml\n│   └── values.customerY.yaml\n└── jenkins\n    ├── builder.sh\n    └── deployer.sh\n\nA Jenkins job runs the builder.sh, then the deployer.sh script for every k8s-tool.\nbuilder.sh is run when we need to build our own Docker images.\ndeployer.sh handles the Helm Chart deployment subtleties.\nAll apps are first deployed on all our staging clusters, then on prod.\n\nConsistency is maintained over all our clusters through this Jenkins job.\n\nResiliency\n\nDNS\n\nLike everyone who’s using Kubernetes on production, at some point, we faced an outage due to DNS. It was either UDP failing because of a kernel race condition, or musl (Alpine Linux’s replacement of glibc) not correctly handling domain or search, or also the default ndots 5 dnsConfig, or even KubeDNS not handling peak loads properly.\n\nAs of today:\n\n\n  We are using a local DNS cache on each worker node, with dnsmasq,\n  We use Fully Qualified Domain Names (trailing dot on curl calls) as much as possible,\n  We’ve defined dnsConfig preferences for all our applications,\n  We use CoreDNS with autoscaling as a replacement for KubeDNS,\n  We forbid as much as possible musl/Alpine\n\n\nExample of a dns configuration in prod:\n\n  dnsConfig:\n    options:\n    - name: use-vc\n    - name: single-request\n    - name: single-request-reopen\n    - name: ndots\n      value: &quot;1&quot;\n  dnsPolicy: ClusterFirst\n\n\ndnsPolicy: ClusterFirst makes sure we’re using the node’s loopback interface, so pods will send their DNS requests to dnsmasq installed locally on each node.\nDnsmasq forwards DNS queries to CoreDNS for cluster.local. sub-domains and to the VPC’s DNS server for the rest.\n\nLots of AutoScalingGroups\n\nWe had a dozen AutoScalingGroups per cluster.\nThis was both for resiliency and because we use Spot instances.\nWith Spot instance reclaims, we needed to have a lot of instance types and family types: m5.4xlarge, c5.4xlarge, m5n.8xlarge, etc.\nThis is an autoscaler recommendation to split AutoScalingGroups so that each ASG has the same amount of RAM and number of CPU cores when using mixed instances policies.\nAs a result, we had ASGs like:\n\n\n  spot_4x_32Gb\n  spot_4x_64Gb\n  spot_4x_128Gb\n\n\nLots of AutoScalingGroups doesn’t work well\n\nAZ rebalancing doesn’t work anymore when using more than one ASG. It becomes totally unpredictable and uncontrollable. It is even a total nightmare with a dozen ASGs.\n\nYou can see the difference of outgoing traffic between our 3 NAT Gateway over 4 hours time range :\n\nThe blue NAT gateway is used way more than the two others between 19h00 and 22h00. The green NAT gateway is used half as much as the other two during peak usage times.\nThis is because AZ-rebalacing has resulted in twice as many instances in one AZ than in the others.\n\nAlso, Kubernetes’s cluster-autoscaler isn’t really compatible with many AutoScalingGroups. We’ll cover how it works later in this post (Scalability/ExpanderPriority), but keep in mind that each application should run on no more than a maximum of 4 ASGs. This is due to the failover mechanism of cluster-autoscaler that doesn’t detect ASGs errors like InsufficientInstanceCapacity, which considerably increases the scale-up time. We are particularly concerned because we need to scale quickly and intensely.\n\nWe’ve rolled-back on the ASG number. We now have a maximum of 4 ASGs per application group (see next section: Resiliency/DedicatedAutoScalingGroups), with 2 being Spot and 2 on-demand fallbacks.\nFor this reason, we no longer respect the recommendation to split AutoScalingGroups so that each ASG has the same amount of RAM and number of CPU cores, in order to reduce ASGs number.\n\nRunning PHP, the CPU is our bottleneck, not RAM. So we made the choice to have mixed ASG with the same number of CPUs, but not the same amount of RAM.\nThis means that our ASG spot-nodes-8x is composed of m5.8xlarge as well as r5.8xlarge\n\nDedicated AutoScalingGroups by app\n\nWe started to dedicate AutoScalingGroups for some applications when Prometheus was eating all the memory of a node, ending up in OOM errors. Because Prometheus replays its WAL at startup and consumes a lot of memory doing so, adding a Limit over the memory was of no use. It was OOMKill during the WAL process, restarted, OOMKilled again, etc. . Therefore, we isolated Prometheus on nodes having a lot of memory so it could use up all of it.\n\nThen, one of our main API experienced a huge load, 60% IDLE CPU to 0% in a few seconds. Because of the brutality of such a peak, active pods started to consume all CPU available on nodes, depriving other pods. Getting rid of CPU limits is a recommendation that comes with drawbacks that we measured and chose to follow the recommendation to ensure performance. As a result, the entire cluster went down, lacking for available CPU. Airbnb shared the same experience: they removed CPU limits because of throttling, but the noisy neighbors forced them to re-introduce limits.\n\nWe tried to isolate this API on its own nodes, as such peaks can repeat in the future, because it’s uncacheable and userfacing. We added Taints on dedicated nodes and Tolerations on the selected API.\n\nSince then, we had to deploy a dedicated overprovisioning on those nodes as the overprovisioning pods didn’t have this Toleration. It turned out we’re also able to adapt the overprovisioning specifically for this API, which wasn’t the base idea, but it has proven to be very effective due to the API’s nature. We talk more about overprovisioning’s conf a little later on (Scalability/Overprovisioning).\n\nNow, we’re setting CPU limits, at least for all applications not using dedicated nodes and also because we’ve updated our kernels to the patched version. We follow their CPU usage through Prometheus alerting, with:\n\n- labels:\n    severity: notice\n    cluster_name: &quot;{ { $externalLabels.cluster_name } }&quot;\n  annotations:\n    alertmessage: &#39;{ { $labels.namespace } }/{ { $labels.pod } }/{ { $labels.container } } : { { printf &quot;%0.0f&quot; $value } }%&#39;\n    description: Container using more CPU than expected.\n      It will soon be throttled, which has a negative impact on performances.\n    summary: &quot;{ { $externalLabels.cluster_name } } - Notice - K8S - Container using 90% CPU Limit&quot;\n  alert: Notice - K8S - Container getting close to its CPU Limit\n  expr: |\n    (\n      sum(rate(container_cpu_usage_seconds_total{job=&quot;kubelet&quot;, container!=&quot;POD&quot;, container!=&quot;&quot;}[1m])) by (container, namespace, pod)\n    / sum(kube_pod_container_resource_limits_cpu_cores{job=&quot;kube-state-metrics&quot;, container!=&quot;POD&quot;, container!=&quot;&quot;}) by (container, namespace, pod)\n    ) * 100 &amp;gt; 90\n\n\nWe don’t currently have alerting on Throttling, only a Grafana graph using the metric:\n\nsum by (pod) (rate(container_cpu_cfs_throttled_seconds_total{job=&quot;kubelet&quot;, image!=&quot;&quot;,container!=&quot;POD&quot;}[1m]))\n\nAfter Prometheus, we later isolated Victoria Metrics and Grafana Loki on their own ASGs.\nWe’re also isolating “admin” tools, like CoreDNS, cluster-autoscaler, HAProxy Ingress Controller, on dedicated “admin nodes” group. That way, admin tools can’t mess with applications pods and vice versa.\n\n\nDevelopers only deploy to Worker nodes. An application’s pods can only be scheduled on 4 ASGs, including 2 on-demand backups.\n\nOur admin nodes are on-demand. Having an ASG of few nodes all Spot is a risk we didn’t want to take regarding the criticality of those pods.\n\nQOS Guaranteed Daemonsets\n\nAll our Daemonsets have Requests and Limits set at the same value.\nWe’ve found out that a lot of Daemonsets don’t define those values by default.\nEnforcing QOS Guaranteed Daemonsets:\n\n\n  ensures our daemonsets request all the resources they need, which is also important for the k8s scheduler to be more effective\n  daemonsets bad behaviours can be contained through Limits, and will not mess up with pods\n  it’s a good indicator of the overhead we add on each node and helps us choose our EC2 instance types better (E.g: 2x.large instances are too small)\n  it’s a reminder that a server with 16 CPUs has in fact only 80% of them usable by application pods\n\n\nScalability\n\nCluster Autoscaler\n\nWe automatically scale our EC2 Instances with cluster-autoscaler.\n\n\n\nAs mentioned before, we have several AutoScalingGroups per cluster.\nWe use the service discovery feature of cluster-autoscaler to find all ASGs to work with and to control them automatically.\nThis is done in two steps:\n\n\n  We add 2 tags on ASGs that the cluster-autoscaler should manage\n\n\nk8s.io/cluster-autoscaler/enabled: &quot;true&quot;\nk8s.io/cluster-autoscaler/{ { $cluster.name } }: &quot;true&quot;\n\n\n\n  Then, inside the Chart, we add those two labels to the node-group-auto-discovery parameter:\n\n\ncommand:\n- ./cluster-autoscaler\n- --cloud-provider=aws\n- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/{ { index .Values.nodes .Values.env &quot;clusterName&quot; } }\n…\n\n\nExpander Priority\n\nWe use cluster-autoscaler with the expander: priority.\nASGs will be chosen as:\n\n\n  spot-nodes-.*\n  on-demand-.*\n\n\nCluster-autoscaler will randomly add an EC2 instance in an ASG in the first group: spot-nodes-*. If a new instance hasn’t joined the cluster after the fallback timeout (--max-node-provision-time), it will try another ASG in the same group. It will try all the ASGs in this group before moving on to the next group: on-demand-*.\n\nWith a dozen ASGs, most of them being Spot, we’ve already waited for 45 minutes to actually be able to successfully add an EC2 instance.\n\nLaunching an EC2 instance sometimes fails with InsufficientInstanceCapacity, especially for Spot instances. With the autoscaler recommendation to split ASGs by the same amount of CPU/RAM, there were just too many ASGs to try before falling back on-demand. We’ve reduced the cluster-autoscaler fallback timeout to 5 minutes and still are facing many scaling problems at Paris, where it seems there are not many Spot instances available.\n\n\n\nExpander priority allows us to have resilience through an automatic fall back to on-demand when there is no more Spot.\nWe have already faced, multiple times, a fallback to on-demand instances even with a dozen different instance types. InsufficientInstanceCapacity errors are not a myth. Even on-demand instances can be in InsufficientInstanceCapacity, which we hope to never face with expander priority, 10+ Spot instance types, 10+ on-demand instance types and low --max-node-provision-time.\n\nOverprovisioning\n\nWe have overprovisioning pods inside the cluster.\nThe objective is to trigger a node scale-up before a legitimate pod actually needs resources. Doing so, the pod doesn’t wait minutes to be scheduled, but a few seconds. This need for speed is linked to our business and sometimes the television audience bringing us many viewers very quickly.\n\nThis works using overprovisioning pods which request resources without doing anything (docker image: k8s.gcr.io/pause). Those pods are also using a low PriorityClass (-10), lower than our apps.\n\nThis trick is the whole magic of this overprovisioning: we request space that can be reclaimed anytime and very quickly. When an app needs it, the Scheduler will free up this space by expelling overprovisioning pods (of lower priority) because the cluster doesn’t have enough free space. The expelled pods then change their state to Pending with the Reason: Unschedulable because we just filled the cluster with higher priority pods from the app. Presence of Pending Pods with Unschedulable reason trigger the cluster-autoscaler to add nodes.\n\nWe follow the efficiency of this overprovisioning with these Prometheus expressions:\n\n\n  kube_deployment_status_replicas_unavailable: we know which pods are waiting to be scheduled,\n  sum(kube_node_status_condition{condition=&quot;Ready&quot;,status=&quot;false&quot;}): we know if there are UnReady nodes, like when nodes are scaling-up and new nodes don’t have their daemonsets Ready.\n\n\nBecause we have some nice load peaks on our applications, we are using the ladder mode of the overprovisioning. That ensures that we always have a minimum amount of overprovisioning running in the cluster, so we’re able to handle huge loads at any time. Also, we ensure that we don’t waste too much resources when heavily loaded, so we don’t reserve 200 nodes in a cluster of 1000 nodes for example.\n\nThe configmap looks like:\n\ndata:\n  ladder: &#39;{&quot;coresToReplicas&quot;:[[16,4],[100,10],[200,20]]}&#39;\n\n\nWe chose to have big overprovisioning pods, bigger than any other pod in the cluster, to ensure that expelling one of the overprovisioning pods is enough to schedule any Pending pod.\n\nPriorityClass\n\nWe sacrifice some applications when overprovisioning is not enough.\n\nThe overprovisioning magic is based on PriorityClass objects.\nWe’re using the same logic for our other applications, using PriorityClass.\nWe have 3 of them which concern applications:\n\n\n  low: -5\n  default: 0\n  high: 5\n\n\nCritical applications are using the “high” PriorityClass.\nMost applications are using the “default” one, so they don’t even have to explicitly use it.\nWorkers doing asynchronous tasks can be cut off for several tens of minutes without any business impact. These are the ones with the “low” PriorityClass we sacrifice when needed.\n\nHere is an example, during a heavy load :\n\nHundreds of unavailable pods for 10 minutes.\n\nIf we filter out “low” PriorityClass pods in the graph above, there’s only one application having unavailable pods:\n\nNew pods for this application stayed in the Unavailable state for 15 seconds.\n\nLow HPA targets\n\nKubernetes takes time to scale-up pods.\n\nWithout overprovisioning, we’ve measured that we wait up to 4 minutes when there’s no available node where pods can be scheduled.\nThen, with overprovisioning, we mostly wait for 45seconds, between the moment the HorizontalPodAutoscaler changes the Replicas of a Deployment and for those pods to be ready and receive traffic.\n\nWe can’t wait so long during our peaks, so we generally define HPA targets at 60%, 70% or 80% of Requests. That gives us more time to handle the load while new pods are being scheduled.\n\nOn the following graphs, we can see two nice peaks at 20h52 and 21h02:\n\nAbove, in green, the number of consumed CPUs for one specific application: +55% in one minute.\n\nBelow, in blue, new pods are created in response to the peak.\n\n\nThis is obviously not a good way of managing resources, as we waste them as soon as the load balances.\nThis waste effect is amplified with the load: the more pods we have, the more we waste.\n\nYou can see it in this graph that shows the number of CPU reserved but not consumed:\n\n\nWe consume more CPU during peaks and therefore, we use more efficiently the reservations that do not have time to move, because we do not yet have scale-up.\nAs soon as the new pods added in response to the peak are Ready, 40% of CPU are wasted again.\n\nWe don’t have a viable solution to solve this.\n\nWe’re thinking about reducing scale-up duration to 10 seconds, so we won’t need these additional resources while we launch new pods. This is a challenge as the scaling mechanism is composed of several tools (metrics-server update frequency, autoscaler controller loop frequency, pod autoscaler initial readiness delay, probe launch times, etc.) and changing only one of them can have catastrophic behavior on the cluster stability. This huge subject will need its own dedicated blogpost…\n\nLong downscale durations\n\nRecently, we have increased the HPA’s downscale durations from 5 to 30 minutes.\n\nIt’s done through Kops spec:\n  kubeControllerManager:\n    horizontalPodAutoscalerDownscaleStabilization: 30m0s\n\n\nWhen an application fails, its traffic decreases. When front A fails, traffic on backend B decreases too. When front A comes back, both A and B will have a big peak load.\n\nThe default five minutes delay for scaling down pods is too short for us. Increasing this delay makes the return to life of front A transparent on the number of pods of the whole platform, at least for the first 30 minutes of shutdown.\n\nWe’ve seen blog posts where people turn off autoscaling for those very situations.\nFailure is not an extreme case. Failure is expected. Autoscaling strategies must adapt to it.\n\nYou can see that one waits 30 minutes after an upscale, before downscaling:\n\n\nDocumentation specifies that: “this duration specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed.”\n\nWe can observe on the graph above that it’s rather: “this duration specifies how long the autoscaler has to wait to perform a downscale after the last upscale”.\n\nObservability\n\nMetrics\n\nWe scrape application and system metrics via Prometheus.\nWe’re using Victoria Metrics as long term storage. We found it really easy to deploy and it needs really few time to administer on a daily basis, unlike Prometheus.\n\nDetails:\n\n\n  Prometheus scrapes metrics of pods having:\n\n\nannotations:\n  prometheus.io/path: /metrics\n  prometheus.io/port: &quot;8080&quot;\n  prometheus.io/scrape: &quot;true&quot;\n\n\n\n  Then, inside prometheus jsonnet files, we define a remoteWrite pointing to VictoriaMetrics:\n\n\nremote_write:\n- url: https://victoria-metrics-cluster-vminsert.monitoring.svc.cluster.local.:8480/insert/001/prometheus\n  remote_timeout: 30s\n  write_relabel_configs:\n  - separator: ;\n    regex: prometheus_replica\n    replacement: $1\n    action: labeldrop\n  queue_config:\n    capacity: 50000\n    max_shards: 30\n    min_shards: 1\n    max_samples_per_send: 10000\n    batch_send_deadline: 5s\n    min_backoff: 30ms\n    max_backoff: 100ms\n…\n\n\nWe have 2 Prometheus pods per cluster, each on separate nodes.\nEach Prometheus scrapes all metrics in the cluster, for resilience.\nThey have a really low retention (few hours, because of the WAL replay issue) and are deployed on Spot instances.\n\nWe have 2 Victoria Metrics pods per cluster (cluster version), each on separate nodes, separated of Prometheus pods through a podAntiAffinity\naffinity:\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 100\n      podAffinityTerm:\n        labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - prometheus\n        topologyKey: &quot;kubernetes.io/hostname&quot;\n\n\nEach Victoria Metrics pod receives all metrics in duplicate, from the two prometheus pods.\nWe use the command-line flag dedup.minScrapeInterval: 15s to deduplicate metrics.\n\nWe’re thinking about totally removing Prometheus from the mix, using only Victoria Metrics Agent to scrape metrics.\n\nLogs\n\nWe collect stderr and stdout of all our containers.\nWe use fluentd for that, as a DaemonSet, which uses the node’s /var/log/containers directory.\nWe use Grafana Loki as an interface to filter those logs.\n\nOur developers catch most of their logs and send them directly to Elasticsearch. Fluentd and Loki are used only for uncatched errors and have little traffic.\n\nFluentd uses around 200MB of memory per node and so we look at replacing it by promtail which uses only 40MB in our case.\n\n\nWe’re happy with Loki, because we have few logs to parse. We’ve tested to get our Ingress Controller access logs sent to Loki and it was a nightmare. Too many entries to parse.\n\nThere’s a default limit of 1000 log entries when querying, which we raised but then Grafana became very slow. Very very slow. 3000 log entries is the best fit for us.\n\nAlerting\n\nWe mostly use alerts defined in the official prometheus-operator repo.\n\nWe also added some alerts of our own. E.g: an alert when our Ingress Controller can’t connect to a pod:\n- labels:\n    severity: critical\n    cluster_name: &quot;{ { $externalLabels.cluster_name } }&quot;\n  annotations:\n    alertmessage: &#39;{ { $labels.proxy } } : { { printf &quot;%.2f&quot; $value } } requests in error per second&#39;\n    description: &#39;HAProxy pods cannot send requests to this application. Connection errors may happen when one or more pods are failing or there&#39;&#39;s no more healthy pods : Application is crashed !!&#39;\n    summary: &quot;{ { $externalLabels.cluster_name } } - Critical - K8S - HAProxy IC - Backend connection errors&quot;\n  alert: Critical -K8S - HAProxy IC - Backend connection errors\n  expr: |\n    sum(rate(haproxy_backend_connection_errors_total[1m])) by (proxy) &amp;gt; 0\n  for: 1m\n\n\nPrometheus generates alerts that it sends to 2 redundant AlertManager instances, in a separate account that centralises alerts from all our clusters.\nWe have several possibilities then:\n\n\n  Send alerts on Slack dedicated channels\n  Send alerts to PagerDuty for the on-call teams\n\n\nOur developers are managing their own alerts (Kubernetes CRD: PrometheusRule) that are following a different path regarding labels defined. They have their own alerts sent in their own channels.\n\nCosts\n\nSpot instances\n\nWe’re running 100% of our application workloads on Spot instances.\n\nIt was easy at first: implement spot-termination-handler and voilà.\nIndeed, but that was only the first step.\n\nInter accounts reclaims\n\nWe created AWS accounts for salto.fr platform, for which we did a lot of load tests with on-demand servers.\nThat’s when we reclaimed our own instances on our other accounts.\n\n\n\nYour accounts are not “linked” to each other in terms of Spot reclaims. Having resources in the same region with different accounts creates a relationship itself that we had never though about.\nIn this case, launching on-demand instances on one account triggered reclaims on our other accounts in the same region.\n\nOn-demand fallback\n\nWe didn’t have on-demand fallback for a year and it went well.\nThere was enough spot capacity and there was no need for fallback. Therefore, we didn’t prioritize automated on-demand fallbacks.\n\nThen, all our instance types (+10) went InsufficientInstanceCapacity at the same time.\nWe could only work around with a manual ASG we have from our first days on Kubernetes at AWS, on which we could launch on-demand instances as a last-resort fallback.\n\nNow, we’re using cluster-autoscaler with the expander: priority to automatically fallback on lower priority ASGs (see above Scalability/Cluster-autoscaler).\n\nIt takes us around 10mn to start a node when all our instances are InsufficientInstanceCapacity.\nThere are other mechanisms that directly detect InsufficientInstanceCapacity on an ASG, so we wouldn’t have to wait 5mn before moving on to the next one. We’re thinking about implementing them, but they’re not really compatible with cluster-autoscaler right now.\n\nAs of today, we have two ASGs per application group, as Spot, and also two ASGs as on-demand automatic fallback.\n\nDraino and node-problem-detector\n\nThe problem came when downscaling : cluster-autoscaler removes the least used node, no matter if it’s a Spot or an on-demand instance.\n\nWe found ourselves with a lot of on-demand nodes after load peaks and they stayed on. And they cost a lot more than Spot instances.\n\nWe were already using node-problem-detector, so we added draino, to detect if an instance is on-demand and try to remove it when it is. Draino waits for 2h after the node is launched before trying to remove it.\n\nSince then, we use on-demand only when there’s no Spot left and only for a few hours.\n\nWe can see on this graph, that we added automated on-demand fallback and we never stopped having on-demand instances, until we added draino:\n\n\nSpot Tips\n\n\n  You need to be in an “old” AWS region to have a large number of Spot available. I.E. consider eu-west-1 instead of eu-west-3, even if it adds latency,\n  Use the maximum number of instance types possible. A dozen is barely enough. I.E. use all instance family letters regardless of what they’re optimised for (compute, memory) as long as your workload can use it,\n  Use CapacityOptimized spot allocation strategy, to limit reclaims to the strict necessary,\n  Do not use Spot on a single AZ (this advice is not limited to spot),\n  Prepare yourself to large reclaims, dozens at a time,\n  Configure and test your on-demand fallback\n\n\nKube-downscaler\n\nOpen-source project from Zalando which allows us to scale down Kubernetes deployments after work hours: nights and week-ends.\n\nWe use it on all our staging clusters. We save 60% of EC2 instances.\n\nHAProxy Ingress Controller\n\nThe whole traffic of a cluster goes through a single ALB.\n\nWe load-balance traffic to the correct pod through HAProxy, which uses Ingress rules to update its configuration.\nWe explained the way HAProxy Ingress controller lives inside the cluster during a talk at the HAProxy Conf in 2019.\n\nReducing the number of managed load balancers at AWS isn’t the only benefit of HAProxy: we have tons of metrics in a single Grafana dashboard. Requests number, errors, retries, response times, connect times, bad health checks, etc.\n\nWhat’s next\n\nA lot has been done to have scalability, resilience, observability and reasoned costs over the last 3 years.\n\nUsing Kubernetes in production is not that simple.\nIt is necessary to be well equipped, to understand finely the workings of the kubernetes mechanics to find the balance that suits us. Avoid falling into the trap of adopting whatever tool everyone is talking about if you don’t need it. There’s a lot of hype around kubernetes and the cloud, it can be dangerous.\n\nThe next step will be for us to increase resilience as much as possible, while at the same time reducing costs.\nPerhaps it will be by speeding up the start-up of pods. Maybe it won’t work. Maybe we can make some modifications to cluster-autoscaler to make it more compatible with aws events (like InsufficiantInstanceCapacity). But we will certainly work around the costs.\n\n\n\nThanks to all the reviewers, for their good advice and their time ❤️\n"
} ,
  
  {
    "title"    : "PHP, c’est vous ! Et vous pouvez contribuer !",
    "category" : "",
    "tags"     : " conference, php, open-source, afup",
    "url"      : "/2020/11/23/php-cest-vous-et-vous-pouvez-contribuer-pascal-martin.html",
    "date"     : "November 23, 2020",
    "excerpt"  : "Quand nous posons la question « qui contribue à PHP ? » lors des évènements que nous organisons ou auxquels nous participons, nous n’obtenons que très peu de réponses. Est-ce parce que peu d’entre nous savent ou aiment coder en C ? Pourtant, parti...",
  "content"  : "Quand nous posons la question « qui contribue à PHP ? » lors des évènements que nous organisons ou auxquels nous participons, nous n’obtenons que très peu de réponses. Est-ce parce que peu d’entre nous savent ou aiment coder en C ? Pourtant, participer et contribuer ne se limite pas à des lignes de code, loin de là !\n\nAvez-vous trouvé l’éditeur en ligne de la documentation de PHP ? Avez-vous soumis des patchs à composer et symfony ou même à magento et wordpress, qui sont open-source et attendent vos contributions ? Avez-vous testé les versions alpha de PHP 8 ? Voyez-vous comment organiser un Apéro PHP ou un Meetup ? Ou même un AFUP Day ou le Forum PHP ? Savez-vous que l’AFUP a besoin de vous ? Que c’est une association qui est là pour vous aider et ce qu’elle peut vous apporter ?\n\nAvec mille et une façons de contribuer, venez faire un tour d’horizon de modes de contribution que vous n’aviez peut-être pas encore envisagés, ou dont vous vous étiez dit qu’ils n’étaient pas pour vous. Vous verrez que, vous aussi, vous pouvez contribuer à PHP ;-)\n"
} ,
  
  {
    "title"    : "L&#39;open source, ce n&#39;est pas que pour le web",
    "category" : "",
    "tags"     : " conference, afup, open-source",
    "url"      : "/2020/10/23/l'open-source-ce-n-est-pas-que-pour-le-web.html",
    "date"     : "October 23, 2020",
    "excerpt"  : "Une conférence sur l’open source hors des solutions informatiques uniquement, lors du forum PHP 2020 qui marquait les 20 ans de l’AFUP.\n\nConnaissez-vous l’open hardware ? Savez-vous ce que la NASA partage sur Github ? Vous avez certainement déjà é...",
  "content"  : "Une conférence sur l’open source hors des solutions informatiques uniquement, lors du forum PHP 2020 qui marquait les 20 ans de l’AFUP.\n\nConnaissez-vous l’open hardware ? Savez-vous ce que la NASA partage sur Github ? Vous avez certainement déjà écouté, ou produit de la musique open source, savez-vous qu’il existe des médicaments open source ? Répliquer une information, et la partager devient rapide et émancipateur, le monde se libère un peu plus.\n\nAprès une petite plongée dans les principes de partage de l’open source, nous ferons un tour d’horizon des initiatives open source dans d’autres domaines que l’informatique, pour en apprendre un peu plus sur la culture du libre, les rapports de force qui y conduisent, et revenir aux bases du partage.\n"
} ,
  
  {
    "title"    : "La scalabilité d’une équipe / d’un pôle technique",
    "category" : "",
    "tags"     : " conference, forumPHP, PHP, Symfony",
    "url"      : "/2020/10/22/la-scalabilite-d-une-equipe-d-un-pole-technique.html",
    "date"     : "October 22, 2020",
    "excerpt"  : "Vous êtes dans l’équipe technique d’une entreprise, composée de quelques développeurs, dans 1 ou 2 équipes, et votre entreprise grandit, et il faut augmenter la capacité de production, et donc la taille de l’équipe technique. Sauf que comme 9 femm...",
  "content"  : "Vous êtes dans l’équipe technique d’une entreprise, composée de quelques développeurs, dans 1 ou 2 équipes, et votre entreprise grandit, et il faut augmenter la capacité de production, et donc la taille de l’équipe technique. Sauf que comme 9 femmes ne font pas un bébé un 1 mois, 4 équipes de 6 personnes ne produisent pas automatiquement 2 fois plus que 2 équipes de 6 développeurs.\n\nJe me propose de vous faire un retour d’expérience sur comment nous avons abordé la scalabilité du pôle technique de Bedrock, pour passer de 10 équipes réparties en 3 verticaux techniques, à plus de 30 équipes dans 5 verticaux techniques, en essayant de conserver une cohésion technique et fonctionnelle, et d’optimiser les flux de développements.\n\n"
} ,
  
  {
    "title"    : "DevOps ? Je n&#39;ai jamais voulu faire ça, et pourtant …",
    "category" : "",
    "tags"     : " conference, afup, php, devops",
    "url"      : "/2020/06/24/devops-je-n-ai-jamais-voulu-faire.html",
    "date"     : "June 24, 2020",
    "excerpt"  : "Développeuse junior : première semaine. Mes collègues m’ont forcée à déployer ma première feature sur 6play ! Malgré un petit frisson, tout s’est bien passé, grâce aux outils et bonnes pratiques qui nous guident.\n\nCe n’était que le début ! Depuis,...",
  "content"  : "Développeuse junior : première semaine. Mes collègues m’ont forcée à déployer ma première feature sur 6play ! Malgré un petit frisson, tout s’est bien passé, grâce aux outils et bonnes pratiques qui nous guident.\n\nCe n’était que le début ! Depuis, je gère l’infrastructure de mon projet. Je choisis mes bases de données, caches, mécanismes de stockage, ressources… En prenant en compte leur coût, les modes de backups ou les compétences dans nos équipes. Et je suis libre d’expérimenter avec n’importe quel service que je voudrais tester.\n\nEn un an, je suis passée de “simple développeuse” à quelqu’un qui a conscience de sa plateforme, qui monitore son code et est responsable de sa production. Comment ai-je vécu cette transition ? Comment ai-je grandi en tant que développeuse ?\n\nVous aussi, profitez de votre nouvelle liberté : devenez DevOps !\n"
} ,
  
  {
    "title"    : "6play_API-v2-Final(1).doc",
    "category" : "",
    "tags"     : " conference, php, afup, api",
    "url"      : "/2020/06/24/6play_API-v2-Final(1).html",
    "date"     : "June 24, 2020",
    "excerpt"  : "Votre API est confrontée à des contraintes techniques mais elle doit surtout répondre à vos problématiques métier qui ne cessent d’évoluer. Nous avons souvent vécu cette situation pour 6play (service de Replay du Groupe M6), et il nous a fallu plu...",
  "content"  : "Votre API est confrontée à des contraintes techniques mais elle doit surtout répondre à vos problématiques métier qui ne cessent d’évoluer. Nous avons souvent vécu cette situation pour 6play (service de Replay du Groupe M6), et il nous a fallu plusieurs générations d’API avant d’arriver à une version adaptée à nos besoins. Micro-services, Rest/GraphQL, Developer eXperience… Un récit et des conseils pragmatiques pour concevoir et maintenir votre API.\n"
} ,
  
  {
    "title"    : "React/Redux: pitfalls and best practices",
    "category" : "",
    "tags"     : " js, react, redux, frontend",
    "url"      : "/2020/04/27/react-redux-pitfalls-and-best-pratices.html",
    "date"     : "April 27, 2020",
    "excerpt"  : "After 2 years using React with Redux for the video platform 6play, I was able to identify good practices and pitfalls to avoid at all costs.\nThe Bedrock team kept the technical stack of the project up to date to take advantage of the new features ...",
  "content"  : "After 2 years using React with Redux for the video platform 6play, I was able to identify good practices and pitfalls to avoid at all costs.\nThe Bedrock team kept the technical stack of the project up to date to take advantage of the new features of react, react-redux and redux.\n\nSo here are my tips for maintaining and using React and Redux in your application without going mad.\n\nThis article is not an introduction to React or Redux. I recommend this documentation if you want to see how to implement it in your applications.\n\nYou could also take a look at Redux offical style guide in which you could find some of those tips and others.\nNote that if you use the Redux Toolkit, some of the tips/practices presented in this article are already integrated directly into the API.\n\nAvoid having only one reducer\n\nThe reducer is the function that is in charge of building a new state at each action.\nOne might be tempted to manipulate only one reducer.\nIn the case of a small application, this is not a problem.\nFor applications expressing a complex and evolving business, it is better to opt in for the combineReducers solution.\n\nThis feature of redux allows to manipulate not one but several reducers which act respectively on the state.\n\n\n  When and how to split its application?\n\n\nWhat we recommend at Bedrock is a functional splitting of the application.\nIn my approach, we would tend to represent the business of the application more than the technical stuff implied.\nSome very good articles explain it notably through the use of DDD principles.\n\nIn Bedrock, we use a folder named modules which groups together the different folders associated with the feature of your application.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.reducer.js\n    product/\n      __tests__/\n        product.reducer.spec.js\n      components/\n      product.reducer.js\n    account/\n      __tests__/\n      account.reducer.spec.js\n      components/\n      account.reducer.js\n  store.js\n  index.js\n\n\nSo in store.js all you need to do is combine your different reducers.\n\nimport { createStore, combineReducers } from &#39;redux&#39;\nimport { user } from &#39;./modules/user/user.reducer.js&#39;\nimport { product } from &#39;./modules/user/product.reducer.js&#39;\nimport { account } from &#39;./modules/user/account.reducer.js&#39;\n\nexport const store = createStore(combineReducers({ user, product, account }))\n\n\nBy following this principle, you will:\n\n\n  keep reducers readable because they have a limited scope\n  structure and define the functionalities of your application\n  facilitate the testing\n\n\nHistorically, this segmentation has allowed us to remove complete application areas without having impacts on the entire codebase, just by deleting the module folder associated with the feature.\n\nProxy access to the state\n\nNow that your reducers have been placed in the functional module, you need to allow your components to access the state via selector.\nA selector is a function that has the state as a parameter, and retrieves its information.\nThis can also allow you to select only the props needed for the component by decoupling from the state structure.\n\nexport const getUserName = ({ user: { lastName } }) =&amp;gt; lastName\n\n\nYou can also pass parameters to a selector by wrapping it with a function.\n\nexport const getProduct = productId =&amp;gt; ({ product: { list } }) =&amp;gt;\n  list.find(product =&amp;gt; product.id === productId)\n\n\nThis will allow you to use them in your components using the useSelector hook.\n\nconst MyComponent = () =&amp;gt; {\n  const product = useSelector(getProduct(12))\n  return &amp;lt;div&amp;gt;{product.name}&amp;lt;/div&amp;gt;\n}\n\n\nIt is specified in the react-redux doc that the selector is called for each render of the component.\nIf the selector function reference does not change, a cached version of the object can be returned directly.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.reducer.js\n      user.selectors.js &amp;lt;--- This is where all module selectors are exported\n\n\nPrefix the name of your actions\n\n\n  I really advise you to define naming rules for your actions and if possible check them with an eslint rule.\n\n\nActions are in uppercase letters separated by ‘_’.\nHere an example with this action: SET_USERS.\n\napp/\n  modules/\n    user/\n      __tests__/\n        user.reducer.spec.js\n      components/\n      user.actions.js &amp;lt;--- This is where all module action creators are exported\n      user.reducer.js\n      user.selectors.js\n\n\nAction names are prefixed by the name of the module in which it is located.\nThis gives a full name: user/SET_USERS.\nA big advantage of this naming rule is that you can easily filter the action in redux-devtools.\n\n\n\nAlways test your reducers\n\nThe reducers are the holders of your application’s business.\nThey manipulate the state of your application.\n\nThis code is therefore sensitive.\n\n➡️ A modification can have a lot of impact on your application.\n\nThis code is rich in business rules\n\n➡️ You must be confident that these are correctly implemented.\n\nThe good news is that this code is relatively easy to test.\nA reducer is a single function that takes 2 parameters.\nThis function will return a new state depending on the type of action and its parameters.\n\nThis is the standard structure for testing reducers with Jest:\n\ndescribe(&#39;ReducerName&#39;, () =&amp;gt; {\n  beforeEach(() =&amp;gt; {\n    // Init a new state\n  })\n  describe(&#39;ACTION&#39;, () =&amp;gt; {\n    // Group tests by action type\n    it(&#39;should test action with some params&#39;, () =&amp;gt; {})\n    it(&#39;should test action with other params&#39;, () =&amp;gt; {})\n  })\n  describe(&#39;SECOND_ACTION&#39;, () =&amp;gt; {\n    it(&#39;should test action with some params&#39;, () =&amp;gt; {})\n  })\n})\n\n\nI also recommend that you use the deep-freeze package on your state to ensure that all actions return new references.\n\nUltimately, testing your reducers will allow you to easily refactor the internal structure of their state without the risk of introducing regressions.\n\nKeep the immutability and readability of your reducers\n\nA reducer is a function that must return a new version of the state containing its new values while keeping the same references of the objects that have not changed.\nThis allows you to take full advantage of Structural sharing and avoid exploding your memory usage.\nThe use of the spread operator is thus more than recommended.\n\nHowever, in the case where the state has a complicated and deep structure, it can be verbose to change the state without destroying the references that should not change.\n\nFor example, here we want to override the Rhone.Villeurbanne.postal value of the state while keeping the objects that don’t change.\n\nconst state = {\n  Rhone: {\n    Lyon: {\n      postal: &#39;69000&#39; ,\n    },\n    Villeurbanne: {\n      postal: &#39;&#39;,\n    },\n  },\n  Isère: {\n    Grenoble: {\n      postal: &#39;39000&#39;,\n    },\n  },\n}\n\n// When you want to change nested state value and use immutability\nconst newState = {\n  ...state,\n  Rhone: {\n    ...state.Lyon,\n    Villeurbanne: {\n      postal: &#39;69100&#39;,\n    },\n  },\n}\n\n\nTo avoid this, a member of the Bedrock team released a package that allows to set nested attribute while ensuring immutability: immutable-set\nThis package is much easier to use than tools like immutable.js because it does not use Object prototype.\n\nimport set from &#39;immutable-set&#39;\n\nconst newState = set(state, `Rhone.Villeurbanne.postal`, &#39;69100&#39;)\n\n\nDo not use the default case\n\nThe implementation of a redux reducer very often consists of a switch where each case corresponds to an action.\nA switch must always define the default case if you follow so basic eslint rules.\n\nLet’s imagine the following reducer:\n\nconst initialState = {\n  value: &#39;bar&#39;,\n  index: 0,\n}\n\nfunction reducer(initialState, action) {\n  switch (action.type) {\n    case &#39;FOO&#39;:\n      return {\n        value: &#39;foo&#39;,\n      }\n    default:\n      return {\n        value: &#39;bar&#39;,\n      }\n  }\n}\n\n\nWe can naively say that this reducer manages two different actions. It’s okay.\nIf we isolate this reducer there are only two types of action that can change this state; the FOO action and any other action.\n\nHowever, if you have followed the advice to cut out your reducers, you don’t have only one reducer acting on your blind.\n\nThat’s where the previous reducer is a problem.\nIndeed, any other action will change this state to a default state.\nA dispatch action will pass through each of the reducers associated with this one.\nAn action at the other end of your application could affect this state without being expressed in the code.\nThis should be avoided.\n\n\n\nIf you want to modify the state with an action from another module, you can do so by adding a case on that action.\n\nfunction reducer(state = initialState, action) {\n  switch (action.type) {\n    case &#39;FOO&#39;:\n      return {\n        value: &#39;foo&#39;,\n      }\n    case &#39;otherModule/BAR&#39;:\n      return {\n        value: &#39;bar&#39;,\n      }\n    default:\n      return state\n  }\n}\n\n\nUse custom middlewares\n\nI’ve often seen action behaviors being copied and pasted, from action to action.\nWhen you’re a developer, “copy-paste” is never the right way.\n\nThe most common example is handling HTTP calls during an action that uses redux-thunk.\n\nexport const foo = () =&amp;gt;\n  fetch(&#39;https://example.com/api/foo&#39;)\n    .then(data =&amp;gt; ({ type: &#39;FOO&#39;, data }))\n    .catch(error =&amp;gt; {\n      // Do something\n    })\n\nexport const bar = () =&amp;gt;\n  fetch(&#39;https://example.com/api/bar&#39;)\n    .then(data =&amp;gt; ({ type: &#39;BAR&#39;, data }))\n    .catch(error =&amp;gt; {\n      // Do something\n    })\n\n\nThese two actions are basically the same thing, we could very well make a factory that would do the code in common.\n\nBasically the meta action we want to represent here when it is dispatched:\n\nFetch something\n-- return action with the result\n-- in case or error, do something\n\n\nWe could very well define a middleware that would take care of this behavior.\n\nconst http = store =&amp;gt; next =&amp;gt; async action =&amp;gt; {\n  if (action.http) {\n    try {\n      action.result = await fetch(action.http)\n    } catch (error) {\n      // Do something\n    }\n  }\n  return next(action)\n}\n\n// in redux store init\nconst exampleApp = combineReducers(reducers)\nconst store = createStore(exampleApp, applyMiddleware(http))\n\n\nThus the two preceding actions could be written much more simpler:\n\nexport const foo = () =&amp;gt; ({ type: &#39;FOO&#39;, http: &#39;https://example.com/api/foo&#39; })\n\nexport const bar = () =&amp;gt; ({ type: &#39;BAR&#39;, http: &#39;https://example.com/api/bar&#39; })\n\n\nThe big advantages of using middleware in a complex application:\n\n\n  avoids code duplication\n  allows you to define common behaviors between your actions\n  standardize redux meta action types\n\n\nAvoid redux related rerender\n\nThe trick when using redux is to trigger component re-render when you connect them to the state.\nEven if rerenders are not always a problem, re-render caused by the use of redux really has to be prevented.\nJust beware of the following traps.\n\nDo not create a reference in the selector\n\nLet’s imagine the next selector:\n\nconst getUserById = userId =&amp;gt; state =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId) || {}\n\n\nThe developer here wanted to ensure that its selector is null safe and always returns an object.\nThis is something we see quite often.\n\nEach time this selector will be called for a user not present in the state, it will return a new object, a new reference.\n\n\n  With useSelector, returning a new object every time will always force a re-render by default.\nDoc of react-redux\n\n\nHowever in the case of an object, as in the example above (or an array), the reference of this default value is new each time the selector is executed.\nSimilarly for the default values in destructuring, you should never do this :\n\nconst getUsers = () =&amp;gt; ({ users: [] }) =&amp;gt; users\n\n\nWhat to do then?\nWhenever possible, the default values should be stored in the reducer.\nOtherwise, the default value must be extracted into a constant so that the reference remains the same.\n\nconst defaultUser = {}\n\nconst getUserById = userId =&amp;gt; state =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId) || defaultUser\n\n\nThe same goes for the selector usage that returns a new ref at each call.\nThe use of the filter function returns a new array each time a new reference even if the filter conditions have not changed.\n\nTo continue, it is important that useSelector does not return a function.\nBasically you should never do this:\n\nconst getUserById = state =&amp;gt; userId =&amp;gt;\n  state.users.find(user =&amp;gt; user.id === userId)\nconst uider = useSelector(getUserById)(userId)\n\n\nA selector should not return a view (a copy) of the state but directly what it contains.\nBy respecting this principle, your components will rerender only if an action modifies the state.\nUtilities such as reselect can be used to implement selectors with a memory system.\n\nDo not transform your data in the components\n\nSometimes the data contained in the state is not in the correct display format.\nWe would quickly tend to generate it in the component directly.\n\nconst MyComponent = () =&amp;gt; {\n  const user = useSelector(getUser)\n\n  return (\n    &amp;lt;div&amp;gt;\n      &amp;lt;h1&amp;gt;{user.name}&amp;lt;/h1&amp;gt;\n      &amp;lt;img src={`https://profil-pic.com/${user.id}`} /&amp;gt;\n    &amp;lt;/div&amp;gt;\n  )\n}\n\n\nHere, the url of the image is dynamically computed in the component, and thus at each render.\nWe prefer to modify our reducers in order to include a profileUrl attribute so that this information is directly accessible.\n\nswitch (action.type) {\n  case `user/SET_USER`:\n    return {\n      ...state,\n      user: {\n        ...action.user,\n        profileUrl: `https://profil-pic.com/${action.user.id}`,\n      },\n    }\n}\n\n\nThis information is then calculated once per action and not every time it is rendered.\n\nDon’t use useReducer for your business data\n\nSince the arrival of hooks, we have many more tools provided directly by React to manage the state of our components.\nThe useReducer hook allows to set a state that can be modified through actions.\nWe’re really very very close to a redux state that we can associate to a component, it’s great.\n\nHowever, if you use redux in your application, it seems quite strange to have to use useReducer.\nYou already have everything you need to manipulate a complex state.\n\nMoreover, by using redux instead of the useReducer hook you can take advantage of really efficient devtools and middlewares.\n\n\n\nUseful resources\n\n\n  Use react with redux doc\n  redux flow animated by Dan Abramov\n\n  redux documentation about middlewares\n  immutable-set\n\n\nThanks to the reviewers: \n@flepretre, \n@mfrachet, \n@fdubost,\n@ncuillery,\n@renaudAmsellem\n"
} ,
  
  {
    "title"    : "How to boost the speed of your webpack build?",
    "category" : "",
    "tags"     : " js, webpack",
    "url"      : "/2020/03/05/hunting-webpack-performances.html",
    "date"     : "March 5, 2020",
    "excerpt"  : "How did I cut in half my project’s webpack build time ?\n\nWho never complained about the infinite duration of a webpack build on a project ?\nI’m currently working on a big web application coded in React/Redux with server side rendering.\nThe applica...",
  "content"  : "How did I cut in half my project’s webpack build time ?\n\nWho never complained about the infinite duration of a webpack build on a project ?\nI’m currently working on a big web application coded in React/Redux with server side rendering.\nThe application exists since 2015 and it has evolved a lot since then\n\n\n\nTLDR;\n\n\n  Never, ever, ever, ever work on performance improvements or optimization without monitoring!\n\n\nIf you want to optimize the duration of a job, you have to monitor precisely the duration of it and all its sub-steps.\nBy doing that, you can really focus on the most expensive task.\nThis will save you from wasting time on optimizations that will have little impact on the system as a whole.\nUse existing monitoring tools! Create them if they don’t exist!\n\nWhat was the problem with webpack ?\n\nFor several weeks/months my colleagues had been complaining about the duration of our yarn build command. \nThe purpose of this command is to build the distributable package of our application in a production target with webpack.\n\nI even heard:\n\n  “This command, I don’t run it locally anymore, it takes too much time.”\n  “My computer starts ventilating heavily every time I run this command. There’s nothing else I can do!”\n\n\nDepending on the machine on which the build was launched, it took between 5 and 12 minutes.\nIt is not possible to have a build that takes so long.\nwebpack is not a slow bundler. \nIt is our use of webpack that makes it slow.\n\nFocus error, a morning lost\n\nSince this command launches a webpack build in production mode, I figured out that the culprit was webpack config itself.\nGiven that I’ve dug deep into webpack, I thought it would be interesting to focus on this performance concern.\nI have indeed open sourced a set of workshop to learn how to use webpack from scratch (https://webpack-workshop.netlify.com).\nSo at the end of January I took one day to improve the situation.\n\nI had my own idea of the task that would take the most. So I tried to improve it, spending my entire morning on it. \nI just managed to gain 17 seconds.\n\nI’m not going to lie, I was very disappointed with what I achieved.\n\nThe concern in my strategy was however obvious. \nI started off with a preconceived idea “This is definitely the stage that takes the longest.”\n\nNothing was objective in my analysis.\nTo improve the performance of an application it is necessary to focus on objective facts.\n\nSuccessful afternoon\n\nWhen I came back from my lunch break, I was motivated to win more than those poor 17 seconds.\nThen I remembered the Pareto principle.\n\n\n  The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes.\nWikipedia\n\n\nThere is probably one step that takes up most of the webpack build time.\nPareto principle adapted to webpack could mean “80% of the build time is caused by 20% of the config”\n\nLet’s find the culprit ! 🎉\n\nI had to determine the build time of each loader, of each plugin.\nI was very lucky, the webpack community has already proposed a plugin that allows to measure everything.\nAnd it is very easy to install. ♥️\n\nSpeed Measure Plugin\n\nHere are the results I got:\n\nSMP  ⏱  \nGeneral output time took 4 mins, 5.68 secs\n\n SMP  ⏱  Plugins\nIgnorePlugin took 57.73 secs\nTerserPlugin took 39.022 secs\nExtractCssChunksPlugin took 3.13 secs\nOptimizeCssAssetsWebpackPlugin took 1.6 secs\nManifestPlugin took 1.55 secs\nWebpackPwaManifest took 0.326 secs\nContextReplacementPlugin took 0.129 secs\nHashedModuleIdsPlugin took 0.127 secs\nGenerateSW took 0.059 secs\nDefinePlugin took 0.047 secs\nEnvironmentPlugin took 0.04 secs\nLoadablePlugin took 0.033 secs\nObject took 0.024 secs\n\n SMP  ⏱  Loaders\nbabel-loader, and \nrev-replace-loader took 2 mins, 11.99 secs\n  module count = 2222\nmodules with no loaders took 1 min, 57.86 secs\n  module count = 2071\nextract-css-chunks-webpack-plugin, and \ncss-loader, and \npostcss-loader, and \nsass-loader took 1 min, 43.74 secs\n  module count = 95\ncss-loader, and \npostcss-loader, and \nsass-loader took 1 min, 43.61 secs\n  module count = 95\nfile-loader, and \nrev-replace-loader took 4.86 secs\n  module count = 43\nfile-loader took 2.67 secs\n  module count = 32\nraw-loader took 0.446 secs\n  module count = 1\n@bedrock/package-json-loader took 0.005 secs\n  module count = 1\nscript-loader took 0.003 secs\n  module count = 1\n\n\nAs expected, it’s not great! \nBut at least I’m starting to get who the culprits are.\nWe can see that for 2222 Javascript modules takes up 2mins but for only 95 Sass files 1min43 🤣.\n\n\n\nDamn node-sass\n\nOnce the migration from node-sass to sass (new Sass re-implementation) and the update of sass-loader, I was shocked!\nIt took me about 10 minutes because there were few breaking changes and I gained more than 1min30 on the build time.\n\nsass-loader made big improvements on performances, you should definitely make sure you use the last version.\n\nI lost a morning on gaining 17 seconds and I spent 10 minutes to win 1min30.🤣\n\nIgnorePlugin, TerserPlugin\n\n\n  \n    TerserPlugin is used to uglify the javascript code in order to reduce its size and readability. It’s a relatively long process, but 39 seconds is too much.\nJust by updating the version of TerserPlugin to use the one integrated in Webpack, I managed to reduce by 20 seconds the build time.\n  \n  \n    IgnorePlugin is a core plugin that was used a lot in our application to avoid loading certain scripts in order to reduce the weight of the site.\nIt was necessary, but today with Webpack we can use much better than that. Dynamic Import, ContextReplacement, there are plenty of solutions. As a general rule, we should avoid compiling files and then not using them.\n  \n\n\nRecommendations from the community\n\nTo improve the build perfs webpack provides a web page listing the actions to take to hunt what takes time.\nI strongly advise to have a look at it.\n\nhttps://webpack.js.org/guides/build-performance/\n\nFinal Result\n\n    SMP  ⏱  \n    General output time took 2 mins, 18.27 secs\n\n\n\n\nBased on precise and concrete measures, I was able to drastically improve the webpack build of my application.\nNo more computers suffering just to compile a bit of JS and SASS.\nI could have lost whole days on futile modifications if I had not measured precisely what penalized the build.\n\nℹ️\n\n  Use Speed Measure Plugin to debug webpack build time\n  Track your build time evolution to detect big evolution before merge\n  Follow webpack performances recommandations\n  Look at webpack 5 new caching strategies\n  Keep your webpack config up to date\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #12",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2020/02/08/m6web-dev-facts-12.html",
    "date"     : "February 8, 2020",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nEn parlant de slack\n\n  stalker ou bosser il faut choisir\n\n\nEn parlant d’ornithorynque:\n\n  Non mais l’australie, c’est le staging du monde\n\n\nLe dresseur\n\n  Mon charisme légendaire a encore frappé… les VPN...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nEn parlant de slack\n\n  stalker ou bosser il faut choisir\n\n\nEn parlant d’ornithorynque:\n\n  Non mais l’australie, c’est le staging du monde\n\n\nLe dresseur\n\n  Mon charisme légendaire a encore frappé… les VPNs filent au pas avec moi…\n\n\nCloudix\n\n  Les gaulois qui avaient peur que le ciel/cloud leur tombe sur la tête, ils étaient visionnaires en fait!\n\n\nLe meilleur support\nDans un bar de Lyon\n\n  Une inconnue: “ je peux pas me connecter à l’amour est dans le pré …”\n\n\n\n  Un dev d’M6web: “c’est quoi ta box ?”\n\n\nDes choix forts\n\n  Si on fait ce choix, c’est comme tuer un chaton\n\n\nObjets trouvés\n\n\nManger ou décider il faut choisir\n\n  Une réunion sans nourriture devrait être un email.\n\n\nQuand un sysadmin craque une allumette à la data\n\n  Scala de toute manière, ce n’est qu’un wrapper Java\n\n\nC’était prévu\n\n  “Quand on passera en prod le bug”\n\n\nChandeleur\n\n  J’ai de la pâte, j’ai du sucre mais je sais pas faire des grep\n\n\nLa fatigue\n\n  Définition de fatigue =&amp;gt;  “Ah des weekend comme ça je n’en veux plus” … Dit-il un vendredi\n\n\nSSO POWA\n\n  Vivement qu’on passe slack sous le SSO, comme ça les gens ne viendront plus nous dire sur slack qu’ils n’ont pas accès au SSO\n\n\nAquarium\n\n\nToutou\n\n  J’ai fait un fichier waf.tf dans le module waf, ca fait waf waf\n\n\nConfluence\n\n  Mais effectivement à ce niveau c’est aussi facile d’écrire des .txt qu’on se file par clé usb que d’utiliser Confluence.\n\n\nQuickwin\n\n  C’est un quickwin jusqu’à ce qu’on fasse F5\n\n\nJavascript\n\n  Un dev PHP: “C’est du JS mais ça se comprend presque”\n\n\nLa persévérance\n\n\nLes coûts\n\n  C’est le type de projet où il y a plus de jour-homme que d’utilisateurs\n\n\nJe l’ai pas touchéééo\n\n  Un sysadmin: “C’est quoi le foot?”\n\n\n\n  Un autre: “C’est le truc qui fait tomber ton infra quand on en diffuse.”\n\n\nLa concurrence et la mauvaise foi\n\n  Du coup tous les trucs inutiles de netflix on les fait, mais tous les trucs utiles on les fait pas\n\n\nOulà!\n\n  C’est fou ce qu’on peut faire aujourd’hui avec des spreadsheets…. Je crois bien que c’est la dernière fois que j’ai codé\n\n\nUn certain K.D\n"
} ,
  
  {
    "title"    : "Migration de 6PLAY - l&#39;amour est dans le cloud",
    "category" : "",
    "tags"     : " conference, video",
    "url"      : "/2020/02/07/pascal-martin-laduckconf.html",
    "date"     : "February 7, 2020",
    "excerpt"  : "Pascal Martin a eu le plaisir d’être invité par Octo pour un REX sur la migration de notre architecture dans le cloud.\n\nNous vous invitons à découvrir sa conférence en vidéo.\n\nSi vous voulez en savoir encore plus, Pascal a écrit également un livre...",
  "content"  : "Pascal Martin a eu le plaisir d’être invité par Octo pour un REX sur la migration de notre architecture dans le cloud.\n\nNous vous invitons à découvrir sa conférence en vidéo.\n\nSi vous voulez en savoir encore plus, Pascal a écrit également un livre sur le sujet.\n"
} ,
  
  {
    "title"    : "ScalaIO Lyon 2019",
    "category" : "",
    "tags"     : " scalaio, scala, lyon, 2019",
    "url"      : "/2019/11/25/retour-scalaio.html",
    "date"     : "November 25, 2019",
    "excerpt"  : "Nous étions à la ScalaIO 2019 organisée à Lyon ! \nNous avons assisté à de très bonnes conférences. Voici quelques mots sur les interventions qui nous ont le plus marquées cette année.\n\nA live-coding introduction to Mill: finally a build tool we ca...",
  "content"  : "Nous étions à la ScalaIO 2019 organisée à Lyon ! \nNous avons assisté à de très bonnes conférences. Voici quelques mots sur les interventions qui nous ont le plus marquées cette année.\n\nA live-coding introduction to Mill: finally a build tool we can all understand!\n\nMill est outil de build qui permet de coder en scala les différentes étapes de notre build. La puissance de cet outil vient principalement du fait qu’on écrit du code. On peut donc effectuer des opérations très complexes lors du build. Les fonctionnalités principales sont :\n\n  La possibilité de builder avec différentes versions de scala une même appli\n  La possibilité de ne builder que le code qui a été modifié (grâce à l’utilisation de zinc)\n  La possibilité de lancer le build automatiquement à la modification d’un fichier (option watch)\n\n\nContext Buddy: the tool that knows your code better than you\n\nContext Buddy est un plugin pour votre IDE (Intellij) qui permet de mieux parcourir l’historique des modifications de votre code. ContextBuddy vous permet de savoir grâce à la coloration syntaxique exactement quel élément de la ligne a été modifié. De plus, comme il se base sur les données du compilateur, il est capable de voir si une même classe utilise une nouvelle version de la lib voire même une nouvelle lib.\n\nRailway Oriented Programming - Une approche fonctionnelle pour la gestion d’erreurs\n\nCette conférence basée sur celle de Scott Wlaschin explique très bien la composition de fonction et l’intérêt dans le cas de la gestion des erreurs. En effet, le code devient plus lisible et plus facilement maintenable.\n\nMetals - your next IDE?\n\nMetals est un Language Server Protocole pour Scala, ce qui permet de l’utiliser en tâche de fond pour “n’importe quel” éditeur de texte ou IDE. \nConcrètement, Metals n’apporte pas aujourd’hui 100% des fonctionnalités d’Idea, mais les manques sont vraiment minimes. En revanche, tout ce qui est implémenté semble être plus efficace que sur Idea.\nLes principaux avantages que j’ai retenu :\n\n  Presque tout comme Idea mais beaucoup plus léger et rapide (pour certaines tâches)\n  Fonctionne avec Maven, SBT, Fury, Gradle, Mill\n  Gloop permet le GoToDefinition et plein de choses sympas, plus rapides que l’indexation intelliJ\n  Fonctionne partiellement pour Java (juste le nécessaire)\n  Compilation incrémentale avec Zinc\n  En plein développement, de super features dans les mois à venir\n  Tout ce que j’oublie ;-)\n\n\nRunning Amok: Igniting a Documentation Revolution\n\nL’idée de Jon Pretty (@propensive) est de décorréler la documentation du code (différent repo git) et de pouvoir rétro-documenter (mettre à jour la doc des version antérieures).\nAmok permet de relier chaque fichier de documentation à un commit, à partir duquel cette documentation est valide, permettant ainsi de sortir une release du document même si la documentation n’est pas entièrement terminée (problème récurrent en open-source).\n\nRefined, des Types sur mesure\n\nPermet de mettre des conditions sur un type. On peut ajouter des prédicats au type (notamment des regex sur les Strings, des range de valeurs pour les Int, etc) et ainsi réduire les valeurs possibles. Validation au compile-time quand possible, et pour le runtime des erreurs très explicites sont jetées.\nPour les avantages, voir T(ype)DD, principalement la sécurité apportée et plus besoin de tester ce qui est inclu dans les prédicats, Refined le fait pour nous.\n\nApache Spark et le machine learning : rêves et réalités\n\nA travers des exemples basés sur une population de citrouilles (c’était Halloween ;-)), Nastasia Saby (@saby_nastasia) nous a fait une présentation de Spark ML en prenant en exemple KMeans, un des algorithmes de clustering disponibles. Elle en a montré les limites et présenté KMedoids, n’existant pas dans Spark ML et plus complexe mais convergeant mieux. Elle a terminé sur la nécessité de mettre en balance l’utilisation d’outils communautaires testés et reconnus mais parfois limités, versus développer ses propres librairies parfaitement adaptées à ses besoins, au risque de se confronter à des problèmes que d’autres ont déjà réglés.\n\nLes slides.\n"
} ,
  
  {
    "title"    : "Machine learning sans magie et sans s&#39;arracher les cheveux",
    "category" : "",
    "tags"     : " machine learning, blendwebmix, conference",
    "url"      : "/2019/11/14/machine-learning-sans-magie-et-sans-sarracher-les-cheveux.html",
    "date"     : "November 14, 2019",
    "excerpt"  : "Comprendre le machine learning en prenant l’exemple d’un barbecue.\n",
  "content"  : "Comprendre le machine learning en prenant l’exemple d’un barbecue.\n"
} ,
  
  {
    "title"    : "Forum PHP Paris 2019",
    "category" : "",
    "tags"     : " forumphp, php, afup, 2019",
    "url"      : "/2019/11/04/retour-forum-php-2019.html",
    "date"     : "November 4, 2019",
    "excerpt"  : "Comme tous les ans, nous étions au Forum PHP 2019 organisé par l’AFUP ! \nNous avons assisté à de très bonnes conférences et échangé avec beaucoup d’entre vous. Voici quelques mots sur les interventions qui nous ont le plus marquées cette année.\n\n“...",
  "content"  : "Comme tous les ans, nous étions au Forum PHP 2019 organisé par l’AFUP ! \nNous avons assisté à de très bonnes conférences et échangé avec beaucoup d’entre vous. Voici quelques mots sur les interventions qui nous ont le plus marquées cette année.\n\n“PHP Pragmatic Development” et “L’architecture progressive”\n\nNous sommes plusieurs à avoir trouvé cette édition du Forum très pragmatique. Les deux conférences de Frederic BOUCHERY et de Matthieu NAPOLI y sont sans doute pour quelque chose !\n\nComme Matthieu l’a rappelé, pas besoin d’une architecture parfaite pour créer un produit qui marche. Au contraire, à nous développeurs de savoir choisir les bonnes solutions pour répondre à un besoin. Et faire simple peut apporter infiniment plus de valeur que mettre en place une architecture parfois trop complexe !\n\nFrederic a souligné qu’être pragmatique c’était savoir écouter son expérience. Peut-être même savoir dialoguer entre développeurs seniors et débutants, pour que l’expérience des uns limite les erreurs des autres ?\n\nPHP 8 et Just In Time Compilation\n\nLe passage de PHP 5 à PHP 7 a apporté des gains énormes en terme de performances, et nous sommes tous impatients de voir si PHP 8 nous réservera les mêmes surprises.\n\nLe JIT est une bonne piste, en permettant de compiler le PHP directement en langage machine, pour se passer de l’exécution sur la machine virtuelle de PHP. Benoit JACQUEMONT a très bien détaillé l’histoire du JIT dans l’écosystème PHP, son objectif et son fonctionnement. Même si les tests qu’il a effectué ne montrent pas de gains perceptibles, le sujet était très intéressant.\n\nÀ retenir : l’optimisation du CPU pour PHP n’a pas beaucoup d’intérêt si votre application passe son temps à attendre des I/O.\n\nAggressive PHP quality assurance in 2019\n\nMarco PIVETTA est très actif dans la communauté PHP, notamment pour l’ORM Doctrine. Il nous a présenté les outils qu’il considère comme indispensables pour assurer la qualité et la robustesse d’un projet, mais aussi l’ordre d’importance pour les mettre en place selon lui.\n\nSi nous étions déjà convaincus par l’importance de l’analyse statique, nous avons été intrigués par la place qu’il accordait à tous ces outils basés sur les annotations PHP. Par exemple, il n’hésite pas à laisser publiques les propriétés de ses classes immutables, sans méthode get ni set, et déléguer à la CI la responsabilité de vérifier que toutes les instances des classes avec l’annotation @psalm-immutable ne soient jamais modifiées… Déroutant, mais à méditer.\n\nMercure, et PHP s’enamoure enfin du temps réel\n\nPouvoir pousser, en temps réel, des informations depuis du code PHP server-side vers des centaines de milliers de clients, sans allumer des dizaines de serveurs ? C’est la promesse du projet Mercure, que Kévin DUNGLAS est venu nous présenter !\n\nNous avions entendu parler de ce projet sans jamais encore prendre le temps de le tester ni d’y penser plus en profondeur… Après cette conférence, un POC s’impose ;-)\n\nTout pour se préparer à PHP 7.4\n\nLa prochaine version de PHP, la 7.4, devrait être publiée en fin d’année. Comme tous les ans, elle apportera un petit lot de nouveautés que Damien SEGUY nous a présentées.\n\nNous avons hâte de pouvoir exploiter certaines d’entre elles. En particulier, le pre-loading, qui pourrait améliorer encore notre tenue à la charge lors de nos pics de trafic quotidiens !\n\n“En vrac”\n\nLa dernière conférence du premier jour de ce Forum, par Marie-Cécile GODWIN et Thomas DI LUCCIO, visait à nous ouvrir les yeux : en tant que designer, concepteurs ou développeurs d’applications et d’outils numériques, nous devons penser au futur ; les ressources de notre planète ne sont pas infinies.\n\nCelle du second jour était plus légère : Roland LEHOUCQ nous a parlé de physique, en tirant ses exemples et anecdotes de Star Wars. Qu’est-ce que la Force ? Quelle puissance est capable d’exploiter Palpatine ? Ou combien de gigawatts extrait un sabre-laser ? Une très bonne clôture pour ce Forum !\n\n\n\nNous avons aussi présenté deux conférences, autour de sujets que nous pratiquons au quotidien chez M6 Distribution :\n\n\n  Pascal MARTIN a donné quelques pistes pour améliorer la résilience d’applications, en insistant sur le fait que nos plateformes, de plus en plus complexes, ne sont jamais opérationnelles : elle se trouvent en permanence dans un état de service partiellement dégradé.\n  Benoit VIGUIER a continué dans la lancée de sa conférence de l’année dernière, en présentant cette fois-ci un retour d’expérience après un an d’utilisation de PHP asynchrone en production. Spoiler alert : PHP répond très bien au besoin et les générateurs sont le bien ! À noter aussi son intervention aux traditionnels Lightning Talks, où il nous a présenté une idée un peu folle : faire des interfaces graphiques avec Php.\n\n\nL’AFUP Day 2020 Lyon est déjà en train de s’organiser ! Nous y serons sans doute en nombre et espérons vous y rencontrer à nouveau !\n\n\n"
} ,
  
  {
    "title"    : "Une application résiliente, dans un monde partiellement dégradé",
    "category" : "",
    "tags"     : " conference, architecture, resilience, afup, cloud",
    "url"      : "/2019/10/25/une-application-resiliente-dans-un-monde-partiellement-degrade-pascal-martin.html",
    "date"     : "October 25, 2019",
    "excerpt"  : "Dans un monde en perpétuelle évolution, pouvons-nous toujours atteindre « four-nines » de disponibilité ?\nCloud et Kubernetes. APIs et Microservices… Nos architectures s’enrichissent et se complexifient. Au prix d’une certaine fragilité ?\n\nNous co...",
  "content"  : "Dans un monde en perpétuelle évolution, pouvons-nous toujours atteindre « four-nines » de disponibilité ?\nCloud et Kubernetes. APIs et Microservices… Nos architectures s’enrichissent et se complexifient. Au prix d’une certaine fragilité ?\n\nNous commencerons par définir SLA, SLO et SLI et rappeler la signification de ces X-nines.\nNous montrerons ensuite comment, dans un contexte en permanence partiellement dégradé, nos assemblages de services distribués nuisent à la fiabilité de nos plateformes.\n\nEn profitant de l’expérience acquise sur 6play, nous verrons quelques pistes pour améliorer la résilience de nos applications, pour qu’elles répondent à nouveau aux besoins de notre public. Nous prononcerons peut-être même le terme de « Chaos Engineering » ;-)\n"
} ,
  
  {
    "title"    : "One year of asynchronous PHP in production",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2019/10/25/one-year-of-asynchronous-code-in-production.html",
    "date"     : "October 25, 2019",
    "excerpt"  : "Oui, il est tout à fait possible de faire de la programmation asynchrone en PHP et il existe des librairies matures pour le mettre en place dans vos projets. Oui, ça peut améliorer considérablement la performance de vos applications, mais si c’éta...",
  "content"  : "Oui, il est tout à fait possible de faire de la programmation asynchrone en PHP et il existe des librairies matures pour le mettre en place dans vos projets. Oui, ça peut améliorer considérablement la performance de vos applications, mais si c’était aussi simple tout le monde le ferait déjà. Cela fait plus d’an que les équipes de 6play ont franchit le pas sur certains projets et les applications asynchrones tiennent toutes leurs promesses en production, mais la mise en place a soulevé beaucoup de questions. À quels critères se fier pour rendre une application asynchrone? Comment former les équipes sur ces nouveaux paradigmes? Comment adapter les outils existants et comment gérer ce nouveau type de charge sur les serveurs? Voici notre retour d’expérience sur le PHP asynchrone, du développement à la production, en passant par la vie de tous les jours.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #11",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2019/09/02/m6web-dev-facts-11.html",
    "date"     : "September 2, 2019",
    "excerpt"  : "Reprenons un rythme soutenu des devfacts !\n\nLa persuasion\n\n  L’important, c’est d’être convaincu qu’t’es convaincu\n\n\nParfois débug c’est compliquer\n\n  Comment je teste l’alerte kidnapping en local ?\n\n\n\n  Bah tu vas dans un parc et tu enlèves un en...",
  "content"  : "Reprenons un rythme soutenu des devfacts !\n\nLa persuasion\n\n  L’important, c’est d’être convaincu qu’t’es convaincu\n\n\nParfois débug c’est compliquer\n\n  Comment je teste l’alerte kidnapping en local ?\n\n\n\n  Bah tu vas dans un parc et tu enlèves un enfant, d’ici deux trois heures tu devrais voir l’alerte.\n\n\nÊtre à sec\n\n  Vous pourriez tirer vers nous! On a plus de flèches de nerf ?\n\n\nSeal of approval (lu sur slack)\n\n  Moi, Mr XXXX, Lead Dev de mon état, sain de corps et d’esprit, atteste de la validation et du bien fondé de cette requête, même si c’est un vendredi\n\n\nPrécision\n\n  Si il faut que ce soit précis, il faut préciser\n\n\nFrench please\n\n  _ Est ce qu’on peut parler en français maintenant ici ?\n_ Yes.\n\n\nLa difficulté\n\n  Il y a 2 choses de compliquées en développement : nommer les choses et invalider du cache.\nFigure toi que je suis en train de nommer une PR qui invalide du cache\n\n\nLa qualité avant tout\n\nif (value instanceof Collection) {\n    return ((Collection) value).isEmpty() ? false : true;\n} else if (value instanceof String) {\n    return StringUtils.isNotBlank((String) value) ? true : false;\n}\n\n\nBoire ou coder\n\n  _ J’ai fait un bateau à la ganane pour la rétro !\n_ Il y a du rhum dans la recette :rolling_on_the_floor_laughing: ?\n_ Même pas !!\n\n\nLe CDD\n\n  Conférence de presse Driven Development\n\n\nMais oui c’est clair\nswitch($categoryId) {\n  case &#39;16&#39;:\n    return 38;\n  case &#39;18&#39;:\n    return 40;\n  case &#39;26&#39;:\n    return 42;\n  case &#39;28&#39;:\n    return 36;\n}\n\n\nLe nommage c’est important\n\n$catId = &#39;turlututu&#39;;\n$programId = &#39;chapeaupointu&#39;\n\n\nLa magie nuagique\n\n\n  Le cloud est un état d’esprit, pas un endroit où on déploie\n\n\nLes index commencent à 0\n\n\n  La reproductivité est l’étape N°1 du debuggage\n\n\nOn fait quoi demain ?\n\n\n  Rappelle moi d’acheter un agenda stp !\n\n\nRIP\n\n// Remplissage d&#39;un tableau de données pour un template de job (pas Steve, il est mort)\n\n\nMme Irma\n\n  Montre moi ton diff, je te dirai qui tu es!\n\n\nLes projets à succès\n\n  Il parait qu’un projet legacy c’est un projet qui a réussi… bin on a certains projets qui ont vachement bien réussi !\n\n\nL’appétit des croissants\n\n  Pourriez-vous péter la prod plus souvent ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #10",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2019/08/08/m6web-dev-facts-10.html",
    "date"     : "August 8, 2019",
    "excerpt"  : "Ça faisait un très très long moment ! Voici le retour des devfacts !\n\nL’erreur est humaine\n\n  Il faut mieux recevoir une erreur 500, que 500 erreurs\n\n\nL’odeur du code\n\n  Le code de merde, tu sais quand c’est le tien, c’est comme les pets, pas la p...",
  "content"  : "Ça faisait un très très long moment ! Voici le retour des devfacts !\n\nL’erreur est humaine\n\n  Il faut mieux recevoir une erreur 500, que 500 erreurs\n\n\nL’odeur du code\n\n  Le code de merde, tu sais quand c’est le tien, c’est comme les pets, pas la peine de git blame.\n\n\nRestons zen !\n\n  _ Ah mais en fait !!! Je comprends pourquoi tu viens tôt le matin! Tu viens parce que c’est calme !!!!\n\n\n\n  _ Tais toi Mehdi !!\n\n\nPrévoir l’imprévisible\n\n  C’est la première étape de l’étape suivante.\n\n\nCe matin au chiffrage\n\n  J’ai pensé 5 mais j’ai mis 3…\n\n\nSchroedinger app\n\n  _ Ça marche ?\n\n\n\n  _ Je sais pas mais c’est en prod !\n\n\nL’effet de serre\n\n  _ Il fait froid dans le bureau\n\n\n\n  _ Bah démarre 2-3 docker sur ton Mac, ça devrait résoudre le problème\n\n\nLes priorités\n\n  _ Alleeeez, s’il te plaiiiiiiiit !\n\n\n\n  _ Les incidents d’abord, les détails graphiques après !\n\n\nPetite nouveauté, voici quelques mèmes maison !\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "React Europe 2019",
    "category" : "",
    "tags"     : " React, JS, 6play, Conference, 2019",
    "url"      : "/2019/05/27/react-europe-2019.html",
    "date"     : "May 27, 2019",
    "excerpt"  : "The M6 Distribution’s (M6 Web’s new name!) front team hasn’t posted for a long time. We took part as listeners of the 5th European React Conference in Paris on May 24th and 25th. It’s an opportunity to talk about what are the last moves in the Rea...",
  "content"  : "The M6 Distribution’s (M6 Web’s new name!) front team hasn’t posted for a long time. We took part as listeners of the 5th European React Conference in Paris on May 24th and 25th. It’s an opportunity to talk about what are the last moves in the React community and at M6.\n\nAs usual, we were waiting for lot of announcements in this conference and a lot of new tools or new libraries. There has been no big declaration, no surprise. But many subjects were interesting and several talks confirmed the way we have taken over the last few years.\n\nHooks, more hooks and suspense\n\nDuring the keynote, Jared Palmer put the emphasis on hooks through an example to simplify the use of GraphQL queries. At M6 Distribution, we use hooks since the beginning of the year and many are in production. That has changed the way we write components. We already have used functionnal components before, but using hooks simplify the readability and the evolutivity of old class component. The bad point is testing… Because we use Enzyme, testing hooks is painful for now. Until Enzyme fully supports hooks, we have implemented custom mocks.\n\nJared Palmer also showed us the interest of React Suspense to manage a main loading state in an app instead of many spinners that don’t offer a good user experience. We can’t use Suspense for our app because of the SSR. As it is recommended by the React team, we use Loadable Components instead. Jared  announced that a new asynchronous server renderer is in progress and could be released in 2019 supporting Suspense. Suspense will also include data fetching.\n\nOthers features or refactoring will come in the future, but there is very limited information:\n\n  React Fire\n  React Fusion (?)\n  React Native Fabric\n  React Flare\n\n\nDo the harlem shake\n\nSeveral conferences have addressed the theme of animations with React. This subject is very often complicated to solve. Finding a high-performance solution that is compatible with as many browsers as possible can be a real challenge.\n\nFor more than a year now, we have been migrating from Sass to styled-components. Despite some resentment, it should be noted that this new tool makes it very easy to create/compose our new visual components. It also helps us a lot with its management of the theme.\n\nBetween the react-spring library and the tips of Joshua Comeau, we now have many ways to integrate the new animations that designers can invent.\n\nTo summarize their comments, there is no single best way to make animations in all situations. Whether with canvas, SVG, web API, CSS, 2D sprite, you should try to do it in several ways to compare performance and rendering in order to choose the best option for your specific case.\n\nMake tests but make good tests\n\nLisa Gagarina gave us some tips to better testing the code. Even If we have been using ESLint and Prettier for a long time in our JS projects, we have chosen not to implement static typing for the moment. Indeed, we think that it is a huge step, it has a big impact on the code and it can make the onboarding of new developpers more complex. For now, we just use the React proptypes well.\n\nLisa also advices to better use Jest snapshots. They are often too many, too big, not clear, hard to review. And we have experienced it too. A best pratice can be to reduce snapshots size and inline it in the test file. The readability of a test is indeed very important and it is not recommended to refer to other files than the test file (this is valid for snapshots but also for fixtures). There are some ESLint rules to ensure this: jest/no-large-snapshots, jest/prefer-inline-snapshots. snapshot-diff can also helps by making a diff between the nominal case and the tested case of a component rendering.\n\nA difficult thing is to keep test agnostic to implementation details. We should consider the code as a black-box and only test the user interaction of the components, otherwise any refactoring of code will be painful and might discourage developers from writing tests.\n\nE2E tests are also complex to setup, write and debug but are absolutely necessary. For the back office app, unlike our front app where we use a custom stack, we choose Cypress a complete E2E framework that has saved us a lot of time.\n\nLisa concluded her very interesting lightning talk by saying that there is no such thing as a one-size-fits-all approach, the way of testing has to be adapted to the team and the project. For example on our 6play project, we have more than 3000 unit tests performed by Jest in less than 4 minutes and 450 E2E scenarios that save your life every day!\n\nIf you are interested in this subject, take a look at this article about JS testing best practices.\n\nFinally a little GraphQL in our projects\n\nOne of the main themes of the conference was also GraphQL. \nThis data exchange paradigm now seems to have its place among many users.\nFor more than a year now, we have been using GraphQL on our back office and we are already seeing a lot of benefits:\n\n\n  a front/back exchange contract materialized in a schema,\n  easy consumption thanks to queries,\n  cache management provided by Apollo (also providing a collection of great tools for GraphQL).\n\n\nKenny already talked about it in 2015 but now we use it!\n\nDevelopment worflow: the expected journey\n\nOne of the most interesting conferences in my opinion was about the need to optimize the development workflow. This is an element that is too often ignored but is very important in the life of a project. Paul Amstrong presents us here an analysis of the workflow of his team in charge of the development of the Twitter Lite application. He also presented some conclusions and solutions he implemented. In a standard development workflow there are 3 points that allow a significant margin of progress:\n\n  increased developer confidence and delivery speed,\n  automate the PR process to maximum,\n  detect errors as early as possible.\n\n\nThese words echo our own questions on the subject and these conclusions confirm our decisions. \nIt is important to have the shortest and most automated workflow possible between the developer and the user without sacrificing the developer’s experience because it goes hand in hand with all the other aspects of a project.\n\nAt M6 Distribution, we use most of the tools presented, but two in particular caught our attention.\n\nThe first, React Component Benchmark, is of particular interest to us because in the past we had started to investigate the subject and used tools that are depreciated today.\n\nThe second, Build Tracker, which allows us to test the evolution of bundle size, will allow us to replace an equivalent tool developed internally while providing a more detailed analysis in order to work more accurately on these issues.\n\nHere is an example of a message posted by our in-house build tracker on a Pull Request.\n\n\n\n\n\nA11y is our new challenge\n\nSeveral very inspiring talks on accessibility seem to show that this issue finally appears to be considered by web actors. In particular Facebook, which has distinguished itself by showing its assistant to detect accessibility errors in the development of its new version. However, it is regrettable that this toolkit is not accessible to the community because it could be a great help to avoid putting people from being in a situation of handicap.\n\nIt is clear that our front web app is not yet very accessible but we are working to correct this error, especially on the new screens we have been integrating for several months.\n\nOur expectations on Yarn finally fulfilled\n\nWe were waiting for it at M6, the new yarn release named berry offers almost everything we were missing in our favorite package manager.\nIndeed, by using yarn in monorepo mode for our front projects, we were confronted with several problems that had to be overcome with in-house tools. Example here with our monorepo-dependencies-check tool which is now becoming obsolete thanks to Constraints.\n\nWe are also delighted with the functionality of Zero install. But what we expected the most was about the workspaces. We will finally have a management of the publication of these.\n\nTake a look at this repository for more information.\n\nSome projects that interest us a lot: Next.js &amp;amp; Code Sandbox\n\nEven if these two tools are not used for the development of our applications, the new features of Next.js and Code Sandbox are clearly very interesting.\n\nAs for Next.js, our front web project has its own configuration of server side rendering (Florent explains it in ‘Last night isomorphic JS saved our life!’). However, NextJS is a great project that we use for many of our side projects. AMP support, client-only pages and API endpoints are clearly welcome.\n\nAs for Code Sandbox, Ives van Hoorne told us his personal story and that of his project. In addition to the great tool that is CodeSandbox, we have seen that the use of WebAssembly seems to have solved a lot of performance and implementation problems. For example, he cites the coloring of the code based on TextMate only available in C could not have been ported to the browser without going through WebAssembly.\n"
} ,
  
  {
    "title"    : "AFUP Day Lyon 2019",
    "category" : "",
    "tags"     : " php, afup, 2019",
    "url"      : "/2019/05/23/afup-day.html",
    "date"     : "May 23, 2019",
    "excerpt"  : "TechM6WEB était très fier de sponsoriser le premier AFUP DAY à Lyon.\n\nPour une première, c’était très réussie. Un programme au top et varié, qui mettait en avant de nombreuses problématiques techs et sociétales.\n\n\n(photo : Benjamin Lévêque)\n\nNos c...",
  "content"  : "TechM6WEB était très fier de sponsoriser le premier AFUP DAY à Lyon.\n\nPour une première, c’était très réussie. Un programme au top et varié, qui mettait en avant de nombreuses problématiques techs et sociétales.\n\n\n(photo : Benjamin Lévêque)\n\nNos conférences préférées :\n\n  OVH.com from 1999 to 2019, car c’est toujours intéressant les REX de projets importants en toute humilité,\n  L’architecture progressive, bien qu’elle ait soulevée pas mal de trolls en interne,\n  Les merveilles méconnues du SQL, car en vrai “you know no SQL” :)\n\n\nEt bien sur la table ronde des CTO pendant laquelle Olivier Mansour est intervenu.\n\nMerci l’AFUP pour cet évènement près de chez nous !\n"
} ,
  
  {
    "title"    : "Migrating production applications from on-premise to the cloud with no downtime",
    "category" : "",
    "tags"     : " Cloud, AWS, Kubernetes, Kops, HAProxy, GOReplay",
    "url"      : "/2019/03/11/Migrating-production-apps-from-on-premise-to-the-cloud-with-no-downtime.html",
    "date"     : "March 11, 2019",
    "excerpt"  : "We are migrating all our on-premise applications to AWS cloud.\nMost of them are/will be migrated to Kops-managed Kubernetes clusters, and some are/will be migrated as lambdas.\nTo secure this migration, we are using HAProxy in front of both on-prem...",
  "content"  : "We are migrating all our on-premise applications to AWS cloud.\nMost of them are/will be migrated to Kops-managed Kubernetes clusters, and some are/will be migrated as lambdas.\nTo secure this migration, we are using HAProxy in front of both on-prem and on-AWS deployments (first only sending 1% of each application’s requests to AWS, then 5%, then 25% and so on).\nDisclaimer: This article describes a feedback from production environment. We have changed the name of applications mentioned here, but everything else is true within the limits of our knowledge.\n\nYou can find the content of this blogpost (and more) in a 25mn talk at the HAProxy conf given in Amsterdam in 2019 :\n\n\nThe first migrated application\n\nIt’s an API written in PHP. It has no external dependency (database, redis…), except for another API, called over HTTP.\nWe have migrated this application to the cloud like we’ve done with other applications since.\n\nThe first step was to deploy this application to a kubernetes cluster and expose it over an ELB.\nThen, we wanted to send real-user requests to that app. We wanted to see how it would behave with real production traffic.\nBut we didn’t want to send 100% of our users over there at once: we’d first rather check everything works fine with just 1% of our users.\n\nHAProxy\n\nWe’ve been using HAProxy for several years now.\nBecause of its features, like advanced backend monitoring or its enormous number of metrics, it’s the perfect tool to help us on this migration.\n\nAt first, we didn’t know how the app, the Horizontal Pod Autoscaler, Liveness probes etc. would react with real live production requests.\nSo, we decided to migrate only 1% of production HTTP requests to our Kubernetes cluster. The other 99% of HTTP requests would remain on premise, where the application works for sure.\n\nHere’s a part of the associated HAProxy configuration:\n\nbackend application-01\n    http-response add-header X-Backend-Server %s\n    balance roundrobin\n    http-request set-header Host application-01.6play.fr\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-01.6play.fr\n    server aws-prod-Kubernetes-application-01 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-01.6play.fr ca-file ca-certificates.crt inter 1s fall 1 rise 2 resolvers m6dns observe layer7 weight 25\n    server onprem-prod-front-application-01 onprem-application-01.6play.fr:80 check resolvers m6dns weight 75\n\nSome explanations on key elements of this configuration:\n\n\n  http-response add-header: adds an HTTP header to the response, with the server chosen by HAProxy to handle the request. We added this for debugging purposes: to know who handled our request when requesting the service\n  balance roundrobin: because we’re using a stateless application\n  check .. inter 1s fall 1 rise 2 in the server directive: healthchecks have 1 second of timeout, only 1 bad healthcheck is enough to mark the server as unhealthy and we need 2 good healthchecks in a row to mark it as healthy again\n  observe layer7: It will simulate a bad healthcheck for each application server error (e.g: 500, 502, 503, etc.) making this server fall in such a situation\n  weight 25: We use weights from 0 to 100 (you can go up to 256), so that corresponds to traffic percentages in our case\n\n\nWith the above configuration, as soon as there is an error, HAProxy won’t send traffic to our AWS/Kubernetes application anymore; consequently, it will have a minimum impact on endusers.\n\nTests\n\nWe first tested this with staging proxies, with a temporary domain name and a 50-50% loadbalancer, to ensure the load-balancing worked fine.\nWe tried to kill the deployment on Kubernetes to check 100% of requests came back on-prem. We tested killing random pods to see if we had some user impact. We tested to slow down the application so it would be slower than 1s to respond. We also tested to slow down only one of two pods running the application.\nIt was all OK, so we were confident to go to production.\n\nMigration steps\n\nBefore its migration, the application infrastructure looked like this:\n\n\nWe inserted HAProxy servers in that schema, so the traffic passes through them before being sent to the caches.\nThat way, HAProxy controls where traffic is sent.\nTo make those migrations as transparent as possible, we first configured HAProxy to send traffic to on-prem servers only.\nIn the same time, developers have deployed all mandatory resources (RDS, DynamoDB, elasticache, etc.) with Terraform and verified the application works fine. The application itself could have changed: either the code or kubernetes manifests.\nWhen ready, both ops and devs gave their approval to send traffic to the cloud.\n\nWe started by load-balancing 1% to the AWS ELB with HAProxy:\n\n\nWe compared everything we could:\n\n  2xx, 3xx, 4xx and 5xx percentages\n  connect and response times\n  failed healthchecks and healthchecks return codes\n  backend retries and bad responses\n\n\nWe were amazed: only 3ms in average difference between on-prem and our Kubernetes cluster in AWS cloud.\nAnd no error. Everything worked as expected. It was almost suspicious.\n\n\nThis graph shows the average connect times from HAProxy.\n\nWe’re using Paris as AWS zone and our datacenters are located in Paris too, so that explains the few milliseconds to go back and forth from HAProxy (on prem) to the cloud. In fact, this one to two millisecond between our on-prem servers and AWS is one of the reasons adding HAProxy in the mix was possible.\n\n\nThis graph shows the average response times from the application.\n\nWe also had some PHP configurations to update to be ISO prod (OPCache, APCu, etc.). Why? Well, at first, we created a quick’n dirty (working, but not optimized) Docker image for our application and it went straight to production, before our sysadmins could take a better look at it.\n\n\nThis graph shows the number of 2xx HTTP codes with 25% of traffic sent to AWS.\n\nOnce those PHP optimisations were fixed, we had only 2ms difference between our on-premise and our Kubernetes on AWS. It’s low enough to allow us to test this setup a bit longer without any visible user impact.\n\nDeploying more and more\n\n1% on our kubernetes cluster was great, but it was not enough to see perfs issues.\nSo we raised HAProxy’s load balancing from 1%/99% to 10%/90%.\nStill not enough. We raised it to 25%/75%.\nWe checked application’s pods CPU usage, it was really low. Too low to even trigger the Horizontal Pod Autoscaler based on cpu usage. We couldn’t validate our pods Requests and Limits for that Kubernetes deployment therefore.\n\nSo far, it was enough for us to validate things we could check were working fine. The application cache was efficient, we had stable performances and no surprise on traffic peaks.\n\nOn this first application migration, we decided to stay at 25% of traffic sent to the cloud for the moment, to observe.\nWe did it because HAProxy would have saved us if something bad happened.\n\nIt did behave well: nothing happened for 26 days.\n\nSome errors encountered\n\nUnknown nodes\n\nOn the 27th day, we noticed two of our three nodes were in Unknown state. As our Replicas specified we wanted several pods, Kubernetes started new pods. But as we only had one Ready node left (3 worker nodes in the cluster, including two in Unknown state), all pods for our application were now on that single node. Not great for reliability.\nWe found that the CoreOS image we used was doing automatic updates that restarted nodes regularly. On those two nodes (and on the third one a few days later!), the restart did not go well and Kubelet wasn’t starting at bootime. After investigation, it appears we changed the KOPS STATE STORE bucket. Nodes started before that change were impacted as Kubelet couldn’t find its configuration. Starting new nodes and killing the old ones solved this issue.\nThat allowed us to identify two problems: we didn’t restrict when and how automatic updates were started; and we didn’t have enough cluster monitoring.\n\nFrom that, we knew we had to monitor:\n\n  Nodes in a state different from Ready for a certain time\n  If the number of Ready nodes is at least equal to the Auto Scaling Group minimum\n  If the number of Ready nodes is at most 90% the Auto Scaling Group maximum\n\n\nDifferences between cluster-autoscaler and ASG values\n\nWhile trying to solve the problem above, we also found that pods were living around 5mns before being destroyed and created again. After some investigation, we found that min and max EC2 instances were not configured to the same values between the AWS Auto Scaling Group and the cluster-autoscaler pod.\nSome day, we modified the Kops configuration for those worker nodes and the configuration was well applied to the ASG, but was not applied to the cluster-autoscaler. As a result, we had an ASG minimum of 4 worker nodes, but a cluster-autoscaler minimum of 3.\nAt some point in the history, the cluster-autoscaler defined that the current amount of worker nodes (four) was too high compared to the real need, so it tried to reallocate pods to free up a node. It have done that by draining pods from a node. At the same time, the cluster-autoscaler tried to change the ASG’s desired value to 3 nodes. Because the minimum nodes configured for the ASG was 4, the latter denied the request. Kubernetes scheduler chose to reschedule those recently-killed pods on available nodes, starting by the one with no running pods, AKA: the one that just drained them all.\nThis last phase started again every 5 minutes, making sure that our pods did not survive longer than that.\n\nApplication latencies undetected by health checks\n\nOur application was really slow on Kubernetes/AWS (due to a network misconfiguration) but HAProxy did not disable it. We specified a 1s timeout as shown in the example above, but this is only for healthchecks. Our global server timeout is upper than 1s. Because our application calls another webservice, those calls were timeouting. HAProxy was not aware of that, because the application’s /HealthCheck health page doesn’t check external webservices and thus, were not impacted by those external webservices timeouts. This is an application choice that we can encounter on-premise too, with the exact same behavior. For that reason, we decided to change nothing for now (and we’ll discuss this with the devs teams to see if there’s something we can do).\nWe don’t check external webservices in our /HealthCheck page on purpose, because that page is also tested by kubernetes for livenessProbe. Kubernetes restarts a pod when it is not healthy anymore but when it comes to an external service that is failing, restarting the current pod is non-sens. Kubernetes will restart pods again and again even if the application itself can’t to anything about it! The livenessProbe should test only what the pod does. The Amadeus team talked about that at the KubeCon EU 2018 while presenting Kubervisor.\n\nPedal to the metal\n\nWe were stabilized again.\nSo we raised HAProxy load-balancing to 50% on our application in the cloud.\nAfter seven days without any error, we pushed it to 75%.\nAfter another seven days, we passed the on-prem server as a backup in HAProxy, making the application in kubernetes receiving 100% traffic.\n\n\n\nWe stayed with that configuration for 2 months.\nThat gave us plenty of time to adapt pods Requests and Limits.\nThat is really important for us, because we use HorizontalPodAutoscaler resources with CPU metrics to scale most of our APIs. Here you can find slides deep diving one of our applications that autoscales in prod with kubernetes.\nWe had several events during those 2 months that helped us optimize Requests and Limits for that app. For example, we had holiday traffic, a football match and some special primetime sessions.\nWe also improved our knowledge of both Kubernetes and AWS during this time (I.e: What happens when we rolling restart worker nodes?). Finally, we have configured our Prometheus servers with effective and non-noisy alerts.\n\nAfter weeks of optimizations, we migrated this app’s DNS directly on ELB without HAProxy.\nEverything works perfectly as expected since that day.\n\nNext applications to migrate\n\nWe’ve done a lot of work for our first migration. We’ve capitalized that time for the next projects to finally be able to migrate them in few days.\nThe workflow stays unchanged:\n\n\n  Deploy the application into a kubernetes cluster\n  Add HAProxy servers in front of both on-prem and in-cloud instances\n  Load balance from 1% to 100% traffic to the in-cloud instance\n  Configure accurate Requests and Limits\n  Create efficient alerting\n  Point DNS to ELB\n\n\nMigrate an application path by path\n\nSome of our applications needed to be partially rewritten to be cloud native.\nOnly specific paths were affected by this rewrite.\n\nSo we decided to use HAProxy to migrate those applications, path by path.\nWe also used GOReplay to replicate production traffic for each path, to be sure we didn’t messed up things before sending end-users traffic.\n\n\nThis schema shows how HAProxy were routing traffic according to specific paths.\n\nThe workflow is almost the same as above, with few changes:\n\n\n  Deploy the application into a kubernetes cluster\n  Add HAProxy servers in front of both on-prem and in-cloud instances\n  Use HAProxy map_reg to route traffic, depending of the requested URL\n  Define path routing preferences in the map file created in step 3 (see example below)\n  Configure and test each path:\n    \n      Let developers rewrite paths, I.E: /HealthCheck\n      Replicate production traffic with GOReplay, to specific paths, including /HealthCheck, from on-prem to the application in the Kubernetes cluster in AWS\n      Stabilize the application: either code optimisations or Kubernetes Requests and Limits adaptations\n      Add this newly created path /HealthCheck on the HAProxy’s routing map file\n      Repeat for each new path\n    \n  \n  Create a specific HAProxy Backend section for each route to load balance traffic differently for each route\n  Increase traffic load balancing up to 100% to the cloud\n  Create efficient alerting\n  Point the DNS to the ELB\n\n\nTraffic replication with GOReplay\n\nWe use a lot GOReplay.\nNot only because it’s light and easy to work with, but because we can do whatever we want with it to replicate traffic. It can rewrite headers, catch only a specific domain or a specific url. It’s the perfect tool to complete our migration workflow.\n\nHere is a script we used in the step 5.b of the workflow above:\n\n#!/bin/bash\n\nreplicate_traffic() {\n    if [[ -z $1 ]]\n    then\n        local REPLICATION_PERCENTAGE=5%\n    else\n        local REPLICATION_PERCENTAGE=$1\n    fi\n\n    if [[ -z $2 ]]\n    then\n        local TIMEOUT=45s\n    else\n        local TIMEOUT=$2\n    fi\n\n    echo &quot;Replicating traffic at ${REPLICATION_PERCENTAGE} for $TIMEOUT&quot;\n\n    ./gor -exit-after $TIMEOUT \\\n    -input-raw :8080 \\\n    -http-disallow-url /v2/critical/sensible_datas/payments/ \\\n    -http-allow-url /v2 \\\n    -input-raw-bpf-filter &quot;dst host 127.0.0.72&quot; \\\n    -output-http &quot;https://application-02.6play.fr/|${REPLICATION_PERCENTAGE}&quot; \\\n    -http-original-host \\\n    2&amp;gt;/dev/null\n}\n\n# The above allows a normal ramp-up of the traffic.\n# That means application replicas can be low and increase naturally without an insane peak\nreplicate_traffic 1% 45s\nreplicate_traffic 2% 45s\nreplicate_traffic 5% 45s\nreplicate_traffic 10% 45s\nreplicate_traffic 20% 60s\nreplicate_traffic 40% 60s\nreplicate_traffic 60% 60s\nreplicate_traffic 80% 60s\nreplicate_traffic 100% 7h\n\nWe’re using this script and not directly the gor command, to do a slow ramp-up of traffic to the application in Kubernetes.\nOtherwise, since the application is not stressed before traffic is replicated, replicating 100% of traffic all of a sudden would not be representative of real user behavior. It would led to unwanted alerts that would disappear in minutes with auto-scaling, but that would have rang anyway. So we chose to avoid that noise by doing a slow ramp-up to make traffic replication more real.\n\nWe could follow the replication with HAProxy dashboard, like the following graph:\n\n\nHAProxy configuration\n\nTo achieve a path-by-path migration of an application, we used this HAProxy configuration:\n\nfrontend application-02\n    ...\n    # Defined with a &quot;map&quot; style, from file /etc/haproxy/domain2backend.map\n    # CF https://blog.haproxy.com/2015/01/26/web-application-name-to-backend-mapping-in-haproxy/\n    use_backend %[base,map_reg(/etc/haproxy/domain2backend.map,bk_default)]\n\nbackend application-02-on-prem\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns\n\nbackend application-02-on-cloud\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns\n\nbackend application-02-mixed\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns weight 75\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns inter 3s fall 1 rise 2 observe layer7 weight 25\n\nbackend application-02-mixed-critical\n    http-response add-header X-Backend-Server %s\n    option httpchk GET /HealthCheck HTTP/1.1\\r\\nHost:\\ application-02.6play.fr\n    server onprem-prod-front-application-02 onprem-application-02.6play.fr:80 check resolvers m6dns weight 99\n    server aws-prod-Kubernetes-application-02 aQuiteLongURLCorrespondingToOurELBEndpoint.eu-west-3.elb.amazonaws.com:443 check ssl verify required sni req.hdr(host) check-sni application-02.6play.fr ca-file ca-certificates.crt resolvers m6dns inter 3s fall 1 rise 2 observe layer7 weight 1\n\nAnd here’s the associated map file:\n\n#HOST Header                                                            #HAP backend_name\napplication-02\\.6play\\.fr\\/v2\\/critical\\/(\\w{1,45})\\/payments\\/         application-02-mixed-critical\napplication-02\\.6play\\.fr\\/v2\\/customers\\/                              application-02-mixed\napplication-02\\.6play\\.fr\\/v2\\/                                         application-02-on-cloud\napplication-02\\.6play\\.fr\\/v1\\/                                         application-02-on-prem\n\n# Catch ALL for application-02.6play.fr\napplication-02\\.6play\\.fr\\/                                             application-02-on-prem\n\n\nSome examples of traffic routing made by HAProxy with the configuration above:\n\n\n  application-02.6play.fr/v2/critical/sensible_datas/payments/\n    \n      sent on the specific application-02-mixed-critical backend,\n      with 1% traffic sent to the cloud\n    \n  \n  application-02.6play.fr/v2/customers/\n    \n      sent on application-02-mixed,\n      load balanced at 25% on the cloud\n    \n  \n  application-02.6play.fr/v2/\n    \n      sent on application-02-on-cloud,\n      only on the AWS ELB: cloud only\n    \n  \n  application-02.6play.fr/v1/\n    \n      sent on application-02-on-prem,\n      only on-premise\n    \n  \n  application-02.6play.fr/admin/\n    \n      sent on-premise only,\n      this is the default\n    \n  \n\n\nWith HAProxy map files and the according backend sections, we’re able to migrate path by path any application from on-premise to our kubernetes cluster in the cloud.\nWith gor on top of it, it’s even easier to allow developers develop a specific path while another is being migrated, and all that, with no downtime.\n\nNext steps\n\nWe’ve done most of our cloud migration with workflows explained in this blogpost.\nThanks to HAProxy, most of our applications could be migrated at the same time with no impact from on migration to another.\n\nThere are still some applications to migrate though and one of them is a tough one. This application is heavily using Cassandra database. There is no Cassandra managed in AWS, so we are completely rewriting the application to adapt it to DynamoDB and also to face upcoming business needs.\nThe challenge is to keep existing pathUrl of the application, working. In other words: the new version have to give same functionalities, keeping the same URLs, but with totally different data management under the hood.\nGOReplay is a wonderful asset to help us in this task.\n\nIf you found this useful and you’d like more production return of experiences like this one, please let us know. We plan to write more in the coming weeks.\n"
} ,
  
  {
    "title"    : "7 conseils pour démarrer avec Spark",
    "category" : "",
    "tags"     : " spark, scalaio, conference",
    "url"      : "/2019/01/14/7-conseils-pour-demarrer-avec-spark.html",
    "date"     : "January 14, 2019",
    "excerpt"  : "Je suis entrée dans le monde de la data avec Spark. \nIl y a eu des moments clairement plus ou moins compliqués. \nAu début, c’était le Far West : un monde inconnu et dangereux (il ne fallait pas casser la production). \nAvec ce retour d’expérience, ...",
  "content"  : "Je suis entrée dans le monde de la data avec Spark. \nIl y a eu des moments clairement plus ou moins compliqués. \nAu début, c’était le Far West : un monde inconnu et dangereux (il ne fallait pas casser la production). \nAvec ce retour d’expérience, je vous propose de vous dire ce que j’aurais aimé qu’on me dise avant de me lancer. \nJe promets aussi de vous parler de ce que bien heureusement mes camarades plus expérimentés m’ont aussi donné comme astuces pour m’aider dans ce grand saut. \nCe sera donc une entrée en matière dans le monde de Spark au travers de 7 conseils qui m’ont été ou m’auraient été bien pratiques pour démarrer.\n"
} ,
  
  {
    "title"    : "Le Plan Copenhague : notre migration vers Le Cloud, retour d’expérience",
    "category" : "",
    "tags"     : " cloud, kubernetes, aws, terraform, livre",
    "url"      : "/2018/12/20/le-plan-copenhague.html",
    "date"     : "December 20, 2018",
    "excerpt"  : "Nous avons commencé à migrer notre plateforme 6play vers Le Cloud il y a un an.\n\nDepuis, nous avons découvert Kubernetes et Helm, AWS et ses services managés, Terraform, Prometheus et une multitude d’autres outils. Nous avons fait des choix, répon...",
  "content"  : "Nous avons commencé à migrer notre plateforme 6play vers Le Cloud il y a un an.\n\nDepuis, nous avons découvert Kubernetes et Helm, AWS et ses services managés, Terraform, Prometheus et une multitude d’autres outils. Nous avons fait des choix, répondu à de nombreuses questions, rencontré et franchi des obstacles. Nous avons mis en place des bases solides ou, parfois, pris des raccourcis pour avancer plus vite.\n\nVous aimeriez découvrir comment nous migrons une plateforme comme 6play vers Le Cloud, comment nous exploitons Kubernetes ou les services managés d’AWS ? Vous vous demandez comment nous optimiserons les coûts ? Vous migrez peut-être vous aussi votre hébergement, et comparer votre expérience à la nôtre vous aiderait à avancer ?\n\nPascal, DevOps qui accompagne cette migration vers Le Cloud, a commencé à rédiger un retour d’expérience autour de ce projet. Les sept premiers chapitres viennent d’être publiés : vous pouvez dès maintenant commencer à lire « Le Plan Copenhague » !\n\nPendant ces cent premières pages, vous découvrirez notre projet, les premières bases que nous avons construites et la mise en place de notre environnement chez AWS et sous Kubernetes. Nous irons jusqu’à présenter comment nous avons migré notre première application vers Le Cloud en toute sécurité. La rédaction des chapitres suivants va s’étaler sur une bonne partie de 2019. N’hésitez pas à vous inscrire pour être prévenu lors de leur publication ;-)\n\nBonne lecture !\n"
} ,
  
  {
    "title"    : "Forum PHP Paris 2018",
    "category" : "",
    "tags"     : " forumphp, php, afup, 2018",
    "url"      : "/2018/11/12/retour-forum-php-2018.html",
    "date"     : "November 12, 2018",
    "excerpt"  : "Comme tous les ans, nous étions au Forum PHP 2018 organisé par l’AFUP ! Encore une fois, nous avons pu assister à plusieurs conférences et échanger avec grand nombre d’entre vous. Voici quelques mots sur celles qui nous ont le plus marqué.\n\n“Boost...",
  "content"  : "Comme tous les ans, nous étions au Forum PHP 2018 organisé par l’AFUP ! Encore une fois, nous avons pu assister à plusieurs conférences et échanger avec grand nombre d’entre vous. Voici quelques mots sur celles qui nous ont le plus marqué.\n\n“Boostez vos applications avec HTTP/2”\n\nNous connaissons et utilisons tous HTTP au quotidien. Mais que se cache-t-il derrière ce protocole ?\nKévin Dunglas nous à présenté les nouvelles fonctionnalités apportées par HTTP/2.\nIl nous a d’abord fait un rappel sur les concepts HTTP et sur l’évolution de ce protocole conçu pour échanger des données documentaires.\nAprès plus de 20 ans en version 1, HTTP évolue et passe en version 2 !\n\nHTTP/2 était initialement propulsé par Google. Première bonne nouvelle, pas de changement nécessaire du côté de nos applications PHP. La version 2 introduit de nouvelles fonctionnalités intéressantes :\n\n\n  Priorisation des requêtes\n  Passage au binaire (optimisation de la taille des messages)\n  Notifications push\n  Et bien d’autre\n\n\nKévin nous a aussi présenté le protocole Mercure. Il permet de faire des notifications push server side vers différents clients. Mercure semble simplifier grandement les échanges push client / serveur. Toutes les parties dialoguent au travers d’un hub (rôle primaire de Mercure) et se synchronisent entre elles. Une petite démo à confirmé l’effet “whaou” de cette nouvelle solution.\n\n“Beyond the design patterns and principles - writing good OO code” et “How I started to love what they call Design Patterns”\n\nCe Forum a aussi été l’occasion de voir (ou revoir) quelques principes fondamentaux autour des Design Patterns, avec Matthias Noback et Samuel Roze.\n\nIls ont présenté des exemples concrets, des mises en application de certains Design Patterns incontournables, toujours dans l’optique de découpler notre logique métier, de mieux réutiliser notre code et en améliorer la maintenabilité. Le Domain Driven Design a donc logiquement été mis à l’honneur, ainsi que le typage fort pour donner du sens au code et favoriser sa bonne utilisation. Pour éviter la dette technique, il faut “acheter en avance la capacité de changer”.\n\n\n\n“We got rid of management”\n\nMichelle Sanver nous a présenté l’holacratie : un système d’organisation basé sur l’intelligence collective, utilisé chez Liip. Elle a démarré la conférence en expliquant les défauts de la hiérarchie pyramidale (la photo de la slide parle d’elle-même ;-)) :\n\n\n\nEn holacratie, la société s’organise en cercles et sous cercles de responsabilité et chaque collaborateur se voit donner des rôles et les moyens de l’assumer. Les décisions sont prises collectivement et le pouvoir n’est pas centralisé dans les mains de quelques personnes. Tout est basé sur la transparence, en particulier la rémunération et les budgets. Le but principal est d’assurer des conditions de travail bienveillantes et que chacun se sente en sécurité pour exprimer au mieux ses talents et réduire les tensions. Notamment, toutes les réunions sont facultatives :-D\n\nLiip a développé un outil communautaire en SAS (www.holaspirit.com) qui simplifie l’organisation en gérant les cercles, les prises de décisions et la remontée des propositions d’améliorations (ou “tensions”). Cette outil paraît indispensable, car central dans l’organisation.\n\nOn voit très bien comment une holacratie peut se mettre en place dans une nouvelle organisation ou de taille réduite mais la conférencière n’aborde pas trop cet aspect pour les entreprises de taille conséquente.\n\n“Vous n’avez pas besoin de ça”\n\nLe début de la conférence était volontairement critique sur l’utilisation des nouvelles technos puis peu à peu se dirigeait sur du bon sens dans le choix des technos avec de bonnes raisons (et non pas car elles sont “à la mode”).\n\nPour cela (Charles Desneuf) nous a cité les avantages et inconvénients de différents outils (micro-services, GraphQL, SinglePageApp, Microservices, GraphQL,…)\n\nLe but était vraiment de pousser à la réflexion dans ce type de choix en prenant bien en compte le contexte (utilisateurs, équipe, qualités attendues,…) et de ne pas apporter de la complexité inutilement (optimisation/abstraction prématurée, modélisation inadapté,..).\n\nUne conférence bien dirigée en concluant par :\n“Vous n’avez (peut-être) pas besoin de ça (maintenant).”\n\n“Cessons les estimations”\n\nFrédéric Leguédois nous a livré une conférence proche du one man show sur les estimations et les deadlines. Il nous a rappelé un point important que l’on oublie parfois : Les estimations ne sont QUE des estimations, on ne peut pas les prendre comme des engagements de la part de celui qui les fait. Ses exemples humoristiques sur les différentes façons dont sont faites les estimations provoquaient fréquemment l’hilarité de l’audience qui répondait par des applaudissements généreux.\n\nMême si le trait était forcément grossi pour le « spectacle » on a passé un très bon moment et ça fait réfléchir, notamment sur le fait que le changement est normal.\n\n“En vrac”\n\nQuelques autres conférences que vous pouvez visionner sur le site de l’AFUP :\n\n\n  « Serverless et PHP » : Matthieu Napoli nous a montré comment déployer des fonctions Lambda chez AWS en PHP – et pourquoi.\n  « Développeurs de jeux vidéo: les rois de la combine » : Laurent Victorino nous a complètement enfumé avec sa présentation interactive !\n  « Voyage au centre du cerveau humain, ou comment manipuler les données binaires » : un retour d’expérience enrichissant de Thomas Jarrand, parlant d’IRM, de binaire, ou encore de voxels.\n\n\nNous avons aussi présenté trois conférences, autour de sujets que nous pratiquons au quotidien chez M6 Web :\n\n\n  Benoit Viguier a parlé de programmation asynchrone avec les générateurs de PHP : une fonctionnalité extrêmement puissante, mais encore trop peu connue. Il a d’ailleurs annoncé la sortie de la bibliothèque Tornado. (vidéo de la conférence)\n  Guillaume Bouyge nous a raconté l’histoire de la migration à l’international de la plate-forme 6play : comment, en partant d’un produit développé pour M6, nous sommes arrivés à un produit en marque-blanche vendue à d’autres clients d’autres pays. (vidéo de la conférence)\n  Et Pascal Martin a présenté Kubernetes, l’outil que nous utilisons pour piloter des conteneurs Docker dans Le Cloud. Il a enchaîné avec plus de détails sur le processus que nous suivons pour migrer nos projets vers cet hébergement ; vous pourrez en apprendre plus en lisant Le Plan Copenhague. (vidéo de la conférence)\n\n\nVivement l’AFUP Day Lyon 2019, où nous aurons sans doute le plaisir de vous rencontrer à nouveau ?\n"
} ,
  
  {
    "title"    : "Generators for Asynchronous Programming: User Manual",
    "category" : "",
    "tags"     : " conference, php, afup",
    "url"      : "/2018/10/26/generators-for-async-programming-user-manual.html",
    "date"     : "October 26, 2018",
    "excerpt"  : "Les générateurs sont souvent réduits à une simplification des itérateurs, mais ils sont surtout très pratiques et performants pour executer des traitements asynchrones. Nous aborderons le fonctionnement d’un programme asynchrone, le rôle des promi...",
  "content"  : "Les générateurs sont souvent réduits à une simplification des itérateurs, mais ils sont surtout très pratiques et performants pour executer des traitements asynchrones. Nous aborderons le fonctionnement d’un programme asynchrone, le rôle des promises, et approfondirons l’utilisation des générateurs pour simplifier l’écriture de notre code. Enfin nous détaillerons des cas pratiques « prêts à l’emploi » pour tout type d’application, avec un retour d’expérience sur ce qui a été mis en place chez M6Web.\n"
} ,
  
  {
    "title"    : "Docker en prod ? Oui, avec Kubernetes !",
    "category" : "",
    "tags"     : " conference, php, open-source, afup, docker, kubernetes",
    "url"      : "/2018/10/26/docker-en-prod-oui-avec-kubernetes-pascal-martin.html",
    "date"     : "October 26, 2018",
    "excerpt"  : "Kubernetes. À en croire certains articles, c’est une solution miracle. Développeurs, vous avez peut-être entendu ce mot ?\nC’est l’outil qui vous permettra de déployer du Docker en production ! Parce qu’autant utiliser Docker en dev c’est facile, a...",
  "content"  : "Kubernetes. À en croire certains articles, c’est une solution miracle. Développeurs, vous avez peut-être entendu ce mot ?\nC’est l’outil qui vous permettra de déployer du Docker en production ! Parce qu’autant utiliser Docker en dev c’est facile, autant en prod…\n\nMais qu’est-ce que Kubernetes ? Quelles possibilités si intéressantes nous fournit cet orchestrateur de conteneurs ?\nPods, nodes, deployments, services, ou auto-scaling et health checks : autant de primitives et de fonctionnalités que vous allez découvrir et adorer, y compris en tant que développeurs !\n\nAprès avoir présenté ces bases, je vous proposerai un retour d’expérience sur la migration vers Kubernetes que nous sommes en train d’effectuer pour 6play.fr. Comment développeurs et sysadmins se répartissent-ils les tâches ? Avons-nous dû adapter nos applications PHP ? Quelles difficultés avons-nous rencontrées, quels compromis avons-nous acceptés et quelle route nous reste-t-il à parcourir ?\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, global review",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/24/kubecon-2018-copenhaguen-global-review.html",
    "date"     : "May 24, 2018",
    "excerpt"  : "After those three days at KubeCon, we’ve seen and heard a lot of interesting ideas. You can read about our first day here, about our second day there, and about our third day there. If we had to do a short recap, here are the points we would list....",
  "content"  : "After those three days at KubeCon, we’ve seen and heard a lot of interesting ideas. You can read about our first day here, about our second day there, and about our third day there. If we had to do a short recap, here are the points we would list.\n\nFirst of all “Cloud native” seems to be the buzzword of the year. Not just cloud anymore, but cloud native! What does it mean? Instead of just deploying your application to the cloud, it should fully use the cloud.\n\nThen, Kubernetes. It is a mature solution in itself. There doesn’t seem to be any doubt left about that. Even if this is the case with Kubernetes, the majority of the ecosystem around it is not mature yet, and that’s a bit of a problem. We saw a lot of tools and some that were presented during talks are still WIP and some demos completely failed. If you’re surfing the Kubernetes wave, please be careful with the tools you choose, and don’t loose yourself adopting a fancy/non-working tool that will bring your infrastructure down. Continue to master what you do without being trapped by the hype brought by certain solutions.\n\nDeployment, CI and CD. Well, not so much. There are a few projects out there and several different approaches (kubectl apply, a bit of Jenkins around it, deployments from inside the cluster, several black boxes like CodeFresh…), but not one thing that everyone is doing/using. We are currently looking at Jenkins-X and hope we’ll be able to build our CI/CD stack using it.\n\nFor monitoring, use Prometheus. It’s pretty much what everyone is using.\n\nService mesh. One of the big things this year, everybody is talking about it, Istio seems to be taking the lead. It’s still moving fast, though and not enough people are truly mastering this in a production setup.\n\nDevelopment environment. Well, “LOL” would do it maybe. This is clearly not a priority yet. Some teams and projects (like Telepresence) have started working on this, but there is still road ahead.\n\nGitOps. Everybody is going this way. Versioning, of course. But also using Git events to pilot things.\n\nThings are beginning to move on the security side of things. Companies are starting to notice there is work to be done, startups are appearing with different services.\n\nAnd, finally, multi-clusters. We felt a few people are using multi-cluster, but it’s often done by hand. That doesn’t seem mature at all. Maybe a subject we’ll hear more about in the future?\n\nIn any case, we had a really great time during this KubeCon in Copenhagen, we saw many interesting talks and discussed with lots of people!\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 3",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/23/kubecon-2018-copenhaguen-day-3.html",
    "date"     : "May 23, 2018",
    "excerpt"  : "Back to KubeCon 2018 in Copenhagen, for the third and last day! You can read about our first day here and about our second day there.\n\nKeynotes\n\nCloud Native ML on Kubernetes - David Aronchick, Product Manager, Cloud AI and Co-Founder of Kubeflow,...",
  "content"  : "Back to KubeCon 2018 in Copenhagen, for the third and last day! You can read about our first day here and about our second day there.\n\nKeynotes\n\nCloud Native ML on Kubernetes - David Aronchick, Product Manager, Cloud AI and Co-Founder of Kubeflow, Google &amp;amp; Vishnu Kannan, Sr. Software Engineer, Google\n\nTrying to run Machine Learning on Kubernetes? Working with Jupyter and Tensorflow? Kuebeflow could be of interest to you.\n\nRunning with Scissors - Liz Rice, Technology Evangelist, Aqua Security\n\nEven if we never fall, running with scissors could be dangerous. We wouldn’t run with scissors, would we? Then, why do we keep running containers with privileges they should not need? And why do we mount more directories than needed as volumes?\n\n\n\nChaos Engineering WG Deep Dive – Sylvain Hellegouarch, ChaosIQ\n\nOne of the goals of Chaos Engineering being to break stuff, the talk started by a word about mindset: one must love supporting the team, and not try to save the day herself. Also, one must nurture empathy (including for the system), be assertive but not arrogant, not blame / be snarky (many of us are not great on that point).\n\nChaos Engineering follows a continuous loop. Start with a steady state (a baseline that comes from objective observation, which means you need business metrics and to collect data), formulate an hypothesis (not necessarily business oriented), and define an experimental method (the things we vary to prove / disprove the hypothesis). Warning: don’t vary too many things at once.\n\nThe talk finished with a few words about the Chaos Toolkit, which aims to be simpler than the famous Chaos Monkey.\n\nIstio - The Weather Company’s Journey - Nick Nellis &amp;amp; Fabio Oliveira, IBM\n\nSeveral istio talks weren’t enough for me, I wanted more. I didn’t learn much more on this new one, except those tips:\n\n  You can define route specific retries with Istio\n  They use vistio to visualize istio traffic. That tool seems based on Netflix’s Vizceral. Unfortunately, I couldn’t find any GH repo nor blog talking about vistio.\n  If you want to implement Istio: start small\n\n\nAre You Ready to Be Edgy? — Bringing Cloud-Native Applications to the Edge of the Network - Megan O’Keefe &amp;amp; Steve Louie, Cisco\n\nThere are a few common problems in a cloud computing setting. Many devices (with IoT for instance) can mean a bottleneck on the network side of things, the cloud being far away can cause latency problems. Edge computing, moving apps (or parts of apps) from a centralized cloud, could help with those problems. Think AR/VR for latency, Video for high bandwidth, facial recognition for temporary/secure data.\n\nThe talk introduced the idea of deploying Kubernetes in edge locations – like in cell towers – and pointing users to the closest location. Those Kubernetes clusters would only host APIs or applications with specific needs, while the main parts would remain in a centralized cloud, which means we would have to develop edge-ready applications, keeping in mind problems such as network splits, data synchronization, deployment, …\n\nIntegrating Prometheus and InfluxDB - Paul Dix, InfluxData\n\nI was a lot surprised in this talk as we were like 30 in a room for 300.\nI guess people don’t need to keep metrics more than 15days with the Prometheus engine, or maybe people are already using InfluxDB as a guaranteed long term storage.\nBecause we are in the second case, we are testing influxDB with Prometheus, so this talk came at the right time.\nI didn’t learn much on InfluxDB + Prometheus, nor on federated queries that comes with HA.\n\nPaul then questioned why not use a single query language for all query engines? That would be more practical and maintainable. The question is still open, even if Paul proposes IFQL to rule them all. The main idea is for all query engines to coordinate and stop creating a new language on each new engine.\n\nA Hackers Guide to Kubernetes and the Cloud - Rory McCune, NCC Group PLC\n\nFirst, a word about threat models. Pretty much everyone will see random Internet hackers looking for easy preys. Some of us will be specifically targeted by attackers. Only a few will be targeted by nation states. You should think about your threat model, which may or may not be the same as your neighbour’s.\n\nThen, time to think about your attack surface. Attackers will find the weakest point, which is not always where your might think. What about the cloud around your Kubernetes cluster? Github is a great way of getting accesses (many commit their credentials and/or do not remove them from history – bots are crawling this!). Developers’ laptop are generally full of interesting data, and are not necessarily protected enough.\n\nOn Kubernetes, external attackers will try to access the API server and etcd, the kubelets, or maybe inject malicious containers. You should turn off the insecure port, control access to kubelet and etcd, retrict the use of service tokens, restrict privileged containers, enable authentication and authorization on the API server, set pod security and network policies, and do regular upgrades. Also, don’t forget about cloud permissions.\n\nCloudbursting with Kubernetes - Irfan Ur Rehman &amp;amp; Quinton Hoole, Huawei Technologies\n\nThat might not be everyone’s problem, but still interesting to hear about. If you have multiple cloud providers with different pricings, you might want to optimize your costs by using the most expensive only on load peaks. That is exactly what they did, using Kubernetes clusters federation and specific annotations. We won’t go over this approach because we’ll stick with one cloud provider, but that may be interesting for some people.\n\nOperating a Global-Scale FaaS on Top of Kubernetes - Chad Arimura &amp;amp; Matt Stephenson, Oracle\n\nThis was about the Fn project. A couples of problems related to multitenancy: network isolation on Kubernetes, noisy neighbors (I/O being the bottleneck). Helm has a few limits, worked around with shell scripts.\n\nInside Kubernetes Resource Management (QoS) – Mechanics and Lessons from the Field - Michael Gasch, VMware\n\nResource management goes through cgroups. With containers, we see all the CPU/RAM, but this doesn’t mean we’ll be able to use them all: we may have to share with other containers. Works with requests. For now, cpu and memory are stable resources, but others (hugepages, ephemeral storage, device plugins) are in beta. You should align Kubernetes’ QoS with the underlying infrastructure, enable quotas in the cluster, and protect critical system pods.\n\nI have to admit I didn’t take much notes during this talk, but noted the slides contain a lot of informations – for more, go read them ;-)\n\nObserving and Troubleshooting your Microservices with Istio - Isaiah Snell-feikema, IBM &amp;amp; Douglas Reid, Google\n\nI promise, this is the last Istio conf I went to.\nIn case that wasn’t obvious, Istio is becoming the default service mesh, like Prometheus is for metrics. I couldn’t work much on it so I wanted to learn the most possible from it during this KubeCon. I can say this talk was one of the best for Istio discovery and even advanced skill. I won’t be able to summarize everything, so here are few tips I kept from it:\n\n  Envoy’s /stats route gives a lot of infos of servers\n  Istio system logs gives also traffic spikes\n  Istio’s access logs can be uploaded to fluentd/elk\n  Canary testing / blue-green deployments can be done via RouteRule CRD.\n\n\nIf you are considering using Istio, you must see the slides\n\nVitess and a Kubernetes Operator - Sugu Sougoumarane, YouTube\n\nI heard about Vitess for the first time during this KubeCon, even though it’s an old project. It’s a middleware for MySQL, sitting between the database and our applications. It helps scale it through sharding – the goal being to answer the common pain points for databases: scalability, cloud and making DBAs happy. It’s also one of the CNCF project I noted a few days ago I should take a closer look at.\n\nFinal words?\n\nThe weather was nice and our plane was only on Saturday, so we finished the day with a walk in the City.\n\n\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 2",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/22/kubecon-2018-copenhaguen-day-2.html",
    "date"     : "May 22, 2018",
    "excerpt"  : "After an interesting first day at KubeCon 2018, we are back for the second day!\n\nAutoscale your Kubernetes Workload with Prometheus - Frederic Branczyk, CoreOS\n\nThe goal of autoscaling, ultimately, is to fullfill SLO of SLA through SLI (yeah, you ...",
  "content"  : "After an interesting first day at KubeCon 2018, we are back for the second day!\n\nAutoscale your Kubernetes Workload with Prometheus - Frederic Branczyk, CoreOS\n\nThe goal of autoscaling, ultimately, is to fullfill SLO of SLA through SLI (yeah, you may have to think for a while after reading this ^^). Demand must be measured by metrics, which must themselves be collected, stored and made queryable. Autoscaling can be horizontal (increase replicas when necessary, the focus of this talk) or vertical (increase resources request/limits when necessary).\n\nAutoscaling on Kubernetes used to rely on Heapster. But Heapster’s API is unspecified, it doesn’t work with Prometheus, and vendor implementations are often unmaintained. Starting with 1.8, Kubernetes provides a resource and custom metrics API. It is a specification (not an implementation), the implementations are developed and maintained by vendors, and each metric returns a single value. Kubernetes’ HPA (HorizontalPodAutoscaler) uses these metrics to scale up/down.\n\nCore metrics are CPU and RAM, by pod/container/node, and there is a canonical implementation called metrics-server. The custom metrics API has the same semantics (a single value is returned) but no canonical implementation is provided (take a look at DirectXMan12/k8s-prometheus-adapter for an implementation). Each metric is related to a Kubernetes object (Pod, Service, Deployment, …). Finally, there is also an external metrics API (currently in alpha stage) for things not related to a Kubernetes object (like the queue length for a queuing service provided by a Cloud provider – SQS on AWS, for example).\n\nPod Anomaly Detection and Eviction using Prometheus Metrics - David Benque &amp;amp; Cedric Lamoriniere, Amadeus\n\nThis talk started with a reminder: stability is hard, especially for a distributed system. And using load-balancers doesn’t help at all, on the contrary. Solutions include proximity-based load-balancing (sharding by Availability Zone, which is not something Kubernetes does natively, but for which Istio can help with its pilot-agent proxy as it is AZ-aware). Using healthchecks (liveness kills the container when it fails, readiness only removes it temporarily from service discovery when it fails) is a good idea, but you should keep probes simple (complexity = bugs) and you shouldn’t check external dependencies. Also, don’t forget about circuit breakers and retries. But note all this suffers from limitations, as it’s only based on technical signals and depends on local decisions (pods/containers, service mesh/proxy).\n\nThis conference was about a tool called Kubervisor, which aims to detect mis-behaving Pods and remove them from the cluster (switching a label on the corresponding service). The decisions are based on metrics (using PromQL for Prometheus metrics) that can include business metrics, and not only technical ones nor data limited to a specific pod.\n\nI didn’t write much about this, I was too busy watching the demo, which was really interesting. In the end, I wrote down I should (and I probably will) take a better look at this in a few months, when we have deployed a few more pieces of software to Kubernetes.\n\nChallenges to Writing Cloud Native Applications - Vallery Lancey, Checkfront\n\nBuilding and deploying an application to the cloud has advantages: automatic scaling, load-balancing, replication, infrastructure provisioning and teardown is done for us, … It also has challenges.\n\nStorage of persistent data is one of these challenges (simple replication is rarely a good idea, working with unreplicated shards is a common pattern, using multiple volatile copies is a good strategy, but the best approach is runtime data replication – which requires a large setup and implies non-negligible maintenance costs). Services coupling (database + services interacting with it) and internal API calls (source of delays) are also common sources of troubles, even if the second can be mitigated with simpler API actions, endpoints for specific actions (not CRUD!), batch endpoints and caching. Testing is a huge concern, especially with microservices, and working with discrete compoments helps, as we can run one service and mock the others. Finally, local development is not a solved problem yet (in any case, you should remove the “build an image” step).\n\nGitOps for Istio - Manage Istio Config like Code - Varun Talwar, Stealth Startup &amp;amp; Alexis Richardson, WeaveWorks\n\nDuring this talk, WeaveWorks team talked about how Istio config can be managed like code through git based workflows.\nThey evoked using Terraform to describe cloud state.\nAs of gitops principles, devs shouldn’t use kubectl to interact with clusters. Additionally, they should push code, not containers. GitHub events must lead deployments, not humans.\nAs part of this automation, deployments must auto-rollout when things break. They either fail or succeed cleanly.\nOne can use operator patterns to help integrating those concepts. The WeaveWorks team also talked about flux to manage environments states.\nWeave Flux brings a lot more annotations for Istio, making automated releases deployments, etc.\n\nOPA: The Cloud Native Policy Engine - Torin Sandall, Styra\n\nPolicy enforcement is a fundamental problem for an organisation, and policy decisions should be decoupled from policy enforcement. Open Policy Agent is an open-source general-purpose policy engine. It uses a high-level declarative language, can be used to implement RBAC and has integrations with Istio or Terraform. This is not my current priority, but it could be worth taking a look at OPA if you need to add policy enforcement to your application.\n\nKubernetes Multi-Cluster Operations without Federation - Rob Szumski, CoreOS\n\nA lot of people are using multiple Kubernetes clusters. For example, Zalando uses 80. It can become a mess to manage all those clusters with their specific components like secrets, controllers, configmaps, etc.\nTo solve this problem, a new cli has been created: kubefed.\nBut Rob explains that this new tool doesn’t solve all the problems. I.E: You’ll have to give access to all-clusters to people and not only few clusters (that breaks isolation), the Federation API must be run by a top-root user (accessing everything), etc.\n\nCoreOs brought the concept of k8s operators.\nRob explains why that solves problems and why you should use that instead of Federation.\n\nClearly I wasn’t convinced at all by this presentation. Besides, we are working in the same building and problems brought by Rob (modifications by hundred of devs/SRE split across the world) do not concern us at the moment.\n\nBuilding a Kubernetes Scheduler using Custom Metrics - Mateo Burillo, Sysdig\n\nThere are so many possibilities when scheduling pods. The scheduler first applies filters (resource requests, volumes, selectors/taints), then ranks (including the default behavior of spreading pods of the same service), then goes to applying hard constraints (taints, node selector), and soft contraints (prefer no schedule, node affinity, pod affinity, weight) and pod priority and does taint-based evictions. To understand what actually goes on and prevent complex situations, you should only add important constraints.\n\nIn some cases, you might want to build a custom scheduler, using custom metrics (when the default scheduler is not good enough for you and/or you have very specific needs). An example, based on sysdig metrics: draios/kubernetes-scheduler. And more informations in this blog-post. Remember creating a scheduler is not an easy task and many things can go wrong (think about concurrency and race conditions).\n\nIn the end, the idea of implementing a custom scheduler might be interesting, but a bit scary: messing things up could mean no pod getting scheduled, which is not a nice scenario. I’m not sure I currently see a situation in which I’d go this way…\n\nClusters as Cattle: How to Seamlessly Migrate Apps across Kubernetes Clusters - Andy Goldstein, Heptio\n\nAs many people, Andy has a lot of clusters. To re-route traffic between clusters, he uses Envoy.\nTo maintain consistent configurations, he uses Ansible to provision everything.\nSo far, I don’t really see the point of having a lot of clusters and even less of migrating a single app between clusters, but that can be interesting for people who like that trend.\n\nParty, Tivoli Gardens\n\nFor the evening, we went to an all-attendees party at Tivoli Gardens, an amusement park and pleasure garden right in the middle of Copenhagen. We walked around for a bit, before settling for a beer and a few snacks, talking with other French Kubenetes fans.\n\n\n"
} ,
  
  {
    "title"    : "KubeCon 2018 Copenhagen, day 1",
    "category" : "",
    "tags"     : " kubernetes, KubeCon",
    "url"      : "/2018/05/21/kubecon-2018-copenhaguen-day-1.html",
    "date"     : "May 21, 2018",
    "excerpt"  : "At the very beginning of May, we (Pascal and Vincent) went to KubeCon 2018. It was a three-days long event, with more than 300 conferences. 4300 people met at Bella Center, a huge conference place close to Copenhagen in Denmark. Here are some of o...",
  "content"  : "At the very beginning of May, we (Pascal and Vincent) went to KubeCon 2018. It was a three-days long event, with more than 300 conferences. 4300 people met at Bella Center, a huge conference place close to Copenhagen in Denmark. Here are some of our notes about some talks we saw.\n\nKeynotes\n\nCNCF Project Update - Liz Rice, Technology Evangelist, Aqua Security; Sugu Sougoumarane, CTO, PlanetScale Data; Colin Sullivan, Product Manager, Synadia Communications, Inc. &amp;amp; Andrew Jessup, Co-founder, Scytale Inc.\n\nDuring this keynote, I realized the CNCF is helping a lot more projects than I thought it was: it’s not just Kubernetes. I will take a closer look to some of them in the future – they are all listed on l.cncf.io.\n\nCERN Experiences with Multi-Cloud Federated Kubernetes - Ricardo Rocha, Staff Member, CERN &amp;amp; Clenimar Filemon, Software Engineer, Federal University of Campina Grande\n\nOK, so, sometimes, it actually is rocket-science (or pretty close to it). It’s nice seeing Kubernetes used for science and research, on a federation of around 400 clusters!\n\n\n\nWhats Up With All The Different Container Runtimes? - Ricardo Aravena, Branch Metrics\n\nOverview of the different containers runtimes, starting with OpenVZ in 2006 (still maintained, but the last 2.7 version is not as stable as the previous one, and it doesn’t support Kubernetes), and LXC (Kubernetes support is low-priority WIP and LXC uses a specific image format) in 2011. Docker (initially based on LXC) arrived in 2013 and things have gone insane since then. With libcontainer and rkt in 2014, OCI in 2015 and CRI in 2016. Today, Kubernetes supports several runtimes.\n\nrkt could be interesting from a security point of view (supports TPM and VMs). With Kubernetes 1.10, the default runtime is runc. crun is the most interesting runtime for performances, but is WIP and isn’t currently used much. Kata, released in 2018 has the best security (it runs containers in VMs) and is OCI compliant, but is slower and more heavyweight. Other very specific runtimes include nvidia, railcar, pouch, lmctfy, systemd-nspawn…\n\nBasically, today and for most workloads, you should go with the standard docker/containerd runtime. There is a convergence towards OCI, it’s the default for Kubernetes and docker is going to adopt the CRI plugin.\n\nIntroduction to Istio Configuration - Joy Zhang, Google\n\nAn introduction to the Istio Service Mesh. All Istio components are CRDs. CRDs are becoming a standard when it comes to kubernetes customizations, here requests proxying.\n\nThis talk described Istio components, notably:\n\n\n  Mesh config - global Istio config\n  Service config - Istio operators config\n  Consumer config - overrides config model\n  Galley - Istio config per cluster\n\n\nFYI: you need as much Galleys as you have clusters + environments.\n\nContinuous Delivery Meets Custom Kubernetes Controller: A Declarative Configuration Approach to CI/CD - Suneeta Mall &amp;amp; Simon Cochrane, Nearmap\n\nKubernetes is great but, CI/CD is not really its job – and CI/CD depends a lot on the company you work at and its culture. Here, they started deploying their applications with kubectl and YAML files, and even if CI usually doesn’t cause much troubles, CD is not that easy. And using a managed CI (CircleCI, shippable, AWS Codepipeline…) means exposing Kubernetes’s control plane on the Internet, which is not great. Also, the Gitops approach with its git is the source of truth mindset is OK, but committing version numbers again and again pollutes the history with a lot of noise – when this history could be kept in the cluster itself.\n\nSo, they went with some kind of CD lite: a service running in the cluster, which monitors the registry and deploys the new version of an image when it sees it. This means the cluster doesn’t need to access git and the CI chain doesn’t need to access the cluster, making the configuration simpler.\n\nThe tool they developed for this is nearmap/cvmanager. It seems relatively easy to install and configure. And I like the idea of having a gap between Git/CI and CD. I may have to test this, especially to see what it can do when it comes to canary or blue/green deployments, but this is one of the things I saw at KubeCon I will discuss with my colleagues!\n\nPractical and Useful Latency Analysis using Istio and OpenCensus - Varun Talwar, Stealth Startup &amp;amp; Morgan McLean, Google\n\nOpenCensus (distributed tracing metrics system) + Istio is the combo provided for this talk to let devs debug the most of their apps.\n\nOpenCensus is a tracing tool, like the CNCF’s project Opentracing. It can be implemented in many languages, starting with those we use: PHP and javascript.\n\nOpenCensus can trace RPC and http APIs calls.\n\nYou can install a dedicated dashboard that gives a lot of metrics out of the box (like 90th percentiles, etc.) and allows customizable ones.\n\nMixer (Istio tool) makes the aggregation between those metrics from what is gathered by OpenCensus.\n\nThis definitely need to be tested, but not sure it is worth migrating from OpenTracing to OpenCensus.\n\nHabitat Builder: Cloud Native Application Automation - Elliott Davis &amp;amp; Fletcher Nichol, Chef.io (Habitat.sh)\n\nI had never heard of Habitat before, so I was kind of curious what this was about. This idea of platform-independent build artifacts (with exporters for docker, Kubernetes, Helm) could be interesting for some teams, but it’s not a need I currently have. The automated deployments might be interesting though, but we are already looking at other tools.\n\nKubernetes and Taxes: Lessons Learned at the Norwegian Tax Administration - A Production Case Study - Bjarte S. Karlsen, The Norwegian Tax Administration\n\nReally nice production case study from the Norwegian Tax Administration about their k8s platform. They are currently using Rancher 2.0 + OpenShift + CodeFresh on top of k8s. All their Docker images are alpine based. They develop Java applications.\n\nOne idea kept my attention is the tagging of their containers:\n\n\n  Pushing docker image tag 1.2.3 also pushes tag 1.2 and tag 1.\n  So going to v2 and rolling back to v1 effectively rolls out 2.0.0 to 1.2.3 without knowing the exact subversions.\n  You are assured that the tag of the major version always points to the latest subversion.\n\n\nIt’s not clear to me how to implement that. Maybe a codefresh hack. But still, I found the approach interesting.\n\nAnother lesson for those who want to migrate from on-premise to the cloud is to keep things that work on-premise and simply migrate them on cloud as it is. That will ease the migration. You can rethink all afterward if needed, but the first step is to migrate, not rebuild from scratch plus migrating.\n\nAs they use CodeFresh for CI/CD, it’s easy for them to automate their pipelines. That’s another lesson: automate everything. To automate, you need to standardize.\n\nAnother lesson is to use what is rock-solid. We all see a lot of tools and startups around the cloud nebula. A lot of them won’t last and some will, like Kubernetes. This is the tip: use what will last. Don’t build your whole infrastructure on something unstable or with poor pro/community support.\n\nThe final point I kept from this really good rex, is to create a predictable infrastructure. You cannot guess what will happen. You have to know and use the right tools/annotations to make it behave the way you want, predictably and repeatedly.\n\nHow to Export Prometheus Metrics from Just About Anything - Matt Layher, DigitalOcean\n\nThis presentation was about a few good practices to follow when it comes to exposing Prometheus metrics from a Go application. Basically, you should use the Go client library, be really careful about concurrency, build reusable packages, write unit-tests, use promtool check-metrics, and read and follow the Prometheus metrics best practices.\n\nContinuously Deliver your Kubernetes Infrastructure - Mikkel Larsen, Zalando SE\n\nAnother really good rex from Zalando from their utilization of k8s in prod and lessons learned.\n\nThey talked of Stups, a Zalando toolset around AWS. That definitely needs to be tested.\n\nFrom their experience of managing a k8s cluster on AWS EC2s, they gave us few tips:\n\n\n  Always upgrade to the latest k8s version\n  Manage the smallest possible number of clusters\n  Automate all the things. The only manual step should be merging PRs. This is a base GitOps principle.\n  Define an AWS HA control plane setup behind ELBs. That can be debated but this is a good first step.\n  All cluster config files must be git versioned (another GitOps principle). An upgrade is then only a git branch merge at some point.\n\n\nSome of the points above can be achieved via a CD tool. I remember they use Jenkins for that, but not 100% sure. Alongside this CD tool, there should be a CI tool (or one tool for both).\n\nThey gave us some points on CI tests too:\n\n\n  Run e2e conformance tests for k8s config files\n  Run statefulSet tests\n  Run any additional homemade tests\n\n\nFor those who are using AWS, keep in mind the following: volumes cannot be mounted across several AZ.\n\nKeep yourself away from unavailability by always setting minAvailable.\n\nFinal tip: If you go for a self-managed k8s cluster (not EKS, GKE, etc.), check that nodes are up and running before continuing upgrade.\n\nI really enjoyed this rex that was full of good prod-ready advices.\n\nI recommend you take a look at the slides\n\nSeamless Development Environments on Kubernetes using Telepresence - Ara Pulido, Bitnami\n\nKubernetes is a great production environment, but it feels like development environment is kind of an afterthought: even for a simple application, if you want to develop (locally?), things are not easy. People are currently using two distinct ways: using docker compose to replicate the production environment (but compose doesn’t do everything: rbac, job, ingress… and having to maintain everything twice is not fun), or build/push/deploy-to-k8s and wait many seconds everytime one wants to F5 on a page, which is unberably slow (I wouldn’t ask my developer colleagues to do this for even half a day!).\n\nThe solution proposed during this conference is Telepresence. It allows a developer to swap out a pod from a cluster and inject her own pod, running locally, at its place. Some sort of VPN is established between her computer and the cluster, which means the pod running locally behaves just like if it was still in the cluster (including DNS, service discovery, access to non-Kubernetes managed services and all).\n\nThere are still limitations and constraints (if two developers want to work on the same service, they’ll each need their own namespace in the cluster, as two people cannot swap out the same pod), but plans for this project are interesting and I will definitely take a closer look at it in a couple of months, when I start thinking more about our development stack!\n\nWe went to Datawire’s booth and saw a nice demo. And also learnt about other tools, such as Forge and Ambassador that can duplicate production requests to a local pod. We found that this feature is ultra dope!\n\nPerformance and Scale @ Istio Service Mesh - Fawad Khaliq, VMware Inc, Laurent Demailly, Google &amp;amp; Surya V Duggirala, IBM\n\nReturn of Istio devs on project’s recent updates: closed PRs, enhancements, etc.\n\nThat was not really what I was looking for so I went to some bootcamps to say hi, especially the HAProxy bootcamp one that is always a good moment. Special thanks to Baptiste for his time and the awesome talk we had!\nFor the record: HAProxy has it’s own Ingress Controller\n\nFrom Data Centers to Cloud Native - Dave Zolotusky &amp;amp; James Wen, Spotify\n\nThis last conference of the first day was about Spotify’s migration from their on-premise datacenters to the cloud. For many years, they were doing everything on-prem (including a 3000 nodes Hadoop cluster – the largest in Europe, at the time), often developing their own proprietary software (like custom monitoring, proprietary messaging framework, custom Java service framework, custom container orchestrator, … Some have been open-sourced). The first step for them has been to get out of their own datacenters, moving everything to another datacenter (but still using their proprietary stuff). It took them three years and a half, trying to make this migration as seamless as possible for the development teams.\n\nNext step is to become cloud native, especially moving to Kubernetes. They did this in several steps, starting small by sending production traffic to one service deployed to one cluster for one hour (allowed them to validate DNS, logging, service discovery, metrics system, networking). Then, three services on one cluster (permissions, namespaces, quotas for each namespace, developers documentation). After that, services on a volunteer basis (clusters, scripted clusters creation, secrets, deployment tooling based on a wrapper around kubectl, CI integration =&amp;gt; a lot of learning for a lot of people). Then, two high-traffic services, including a service receiving 1.5 million requests per second (horizontal auto-scaling, network setup, confidence, reference for other projects). And, finally, self-service migration, with teams migrating when they want, following the docs, and ops not always knowing what’s running in the cluster (reliability, alerts, on-call, disaster recovery, backups, sustainable deploy). Everything going pretty much fine by now, it’s time to investigate on a few odd things and specific needs, with a temporary ops team assembled to help.\n\nThe most important idea here is you don’t have to do everything right from the start. For example, they waited quite a long time before setting up a sustainable deployment method, which might seem odd to many of us. But it allowed them to move forward and validate a lot of things one after the other. That’s something I will keep in mind: if it worked for them (4000 employees, including 500 techies), it could work for many other companies!\n\nJenkins X: Easy CI/CD for Kubernetes - James Strachan, CloudBees\n\nThis might be one of the hottest project of this early 2018.\n\nWe already saw this project that was created in February, and we are using it for testing purposes. We hope to use it in production very soon.\n\nFor those of you who don’t know Jenkins-X:\n\n\n  It’s piloted by jx, a command line tool (Mac/Linux)\n  It drives a Jenkins instance + Docker Registry + Nexus + Chartmuseum + Monocular\n  It allows you to manage your app’s deployments via Jenkins blueocean’s pipelines with k8s endpoints\n  That means Jenkins will be able to run CI tests, Continuously Deploy your project to preview, staging, prod and so on with Skaffold/Helm to k8s\n  Jenkins will run pipelines from the Jenkinsfile in the repo to do that CI/CD part\n  In the provided pipelines given with jx import, you will use provided docker images that embed jx cli and other tools to manage the deployments of your app.\n  That allows you to promote your app between stages, build your docker image, etc. in your pipeline steps.\n  Those deployments are based on Helm Charts in the repo.\n  Jenkins-x follows GitOps strategy, that means anything useful is stored in each app’s repo: it is versioned and git events will trigger pipelines.\n\n\nJenkins-X brings this CI/CD part that was missing for k8s users. Gitlab + gitlab-ci were already doing that for some years now, but nothing was that fancy for GH users.\n\nWe are very excited about Jenkins-X, as it answers a lot of problematics and brings in gitops as core concept. We’re actively adopting it and we hope to give feedback asap.\n\nKeynote: Anatomy of a Production Kubernetes Outage - Oliver Beattie, Head of Engineering, Monzo Bank\n\nThis keynote was about  major outage at Monzo. You can read more about it in the post-mortem they posted after it happened. Basically, even when you are careful, an outage can still happen: several causes combined with a very specific bug happening with specific versions in a specific case and voilà. Nice talk, and nice to hear a bank being so open!\n\nKeynote: Prometheus 2.0 – The Next Scale of Cloud Native Monitoring - Fabian Reinartz, Software Engineer, Google\n\nPrometheus is the monitoring stack everyone seems to be using now. This keynote presented how much faster Prometheus 2.x is, compared to Prometheus 1.x. Having never used the 1.x versions, I have to admit I never suffered from it. It is still nice noting 2.x scales much better (requires less RAM/CPU and its performances don’t degrade much with a huge number of metrics).\n\nWelcome reception\n\nThis first day ended with a nice buffet at Bella Center, next to the sponsor booths. As we each one went to see different talks, it allowed us to chat about what we saw and heard, even if we didn’t stick around too long, after such a long day – especially knowing there would be two more just after!\n\n\n"
} ,
  
  {
    "title"    : "PHP Tour Montpellier 2018",
    "category" : "",
    "tags"     : " phptour, php, afup, 2018",
    "url"      : "/2018/05/17/retour-php-tour-2018.html",
    "date"     : "May 17, 2018",
    "excerpt"  : "Cette année encore, M6Web a sponsorisé le PHP Tour, organisé cette année par l’AFUP à Montpellier.\nNous étions donc nombreux pour assister à l’ensemble des conférences. Comme d’habitude avec l’AFUP, les conférences étaient de bonne qualité, et il ...",
  "content"  : "Cette année encore, M6Web a sponsorisé le PHP Tour, organisé cette année par l’AFUP à Montpellier.\nNous étions donc nombreux pour assister à l’ensemble des conférences. Comme d’habitude avec l’AFUP, les conférences étaient de bonne qualité, et il y en avait pour tous : débutants comme utilisateurs avancés.\n\nPour la première fois, les conférences étaient données dans un cinéma Gaumont. Un très bon choix en termes de configuration : visibilité, confort, son et lumière !\n\nEn attendant la mise en ligne des vidéos, nous remercions les conférencières et les conférenciers pour leurs présentations. Vous trouverez ci-dessous quelques mots sur les conférences que nous avons particulièrement appréciées.\n\n“Tirer le maximum du moteur PHP7”\n\nConférence donnée par Nicolas Grekas.\n\nL’approche de Nicolas était très intéressante et nous à permis de mieux comprendre le fonctionnement interne de Symfony, en lien avec les optimisations apportées par le nouveau moteur de PHP7.\n\nLes exemples cités nous ont permis de voir qu’avec quelques “tips”, il est possible de “bypasser” des étapes coûteuses lors de l’exécution de notre code.\n\n“100% asynchrone - 0% callback en PHP”\n\nUne présentation de Joel Wurtz.\n\nCette conférence nous a permis d’aborder un sujet assez peu connu dans l’univers PHP : l’asynchrone.\n\nDès qu’un projet commence à être complexe, il est souvent possible de réaliser des tâches en parallèle, non bloquantes, permettant d’optimiser les temps de réponse.\n\nPour répondre à ce besoin, Joel nous a présenté le concept de l’asynchrone : l&#39;event loop.\nVia cette boucle, Joel nous a expliqué comment les évènements sont “dispatchés” au travers de générateurs.\n\nPour aller plus loin, Joel nous a aussi parlé des outils existants qui implémentent cette logique d’event loop : AMP.\n\nEnfin pour terminer, pour être 0% callback, Joel nous a présenté Fiber. Cette extension implémente la RFC Fiber actuellement en cours d’homologation.\n\nNous vous recommandons de creuser ce sujet, qui selons nous, ouvre de belles perspectives dans l’univers PHP !\n\n“Bienvenue dans la matrice !”\n\nCette conférence était animée par Benoit Jacquemont.\n\nAujourd’hui encore, les développeurs ont trop peu de connaissance sur ce qu’il se passe à bas niveau sur nos serveurs.\n\nCette conférence, qui présentait notamment strace (pour suivre les appels système) et ltrace (pour suivre les appels aux fonctions de bibliothèques), était donc particulièrement rafraîchissante. La démo “comment voir les requêtes et réponse en HTTPS, en clair”, était complètement bluffante !\n\n“Sans documentation, la fonctionnalité n’existe pas !”\n\nCe talk était proposé par Sarah Haïm-Lubczanski.\n\nTout le monde, dans sa vie de développeur, a été confronté au problème suivant : écrire la documentation des fonctionnalités développées. \nC’est un challenge auquel nous nous sommes nous-même confrontés lorsque nous avons travaillé, l’année dernière, sur l’internationalisation de notre plate-forme, puisque nous avons dû documenter nos API, désormais appelées par des collègues basés dans d’autres pays.\n\nSarah nous a montré comment faire face à cette barrière souvent perçue comme insurmontable par bon nombre d’entre nous.\nElle nous a pour cela donné les clés et les bonnes pratiques pour créer, maintenir et rédiger une documentation cohérente.\n\nOn retiendra aussi la présentation des différents outils open source de gestion de documentation.\n\n“A la découverte du Workflow”\n\nConférence animée par Gregoire Pineau.\n\nCette conférence a retenu notre attention. Particulièrement bien faite, elle résume les fonctionnalités de ce nouveau composant de Symfony, en partant d’un workflow simple jusqu’au réseau de Pétri. Grégoire donnait des exemples d’utilisations concrètes.\n\nDifficile à résumer, je vous invite à consulter la documentation Symfony sur ce composant.\n\nMais encore ?\n\nDe plus, trois conférences ont attiré notre attention de par leur valeur pédagogique. Elles étaient à nos yeux particulièrement  intéressantes pour des débutants ou des personnes ne connaissant pas encore le fonctionnement de certains processus suivis par notre communauté.\n\nDans l’ordre, vous trouverez :\n\n\n  IT figures par Sara Golemon, qui revient sur ce qu’est le FIG, organisme important qui régit aujourd’hui une partie de l’organisation de la communauté PHP, et sur ce que sont les PSRs.\n  Nommer les choses ? Oui : avec le DNS par Julien Pauli. Cette conférence revient sur les bases du fonctionnement du DNS et son utilité.\n  Et, pour finir : Caching with PSRs par Hannes Van De Vreken. Dernière des conférences “à voir une fois”, celle-ci revient sur ce qu’est le cache en général, pourquoi on en utilise. Puis s’intéresse au cache applicatif via les PSRs.\n\n\nLe dernier PHP Tour\n\nCe PHP Tour était le dernier, puisque l’AFUP proposera à partir de 2019 un nouveau format pour les événements en région : l’AFUP Day. Nous aurons grand plaisir à vous y rencontrer à nouveau, à Lyon cette fois-ci !\n\nEncore un grand merci à l’AFUP !\n\nEnfin, retrouvez toute l’actualité de l’événement sur #phptour.\n"
} ,
  
  {
    "title"    : "How a fullscreen video mode ended up implementing React Native Portals?",
    "category" : "6play",
    "tags"     : " React, ReactNative, mobile",
    "url"      : "/6play/2018/04/15/how-a-fullscreen-video-mode-ended-up-implementing-react-native-portals.html",
    "date"     : "April 15, 2018",
    "excerpt"  : "This story introduces a declarative native side portal implementation module called rn-reparentable.\n\nMy teammate (Laetitia BONANNI) and I are working on a React Native module embedded in the 6Play application that aims to provide best moments of ...",
  "content"  : "This story introduces a declarative native side portal implementation module called rn-reparentable.\n\nMy teammate (Laetitia BONANNI) and I are working on a React Native module embedded in the 6Play application that aims to provide best moments of different TV shows from the M6 channel.\n\nThe module, called Refresh, is a list of videos that are playing while the user is scrolling. It also provides a “theater mode” which is a way to create an immersive user experience by obscuring the cards that are not focused:\n\n\n\nAs any other video application, it provides a fullscreen experience to the user by rotating the device.\n\nAt the time I’m writing this article, creating such a thing using React Native is a pain. Here’s the story why.\n\nCreating a fullscreen, the web developer way\n\nWhile being a web developer, we usually work with positioning to display something over the rest (like a popup for example).\n\nDealing with React Native and its style APIs (which really looks like the web one), we thought that it would be super-easy to simulate the exact same behaviour.\n\nThat’s why our main idea was to manage the fullscreen mode by adding a style that takes the screen size and an absolute position. Thus, the video would have followed the device edges while rotating:\n\n\n\nReact Native styles are not the same as the web ones\n\nThe idea of creating an almost equivalent API as the web one is really good for the learning curve of React Native. It is a real asset when you want to create simple user interfaces.\n\nBut there is a drawback. This approach makes us want to get the exact same result as we would have on the web.\n\nIn our case, the use of absolute positioning was sadly not working. In fact, it is written in the React Native documentation:\n\nPosition\n\nposition in React Native is similar to regular CSS, but everything is set to relative by default, so absolute positioning is always just relative to the parent.\n\nIf you want to position a child using specific numbers of logical pixels relative to its parent, set the child to have absolute position.\n\nIf you want to position a child relative to something that is not its parent, just don’t use styles for that. Use the component tree.\n\nSee https://github.com/facebook/yoga for more details on how position differs between React Native and CSS.\n\n\nThe fact that an element is always positioned relatively to its parent has been a problem for us since our player component is part of the list.\n\nOverflow and android are not friends\n\nAnd even if we would have found a way (we could have cheated by calculating negative values, relative to the parent, in order to stick to the edges of the device), we would have met other problems such as the fact the equivalent of overflow prop doesn’t work on Android.\n\nThere are actually multiple opened issues concerning this problem. Grabbou gave a shot on this one #7229:\n\n\n\nLet’s make a fullscreen, the native way\n\nHopefully, we are working with native developers, from both platforms. We have shared a lot of information and finally have found a solution.\n\nThis time, while rotating the device, we would have hidden everything around the VideoPlayer component. No more headers, no more footers, nothing except the player. Then, we would have set the player size so that it matches the device size:\n\n\n\nHere’s the result we have got:\n\n\n\nWhat is happening on here?\n\nThere are multiple interesting things here, at ~200 cards down:\n\n\n  \n    Special visual effects are appearing (gray and blue background color)\n  \n  \n    The list scrolls too high and then refocuses\n  \n  \n    The video restarts\n  \n\n\nExplanations\n\nThe first thing to know is that we only keep 5 players alive (2 above, the focused one, and 2 below) and otherwise we display images. It’s important because of memory. Without this limitation, the application would have thrown some OutOfMemory errors (we met this kind of problems with Bitmap objects).\n\nThe second thing to notice is that we are always playing the video that is the most centered on the screen.\n\nThe last thing to know is that we actually have multiple rendering cycles to hide the different components around the VideoPlayer.\n\nFor now, with that information, let’s imagine the following scenario:\n\n\n  \n    Scroll ~200 cards down\n  \n  \n    The most centered video is now playing\n  \n  \n    Rotate the device\n  \n  \n    It resizes all the images / VideoPlayer to match the device size\n  \n  \n    It removes ~200 headers + ~200 footers\n  \n\n\nDuring the 5th step, the list is scrolling up, because it has earned some space with the headers and footers disappearing. This creates the strange behaviour of “yo-yo” list scrolling. Moreover, when the list is scrolling, the application finds a new “most centered card”, and creates the associated player. If the previous player is not part of the 5 new conserved ones, it’s destroyed. Thus, the further we scroll in the list, the worse it becomes.\n\nThe combination of the 4th and 5th step creates the actual gray / blue screen in background.\n\nFor now, we have a quasi-functional solution. It’s not really user friendly but we have something close to work. The key point here is that improving the functional solution (avoid the “yo-yo” effect) would also give a better user experience.\n\nSo, how can we avoid this “yo-yo” behaviour ?\n\nPortal to the rescue\n\nRecently, we heard about React portals. It seems that it could have saved us from this specific situation. The idea is quite simple, we would have teleported the player from its current location to somewhere higher in the component tree, like the React Native documentation encourages us to, without triggering special state based rendering-cycles (aka: Headers + Footers removals):\n\n\n\nThe problem is that React Native doesn’t support them natively: portals are part of ReactDOM, not React itself. We can’t use it in our application.\n\nWe’ve found and experienced some great open source alternatives on the JavaScript side such as react-gateway and we even managed to create our own one for this specific case.\n\nThe problem is that React would have created a new instance of the VideoPlayer each time we would have moved it, instead of keeping the old one. It means that we would have created 2 VideoPlayer, and lost both context.\n\nEach time we rotate the device, the video will restart from the beginning.\n\nWhat can we do with portal ? On the native side?\n\nThe portal idea is quite interesting: we need to find a way to create a portal-like behaviour with React Native, but on the native side, so that we won’t lose the VideoPlayer native context.\n\nSince we had the chance to be at the React Native Europe, we have learnt the way React Native is managing views thanks to Emile Sjolander.\n\nTo demonstrate this idea, let’s take an example :\n\n\n\nThis is a simple application which provides two  components and displays some content. On the right, we can see the native tree view. The cursor shows the two native views that need to permute. The idea is to make First taking place of Second and vice versa.\n\nIt’s possible, using React Native, to use the module responsible of view management: UIManager (available directly from react-native module):\n\n componentDidMount() {\n    setTimeout(() =&amp;gt; {\n      // Permute child at indice 0 and 1 of parent tag 6\n      UIManager.manageChildren(6, [0], [1], [], [], []);\n    }, 3000);\n  }\n\n\nThis will end up making something like:\n\n\n\nIt seems that creating a portal-like behaviour is possible using ReactNative.\n\nThe main reason we didn’t choose this solution is the fact that we didn’t find a way to get the UIView native identifier from the JavaScript side (I’m not talking about nativeID or testID props, but the unique identifier of the view set on the native side).\n\nHere’s a tweet from me concerning unique identifier\n\nNative implementation of “portals”\n\nWe finally decided to implement a React Native native component called  that is able to move View children from a parent view to another one using a declarative API.\n\nUsing this approach, we gain more control over what we would like to do leveraging native side power.\n\nReparentable owns two props :\n\n\n  \n    name that represents the destination of the teleportation\n  \n  \n    target that represents the name of the target\n  \n\n\n&amp;lt;View style={styles.container}&amp;gt;\n  &amp;lt;Reparentable name=&quot;1&quot; target=&quot;&quot;&amp;gt;\n    &amp;lt;Text&amp;gt;First&amp;lt;/Text&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n\n  &amp;lt;Reparentable name=&quot;2&quot; target={this.state.shouldGo ? &quot;1&quot; : &quot;goNowhere&quot;}&amp;gt;\n    &amp;lt;Text&amp;gt;Second&amp;lt;/Text&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n&amp;lt;/View&amp;gt;\n\n\nOn this gist, &amp;lt;Reparentable name=”2” …/&amp;gt; will take place of &amp;lt;Reparentable name=”1” …/&amp;gt; when the state shouldGo will change.\n\nWhat does it mean?\n\nIn our context, it means that when the state isFullscreen is true, we are able to move the player from its current view to the higher one:\n\n&amp;lt;View style={styles.container}&amp;gt;\n  &amp;lt;Reparentable name=&quot;fullscreenView&quot; target=&quot;&quot;&amp;gt;\n    &amp;lt;FullScreenContainer /&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n\n  &amp;lt;Reparentable\n    name=&quot;videoPlayerId&quot;\n    target={this.state.isFullscreen ? &quot;fullscreenView&quot; : &quot;&quot;}\n  &amp;gt;\n    &amp;lt;VideoPlayer /&amp;gt;\n  &amp;lt;/Reparentable&amp;gt;\n&amp;lt;/View&amp;gt;\n\n\nHere’s the result we’ve got:\n\n\n\nComparing both variants\n\n\n\nIt took us time to get this result, but we finally have something that meets our needs.\n\nLink to the library : https://github.com/mfrachet/rn-reparentable\n\nThanks for reading,\n"
} ,
  
  {
    "title"    : "The 6play platform goes international",
    "category" : "6play",
    "tags"     : " 6play, intl",
    "url"      : "/6play/2018/03/26/6play-goes-international.html",
    "date"     : "March 26, 2018",
    "excerpt"  : "Within less than a year m6web and techm6web managed to launch websites, android apps and ios apps for three RTL TV channels in Europe, all based on the 6play technology.\n\nThese deployments include:\n\n\n  broadcasted and themathic channels,\n  live st...",
  "content"  : "Within less than a year m6web and techm6web managed to launch websites, android apps and ios apps for three RTL TV channels in Europe, all based on the 6play technology.\n\nThese deployments include:\n\n\n  broadcasted and themathic channels,\n  live streaming, video catchup encoding and playout,\n  video resuming,\n  local adservers (for videos and display), DMP system and CDN,\n  contribution via our backoffice and automated through our API,\n  translations system,\n  and almost all the features of the 6play platform.\n\n\nRTL Play (Belgium)\n\n\n\nhttps://www.rtlplay.be/\n\nRTL Play (Croatia)\n\n\n\nhttps://play.rtl.hr/\n\nRTL Most (Hungary)\n\n\n\nhttps://www.rtlmost.hu/\n"
} ,
  
  {
    "title"    : "Useful (or not) M6Web OSS stuff",
    "category" : "OSS",
    "tags"     : " OSS, Open source, php, js",
    "url"      : "/oss/2018/03/20/useful-or-not-usefull-m6web-stuff.html",
    "date"     : "March 20, 2018",
    "excerpt"  : "At M6Web we do love open source and we are trying to be good open-source dev citizens! Here is a short presentation on our most interesting contributions:\n\n\n\nYou can find all our open source contributions on their dedicated page on our tech blog. ...",
  "content"  : "At M6Web we do love open source and we are trying to be good open-source dev citizens! Here is a short presentation on our most interesting contributions:\n\n\n\nYou can find all our open source contributions on their dedicated page on our tech blog. Enjoy !\n"
} ,
  
  {
    "title"    : "Atteindre les étoiles avec PHP et Symfony",
    "category" : "",
    "tags"     : " conference, confoo, PHP, Symfony",
    "url"      : "/2018/03/07/atteindre-les-etoiles-avec-php-et-symfony.html",
    "date"     : "March 7, 2018",
    "excerpt"  : "À l’automne 2014, M6 décide d’adapter le programme Rising Star en France, un concours de chant en direct, mais dont le jury est le public, qui vote en direct depuis son application mobile.\nA travers cette conférence, je me propose de vous présente...",
  "content"  : "À l’automne 2014, M6 décide d’adapter le programme Rising Star en France, un concours de chant en direct, mais dont le jury est le public, qui vote en direct depuis son application mobile.\nA travers cette conférence, je me propose de vous présenter l’architecture mise en place pour être capable de traiter plusieurs dizaines de millions de votes dans un délais de quelques secondes, tout en se synchronisant avec une émission de télé en direct.\n"
} ,
  
  {
    "title"    : "Migration to Spark 2.2",
    "category" : "",
    "tags"     : " Data, Hadoop, BigData, Airflow, Hive, Spark, Java",
    "url"      : "/2017/12/13/spark-2.html",
    "date"     : "December 13, 2017",
    "excerpt"  : "To value our data in order to understand better our service and improve it, we use Spark. You can find more information in a recent article about our datalake. We recently migrated our biggest project from Spark 1.5 to Spark 2.2 and wanted to shar...",
  "content"  : "To value our data in order to understand better our service and improve it, we use Spark. You can find more information in a recent article about our datalake. We recently migrated our biggest project from Spark 1.5 to Spark 2.2 and wanted to share that story.\n\nSpark 2 has been released a year ago (July 26, 2016). Maybe we are a bit late, but better late than never.\n\nWe are working with an official version from Cloudera with Spark 1.6 as the default version.\n\nOur project runs everyday to get data from different sources and send them to different destinations.\n\nIt is built with Java and Spark 1.5, but we encountered several problems with those technologies. First of all, the Java + Spark community is smaller than the ones for Python or Scala. Secondly, the Spark 1.5 community is also smaller than the one of version 2.2.\n\nThat sometimes made information hard to find.\n\nBut most of all, we did not succeed to integrate new components that work with more recent versions.\n\nWe wanted to migrate for bugs fixes in general and in a performance purpose too.\n\nI) Workflow\n\n\n\na) Spark 1.5 to Spark 1.6\n\nFirst, we had decided to migrate to 1.6 to do a progressive migration. But we bumped into a bug with a UDF. We had difficulties fixing it, and it was resolved in 2.2.\n\nWe finally decided to migrate directly to 2.2.\n\nb) First validation with unit tests\n\nWe did the migration and ran our unit tests to see and fix the problems.\n\nc) Functional tests\n\nThen, we ran our jobs with some data sets. The idea was to check the differences with Spark 1.5.\n\nWe wanted to be sure that our unit parts were working together.\n\nd) Double run\n\nThen, we set out for a double run. It means that we had our jobs running both with Spark 1.5 and Spark 2.2 and we compared the outputs each day.\n\nWe used Airflow to deal with that. If you know Airflow, you will understand that we added a new DAG to run our project with Spark 2.2.\n\nThe idea was to see the potential differences between the two on a daily basis.\n\nAt the end, we merged our branch into master.\n\n2) Changes to migrate to 2.2\n\nThere are different changes from Spark 1.5 to 1.6 to 2.2. You will find them described in the documentation.\n\nThe idea here is to focus on the problems we met, the noticeable changes for us and how we dealt with them.\n\na) Dataset\n\nOf course, the main change is that “dataFrame” does not exist anymore. You must replace it by “Dataset&amp;lt;Row&amp;gt;”.\n\nActually, “DataFrame” and “Dataset” were unified with Spark 2.0. In reality, for untyped API like Python, “DataFrame” still exists. But, we work with Java.\n\n\n\nUsing “Dataset&amp;lt;T&amp;gt;” is a way to apply a schema at the compilation. If there is a problem, you will get a logical exception. Before, with “DataFrame”, you could only have runtime exceptions.\n\nAs a first step, we replaced “DataFrame” by “Dataset&amp;lt;Row&amp;gt;”\n\nb) SparkSession\n\nA second major difference is “SparkSession”. It is the new entry to Spark. \nThere is no need anymore to create a “SparkConf”, a “SparkContext” and a “SQLContext”. It is possible to get all of it just with a “SparkSession”.\n\nBut, it is important to understand that if you just want to migrate your code in a first step to get it work with Spark 2, it is not a need to use “SparkSession”. “SparkConf”, “SparkContext” and “SQLContext” still work.\n\nThat is what we decided to do.\n\nc) Iterable to Iterator\n\nThe return type “Iterable” is incompatible with “PairFlatMapFunction”. We had to replace “Iterable&amp;lt;&amp;gt;” with “Iterator&amp;lt;&amp;gt;”.\n\nWe replaced code like that:\n\npublic Iterable&amp;lt;String&amp;gt; call(String s) throws Exception {\n    ...\n    return list;\n}\n\n\nby something like that:\n\npublic Iterator&amp;lt;String&amp;gt; call(String s) throws Exception {\n    ...\n    return list.iterator();\n}\n\n\nd) Creating a UDF using hiveContext is not possible anymore the same way\nBefore, you could do something like that :\nhiveContext.sql(&quot;CREATE TEMPORARY FUNCTION function AS ...&quot;)\n\n\nBut now, you have to enable hive support first. You must do it with the SparkSession:\n\nSparkSession spark = SparkSession\n    .builder()\n    .appName(&quot;Java Spark Hive Example&quot;)\n    .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)\n    .enableHiveSupport()\n    .getOrCreate();\n\n\nIf “enableHiveSupport” is not enabled, there is an error like this :\n\njava.lang.UnsupportedOperationException: Use sqlContext.udf.register(...) instead.\n\n\nWe decided not to use “SparkSession” in a first step and to follow the error instructions.\n\nWe replaced our direct call to Hive by a UDF registration.\n\ne) Deprecations\n\nWe noticed some deprecations like HiveContext or Accumulators for instance. But we decided not to deal with them for the moment.\n\nd) Performance\nWe have made some gains in performance. \nBefore, running our jobs lasted around three hours. Now, it lasts around two and a half hours.\n\nWe hope we will make some other gains by migrating to the Spark 2.2 philosophy (“SparkSession”, etc).\n\nConclusion\nAs there are many backward compatibilities with Spark 2, it is not so difficult to make a first migration to make your project work. Nonetheless, it could be long to validate. It depends on your tests stategy too.\n\nOur next step now will be to integrate the new philosophy of Spark 2.2 to get the best of the new version.\n\nNastasia Saby (Zenika consultant)\n"
} ,
  
  {
    "title"    : "Forum PHP AFUP 2017",
    "category" : "",
    "tags"     : " afup, php",
    "url"      : "/2017/11/07/forum-php-2017.html",
    "date"     : "November 7, 2017",
    "excerpt"  : "M6Web était sponsor de cette édition du Forum PHP organisée par l’AFUP et une grande partie de l’équipe backend avait fait le déplacement. \nCe forum était vraiment inédit de par sa taille sans précédent : plus de 650 participants ! Il a été aussi ...",
  "content"  : "M6Web était sponsor de cette édition du Forum PHP organisée par l’AFUP et une grande partie de l’équipe backend avait fait le déplacement. \nCe forum était vraiment inédit de par sa taille sans précédent : plus de 650 participants ! Il a été aussi pour l’équipe l’occasion de voir des présentations de grande qualité et très inspirantes. (sans compter celle de nos collègues Fabien et Nastasia sur l’AB testing).\n\n\n\nDe très nombreux retours exhaustifs sont disponibles sur le web et je pense que les vidéos seront rapidement en ligne sur la page listant tous les talks organisés par l’AFUP. On peut noter, comme à chaque Forum, les tendances qui se dégagent de l’ensemble des talks et suite aux discussions endiablées qui suivent les présentations :\n\n  DDD commence à être présent dans tous les talks type méthodo,\n  GraphQL fait parler de lui, et c’est tant mieux,\n  des reality checks sur les modes de ces dernières années (comme les micro services, la qualité au sens large), mais une maturation et un recul sur des pratiques modernes qui font plaisir à voir,\n  des considérations très intéressantes sur la gestion du code source (refactoring, clean code, nommer les choses ;) …).\n\n\nEnfin, j’ai vraiment (en tant que lyonnais) apprécié la régionalisation de l’AFUP, avec une multitude d’antennes locales créees ces dernières années.\n\nBravo l’ @afup pour la régionalisation des badges au #ForumPHP ! pic.twitter.com/ekewUkCoKS&amp;mdash; Olivier Mansour (@omansour) 26 octobre 2017\n\n\nUn excellent cru que M6Web était ravi de soutenir ! Et rendez-vous au PHP Tour !\n"
} ,
  
  {
    "title"    : "Genesis of M6&#39;s Datalake",
    "category" : "Data",
    "tags"     : " Data, Hadoop, BigData, Airflow, Hive, Spark, DMP",
    "url"      : "/data/2017/10/23/genesis-of-m6-datalake.html",
    "date"     : "October 23, 2017",
    "excerpt"  : "At M6 we work hard to leverage Data to help our ad sales team, our CRM teams and our product innovation process.\nOver the past 2 years, we have gone from using a market DMP to creating our second Hadoop platform. We now feel that our stack is stab...",
  "content"  : "At M6 we work hard to leverage Data to help our ad sales team, our CRM teams and our product innovation process.\nOver the past 2 years, we have gone from using a market DMP to creating our second Hadoop platform. We now feel that our stack is stable, reliable and scalable so it feels like the right time to share our experience with the community.\n\n\n\nStep 1: embracing the DMP\n\nOur first use case was to monetize data through targeted publicity.\nWe decided to start by installing a DMP (Data Management Platform) because it was a very fast solution to deliver our major needs, in particular :\n\n\n  Collect data from all our services and combine it with our user\nknowledge =&amp;gt; DMPs offer that off the shelf\n  Create segments for audience targeting =&amp;gt; The segmentation approach offered by DMPs was well adapted to the ads market\n  Activate our Data, both in house via our adservers and in the outer market =&amp;gt; DMPs generally offer simple integration with most adservers, and a very straight forward third party integration\n\n\nThe match seemed quite obvious and there’s a good reason for that: DMPs are designed for this use case above all others.\n\nWe chose Krux (now Salesforce) and deployed it over our ~30 sites and applications. Installing Krux on our network and plugging it to our video and display adservers ended up taking a few months and a decent effort. Convincing all our teams that the increase in ad revenue would make it worth the development time and the negative impact on webperf wasn’t trivial, but got through thanks to our top management sponsoring. Once on the job, the deployment was quite smooth on the web and mobile apps, but validating the quality of the ingested data turned out to be an endless project.\n\nAt the end of the day, Krux’s DMP did the job. In November 2015 we launched Smart6tem, our Data platform &amp;amp; advertisement offer based on segments (announcement here, articles here here or here). This move had a very positive effect on our advertisement market, and allowed to start making Data mean something at M6.\nTo give some detail of our use of the DMP, it turned out building our own segments was very successful, but we didn’t use any 3rd party interconnection because we didn’t find any valuable Data to buy and didn’t want to reduce the value of our own Data by sharing it out.\n\n\n\nKrux’s segment builder\n\nOnce the Advertisement use case was out in the market, we moved our efforts towards leveraging the DMP for our CRM teams. The rationale was simple: targeted emails are more efficient than newsletters. We were hoping to reduce the email pressure on our users while increasing the performance both for revenue and traffic.\n\nStep 2: first round testing Hadoop\n\nHaving a DMP is both a great accomplishment and frustrating.\nIt’s great because you can start to combine the use of your service with the user profiles to produce segments and activate use cases to address them.\nBut for CRM, the workflows to plug segments into our emailing systems weren’t native and we needed to build some custom workflows. No rocket science, but when we first received a 2 Billion line file for the user/segment map that we needed to filter and convert into another format, our developers went grumpy.\nWe also got frustrated very fast because we wanted to start to extract some unpreceded analytics insights combining our user knowledge (our major service, 6play had just switched to fully logged-in users) with usage stats or with external sources like our adserver logs. Advanced analytics was clearly not the field of Krux.\nLast but not least came some limitations (either due to the design of Krux or the pricing):\n\n\n  We could only work on 3 months of history if we wanted to keep the price reasonable (on 6play we have a lot of TV shows that run for 3 months per year, and segmenting the users who watched the show last year is important).\n  It’s based on cookies + device ids on mobile (it’s the best solution for most use cases, but if your users are logged in, it introduces quite a lot of risk to make mistakes).\n  We never managed to convince our users that the amount of cookies or users inside segments was correct. Every single study we made on this point led to doubt, and our DMP support team never came up with serious answers.\n\n\nAt this point, Hadoop came in as an evidence, so we created our first cluster.\nThe process of creating this proof of concept cluster was pretty much a black box for us since we charged a partner with the job. We ended up with the following setup, all hosted by AWS :\n\n\n  2 name nodes with 16 VCPUs and 30G RAM each\n  4 data nodes adding up to 64 vcpu’s and 120G RAM\n  Cloudera Enterprise with Hive, Impala, Hue, Python, R and a kinky crontab\n  Tableau Desktop + Tableau Server\n\n\nNothing crazy but that brought us into the world of Hadoop, and that was a major move. We also staffed our first Data Scientist to start to explore our Data and imagine use cases.\n\nOur first steps in Hadoop were hesitant, but within a few months, we had created our first Data Lake, our targeted CRM was live and we had produced a few dozen dashboards providing unpreceded insights throughout the company. From the business perspective, it was a success.\nFor the people who got their hands on a Data Lake for the first time the experience was ground breaking. For the first time, we could connect information from half a dozen different tools seamlessly.\nAn example: finding how many ads were seen by women from 25 to 49 years old during the NCIS TV show.\nBefore the Data Lake, this would have been impossible. The closest we could get would take the following process :\n\n\n  Extract the amount of ads viewed on NCIS from our adserver stats to a text file\n  Extract the NCIS traffic from our video consumption tracking tool (in Cassandra) to a text file\n  Extract our users Database with age and gender (in a third party tool named Gigya) to a text file\n  Load all this up into an Excel spreadsheet\n  Write a bunch of Excel formulas to produce the percentage of the traffic on NCIS that’s generated by women between 25 and 49 years old\n  Apply that percentage to the adserver stats\n\n\nAs you can see, combining information between our ecosystems involved some very manual processes and could only lead to approximations, so basically we never did them.\n\nWith our Data in Hadoop, all this turns out to be a simple SQL query in Hue (a PhpMyAdmin style interface for Hadoop):\n\nSELECT COUNT(*) FROM adserver_logs A\n   JOIN users U ON A.uid = U.uid\n   JOIN programs P on A.pid = P.id\nWHERE A.type = &#39;impression&#39;\n   AND U.age &amp;gt;= &#39;25&#39;\n   AND U.age &amp;lt;= &#39;49&#39;\n   AND U.gender = &#39;F&#39;\n   AND P.name = &#39;NCIS&#39;\n\n\nHadoop and our Data Lake, we could just jump over the barriers between tools and ecosystems within seconds. Combined with the ability to code in various languages, we could instantly start to industrialize such insights and start going further.\n\nWe convinced our top management very fast about the value of having our own Hadoop cluster, and since it was very (VERY) expensive, we decided to internalize it.\n\nStep 3: building our internal Hadoop cluster\n\nSo there we were with a quite simple roadmap: replace our v1 Hadoop cluster to reduce costs and improve performance as much as possible. We managed to divide the price by 3 while multiplying the resources by 8.\n\nThe first step on this road was to staff a tech team to design and create our platform. That ended up being very tricky and finally took us 10 months to complete.\n\nOnce the team was staffed, we got onto the job. We had 5 steps :\n\n\n  Choose the hosting platform (4 months)\n  Choose the hardware\n  Choose the software stack (2 months, done in parallel)\n  Set up the cluster (2 months)\n  Migrate all our projects to the new platform (3 months)\n  Check to be sure everything was done (1 month)\n\n\na) Hosting platform\n\nThis stage of the project was a very religious one. Many people at M6 had a very strong desire to go towards cloud and managed services, others were totally in favor of Hadoop and have full in house control over the platform. The major options were:\n\n\n  AWS\n    \n      Amazon EMR + S3\n      Amazon EC2 + S3\n    \n  \n  Google Cloud Platform\n    \n      Managed services (Dataflow, BigQuery, Compute engine, Pub/Sub…) + Cloud storage\n      Dataproc + Cloud storage\n    \n  \n  On premise\n    \n      Add servers to our 6play platform at Equinix\n      Work with our hosting subsidiary, Odiso\n    \n  \n\n\nWe spent 3 months talking to the different vendors and considering options.\n\nThe first decision we took was to use Hadoop instead of managed services.\nThe AWS and Google sales teams were very convincing, but we finally declined for 2 main reasons:\n\n\n  People in our company were starting to learn how to use Hadoop, changing the stack would have forced everyone to re-learn what they were just starting to dominate. Not very efficient while building up expertise.\n  Using proprietary solutions like Big Query involves a strong locking risk. If we developped all our projets to leverage a specific platform, changing providers in a few years would involve a lot of reworking on all our code base.\n\n\nThe next step was to choose between the 3 hosting options. On a side note, we compared the price for x4 and x10 resources compared to our v1 platform.\nAt the end of the process we wrote up an evaluation grid. Here is the summary version.\n\n\n\nThe decision was there, we went for a fully on premise stack with Odiso.\nTo detail some of that evaluation, here’s a few insights on what it came down to.\n\n\n  AWS is cool, but ultra expensive. I mean it’s 10 times more than our on premise option! We would have gone full AWS if the price was reasonable. The possibility to pop clusters up and down is very interesting and reduces costs, but our v1 platform was using our 4 EC2 Data Nodes at ~80% 24/7, so we could never go down to 0 servers.\n  Google feels better on the service side of things, but it involved taking chances because the commercial product is young and support + community experience seemed weak.\n  On premise was clearly much cheaper, and felt more secure for our low experience on Hadoop since we’re used to managing servers and our team had managed serious Hadoop before.\n\n\nb) Hardware\n\nGoing on premise means buying physical servers and building them.\nOur goal here was to massively upgrade our current platform to scale with the company’s usage of Big Data. Since the price was very reasonable, we settled down to x8 on CPU, RAM and storage compared to our initial Hadoop cluster. Here’s the stack we bought:\n\n4 KVM servers:\n\n\n  DELL PowerEdge R630\n  OS Disks: 2x 400GB ssd, RAID 1\n  Data disks: 8 * 2To\n  RAM: 256Go (8*32G)\n  CPU: 2x12 Cores (3.0Ghz)\n\n\n15 Data Nodes:\n\n\n  DELL PowerEdge R630\n  OS: 2x 400GB ssd, RAID 1\n  Data disks: 8 * 2To\n  RAM: 384Go (24*16G)\n  CPU: 2x12 Cores (3.0Ghz)\n\n\nBuilding and racking the servers was quite straight forward, there’s nothing special about Hadoop in this process except the high quality network connectivity.\n\nc) Software stack\n\nDesigning the software stack was very straight forward.\nWe had the desire to stay as close as possible to the stack our users were getting used to, and it was pretty much a standard Cloudera stack. That suited us very well because our first priority was to avoid any regression, both for the projects (during this period, they had massively multiplied as we’ll detail in the migration part below) and for the users.\nAnother early choice was to use virtual machines with Proxmox and not dive into the Kubernetes + Docker adventure. Although that was tempting and will probably be an option in future, we considered mastering the Hadoop stack was enough on our plate for the moment, we needed to reduce risk.\n\nHere’s the stack we chose:\n\n\n  Puppet\n  Centos 7\n  Proxmox\n  Cloudera Hadoop 5.11 (free version)\n  Hadoop 2.6\n  Hive 1.1\n  Spark 1.6 and 2.1 (we had 1.6 before but our Data Scientists really wanted to use new features)\n  Supervisord\n  MariaDB\n  LDAP\n  Ansible\n  OpenVPN\n  Python 2.7 and 3.6 with Anaconda\n  Java 8\n  R 3.3\n  Scala\n  Airflow 1.8 (this is out of the Cloudera stack, an important and epic part of our toolkit that we’ll surely talk about in more detail in a future post)\n  Sqoop\n  Hue 3 with Hive on Spark as default\n  Tableau Desktop + Tableau Online\n  Jupyter\n\n\nd) Install Hadoop and all our tools\n\nOne of the fun parts of our design process was to choose a name for our new cluster. We called it Cerebro (in reference to X-Men and the global view of Professor Xavier), and created a logo :)\n\n\n\nSetting this stack up felt very simple from my perspective, but that’s surely because our awesome team overcame the issues silently.\nOn the timeline, the biggest part of the setup was receiving the physical servers. That took about 3 months because some parts (SSD disks) were out of stock for a long time.\nWe received a first part of the Data Nodes a couple of months before the rest of the servers, so we decided to start building the cluster with temporary Name Nodes and services, and migrate them after.\n\nWe deployed Cloudera Hadoop via KVM servers (managed with Puppet) and the Cloudera Manager. Very straightforward.\nWe used Ansible to install our stack, manage all our configuration files and user access.\n\ne) Migrate our projects and Data\n\nMigration was a project in the project.\nBetween the day we decided to build our internal platform and the day we delivered, 20 months had gone by. During all that time, Big Data had been going through high pace growth inside M6. We scaled from ~1 to ~25 users, from 0 to ~200 Dashboards and ~60 projects. All of this relying on our “Proof Of Concept” platform created with a partner.\nTo be honest, it was an utter mess in any Software Engineer’s eye. Imagine: no version control, a unique user hosting all the projects and executing 6000 crontab lines each day. No job optimisation whatsoever. Moreover, most of our users had no developpement process knowledge, so they didn’t see any problem with all this and weren’t all in favour of any change. The context was challenging.\n\nThe first step of our migration project was to bring all this back into a “migratable” state. To do that, we went through the following steps :\n\n\n  Put all the code base in Git\n  Create a code deployment process\n  Split the production jobs down to a 1 user per project approach, both for code execution and data storage\n  Make all paths to data relative\n  Switch from crontab to Airflow\n  Add backups on S3\n\n\nWe reached this milestone after 4 months of a large rework of all our projects by all our teams. The collective investment in this process was a real team success.\n\nThe second step was to rebuild all the projects and databases on the new platform.\nThanks to our new backup system that copied all our Data to S3, rebuilding databases was easy. Basically it took creating a script to restore the backup in the new platform, and we could start checking integrity by querying the datasets. Rebuilding projects was a similar process, we just had to deploy each project and it was ready to test. Everything went fast and easy, proving that all the preparation moves we made were very valuable.\n\nThe third step was to double run all our projects so we could be sure everything worked on the new platform while not breaking production.\nThere’s a tricky part to this because a fair amount of our projects include an output towards external servers (either other teams within M6 or 3rd parties). For this we had to add an “only run on” logic. That lead us to create a unified configuration and a library for exports.\nWe also had to distinguish all our code execution monitoring so we could keep an eye on what each workflow was producing, both in production and on the new platform. For this we added the platform name to all our Graphite nodes and updated all our dashboards to filter by platform.\nWith those 2 moves, most projects managed to run “out of the box”. Some needed some refactoring, mostly for parts that had been forgotten in the first step.\n\nThe fourth step was validating that our double run was working well.\nThe theory of this validation was quite elaborate. For each table or output job we would count the number of lines in each partition produced, run checksums, dive into the details of the monitoring, and run manual tests.\nIn practice, that part cracked up quite fast because our v1 platform was being totally outscaled and therefore all our users really didn’t want to look back. We checked that the backups were good with file sizes and line counts, and for the rest we relied on our monitoring to be sure that the jobs runned and produced the same output volumes. For the most critical production jobs we went into some detailed manual checking, but we took the jump very fast.\n\nThe fifth and last step was migrating all our Tableau Dashboards to Tableau online.\nWe needed all our ingestion and treatment jobs to be up and running before we could migrate our 200+ Dashboards. Once that was done, most dashboards took nothing more that being opened in Tableau Desktop and published to Tableau Online. The only exceptions were the bunch of users who had missed some tables out in step 1. Those had to run through the whole process at fast speed… Not very pleasant for them.\n\nSo there we are, we now have our 2 feet in our second Hadoop platform. Now we’re looking forwards, both on how we make this platform evolve to empower our future use cases, and to raise our innovation pace for Big Data to count much more within M6.\nBy all means stay posted, we’ll update you on some of the awesome projects we’ve been working on!\n\nTake away\n\n\n  Deciding to create an internal Hadoop platform took time and a few previous steps for our organisation to start to understand what Big Data was about and the way to go around it.\n  Choosing our hosting solution was hard and very conviction driven.\n  On premise hosting is cheaper than cloud solutions, but obviously less flexible.\n  No surprise for tech people and it’s valid way beyond Big Data, migrating projects developed without any engineering good practices was hard and risky work.\n\n\n"
} ,
  
  {
    "title"    : "Elasticsearch: la grande migration",
    "category" : "",
    "tags"     : " Elasticsearch, Php",
    "url"      : "/2017/06/01/migration-elasticsearch.html",
    "date"     : "June 1, 2017",
    "excerpt"  : "Pour assurer la scalabilité des performances de l’API 6play, les données suivent tout un workflow pour être dénormalisées et stockées dans Elasticsearch.\nMi-2016, nous avons identifié des dysfonctionnements majeurs sur nos serveurs, entrainant par...",
  "content"  : "Pour assurer la scalabilité des performances de l’API 6play, les données suivent tout un workflow pour être dénormalisées et stockées dans Elasticsearch.\nMi-2016, nous avons identifié des dysfonctionnements majeurs sur nos serveurs, entrainant parfois des interruptions de service.\nSuite à quelques mesures d’urgences pour stabiliser l’existant, nous avons entrepris de mettre à jour notre version d’Elasticsearch pour bénéficier des dernières améliorations.\nNous étions alors sur la version 1.7, et souhaitions passer en version 2.0.\nAprès plusieurs mois d’efforts pour effectuer cette migration sans interruption de service ni gel technique, nous voici en version… 5.2!\nVoici le récit de cette grande migration, et ce que l’on a appris tout au long de ce périple.\n\n\n\nLa théorie\n\nIl n’y a pas de méthode magique pour changer de cluster sans coupure, la stratégie adoptée est assez classique:\n\n\n  dupliquer les écritures sur le nouveau cluster\n  basculer les lectures sur le nouveau cluster\n  arrêter les écritures sur l’ancien cluster\n\n\nIl n’est pas nécessaire d’enchaîner toutes les étapes dans la même journée, cela présente donc l’avantage de pouvoir étaler les déploiements dans le temps en fonction des disponibilités,\nainsi que de surveiller attentivement le monitoring pendant quelques jours pour vérifier que l’infrastructure supporte bien les changements apportés.\n\nÉcritures en Y\n\nNous utilisons des workers Php pour détecter les changements dans notre BDD,\nsuite à quoi un message est publié dans une file d’attente pour être traité par un autre worker qui se chargera de synchroniser les entités entre MySQL et Elasticsearch.\nIl était primordial que le cluster de production ne soit pas impacté par les éventuelles erreurs rencontrées sur le nouveau cluster.\nUne de nos premières intentions était de publier le message de mise à jour dans une deuxième file d’attente, consommée par des workers dédiés eux aussi au nouveau cluster.\n\n\nLe gros inconvénient est que cela impliquait de doubler toutes les lectures sur la BDD, toutes les requêtes devaient être executées une fois par cluster,\ncela risquait donc d’impacter d’autres services.\nNous sommes donc partis sur une solution purement logicielle, puisque pour chaque entité mise à jour ce sont les workers qui les envoient sur chaque cluster.\n\n\nIl est par contre nécessaire de gérer correctement les erreurs, que faire si une erreur intervient sur un cluster mais pas l’autre?\nSi le nouveau cluster devient instable et que l’on renvoie les messages systématiquement dans la file d’attente, on risque d’accentuer inutilement la charge en écriture sur le cluster stable en production.\nNotre compromis est de définir comme master le cluster de production (celui où les données sont lues), et seules ses erreurs provoquent la génération d’un nouveau message.\nLes erreurs sur le cluster slave sont monitorées, mais ne génèrent pas de nouveaux messages dans la file d’attente.\nEffectivement, puisque chaque soir nous resynchronisons toutes les données entre MySql et Elasticsearch, on peut se permettre d’avoir des données moins fraiches sur le cluster slave le temps d’une journée.\n\nInitialement, nous pensions que ce système master/slave serait temporaire, mais très rapidement nous avons pérennisé ces développements, cela nous permettait de tester facilement différents clusters,\nou encore de vérifier que les données étaient bien indexées de la même manière, faire des rollbacks en urgence…\n\nUne dernière difficulté était de faire cohabiter deux version différentes du Sdk Elasticsearch Php dans le même projet.\nIl y a effectivement une incompatibilité entre les versions 2.* et 5.*, et nous n’avons pas eu d’autre choix que de cloner la librairie concernée et de changer tous les namespaces pour éviter les conflits de noms.\nMalgré tout, ce ne sont pas les écritures dans Elasticsearch qui nous ont posé le plus de problèmes.\n\nMigration des requêtes\n\nIl y a eu de nombreux changements apportés entre la version 1.7 et 5.0 et souvent pour le mieux.\nLes différentes évolutions de syntaxes ont généralement vite été faites, car nous avions pris soin d’encapsuler la construction des requêtes via quelques fonctions helper (une sorte de query builder).\nIl nous a donc suffit de changer ces quelques fonctions pour traduire les anciennes requêtes vers la nouvelle syntaxe.\n\nUne erreur classique que nous faisions en 1.7 était de se contenter d’un mapping par défaut, qui avait le mérite de fonctionner sans efforts avec nos requêtes et nos données.\nLors du passage à la version 5.0, Elasticsearch a commencé à refuser certaines de nos requêtes car elles ne pouvaient pas être performantes.\nIl fallait choisir, soit activer explicitement des options de mapping en faisant un compromis sur les performances générales,\nsoit affiner le mapping pour qu’il soit plus adapté à la nature de nos requêtes.\nIl s’agit d’un bon exemple de Leaky Abstraction,\non a beau utiliser des outils pour s’abstraire de la façon dont sont stockées les données, nous sommes toujours obligés de comprendre ce qu’il se passe à l’intérieur pour en tirer les meilleures performances.\nHeureusement pour nous notre mapping était assez trivial à changer, car nous n’utilisons pas Elasticsearch pour faire de la recherche full-text\nmais seulement pour des recherches exactes sur des identifiants, des codes… Il nous a généralement suffit d’utiliser le nouveau type keyword\npour que nos requêtes puissent être acceptées.\n\nPour s’assurer du bon fonctionnement des APIs suite à ces nombreux changements, nous avons investi du temps pour écrire des tests fonctionnels de bout en bout,\npour vérifier que les résultats restaient inchangés malgré le changement de version de cluster.\nBien sûr, même si en local nos tests étaient au vert, des erreurs pouvaient apparaître en production.\nLe scénario était alors simple, faire un rollback, ajouter les tests correspondants aux nouvelles erreurs detectées, les faire passer en local, puis recommencer !\n\nCe qui nous a peut-être le plus éprouvé dans cette migration, c’est une regression introduite dans la version 5.2\nqui avait pour conséquence de changer certains de nos tableaux vides en valeur null.\nIl nous a fallut quasiment repasser sur chaque requête pour retransformer ces valeurs en quelque chose de cohérent. \nQuand on avait de la chance, ce bug faisait échouer nos tests, mais il est malheureusement arrivé que ce soient les parseurs json des applications 6play de production qui en fassent les frais, avec différents plantages à la clé…\n\nConclusions\n\nCette migration fut longue et parfois douloureuse, heureusement les résultats sont maintenant au rendez-vous!\n\n\nDe plus, cela nous a donné l’occasion d’investir un peu de temps pour améliorer nos tests fonctionnels, et pour développer un système robuste pour la réplication des données sur plusieurs clusters Elasticsearch.\nOn peut néanmoins se poser des questions sur la stratégie très offensive de changement de version de la part de Elasticsearch,\nautant de releases avec autant de changements en si peu de temps, il faut être capable de suivre!\nPendant que nous finalisions notre production sur la version 5.2, la 5.3 a eu le temps de sortir, et la 6.0 est apparue en beta.\nNous allons essayer de profiter un peu de ce nouveau cluster avant de poursuivre vers une nouvelle grande migration :)\n"
} ,
  
  {
    "title"    : "Last night isomorphic JS saved our life!",
    "category" : "",
    "tags"     : " SPA, SSR, isomorphic, javascript, node.js, high availability",
    "url"      : "/2017/05/17/spa-mode-isomorphism-js.html",
    "date"     : "May 17, 2017",
    "excerpt"  : "For more than a year and a half, we use Node.js and React together to make the best app possible for our users. These 2 technologies are complementary to write only once code executed on the server and the client side: that’s the isomorphic way! T...",
  "content"  : "For more than a year and a half, we use Node.js and React together to make the best app possible for our users. These 2 technologies are complementary to write only once code executed on the server and the client side: that’s the isomorphic way! This approach helped us to develop a reliable app with a fast first render and SEO friendly.\n\nSSR caching\n\nHere is the architecture we use for the 6play web app.\n\n\n\nYou can see that Node.js server responses are cached with Varnish. Indeed, React is not efficient with server side rendering because it just has not been designed for that. The React renderToString method blocks the event loop. Consequently the server can not process an acceptable responses rate for a service like 6play that can reach a lot of requests per second without cache. Particularly when the European Football Championship final bring together France and Portugal and is live or when the last episode of « Les Marseillais », one of the teenagers favorite programs, has just been released on the platform. So caching server responses, with a quite low caching time, is required for our application health!\n\nSPA mode\n\nIsomorphism enables search engines to parse our website without executing any line of JavaScript, only using the server side rendering. We thought that the opposite could be useful too. Imagine our Node servers are down, for various reasons. Our Varnish servers continue to deliver the application pages but only during the cache time. After, the user would get an error… or not!\n\nIn this case, we would switch to a Nginx server that simply delivers a blank page with the client JavaScript code. The server was responsible for the app state initialization before, the user browser has to do so now. Then it can render the page: our application becomes a simple SPA. And this is almost imperceptible for the user, the first render is just a little longer. This way secures the availability of our service.\n\n\n\nThe Varnish servers check the status of the Node ones via a specific route. When every instance is down, they route all requests to a static HTML file on the Nginx server.\n\nReally useful ?\n\nYes! It is not used until it is! A few months ago we went through a memory leak. Consequences? After some time, we saw an increase in CPU usage, then the servers fell down and SPA mode was enabled. We didn’t notice the memory leak immediately because we often deploy new versions of our app and it resets the memory. When we detected the problem, it was too late to rollback because the incriminated version was probably weeks or months old.\n\nYou certainly know how difficult it is to find the code responsible for a memory leak in Node.js. It is often not a matter of hours but of days or weeks. With our SPA mode, we could debug our code with serenity. When the Node servers were down, the SPA mode took the reins. Then we simply restart the server to restore the nominal state when we were alerted (sometimes immediately, sometimes several hours after because it happened in the night). This situation went on some weeks. And we finally fixed the memory leak. No user has been affected. For us, this SPA mode is a significant safety for the high availability of our app.\n"
} ,
  
  {
    "title"    : "Symfony Live Paris 2017",
    "category" : "",
    "tags"     : " symfony, symfony live",
    "url"      : "/2017/04/03/sf-live-2017.html",
    "date"     : "April 3, 2017",
    "excerpt"  : "In March we attended Symfony Live Paris 2017, and it was very interesting.\nHere are some special feedbacks about some of our favorite talks.\n\n\nVarnish tags and invalidation\nSpeaker : Jérémy Derussé\nDescription\nJérémy presented us how to host appli...",
  "content"  : "In March we attended Symfony Live Paris 2017, and it was very interesting.\nHere are some special feedbacks about some of our favorite talks.\n\n\nVarnish tags and invalidation\nSpeaker : Jérémy Derussé\nDescription\nJérémy presented us how to host applications on an production environment on a Raspberry PI using varnish cache.\nToday, the internet traffic and the number of unique visitors increase every days.\nPeople imagines that we need strong hardware servers to be able to respond to this traffic.\n\nThe interesting part of this presentation was the way of how to better use the cache in front of our applications.\nTo be able to minimize the number of requests to the server by managing the cache duration\nand being able to invalidate it on demand when a resource expire.\n\nHow to tag and segment the cache\nBy tagging every resources by a unique id (entityName_id by example), you’ll be able to invalidate later the cache linked to it.\nYou can simplify and automate this tagging by using event dispatcher and listen all entities creations / modifications. With\nthis solution you also centralize your code logic.\n\nCache tagging also helps you with caching / invalidating many-to-many relationships between content items.\n\nHow to invalidate the cache\nJérémy showed us a very interesting interface, displaying the entity content, automatically refreshed when the backend needs to be called because of a cache invalidation.\n\nOnce a resource is modified and needs to be uncached, your backend needs to call the Varnish to purge the tag linked to\nresources. Once again the FOS HttpCache bundle does it very well.\n\nAdvantages / drawbacks\nCaching everything is fine, but it has limitations :\nWorks well when :\n\n  You have mostly read access\n  You have more cache hit than miss\n  Your application is able to communicate with your cache servers\n\n\nCan be difficult when :\n\n  Operations are not atomic\n  It’s complexifying / slowing backend writes\n\n\nHosting on a Raspberry PI\nJérémy finally showed us an impressive demonstration of an application running on Raspberry PI.\nThe configuration :\n\n  Docker\n    \n      MySQL\n      NGINX\n      PHP7-FPM\n      Varnish\n    \n  \n\n\nThe results :\n\n  Number of calls with no caching handling : around 8 / secs\n  Number of calls with varnish cache response : around 900 / secs\n    \n      impressive !\n    \n  \n\n\nResources\nYou can manipulate cache by using the FOSHttpCacheBundle : https://github.com/FriendsOfSymfony/FOSHttpCacheBundle\n\nLink to the speaker presentation : https://www.slideshare.net/JrmyDeruss/grce-aux-tags-varnish-jai-switch-ma-prod-sur-raspberry-pi\n\nEverything a dev should know about Unicode\nSpeaker : Nicolas Grekas\nDescription\n\nNicolas Grekas made a talk about Unicode, from its origin to the latest advanced uses, and more precisely in the PHP ecosystem. He explained how utf8/16 works and the complexity of language management, especially the folding, graphs clusters, modifiers etc.\n\nPHP doesn’t natively handle the unicode #RIPphp6, so it is important to understand the specificities of utf8 in order to avoid some traps, especially concerning string length calculation, comparisons and insertions in database.\n\nIn order to manage the unicode, you must use the php functions of mbstring, iconv, graph and inttl. The “u” modifier allows the utf8 to be processed correctly for regular expressions. For MySQL, utf8_unicode_ci handles ligatures whereas utf8_general_ci does not handle them - but is therefore faster.\n\nConcerning security and avoiding typosquatting, there is a list of confusable characters for filtering: https://unicode.org/cldr/utility/confusables.jsp?a=6play&amp;amp;r=None\n\nThis presentation was very interesting and very useful. Thank you Nicolas ;-)\n\nResources\nLink to the speaker presentation : https://speakerdeck.com/nicolasgrekas/tout-ce-quun-dev-devrait-savoir-a-propos-dunicode\n\nPerformances optimisation with Php7\nSpeaker: Julien Pauli\nDescription\n\nAs usual, it was a real pleasure to listen Julien Pauli speaking about what’s under the hood of Php7.\nEverybody knows there is an actual performances gap between version 5 and 7, but this talk gave some clues to understand the technical reasons behind these major improvements.\n\nFirst, Php compiler has been totally rewritten (yes, Php is compiled in opcode and cached in opcache).\nThanks to an AST, the compiler really understands the analyzed code,\nand lot of optimizations can be done at compilation time instead of runtime.\nFor example, all constant expressions (like $a = 1024 * 2048) are now computed once for all.\nOf course, this compilation pass is now longer but it is exactly the reason why opcache\nhas been made, and why you should warm up your opcache during your scripts’ deployment.\nJulien also introduced the parameter opcache.interned-strings-buffer,\nused to configure string interning in Php.\nHe advised to increase its default value (or at least to check relevance),\nbecause even a minimal Symfony application contains a lot of huge DocBlocks (and then huge strings) so the default\nvalue could be underestimated in some cases. Ready to benchmark! :)\n\nWe had also some interesting information about packed arrays optimisations,\nand some string tips (&quot;$a - $b&quot; is better than $a . &#39; - &#39; . $b for example).\nThen he gave a very good overview of all the efforts done on internal Php structures in order to optimize memory access,\nCPU cycles… an impressive work!\n\nTo finish, we have now an (very) approximated date for Php8 release!! Not before 2020… be patient :)\n\nResources\n\nLink to the speaker presentation: https://www.slideshare.net/jpauli/symfony-live-2017php7performances\n\nDo not hesitate to visit Julien’s blog: https://jpauli.github.io/\n\nAnd many more\n\nWe also liked :\n\n\n  Blog posts about Symfony 4 here : https://fabien.potencier.org/\n  Micro-Services Symfony at Meetic: feedback after 2 years of redesign! https://fr.slideshare.net/meeticTech/php-symfony-microservices-migration-meetictech\n  Introduction to CQRS and Event Sourcing: https://www.slideshare.net/samuelroze/introduction-to-cqrs-and-event-sourcing-74061563\n  Web security: and if we continued to break everything? https://github.com/ninsuo/slides\n\n\nYan can find most of the slides here: https://joind.in/event/symfonylive-paris-2017/schedule/list\n"
} ,
  
  {
    "title"    : "Format all the things",
    "category" : "",
    "tags"     : " prettier, javascript, react, lint, eslint, 6play",
    "url"      : "/2017/03/30/prettier.html",
    "date"     : "March 30, 2017",
    "excerpt"  : "Prettier\n\nPrettier is a brand new javascript library. Its simple goal is to reformat your code. For instance, when I write this:\n\n\nand run prettier on it, I’ll ultimately get this:\n\n\nYou can try it for yourself at prettier’s website\n\nLooks awesome...",
  "content"  : "Prettier\n\nPrettier is a brand new javascript library. Its simple goal is to reformat your code. For instance, when I write this:\n\n\nand run prettier on it, I’ll ultimately get this:\n\n\nYou can try it for yourself at prettier’s website\n\nLooks awesome doesn’t it? You don’t need to be a stylish developer anymore! But let’s be honest: not all developers are enthusiastic about this new tool. There are two kinds of developers, those who like when a program helps them to format their code, and those who don’t.\n\nThere are some benefits: using prettier you won’t waste your time reformatting your code because your destructuring expression overreaches your config’s character cap. You will no longer have conflicts because your colleagues changed indentation on the code you’re currently working on.\n\nBut it also comes with some tradeoffs. The first one is your coding style. We generally add line breaks between chained calls on an API (example here). Prettier will make this fit on one line if it can, and your code will lose some clarity. The same thing goes with the parentheses you add in complex boolean operations (example here). You will have to remember operator priorities.\n\nFor a relatively big project like ours, we prefer adding some consistency to our code in order to avoid wasting our time on solving conflicts, at the expense of losing a little bit of readability and style.\n\nHow to configure it?\n\nWe already have eslint setup in our project with some rule tweaks. So for us, Prettier had to interface smoothly with eslint. Fortunately it comes with a bunch of useful plugins.\n\n\n  eslint-config-prettier: this library disables all eslint rules that conflict with the Prettier formatting. Without it we would need to turn off those rules manually.\n  eslint-plugin-prettier: this library includes Prettier proper formatting as an eslint rule. So if our code is not well formatted, eslint will throw errors. This is the most important plugin for us, as it makes the linting fail if the code is not well formatted. Formatting is now mandatory!\n  prettier-eslint (prettier-eslint-cli): this plugin just runs prettier followed by eslint --fix command. The CLI version allows us to use it from the command line.\n\n\nWith these libraries, we can format all our code base and apply Prettier and eslint rules.\n\nMigration\n\nFirst we had to format the whole repository. This results in a pretty big PR, 670 modified files, +11700 -11113 changes… The implication is obvious: if you choose to use Prettier on your project, it had better be set up from the start.\n\nAnyway, once this huge PR was merged, we had to rebase other PRs. You can see it coming: if your rewrite most of your code and rebase other changes on it, there is nearly no way you can avoid conflicts.\nBut in reality it’s (almost) easier than it seems. Since all modifications were generated by Prettier, we can simply discard them and regenerate them after the rebase.\n\nSo, the first thing to do was to rebase on the parent of the format commit in order to resolve all conflicts that were not related to the formatting. To put it differently, we are sure that in next rebase conflicts will only be related to Prettier’s changes.\n\ngit rebase 0404b07~\n# where 0404b07 is the git hash of the format commit parent\n\nAfter that, we rebased our branch on the “prettier” commit, and we asked git to automatically keep the conflicting changes from the branch and to discard those from Prettier.\n\ngit rebase 0404b07 -s recursive -X theirs\n\nThen we just needed to re-run Prettier to reformat the rebased code. After this, branches were well formatted and could get back to their normal life-cycle.\n\nHow does it work on daily basis?\n\nFirst, adding the following scripts in the package.json file enables us to use prettier as a yarn (or npm) command.\n\n\n\nThe first line is used to format files provided as parameters and is used in a git pre-commit hook. The second line was there to format the whole codebase and should not be used anymore. This command takes around 1 minute to execute which is a little too long to be used in the development process. It’s more interesting to plug Prettier in our IDE and only format modified files.\n\nEven though we now enforce a machine-generated code style, everyone is still free to use their favorite IDE with any formatting and syntax settings they like.\nThose using atom or sublime-text can use plugins for the save action (atom plugin here with “ESLint Integration” checkbox and sublime-text plugin here). Every saved file will be automatically formatted by Prettier. This is clearly the most comfortable solution.\n\nThose used to applying the format action in Webstorm will have to configure an external tool to do it. Here is a good article to help you setup an external tool if you are interested in this solution.\n\nFinally we wrote a pre-commit hook and added it to our documentation. It automatically runs prettier on all added files from our javascript sources. lint-stage does the same, but we don’t want to force the whole team to use it. It’s clearly not necessary to run it twice, for those who have a save action which already runs Prettier.\n\nHere’s an example of our pre-commit hook:\n\ngit diff --name-only HEAD | grep -E &quot;src/.*\\.js.?$&quot; | xargs yarn format\n\nIn conclusion\nPrettier is a new tool to add to your chain. Its role is to format the code for you in a very strict way. Thanks to a bunch of plugins that complement it, it also plays nicely with and applies eslint rules. Like we said, there are few sacrifices to make in terms of clarity, but it allows you to stop taking care of things that add no real value to the code you write. It also helps you to reduce meaningless conflicts and debates on “how we should write this”. We plan to use it on all our javascript repositories, for greater consistency and good.\n"
} ,
  
  {
    "title"    : "Nouveau socle pour une nouvelle vie",
    "category" : "",
    "tags"     : " conference, confoo, PHP, Symfony",
    "url"      : "/2017/03/10/confoo-2017-nouveau-socle-nouvelle-vie.html",
    "date"     : "March 10, 2017",
    "excerpt"  : "A travers cette conférence, je me propose de vous tracer l’histoire de la migration de 6play (système de télévision de rattrapage du groupe M6, premier groupe de télévision privé français) d’une application monolithique vers un univers de micro-se...",
  "content"  : "A travers cette conférence, je me propose de vous tracer l’histoire de la migration de 6play (système de télévision de rattrapage du groupe M6, premier groupe de télévision privé français) d’une application monolithique vers un univers de micro-service, des avantages en terme de maintenance, d’évolution, de montée en charge, mais également des différents écueils rencontrés lors de ce changement de paradigme : caching, logging, complexité globale.\n"
} ,
  
  {
    "title"    : "L&#39;équipe Player de 6play.fr au Paris Video Tech",
    "category" : "",
    "tags"     : " video, ott, react, dash, hls, mse, cmaf, 6play, html5",
    "url"      : "/2017/02/20/retour-de-paris-video-tech.html",
    "date"     : "February 20, 2017",
    "excerpt"  : "\n\nPrésentation du Paris Video Tech\nMercredi 1er février avait lieu la troisième édition du Paris Video Tech, un meetup orienté autour de tous les sujets techniques de la vidéo : players HTML5, formats, encodage, distribution, publicité, …\n\nL’équip...",
  "content"  : "\n\nPrésentation du Paris Video Tech\nMercredi 1er février avait lieu la troisième édition du Paris Video Tech, un meetup orienté autour de tous les sujets techniques de la vidéo : players HTML5, formats, encodage, distribution, publicité, …\n\nL’équipe Player de M6 Web (Frédéric Vieudrin, Nicolas Afresne, Malik Baba Aïssa et Vincent Valot) présentait le nouveau player HTML5 de 6play.fr : un player MSE multi-formats, développé entièrement en React, le framework JS qu’on ne présente plus et qui fait le succès du nouveau 6play.fr depuis 2015.\n\nLa rencontre se déroulait dans les locaux de France Télévision à Paris et proposait trois talks :\n\n\n  6play : un player MSE en React par l’équipe Player de M6Web\n  CMAF Démystifié par Cyril Concolato\n  Retour d’Expérience de Roland Garros 360 par l’équipe innovation de France Télévision\n\n\n\n\n6play : un player MSE en React\n\nPrésentation de 6play.fr\nDans la première partie, nous avons présenté le contexte technique de 6play.fr, autour de React, ainsi que les chiffres clés du site.\n\nAprès un rappel de l’historique des players du site de Replay des chaînes du Groupe M6, nous avons présenté les enjeux de la refonte de notre précédent player et évoqué nos contraintes.\n\n\n\nArchitecture du player en React / Redux\n\n\n\n\nEn octobre 2015 sortait le nouveau 6play.fr, une Single Page App développée en React-Redux et Isomorphique. Le succès de cette refonte nous a poussé à étudier le refonte du player 6play sur la même stack technique, historiquement en Video.js.\n\nEn complément de l’approche composant proposée par React, Redux nous a apporté la solution à la gestion de l’état du player dans le temps. En effet, son fonctionnement par événements et actions était parfaitement adapté aux événements de la balise &amp;lt;video&amp;gt;.\n\nMedia Engines\nInspiré du système multi-techs de Video.js, nous avons développé notre propre système de Bridge pilotant les différents SDK Video du marché : hls.js, dash.js, Adobe Primetime Browser TVSDK, et HTML5.\n\nTous les Bridges communiquent ainsi de la même manière avec notre player React au travers des MediaEvents HTML5.\n\nIntégration continue, tests, outils et méthodes de travail\n\nLes tests automatisés font partie intégrante des développements chez M6Web. Tester unitairement nous permet de valider nos classes et méthodes, tester fonctionnellement assure le bon fonctionnement du player sur plusieurs navigateurs et évite les régressions.\n\nNous utilisons les Webhooks de Github pour executer nos tests, déployer un environnement de recette dédié et nous notifier du statut de la Pull Request dans Slack.\n\nLes slides de notre présentation\n\nRevoir l’événement en replay\n\n\n\n"
} ,
  
  {
    "title"    : "Get your brownfield React Native app built on demand",
    "category" : "",
    "tags"     : " mobile, github, ci, react-native",
    "url"      : "/2017/01/31/get-brownfield-react-native-app-built-on-demand.html",
    "date"     : "January 31, 2017",
    "excerpt"  : "As you may know, at M6Web we decided to embrace React Native a few months ago.\nIt’s a really exciting piece of software that adds a lot of value in the mobile development ecosystem.\n\nWe already use it for a side project on a standalone app (not pu...",
  "content"  : "As you may know, at M6Web we decided to embrace React Native a few months ago.\nIt’s a really exciting piece of software that adds a lot of value in the mobile development ecosystem.\n\nWe already use it for a side project on a standalone app (not public yet, stay tuned!) to record table soccer games, that’s why, we (mostly @ncuillery 😏) decided to improve the upgrade process for apps made with the embedded generator. See Nicolas’ blog post on it: Easier Upgrades with React Native.\n\nAs a result, we wanted to start using React Native for our most popular app: 6Play (6play iOS, 6play Android).\nSo they would become what Leland Richardson from Airbnb calls “brownfield” apps.\n6play is the catchup TV platform for the French TV group M6. It offers live-streaming and full episodes for web, mobile and set-top box. Since the apps launched in 2016, there have been over 1.5 billion videos streamed. Our iOS (mostly Swift) and Android native applications, both important parts of the 6play platform, were exclusively developed externally until now.\n\nWe wanted to use React Native to develop this project in-house and to take advantage of the benefits this hybrid technology could bring into our native apps. Here are just some of the benefits we found when using React Native:\n\n\n  JavaScript development for mobile. We have a lot of awesome JavaScript developers internally who develop the 6play website using React. We love React &amp;amp; Redux and want to mutualize this piece of technology we use on most of the frontends of the 6play platform.\n  Hot fixing with CodePush. For our mobile apps, we want to accomplish the same continuous delivery process we have for the website. CodePush helps us to keep the same flexibility by allowing us to make deployments on a weekly or even daily basis.\n  Knowledge sharing. We would like to be closer to the external development of our mobile apps, which was difficult without native knowledge and without any Android or Swift internal developers. React Native allows us to be part of that, we started working closely with the native team, sharing all developments between the two teams and bringing the best of both worlds (native and web) into the same project.\n  Code Sharing. We also want to share major parts of the mobile code base between apps (Android &amp;amp; iOS). Today, the code bases for each app are completely separate and are managed by two separate teams. With React Native, we could have one common code base while being able to implement specificities for a particular platform if needed. We have also imagined some ways to share code with the 6play website.\n\n\nAs we mentioned in a previous blog post, we use Github pull requests extensively in our development process, especially for testing (automatically and manually) each new commits before merging them into the master branch.\n\nIn the past, we tried to use Appetize to preview  our apps in the browser. It was a first shot, but the functionality was quite limited: animations felt janky, some features wouldn’t work (in-app purchase, video with DRM, …), user identification was painful. We needed a better solution, and as a result we decided to rethink the way we develop the 6play apps.\n\nFor the second iteration of our development process, we had a few simple requirements:\n\n\n  Test each pull request in conditions as close to the reality as possible,\n  Use the same testing workflow for both iOS and Android apps.\n\n\nThis post outlines our new mobile development process for the 6play apps. We’ll walk through how we manage the environment of a brownfield React Native app, our Git repository structure, our build and release workflows, and how we’ve created a CI environment that mirrors our production environment.\n\nMono Repository / Multi Repositories?\n\nThe first thing we had to do, was to decide how we wanted to organize our Git repositories.\n\nFor this, we looked into how the AirBnb team work with their brownfield app.\n\nWe soon realized we had two options here:\n\nMulti-repositories:\n\n\n  the iOS one\n  the Android one\n  and one for React Native code\n\n\nMono-repository:\n\n\n  One giant repository that has iOS, Android, and React Native folders inside.\n\n\nLet’s take a look at the pros and cons of both solutions.\n\nThe Mono Repository\n\n├── app-6play/\n│   ├── app-android/\n│   ├── app-ios/\n│   ├── react-native-views/\n\n\nAt first glance, this solution seems like the Holy Grail:\n\n\n  (+) Everything is in the same place\n  (+) If a modification needs both native &amp;amp; React Native developments, changes can be contained in a unique pull request.\n  (-) Code, Documentation, Setup, are more difficult at the beginning (For example, how can we keep Git history of each existing repository?).\n  (-) For our workflow, we need to have everyone working the same way, with the same git workflow, and the same review process. Remember that our native team is an external team (in Belgium), the Android &amp;amp; iOS teams are two different teams (located in the same place) and the React Native one is an internal team (in France). If we succeed, we’ll have synchronous development between teams, this is a really positive point, but it may be difficult to reach.\n  (-) Android, iOS and Javascript CI environments are very different (different tools, different needs), so it is really complex to setup.\n\n\nUltimately, the initial cost of setup and maintenance outweighed the benefits of a mono-repository.\n\nThe Multi Repositories\n\n├── app-android/\n├── app-ios/\n├── react-native-views/\n\n\n\n  (+) Each team could have its own Git workflow, branching model, review process,\n  (+) Each platform has its own CI, code conventions,\n  (-) Building the native apps including the React Native bundle is complicated,\n  (-) Three pull requests (one on each repository) are needed if the functionality includes a native bridge and React Native development.\n\n\nNeither approach was perfect. So we decided to choose the safest one, and create multiple repositories. Also, this choice doesn’t forbid any change of direction toward the mono repository in the future… The reverse seems much more complicated.\n\nDevelopment workflow\n\nEach native developer is now forced to have the react-native-views to be able to work on the native app.\nYou need to know that the native apps need node_modules dependencies of the React Native project, because they also contain the native part of React Native, and maybe some native code for React Native 3rd party you use.\nSo, we will need to clone the native app and the React Native repository.\n\nFor Android\n\ngit clone app-android\ngit clone react-native-views\n\nSo we will have two sibling folders:\n\n├── app-android/\n├── react-native-views/\n\n\nWe decided to use symlink to have a cleaner structure (and that will make the CI configuration easy later, see Continuous Integration), so the setup for the Android project will look like this:\n\ncd app-android\nln -s ../react-native-views ./react-native-views\ncd ../react-native-views\nnpm install\n\n├── app-android/\n│   ├── react-native-views -&amp;gt; ../react-native-views\n├── react-native-views/\n│   ├── node_modules/\n│   ├── package.json\n\n\nFor iOS\n\nSimilar steps to the Android process, but it seems that Xcode has difficulty following package with a symlink … so we have to be a little smarter:\n\ngit clone app-ios\ngit clone react-native-views\n\ncd app-ios\nmkdir -p react-native-views/node_modules\ncd ../react-native-views\nln -s ../app-ios/react-native-views/node_modules ./node_modules\nnpm install\n\nWith this method, the node_modules files will be written in the symlink. So those files will be located in the source of the symlink, the app-ios/react-native-views/node_modules directory (This is pretty twisted, we had to admit).\n\n├── app-ios/\n│   ├── react-native-views/\n│   │   ├── node_modules/\n├── react-native-views/\n│   ├── node_modules -&amp;gt; ../app-ios/react-native-views/node_modules\n│   ├── package.json\n\n\nReact Native\n\nNow we can choose: JavaScript developers are able to develop on any native app with the React Native packager (npm start in the react-native-views directory) and native developers can develop either with the packager started or with a pre-built React Native bundle (if their developments don’t concern React Native) by switching a Scheme (iOS) or a flavour (Android).\n\nContinuous integration\n\nThe next step was to find a way to improve the mobile development workflow.\nDuring our research, we found a SAAS tool named buddybuild that’s able to build the iOS &amp;amp; Android apps on each pull request. The setup for the native apps (before the React Native integration) or the React Native side project was really straightforward. It just magically works!\n\nWith the 3 Git repositories of our brownfield apps, it’s a bit more complicated than that. For this, buddybuild provides two useful hooks during the CI process. We just have to add a shell file in the repository:\n\n\n  buddybuild_postclone.sh: This is the hook that happens just after the cloning of the current repository by buddybuild\n  buddybuild_prebuild.sh: This hook is called after postclone and after buddybuild gets all dependencies (npm, Pod, Gradle …), but just before the build starts\n\n\nTo allow our Product Owners to test the app’s functionality, whether it’s related to React Native or not, we’d need:\n\n\n  An iOS build on each pull request on the iOS repository\n  An Android build on each pull request on the Android repository\n  An iOS &amp;amp; Android build on each pull request on the React Native repository\n\n\nTo meet the specific needs of our app development, we required:\n\n\n  For iOS &amp;amp; Android, we need a way to include the React Native code which lies in another repository.\n  For the React Native repository, we need a way to build the iOS &amp;amp; Android apps which lie in other repositories as well, and including the React Native code in it.\n  Our iOS and Android apps up-to-date with both the master branch of the native app, and the master branch of the React Native repository.\n  If a feature needs modifications on both the native code and the React Native code (multiple pull requests, one on each concerned repositories), we want an app synchronized with all repositories.\n\n\nSo let’s dig in these 4 points.\n\nBuild the iOS &amp;amp; Android apps including the React Native bundle\n\nThe key here is to clone the React Native repository in the postclone buddybuild hook and reproduce the directory structure we have in development mode.\n\nfor iOS\n\nbuddybuild_postclone.sh:\n\ngit clone react-native-views\n\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n# Make Xcode able to access to the node dependencies\nln -s react-native-views/node_modules node_modules\n\nbuddybuild_prebuild.sh:\n\n# export React Native bundle:\nnode_modules/.bin/react-native bundle --platform ios --entry-file index.ios.js --bundle-output ../&amp;lt;appFolder&amp;gt;/main.ios.jsbundle --dev false\n\n├── buddybuild workspace/ (app-ios inside)\n│   ├── react-native-views/\n│   │   ├── package.json\n│   │   ├── node_modules/\n│   ├── package.json -&amp;gt; react-native-views/package.json\n│   ├── node_modules -&amp;gt; react-native-views/node_modules\n\n\nfor Android\n\nbuddybuild_postclone.sh:\n\ngit clone react-native-views\n\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n# When buddybuild will run `npm install`, the node dependencies will be at the right place\nln -s react-native-views/node_modules node_modules\n\nbuddybuild_prebuild.sh:\n\n# export React Native bundle:\nnode_modules/.bin/react-native bundle --platform android --entry-file index.android.js --bundle-output ../&amp;lt;appFolder&amp;gt;/main.android.jsbundle --dev false\n\n├── buddybuild workspace/ (app-android inside)\n│   ├── react-native-views/\n│   │   ├── package.json\n│   │   ├── node_modules/\n│   ├── package.json -&amp;gt; react-native-views/package.json\n│   ├── node_modules -&amp;gt; react-native-views/node_modules\n\n\nThe only thing you have to do in the buddybuild dashboard is to create the app for each platform and activate the build on pull request only (see screenshot below). Buddybuild will automatically trigger an iOS &amp;amp; Android build on each pull request for the native repositories.\n\n\n\nBuild the iOS &amp;amp; Android apps on each pull request from the React Native repository\n\nNow, we’d like to easily test each react-native-views pull request on both iOS and Android apps.\n\nFor that purpose, we used the buddybuild hook again. Here is the buddybuild_postclone.sh:\n\n# Create a react-native-views folder\nmkdir react-native-views\n# Move everything in it\nmv * react-native-views\n\n# The postclone hook is ran by buddybuild for both iOS and Android builds. We distinguish the platform here, thanks to the env variable BUDDYBUILD_APP_ID (set by buddybuild)..\nif [ &quot;$BUDDYBUILD_APP_ID&quot; = &quot;&amp;lt;buddybuildAndroidAppID&amp;gt;&quot; ]; then\ngit clone app-android\ncd app-android\nelse\ngit clone app-ios\ncd app-ios\nfi\n\n# Move the native app to the root of the workspace\nmv * ..\ncd ..\n\n# Create the future node_modules location folder\nmkdir -p react-native-views/node_modules\n# Create the symbolic link for the app to be able to found the node_modules at the good place\nln -s react-native-views/node_modules node_modules\n# Create the symbolic link of the package.json at the root to make buddybuild triggering the `npm install`\nln -s react-native-views/package.json package.json\n\nFor iOS, you’ll have:\n\n├── buddybuild workspace/ (app-ios inside)\n│   ├── react-native-views/\n│   │   ├── package.json\n│   │   ├── node_modules/\n│   ├── package.json -&amp;gt; react-native-views/package.json\n│   ├── node_modules -&amp;gt; react-native-views/node_modules\n\n\nFor Android, you’ll have:\n\n├── buddybuild workspace/ (app-android inside)\n│   ├── react-native-views/\n│   │   ├── package.json\n│   │   ├── node_modules/\n│   ├── package.json -&amp;gt; react-native-views/package.json\n│   ├── node_modules -&amp;gt; react-native-views/node_modules\n\n\nBy doing that, buddybuild will automatically install the npm dependencies, then launch the same prebuild hook as the native repository to build the React Native bundle.\n\nUsing buddybuild, you can create the app for each platform, and trigger new builds only when pull requests are opened, or when commits are added to existing pull requests. Buddybuild also builds both apps when React Native pull requests are opened as well.\n\nWhen master of React Native change, update the master iOS &amp;amp; Android apps\n\nBuddybuild makes it very easy to trigger a build programmatically via the API. We also use Jenkins for unit tests and lint, so we have a job triggered every time a push is made on the master branch of react-native-views. We have reused this job and append the following:\n\n# Our credentials\nACCESS_TOKEN_BB=&amp;lt;AccessToken&amp;gt;\nAPP_ID_BB_IOS=&amp;lt;buddybuildiOSAppID&amp;gt;\nAPP_ID_BB_ANDROID=&amp;lt;buddybuildAndroidAppID&amp;gt;\n\n# Build iOS\ncurl -X POST -H  &#39;Authorization: Bearer &#39;$ACCESS_TOKEN_BB” -d &#39;branch=master’ &#39;https://api.buddybuild.com/v1/apps/&#39;$APP_ID_BB_IOS&#39;/build&#39;\n\n# Build Android\ncurl -X POST -H  &#39;Authorization: Bearer &#39;$ACCESS_TOKEN_BB” -d &#39;branch=master’ &#39;https://api.buddybuild.com/v1/apps/&#39;$APP_ID_BB_ANDROID&#39;/build&#39;\n\nNow, you can activate the master build on the native iOS &amp;amp; Android buddybuild build, and you’ll have those apps up-to-date with the master branch.\n\n\n\nCross platform feature (both native &amp;amp; React Native)\n\nAt this point, this is not enough, because if you develop a feature that needs native and React Native modifications, you will not have the corresponding app before merging everything.\n\nWe have decided here to add a rule: for a “cross platform feature” (like a bridge for a native component for example), we have to define the same name for the branches in each repositories.\n\nA bridge for a native component (the authentication bridge as an example) would have three Git branches with the same name, and three pull requests (one on each repository).\n\nBy following this convention, we only have to checkout that branch when we clone the external repository in our postclone hooks:\n\n{\n  # Detect with the env variable BUDDYBUILD_BRANCH (given by buddybuild) the branch we are on.\n  echo &quot;Git checkout branch: $BUDDYBUILD_BRANCH&quot;\n  git checkout $BUDDYBUILD_BRANCH\n} || {\n  echo &quot;Git default branch: master&quot;\n  git checkout master # if master is the name of your default branch\n}\n\nWe do that branch name checking on the three repositories. This way, the four buddybuild projects (app-ios, app-android, react-native-views-ios, and react-native-views-android) can build native applications with modification on both sides.\n\nConclusion\n\nThanks to React Native and buddybuild, we now have a complete workflow as powerful as we have on the website. Being able to review either React Native or native code, and testing a real app before the code lands on the master branch is a big improvement for code quality and a huge step forward towards more agility.\n\nBig up to Tapptic Team, M6Web React Native team for this work, to the buddybuild support team for the help when needed.\n\nSpecial thanks to Nicolas Cuillery and Alysha for their proofreading!\n"
} ,
  
  {
    "title"    : "Une donnée presque parfaite sur 6play",
    "category" : "",
    "tags"     : " lyon, conference, elasticsearch, video",
    "url"      : "/2016/11/24/une-donnee-presque-parfaite.html",
    "date"     : "November 24, 2016",
    "excerpt"  : "Benoit Viguier, prestataire de la société Elao pour M6Web, a fait un retour d’expérience au Forum PHP de l’AFUP sur l’architecture technique mise en place autour de la mise à  disposition des données nécessaires à 6play.\n\n\n\nLes slides sont égaleme...",
  "content"  : "Benoit Viguier, prestataire de la société Elao pour M6Web, a fait un retour d’expérience au Forum PHP de l’AFUP sur l’architecture technique mise en place autour de la mise à  disposition des données nécessaires à 6play.\n\n\n\nLes slides sont également disponibles en PDF.\n"
} ,
  
  {
    "title"    : "Enquête exclusive au coeur de la technique de 6play. Les slides.",
    "category" : "",
    "tags"     : " conference",
    "url"      : "/2016/11/03/blendwebmix-6play-conference.html",
    "date"     : "November 3, 2016",
    "excerpt"  : "Voici les slides de la conférence “Plus d’un milliard de vidéos vues par an sur 6play - Enquête exclusive au coeur de la technique” que nous avons donné le 2 novembre 2016 lors de la conférence Blend Web Mix à Lyon.\n\nhttps://docs.google.com/presen...",
  "content"  : "Voici les slides de la conférence “Plus d’un milliard de vidéos vues par an sur 6play - Enquête exclusive au coeur de la technique” que nous avons donné le 2 novembre 2016 lors de la conférence Blend Web Mix à Lyon.\n\nhttps://docs.google.com/presentation/d/1BZGvoiubQsIzVjH9Px22wQyYmkboXZjpiubX2UtkMA4/edit?usp=sharing\n\nNous étions ravis d’être sponsor de cet évenement. Merci encore à toute l’organisation.\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity New York Conference 2016",
    "category" : "",
    "tags"     : " conference, velocity, webperf, devops",
    "url"      : "/2016/10/12/velocity-nyc-2016.html",
    "date"     : "October 12, 2016",
    "excerpt"  : "Nous étions cette année à New York, à quelques blocs de Time Square, pour suivre l’édition New Yorkaise de la Velocity Conference 2016.\nC’est une conférence que nous apprécions particulièrement et à laquelle nous nous rendons quasiment chaque anné...",
  "content"  : "Nous étions cette année à New York, à quelques blocs de Time Square, pour suivre l’édition New Yorkaise de la Velocity Conference 2016.\nC’est une conférence que nous apprécions particulièrement et à laquelle nous nous rendons quasiment chaque année, soit dans son édition européenne (Berlin, Londres, Barcelone, et Amsterdam cette année en novembre), soit aux U.S. (précédemment Santa Clara, New York cette année, et San José l’année prochaine).\nC’est l’occasion de suivre une conf de très haute qualité composée de 4 ou 5 tracks en parallèle, dédiée aux problématiques de performance et de scalabilité.\nOn remarque que d’année en année la conférence s’est réorientée autour du mouvement DevOps, alors qu’elle était précédemment beaucoup plus centrée sur la WebPerf (desktop et mobile).\n\nLa conférence commence par l’Ignite (sorte de mini conférence dans la conférence), basée sur un format court (type Lightning Talk) de 5 minutes pour une présentation de 5 slides défilant automatiquement. On retiendra de cette première partie des talks intéressants exposant les tristes chiffres de la diversité dans la tech aux US, mais aussi une conférence très drôle de @beldhalpern de @ThePracticalDev sur des parodies des livres OReilly (voir le O RLY Cover Generator) :\n\nEnjoying the heck out of @ThePracticalDev at Ignite #velocityconf. Lololol! pic.twitter.com/ZlJWoP4cjh&amp;mdash; Bridget Kromhout (@bridgetkromhout) 20 septembre 2016\n\n\nL’ignite s’est fini sur le célèbre Ignite Karaoké où 16 volontaires se sont prêtés au jeu de cet exercice hilarant mais tellement difficile, consistant à improviser une conférence sur le sujet de son choix sur 5 slides inconnues de l’orateur et qui défilent automatiquement au bout de quelques secondes 😃. Ce qu’on fait aussi chez M6Web de temps en temps nommé Karaoké Slideshow et que Kenny avait animé lors d’un Forum PHP (voir la vidéo).\n\n\nCrédit : Flickr\n\nNous avons ensuite suivi deux jours de conférences dont les thèmes majeurs étaient :\n\n\n  Les Service Workers\n  Les microservices\n  Le monitoring\n  HTTP2\n  La sécurité des apps\n  Les détections d’anomalie\n  Les ChatOps\n  Le WebMobile, AMP et les PWA\n\n\nChatOps\n\nUn des sujets assez récurrent, notamment dans la mouvance DevOps est l’utilisation des ChatOps, sujet popularisé par Github (via Hubot).\nCela consiste généralement en un bot ou une IA posée sur un outil de Chat type Slack, Flowdock ou Hipchat, permettant de simplifier la communication entre différentes équipes et les différents outils (ticketing, alerting, monitoring, état d’une machine, etc). Une démo de l’IA de Dynatrace à reconnaissance vocale à été faite, montrant comment par la voix, on pouvait recevoir dans l’outil de Chat les infos sur les incidents de la veille, créer les tickets de support etc. Voir ici. Un peu gadget, mais rigolo.\n\nL’un des points à retenir, c’est que même si ces outils font partie de la « culture » DevOps, ce n’est pas l’ajout d’un de ces outils qui fera apparaître cette culture dans votre entreprise si vous ne l’avez pas.\n\n\n  Tools will not fix a broken culture\n\n\n\nCrédit : Flickr\n\nLe WebMobile, AMP, et les PWA\n\nPlusieurs conférences avaient pour but de comparer ce que l’on pouvait obtenir de nos jours via du WebMobile versus ce que l’on a sur les apps natives. Le fossé s’est énormément rétréci et les WebApps ont désormais accès à la plupart des fonctionnalités présentes côté natif :\n\n\n  Notifications\n  Ajout sur le Home Screen de l’icone\n  Full Screen\n  Orientation\n  Gestion hors ligne\n  …\n\n\nCe qui nous amène aux Progressive Web Apps : PWA\n\nPete Lepage @petele de chez Google nous a notamment présenté des projets open-source de Google pour mettre en place différentes politiques de cache via les « serviceWorkers » (voir https://developers.google.com/web/tools/service-worker-libraries/), ainsi que les futures api : Web Payments, Credential Management …\n\n\n  Les slides.\n  Exemple de la PWA du Washington Post\n\n\nToujours sur la partie mobile, Malte Ubl (@cramforce), core développeur de AMP, nous a présenté le futur de ce protocole de Google pour offrir des pages plus rapides pour la consultation de site média sur mobile.\n\n\n  AMP is a web component library, validator and caching layer for reliably fast web content at scale\n\n\nEn commençant par un bilan d’AMP, 3000 PR + 200 contributeurs (au bout d’un an seulement !), Malte nous a expliqué qu’un site mobile très optimisé pouvait logiquement être plus performant qu’AMP.\n\nArriveront prochainement sur AMP, le support des formulaires, des optimisations avancées d’images via le Google AMP Cache, des Service Workers pour AMP pour ne jamais télécharger AMP dans le « chemin critique » du chargement de la page.\n\nUn petit focus a aussi été fait sur les PWA et AMP avec amp-install-serviceworker qui est un Service Worker permettant d’installer la PWA après chargement de AMP, pour faire une upgrade transparente de AMP vers une PWA (Voir une démo ici choumx.github.io/amp-pwa)\n\n\n  AMP : « Start Fast, Stay Fast »\n\n\nNous avons aussi vu une conférence sur l’optimisation de la consommation des webApps en terme de CPU / temps de réponse, notamment via l’étude des capacités JS de chacun des devices/OS avec le benchmark JetStream Javascript.\n\nOn découvre notamment que l’iPhone 7 a des capacités assez impressionnantes, contrairement à l’iPhone 5C, que le mode « économie d’énergie » ou encore une bonne insolation rendent les devices beaucoup moins performants. D’excellentes slides à voir ici : hearne.me/2hot\n\nWebPerf\n\nCôté WebPerf, peu de grosses nouveautés, on retiendra @nparashuram qui nous a montré comment automatiser le “profiling” des ChromeDevTools dans Node.js via ChromeDriver !\n\nPlus d’infos ici : https://blog.nparashuram.com/2016/09/rise-of-web-workers-nationjs.html\n\nTammy Everts (@tameverts de Soasta) et Pat Meenan (@patmeenan de Google et créateur de WebPageTest) nous ont fait un gros retour basé sur toutes les métriques récoltées par Soasta mPulse (outils de Real User Monitoring en SAAS) afin de déterminer des corrélations entre les temps de chargement et d’autres métriques (taux de rebond, conversion, etc.) grâce à l’application de concept propre au Machine Learning sur une quantité énorme de data. Toujours intéressant.\n\nSlides ici : https://conferences.oreilly.com/velocity/devops-web-performance-ny/public/schedule/detail/51082\n\n\nCrédit : Flickr\n\nCôté Single Page App, le Server Side Rendering est revenu à plusieurs reprises afin d’avoir des SPA performantes dont le premier rendu est généré côté serveur, ce que permet nativement React, et désormais Ember et Angular 2. Voir notre article sur l’isomorphisme.\n\nCoté HTTP, on retiendra Hooman Beheshti qui nous a fait un retour d’expérience sur HTTP2. Après une explication des nouveautés du protocole (binary, single, long-lasting TCP connection, streams encapsulation, frames, bi-directional…), une comparaison avec HTTP 1 nous a été exposée. En conclusion, HTTP2 est complexe et la migration n’est pas une simple modification de paramètre. Bien que cette nouvelle version est légèrement plus rapide, en particulier sur un réseau lent (&amp;lt;1Mbps), le protocole supporte très mal les pertes de paquets ou les fortes congestions à cause de l’unique connexion (TCP slow start). La recommandation est de tester sur chaque site et d’optimiser ses pages selon la version d’HTTP utilisée. Une piste serait HTTP2 over UDP.\n\nLes slides\n\nDevOps\n\nDe nombreuses conférences avaient pour objectif d’aborder les bienfaits du DevOps et plus largement les bonnes pratiques liées au mouvement afin de gagner en qualité et fiabilité.\n\nOn retiendra notamment la conférence de Cornelia Davis (DevOps: Who does what?) explicitant les différents rôles dans un SDLC (Software Development Life-Cycle) et leur répartition en équipe dans l’organisation.\n\nLes rôles dans le SDLC :\n\n\n  Architecte : Ent Archi, Biz Analyst, Portfolio Mgmt\n  SCO : Info sec\n  Infra : Srv Build, Cap Plan, Network, Ops\n  Middleware/AppDev : Middleware Eng, SW Arch, SW Dev, Client SW Dev, Svc Govern\n  Data : Data Arch, DBA\n  Biz : Prod Mgmt\n  Ent Apps : DCTM (Documentum) Eng.\n\n\nLa répartition en équipe proposée :\n\nPlatform (unique / transverse) :\n\n\n  Middleware/AppDev : Middleware Eng, Svc Govern\n  Infra : Srv Build, Cap Plan, Network, Ops\n  SCO : Info sec\n  Data : DBA\n\n\nCustomer Facing App (de 1 à n équipes)\n\n\n  Middleware/AppDev : SW Arch, SW Dev, Client SW Dev\n  Data : Data Arch\n  Infra : Cap Plan, Ops\n  Biz : Prod Mgmt\n  Architecte : Biz Analyst\n\n\nEnablement (unique / transverse) :\n\n\n  Architecte : Ent Archi, Portfolio Mgmt\n\n\nDCTM - Documentum (Enterprise Content Management Platform) (unique / transverse) :\n\n\n  Infra : Ops, Cap Plan\n  Ent Apps : DCTM (Documentum) Eng.\n\n\nOn notera notamment la présence d’Ops dans les équipes Customer Facing App et inversement de Middleware Eng dans l’équipe Platform.\n\nDe même, la présence d’architectes transverses (enablement) permet de garder une architecture cohérente. (Pas de slides disponibles pour cette conférence)\n\n\nCrédit : Flickr\n\nMicroservices\n\nLa conférence de @susanthesquark, axée sur les microservices, rappelait quelques bonnes pratiques :\n\n\n  Architecture sans SPOF\n  Ne pas laisser la dette technique s’accumuler\n  Déploiement continu\n  Travail en équipe entre Dev / PM / SRE\n  Monitoring\n  Procédures standard de gestion des incidents\n  Post-mortem pour apprendre de ses erreurs…\n\n\nLes slides\n\nConcernant le monitoring des microservices, la conférence de Reshmi Krishna @reshmi9k s’intéressait à l’analyse de la latence, inhérente à ce type d’architecture. La principale technique proposée est celle du suivi d’une requête de bout en bout, grâce notamment à l’outil Zipkin. De même, une gestion des timeouts globale (pour chaque requête pour tous les microservices) et dynamique (selon le contexte) permet de maîtriser les problèmes en cas de ralentissement d’un service en particulier.\n\nLes slides\n\nSécurité\n\nConcernant la sécurité, la conférence de Kelly Lum @aloria, passait en revue le minimun vital :\n\n\n  La sécurité doit être pensée dès la conception\n  Permettre aux utilisateurs de reporter facilement des problèmes de sécurité et être à l’écoute des réseaux sociaux\n  Toujours remercier les utilisateurs signalant les failles\n  Avoir une équipe testant régulièrement la sécurité (Crack Team).\n  En cas de failles de sécurité, après correction, toujours analyser les causes et apprendre de ses erreurs.\n\n\nLes slides\n\nConclusion\n\nVous pouvez retrouver la plupart des slides ici.\net voir les vidéos de certaines conférences ici.\nou ici.\n\nLes photos officielles de la conf sont ici.\n"
} ,
  
  {
    "title"    : "Use the Sensiolabs Security Checker to check potential vulnerabilities on Symfony projects",
    "category" : "",
    "tags"     : " 6tech, lyon, symfony, security, php, jenkins",
    "url"      : "/2016/07/20/sf2-security-checker.html",
    "date"     : "July 20, 2016",
    "excerpt"  : "Numerous vulnerabilities are detected every day. That’s a good thing and a key benefit of using open source products. At m6web we don’t want to be exposed to known vulnerabilities, so we use a service provided by Sensiolabs in our continuous integ...",
  "content"  : "Numerous vulnerabilities are detected every day. That’s a good thing and a key benefit of using open source products. At m6web we don’t want to be exposed to known vulnerabilities, so we use a service provided by Sensiolabs in our continuous integration tool (Jenkins) to check it.\n\nJust add those lines in your ant build file (and adapt basedir) :\n\n    &amp;lt;!-- =================================================================== --&amp;gt;\n    &amp;lt;!-- Security checker                                                    --&amp;gt;\n    &amp;lt;!-- =================================================================== --&amp;gt;\n    &amp;lt;target name=&quot;sf2-security-checker&quot;&amp;gt;\n     &amp;lt;exec executable=&quot;bash&quot; dir=&quot;${basedir}/sources/bin&quot; failonerror=&quot;true&quot;&amp;gt;\n         &amp;lt;arg value=&quot;-c&quot;/&amp;gt;\n         &amp;lt;arg value=&quot;curl -Os https://get.sensiolabs.org/security-checker.phar&quot; /&amp;gt;\n     &amp;lt;/exec&amp;gt;\n     &amp;lt;exec executable=&quot;php&quot; dir=&quot;${basedir}/sources&quot; failonerror=&quot;true&quot;&amp;gt;\n         &amp;lt;arg line=&quot;${basedir}/sources/bin/security-checker.phar security:check composer.lock&quot; /&amp;gt;\n     &amp;lt;/exec&amp;gt;\n    &amp;lt;/target&amp;gt;\n\n\nAnd automatically check your composer.lock againts vulnerabilities. Your build will fail if something wrong is detected.\n\nFor example, with the recent Guzzle one :\n\n\n\nYou can contribute to the vulnerabilities database and the checker via Github.com.\n"
} ,
  
  {
    "title"    : "Retour d&#39;expérience sur l&#39;utilisation de Cassandra sur 6play en vidéo",
    "category" : "",
    "tags"     : " 6tech, lyon, conference, cassandra, video",
    "url"      : "/2016/07/04/rex-cassandra.html",
    "date"     : "July 4, 2016",
    "excerpt"  : "\n\nErratum : dans les phases de questions réponses, j’annonce une phase de test à 10K RPS (requêtes par seconde) ; il s’agissait de RPM (requêtes par minute).\n\nLors du match Suisse vs France, diffusé sur M6 pendant la coupe d’Europe de football, la...",
  "content"  : "\n\nErratum : dans les phases de questions réponses, j’annonce une phase de test à 10K RPS (requêtes par seconde) ; il s’agissait de RPM (requêtes par minute).\n\nLors du match Suisse vs France, diffusé sur M6 pendant la coupe d’Europe de football, la brique users est montée à 75K RPM (soit 1200 rps) et 84K pour Islande vs France.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Migrate smoothly your Flux isomorphic app to Redux",
    "category" : "",
    "tags"     : " react, flux, redux, fluxible, isomorphic, javascript",
    "url"      : "/2016/07/04/migrate-smoothly-flux-isomorphic-app-to-redux.html",
    "date"     : "July 4, 2016",
    "excerpt"  : "Flux, history reminders…\n\n« Flux is the application architecture that Facebook uses for building client-side web applications. » That’s the definition of Flux on the Facebook website. So Flux is just a pattern, not a framework, that goes well with...",
  "content"  : "Flux, history reminders…\n\n« Flux is the application architecture that Facebook uses for building client-side web applications. » That’s the definition of Flux on the Facebook website. So Flux is just a pattern, not a framework, that goes well with React, but not only. The model is focused on user interactions. Its main strength is the unidirectional data flow that enforces developers to be careful and ensures code consistency when application grows up.\n\nSeveral libraries propose tools to implement Flux pattern easily. If no one stood out from the crowd at the beginning, now Redux, created by Dan Abramov, is clearly the one that the community have chosen as you can see below. Most Flux based React start kits you can find are based on Redux.\n\n\n\nAt M6Web, our 6play web application is not designed with Redux, but we use Fluxible. Fluxible is another Flux library, developed by Yahoo. We chose it back in December 2014, when we started the project, because Fluxible was at the time one of the few tool designed for isomorphic applications. Moreover it was already used in production by Yahoo.\n\nWhy do we think Redux is a better choice\n\nEven though Fluxible did get the job done, we are now willing to upgrade our application to Redux. Why?\n\n\n  The popularity of Redux will certainly affect other libraries life and support in the future, maybe Fluxible will be concerned. Fluxible is only supported by a firm and not really by the community.\n  Fluxible has a powerful but complex structure based on contexts and plugins. This can be useful, however when new developers come on the project, this is not always easy to understand. We are always searching to make code simpler for maintainability and we think that Redux is a better alternative than Fluxible on this topic.\n  For a given feature, developers write less code using Redux because the design is very simple, there is no extra boilerplate, the flow is condensed as much as possible. As a consequence, unit tests are easier to write.\n  There are very useful tools about Redux that make Developer eXperience better. For instance, the Redux DevTools allows to time travel live between Flux events. The middleware concept extending capabilities of actions is also interesting.\n  We are beginning to make our development processes converge. Every new React project here starts on Redux, including the Proof Of Concept we made with React Native. Using the same libraries made code sharing easier for us.\n\n\nMigrating a big app from Fluxible to Redux is crazy, isn’t it?\n\n6play is a very big web application. How to migrate to Redux in a reasonable amount of time and without risk?\n\nWe were quite sure that Redux and Fluxible could work together. The goal would be to migrate gradually to Redux without having to remove Fluxible in one giant step. First of all, because we can’t mobilise enough resources to do this in a relatively short time. Secondly, we want to avoid a big deploy in production and potentially critical bugs (even though our application is well tested, there are always cases that we can’t control like memory load for example).\n\nWe tried it… And we succeeded! And this is quite simple.\n\nFirst, we define the store configuration like other Redux application.\n\n// configureStore.js\n\nimport {createStore, combineReducers, applyMiddleware, compose} from &#39;redux&#39;;\nimport thunk from &#39;redux-thunk&#39;;\nimport {canUseDOM} from &#39;fbjs/lib/ExecutionEnvironment&#39;;\n\nimport myReducer1 from &#39;./modules/myModule1/myModule1.reducer&#39;;\nimport myReducer2 from &#39;./modules/myModule2/myModule2.reducer&#39;;\n\nexport default initialState =&amp;gt; createStore(\n  combineReducers({myReducer1, myReducer2}),\n  initialState,\n  compose(\n    applyMiddleware([thunk]),\n    canUseDOM &amp;amp;&amp;amp; window.devToolsExtension ? window.devToolsExtension() : f =&amp;gt; f\n  )\n);\n\nThen we initialize Redux store in our server file. For isomorphic purposes, we have to serialize stores’ state and give it to the html so that client side can take control of the application with server’s data. So here, we build data for the client by combining Redux and Fluxible states.\n\n// server.js\n\nimport {provideContext} from &#39;fluxible-addons-react&#39;;\nimport {match, RouterContext} from &#39;react-router&#39;;\nimport configureStore from &#39;./configureStore&#39;;\n\nprocessAppRequest() {\n  // ...\n\n  const fluxibleContext = FluxibleApp.createContext();\n  const reduxStore = configureStore(initialState);\n\n  match({routes: FluxibleApp.getComponent(), location: url}, (error, redirectLocation, routerState) =&amp;gt; {\n    // ...\n\n    // Original Fluxible root element\n    const rootElement = React.createElement(\n      provideContext(RouterContext, customContextTypes),\n      {...routerState, context: fluxibleContext.getComponentContext()}\n    );\n\n    // Now with Redux\n    const markup = ReactDOMServer.renderToString(\n      React.createElement(Provider, {store: reduxStore}, rootElement)\n    );\n\n    // Build state for client\n    const finalState = {\n      ...FluxibleApp.dehydrate(fluxibleContext),\n      reduxStoreState: reduxStore.getState()\n    };\n\n    // Then build the response layout with the markup and the whole state as usual\n    // ...\n  }\n}\n\nOn client side, we do the opposite operation.\n\n// client.js\n\nimport {provideContext} from &#39;fluxible-addons-react&#39;;\nimport {Router, browserHistory} from &#39;react-router&#39;;\nimport configureStore from &#39;./configureStore&#39;;\n\nconst dehydratedState = window[stateVarName];\nconst reduxStore = configureStore(dehydratedState.reduxStoreState);\n\n// Fluxible rehydrate its state\napp.rehydrate(dehydratedState, (error, fluxibleContext) =&amp;gt; {\n  // ...\n\n  // Original Fluxible root element\n  const rootElement = React.createElement(provideContext(Router, customContextTypes), {\n    history: browserHistory,\n    routes: app.getComponent(),\n    context: fluxibleContext.getComponentContext()\n  });\n\n  // Now with Redux\n  ReactDOM.render(\n    React.createElement(Provider, {store: reduxStore}, rootElement),\n    document.getElementById(rootId)\n  );\n});\n\nAnd that’s it! We can now use Redux in our component as usual, in combination with Fluxible. We can define actions and reducers for new features (instead of using Fluxible) but we can also transform progressively some Fluxible stores and actions into Redux flow, this is very easy. API requests stay in actions but data processing moves to reducers. Then data sorting and filtering logic in Fluxible stores moves to selectors.\n\nComponents connection to stores\n\nTwo files to rule them all\n\nWith Fluxible, we linked components with stores through connectToStore in the same file and exported only the connected component. But we think now it is a bad practice:\n\n\n  splitting data fetching from stores and display logic is interesting for maintainability and code understanding,\n  it is much easier to unit test the component without the connection to store, connectToStores (Fluxible) or connect (Redux) methods are parts of a 3rd-party library, and we don’t need to test it.\n\n\nFrom now on, components are files named *.component.js and stores connections are in *.connector.js files in the same folder. We can link a component both with Redux and Fluxible stores.\n\n// myComponent.connector.js\n\nimport MyComponent from &#39;./myComponent.component&#39;;\n\n// Stores\nimport connectToStores from &#39;fluxible-addons-react/connectToStores&#39;;\nimport {connect} from &#39;react-redux&#39;;\nimport MyFluxibleStore from &#39;../stores/myFluxible.store&#39;;\n\n// Utils\nimport {getSomeDataFromState} from &#39;../myModule.selectors&#39;;\n\n// Redux\nexport const mapStateToProps = (state, props) =&amp;gt; {\n  return {dataFromRedux: getSomeDataFromState(state, props.myProps2)};\n};\n\n// Fluxible\nexport default connectToStores(\n  connect(mapStateToProps)(MyComponent),\n  [MyFluxibleStore],\n  (context, props) =&amp;gt; ({\n    dataFromFluxible: context.getStore(MyFluxibleStore).getSomeData(props.myProps1)\n  })\n);\n\nWe export mapStateToProps function because in a few cases it contains logic that may be interesting to unit test.\n\nStores connections order\n\nIn this example, the link to Fluxible store is higher in components tree than the Redux one as we can see below.\n\n\n\nIt means that if Redux state changes, the Fluxible wrapper component won’t be reloaded but in the reverse case, both Fluxible and Redux wrapper components will rerender. In most scenarios, it doesn’t matter. Connection order of Redux and Fluxible is significant in two situations:\n\n\n  If one connection depends on data stored in the other library state, it has to be lower in components tree.\n\n\n// myComponent.connector.js\n\nimport MyComponent from &#39;./myComponent.component&#39;;\n\n// Stores\nimport connectToStores from &#39;fluxible-addons-react/connectToStores&#39;;\nimport {connect} from &#39;react-redux&#39;;\nimport MyFluxibleStore from &#39;../stores/myFluxible.store&#39;\n\n// Utils\nimport {getSomeDataFromState} from &#39;../myModule.selectors&#39;;\n\n// Fluxible wrapper depends on data from Redux state\nconst MyComponentFluxibleConnector = connectToStores(\nMyComponent,    \n  [MyFluxibleStore],\n  (context, props) =&amp;gt; ({\n    dataFromFluxible: context.getStore(MyFluxibleStore).getSomeDataFromReduxState(props.myPropsFromRedux)\n  })\n);\n\n// Redux\nexport const mapStateToProps = state =&amp;gt; {\n  return {myPropsFromRedux: getSomeDataFromState(state)};\n};\n\nexport default connect(mapStateToProps)(MyComponentFluxibleConnector);\n\n\n  If the higher connection is made on Fluxible stores (like first example of myComponent.connector.js), data passed to props must be immutable otherwise it can cause edge effects. Indeed, Redux wrapper component checks if it has to rerender when props change by comparing their references. So, if we mutate data in Fluxible store when dispatch is handled, references don’t change and Redux wrapper (and sub-components) will not rerender (unless you tell the connect method that your component isn’t “pure”).\n\n\nIf we watch carefully to those particular cases, we will succeed in our quest!\n\nIn a nutshell, Redux can easily work in addition to Fluxible (and certainly to other Flux libraries), most likely because of the lightness of its implementation. It is very convenient to upgrade smoothly a big application on Redux! But be aware that it is only a transitory situation, the final goal is to use only Redux. We wrote 50% less code with this upgrade, not bad… Developers are lazy, don’t forget this! If you have some feedback on Redux and/or Fluxible, don’t hesitate to share your experience with us :)\n\n"
} ,
  
  {
    "title"    : "Retour d’expérience : réaliser des Workers en PHP - Fabien de Saint pern au PHP Tour 2016 ",
    "category" : "",
    "tags"     : " 6tech, lyon, conference, video, phptour, php, Symfony",
    "url"      : "/2016/06/23/video-phptour-worker-php.html",
    "date"     : "June 23, 2016",
    "excerpt"  : "Fabien de Saint pern - lead dev de notre team back-end 6play - était au PHP Tour et a fait une présentation sur la façon dont nous faisons des workers en PHP.\n\n\n\n",
  "content"  : "Fabien de Saint pern - lead dev de notre team back-end 6play - était au PHP Tour et a fait une présentation sur la façon dont nous faisons des workers en PHP.\n\n\n\n"
} ,
  
  {
    "title"    : "Preview your Android &amp; iOS React Native apps on your Github Pull Request",
    "category" : "",
    "tags"     : " reactnative, react, mobile, github, jenkins, fastlane, appetize",
    "url"      : "/2016/06/20/preview-android-ios-react-native-on-github-pull-request.html",
    "date"     : "June 20, 2016",
    "excerpt"  : "We are playing since a few weeks with React Native for a Proof Of Concept and wanted to have the same development workflow for mobile apps, as we have for the web.\n\nHere is the workflow we use for web development:\n\n\n  Branch  : every bugfix or fea...",
  "content"  : "We are playing since a few weeks with React Native for a Proof Of Concept and wanted to have the same development workflow for mobile apps, as we have for the web.\n\nHere is the workflow we use for web development:\n\n\n  Branch  : every bugfix or feature is developed on a new git branch,\n  Pull Request (PR)  : we make PR for each bugfix or feature to propose the modification to the « master » git branch,\n  Code Review  : other teammates have to review each PR and add :+1: when they agree with the modification,\n  Test  : a CI system (Jenkins) runs Unit and Integration tests, and Lint on each PR,\n  Preview  : an internal tool (Github Hooker) is called with a Github webhook on each PR to create a staging environment.\n\n\nWhen every step is ok, the PR is merged.\n\nThe « Branch step », « PR step » and « Code Review step » are mostly related to our CVS (Github Enterprise) and are not a problem.\nThe « Test step » is related to React Native. We already use Jest and ESLint, but we have to dig more for Integration test (Appium ?).\n\nThe « Preview step » is more interesting. It was not the simplest thing to do on our web project, but this is probably one of the most useful feature we have on our stack.\nHaving a staging environment for all open PR allows devs, PO, PM and scrum masters to play with this exact version of the code (on any browser they want), and really see if the bug is fixed, or if the feature correspond to the PO needs. It allows everyone to iterate and make feedbacks before the code lands on the master branch. It’s also a good way to be sure your app build didn’t fail.\n\nSo, what we want is to have on each of our React Native PR, a link to preview iOS and Android version of our app in a web browser, refreshed after every commit on the branch.\n\nThe goal of this blog post is just to show you, that it is something doable and really useful. If you are interested in, here are some more information, that maybe can help you.\n\nThe stack\n\nConcerning the CI, we already use Jenkins, so we will continue to. Beware that for building iOS apps, a CI running on OSX is needed. In our case, we had added a Jenkins slave to our Jenkins pool. If you don’t have CI system internally, you should take a look at Bitrise or CircleCi because they propose OSX CI systems.\nOur CVS is Github Enterprise, but everything is also possible with Gitlab (or any other CVS).\nWe use Fastlane.tools to automate build and credentials support. (Mostly because it was recommended by some of our iOS developers).\n\nIn order to preview iOS and Android app in a web browser, we use the amazing SAAS service Appetize.io (free for 100min/month).\n\n\n\nHow did we do ?\n\nWe had set up an OSX machine with a fresh Jenkins install, and created a job that triggers a build everytime a push is made on a PR, thanks to the “Github Pull Request Builder » Jenkins plugin. There is also a lot of things to configure on this machine (Nodejs, Ruby, xCode …), and i recommend you to do some builds (iOS and Android) manually to be sure everything is ready.\n\nFastlane is an open-source automation toolset for iOS &amp;amp; Android. It lets you write « lane » to automate a lot of things. We set up a unique Fastlane file at the root of our React Native project directory dealing with Android &amp;amp; iOS lanes.\n\nTo suit our needs, we created one lane « deployAppetize » for each platform: it performs the corresponding build, uploads it to Appetize.io via their API, and updates the Github PR Statuses during the process.\n\nI’m not a Ruby programmer, so please, don’t blame me, and feel free to improve the code below if you want (on this Github Gist).\nThis is neither the state of the art, nor a beautiful open source thing, we just share what we did in case it helps someone :-)\n\nBefore doing anything, you’ll have to set some variables on Fastlane, so go to the Fastfile file in your fastlane folder:\n\n#3rd party lib to do some http calls\nrequire &#39;httparty&#39;\n\nfastlane_version &quot;1.95.0&quot;\ndefault_platform :ios\n\nbefore_all do\n  # put here your token and iOS scheme app\n  ENV[&quot;GITHUB_TOKEN&quot;] = &quot;----&quot;\n  ENV[&quot;APPETIZE_TOKEN&quot;] = &quot;----&quot;\n  ENV[&quot;APP_IOS_SCHEME&quot;] = &quot;----&quot;\n\n  # get the last git commit information\n  ENV[&quot;GIT_COMMIT&quot;] = last_git_commit[:commit_hash]\n\n  # Use ghprbSourceBranch env variable on CI, git_branch lane elsewhere\n  if !ENV[&quot;ghprbSourceBranch&quot;]\n    ENV[&quot;ghprbSourceBranch&quot;] = git_branch\n  end\n\nend\n\nCreate a private lane to make the POST request to your Github statuses API to avoid DRY:\n\n# Update git statuses of your commit.\nprivate_lane :githubStatusUpdate do |options|\n\n  response = HTTParty.post(\n    &quot;https://&amp;lt;yourgithubenterprisedomain.tld&amp;gt;/api/v3/repos/&amp;lt;orga&amp;gt;/&amp;lt;repos&amp;gt;/statuses/#{ENV[&quot;GIT_COMMIT&quot;]}?access_token=#{ENV[&quot;GITHUB_TOKEN&quot;]}&quot;,\n    :body =&amp;gt; {\n      :context =&amp;gt; options[:context],\n      :state =&amp;gt; options[:state],\n      :description =&amp;gt; options[:description],\n      :target_url =&amp;gt; options[:url]\n    }.to_json,\n    :headers =&amp;gt; { &#39;Content-Type&#39; =&amp;gt; &#39;application/json&#39; }\n  )\nend\n\nAppetize allows you to create different apps. We want one app per PR, and update the corresponding app when a new commit is made on a PR. For that, we keep track of the branch name by storing it in the « notes » field of the app on Appetize.io.\n\nSo, here’s a private lane to get back the public key of the corresponding app on Appetize.io, to update the good one if it already exists.\n\n# get the publicKey of the appetizeApp corresponding to your git branch\nprivate_lane :getAppetizePublicKey do |options|\n  publicKey = &quot;&quot;\n\n  response = HTTParty.get(&quot;https://#{ENV[&quot;APPETIZE_TOKEN&quot;]}@api.appetize.io/v1/apps&quot;)\n  json = JSON.parse(response.body)\n\n  # Find branch name in notes\n  json[&quot;data&quot;].each do |value|\n    if value[&quot;note&quot;] == ENV[&quot;ghprbSourceBranch&quot;] &amp;amp;&amp;amp; value[&quot;platform&quot;] == options[:platform]\n      publicKey = value[&quot;publicKey&quot;]\n    end\n  end\n\n  publicKey\nend\n\nNow, we have everything ready to do the deployAppetize lane for iOS :\n\nplatform :ios do\n\n  desc &quot;Deployment iOS lane&quot;\n\n    lane :deployAppetize do\n\n      githubStatusUpdate(\n        context: &#39;Appetize iOS&#39;,\n        state: &#39;pending&#39;,\n        url: &quot;https://appetize.io/dashboard&quot;,\n        description: &#39;iOS build in progress&#39;\n      )\n\n      Dir.chdir &quot;../ios&quot; do\n        tmp_path = &quot;/tmp/fastlane_build&quot;\n\n        #seems not possible to use gym to do the simulator release ?\n        xcodebuild_configs = {\n          configuration: &quot;Release&quot;,\n          sdk: &quot;iphonesimulator&quot;,\n          derivedDataPath: tmp_path,\n          xcargs: &quot;CONFIGURATION_BUILD_DIR=&quot; + tmp_path,\n          scheme: &quot;#{ENV[&quot;APP_IOS_SCHEME&quot;]}&quot;\n        }\n\n        Actions::XcodebuildAction.run(xcodebuild_configs)\n\n        app_path = Dir[File.join(tmp_path, &quot;**&quot;, &quot;*.app&quot;)].last\n\n        zipped_bundle = Actions::ZipAction.run(path: app_path, output_path: File.join(tmp_path, &quot;Result.zip&quot;))\n\n        Actions::AppetizeAction.run(\n          path: zipped_bundle,\n          api_token: &quot;#{ENV[&quot;APPETIZE_TOKEN&quot;]}&quot;,\n          platform: &quot;ios&quot;,\n          note: &quot;#{ENV[&quot;ghprbSourceBranch&quot;]}&quot;,\n          public_key: getAppetizePublicKey({platform: &quot;ios&quot;})\n        )\n\n        FileUtils.rm_rf(tmp_path)\n\n      end\n\n      githubStatusUpdate(\n        context: &#39;Appetize iOS&#39;,\n        state: &#39;success&#39;,\n        url: &quot;#{lane_context[SharedValues::APPETIZE_APP_URL]}&quot;,\n        description: &#39;iOS build succeed&#39;\n      )\n    end\n\n    error do |lane, exception|\n      case lane\n        when /deployAppetize/\n          githubStatusUpdate(\n            context: &#39;Appetize iOS&#39;,\n            state: &#39;failure&#39;,\n            url: &quot;https://appetize.io/dashboard&quot;,\n            description: &#39;iOS build failed&#39;\n          )\n        end\n      end\nend\n\nFor Android, it’s almost the same things, except we have to do some small business logic to find the apk generated by Gradle, with this private lane :\n\n# find the path of the last apk build\nprivate_lane :getLastAPKPath do\n  apk_search_path = File.join(&#39;../android/&#39;, &#39;app&#39;, &#39;build&#39;, &#39;outputs&#39;, &#39;apk&#39;, &#39;*.apk&#39;)\n  new_apks = Dir[apk_search_path].reject { |path| path =~ /^.*-unaligned.apk$/i}\n  new_apks = new_apks.map { |path| File.expand_path(path)}\n  last_apk_path = new_apks.sort_by(&amp;amp;File.method(:mtime)).last\n\n  last_apk_path\nend\n\nAnd now you should be able to also deploy to Appetize.io on Android :\n\nplatform :android do\n\n  desc &quot;Deployment Android lane&quot;\n\n    lane :deployAppetize do\n\n      githubStatusUpdate(\n        context: &#39;Appetize Android&#39;,\n        state: &#39;pending&#39;,\n        url: &quot;https://appetize.io/dashboard&quot;,\n        description: &#39;Android build in progress&#39;\n      )\n\n      gradle(\n        task: &quot;assemble&quot;,\n        build_type: &quot;Release&quot;,\n        project_dir: &quot;android/&quot;\n      )\n\n      Actions::AppetizeAction.run(\n        path: getLastAPKPath,\n        api_token: &quot;#{ENV[&quot;APPETIZE_TOKEN&quot;]}&quot;,\n        platform: &quot;android&quot;,\n        note: &quot;#{ENV[&quot;ghprbSourceBranch&quot;]}&quot;,\n        public_key: getAppetizePublicKey({platform: &quot;android&quot;})\n      )\n\n      githubStatusUpdate(\n        context: &#39;Appetize Android&#39;,\n        state: &#39;success&#39;,\n        url: &quot;#{lane_context[SharedValues::APPETIZE_APP_URL]}&quot;,\n        description: &#39;Android build succeed&#39;\n      )\n    end\n\n    error do |lane, exception|\n      case lane\n        when /deployAppetize/\n          githubStatusUpdate(\n            context: &#39;Appetize Android&#39;,\n            state: &#39;failure&#39;,\n            url: &quot;https://appetize.io/dashboard&quot;,\n            description: &#39;Android build failed&#39;\n          )\n      end\nend\n\nIt’s over. You just have to add those commands to your CI to do the job :\n\nnpm install\nFastlane ios deployAppetize\nFastlane android deployAppetize\n\n\nYou have now two new checks on each PR with a link to the iOS or Android instance on Appetize.io.\n\n\n\nThe complete Fastfile on a Github Gist : FastFile\n\nConclusion\n\nAt M6web, we are glad to see the whole React Native promise taking a concrete shape: the developer experience is the same for both mobile &amp;amp; web development, even about tooling. We are continuing to play with it and we’ll certainly keep posting articles here, stay tuned !\n\nP.S.: You could look at the Fabric Blog post on the device grid for Fabric but with Danger commenting on the PR instead of Github Statuses, and iOS only.\n\nP.S.2: You could also look at Reploy.io, which try to improve this workflow with extra features and a more cleaner UX than Appetize.io, but it is “alpha” for now.\n"
} ,
  
  {
    "title"    : "M6web fera un retour d&#39;expérience sur l&#39;usage de Cassandra sur 6play le 14/06/2016",
    "category" : "",
    "tags"     : " 6tech, lyon, conference",
    "url"      : "/2016/05/25/m6web-retourdxp-cassandra.html",
    "date"     : "May 25, 2016",
    "excerpt"  : "Olivier Mansour, responsable R&amp;amp;D, sera présent au Cassandra Days le 14 Juin à Paris pour faire un retour d’expérience sur l’utilisation de Cassandra sur 6play.\n\n\n\nL’évènement est gratuit : https://www.eventbrite.co.uk/e/billets-datastax-day-pa...",
  "content"  : "Olivier Mansour, responsable R&amp;amp;D, sera présent au Cassandra Days le 14 Juin à Paris pour faire un retour d’expérience sur l’utilisation de Cassandra sur 6play.\n\n\n\nL’évènement est gratuit : https://www.eventbrite.co.uk/e/billets-datastax-day-paris-25165891860.\n\n"
} ,
  
  {
    "title"    : "Arrêtons de perdre du temps à débuguer !",
    "category" : "",
    "tags"     : " afup, php, debug, conference",
    "url"      : "/2016/05/24/arretons-de-perdre-du-temps.html",
    "date"     : "May 24, 2016",
    "excerpt"  : "Arrêtons de perdre du temps à débuguer ! Débuguer peut se révéler long et fastidieux. \nC’est du temps perdu qu’on pourrait passer à créer de la valeur ajoutée. \nC’est d’une manière ou d’une autre une perte pour le business. \nAyant commencé mon ent...",
  "content"  : "Arrêtons de perdre du temps à débuguer ! Débuguer peut se révéler long et fastidieux. \nC’est du temps perdu qu’on pourrait passer à créer de la valeur ajoutée. \nC’est d’une manière ou d’une autre une perte pour le business. \nAyant commencé mon entrée dans la vie active par une TMA, j’ai compris vite et de manière un peu brutale que ça fait pourtant partie de la vie du développeur qui devient parfois débugueur. \nQuelles solutions et astuces pouvons-nous mettre en place afin d’être plus efficace dans cette tâche rébarbative ?\n"
} ,
  
  {
    "title"    : "M6web sera présent au sfpot de Lille du 16/06/16",
    "category" : "",
    "tags"     : " 6tech, lille, sfpot, conference",
    "url"      : "/2016/05/19/6tech-sfpot-lille.html",
    "date"     : "May 19, 2016",
    "excerpt"  : "Pierre Marichez, Renaud Bougré et Nicolas Beze une partie de l’équipe PHP de M6Web Lille, vous feront part d’un retour d’expérience sur l’industrialisation des développements.\nCa parlera jenkins, gitlab, gitlab-ci, outil de gestion de projets, api...",
  "content"  : "Pierre Marichez, Renaud Bougré et Nicolas Beze une partie de l’équipe PHP de M6Web Lille, vous feront part d’un retour d’expérience sur l’industrialisation des développements.\nCa parlera jenkins, gitlab, gitlab-ci, outil de gestion de projets, api, sentry, capistrano, user scripts, docker, grafana, slack…\n\nLors de ce sfpot, Kevin Dunglas présentera le DunglasActionBundle et Alexandre Salomé et Luc Vieillescazes vous feront un retour sur le sflive 2016.\n\nAlors rendez-vous tous le 16 juin 2016 à partir de 19h00 au Liberch’ti, 169 Boulevard de la Liberté à Lille (Métro République).\n\nPour vous inscrire, ça se passe ici\n\n\n\nPour plus d’informations sur cet événement et les autres sfpot lillois, rendez-vous sur le site des Tilleuls.\n"
} ,
  
  {
    "title"    : "M6web sera présent au PHPTour Clermont-Ferrand",
    "category" : "",
    "tags"     : " 6play, afup, phptour, conference",
    "url"      : "/2016/05/09/6tech-phptour-clermont.html",
    "date"     : "May 9, 2016",
    "excerpt"  : "Fabien de Saint Pern, un des leads devs sur la plateforme 6play, aura l’occasion de présenter une conférence au PHPTour Clermont-Ferrand le 24 Mai. Il fera un retour d’expérience concret sur nos pratiques autour de la réalisation de workers asynch...",
  "content"  : "Fabien de Saint Pern, un des leads devs sur la plateforme 6play, aura l’occasion de présenter une conférence au PHPTour Clermont-Ferrand le 24 Mai. Il fera un retour d’expérience concret sur nos pratiques autour de la réalisation de workers asynchrones en PHP (et oui !).\n\nLe PHPTour est un cycle de conférences itinérant organisé par l’AFUP réunissant toutes les communautés PHP, professionnelles et open-source, dédié au langage et à son écosystème. Ne manquez pas cette conférence ainsi que cet évènement qui s’annonce particulièrement riche !\n\n"
} ,
  
  {
    "title"    : "La retrospective Agile ‘Garde à vous’",
    "category" : "",
    "tags"     : " agile, scrum",
    "url"      : "/2016/03/29/retro-agile-garde-a-vous.html",
    "date"     : "March 29, 2016",
    "excerpt"  : "Depuis quelques années les équipes d’M6Web se sont organisées autour des méthodes agiles. Scrum, Kanban, Lean, méthodes adaptées, nous nous efforçons de toujours garder en tête l’amélioration continue et le fun spirit au coeur du travail de nos éq...",
  "content"  : "Depuis quelques années les équipes d’M6Web se sont organisées autour des méthodes agiles. Scrum, Kanban, Lean, méthodes adaptées, nous nous efforçons de toujours garder en tête l’amélioration continue et le fun spirit au coeur du travail de nos équipes.\n\nAu delà des rituels “classiques”, l’équipe des scrum master cherche de temps en temps à thématiser et casser les routines en créant des jeux autour de l’agilité.\n\nNous souhaitons aujourd’hui au travers de ce blog, partager avec vous ces jeux et surtout vous permettre de les reproduire. Ainsi chaque jeu s’accompagnera de règles et d’un « kit » vous permettant d’imprimer le matériel nécessaire au bon déroulement.\n\nAu menu de ce premier jeu, nous avions choisi de profiter de l’arrivée de l’émission “Garde à vous” sur M6 afin de proposer une rétrospective pas comme les autres.\n\n\n  Nom : La rétro Garde à vous !\n  Type : Rétrospective d’équipe.\n  Durée : 1 heure.\n\n\nPunchLine :\n\nVotre équipe est sélectionnée pour une retro spéciale. Un défi difficile qui les mènera à dépasser leur limite. \nÉpreuve physique et mentale, il y en aura pour tous. Mais surtout c’est en groupe qu’ils réussiront les épreuves. :)\n\n\n\nObjectifs :\n\n\n  Changer de la rétro classique : 2 activités sur les 4 sont des mini-jeux.\n  Créer du team building : Le 1er jeu demande confiance et cohésion entre les membres de son équipe. Et accessoirement c’est très fun !\n  Stimuler les équipes entre-elles autour d’une compétition sympa (nos 5 équipes ont fait la même rétro lors de la rotation).\n  Garder à l’esprit l’amélioration continue au travers des 2 activités post-it.\n\n\nPréparation / matériel :\n\n\n  Vous trouverez ici un lien vers le kit de la rétro garde à vous.\n  Créer un décor : Mettez vos équipes dans l’ambiance et poussez l’aspect « jeux de rôle ». \nExemple : filet à chat, bâche de tente Quechua, palissade, affiche militaire, etc..\n  Costume de l’animateur : pantalon/veste militaire (demander autour de vous), cravache, casque avec lunette de ski, chemise beige, etc..\n  Pour l’étape #1 : Post-it, feutres pour écrire, l’affiche paperboard « Motivation ».\n  Pour l’étape #2 : 4 bandanas ou serviettes pour bander les yeux + gilets fluos de sécurité + un chronomètre + un parcours dans vos bureaux.\n  Pour l’étape #3 : Post-it, feutres pour écrire, l’affiche paperboard « 4 thèmes d’amélioration ».\n  Pour l’étape #4 : 2 Nerfs, 4 canettes vides, 4 peluches (ou autre), 4 balles en mousse, une poubelle, tapis de sol.\n  La feuille des scores.\n\n\n\n\nLe déroulement :\n\nVolontairement nous n’avons donné aucun détail à nos équipes sur cette rétro. \nQuelques jours avant, nous leur avons envoyé la vidéo bande annonce sous forme de « convocation ». \nLe jour même, ils ont eu la surprise de voir la salle décorée et leur scrum master déguisé.\n\nÉtape.1 - Motivation - 10 minutes :\n\nTexte possible : « Bonjour équipe [nom_équipe]. Je suis le sergent “Badass”, on vous a placé chez moi aujourd’hui pour évaluer votre trouillomètre.\n\n\n  « Cette rétro va se dérouler en 4 étapes. Comme je suis sympa, je ne vous dis rien. ça permettra de voir votre capacité d’adaptation.»\n\n\n\n  «  Sachez que nous aurons 2 épreuves physiques et 2 épreuves mentales. Lors des épreuves physiques, nous noterons vos scores afin de  déterminer quelle est la meilleure équipe du plateau. »\n\n\n\n  «  êtes vous prêts ? »\n\n\nPaperboard #1 : La motivation\n\n\n  « On va commencer doucement. Prenez vos post-it et vos crayons.»\n\n\n\n  «  Dites moi ce qui vous motive à vous levez le matin ? pourquoi vous aimez venir bosser ? »\n\n\n\n  «  Si un aspect du boulot vous ennuie, vous cloue au lit, dites le également.»\n\n\n\n  «  TimeBox : 2 minutes. »\n\n\nAu bout des 2 minutes : chacun passe au paperboard coller ses post-it et les expliquer.\n\nObjectif du scrum master :\n\n\n  récupérer les aspects positifs de l’environnement, du travail de vos équipes : ce sont des bases solides à avoir en tête et à maintenir dans le groupe.\n  récupérer les aspects négatifs : ça peut être la cantine, la distance des locaux, etc.. même si certains post-it sont difficilement ‘améliorables’ c’est toujours bien de l’exprimer.\n\n\nÉtape.2  - En avant, Marche ! - 10 minutes\n\nRègle du jeu :\n\n\n  On se met par 2. Si vous êtes un nombre impair, explication plus bas.\n  L’une des 2 personnes va avoir les yeux bandés. On l’équipe d’un gilet fluo de sécurité afin d’éviter de lui rentrer dedans… :p\n  L’autre personne devra le guider en utilisant les mots : « avance / recule / à droite / à gauche ».\n  Il est interdit de toucher son coéquipier pendant la course.\n  \n    Vous êtes chronométrés. Un classement général sera fait avec les autres équipes pour déterminer les plus rapides.\n  \n  Faites mettre le bandana ou la serviette. Prenez les guides et montrez leur le parcours.\n  Faites aligner les paires devant la ligne de départ.\n  3, 2, 1, Partez. N’hésitez pas à les encourager ou à parler fort afin de les stresser ^^.\n\n\nVersion à 3 : vous aurez 1 guide et 2 personnes avec les yeux bandés. Les 2 yeux bandés sont en file indienne.\n\nLa personne derrière pose ses mains sur les épaules sur la personne de devant. Bonne chance :p\n\nFeuille des scores : Notez le temps de chaque paires.\n\nÉtape.3 - les 4 thèmes - 20/25 minutes\n\nRègles :\n\n\n  Nous avons 4 thèmes affichés au paperboard.\n  Pour chaque thème, vous pouvez écrire au maximum 2 post-it positifs &amp;amp; 2 post-it négatifs\n\n\nNous limitons le nombre de post-it pour une question de temps. Libre à vous d’ajuster.\n\nLes thèmes sont :\n\n\n  Communication - Dialogue dans l’équipe, avec le PO, les clients finaux, utilisation des mails, etc…\n  Les outils - De développement, méthode agile, communication, de déploiement/MEP, de testing, etc..\n  Réactivité - Lors d’une demande PO, d’un incident de production, phase de cadrage avec PO, etc…\n  Leadership - Présence de votre Lead-Dev / Responsable R&amp;amp;D, écoute de vos managers, méthode d’organisation dans le travail, etc…\n\n\nNous laissons 7 minutes d’écriture de post-it (à ajuster selon vous).\nAu bout des 7 minutes : chacun passe au paperboard coller ses post-it et les expliquer.\n\nObjectif du scrum master :\n\n\n  Récupérer des axes d’amélioration de l’équipe.\n  Conclure sur les aspects positifs et définir les post-it négatifs sur lesquels on cherche à agir en 1er.\n  Éviter de définir les actions à mettre en oeuvre pour les post-it négatifs. Faites le en dehors sinon ça prendra trop de temps et cassera la dynamique.\n\n\nÉtape.4 - Duck Hunt - 15 minutes\n\nRègles : 3 stands sont proposés\n\n\n  Chacun choisit un stand. Les participants peuvent faire un essai rapide si c’est demandé.\n  Nous vous laissons ajuster la distance entre les cibles et le joueur.\n  Stand 1 - Le grenadier : 4 grenades (balle en mousse), une poubelle =&amp;gt; lancer les grenades dans la poubelle. 4 essais, 1 réussite = 1 point\n  Stand 2 - le chasseur : 4 canettes, un nerf =&amp;gt; toucher les canettes. 4 essais, 1 réussite = 1 point\n  Stand 3 - le sniper : 4 peluches éléphant PHP, un gros nerf =&amp;gt; mode allongé dans les bois, 4 essais, 1 réussite = 1 point\n\n\nCeci afin de répartir les personnes sur plusieurs stands.\n\nFeuille des scores : Notez le score de chacun.\n\n\n\nConclusion :\n\nAfficher le tableau des scores et féliciter tout le monde. \nNext step :\n\n\n  Les résultats des équipes seront affichés le lendemain / fin de journée / autres (à vous de voir)\n  Les post-it négatifs de l’étape 1 et 3 seront pris en compte par les scrum master qui travailleront avec les personnes adéquates pour continuer à s’améliorer. Vous pouvez ajouter à votre DSK (Do, Store, Keep) certaines actions.\n\n\nN’hésitez pas à nous envoyer vos feedbacks sur ce jeu.\n\nForce et Scrum !\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un développeur player vidéo JavaScript (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/2016/01/26/m6web-lyon-recherche-developpeur-player-video-web-h-f-en-cdi.html",
    "date"     : "January 26, 2016",
    "excerpt"  : "Mise jour : Le poste n’est à plus pourvoir. Merci\n\nAu sein de la team Tube (équipe Lecteur Vidéo), en charge entre autre du lecteur de 6play et des lecteurs vidéos des autres portails Internet d’M6 Web (Clubic.com, Deco.fr, …), vous participez à l...",
  "content"  : "Mise jour : Le poste n’est à plus pourvoir. Merci\n\nAu sein de la team Tube (équipe Lecteur Vidéo), en charge entre autre du lecteur de 6play et des lecteurs vidéos des autres portails Internet d’M6 Web (Clubic.com, Deco.fr, …), vous participez à la conception technique et au développement de nos lecteurs vidéos.\n\nVous maitrisez les problématiques et les technologies Web :\n\n\n  ECMAScript 2015 « ES6 »\n  VideoJS\n  Gulp / Grunt\n  Les outils de tests (Jasmine, qUnit, Jest, PhantomJS, BrowserStack …)\n\n\nVous avez une bonne connaissance des problématiques vidéos :\n\n\n  Les nombreux formats en diffusion continue (streaming) mais aussi en téléchargement progressif (Progressive Download)\n  Les contraintes d’encodage vidéo\n  Les problématiques autour de la sécurité (le chiffrement, les DRMs du marché, …)\n\n\nPar ailleurs, vous avez déjà eu à travailler sur l’intégration de formats publicitaires.\n\nEnfin, une connaissance d’ActionScript sera appréciée.\n\nVous aurez des interactions avec les équipes Produit de Paris, ainsi qu’avec nos autres développeurs basés à Lille.\n\nLe profil recherché se caractérise par :\n\n\n  Une très forte sensibilité sur les sujets Vidéo et Qualité de Service\n  Un goût prononcé pour l’innovation\n  Une bonne culture du web et du monde du numérique\n  Une aptitude à la prise d’initiatives, un grand dynamisme, une curiosité et une ouverture d’esprit\n  Une connaissance (idéalement validée par une première expérience) des Méthodes Agiles (Scrum), et une culture de l’amélioration continue\n\n\nPour postuler : https://www.groupem6.fr/ressources-humaines/offres-emploi/developpeur-frontend-javascript-video-h-f-258357.html\n\n"
} ,
  
  {
    "title"    : "On a testé fonctionnellement notre app JS",
    "category" : "",
    "tags"     : " tests fonctionnels, javascript, phantomjs, webdriver, Cytron",
    "url"      : "/2016/01/25/tests-fonctionnels-app-js.html",
    "date"     : "January 25, 2016",
    "excerpt"  : "L’utilité des tests fonctionnels pour les applications web n’est plus à démontrer (comment ça, vous ne testez pas encore vos apps ?). Malheureusement, tout ne peut pas être totalement testé fonctionnellement, ou de façon aisée : je pense par exemp...",
  "content"  : "L’utilité des tests fonctionnels pour les applications web n’est plus à démontrer (comment ça, vous ne testez pas encore vos apps ?). Malheureusement, tout ne peut pas être totalement testé fonctionnellement, ou de façon aisée : je pense par exemple au player chez nous, un composant stratégique mais pauvrement testé fonctionnellement de par sa nature un peu hybride (mélange de flash et de JS). Dans tous les cas, pour ce qui peut l’être, nous sommes partisans dans l’équipe Cytron d’user sans mesure (ou presque !) de cet outil de manière à être le plus zen possible au moment d’appuyer sur le bouton “deploy”.\n\nQuelle stack ?\n\nNotre application est codée en JS isomorphique (ou Universal JS) grâce à React et Node.js.\n\nPour les tests fonctionnels, nous utilisons le trio Cucumber.js + WebdriverIO + PhantomJS :\n\n\n  Cucumber.js est l’outil qui permet de dérouler la suite de tests écrits dans la syntaxe Gherkin,\n  WebdriverIO permet d’interfacer les tests traduits en JS avec un serveur Selenium (dialoguant grâce au protocole WebDriver Wire et permettant de contrôler un browser),\n  PhantomJS est le browser dans lequel les scénarios de tests seront exécutés, il embarque son propre serveur Webdriver, Ghostdriver.\n\n\nToutes nos Pull Requests lancent les tests indépendamment via Jenkins dans un environnement “dockerisé”, donc complètement autonome et isolé. De façon à respecter ce principe jusqu’au bout et à ne pas dépendre de données versatiles, nos API sont aussi mockées grâce à superagent-mock.\n\nSetup\n\nArborescence\nDans notre projet, nous avons un dossier pour les tests fonctionnels organisés comme suit :\n\n├─┐ tests\n│ ├─┐ step_definitions\n│ │ └── my_feature.steps.js\n│ ├─┐ screenshots\n│ │ └── my_scenario.png\n│ ├─┐ support\n│ │ ├── config.json\n│ │ ├── constants.json\n│ │ ├── hooks.js\n│ │ └── world.js\n│ └── my_feature.feature\n\nFeatures\nUne feature est un fichier testant une fonctionnalité de l’application et regroupant plusieurs scénarios de test. Il est écrit en langage naturel (Gherkin) de façon à être lisible par tous.\n\n# tests/support/cookie.feature\nFeature: Scenarios about the cookie banner\n\n  Scenario: See the cookie banner and close it\n    Given My browser storage is empty\n    When I visit the &quot;homepage&quot; page\n    Then I should see the &quot;cookie banner&quot;\n\n    When I click on &quot;Accept cookie&quot;\n    Then I should not see a &quot;cookie banner&quot;\n\n    When I visit the &quot;homepage&quot; page\n    Then I should not see a &quot;cookie banner&quot;\n\nWorld\nLe fichier world.js est le point de départ pour Cucumber.js. C’est ici que nous initialisons WebdriverIO et que nous mettons un place un contexte qui sera disponible pour tous les tests.\n\n// tests/support/world.js\nvar Webdriver = require(&#39;webdriverio&#39;);\nvar config = require(&#39;./config.json&#39;);\nvar assert = require(&#39;assert&#39;);\n\nvar browser = Webdriver.remote({\n  logLevel: config.logLevel || &#39;silent&#39;,\n  host: config.webdriver.host,\n  port: config.webdriver.port,\n  waitforTimeout: config.waitTimeout,\n  desiredCapabilities: {browserName: &#39;phantomjs&#39;}\n});\n\nfunction WorldConstructor() {\n  var world = {\n    browser: browser,\n\n    // Global visit method\n    visit: function (baseUrl, params) {\n      var pathUrl = url.format({\n        pathname: baseUrl,\n        query: params\n      });\n\n      return this.browser.url(pathUrl);\n    },\n\n    // Take screenshot\n    screenshot: function (filename) {\n      return browser.saveScreenshot(path.join(config.screenshot.path, filename));\n    },\n\n    assert: {\n      /**\n       * Assert if element(s) are visible\n       *\n       * @param selector    {String}   Can be query multiple DOM elements\n       * @param failMessage {String}   Fail message if no visible\n       */\n      visible: function (selector, failMessage) {\n        // ...\n      },\n    }\n\n    // ...\n  }\n\n  return world;\n}\n\nmodule.exports = WorldConstructor;\n\nHooks\nCucumber.js permet de déclencher des traitements sur certains évènements clés lors de l’exécution de la suite de tests. Nous utilisons ce système pour réaliser une capture d’écran sur chaque scénario de test en échec qui viendra s’ajouter dans le dossier screenshots.\n\n// tests/support/hook.js\nvar config = require(&#39;./config.json&#39;);\nvar sprintf = require(&#39;sprintf-js&#39;).sprintf;\n\nmodule.exports = function () {\n  this.Before(function (scenario) {\n    return this.browser.init().then(function () {\n      return this.browser.setViewportSize({\n        width: config.screenshot.width,\n        height: config.screenshot.height\n      });\n    }.bind(this));\n  });\n\n  this.After(function (scenario) {\n    if (scenario.isFailed()) {\n      return this.screenshot(sprintf(\n        &#39;%s_%d.png&#39;,\n        scenario.getName().toLowerCase().replace(&#39; &#39;, &#39;-&#39;),\n        new Date().getTime()\n      )).then(function () {\n        return this.browser.end();\n      }.bind(this));\n    } else {\n      return this.browser.end();\n    }\n  });\n};\n\nStep definitions\nCe sont les fichiers qui font le lien entre les features (écrit en langage naturel) et WebdriverIO (initialisé dans world.js).\n\n// tests/step_definitions/cookie.steps.js\nvar sprintf = require(&#39;sprintf-js&#39;).sprintf;\n\nmodule.exports = function () {\n  /**\n   * Visit a page\n   *\n   * @param page {String}\n   *\n   * @require config routes object\n   */\n  this.When(/^I visit the &quot;([^&quot;]*)&quot; page$/, function (page) {    \n    return this.visit(this.getRoute(page)).then(function () {\n      return this.assert.existing(&#39;#__main&#39;, &#39;React application is not loaded.&#39;);\n    }.bind(this));\n  });\n\n  /**\n   * I click on &quot;label&quot;\n   *\n   * @param label {String}   DOM selector label\n   */\n  this.When(/^I click on &quot;([^&quot;]*)&quot;$/, function (label) {\n    var selector = this.getDOMSelector(label);\n    \n    return this.action.click(selector);\n  });\n\n  /**\n   * Assert element matching the given selector is visible.\n   *\n   * @param label {String}\n   *\n   * @require config DOMSelectors object\n   */\n  this.Then(/^I should see a &quot;([^&quot;]*)&quot;$/, function (label) {\n    var selector = this.getDOMSelector(label);\n    var failMessage = sprintf(&#39;%s is not visible&#39;, label);\n    \n    return this.assert.visible(selector, failMessage);\n  });\n\n  // ...\n}\n\nDesign\nNous n’avons pas mis en œuvre le pattern Page Object. Ce n’était pas un choix délibéré mais le contexte et les enjeux du projet nous ont fait passer à côté, ou ce n’était peut être simplement pas le moment. Malgré tout, nous avons tenté de rationaliser au mieux l’organisation du code. Par exemple, afin de ne pas se retrouver avec des sélecteurs CSS éparpillés dans plusieurs fichiers de “features” ou de “step definitions”, nous avons choisi de les regrouper dans un fichier constants.json et d’utiliser seulement des labels ailleurs. Nous faisons le lien entre le label et le sélecteur CSS avec la méthode getDOMSelector, visible ci-dessus et définie dans le fichier world.js.\n\nRun\nPour lancer les tests, il faut :\n\n\n  lancer le serveur de l’app en local (l’URL du serveur est paramétrable dans le fichier de config),\n  lancer un phantomjs en mode webdriver phantomjs --webdriver=5024 où 5024 est le port du serveur (également configurable dans config.json),\n  lancer une suite de tests via Cucumberjs, au choix :\n    \n      tous les tests cucumberjs tests/,\n      une feature cucumberjs tests/cookie.feature,\n      un scénario cucumberjs tests/cookie.feature:3 où 3 correspond à la ligne du début du scénario ciblé dans le fichier cookie.feature.\n    \n  \n\n\nParticularité de l’isomorphisme\n\nDeux chemins sont possibles avec l’isomorphisme. Soit l’utilisateur arrive directement sur la page, auquel cas celle-ci sera générée sur le serveur, soit il y arrive en naviguant sur l’app et c’est le client qui aura exécuté le code. Il faut tester ces deux cas car le code concerné n’est pas toujours le même (la variable window par exemple n’est pas accessible côté serveur).\n\nIl est bien sûr impossible d’être exhaustif. L’idée est d’abord de couvrir les cas les plus fréquents et les plus critiques pour l’application. Ensuite, il faut s’astreindre à ajouter un test à chaque fois qu’un bug est détecté de façon à s’assurer qu’on ne le rencontrera plus dans le futur.\n\nPhantomJS, la stabilité en question…\n\nBasé sur Webkit, PhantomJS est le plus connu des navigateurs headless, c’est-à-dire exécutables sans interface visuelle. D’autres navigateurs légers et créés pour les tests fonctionnels existent comme SlimerJS (basé sur Gecko et pas vraiment headless) ou Zombie.js (pas de moteur de rendu). Cependant aucun n’offre toutes les fonctionnalités de PhantomJS qui se rapprochent le plus d’un vrai browser. Il émule de façon transparente tout le rendu graphique avec la possibilité de réaliser des screenshots par exemple ou de tester la visibilité d’un élément du DOM (non opaque, dans le viewport, sur la couche z-index la plus haute…).\n\nNéanmoins celui-ci n’intègre pas toutes les dernières avancées en terme de JS et de CSS. Flexbox n’est par exemple pas pris en charge ce qui nous a posé quelques problèmes sur les vérifications liées à la visibilité des éléments. Sa version 2.0 qui date de début 2015, malgré la bonne volonté des contributeurs, n’a toujours pas de build officiel sous Linux, ce qui oblige à compiler les sources sur sa machine de tests ou à trouver sur le net un build officieux correspondant à sa distribution. C’est ce que nous avons fait via M6Web/phantomjs2. Cependant, l’outil est assez instable (builds officiels ou pas) et nous avons rencontré beaucoup de crashs aléatoires ou reproductibles mais incompréhensibles (dus par exemple à l’ajout de quelques lignes de CSS anodines…).\n\nEn local, sur sa machine, PhantomJS est encore moins stable que sur Jenkins. Il semblerait qu’exécution après exécution, il garde des “choses” en cache quelque part qui, à terme, produisent des crashs systématiques de l’outil. Nous n’avons pas réussi à établir un scénario reproductible qui nous permette de poser une issue sur le projet. N’hésitez pas à réagir en commentaire si vous vous êtes trouvé dans un cas similaire.\n\nPour régler temporairement ce problème, nous avons utilisé l’image docker de Gabe Rosenhouse pour le faire tourner dans un environnement indépendant mais ce n’est pas faciliter la vie des développeurs qui veulent juste lancer des tests sans avoir à mettre en œuvre une usine à gaz derrière.\n\nEdit: hier, la version 2.1 de PhantomJS a (enfin) été publiée avec un build pour chaque plateforme. Plusieurs de nos soucis pourraient être réglés avec cette nouvelle release, à suivre…\n\nChrome+ChromeDriver, une alternative ?\n\nNous avons alors opté pour la solution Chrome+ChromeDriver. ChromeDriver a le rôle du serveur Selenium qui permet de faire communiquer WebriverIO avec Chrome. Les avantages de cette stack sont multiples. D’abord, l’ensemble est beaucoup plus stable, fini les crashs impromptus. Ensuite, le debug des tests en échec est bien plus aisé : on voit en effet la suite se jouer en temps réel dans son navigateur, on peut ainsi tout à fait mettre un point d’arrêt et utiliser la console de développement. Enfin, on utilise la version de Chrome que l’on souhaite, donc plus de problème de CSS non supportés.\n\nAlors pourquoi se cantonner à n’utiliser Chrome+ChromeDriver qu’en local et pas en intégration continue sur Jenkins ? Chrome n’est pas un browser headless et a besoin d’une interface visuelle qui n’est pas disponible sur Jenkins. Il existe des solutions pour simuler un affichage graphique avec Xvfb par exemple. Nous avons tenté de mettre en place une telle stack sur l’image docker utilisée pour créer notre environnement de test sur Jenkins en se basant sur l’image de Rob Cherry. Malheureusement, après y avoir consacré un peu d’énergie, le résultat n’a pas été au rendez-vous car :\n\n\n  l’exécution des tests dans Chrome est bien plus lente que sur PhantomJS (2 à 3 fois plus lent), notre intégration continue prenant déjà plus de 10 minutes sur ce projet,\n  il semble difficile d’obtenir ici aussi une stabilité du dispositif, les sessions Webdriver étaient souvent perdues, sans que nous en trouvions la cause.\n\n\nCes raisons nous ont conduit à abandonner cette piste.\n\nQuelques tips pour améliorer la stabilité de ses tests\n\nNous avons continué d’espérer avoir une stack stable pour nos tests fonctionnels. Avec persévérance, nous pouvons dire qu’à l’heure actuelle grâce à ces quelques tips, nous avons une plateforme de test stable (à 99%) !\n\nwaitUntil\nC’est la première chose à faire et la plus importante de notre point de vue. On ne sait jamais vraiment quand un élément s’affichera dans la page car son chargement dépend de trop de facteurs non prédictibles (la connexion, l’utilisation cpu, gpu, mémoire, etc.). Sur notre projet, nous avons par exemple beaucoup d’animations CSS qui retardent le timing d’apparition des pages et des éléments du DOM. Notre première approche a été de rajouter des sleep un peu de partout dans nos tests. Chose à ne pas faire. L’usage des sleep doit être cantonné à des cas très spécifiques. Pour tout le reste, il faut user et abuser du waitUntil de WebdriverIO, que ce soit pour des actions ou des vérifications dans la page, et en adaptant le timeout à votre projet (certaines de nos animations sont assez longues).\n\nrollover\nUn autre problème que nous avons rencontré est la bonne exécution des rollovers. En utilisant la méthode moveToObject pour pointer la souris sur un élément, il nous arrivait que le comportement “hover” ne soit pas déclenché, mettant en échec la suite du test. Nous avons donc changé notre manière d’effectuer le rollover : on répète l’action grâce au waitUntil tant que l’élement devant apparaître au hover n’est pas visible.\n\nNous n’écrivons plus\n\nI rollover the &quot;Header login icon&quot;\n\nmais\n\nI rollover the &quot;Header login icon&quot; to make &quot;Submenu&quot; appear\n\nrerun\n“Rerun” est une fonctionnalité existante sur d’autres frameworks de tests fonctionnels tel que Behat et créée pour les tests récalcitrants encore instables. Elle permet de stocker dans un fichier texte la liste des scénarios en échec pour les relancer ensuite afin de vérifier qu’ils le sont réellement. Nous avons mis en place ce process sur Jenkins, bien qu’il y ait quelques subtilités qui ne facilitent pas la tâche (mais qui devraient être bientôt corrigées), et nous en sommes satisfaits.\n\nisVisible\nA nos débuts, nous avons eu quelques problèmes avec la fonction isVisible de WebdriverIO car les éléments opaques ou en dehors du viewport étaient considérés comme visibles. Nous avons alors choisi d’utiliser une fonction custom injectée via execute. Récemment, dans la version 3 de WebdriverIO, la fonction isVisibleWithinViewport a fait son apparition mais nous n’avons pas encore tenté de l’utiliser dans nos tests.\n\nCet article est un retour d’expérience sur notre usage des tests fonctionnels sur un projet précis mais il est loin d’exposer des vérités absolues. Si vous avez des remarques ou n’êtes pas d’accord avec certaines choses, n’hésitez pas à nous le faire savoir !\n"
} ,
  
  {
    "title"    : "L&#39;envers du décor du nouveau 6play",
    "category" : "",
    "tags"     : " 6play, REST, Symfony, Elasticsearch, Cassandra",
    "url"      : "/2015/11/30/beta-nouveau-6play-backend.html",
    "date"     : "November 30, 2015",
    "excerpt"  : "Il y a quelques semaines, nous vous parlions ici même de la stack technique mise en place pour le nouveau front web de 6play.\n\nAujourd’hui, nous vous proposons un retour sur ce qui a été mis en place côté backend pour assurer la mise à disposition...",
  "content"  : "Il y a quelques semaines, nous vous parlions ici même de la stack technique mise en place pour le nouveau front web de 6play.\n\nAujourd’hui, nous vous proposons un retour sur ce qui a été mis en place côté backend pour assurer la mise à disposition des données aux différents frontaux 6play.\n\nTout d’abord, il faut commencer par expliquer que l’univers 6play ne se résume pas que à son application web. Il existe aussi une version iOS et Android, mais également une version par Box IPTV (disons une version par FAI).\n\nPas mal de REST …\n\nC’est donc tout naturellement que nous sommes partis sur la mise à disposition d’une API REST permettant à ces différents fronts de consommer simplement les données.\n\nNotre stack technique habituelle côté backend étant Symfony2, nous sommes donc partis sur ce framework, ainsi que les habituels bundles :\n\n\n  FOSRestBundle pour la gestion simple des controlleurs REST (validation des paramètres, routing adapté, view au format JSON, gestion des retours d’erreur)\n  BazingaHateoasBundle pour intégrer les liens entres les différents endpoints directement dans les différentes réponses.\n  NelmioApiDocBundle pour proposer une documentation complète et auto-générée depuis le code\n\n\nPour sécuriser tout ça, nous utilisons toujours notre bundle DomainUserBundle permettant de sécuriser et contextualiser les données par sous-domaine (voir notre article dédié à ce bundle).\n\n… mais pas que\n\nUne fois mise en place la théorie brute, nous nous sommes heurtés à la réalité des choses : face à un modèle de données complexe, si on reste très strict face à la philosophie RESTful, cela peux demander aux clients de réaliser un nombre conséquent de requêtes afin d’afficher une simple page.\n\nAinsi, nous avons un second applicatif, que nous nommons “middleware” qui est un hybride entre une API REST et un catalogue de données préformaté. Dans cet applicatif, nous réalisons les agrégations qui permettent de récupérer de manière unifiée les données liées, permettant aux frontaux de réduire leurs appels.\n\nDans ce middleware, nous essayons tout de même de respecter au maximum les verbes HTTP et le format de retour pour que les utilisateurs de ces API obtiennent des réponses cohérentes d’un service sur l’autre.\n\nDes données élastiques\n\nPour que ce middleware puisse retourner des données qui sont stockées dans plusieurs tables, de manière rapide, tout en gérant les contraintes de données non publiées (notre SI contient les anciennes émissions diffusées, mais également celles à diffuser), nous avons fait le choix d’utiliser Elasticsearch en le remplissant avec les données “publiables”.\n\nNon seulement nous disposons d’un système de recherche de données très performant, permettant des requêtes très puissantes et très rapides, dans lequel les données sont stockées de manière optimisée pour l’utilisation (pas de forme normale à respecter), mais nous nous permettons de n’y stocker que les données disponibles publiquement, simplifiant donc grandement les requêtes sur ces données.\n\nWorkerize all the things\n\nPour maintenir les données à jour dans cet index Elasticsearch, nous avons mutualisé sur l’expérience et le travail que nous avions réalisé pour RisingStar, qui nous a apporté l’expérience que des daemons sont beaucoup plus efficaces que des crons. Cette technique nous apporte plusieurs avantages :\n\n\n  Scalabilité : il est facilement possible de multiplier les process qui traitent les données, et donc d’augmenter la capacité de traitement\n  Rapidité : le fait d’avoir des daemons qui tournent en continue permet de traiter les demandes dès leur arrivée, et pas lors de la minute suivante. Cela permet aussi de lisser au maximum les traitements sans créer de piles d’attente inutiles.\n\n\nNous nous sommes donc appuyés sur notre DaemonBundle pour mettre en place un double système d’indexation :\n\n\n  une fois par jour, l’index est complétement reconstruit\n  un daemon tourne en continue pour détecter les modifications en base de données, et envoyer des messages dans une file RabbitMQ\n  un dernier daemon est dédié au traitement des messages de cette file pour mettre à jour de manière ciblée les données dans Elasticsearch\n\n\nAinsi, nous assurons une fraicheur des données quasi-immédiate et optimale.\n\nAu cours de ce travail, nous avons construit 2 nouveaux bundle : ElasticsearchBundle et AmqpBundle. L’un comme l’autre sont des bundles permettant de faciliter la configuration et l’utilisation des clients natifs dans Symfony2, en tant que service.\n\nEt la grosse donnée ?\n\nSi vous avez essayé la nouvelle version web de 6play, vous avez certainement remarqué que la personnalisation de votre compte est fortement mise en avant. Pour stocker ce fort volume de données, nous avons fait le choix d’utiliser Cassandra, pour son approche distribuée permettant une forte scalabilité, et un ratio rapidité/redondance optimal.\n\nComme pour le reste, nous avons là aussi créé un bundle Symfony2 permettant de configurer et manipuler simplement des clients Cassandra en tant que service : CassandraBundle\n\nTout le reste\n\nCôté monitoring, pour respecter nos bonnes habitudes, nous utilisons toujours Statsd à outrance, surtout via notre bundle StatsdBundle.\n\nCôté tests, tous les tests unitaires ont été écrits avec atoum.\n\nConclusion\n\nAu cours de ce projet, nous avons eu l’occasion de transformer l’essai de beaucoup de choses que nous avions faites pour RisingStar, de découvrir de nouvelles technos et de mettre en place une architecture moderne et adaptée aux nouveaux challenges des fronts.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #9",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2015/11/06/m6web-dev-facts-9.html",
    "date"     : "November 6, 2015",
    "excerpt"  : "C’est le digital\n\n\n  On a le doigt quelque part …\n\n\nUne vague histoire de pouce\n\n\n  Sache que sans ton pouce, je ne fais rien. (et non, c’est pas dégueulasse)\n\n\nJe croyais que ça rendait sourd plutôt\n\n\n  N’empeche je me trompe toujours quand je li...",
  "content"  : "C’est le digital\n\n\n  On a le doigt quelque part …\n\n\nUne vague histoire de pouce\n\n\n  Sache que sans ton pouce, je ne fais rien. (et non, c’est pas dégueulasse)\n\n\nJe croyais que ça rendait sourd plutôt\n\n\n  N’empeche je me trompe toujours quand je lis pom.xml hein\n\n\nDyslexie\n\n\n  \n    X : Il parait que Marven n’arrive pas à hériter de la config de java par défaut °°\n    Y : marven ? connais pas\n    Z : c’est l’équivalent Java de comrposer :)\n  \n\n\nRégime Fifa\n\n\n  Mince, j’ai oublié de finir de manger …\n\n\nLa faille\n\n\n  Le plus long dans cette MEP, ça va être de se connecter au Wifi\n\n\nSoif\n\n\n  \n    X : Rappel important : on a 12 litres de rosé à boire avant vendredi prochain (et un pack de heinekein).\n    X : si on ne s’y prend pas en avance ça va être la panique\n  \n\n\nLa prod de la dev ?\n\n\n  Est ce qu’on a la base de données de dev de prod ?\n\n\nError or not error\n\n\n  \n    X : c’est parce que tu pars du principe que le message d’erreur a un rapport avec l’erreur\n    Y : euh ce qui devrait être le cas non ?\n    X : ouais, mais bon…\n  \n\n\nC’était un message digital\n\n\n  \n    X : pour les massage d’erreur\n    Y : Les massages d’erreur ?\n    Z : humm les massages\n    Y : La première partie me tente, mais d’erreur ?\n    X : un laius digital\n    Y : oula … massage … digital\n  \n\n\nEn Famille\n\n\n  Les mecs ils regardent leur code ! On dirait que c’est leur photo de famille\n\n\nDeployception\n\n\n  Pour tester le deployer, il faut le deployer en test, mais pour ça, il faut deployer le deployer en prod pour pouvoir deployer en test\n\n\nFormalisme\n\n\n  En tant qu’ordinateur ayant accès à l’INTERNET MONDIAL et voulant naviguer sur le site internet de M6 se nommant 6PALY.FR mettre un titre dans la balise &amp;lt;title&amp;gt; … merci\n\n\nLes gouts et les couleurs\n\n\n  Moi, je ne fais pas confiance aux technos qui ont moins de 15 ans. Sauf si elles viennent du Brésil\n\n\nL’heure est à la blague\n\n\n  C’est l’heure dammer …\n\n\nUne vérité vrai\n\n\n  Quand on dit “24h” généralement c’est 24h\n\n\nAuto-troll\n\n\n  J’ai créé un fichier “echoUrl” qui écrit dans un fichier …\n\n\nIncohérence cohérente\n\n\n  Comme ça, on est cohérent dans l’incohérence !\n\n"
} ,
  
  {
    "title"    : "La bêta du nouveau 6play est disponible",
    "category" : "",
    "tags"     : " 6play, react, isomorphic, javascript, flux",
    "url"      : "/2015/10/21/beta-nouveau-6play-react-isomorphic.html",
    "date"     : "October 21, 2015",
    "excerpt"  : "Nous vous parlions en fin d’année dernière sur ce blog, de notre vision de la Single Page App parfaite.\n\nNous avons donc travaillé depuis le début d’année à la mise en place du nouveau 6play sur cette stack technologique :\n\n\n  React(isomorphic/uni...",
  "content"  : "Nous vous parlions en fin d’année dernière sur ce blog, de notre vision de la Single Page App parfaite.\n\nNous avons donc travaillé depuis le début d’année à la mise en place du nouveau 6play sur cette stack technologique :\n\n\n  React(isomorphic/universal) avec du Node.js en backend\n  Fluxible pour la gestion de Flux client et serveur\n  webpack pour la gestion du build js coté client\n  React Router pour le routing\n  ES6 avec Babel parce que.\n\n\nAu niveau tests, et parce que nous ne concevons plus de développer de tels projets sans une approche qualité complète :\n\n\n  ESLint pour le respect des conventions de codage\n  Jest pour les tests unitaires\n  Cucumber.js, WebdriverIO et PhantomJS pour les tests fonctionnels\n  superagent-mock (\\o/) pour mocker les requêtes HTTP des services externes\n  Jenkins pour l’intégration continue\n  React Hot Loader pour améliorer la DX (Developer eXperience)\n\n\nDepuis lundi, vous pouvez désormais tester la bêta de ce service vidéo à l’adresse suivante : https://beta.6play.fr\n\n\n\nPour ceux qui veulent en savoir plus sur cette refonte (notamment front-end), une conférence sera tenue par Kenny Dits (@kenny_dee) lors de Blend Web Mix, le 29 octobre à Lyon à 16h.\n"
} ,
  
  {
    "title"    : "Simplifiez vous la vie avec les hooks Git",
    "category" : "",
    "tags"     : " git, hooks, workflow, composer, coke",
    "url"      : "/2015/09/09/hooks-git.html",
    "date"     : "September 9, 2015",
    "excerpt"  : "La stack de base de tout projet un minimum sérieux à tendance à devenir de plus en plus lourde.\n\nChez M6Web, et particulièrement dans la team Burton, nous développons principalement des projets Symfony2.\nCette stack est donc composée, entre autre ...",
  "content"  : "La stack de base de tout projet un minimum sérieux à tendance à devenir de plus en plus lourde.\n\nChez M6Web, et particulièrement dans la team Burton, nous développons principalement des projets Symfony2.\nCette stack est donc composée, entre autre de coke et de composer.\n\nConcernant coke, l’idée est de ne jamais versionner de code qui ne respecte pas les standards de développements.\nInterdit les commits “fix standards” ou autre “fix coke” qui alimentent une PR avant d’être rebasés !\n\nConcernant composer, il s’agit de toujours travailler sur la “bonne version” des dépendances. \nCertes, composer (via le composer.lock) permet d’être sûr que toute personne qui lance un composer install aura la même version des dépendances.\nToutefois, il faut encore penser à lancer cette commande, surtout lorsque l’on récupère du code depuis le repository central où le composer.lock peut avoir évolué.\n\nPour répondre à ces besoins, nous nous sommes basés sur l’excellent système de hooks de git et nous avons développé 2 petits scripts.\n\ncheck-coke.sh\n\nCoke est un petit utilitaire qui facilite le lancement de PHP_CodeSniffer sur son projet.\nToutefois, ce dernier est relativement lent, surtout sur les projets composés d’un grand nombre de fichiers.\nPour optimiser son exécution, le script “check-code.sh” se charge de lancer coke uniquement sur les fichiers en cours de modification d’un point de vue Git.\nAinsi, son exécution est extrêment rapide, ce qui permet de l’exécuter en pre-commit pour ne jamais commiter un code ne respectant pas les standards de développement\n\ncheck-composer\n\nComme indiqué plus haut, le soucis avec composer est qu’il faut penser à lancer la commande composer install pour s’assurer que les dépendances sont à jour.\nNon seulement cette commande prend un certains temps, mais il faut surtout penser à la lancer, même pour des actions qui semblent anodines, comme changer de branche.\n\nPartant de ce constat, nous avons créé le script “check-composer.sh”, qui vérifie s’il y a une différence entre la version de départ et d’arrivée du fichier composer.lock, et qui lance la commande composer install si nécessaire.\n\nN’hésitez pas à les essayer et nous faire part de vos retours, voir de proposer vos hooks.\nLe but de ce repository partagé est de nous simplifier la vie en nous permettant de ne plus penser aux outils qui sont autour de notre code, mais de nous concentrer sur ce que nous avons à faire.\n"
} ,
  
  {
    "title"    : "CR React Europe Conférence 2015 - Day 2",
    "category" : "",
    "tags"     : " javascript, react, reactnative, video, graphql",
    "url"      : "/2015/07/10/cr-react-europe-2015-day-two.html",
    "date"     : "July 10, 2015",
    "excerpt"  : "Après une première journée pleine de nouveautés et d’annonces, voici la suite du compte rendu avec un programme encore très chargé pour cette deuxième journée de la React Europe.\n\nImproving Your Workflow With Code Transformation\n\n\n(crédits : Fabie...",
  "content"  : "Après une première journée pleine de nouveautés et d’annonces, voici la suite du compte rendu avec un programme encore très chargé pour cette deuxième journée de la React Europe.\n\nImproving Your Workflow With Code Transformation\n\n\n(crédits : Fabien Champigny - React Europe)\n\nNous commençons la journée avec le créateur du fameux Babel: Sebastian McKenzie.\nBabel est un transpiler JS permettant de transformer le code ES6/7 en code ES5.\nAprès un petit historique sur le nom, car cet outil s’appelait 6to5 avant l’arrivée d’ES7, pour finalement se renommer Babel :)\nL’adoption par la communauté a ensuite été assez massive !\n\nSébastian nous explique le fonctionnement interne de Babel avec le découpage en 3 sections : Parser / Transformer / Generator\nIls utilisent l’AST (abstract syntax tree) pour avoir une “data structure” du code et pouvoir faire des traitements sur cette structure.\nOn rentre très (trop ?) en profondeur dans les bas-fond de Babel, afin de partager les différentes difficultés et trucs et astuces pour les transformations que Babel réalise.\n\nLe talk se finit sur le futur de Babel, qui sera à chercher du côté de :\n\n\n  Dead code elimination/minification\n  constant folding/static evaluation\n  static analysis / linting\n\n\nSteven Lusher, l’un des dev Facebook travaillant sur Relay vient de mettre un blog post sur le site Babel concernant l’utilisation ES6 de React\n\n\n\nThe State of Animation in React\n\n\n(crédits : Fabien Champigny - React Europe)\n\n\n\nCheng Lou fait le point sur les animations en React et nous présente sa nouvelle approche react-motion.\n\nIl est convaincu qu’il faut abandonner les ReactCssTransitionGroup au profit des animations basées sur des interpolations “à-la-flash”.\n\nCSS Transitions\nLes Transitions CSS présentent plusieurs défaut, elles sont difficiles à controler et elles sont étroitement liées au DOM. En revanche, elles sont plutôt performantes, non bloquantes et répondent à la plupart des usages.\n\nDeclarative Tweens\nLes Declarative Tweens sont une solution alternative interessante qui permettent de composer une animation selon des critères précis (début, durée, direction, …). Cette solution présente aussi l’avantage de pouvoir créer des animations composées de plusieurs sous-animations et d’interrompre leur execution sur demande.\n\nSpring\nCheng Lou a développé une librairie appelée &amp;lt;Spring /&amp;gt; qui parlera aux anciens Flasheurs, tout comme lui, puisqu’elle reprend le principe d’interpolation et de courbes. Elle permet de définir une animation sur un composant React, en précisant sur chacune de ses propriétés des critères simples de transformation. La librairie se charge d’interpoler la structure du composant pour une animation fluide.\n\ndémo de Spring\n\n\n\nSimplifying the data layer\n\n\n(crédits : Fabien Champigny - React Europe)\n\nKevin Robinson nous présente comment Twitter utilise React. L’approche présentée fait la part belle au “fonctionnel”, à l’image de leur infrastructure backend.\n\nIl nous détaille les mécanismes mis en place pour l’accès aux données, notamment la possibilité de gérer de manière déclarative les dépendances aux données au niveau des composants.\nAu niveau des stores, il prone l’utilisation de structures immuables en stockant un “log” d’événement et en utilisant des “reducers” pour en extraire l’état des données réel.\n\nOn retrouve dans leur approche beaucoup de concepts de la programmation fonctionnel (de la même manière que Redux), mais malheureusement, aucun code n’est ouvert par Twitter à ce sujet.\n\nOn reste un peu sur notre faim en ne pouvant pas aller jouer “concrètement” avec leurs outils.\n\nGoing Mobile with React\n\n\n(crédits : Fabien Champigny - React Europe)\n\nJed Watson, créateur du framework TouchStone JS, un framework JS (basé sur React) permettant de faire des applications mobiles hybride (à base de Webview via Apache Cordova), nous explique comment réaliser des applis hybride grâce à React.\n\nLe débat ici est plutôt de démontrer qu’on peut malgré les dires de certains et en connaissant quelques astuces, faire une appli mobile hybride qui ressemblera à une appli native. Pour nous prouver cela, Jed annonce que l’appli de la React Europe, dispo sur iOS et Android, et que nous avons tous utilisé a été faite avec TouchStone JS !\n\n\n  If you have great developer experience, you are much more likely to get to a great UX\n\n\nJed conseille de ne pas faire de l’hybride lorsqu’on :\n\n\n  est Facebook ou Twitter\n  a beaucoup de données\n  a une utilisation processeur intensive\n  a des animations complexes sur l’UI\n  a des interactions complexes\n  a de la gestion mémoire avancée\n\n\nLes points les plus importants pour qu’une application hybride fonctionne sont :\n\n\n  React\n  la gestion du Touch\n  le Layout\n  la gestion de la Nav\n\n\n\n  You should not do everything in a webview, but you can\n\n\nLa démo présente l’ensemble des composants et des transitions disponibles. En plus de React, Touchstone.js utilise cordova, une bibliothèque d’APIs permettant d’accéder en Javascript aux fonctions natives du mobile, comme l’accéléromètre, le GPS ou l’appareil photo.\n\nLe code de l’application React Europe sera rendu open-source à la fin de la démo : Sketch &amp;amp; Code de l’app React Europe\n\n\n\nReact Router\n\n\n(crédits : Fabien Champigny - React Europe)\n\nBelle présentation de Michael Jackson (@mjackson), pas aussi spectaculaire qu’un concert de l’artiste homonyme, mais assez surprenante et délirante quand même ! Il introduit la librairie qu’il porte avec Ryan Florence (@ryanflorence) depuis plus d’un an et qui est majoritairement utilisée par les utilisateurs de React pour mettre du routing dans leur application : React Router, “The” Router.\n\nMichael commence par nous présenter les bases de la librairie : la définition des routes dans un composant React et le composant Link permettant de générer les liens. Il explique que ce sont des concepts simples et qu’ils permettent à de nouveaux développeurs peu expérimentés de rentrer facilement dans un projet.\n\nIl fait ensuite l’analogie entre les vues et les URLs, affirmant que de bonnes URLs, bien formées, augmentent la confiance de l’utilisateur envers l’application. Une notion importante dans React Router est celle des transitions permettant de changer de vues (et donc d’URLs) et de gérer le “browser history”.\n\nMichael nous annonce une nouveauté dans la prochaine version : l’attribut onEnter sur la définition de route, permettant d’executer une callback avant d’afficher la page (utile par exemple pour protéger une page par authentification).\n\nIl nous expose sa vision du composant comme une fonction prenant en entrée props et state et renvoyant en sortie une UI. Le router n’est finalement qu’un composant comme un autre qui reçoit en entrée l’URL. L’idée que ce qui est explicite est bien meilleur que ce qui est “magique” dans une implémentation lui permet de présenter les changements de l’API dans les dernières versions du React Router avec la récupération des paramètres de l’URL via les props du composant (et plus via une mixin) ou la disparition du composant RouteHandler qui peut simplement être remplacé par props.children pour utiliser les “nested routes” dans ses composants.\n\nDans les travaux en cours, on retiendra les transitions animées qui permettent à Michael de faire une démo “wahou”. L’animation est, bien entendu, répétée en sens inverse sur l’utilisation du back du navigateur. Cette fonctionnalité a d’autant plus d’importance que changement d’URLs et animations ne sont traditionnellement pas de bons amis et posent souvent problème.\n\nLe “dynamic routing” est la deuxième démo montrant la possibilité de contextualiser l’ouverture d’une URL : sur une page, un contenu peut être ouvert dans une popup, mais en copiant et ouvrant l’URL obtenue dans un autre onglet, on a une page avec le même contenu mais une présentation différente (plus de popup).\n\nEnfin, le clou du spectacle sera la dernière démo qui échouera (un dernier commit sur le repo qui aurait provoqué une erreur) qui nous aura valu une fabuleuse danse de Ryan Florence sur la scène (venu en renfort de Michael) ! L’idée initiale était de présenter une fonctionnalité assez énorme permettant de lazy loader les JS de sa SPA en fonction des besoins de chaque route (le “gradual loading”) évitant de charger dès le départ les 3Mo de son bundle webpack alors qu’on en utilise qu’une petite partie. Il faudra attendre pour voir cette fonctionnalité en action…\n\nCreating a GraphQL Server\n\nAprès la conférence de la veille sur GraphQL, Nick Schrock et Dan Schafer nous montrent comment réaliser un serveur GraphQL.\n\nGraphQL est une spécification d’échange et ne présuppose aucune technologie backend. Comme nous l’avons déjà vu dans la conférence précédente, l’idée est de faire de GraphQL une couche entre le client et le code backend déjà existant.\n\nFacebook, en ouvrant cette spécification et l’implémentation de référence, espère fédérer une communauté autour de cette solution. Si d’autres personnes réalisent des implémentations dans différents langages, cela permettrait à tout le monde de capitaliser sur ces techniques et de faciliter la réutilisation de code que ce soit côté client ou serveur.\n\nLa “stack” imaginée pour GraphQL est présentée en partant du serveur jusqu’au client :\n\n\n  GraphQL App Servers\n  Libs (Parse, SQL)\n  Core\n  Spec\n  Common tools (ex: graphicQL, IDE-like tool)\n  Client SDKs (Relay)\n  GraphQL Clients\n\n\nAu delà de la présentation théorique, on se pose quand même la question de la mise en oeuvre concrête au dessus de code existant.\nFacebook utilise maintenant intensivement GraphQL. Par contre, ils n’utilisent pas l’implémentation de référence mais sans doute une implémentation très imbriquée à leur backend (et donc difficile à rendre open-source).\nOn manque malheureusement de retours sur des questions de mise en oeuvre comme le cache ou la gestion des droits par exemple.\nEspérons que des “early-adopters” puissent nous faire des retours là-dessus dans les semaines/mois à venir.\n\nPour en savoir plus, un bon article sur le sujet : GraphQL overview : Getting start with GraphQL and Node.JS\n\nIsomorphic Flux\n\n\n(crédits : Fabien Champigny - React Europe)\n\nMichael Ridgway (@theridgway) aborde une notion souvent abordée ces 2 jours et sur laquelle nous avions fait un article en décembre dernier.\n\nSelon Michael, les avantages du “server rendering” sont multiples :\n\n\n  le SEO\n  le support des anciens navigateurs\n  le gain de performance perçu par l’utilisateur\n\n\nUn des objectifs de cette démarche est de partager le maximum de code entre le serveur et le client.\n\nLa stack proposée par Michael est la suivante :\n\n\n  pour la gestion des vues, React évidemment qui expose une API client et serveur\n  pour le routing, React Router (https://github.com/rackt/react-router)\n  pour le data fetching, superagent (https://github.com/visionmedia/superagent)\n  pour la logique applicative, un pattern léger et célèbre : Flux\n\n\nPour la mise en oeuvre de Flux côté serveur, nous avons déjà vu au cours de ces 2 journées : Redux et React Nexus. Il en existe d’autres comme marty.js, flummox ou alt. Michael nous propose Fluxible, la librairie développée par Yahoo.\n\nFluxible crée un contexte pour chaque requête côté serveur avec un dispatcher custom optimisé pour cette opération. L’état de l’application est transmis du serveur vers le client grâce à un mécanisme de déshydratation/réhydratation des stores.\n\nMichael précise que Fluxible force les développeurs à utiliser Flux de manière conforme sans transgresser les pratiques définies par le modèle. La librairie fournit des composants de haut niveau permettant une parfaite intégration avec React. Enfin, la particularité de Fluxible est son système de plugins permettant de faciliter l’ajout de nouvelles fonctionnalités.\n\nMichael nous montre un exemple de chat isomorphique et la différence observée au chargement avec une SPA classique. Il précise ensuite les outils de développement qu’il utilise :\n\n\n  Babel\n  ESLint\n  webpack\n  babel-loader\n  Grunt / Gulp\n  Yeoman Generators\n\n\nMichael termine sa conf en indiquant que plusieurs applications en prod chez Yahoo utilisent la stack présentée et Fluxible mais qu’il reste encore quelques améliorations à apporter pour les raisons suivantes :\nLes dépendances des composants envers les données ne sont pas facilement connues (rendant le data fetching en amont du rendering côté serveur délicat). Relay pourrait être une solution.\nLe rendu côté serveur de React est relativement lent (mais pourrait être amélioré dans les futures version de React).\nLe Hot Reloading (avec React Hot Loader) ne fonctionne pas avec les stores Fluxible.\n\nConclusion\n\nQue dire après ces deux jours de conférence ? \nDéjà que la communauté et l’engouement autour de React ne cesse de grandir, mais aussi que ca ne chôme pas coté Facebook avec Relay, GraphQL, Animated, React Native Android qui ne devraient pas tarder à pointer le bout de leur nez, avec aussi la mise en place d’une personne full time sur Jest ! C’est rassurant sur l’avenir court/moyen terme de React.\n\nL’organisation était vraiment impeccable (mise à part les soucis de climatisation) avec beaucoup de très bonnes idées, notamment, les bureaux au fond et sur les cotés de la salle de conférence pour que les personnes avec LapTop puissent suivre confortablement.\n\nC’est aussi plutôt étonnant, pour une conférence en France, d’avoir vu aussi peu de personnes francophones. Le public étant très majoritairement anglophone. On se dit que React n’a pas encore complètement pris en France.\n\nCoté tendance, on voit qu’au niveau des librairies Flux, Redux parait clairement être celle qui attire tous les buzz. A voir dans le temps si cela suit, mais le talent indéniable de son créateur, combiné aux bonnes idées (reducers, hot reload) donne vraiment envie de s’y pencher. On regrette aussi toujours le manque de sujets autour des tests.\n\nNous attendons aussi impatiemment React Native Android, pour voir si le buzz et les superbes promesses sont toujours présentes avec deux environnements cibles, et on espère voir sur nos stores de plus en plus d’applis React Native.\n\nGraphQL + Relay parait vraiment être la solution idéale pour réaliser simplement du data fetching coté client (React Web ou React Native), mais l’absence de Relay (toujours pas open-sourcé), combinée au manque de retour sur GraphQL pose encore de nombreuses questions.\n\nNous avons donc hâte d’être à la prochaine React Conférence ou React Europe pour voir la suite de l’évolution de React.\n\nVous pouvez retrouvez le compte rendu de la première journée ici\n"
} ,
  
  {
    "title"    : "CR React Europe Conférence 2015 - Day 1",
    "category" : "",
    "tags"     : " javascript, react, reactnative, video, graphql",
    "url"      : "/2015/07/06/cr-react-europe-2015-day-one.html",
    "date"     : "July 6, 2015",
    "excerpt"  : "Après la première conférence officielle sur React, que nous avons déjà couvert en janvier (Jour 1 et Jour 2), nous nous sommes rendus les 2 et 3 juillet à Paris sous une chaleur infernale pour cette première édition de la React Europe avec l’envie...",
  "content"  : "Après la première conférence officielle sur React, que nous avons déjà couvert en janvier (Jour 1 et Jour 2), nous nous sommes rendus les 2 et 3 juillet à Paris sous une chaleur infernale pour cette première édition de la React Europe avec l’envie de voir et de mesurer les évolutions autour de ReactJS.\n\nKeynote\n\nAu départ, React c’était simplement le V de MVC. Maintenant, on parle de “View First” ou “ User interface First”.\n\nChristopher Chedeaux @vjeux, l’un des core-dev de React, va faire un focus sur 4 axes principaux :\n\n\n  Data\n  Language\n  Packager\n  Targets\n\n\n1) Data\n\nDepuis l’annonce de Flux ont fleuri beaucoup d’autres implémentations du pattern, notamment :\n\n\n  Mcfly\n  Barracks\n  Reflux\n  Fluxy\n  Fluxxor\n  Redux\n\n\nD’après Christopher, certaines vont mourir dans les prochains mois laissant seulement la place aux implémentations les plus pertinentes (et Redux a fait un buzz sans pareil lors de ces 2 jours, voir plus bas).\n\nL’immuabilité revient aussi énormément en regardant du coté de ClojureScript ou ImmutableJS.\n\nCoté Data fetching, cela commence à bouger pas mal avec :\n\n  Relay et GraphQL\n  Falcon &amp;amp; JSON Graph\n  Flux over the wire\n  Om Next\n\n\nIl reste encore les cotés Persistence et Temps réel qui ne sont pas traités dans l’écosystème de React.\n\n2) Languages\n\nLe langage JS a énormement évolué avec CoffeeScript, jsTransform (utilisé chez facebook pour la gestion du jsx, “internalization pipeline”, …)\n\n\n  “think of js as a compile target”\n\n\nIl y a eu Traceur et Recast, et désormais Babel qui a tout ecrasé sur son passage. Facebook convertit en ce moment tout son code Front JS à Babel.\n\nOn retrouve aussi ESLint, un “linter” de code, et du typage de données avec TypeScript et Flow.\n\n3) Packager\n\nNous retrouvons Node.js, CommonJS, npm. \nDans le browser : Browserify et Webpack.\nMême s’il y a encore du travail à faire pour avoir de bonnes performances, et ne pas attendre une compilation via les mises à jour incrémentales, ou React Hot Loader sur lequel nous reviendrons.\n\n4) Targets :\n\nLes cibles de React sont désormais multiples grâce au Virtual DOM :\n\n\n  DOM\n  SVG\n  Canvas\n  Terminal\n\n\nUn focus est ensuite fait sur React Native, permettant de développer des apps natives sur iOS et Android tout en faisant du React.\n\n\n  “UX of a native app / DX of a web app”\n\n\nChristopher insiste sur le terme DX qu’on ne voit jamais dans des slides tech, signifiant “Developper Experience”. \nIl compare aussi le développement de l’appli Ads de Facebook, réalisé avec React Native sur iOS (7 ingénieurs pendant 5 mois), et celui qui a suivi avec React Native Android avec les mêmes 7 ingénieurs durant seulement 3 mois en réutilisant 87% du code !\n\nReact Native Android sera open-sourcé au mois d’Août.\n\n\n  “Learn once : write anywhere”\n\n\nUn appel est fait pour stopper le “bashing” sur les autres frameworks. C’est en travaillant main dans la main entre les communautés Ember, Angular et React notamment que le web avancera.\n\n\n\n\n\nInline Styles: themes, media queries, contexts, and when it’s best to use CSS\n\nStyle are not CSS\n\nMichael Chan @chantastic va nous soumettre une “terrible” idée lors de cette conf qui va en faire crier plus d’un ! “It’s time to learn CSS” est une phrase d’une autre époque, Michael n’hésite d’ailleurs pas à qualifier cette idée de bullshit !\n\nCitant Jeremy Ashkenas @jashkenas, créateur de CoffeeScript et de Backbone.js, il soumet une nouvelle vision : unifier les 3 syntaxes (CSS, HTML et JS) qui permettent de déclarer le style d’une application web car contrairement à ce qu’on pense “le style n’est pas le CSS”.\n\nMichael défend 2 autres axes importants dans React :\n\n\n  les changements de l’état de l’application (piloté en JS via les “states” des composants) sont des changements de l’UI,\n  les composants doivent être réutilisés comme partie entière et indépendante et ne doivent pas être détournés de leur vocation initiale, “je préfère avoir 1000 composants qui font 1 choses que 100 composants qui font 2 choses”.\n\n\nStyle over the time\n\nMichael reprend ensuite l’histoire des CSS. A l’origine, on déclarait les styles dans l’attribut HTML “style”. Puis, on s’est rendu compte de cette façon que le code était dupliqué, d’où l’introduction et la déclaration des classes CSS. Le web est devenu sémantique avec l’utilisation des balises &amp;lt;h1&amp;gt;,&amp;lt;p&amp;gt;, &amp;lt;b&amp;gt;, etc. séparant la présentation dans le HTML et le CSS. L’arrivée du web 2.0 a donné au JS le moyen d’intérargir avec le HTML pour diriger le comportement de l’application complétant la couche présentation HTML + CSS.\n\nNot coupled state\n\nAvec le web interactif actuel, l’état de l’application est noyé entre ces 3 parties constituantes. Heureusement, React permet d’organiser la structure en faisant du “state” la partie centrale de l’application et le “markup”, confondu avec le JS, devient l’interface. Néanmoins, l’état de l’application est toujours couplé avec la présentation et le CSS. Grâce à l’exemple d’une todolist basique, Michael explique comment extraire le “state” des CSS (représenté par la classe “is-complete”) pour l’intégrer en inline dans le render du composant React. Le CSS devient uniquement une couche gérant l’apparence de l’application et les composants (donc le JS) gère intégralement leur état.\n\nNo more CSS\n\nMichael nous montre enfin comment aller plus loin en gérant variables de style, pseudo-classes et pseudo-elements en inline dans le composant, et sans trop de difficultés. La gestion des hovers et des media queries est beaucoup plus ardue et n’est clairement pas recommandé. L’utilisation d’une librairie comme Radium (mais il en existe d’autres) permet de surmonter cet obstacle et d’écrire du style inline très clairement. On aborde quelques conseils pour gérer au mieux les couleurs et le layout. Pour voir un exemple illustrant tous les concepts abordés par Michael, vous pouvez explorer son projet React Soundplayer.\n\nPour conclure sa conf, Michael cite Sandi Metz @sandimetz, designeuse Ruby, défendant l’idée que l’objectif du design est de permettre de (re-)designer plus tard son application et donc de réduire les coûts du changement. Le composant React est l’interface, il se suffit à lui-même.\n\nLes slides sur SpeakerDeck\n\n\n\nFlux over the Wire\n\nElie Rotenberg @elierotenberg introduit Flux, le pattern créé par Facebook massivement utilisé avec React pour gérer le cycle de vie des données à l’intérieur de son application. Le fondement de Flux est de pouvoir partager les états de l’application (les “states”) de façon simple et scalable entre l’ensemble de ses composants car tous n’ont pas que des répercussions locales.\n\nElie nous montre qu’on peut voir Flux comme un modèle symétrique : les composants React sont le miroir des stores (là où sont stockés les states de l’application) et les actions déclenchées par les composants sont le pendant des évènements de mise à jour des stores. Le pattern tourne donc autour de 4 méthodes “symétriques” : onUpdate/dispatch côté composant et onDisptach/update côté store. La nouveauté mise en exergue par Elie est de considérer que le flux entre les composants et les stores peut être implémenté par n’importe quel canal de communication : callbacks/promises par exemple mais aussi streams/EventEmitter et, plus étonnant, websockets. Ce dernier canal permettrait de partager l’état de son application entre plusieurs composants existants sur de multiples clients grâce aux stores qui persisteraient sur un serveur node distant. Elie donne l’exemple d’un chat fonctionnant sur ce principe.\n\nIl présente ensuite les librairies qu’il a élaboré autour de ses idées :\n\n\n  nexus-flux implémentant le pattern Flux de manière “classique”, notamment autour de l’EventEmitter,\n  nexus-flux-socket.io, l’implémentation de Flux autour des websockets,\n  react-nexus une surcouche aux précédentes librairies permettant d’écouter les stores depuis les composants React en utilisant les decorators ES7,\n  react-nexus-chat, l’implémentation du chat donné en exemple.\n\n\nUne des forces de sa librairie est la facilité à mettre en oeuvre l’asynchronisme des actions Flux côté serveur.\n\nEnfin, on découvre l’utilisation réel de ces concepts chez Webedia :\n\n\n  Utilisation de PostgreSQL, Redis et Varnish pour la tenue en charge,\n  React Nexus est utilisé pour la gestion des commentaires et le système utilisateur de millenium.org,\n  Une refonte complète de jeuxvideo.com est en cours avec React Nexus,\n  Des modules React sont déjà présents sur d’autres sites de Webedia.\n\n\nLes slides sur SpeakerDeck\n\n\n\nReact Native: Building Fluid User Experiences\n\nSpencer Ahrens @sahrens2012 de chez Facebook nous présente une librairie, qui devrait être open sourcé sous peu pour gérer les animations dans React Native iOS : Animated.\n\n \nvar { Animated } = require(‘react-native’) \n\nCette librairie devrait marcher directement sur React Native Android et arriver ensuite sur le web.\nL’implémentation est 100% JS.\nNous avons suivi un live coding démo sur iOS d’une application sans animation au départ, consistant à enrichir l’expérience utilisateur en rajoutant des animations fluides via la librairie Animated.\n\nLe code des exemples et les slides, ainsi qu’un nouvel exemple sur l’animation “Tinder”\n\n\n\nExploring GraphQL + Relay: An Application Framework For React\n\n\n\nLee Byron @leeb a introduit GraphQL, une solution permettant de résoudre les problématiques d’accès aux données.\nL’idée est de résoudre les problèmes de l’approche RESTful (qui entraîne beaucoup d’aller-retours avec le serveur) et l’approche FQL (variante de SQL permettant de limiter les aller-retours, mais très compliquée à maintenir).\n\nGraphQL permet au client de définir très précisément les données qu’il souhaite obtenir via leur relations.\n\nLe principe de base est que la structure de la requête permet de définir le format de la réponse. Ex :\n\nQuery\n{\n  user(id: 4) {\n    id,\n    name,\n    smallPic: profilePic(size: 64),\n    bigPic: profilePic(size: 1024)\n  }\n}\n\nResponse\n{\n  &quot;user&quot;: {\n    &quot;id&quot;: 4,\n    &quot;name&quot;: &quot;Mark&quot;,\n    &quot;smallPic&quot;: &quot;https://cdn.site.io/pic-4-64.jpg&quot;,\n    &quot;bigPic&quot;: &quot;https://cdn.site.io/pic-4-1024.jpg&quot;\n  }\n}\n\nLe tout donne un code très facile à lire et à raisonner. Le serveur expose un schéma des données disponibles, ce qui permet :\n\n\n  au client de construire sa requête et de la valider\n  de générer du code côté client à partir du schéma\n  une bonne intégration dans les IDE (autocompletion)\n  génération d’une API Doc\n\n\nGraphQL ne s’occupe pas du stockage, c’est uniquement la couche de requêtage qui peut être implémentée avec votre code actuel.\n\nGraphQL est utilisé depuis plus de 3 ans chez Facebook et sert à l’heure actuelle environ 260 milliards de requêtes par jour.\n\nLee Byron a annoncé lors de sa conférence la diffusion d’un “working draft” d’une RFC GraphQL, ainsi qu’une implémentation de référence en Javascript.\n\n\n\nSuite à cette présentation de GraphQL, Joseph Savona introduit Relay, un framework proposé par Facebook qui permet de gérer côté client le data-fetching via GraphQL dans les applications React.\nLe principe de Relay est que chaque composant définit ses propres dépendances en utilisant le langage de requête de GraphQL. Les données sont mises à disposition dans le composant dans this.props par Relay.\n\nLe développeur fait ses composants React naturellement, et Relay s’occupe de composer les requêtes, permettant ainsi de fournir à chaque composant les données précises dont il a besoin (et pas plus), de mettre à jour les composants quand les données changent et de maintenir un store côté client (cache) avec toutes les données.\n\n\n\nDon’t Rewrite, React!\n\n\n\nRyan Florence @ryanflorence nous propose de profiter de la réécriture de code d’application historique pour introduire de nouvelles technologies et outils.\n\nLe problème avec les réécritures est que l’on est généralement obligé de le faire pour des morceaux assez important de l’application (en partant du haut de l’arbre fonctionnel de l’application). Cela peut bloquer la correction de bug sur le code historique, empêcher de faire quelques évolutions, obliger à maintenir des branches “à longue durée de vie”,…\n\nAu lieu d’utiliser cette approche, de haut en bas, Ryan nous propose d’utiliser React en partant du bas de l’arbre, c’est à dire par une fonctionnalité unitaire très limitée.\n\nReact se prête parfaitement à ce type de travail puisque son design permet de l’utiliser dans un contexte isolé très facilement. Petit à petit, on arrive à remonter de plus en plus, en réécrivant des fonctionnalités de plus en plus importantes, jusqu’à avoir réécrit l’application complète.\n\n\n\nLive React: Hot Reloading with Time Travel\n\n\n\nDan Abramov @dan_abramov nous présente son workflow React.\nIl est notamment le créateur de React Hot Loader, et de Redux, l’une des dernières implémentations de Flux jouissant déjà d’une très grande popularité.\n\nL’un des messages à retenir de sa présentation est l’importance de travailler sur ses outils de développement afin d’avoir plus de temps à passer sur ses applications.\n\nQuelques outils pour accélérer le workflow de développement :\n\n\n  amok\n  figwheel\n  livereactload\n  React Hot Loader\n  webpack\n\n\nNous faisons ensuite un focus sur son workflow autour de ces principaux outils :\n\n\n  Redux\n  Redux Dev Tools\n  React Hot Loader\n  webpack\n\n\nReact Hot Loader permet de rafraîchir son application instantanément à chaque modification de code, et ce, sans refresh de page, uniquement en rafraichissement les composants ayant changé !\nC’est très impressionnant en Live démo !\n\nRajouter à ça le Redux Dev Tools qui permet de suivre en temps réel les actions lancées, ainsi que l’état des states, de pouvoir revenir en arrière dans les actions “à la git”, mais aussi d’avoir un error handler très quali en live (inspiré j’imagine de la gestion d’erreur de React Native).\n\nL’idée derrière Redux (son implémentation du pattern Flux) est de faire un Store immuable. On peut résumer une action à une fonction prenant en entrée un état du store et donnant en sortie un nouvel état du Store (sans toucher au premier). En partant de ce principe, appliquer une série d’actions revient simplement à effectuer une réduction (un “reduce”). \nOn applique ici les principes d’Event Sourcing.\nL’immuabilité permet de stocker les différents états intermédiaires du store et donc de naviguer extrêmement facilement dans les différentes versions pendant le développement.\n\nPlus d’infos ici : The evolution of flux\n\n\n  Reducer + Flux = Redux\n\n\n\n\nBack to Text UI\n\nMikhail Davydov @azproduction a eu l’idée folle de créer une interface texte pour le terminal avec les outils web : HTML, CSS, JS et donc React.\nC’est complétement fou, assez impressionnant, mais on se demande quand même pourquoi ?\n\nVoir les slides\n\n\n\nLightning Talk\n\nPour finir la journée, nous avons eu le droit à quelques Lightning Talk de qualité inégale, abordant l’intégration de D3 avec React, de l’outil Cosmos permettant de tester dans un browser ses composants React un par un, de React Native Playground , un bel outil pour tester facilement online dans un simulateur des applis ou exemple de code de React Native voir vidéo du LT, et Turbine une sorte de remplacant de Relay en l’attendant (voir cet article).\n\nConclusion\n\nExcellente organisation (et on ne dit pas ca seulement pour les bières à volonté), un line-up du tonnerre et de belles annonces (React Native Android en Août, GraphQL etc).\nC’est déjà avec plein d’idées et de pistes d’améliorations pour nos projets React que nous sortons de ce premier jour très complet.\n\nVous pouvez retrouvez le compte rendu de la deuxième journée ici\n"
} ,
  
  {
    "title"    : "On était au PHPTour ! ",
    "category" : "",
    "tags"     : " conference, afup, phptour",
    "url"      : "/2015/06/04/m6web-au-phptour-luxembourg.html",
    "date"     : "June 4, 2015",
    "excerpt"  : "On était au PHP Tour et c’était bien !\n\n(y avait un gros gâteau et des biscuits en forme d’elephpant)\n\nLe voyage fut un peu épique, surtout les quelques kilomètres en plus quand le meilleur d’entre nous a oublié son sac à dos dans une station à 15...",
  "content"  : "On était au PHP Tour et c’était bien !\n\n(y avait un gros gâteau et des biscuits en forme d’elephpant)\n\nLe voyage fut un peu épique, surtout les quelques kilomètres en plus quand le meilleur d’entre nous a oublié son sac à dos dans une station à 150 km de là :)\n\n\n\nEt on n’a pas pu battre la team Blablacar et Jolicode au concours de levé de coudes - on est forfait les gars !\n\nPlutôt qu’un retour exhaustif (et parce qu’avec les aqueducs de Mai on cherche un peu le temps), voici quelque chose de plus informel, sur notre ressenti des tendances communautaires (forcément subjectif).\n\nRadio moquette !\n\nIl y a une bonne maturité autour des tests et du CI dans la communauté PHP. On commence aussi à voir de plus en plus des pratiques autour du partage de la responsabilité du provisionning entre ops et dev (avec Ansible et Vagrant notamment) mais, comme chez M6Web, c’est très balbutiant - et chacun a sa façon de faire. On voit des infras de dev qui passent dans le cloud (variabilisation des coûts, flexibilité, possibilité d’expérimenter). Les services managés n’ont pas la cote, on reste sur du IAAS, principalement chez AWS.\n\nDes solutions pour faire du PHP async se dessinent. Cela reste à expérimenter (libevent, ReactPHP, le tradeoff vitesse, consommation CPU étant inconnu. C’est à creuser, car cela peut sortir à moindre coût de quelques situations difficiles. L’intégration avec certaines librairies comme Guzzle est très intéressante.\n\nMySQL 5.7 est annoncé par Oracle avec pleins de features + 2x plus rapide que 5.6 et 3x que 5.5 (query) et encore plus sur le connection time. Ils annoncent une meilleur intégration avec FusionIO et ils semblent pousser des solutions de cluster multi-master (via Fabric) alors que c’était considéré expérimental avant, c’est maintenant annoncé stable.\n\nPHP7 va être important pour le langage. Pour la performance (au moins x2 vitesse, x0.5 mémoire), les nouvelles fonctionnalités (classes anonymes, scalar type hints, stricts type hints, return type declaration, exceptions on fatals, …). Presque pas de BC break, on devrait surement chez M6Web faire des tests avec la RC dès que possible et migrer rapidement quelques services à la sortie d’une stable.\n\nAnother (php) brick in the wall\n\nM6Web était représenté par Olivier qui a fait une présentation sur l’architecture backend du second écran.\n\n\n\nN’hésitez pas à commenter la conférence.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #8",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2015/05/07/m6web-dev-facts-8.html",
    "date"     : "May 7, 2015",
    "excerpt"  : "Une fois n’est pas coutume, cette nouvelle fournée des Dev Facts est publiée un jeudi. Mais c’est pour vous donner de quoi lire en ce grand week-end !\n\nEt comme ça fait longtemps, voici une grosse fournée. Enjoy !\n\nIncident planifié\n\n\n  \n    X : n...",
  "content"  : "Une fois n’est pas coutume, cette nouvelle fournée des Dev Facts est publiée un jeudi. Mais c’est pour vous donner de quoi lire en ce grand week-end !\n\nEt comme ça fait longtemps, voici une grosse fournée. Enjoy !\n\nIncident planifié\n\n\n  \n    X : normal les erreurs depuis 14h ?\n    Y : c’est pas la maintenance ?\n    Z : oui, maintenance\n    Z : ils me préviennent au moins\n    Y : un incident planifié quoi\n  \n\n\nLa finesse.\n\n\n  Ca va bien rentrer, à force qu’on leur en mette partout.\n\n\nUne bi-douille\n\n\n  Il vient de faire une bidouille pour bidouiller\n\n\nLe fullscreen fenêtré\n\n\n  Le fullscreen ne marche pas en plein écran !\n\n\nLa kalitay\n\n\n  Ils sont gardien de la qualité avec un K majuscule\n\n\nLa méthode argile\n\n\n  Nous on fait de l’agile en V\n\n\nAutomagique\n\n\n  C’est fait manuellement à la main\n\n\nLa dure loi du travail\n\n\n  J’ai même pas réussi à refourger mon travail aux autres !\n\n\nReproduction\n\n\n  Je ne sais pas si ça corrige le bug qu’on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nNos amis les belges\n\n\n  le script pour les belges, comme c’est un “one shot”, on peux dire qu’on va l’exécuter, [accent belge]une fois[/accent belge]\n\n\nProprement sale\n\n\n  C’est pas forcément plus propre, mais c’est moins sale\n\n\nPas pareil, mais différent\n\n\n  \n    X : tu peux vérifier que c’est différent ?\n    Y : Différent comment ?\n    X : bah, pas pareil …\n  \n\n\nÀ la louche !\n\n\n  C’est à peu près approximatif…\n\n\nOn a pas l’habitude\n\n\n  \n    X : Pourquoi tu penses que ça va prendre du temps ?\n    Y : Parce qu’il faut réfléchir\n  \n\n\nIl faut savoir ce qu’on veut\n\n\n  C’est pas prévu pour être utile\n\n\nC’est le Java bleu\n\n\n  Donc en fait tu regrettes Java car tu sais pas coder\n\n\nLe mythe … et la réalité\n\n\n  \n    X : C’est bien connu, quand tu installes Linux, il y a 18 mannequins topless qui défilent dans le bureau\n    Y : Oui, mais des mannequins pour linuxiens, donc bof quoi\n  \n\n\nROI !\n\n\n  y a autant d’utilisateurs que de jour homme pour ce projet !\n\n\nMais ca marche chez moi\n\n\n  \n    Y : navigation privée = pas de DRM = erreur 3365\n    Z : pas de DRM ? mais pourquoi ?\n    Z : mais sur les sites X ca marche, je comprends pas\n  \n\n\nPooh\n\nAu sujet d’une sombre histoire d’expression de besoin\n\n\n  Je ne peux pas toujours les aider à faire leurs besoins…\n\n"
} ,
  
  {
    "title"    : "Mix-IT 2015 - Jour 2",
    "category" : "",
    "tags"     : " mixit, conference, agile, technique",
    "url"      : "/2015/04/17/mixit-2015-jour-2.html",
    "date"     : "April 17, 2015",
    "excerpt"  : "Cet article est le retour du second jour du Mix-IT 2015, le vendredi 17 avril 2015.\nVous pouvez également consulter le retour du jour 1.\n\nAller sur Mars … ou presque\n— Florence Porcel\n\nFlorence est venue nous présenter son rêve : devenir une marso...",
  "content"  : "Cet article est le retour du second jour du Mix-IT 2015, le vendredi 17 avril 2015.\nVous pouvez également consulter le retour du jour 1.\n\nAller sur Mars … ou presque\n— Florence Porcel\n\nFlorence est venue nous présenter son rêve : devenir une marsonaute, à savoir une personne qui va aller physiquement sur Mars.\n\nAprès avoir pris le temps de faire le point sur tout ce qui a été fait pour permettre un jour à l’homme de faire le voyage pour mettre le pied sur Mars, Florence nous a expliqué les différentes actions qu’elle avait entrepris pour agir sur ce cheminement :\n\n\n  Participation à un projet de simulation de vie sur Mars : elle a fait partie d’un groupe de personnes qui se sont isolées pendant 15 jours dans des conditions de vie semblables à celles sur Mars (au milieu du désert de l’Utah, scaphandre pour sortir, nourriture lyophilisée, rationnement);\n  Participation à un programme de volontaire pour le premier départ habité vers Mars;\n  …\n\n\nLe but de tout ça ? Suivre son rêve.\n\nFlorence a fini sa conférence en nous rappelant que ce n’est pas notre éducation, notre formation ou notre passé qui dicte ce que nous pouvons faire de notre vie, mais ce sont nos rêves.\nElle nous a rappelé que, malgré sa formation littéraire, et son métier de comédienne, que son rêve d’aller sur Mars un jour l’a conduite à faire ses actions très concrètes, et que c’est grâce à l’action un peu utopique de beaucoup de gens qu’on peux changer le monde.\n\nEn 2 mots, poursuivons nos rêves !\n\nReactJS pour les néophytes\n– Nicolas Cuillery, Matthieu Lux et Florent Lepretre\n\nCet atelier était un peu corporate puisque que Nicolas et Florent sont actuellement en mission chez M6Web.\n\nLors de cet atelier, qui se décomposait en plusieurs TP permettant de découvrir une à une les différentes spécificités de React et du pattern Flux : React et sa notion de composant, le pattern Flux et une de ses variantes, ReFlux, le système de routing et les tests avec Jest.\n\nBien qu’il leur ait manqué du temps pour finir l’atelier, ils étaient très présents pour nous aider à franchir les premiers pas qui permettent d’entrer dans ce nouveau monde.\n\nEn tout cas, félicitations à tous les 3 pour leur implication pour ce difficile exercice qu’ils ont plutôt bien surmonté.\n\nStartups d’états\n— Pierre Pezziardi\n\nPierre, qui dirige un incubateur d’état, soit un incubateur qui sélectionne et héberge des petites équipes dont le but est de faire évoluer le système informatique public, mais à la sauce d’une startup : budget réduit, équipe réduite, mode “survie”.\n\nIl a eu l’idée de ce système lorsqu’il s’est posé la question sur les outils qui “marchent” et leurs raisons. Pourquoi utilisons nous les outils de Google ou Dropbox ? Parce qu’ils sont simples et efficace ! Ils vont au but, et correspondent à ce que l’utilisateur désire.\nEn poussant cette réflexion, il en est arrivé à la conclusion que tout système informatique est le reflet de l’organisation qui le pilote : plus l’organisation est tournée sur sa propre organisation, plus le produit final correspondra à ce qu’un grand manager a demandé, et pourra être décalé de ce que les utilisateurs attendent.\n\nÀ l’inverse, quand une organisation n’a pas de marge, elle va à l’essentiel pour que son produit convienne aux utilisateurs.\n\nVoici quelques exemples de projets qui sortent de cet incubateur d’état :\n\n\n  data.gouv.fr\n  Marchés Publics Simplifiés\n  mes-aides.gouv.fr\n\n\nCoding Dojo et Mob Programming dans les tranchées\n— Bernard Notarianni\n\nCette conférence était un retour d’expérience expliquant comment une équipe de développement habituée a travailler avec des releases fixes à dates régulières a tenté de mettre en place un découpage en sprint pour améliorer sa productivité.\n\nJe dis volontairement “en sprint” sans parler de Scrum parce qu’en fait, cette équipe s’auto-organisait de la sorte, mais sur un cahier des charges fixe, un périmètre fixe pour chaque release, et pas de feedback avec le produit.\n\nAu final, la conclusion de Bernard a été sans appel : ils ont essayés, l’équipe a fait son travail, a essayé l’amélioration continue, mais comme le client ne jouait pas le jeu, ça c’est mal passé.\n\nFabriquez votre devbox portable avec Docker\n— Jean-Marc Meessen et Damien Duportal\n\nLors de cette conférence, Jean-Marc et Damien nous ont expliqués comment ils avaient réussi à utiliser Docker pour réaliser une “devbox” portable.\nAvant cette conférence, je pensais qu’ils allaient nous expliquer comment ils avaient organisé leurs conteneurs pour que ça soit le plus efficace, mais en fait, une “devbox” est plus un poste de développement complet (bureau et IDE compris)\n\nLe résultat est assez impressionnant dans le fait qu’ils ont réussi à virtualiser une Debian avec son UI via Docker, et qu’ils peuvent le faire tourner sur n’importe quel poste.\n\nToutefois, je ne suis pas convaincu par cette approche. À mon sens, Docker permet à tous de développer dans l’environnement qui lui convient, tout en exécutant son code dans un environnement qui est le plus proche possible de la production.\n\nIl n’en reste pas moins que je les félicite pour le résultat qu’ils ont obtenu, et je suis encore plus convaincu de la puissance de Docker suite à cette présentation.\n\nReading code good\n— Saron Yitbarek\n\nSaron est venue nous partager sa vision sur le moyen qu’elle trouve le plus efficace pour apprendre un langage, un framework, une librairie : lire du code. \nDe la même manière, pour progresser, lire le code des autres permet d’aller au delà de ce que nous pensons faire. En se confrontant au code des autres, nous apprenons sur les autres manières de résoudre un même problème, sur d’autres approches de code, et nous élargissons notre connaissance.\n\nSuivre un tutorial, c’est bien, mais le soucis, c’est que le code est basique, sur un usage basique.\nLire un code réel, c’est voir un cas d’utilisation réel, c’est voir comment le développeur l’a résolu, voir les techniques qu’il utilise, … et le moyen le plus sûr de toujours découvrir et apprendre.\n\nPour aller encore plus loin, il ne faut pas hésiter à discuter avec l’auteur du code que l’on vient de lire.\n\nL’énergie de Saron et la conviction qu’elle met dans sa présentation ont fait de cette conférence un vrai coup de coeur de ma part !\n\nCome to the dark side\n— Stéphane Bortzmeyer - [Présentation sur InfoQ]\n\nLors de cette Keynote, Stéphane nous a fait part de son ressenti quand à l’impact de l’informatique dans notre vie.\n\nIl n’y a encore que 20 ans, l’informatique était au service de l’homme. Elle servait à améliorer son quotidien à faciliter son travail.\nAujourd’hui, c’est l’informatique qui dirige nos vies. Si vous êtes anti Facebook, vous perdez contact avec pas mal de gens. Si vous ne voulez pas d’ordinateur, il y a de plus en plus de démarches que vous ne pouvez faire.\n\nEncore plus important, nous avons délégués de plus en plus de décisions à l’informatique, sur des aspects qui impactent de plus en plus notre quotidien. Par exemple, lorsque vous faites un paiement, c’est un algorithme qui va décider si la transaction est autorisée ou non, de manière froide et automatique, sans chercher à comprendre si sa décision peux vous laisser dans une situation compliquée.\n\nÀ partir de là, nous, développeurs, avons une grande responsabilité. Le code que nous produisons, les algorithmes que nous acceptons d’implémenter sont ceux qui se retrouvent dans les systèmes qui régissent nos vies.\nIl est donc primordial que nous prenions conscience de cette responsabilité et que nous nous posions des questions sur ce que nous faisons, quitte à refuser de le faire si cela va à l’encontre de notre éthique.\n\nJ’ai été fortement touché par cette claque, enfin, cette conférence, car elle nous place devant nos responsabilités, et devant notre devoir de prendre du recul sur ce que nous faisons pour ne pas être un simple robot.\n\nConclusion\n\nComme à leur habitude, les organisateurs de ce Mix-IT 2015 ont réalisés un superbe travail.\nUn grand bravo à eux !\n\nRappel : cet article est découpé en 2 parties. N’oubliez pas de consulter le retour des conférences suivies lors du premier jour.\n"
} ,
  
  {
    "title"    : "Mix-IT 2015 - Jour 1",
    "category" : "",
    "tags"     : " mixit, conference, video",
    "url"      : "/2015/04/16/mixit-2015-jour-1.html",
    "date"     : "April 16, 2015",
    "excerpt"  : "Il est tout naturel que M6Web soit présent à une conférence qui mêle sujet technique d’avant-garde et agilité, 2 sujets qui nous sont chers, surtout lorsqu’elle se déroule à Lyon.\n\nJ’ai donc eu la chance de participer au Mix-IT 2015 qui se tenait ...",
  "content"  : "Il est tout naturel que M6Web soit présent à une conférence qui mêle sujet technique d’avant-garde et agilité, 2 sujets qui nous sont chers, surtout lorsqu’elle se déroule à Lyon.\n\nJ’ai donc eu la chance de participer au Mix-IT 2015 qui se tenait les 16 et 17 avril derniers au CPE Lyon.\n\nCet article est découpé en 2 parties. Dans l’article que vous êtes en train de lire, vous trouverez les retours des conférences que j’ai suivies le premier jour, mais vous pourrez également trouver le retour des conférences suivies lors du second jour.\n\nThe three ages of innovation\n— Dan North - [Slides]\n\nDan nous a partagé sa vision de l’innovation à travers l’évolution d’une technologie.\n\nSelon lui, il existe donc 3 âges dans l’évolution :\n\n\n  Explore (Maximize discovery)\n\n\nIl s’agit de la phase initiale, celle de la découverte, de l’expérimentation.\nDans cette phase, on essaye, on se trompe, on apprend.\n\n\n  Stabilize (Minimize variance)\n\n\nIl s’agit de la phase où on fait le tri sur tout ce qu’on a testé, initié, et qu’on essaye de catégoriser, stabiliser tout ça, voir retirer ce qui n’est pas une bonne idée.\n\nC’est le moment où on est capable de reproduire ces créations, et donc de les apprendre (repeatability, predictability, teachability).\n\nDans cette phase, nous apprenons à réduire l’incertitude autour de la manière de réaliser un code.\n\n\n  Commoditize (Maximize efficiency)\n\n\nCette phase est celle de l’industrialisation, celle où on essaye de réduire les coûts pour augmenter l’efficacité.\n\nEt même si ces 3 phases sont conflictuelles, l’innovation existe dans chacune de ces 3 phases, et il faut savoir les respecter et s’impliquer dans chacune.\n\nLe pourquoi du pourquoi de l’agilité\n— Cédric Bodin - [Screencast de la même conférence, mais à Nantes]\n\nAu cours de cette présentation, Cédric nous a poussé à réfléchir sur la raison profonde du succès de l’agilité.\n\nNous le savons, l’agilité est une solution efficace contre les plannings qui glissent, les cahiers des charges non tenables, le syndrome de la tour de cristal et autres dysfonctionnement organisationnels.\n\nDepuis 1994, le Chaos Manifesto publie un rapport indiquant le pourcentage de projets qui échouent ou réussissent, et il y a 2 fois plus de projets qui échouent que de projets qui réussissent. Il est courant que des projets qui s’éternisent, dérapent, … soient finalement abandonnés.\n\nDe même, il est courant que le produit fini ne corresponde pas à ce que l’utilisateur attend, à cause de la distance entre eux et les équipes qui développent le produit.\n\nMais pourquoi est-ce que nous avons besoin de l’agilité ? Pourquoi en sommes nous arrivés à cette situation où tout ces syndromes apparaissent ?\n\nTout simplement parce que nous avons construit l’Entreprise d’aujourd’hui sur la base du Taylorisme qui permet d’être le plus rentable possible en prévoyant tout les cas pour réduire le hasard et donc l’échec. Le socle de base de cette théorie est que rien ne change et que tout est prévisible.\n\nOr, la société actuelle va plus vite, veut pouvoir être très réactive, et notre branche en particulier.\n\nÀ partir de là, le modèle qui veux que certaines personnes pensent, prévoient, organisent, pour que les techniciens n’aient “que” à exécuter est dépassée. Et cela se voit ! Combien de “j’ai fait ce que le cahier des charges demandait”, combien de discussions stérile autour d’un changement de périmètre pour recalculer des délais ?\n\nVoici 2 exemples de phrases qui montrent le décalage entre la vision du Taylorisme dans l’informatique et la réalité :\n\n\n  En informatique, la production, c’est la compilation. Et elle est tellement efficace qu’on ne la facture plus.\n\n\n\n  Nos métiers de services ne sont pas des métiers de production, mais des métiers de conception.\n\n\nÀ partir de là, il faut considérer le travail du développeur comme du côté “pensant” et pas du côté “exécutant” : ne pas essayer de maximiser sa productivité en réduisant sa réflexion. Bien s’assurer de sa compréhension du besoin au lieu de lui lister des tâches à accomplir sans réflexion.\n\nLes principes mis en avant par l’agilité vont en ce sens : rapprocher l’utilisateur et le développeur, laisser l’équipe décider du planning, permettre l’imprévu.\n\nToutefois, l’agilité n’est pas un déclencheur. Une société qui passe à l’agilité sans modifier son fonctionnement va à l’échec. L’agilité n’est qu’un moyen de changer.\n\nN’oublions pas la loi de Conway :\n\n\n  Tout logiciel reflète l’organisation qui l’a créé.\n\n\nSolution focus in team\n— Vincent Daviet et Géry Derbier\n\nCet atelier destiné plutôt aux managers donnait des pistes et des exemples pour réussir à relancer de la synergie dans des équipes qui ont tendance à se bloquer lors de la mise en place de l’agilité.\n\nLe principal ressort de l’agilité est la communication. Si cette communication est brisée, tout est en péril, et c’est souvent la principale cause d’échec de son adoption.\n\nNous avons réalisé des mises en condition pour voir comment la communication peut être un véritable frein ou un formidable moteur pour avancer. En posant une même question de plusieurs manières, nous avons constaté qu’il est tout aussi possible de bloquer un échange qui était intéressant, que d’aller voir plus loin que les explications qui nous étaient proposées.\n\nSi le TDD est mort, alors pratiquons une autopsie\n— Thomas Pierrain et Bruno Boucard - [Slides]\n\nDerrière ce titre un peu provocateur, Thomas et Bruno voulaient faire une analyse à date du TDD, pour voir comment il est utilisé, et pourquoi il a tendance à être délaissé.\n\nComme il est un peu facile de dire que les développeurs peuvent avoir du mal à changer leurs habitudes, nous sommes allés voir un peu plus loin :\n\nLe développeur qui est habitué à écrire du code, voir s’il marche, l’éditer, voir s’il marche, … fonctionne selon un principe d’expérimentation. Il ne sait pas très bien où il va, il essaye du code jusqu’à ce qu’il fonctionne. Et une fois qu’il maitrise le code, il va continuer à travailler de la même manière, même si le code fonctionne beaucoup plus rapidement qu’avant.\n\nMais cette manière de travailler reste fortement exposée à 2 limitations :\n\n  perte de vue de l’objectif réel;\n  sur-architecture.\n\n\nAvec le TDD, le fonctionnement est de décrire ce qu’on veut faire d’abord (se fixer un objectif) en l’éclaircissant au maximum, au plus tôt.\n\nPour être efficace en TDD, il faut commencer par creuser son sujet et s’assurer de comprendre ce qu’on veut, comment, avec quelles limites (les 5 pourquois). Ensuite, il faut le formuler, de préférence à haute voix pour bien s’assurer de comprendre ce qu’on est en train de dire (méthode du canard en plastique), puis finalement lister une série de phrases en “mon code devrait” indiquant le fonctionnement nominal et les cas limites.\n\nUne fois que tout ça est respecté, il est possible de démarrer le TDD proprement dit, à savoir d’écrire les tests, de le voir échouer, puis de réaliser le code qui permet à ces tests de fonctionner. Ainsi, il est possible de dégager son esprit du fonctionnel pour se concentrer sur le technique, jusqu’à voir le code réussir à atteindre le but fixé.\n\nRappel : cet article est découpé en 2 parties. N’oubliez pas de consulter le retour des conférences suivies lors du second jour.\n"
} ,
  
  {
    "title"    : "Introduction à Immutable.Js, Relay + GraphQL et React Native",
    "category" : "",
    "tags"     : " javascript, react, reactnative, lft, video",
    "url"      : "/2015/04/01/immutablejs-relay-graphql-react-native.html",
    "date"     : "April 1, 2015",
    "excerpt"  : "Voici un petit compte rendu vidéo, filmé lors de notre Last Friday Talk de Mars, d’un retour de veille techno suite à la React Conférence.\n\nLe retour est une introduction sur 3 des sujets qui m’ont paru les plus importants lors de cette conférence...",
  "content"  : "Voici un petit compte rendu vidéo, filmé lors de notre Last Friday Talk de Mars, d’un retour de veille techno suite à la React Conférence.\n\nLe retour est une introduction sur 3 des sujets qui m’ont paru les plus importants lors de cette conférence :\n\n\n  Immutable.Js\n  Relay + GraphQL\n  React Native\n\n\nLes slides :\n\n\n\nPour plus d’informations sur la React Conférence, nos CR sont disponibles ici :\n\n\n  Compte rendu React Conférence Jour 1\n  Compte rendu React Conférence Jour 2\n\n\nMalheureusement, la vidéo n’est plus disponible…\n"
} ,
  
  {
    "title"    : "Comment a-t-on bouchonné les développeurs backend ?",
    "category" : "",
    "tags"     : " javascript, superagent, mock, isomorphic, cytron, open-source",
    "url"      : "/comment-a-t-on-bouchonne-les-developpeurs-backend",
    "date"     : "March 30, 2015",
    "excerpt"  : "Chez M6Web, nous travaillons actuellement sur la nouvelle version d’un site web pour lequel sont dédiées deux teams :\n\n\n  l’équipe backend fournit l’accès aux données via des API sous Symfony2,\n  nous, l’équipe frontend, développons une applicatio...",
  "content"  : "Chez M6Web, nous travaillons actuellement sur la nouvelle version d’un site web pour lequel sont dédiées deux teams :\n\n\n  l’équipe backend fournit l’accès aux données via des API sous Symfony2,\n  nous, l’équipe frontend, développons une application SPA isomorphe utilisant React.JS et le pattern Flux.\n\n\nDévelopper le front avant les API…\n\nNous avons démarré le projet au même moment que l’équipe backend, donc sans avoir accès aux API qui nous fournissent les données nécessaires au fonctionnement de l’application. Nous nous sommes alors interrogé sur la meilleure façon de développer notre front sans dépendre des API tout en impactant un minimum le code cible.\n\nLe contrat d’interface\n\nLe choix technique pour notre SPA a été guidé par une réflexion poussée sur les app isomorphique. Cette approche, React, Flux et tout l’environnement qui tourne autour nous étaient alors totalement inconnu. Nous avons eu une phase importante en amont pour poser les bases de l’architecture du site, démontrer la faisabilité du projet et documenter l’ensemble.\n\nCe petit délai a permis à l’équipe backend d’établir des contrats d’interface pour les principales routes de l’API. À partir de ces informations, plus ou moins précises, nous avons établi des fichiers de fixtures. L’idée était donc de retourner les données bouchonnées pour chaque appel à une route d’API non existante.\n\nSuperagent et superagent-mock\n\nPour réaliser les requêtes aux API, nous utilisons la librairie superagent, un client HTTP javascript facilement extensible. Il est isomorphe, c’est-à-dire qu’il fonctionne aussi bien sur un serveur node.js via npm que côté browser dans une application packagée via un bundler (webpack, browserify).\n\nNous avons développé superagent-mock, un plugin pour superagent, dont le rôle est de simuler les appels HTTP lancés par superagent en retournant des données de fixtures en fonction de l’URL appelée.\n\nEn pratique\n\nComme superagent, superagent-mock s’installe via npm et peut être utilisé sur des applications serveurs ou clientes (via un bundler). Tout d’abord, il faut rajouter la dépendance à la librairie dans son  package.json.\n\nnpm install superagent-mock --save-dev\n\nIl faut ensuite créer le fichier de configuration. C’est ici que vous allez décider des routes à bouchonner. Prenons l’exemple d’une route qui n’existe pas et qui devra nous retourner la liste des auteurs du blog technique de M6Web : https://tech.bedrockstreaming.com/api/authors.\n\nVoici la structure du fichier de configuration à mettre en place :\n\n// ./config.js file\nmodule.exports = [\n  {\n    pattern: &#39;https://tech.bedrockstreaming.com/api/authors&#39;,\n    fixtures: &#39;./authors.js&#39;,\n    callback: function (match, data) {\n      return { body : data };\n    }\n];\n\n\n  L’attribut pattern peut être une expression régulière, dans le cas d’une route qui contiendrait des paramètres variables (ex : https://tech.bedrockstreaming.com/api/authors/(\\\\d+)).\n  L’attribut fixtures représente le lien vers le fichier de fixtures ou une callback.\n  L’attribut callback est une fonction à deux arguments. match est le résultat de la résolution de l’expression régulière et data correspond aux données retournées par les fixtures. match permet d’utiliser certains paramètres de l’appel (ex : l’id de l’auteur) pour retourner des données ciblées (ex : l’auteur dans le fichier de fixtures correspondant à cette id).\n\n\nEnsuite, il faut créer le fichier de fixtures. C’est un fichier JS qui exporte une fonction retournant les données bouchonnées.\n\n// ./authors.js file\nmodule.exports = function () {\n  return [\n    {\n      id: 1,\n      name: &quot;John Doe&quot;,\n      description: &quot;unidentified person&quot;\n    },\n    ...\n  ];\n};\n\nPour finir, au début du fichier JS appelé par node, il suffit de patcher superagent avec le plugin superagent-mock de cette manière :\n\n// ./server.js file\nvar request = require(&#39;superagent&#39;);\nvar config = require(&#39;./config.js&#39;);\nrequire(&#39;superagent-mock&#39;)(request, config);\n\nCes quelques lignes permettent de surcharger certaines méthodes de superagent pour lui appliquer la configuration et simuler les requêtes bouchonnées. Pour comprendre plus en détail le fonctionnement, c’est par ici.\n\nEt après ?\n\nAvec cette astuce, vous pouvez développer votre front sans qu’aucune API en face ne soit accessible. C’est très pratique pour travailler en local, sans accès au net, ou pour rendre les tests fonctionnels de son application complètement indépendants d’un service tiers externe.\n\nLa partie délicate de cette approche intervient lorsque l’on câble son application avec la vraie API… et que l’on s’aperçoit que le contrat d’interface n’a pas été respecté ! Nous avons souvent des corrections à réaliser dans notre code lors de cette étape, mais les changements sont généralement mineurs et le gain de temps apporté par l’utilisation du bouchon en amont n’est pas remis en cause. La partie fastidieuse reste de maintenir ses fichiers de fixtures avec l’évolution de l’API, particulièrement nécessaire si on s’en sert dans ses tests fonctionnels.\n\nToujours plus\n\nNotre application forge elle-même l’URL des images récupérées via l’API : elle nous fournit un id et nous reconstituons l’URL finale grâce à un paramètre de configuration. Ce n’est pas REST compliant mais nous avons de bonnes raisons de le faire. Cette génération d’URL utilise la librairie sprintf-js. Pour avoir une application complètement indépendante de toute requête externe, nous avons dû également bouchonner ces appels sur des images locales. Dans cette optique, nous avons développé sprintf-mock dont le mode de fonctionnement est étrangement similaire à celui de superagent-mock.\n\nLes projets superagent-mock et sprintf-mock sont open source. Très simple d’utilisation, ils nous permettent de paralléliser nos développements avec l’équipe backend et de rendre autonomes nos tests fonctionnels. Alors n’attendez plus la finalisation de vos API pour commencer vos développements front !\n\n"
} ,
  
  {
    "title"    : "How did we mock the backend developers?",
    "category" : "",
    "tags"     : " javascript, superagent, mock, isomorphic, cytron, open-source",
    "url"      : "/how-did-we-mock-the-backend-developers",
    "date"     : "March 30, 2015",
    "excerpt"  : "At M6Web we are currently working on a new version of a web site, with two separate teams:\n\n  the backend team providing data access through APIs;\n  us, the frontend team, building an isomorphic SPA application using React.JS and the flux pattern....",
  "content"  : "At M6Web we are currently working on a new version of a web site, with two separate teams:\n\n  the backend team providing data access through APIs;\n  us, the frontend team, building an isomorphic SPA application using React.JS and the flux pattern.\n\n\nDevelop the frontend before the APIs\n\nBoth teams started the project at the same time, meaning that at the beginning, we didn’t have the web services needed for our application. We looked for the best way to develop it without waiting for those web services to become available.\n\nInterface\n\nOur technical choices for the SPA has been guided by a deep thinking about isomorphic applications. This approach, with React, Flux and their surrounding environment, was at the time, totally unknown. Our first important task was to build the foundations of the web site architecture, demonstrate the feasibility of the project and document everything.\n\nThis resulting delay allowed the backend team to specify the output of the API. Based on those informations, we wrote fixtures. The idea was to have data from a nonexistent web service.\n\nSuperagent and superagent-mock\n\nTo request the API we use the superagent library, an easily-extensible Javascript HTTP client. It is isomorphic, so it can be used both on server and client sides.\n\nThen we developed superagent-mock, a superagent plugin dedicated to simulate HTTP requests returning fixtures data.\n\nApplication\n\nLike superagent, superagent-mock can be installed via npm, and be used by server or client side libraries. First, you need to add the library in your package.json.\n\nnpm install superagent-mock --save-dev\n\nThen, create the configuration file, where you will define which data will be mocked. Let’s take for example a nonexistent API, the authors list on our technical blog: https://tech.bedrockstreaming.com/api/authors.\n\nHere is the file structure we need:\n\n// ./config.js file\nmodule.exports = [\n  {\n    pattern: &#39;https://tech.bedrockstreaming.com/api/authors&#39;,\n    fixtures: &#39;./authors.js&#39;,\n    callback: function (match, data) {\n      return { body : data };\n    }\n];\n\n\n  The pattern attribute should be a regular expression, in case of a route containing variable parameters (ie: https://tech.bedrockstreaming.com/api/authors/(\\\\d+)).\n  The fixtures attribute represents the link to a file or a callback.\n  The callback attribute is a function with two arguments: match is the result of the regular expression and data the fixtures. match allows to use some call parameters (ie: the author id) to return relevant data (ie: the author in the fixture).\n\n\nNext, you have to create the fixture file. This is a JS file exposing a function returning the mocked data.\n\n// ./authors.js file\nmodule.exports = function () {\n  return [\n    {\n      id: 1,\n      name: &quot;John Doe&quot;,\n      description: &quot;unidentified person&quot;\n    },\n    ...\n  ];\n};\n\nFinally, at the top of the file called by node, you have to patch superagent with superagent-mock this way:\n\n// ./server.js file\nvar request = require(&#39;superagent&#39;);\nvar config = require(&#39;./config.js&#39;);\nrequire(&#39;superagent-mock&#39;)(request, config);\n\nThose few lines allow us to overload some superagent methods to apply the configuration of the mocked requests (check the source code).\n\nWhat’s next\n\nWith this tip, you can develop the frontend without access to any API. It’s very useful in order to work locally on your computer, without the internet, or to make your functional tests independent of any third party.\n\nHowever it gets tricky when you connect your application with the real API… and you realize that the interface was not respected. We often have to fix our code at this stage, but the changes are usually minor and time saved by the mock isn’t questioned. The tedious part is still to maintain fixtures with the API evolution, especially necessary if it’s used with functional tests.\n\nEven more!\n\nOur app build itself the URLs of images retrieved via the API: it provides us an id and we guess the final URL through a configuration setting. This isn’t REST compliant but we have good reasons to do this. The URL generation uses the library sprintf-js. To have a completely independent application of any external request, we also had to mock these calls to local images. With this in mind, we have developed sprintf-mock whose operating mode is curiously similar to that of superagent-mock.\n\nProjects superagent-mock and sprintf-mock are open source. Very easy to use, they allow us to parallelize our developments with the backend team and to make our functional tests autonomous. So don’t wait API completion to start your frontend developments!\n\n"
} ,
  
  {
    "title"    : "CR React Conférence 2015 - Day 2",
    "category" : "",
    "tags"     : " javascript, react, flux, isomorphic, conference",
    "url"      : "/2015/02/10/cr-react-conf-2015-day-two.html",
    "date"     : "February 10, 2015",
    "excerpt"  : "De retour à Menlo Park pour cette deuxième journée de la React conférence.\n\nKeynote React Native\n\nChristopher Chedeau, @vjeux, revient sur les origines de React Native et les raisons pour lesquelles ils ont décidé de le créer.\n\nLes 3 piliers d’une...",
  "content"  : "De retour à Menlo Park pour cette deuxième journée de la React conférence.\n\nKeynote React Native\n\nChristopher Chedeau, @vjeux, revient sur les origines de React Native et les raisons pour lesquelles ils ont décidé de le créer.\n\nLes 3 piliers d’une appli natives qu’ils ont dûs traiter pour React Native sont :\n\n\n  Touch Handling : la vraie différence entre appli native et web\n  Native Components : tout le monde essaye de s’en rapprocher mais personne n’y arrive, et il y a déjà beaucoup de très bons composants natifs\n  Style &amp;amp; Layout : le layout impacte énormément la façon dont on code, que l’on soit sur le Web, iOS ou Android\n\n\nNous voyons que chaque composant natif a été recréé comme un composant React : &amp;lt;View&amp;gt;, &amp;lt;Text&amp;gt; …, et Christopher explique comment un composant React est transformé en composant natif iOS.\n\nLa transformation du JS en natif se fait via JSCore (le moteur JS dans iOS).\n\nUne démonstration nous prouve qu’on peut utiliser la console Dev Tools de Chrome, pour débugger l’application et voir tout le code « DOM » React, comme si nous faisions du web classique.\n\nLa dernière partie explique l’approche des équipes de Facebook sur la manière de faire du CSS. Christopher avait déjà fait hurler pas mal de personnes lors de sa conférence “faire du CSS en JS” (React CSS in JS). Il déclare le style en javascript dans une variable styles, et utilise l’attribut style : &amp;lt;Text style={styles.movieYear}&amp;gt; qui inlinera le CSS.\n\nC’est assez déstabilisant mais aussi ultra prometteur. Cela permet de résoudre quasiment tous les défauts de CSS (Global Namespace, Dependencies, Dead Code Elimination, Minification, Isolation …)\n\nNous parcourons ensuite les manières de gérer du layout nativement dans iOS, que Christophe décrit comme « ultra-compliqué » ! Alors que coté web, nous avons le Box Model et Flexbox qui résolvent tous ces problèmes assez facilement.\n\nLes équipes de Facebook ont donc décidé de re-coder Flexbox et le Box Model en JS avec une approche TDD, de manière à pouvoir utiliser la plupart des bases de Flexbox dans React Native pour faire du layout facilement sur iOS !\n\nVous pouvez retrouver le résultat « Css-Layout » sur le Github de Facebook.\n\nLa démonstration continue sur un « live coding » montrant le « live reload » entre la modification du JS et le rafraîchissement instantané du Simulator iOS.\n\nNous apprenons aussi que les modules ES6 ou Node comme Underscore, ou le SDK de Parse par exemple, fonctionneront sans problème du moment qu’ils n’ont pas de dépendance dans le browser ! \nC’est encore une fois très prometteur, et si React Native vous intéresse, la vidéo ci-dessous est une excellente introduction.\n\nJ’ai, de mon coté, pu jouer quelques heures avec et c’est effectivement très sympa, intuitif et très rapide.\nLa version que nous avons ne contient pas encore tout ce que l’on voit dans la vidéo (je n’ai par exemple pas trouvé le Live Reload ou le Remote Debugging pour l’instant), mais cela ne saurait tarder, les équipes de Facebook travaillant d’arrache-pied sur le projet.\n\n\n\nThe complementarity of React and Web Components\n\nAndrew Rota, @AndrewRota, est de Boston, travaille pour Wayfair.com et explique comment utiliser des Web Components avec React, ou du React dans des Web Components.\nIl nous montre un exemple d’un player html5 vidéo avec un shadow dom qui contient tous les contrôles du player (de simple input HTML).\n\nLa communauté WebComponent a déjà partagé pas mal de WebComponents :\n\n\n  les x-\n  les core-\n  google-\n  paper-\n  et d’autres …\n\n\nPour conclure, Andrew partage les bonnes pratique pour créer un WebComponent :\n\npetit\ntrès encapsulé\naussi stateless que possible\nperformant\n\nPour en savoir plus sur les Web Components : Polymer-Project\n\n\n\nImmutable Data and React\n\nLee Byron, @leeb, enchaîne sur l’immuabilité !\nConcept passionnant que nous avons entendu dans presque l’intégralité des conférences.\n\n\n  Un objet immuable, en programmation orientée objet et fonctionnelle, est un objet dont l’état ne peut pas être modifié après sa création. Ce concept est à contraster avec celui d’objet variable. Source : Wikipédia\n\n\nLee Byron est donc le créateur de la librairie Immutable-JS permettant de gérer facilement des collections immuable en JS.\n\n\n  Immutable data cannot be changed once created, leading to much simpler application development, no defensive copying, and enabling advanced memoization and change detection techniques with simple logic. Persistent data presents a mutative API which does not update the data in-place, but instead always yields new updated data. Source : immutable-js\n\n\n\n  React is the V in MVC. We don’t need an M. We already have arrays and objects.\n\n\nD’ailleurs, on parle aussi d’objets immuables coté Angular 2 : Change Detection in Angular 2\n\n\n\nBeyond the DOM: How Netflix plans to enhance your television experience\n\nL’une des conférences que j’attendais le plus, par Jafar Husain, @jhusain, Technical Lead chez Netflix.\nPour plusieurs raisons, déjà parce que Netflix … qui en a profité pour annoncer la veille que « Netflix aimait React » mais aussi parce que Jafar est connu pour pas mal de choses (différents blog posts ou présentations), ainsi qu’un cours interactif sur la programmation fonctionnelle en Javascript sur lequel j’ai passé pas mal de temps.\n\nIl nous a donc expliqué les plans de Netflix pour améliorer l’expérience Télé sur leurs services, et comment React les a grandement aidés à le faire.\n\nPour connaître les raisons pour lesquels ils ont choisi React, je vous invite à lire l’article Netflix like React (Startup Speed et Server Side Rendering \\o/, Runtime Performance, Modularity).\n\nAujourd’hui, Netflix développe majoritairement en Javascript et ont 3 UI en JS, une pour le mobile, une pour le web et une pour les télés.\nIls ont vu assez vite que le DOM était très loin, c’est pourquoi ils ont créé et introduit Gibbon (une sorte de Webkit maison plus rapide et adapté à leur besoin sur les téléviseurs).\nIls ont donc fait évoluer React (un fork au départ) pour permettre de sortir vers quelque chose d’autres que du DOM afin de correspondre à leur moteur Gibbon et vont donc continuer en 2015 le déploiement de leur nouvelle UI avec React sur tous les services y compris télés.\n\nVous pouvez retrouver le Netflix Open Source Software Center pour découvrir le grand nombre d’outils Open source de qualité qu’ils délivrent.\n\n\n\nScalable Data Visualization\n\nZach Nation travaille chez Dato (anciennement GraphLab) et doit traiter de très grandes quantités de données dans ses applications.\n\nIl démontre l’intérêt de React couplée à d3.js (une librairie de visualisation exceptionnelle) pour représenter à l’écran des transactions Bitcoin (en parsant un fichier de 21G ! en live).\n\n\n\nRefracting React\n\nTalk par David Nolen, @swannodette, personne très influente dans la communauté JS (son blog). Créateur de Om, ClojureScript, il nous explique que React doit être vue comme une plateforme (plutôt que librairie ou framework). On apprend aussi les concepts à l’origine d’Om.\n\n\n\nFlux Panel\n\nBill Fisher (Facebook) a rassemblé une partie des utilisateurs (voir des contributeurs) à React pour confronter les différentes approches sur l’utilisation de Flux, ainsi que sur la manière de gérer de l’isomorphisme.\n\nOn parle notamment, via Michael Ridgway, @theridgway, de Fluxible, la librairie open source proposée par Yahoo (que nous utilisons), qui a annoncé le jour même la création de sa documentation en ligne isomorphique, elle-même open source et utilisant Fluxible.\n\nSpike Brehm est aussi intervenu pour AirBnb, qui fut la première société, je pense, à parler d’isomorphisme.\n\nAndres Suarez a aussi présenté la manière de gérer l’isomorphisme chez Soundcloud. Vous pouvez avoir beaucoup plus d’infos de sa part dans cette excellente vidéo.\n\nSi vous êtes intéressés par les différentes approches de Flux, vous pouvez comparez ici les implémentations : Flux Comparison\n\nQuelques questions sont aussi posées à Jing Chen sur Relay.\n\nL’approche était intéressante mais le résultat un peu décevant car les sujets et implémentations ne sont que trop peu effleurés.\n\n\n\nCodecademy’s approach to component communication\n\nBonnie Eisenman, @brindelle, travaille pour CodeAcademy.\nCodeAcademy est un service gratuit du secteur éducatif permettant d’apprendre à coder dans certains langages.\n\nBonnie a partagé la manière dont son équipe a appréhendé React, et notamment la communication entre composant. Ces réflexions ont eu lieu avant qu’ils n’aient connaissance du pattern Flux.\n\nLes slides\n\n\n\nStatic typing with Flow and TypeScript\n\nJames Brantly, de chez AssureSign, commence avec une citation vu le matin (merci le JetLag) sur twitter :\n\nOn the 1st day God created the web. On the 2nd day God wrote jQuery. Then God blacked out, 3 days later awoke &amp;amp; invented React. #reactjsconf&amp;mdash; Matt Huebert (@mhuebert) 29 Janvier 2015\n\n\nIl présente ensuite TypeScript et Flow, deux outils pour améliorer et sécuriser la production de code.\n\nEn partant d’une application React d’exemple, il nous montre comment on intégre TypeScript et comment il l’a “hacké” pour qu’il reconnaisse le JSX, puis comment il ajoute Flow.\n\nAu final il recommande d’utiliser plutôt Flow, même si TypeScript est un peu plus mature, et fonctionne lui sous Windows.\n\n\n\nQA with the team\n\nLes deux journées se sont finies sur une session de Questions/Réponses avec les équipes de React chez Facebook : Tom Occhino, Ben Alpert, Lee Byron, Christopher Chedeau, Sebastian Markbåge, Jing Chen, et Dan Schafer.\n\n\n\nConclusion\n\nPour une première conférence officielle sur React, ce fut une excellente surprise ! On retiendra clairement l’annonce et les démos de React Native, l’emballement général autour de React, ainsi que l’approbation global de beaucoup de gros acteurs du web (Yahoo, Mozilla, Netflix, Uber, …), le succès du pattern Flux (malgré le manque de clarté sur la manière de faire du Data Fetching), les promesses de Relay, les sujets récurrents autour de l’immuabilité …\n\nBref, une vraie belle réussite. Chapeau aux organisateurs (merci @vjeux ;-) ) et équipes de Facebook, ainsi qu’aux speakers pour cette superbe conférence.\n\nIl se passe réellement quelque chose de grand dans la communauté Front-End grâce à React. Il suffit de voir la vitesse à laquelle les tickets d’entrée sont partis (même chose au React Meetup parisien de Décembre 2014), de voir que tous les frameworks MVC tentent de s’en inspirer (pré-render, SSR, Virtual-Dom …).\n\nPour finir, je voulais aussi partager le travail d’une des personnes présentes lors de cette conférence, ayant une façon très particulière de prendre des notes sur chacun des talks : https://chantastic.io/2015-reactjs-conf/\n\np.s: Retrouvez les retours sur la première journée de la React conférence 2015.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "CR React Conférence 2015 - Day 1",
    "category" : "",
    "tags"     : " javascript, react, flux, isomorphic, conference",
    "url"      : "/2015/02/04/cr-react-conf-2015-day-one.html",
    "date"     : "February 4, 2015",
    "excerpt"  : "Les 28 et 29 janvier 2015, sur le campus de Facebook (à Menlo Park), avait lieu la première conférence officielle sur la librairie open-source React (créée par les équipes de Facebook).\n\n2 jours de conférences riches en talks et en annonces dont v...",
  "content"  : "Les 28 et 29 janvier 2015, sur le campus de Facebook (à Menlo Park), avait lieu la première conférence officielle sur la librairie open-source React (créée par les équipes de Facebook).\n\n2 jours de conférences riches en talks et en annonces dont voici un petit compte-rendu, pour ceux n’ayant pas eu la chance de pouvoir y assister ou de suivre les différents LT sur Twitter, en commençant par la première journée.\n\nL’ouverture de la conférence par Tom Occhino\n\nTom Occhino, @tomocchino, a permis de rétablir la vérité sur l’origine de React.\nCe sont les équipes de Facebook Ads qui sont à la genèse de ce projet.\n\nA l’époque, sur des applis MVC côté client, plus les applications et le nombre de développeurs grandissaient, plus elles étaient difficiles à maintenir et devenaient lentes !\nLe « Two Way Data Binding » rendait les mises à jour en cascade trop compliquées (tout l’écran devait être rafraîchi) et le code devenait vraiment non prévisible. Mais malgré tout, cela “marchait” ! L’appli de Chat Facebook fonctionnait aussi de la même manière.\n\nC’est ainsi que React a été créé pour améliorer le rendu, l’organisation et les performances de ces applications.\n\nInstagram a ensuite rejoint Facebook et les équipes ont voulu utiliser React pour refondre le site, mais React était à l’époque trop couplé à Facebook. \nPete Hunt a donc re-factoré l’ensemble pour créer le React “open-source” que l’on connaît aujourd’hui.\n\nAprès cette introduction sur React, Tom a expliqué que l’un des problèmes de React est qu’il n’est utilisé que pour le Web.\nAujourd’hui, tout le monde tente de créer des composants web ressemblant aux composants natifs, mais à chaque fois, le résultat est mauvais, l’environnement natif étant plus performant que celui d’un browser. Un exemple donné d’application native plutôt sexy est celui de “Facebook Paper”.\n\nIl a ensuite révélé l’annonce certainement la plus importante de ces 2 jours, l’arrivée prochaine sur Github de « React Native », permettant de développer en Js via React des composants entièrement natifs, avec comme exemple, l’application Facebook Groups présente sur l’App Store iOs ! \nLa conférence est lancée !\n\nAvec React Native, ils ne veulent pas faire du “write once, run anywhere », mais du “learn once, write anywhere » de manière à optimiser les composants et usages suivant les devices.\n\nLe code sera fourni sur un dépot privé à tous les participants de la conférence lors de la Keynote de clôture !\n\nPour finir sa keynote d’entrée, il a voulu lister les frameworks JS qui ont été influencés par React ces derniers mois : Tous !\n\n\n\nEbay : Tweak your page in real time, without leaving the comfort of your editor\n\nBrenton Simpsons d’Ebay, @appsforartists, nous a montré comment coder en live du React de son mac, avec le rendu affiché en temps réel sans reload sur un ipad.\n\nL’avantage d’un iPad étant sa taille qui lui permet de représenter 3 écrans d’iPhone 5 sur sa largeur, soit 3 états de son application.\nIl utilise « WebPack » et l’extension pour WebPack « react-hot-loader » de Dan Abramov.\n\nEbay a aussi open-sourcé un framework assez experimental (6 mois d’ancienneté) du nom d’Ambidex pour gérer du server side rendering avec React et Flux : https://github.com/appsforartists/ambidex\n\n\n\nData fetching for React Applications at Facebook\n\nJing Chen, @jingc, et Daniel Schafer, @dlschafer, nous ont présenté Relay, une nouvelle approche au pattern Flux orienté Data Fetching, permettant grâce à GraphQL de définir au niveau de son composant les data nécessaires.\nRelay se chargeant ensuite de générer les bons appels HTTP grâce à GraphQL.\n\nUne approche intéressante, mais qui parait très couplée au fonctionnement de Facebook et soulève pas mal de questions : dois-je modifier toutes mes API pour supporter GraphQL ? Quid de l’optimisation du cache coté API ? …\n\nRelay sera open-sourcé prochainement (ainsi que GraphQL j’imagine ?).\n\nBeaucoup plus d’infos sont disponibles ici : https://gist.github.com/wincent/598fa75e22bdfa44cf47\n\n\n\nCommunicating with channels\n\nJames Long, @jlongster, assez réputé via son blog https://jlongster.com et pour son travail chez Mozilla sur les Dev Tools de Firefox, a présenté une manière de communiquer entre composants via des « channels » en utilisant la librairie “ js-scp”  permettant de coder à la manière des « goroutine » de Go ou des « core.async » de Clojurescript.\n\n \n\n\n\nReact-router increases your productivity\n\nMichael Jackson, @mjackson, co-créateur du routeur le plus populaire de React “react router”, est venu avec Ryan Florence (l’autre co-créateur), nous expliquer les origines du routeur, l’inspiration très forte du router d’Ember.Js, ainsi que quelques techniques avancées d’utilisations (transitions, etc). Un excellent speaker et une introduction très drôle sur les origines de React-Router.\n\n\n  “Url should be part of your design process”\n\n\n\n\n\n\nFull Stack Flux\n\nPete Hunt, @floydophone, l’une des personnes responsable des origines de React et de son « open-sourcage », ancien Lead-Dev d’Instagram chez Facebook, a présenté un talk un peu particulier expliquant comment on pouvait, coté architecture serveur, reproduire le pattern Flux.\n\n\n  “shared mutable state is the root of all evil.”\n\n\n\n\nMaking your app fast with high-performance components\n\nJason Bonta, de l’équipe Facebook Ads, à l’origine de la création de React, a ciblé sa présentation sur les problèmes de performances que résout React.\nCoté Ads Manager, l’équipe doit faire des interfaces ultra complexes, avec notamment le besoin de présenter un nombre d’éléments très important dans un tableau.\n\nUn composant qui sera annoncé comme « open-sourcé » durant sa conférence : FixedDataTable\nVous pouvez aussi retrouver une « review » du composant ici : https://www.reactbook.org/blog/fixed-data-table-reactjs.html\n\nOnt été abordé :\n\n\n  le ReactAddons : PureRenderMixin\n  l’utilisation du shallowEqual sur le shouldComponentUpdate\n  Ainsi qu’une bonne pratique pour la réalisation des composants, qui est revenue plusieurs fois pendant la conf, consistant à englober le composant, dans un autre composant de type container ne contenant aucune « props ».\n\n\nEn résumé : \n\n\n\n\nFormat data and strings in any language with FormatJS and react-intl\n\nDernière conférence de la journée par Eric Ferraiuolo, @ericf, sur l’internationalisation et la manière de la gérer dans React, grâce à react-intl (open-sourcé par Yahoo).\n\nPour ceux qui douteraient encore de la complexité de gérer plusieurs langues, ainsi que les chiffres et pluralisations, et qui ont cette problématique sur un projet React, cette vidéo est un must-see.\nFormat.Js a aussi été cité et s’apparente à une collection de module Js pour l’internationalisation.\n\n\n\nHype\n\nRyan Florence a fini la journée sur un showcase d’exemple très intéressant. \nIl nous a aussi raconté son histoire, et comment il est devenu développeur : principalement, parce qu’il voulait toujours répondre “oui” quand on lui demandait si il pouvait faire quelque chose.\n\nBref, une excellente manière de finir la journée de manière fun avec quelques exemples très intéressants, notamment autour des “portals”.\n\nVous pouvez retrouver toutes les démos ici : https://github.com/ryanflorence/reactconf-2015-HYPE\n\nPour ceux qui douteraient encore des performances de React, je vous invite à regarder les 5-6 premières minutes de la vidéo.\n\n\n\nConclusion du premier jour\n\nBonne grosse claque sur cette première journée, notamment avec l’annonce de React Native. Nous avons eu le droit à une organisation absolument parfaite (snack, boisson chaude et froide à volonté) et des speakers de très grand talent (ce qui n’est pas toujours le cas de certaines conférences, surtout aussi ciblée que celle-là).\n\np.s: Retrouvez les retours sur la deuxième journée de la React conférence 2015.\n"
} ,
  
  {
    "title"    : "App Isomorphic: la Single Page App parfaite ?",
    "category" : "",
    "tags"     : " javascript, webperf, angular, react, flux, isomorphic",
    "url"      : "/2014/12/04/isomorphic-single-page-app-parfaite-react-flux.html",
    "date"     : "December 4, 2014",
    "excerpt"  : "Qu’est ce qu’une Single Page App (SPA) ?\n\n\n  « As rich and responsive as a desktop app but built with HTML5, CSS and Javascript »\n\n\nLes SPA se répandent de plus en plus, et deviennent un choix « commun » lorsque l’on veut développer un Front riche...",
  "content"  : "Qu’est ce qu’une Single Page App (SPA) ?\n\n\n  « As rich and responsive as a desktop app but built with HTML5, CSS and Javascript »\n\n\nLes SPA se répandent de plus en plus, et deviennent un choix « commun » lorsque l’on veut développer un Front riche (souvent câblé sur des API REST) que l’on souhaite :\n\n\n  testable (unitairement et fonctionnellement)\n  fluide (pas de rechargement d’url etc)\n  bien organisé\n  maintenable et évolutif\n  …\n\n\nLes Frameworks type AngularJs et EmberJs tiennent le haut du panier et ont largement fait leurs preuves, mais ils continuent à échouer sur deux sujets pourtant primordiaux dans beaucoup de cas :\n\n\n  La performance (dont le rendu initial)\n  Le référencement\n\n\nLa performance\n\nAujourd’hui, quand vous chargez une SPA, voici grossièrement ce qui se passe coté client :\n\n\n  Chargement du fichier HTML\n  Chargement des différents Assets (Css, image, scripts JS externe comme Angular et Jquery par exemple)\n  Ainsi que de l’intégralité du code JS de votre application (sauf si vous lazyloadez)\n  Execution de tout ce petit monde, qui va devoir savoir où vous êtes dans l’application afin de générer le HTML correspondant à l’état demandé.\n\n\nAvoir ces quelques secondes à attendre avant de se retrouver dans un état fonctionnel est peut être acceptable pour un backoffice. Mais ça l’est beaucoup moins pour un front riche.Et ce temps aura tendance à augmenter fortement, parallèlement à l’enrichissement de votre application.\n\nSi l’on se soucie un minimum des aspects de performances Web, c’est forcément dérangeant.\nEt d’un point de vue plus global, tout le monde sait aujourd’hui que la performance brute n’est pas le point fort de ces frameworks.\n\nLe référencement\n\nAutre sujet, qui peut être très problématique, si le site en question s’y prête. Ces applications vont fournir comme « source HTML » quelque chose de ce style (pour du Angular) :\n\n&amp;lt;!doctype html&amp;gt;\n&amp;lt;html class=&quot;no-js&quot;&amp;gt;\n&amp;lt;head&amp;gt;\n    ...\n&amp;lt;/head&amp;gt;\n&amp;lt;body ng-app=&quot;myApp&quot;&amp;gt;\n    &amp;lt;ng-view&amp;gt;&amp;lt;/ng-view&amp;gt;\n    &amp;lt;script src=&quot;scripts/vendor.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&quot;scripts/main.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n\n\n  Container qui servira à recevoir le HTML généré par votre appli JS une fois exécutée.\n\n\nDe base, Google (et autres moteurs/crawler) ne verra donc rien, tout votre contenu allant être injecté via JS dans votre balise ng-view. \nExcepté le fait qu’il parait que depuis des mois/années, Google commence à réellement crawler du JS … Si le site est important, cette supposition ne devrait pas suffire à vous convaincre, et vous avez raison.\n\nRassurez vous, à ce stade, des solutions existent pour fournir spécifiquement à Google une version correspondante aux snapshots HTML générés par vos applications.\nCes solutions sont accessibles soit en mode SAAS (payante et hébérgé), soit en mode Open-Source à héberger vous même. Je pense notamment à Prerender.io qui fait plutôt bien le job, et vous propose d’indiquer aux moteurs que vous faites une application de type « Ajax » en respectant les recommandations de Google.\n\nPrerender est composée de plusieurs briques :\nUn middleware applicatif (Rails, Node, Varnish, Nginx, etc selon votre infrastructure), qui va intercepter les moteurs et les renvoyer sur votre service de Prerender \nUn service de Prerender qui est une brique Node.js qui va lancer des HeadLess Browser (PhantomJS ou SlimerJs …) pour executer votre appli JS et renvoyer un snapshot HTML une fois le rendu JS terminé.\n\nLa solution permet à priori de faire le boulot, mais cela reste une gymnastique complexe, et beaucoup d’interrogations subsistent (pertinence, maintenance, stabilité, Page Rank, pondération vs sites classiques …)\n\nLa lumière au fond du tunnel ?\n\nVous l’avez donc compris, dans certains cas, les SPA basées sur des frameworks Js posent deux problèmes très gênants et difficilement résolvables.\nC’est là qu’entre en piste, une nouvelle façon de penser les SPA, grace à une librairie développée par Facebook : React.JS\n\nReact fait parler de lui car il commence à être utilisé massivement par des très gros acteurs Web, Facebook bien entendu pour ses composants Chat, ou son éditeur vidéo, Instagram pour l’intégralité du site, Yahoo Mail, Github avec l’IDE Atom, Khan Academy, NyTimes, Feed.ly …\n\nAu premier abord, React n’est qu’une librairie qu’on pourrait comparer à la partie Vue d’un Framework MVC (voir aux Directives d’Angular), mais il a la particularité d’être basé sur un Virtual DOM.\nCe qui parait au départ simplement une bonne idée pour avoir des performances bien supérieures à celle d’un framework MVC basé sur le DOM, et éviter par exemple les Dirty checking du DOM (qui explique en partie le manque de perf d’Angular), permet aussi d’utiliser ces mêmes composants coté serveur !\n\nC’est ce qu’on appelle l’approche « Isomorphic » .\n\nUn composant React n’est finalement qu’un module CommonJs et peut donc aussi bien être utilisé coté browser sur le client, que coté server dans du Node.Js (ou IO.js devrais-je dire maintenant ?).\nL’idée de l’isomorphisme est aussi d’être capable de servir le premier rendu directement par le serveur.\nExemple:\n\n\n  Vous accédez à votresite.com/votrepage.html\n  Votre serveur Node, construit votre page et sert le rendu HTML généré par votre appli au client\n  Il sert aussi votre application JS dans un Bundle (généré via du Gulp ou Grunt par WebPack ou Browserify)\n  Le client reçoit un fichier statique et l’affiche (sans attendre le moindre JS)\n  Il reçoit aussi le bundle Js\n  Une fois affiché, React sait reprendre la main sur votre appli afin de continuer en mode SPA pour la suite de l’application.\n\n\nEt là, vous répondez de manière parfaite aux deux points problématiques.\nGoogle n’y verra que du feu, et pourra crawler votre site entièrement comme si il n’était composé que de fichiers statiques. \nLa performance du premier rendu sera quasi imbattable, car ne nécessitant aucun JS !\n\nSur le papier, c’est juste le rêve ultime de tout développeur Front-end : tous les avantages d’une SPA sans les inconvénients !\n\nFacebook propose aussi sur son Github, une solution pour ceux ayant déjà un applicatif dans un autre language (ici PHP) : Server side rendering\n\nLa solution parfaite ?\n\nPresque.\nReact n’est au final que la partie Vue de votre application, il va falloir encore organiser tout ça. C’est ici qu’entre en compte Flux, un pattern d’architecture unidirectionnel proposé aussi par Facebook, à priori plus scalable que ne l’est le pattern MVC.\n\nMais là encore, l’approche de Flux est plutôt prometteuse, alors quel est le problème ?\n\n\n  Finalement c’est encore peu mature (déjà React et Flux, mais encore plus l’approche Isomorphic)\n  La montée en compétence n’est pas négligeable\n  Il n’y a pas vraiment de Framework comparable à date, et vous allez surement devoir réinventer la roue à certains moments (à suivre l’arrivée imminente de React Nexus notamment)\n  La documentation est très faiblarde encore\n  Les ressources très difficiles à trouver et de qualités très différentes\n  Pas vraiment de starter-kit ou générateur digne de ce nom\n  Le coté Isomorphic va aussi engendrer une certaine complexité :\n    \n      Est-ce que mon client reçoit bien le même état que celui qu’avait mon serveur au moment du rendu initial\n      Obligation de n’utiliser que des composants Isomorphic, typiquement un router qui fonctionne aussi bien coté client que serveur (React-Router ou Director), même chose pour les requêtes HTTP (Superagent par exemple) …\n    \n  \n\n\nSi malgré ces points, vous souhaitez tester cette approche, je vous conseille de regarder du coté de Yahoo, qui après avoir annoncé la migration de Yahoo Mail de PHP/YUI vers React/Flux Isomorphic a aussi publié quelques packages Open-Source très intéressants, pouvant constituer une bonne base de départ pour un projet isomorphic :\n\n\n  Fluxible-App\n  Flux-examples\n  ou cet exemple utilisant Fluxible-app : Isomorphic-React\n\n\nSi vous souhaitez plus d’infos sur React et Flux, je vous conseille ces deux articles en anglais de @andrewray:\n\n\n  React for stupid people\n  Flux for stupid people\n\n\nOu ce tuto chez nos amis de Jolicode, pour faire un Gifomatic avec React et Flux\n\nD’autres solutions existent aussi conservant la même approche, mais sur la base d’autres technos, notamment celle d’Airbnb: RendR, permettant d’utiliser du Backbone coté client et serveur.\n\nEt pour finir, si ces sujets vous passionnent tout comme nous, restez à l’écoute ici, d’autres posts pourraient arriver à l’avenir ;-)\n\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - troisième journée",
    "category" : "",
    "tags"     : " conference, velocity, webperf, devops, sysadmin",
    "url"      : "/velocity-europe-2014-day-3",
    "date"     : "December 3, 2014",
    "excerpt"  : "Velocity Barcelone, troisième journée\n\nLe troisième jour étant dédié aux tutoriaux, on passe de conférence de 45min à des ateliers de 1h30.\n\nExtreme Web Performance for Mobile Devices\n\nMaximiliano Firtman nous a dressé un portrait vraiment exhaust...",
  "content"  : "Velocity Barcelone, troisième journée\n\nLe troisième jour étant dédié aux tutoriaux, on passe de conférence de 45min à des ateliers de 1h30.\n\nExtreme Web Performance for Mobile Devices\n\nMaximiliano Firtman nous a dressé un portrait vraiment exhaustif du web mobile et de l’état actuel des navigateurs.\n\n\n\nEn gros c’est compliqué. Le marché est très fragmenté, certains constructeurs comme Samsung ajoute du bruit en diffusant massivement un navigateur modifié. L’usage des sites en webview depuis une application native n’arrange pas les choses (par exemple, l’application Facebook).\n\nAprès un rappel sur l’importance de la performance, l’orateur a distillé de nombreuses pratiques permettant de faire un web mobile plus performant.\n\nOn peut retenir :\n\n\n  Le RWD est un outil, pas une fin en soi,\n  il faut s’imposer de tester sur du hardware cheap avec une connection faible,\n  ne pas oublier le temps perdu sur le réseau (600ms mandatory network overhead),\n  ne pas oublier l’impact que le parsing du JS et le rendu CSS est bloquant,\n  utiliser les solutions de stockage coté client,\n  de très nombreux outils de simulation existent, il faut les maitriser.\n\n\nIl propose un site récapitulant toutes les informations délivrées : https://firtman.github.io/velocity/.\n\nSlides :\n\n \n  Extreme Web Performance for Mobile Devices - Velocity Barcelona 2014  from Maximiliano Firtman \n\n\n\nZero Downtime Deployment with Ansible\n\n\n\nSlides\n\nGithub Repo\n\nTutorial intéressant conduit par un développeur (sur un sujet à priori plus opérationnel) qui démontre bien la flexibilité et la simplicité d’Ansible.\n\nAprès avoir mener le tutorial à son terme vous aurez deployé deux machines avec du code Java, un load balancer NGINX, et une base de données PostgreSQL (utilisateur + base).\n\nA contre courant des systèmes de gestion de configurations comme SaltStack, Puppet ou Chef, Ansible est basé sur le modèle push et ne nécessite aucun agent, il repose entièrement sur SSH. D’autre part il mixe gestion de configuration et orchestration, ce que qu’on doit bien souvent faire via des outils tiers comme MCollective.\n\nLa simplicité de ce modèle en fait sa plus grande force. Ansible est capable de gérer dynamiquement les inventaires (de base c’est une liste statique contenue dans un fichier). Par exemple il est capable d’interroger les APIs Amazon, Google Cloud ou RackSpace pour récupérer la liste de vos machines, celles de votre Cluster VMWare ou n’importe quel script qui sortira une liste en JSON.\n\nAlors que Chef et Puppet offrent une DSL pour décrire votre infrastructure sous forme de code, Ansible a opté pour une description au format YAML. Sur l’Ansible Galaxy vous retrouverez tout les modules disponibles (quelques milliers) comme Nginx, PHP etc… Développés en Python, il est évidemment possible de faire soit même ses modules.\n\nLe déploiement avec zéro temps de panne peut être implémenté avec Ansible de la façon suivante:\n\n\n  récupération de la liste des machines\n  sortie du load balancer d’une machine (Ansible est compatible GCE et AWS)\n  mise à jour de la configuration (code et/ou logiciel)\n  période d’attente: vous spécifiez si un port TCP doit être disponible, un fichier, etc…\n  itération sur la machine suivante\n\n\nLe nombre de machine traitées en parallèle est bien entendu configurable.\n\n\n\nJe suis Ansible depuis quelques mois déjà et j’ai été conforté dans l’idée que c’est un excellent produit: pas d’agent, basé sur une brique solide qu’est SSH, et développé en Python :) La gestion de l’inventaire peut être délicate, mais un CMDB comme Collins de Tumblr ou un taggage précis peuvent résoudre l’équation.\n\nAnsible facilite le déploiement d’infrastructure immuable, le blue/green, violet et canary deployment de par son modèle. C’est un atout qui en fait à mon sens le meilleur système de gestion de configuration aujourd’hui.\n\nCependant je reste encore un peu dubitatif sur le déploiement et le rollback de code qui ne sont pas encore à la hauteur de Capistrano. Un aperçu du workflow et des schémas de développement auraient été aussi bienvenus.\n\n\n  Ansible Galaxy\n  Ansible Docs\n  How Twitter use Ansible\n  Ansible Tower (Payant)\n  Thoughts on deploying Symfony with Ansible\n\n\n\n\nLinux Containers from Scratch\n\n\n\nSlides\n\nQuelle est la différence entre le cloud, les containers et un repas gratuit ? Aucun n’existe :)\n\nJoshua Hoffman (SoundCloud) est dans le top 5 de mes orateurs préféré. J’ai beaucoup apprécié ce tutorial car il fait clairement la part entre virtualisation, containers, LXC et Docker (nom qui ne sera prononcé qu’à la fin lors des questions, pas de buzzword, de hype ni de marketing, merci Joshua).\n\n\n\nLe tutorial vous aménera à créer plusieurs containers portable, du plus simple ou plus complexe, avec les outils de bases du noyau. Vous apprendrez aussi à vous servir des cgroups, des namespaces process, network, et mount, et serez amené à utiliser des systèmes de fichiers unis, ici AUFS.\nJ’aurais bien aimé une démo avec le format de QEMU ou btrfs pour ce qui est des systèmes de fichiers unis au niveau bloc.\n\nCe tutorial est un must-do pour tout personne désirant s’initier aux architectures de containers. Le marketing relativement agressif de Docker ne doit pas faire oublier qu’il existe d’autres alternatives, et que Docker est un choix de design bien particulier pas forcement adapter à tous.\nEx: un container en 3 lignes:\n\n\n\nPour rappel:\n\nLXC/LXD = Ensemble d’APIs et d’outils dans l’espace utilisateur linux exposant les capacités d’isolation du noyau (cgroups, chroot, namespaces, selinux, iptables etc…), alternative légère à la virtualisation telle qu’on la connaît (avec Vmware par exemple)\n\nDocker = un des cas d’usage des containers, application unique, statique, immuable, single app delivery plateform\n\nLXC\n\nDocker F.A.Q\n\n\n\nCoreOps - CoreOS for Sysadmins\n\n\n\nGithub\n\nTutorial très attendu par beaucoup, Kelsey Hightower (CoreOS Inc.) nous a présenté l’écosystème de CoreOs et les problèmes qu’il tente de résoudre. Suite à la demande générale il nous a aussi fait une démonstration de Kubernetes, l’outil de gestion de containers de Google.\n\nCoreOS est distribution Linux accompagnée d’outils qui vise à penser le datacentre comme une seule machine (voir Mesos/Yarn). En d’autres termes, vous n’avez que faire de savoir quelle application tourne sur quel serveur. Le datacentre apparaît comme une entité unique où l’on déploie des applications.\n\nTechniquement, CoreOS est un Linux + systemd + docker + etcd + fleet. CoreOS est basé sur Chrome OS, épuré et léger, il bénéficie du système d’update en arrière plan bien connu de Chrome. On oublie donc le gestionnaire de paquets, les outils de debug (tcpdump etc..) et tout ce qui fait un Linux en mode serveur tel qu’on le connaît.\n\n\n  gentoo: parfum de distribution Linux (ex: Ubuntu, Debian, Centos)\n  systemd: alternative à SysV Init, le gestionnaire des démons, le premier programme lancé au démarrage (PID: 1)\n  docker: Système de containers légers, ensemble d’apis et librairies centrés sur le déploiement et la gestion d’application isolée du kernel.\n  etcd: base de données clé/valeur distribuée, utilisée pour centraliser la configuration et la découverte de service, fondée sur le protocole de consensus Raft.\n  fleet: SysV Init distribué (c’est la glue entre systemd et etcd), votre programme doit au minimum avoir 3 instances ? fleet s’en assurera !\n\n\n\n\n\n\nLa démonstration vous amènera à lancer 1 master et plusieurs machines “workers” et quelques containers Docker.\n\n\n\nKubernetes est la réponse de Google à la question des gestionnaires de containers disitribués.\n\nConstitué d’un certain nombre de composants qu’on ne détaillera pas ici, il permet de gérer des pods (un ou plusieurs containers qui doivent fonctionner localement sur le même host). Il intervient dans la répartition des applications dans le cluster, la distribution et l’ordonnancement des containers Docker.\n\nLiens:\n\n\n  CoreOS Doc\n  Kubernetes\n  Introduction to Kubernetes\n\n\n–\n\nResponsive and Fast: Iterating Live on a RWD Site\n\nCette conférence est globalement une redite des autres sur l’optimisation côté front. Colin Bendell d’Akamai nous présente plusieurs outils comme webpagetest, mais aussi des astuces pour tester sur Device depuis chrome. Il nous rappelle qu’il faut faire attention aux conditions de tests avec certains facteurs comme la connexion. Il faut faire aussi attention à limiter le nombre d’images, de ressources (js, css …). Un des gros problèmes sur un site responsive, est celui des images. Pour éviter de charger des images trop importantes, il faut utiliser la balise . Cette nouvelle balise n’étant pas disponible sur tous les navigateurs, il nous conseille d’utiliser un composant Picturefill. En ce qui concerne les CSS, il conseille d&#39;intégrer directement les css critiques dans le corps de la page et de ne charger, par la suite, que les css correspondants au device que l’on utilise. Pour conclure, l’utilisation d’un CDN avancé est hautement recommandée grâce à des options permettant de différencier navigateurs / devices.\n\nLiens :\n\n  Slide de la présentation\n\n\n\n\nBuild a device lab\n\n\n  “Qui a un placard avec pleins de devices en vrac qui n’ont ni câble, ni batterie et dont vous ne connaissez plus le mot de passe ?”\n\n\nJ’ai levé la main ;) .\n\nLara Hogan et Destiny Montague nous ont expliqué comment Etsy avait construit un device lab, permettant à leurs collaborateurs d’emprunter des appareils mobiles pour tester leurs applications, sites mobiles et newsletters.\n\nL’idée est d’outiller puissamment les équipes et de leur donner un accès extrêmement simple à un parc complet (même un chromebook pixel !) - afin d’assurer un maximum de tests sur les différents équipements.\n\nBien sûr il y a un device lab pour les équipes techniques et un autre pour le produit / marketing.\n\nLes sujets suivants ont été abordés :\n\n\n  choix des appareils\n  consommation électrique\n  le setup des devices (à l’aide d’un Mobile Device Management)\n  les tests\n  le réseau\n  un retour complet sur l’expérience utilisateur\n\n\nUn site complet dédié à leur conférence est disponible : https://larahogan.me/devicelab/.\n\nUne vidéo de la même conférence à New York est également en ligne :\n\n\n\n\n\nUne conférence un peu #old car déjà faite, mais toujours d’actualité concernant la problématique. Je suis bluffé par la capacité d’Etsy à mettre en oeuvre des moyens et des compétences sur des sujets qu’ils estiment importants. C’est sûrement en lien avec le succès que la société rencontre actuellement.\n\n\n\nConclusion\n\nUne conférence dense et intéressante, qui nous a donné l’opportunité de rencontrer pleins de gens intéressants et même de visiter (un peu) Barcelone !\n\n\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - seconde journée",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2014-day-2",
    "date"     : "November 24, 2014",
    "excerpt"  : "Velocity Barcelone, seconde journée\n\nDeuxième jour de conférence avec un programme encore plus chargé et quelques conférences alléchantes repérées au préalable.\n\nMorning Keynotes\n\nUpgrading the Web: Polyfills, Components and the Future of Web Deve...",
  "content"  : "Velocity Barcelone, seconde journée\n\nDeuxième jour de conférence avec un programme encore plus chargé et quelques conférences alléchantes repérées au préalable.\n\nMorning Keynotes\n\nUpgrading the Web: Polyfills, Components and the Future of Web Development at Scale - Andrew Betts (FT Labs)\n\nL’orateur fait remarquer que de nombreux systèmes existent pour packager et gérer les dépendances des applications backends, mais rien n’est disponible pour les composants webs. Il nous a présenté le projet Origami qui permet de réutiliser massivement des composants HTML.\n\n\n\nSlides : the Future of Web Development at Scale\n\nTroubleshooting Using HTTP Headers - Steve Miller-Jones (Limelight Networks)\n\nSlides : Troubleshooting Using HTTP Headers\n\nUn employé de Limelight nous a présenté comment l’ajout de headers dans une requête pouvait renvoyer des headers supplémentaires dans la réponse HTTP. Cela peut être utile pour débugguer et analyser un incident.\n  Cette présentation nous a rappelé, qu’en interne, nos gentils ops nous permettent déjà de faire ce genre chose sur nos proxy cache.\n\nMonitoring without Alerts - and Why it Makes Way More Sense than You Might Think - Alois Reitbauer (ruxit.com)\n\nAlois Reitbauer a évoqué la solution Ruxit développée depuis plus de trois ans. Cette solution consiste à installer un agent sur vos serveurs qui va automatiquement détecter des anomalies statistiques et corréler cette information avec d’autres déviations dans le but de trouver la root cause d’un incident.\n\nBeaucoup d’autres solutions de ce genre existent (et la plupart étaient dans le salon des sponsors). Nous n’avons pas été totalement convaincu de leurs capacités à détecter des root cause, mais elles sont toutes assez intéressantes et matures.\n\n\n\nLowering the Barrier to Programming - Pamela Fox (Khan Academy)\n\nPamela Fox nous a présenté l’initiative code.org, dont le but est de promouvoir l’enseignement de l’informatique (bon, apparement seulement aux US).\n\nElle a également donné quelques conseils si on veut s’investir dans l’enseignement de l’informatique à destination des plus jeunes. Par exemple créer un code club.\n\n\n\nSlides : Lowering the Barrier to Programming\n\nVelocity at GitHub - Brian Doll (GitHub)\n\nBrian a fait une présentation très … minimaliste. Il est revenu rapidement sur 7 ans de développement à GitHub et comment ils sont venus à développer l’Enterprise Edition. Il a évoqué différents problèmes que certains de leurs clients avaient et notamment avec l’utilisation de GE en environnement cloud.\n\nIl a donc annoncé le lancement de GitHub Enterprise 2.0 qui fonctionne maintenant sur AWS (et un changement de la grille tarifaire) !\n\n\n\nJ’ai profité d’un instant avec lui pour lui présenter GitHubTeamReviewer (un outil interne open-sourcé). Il était enchanté de découvrir ce qui avait été fait avec l’API de Github. Il a indiqué que l’entreprise travaillait actuellement sur des vues permettant de pallier aux problèmes résolus par GitHubTeamReviewer.\n\nHTTP Archive and Google Cloud Dataflow - Ilya Grigorik (Google)\n\nIlya Grigorik a présenté https://bigqueri.es/, un outil permettant d’interroger HTTP archive. La nouveauté est que le body des requêtes est maintenant conservé et que l’on peut l’analyser. Un engine Javascript a été intégré au SQL de bigqueries permettant de faire des requêtes très puissantes.\n\n\n\nPour ceux qui ne voudraient pas se plonger dedans, beaucoup de recherches faites par d’autres utilisateurs sont disponibles et abondamment discutées (exemple).\n\n\n\nWebpagetest-automation 2.0 - Nils Kuhn (iteratec GmbH), Uwe Beßle (iteratec GmbH)\n\nWebpagetest est un outil formidable mais il est difficile à automatiser. Les orateurs ont présentés un outil pour le faire, permettant donc de réaliser une mesure continuelle de la webperf avec un parcours utilisateur complet - démonstration à l’appui.\n\nLeur travail est disponible sur GitHub sous licence Apache : https://github.com/IteraSpeed/OpenSpeedMonitor. Un grand merci &amp;lt;3 ! (à 10 minutes sur la vidéo).\n\n\n\n\n\n\n\nEtsy’s Journey to Building a Continuous Integration Infrastructure for Mobile Apps - Nassim Kammah (Etsy)\n\nUne parmi les très nombreuses conférences Etsy sur la Vélocity (le moment de renouveller les conférenciers ?). Nassim Kammah nous a expliqué comment Etsy délivrait ses applications iOS.\n\nLa livraison des applications sous iOS est au même stade que la diffusion des logiciels via CD-ROMs. Partant de ce constat un système de build (avec 25 mac-minis derrière) a été mis en place à chaque commit sur le master. On ne peut pas délivrer une version de l’application tous les jours aux clients, mais on peut le faire pour les employés (and eat your own dog food) !\n\nIl y a également un système de gamification, autour de l’application livrée journalièrement, afin de motiver tout le monde à trouver des bugs.\n\n\n\nDes tests unitaires sont mis en place, ainsi que des tests fonctionnels avec AppThwack. Il est intéressant de constater qu’ils n’attendent pas, pour les tests fonctionnels, une réussite à 100% de la suite mais une tendance positive.\n\nLes équipes ont également mis en place des testing dojos dans lesquels les ingénieurs QA encadrent des salariés d’Etsy et testent à fond les applications.\n\nOn peut retrouver tous les éléments de cette conférence sur le blog technique d’Etsy.\n\n\n\nRecycling: Why the Web is Slowing Your Mobile App - Colin Bendell (Akamai)\n\nPourquoi recycler nos contenus pour les applications mobiles ?\n\n\n  accélérer le time to market.\n  réduire le risque\n\n\nLes APIs encouragent le recyclage.\n\n\n\nColin Blendel nous encourage à utiliser les mêmes recettes que pour les navigateurs web et à en ajouter d’autres :\n\n\n  gérer le pool de connexions en groupant les appels par domaine (quitte à les passer séquentiellement, par exemple, si des cookies sont utilisés),\n  surveiller les packet eaters (headers inutiles, Set-Cookies répétés),\n  setter correctement Content-Type sur des types standard (les exemples de content-type tirés des logs d’Akamai sont assez drôles, comme par exemple test/binary ^^ !),\n  faire un minimum de redirections,\n  fragmenter son cache au minumum (quitte à calculer des clés plus consistantes coté client),\n  ajouter du cache (Max-Age: 30s c’est à peu près du temps réel et ça change tout pour un CDN),\n  préfetcher les urls présentes dans les retours d’API, car on va surement en avoir besoin immédiatement après,\n  ne pas hésiter à mettre CRUD au placard et merger plusieurs appels API en un seul ; il faut trouver une balance efficace pour bien gérer la webperf.\n\n\nUne présentation dense et vraiment intéressante !\n\nSlides : Why the Web is Slowing Your Mobile App\n\n\n\nBreaking News at 1000ms\n\nLe Guardian est un journal Anglais présent sur le web et sur tout type de device. Ils ont récemment fait une refonte de leur site pour passer à une version Responsive avec pour challenge d’afficher son contenu en moins d’une seconde.\n\nLe Guardian c’est 110 000 utilisateurs, 7000 différents devices. L’ancien site avait un début de rendu en 8 secondes pour un affichage complet en 12. Avec la nouvelle version le site s’affiche en 1 seconde et le chargement complet au bout de 3. Quelles sont les principales optimisations ?\n\nPour commencer, il faut charge le contenu important pour l’utilisateur en premier, à savoir le menu, l’article, et le widget d’article populaire. Le reste du contenu sera chargé dynamiquement en JS.\n\nEn ce qui concerne le css, c’est la même chose. Les CSS importantes (critiques) qui concernent l’article et le rendu global sont intégrées inline. Ainsi, nous n’avons pas de blocage du rendu de la page. Le reste des css est chargé via Javascript. Avec ce système, on gagne au moins une demi-seconde sur le début d’affichage du contenu.\nPour gagner en fluidité pour les prochains affichages, le css est stocké en localStorage. On gagne ainsi des ressources pour les prochains chargements.\n\nPour les fonts ? C’est la même chose, elles sont mises en cache dans le localStorage pour supprimer de nouveaux chargements.\n\nEnfin le gros morceau : les images ! Elles sont chargées de façon asynchrone en lazyloading. Cela permet de ne pas bloquer le rendu principal de la page.\n\nEn complément, ils ont mis en place des outils, notamment pour monitorer dans Github la taille des Assets afin de vérifier qu’il n’y a pas de grosses variations.\n\nAvec ces optimisations et un système de Proxy qui va gérer les données mises en localStorage, le site peut même être accessible en mode offline.\n\n\n  Github du Front\n  Slide de la présentation\n\n\n\n\nOffline-first Web Apps\n\nMatt Andrews nous présente comment rendre une application web disponible Offline.\n\nPlusieurs contraintes peuvent nous pousser à avoir besoin d’une app (signet d’accueil) disponible même sans connexion. Que ce soit un article dans le métro ou une carte au milieu de nulle part sans connexion, il y a une réelle attente utilisateur.\n\nPremièrement, il faut activer AppCache en précisant qu’il faut faire un petit Hack pour qu’il soit vraiment utile (voir slide).\n\nEnsuite l’utilisation de plusieurs outils nous permet d’arriver à nos fins :\n\n  Utilisation de FetchApi : Il permet de remplacer nos appels Ajax avec une fonction succès , d’erreur et les Promises pour charger le contenu, ou lire le cache en cas d’absence de connexion.\n  Cache API : Il permet de choisir des Url a mettre en cache. Ainsi que de forcer le contenu de ces urls dans le code.\n  Service Worker : Il permet d’intercepter les events de chargement pour ensuite appeler le système de Cache API.\n\n\nToutes ces optimisations nous permettent d’accéder au site en Offline. Mais ces optimisations nous permettent aussi d’optimiser le chargement de nos pages puisqu’on limite le nombre d’appels HTTP avec la mise en cache de certaines ressources.\n\nSlide de la présentation\n\n\n\nLook, Ma, No Image Requests!\n\nPamela Fox nous présente comment elle a optimisé les images d’un site internet.\n\nLa première astuce est de compresser ces images au maximum. Il existe des outils online comme le site TinyPng qui compresse vos images et vous permet de les télécharger directement.\n\nDeuxième astuce, mettre les images dans les css en base 64. \nA noter qu’il existe des outils javascript qui effectuent la conversion dans les css à l’aide d’un petit commentaire en bout de ligne (voir les slides de présentation).\n\nTroisième solution : Les Fonts ! \nPour remplacer les petites images et surtout pour remplacer les sprites qui ne sont pas forcément adaptés, vous pouvez utiliser des Fonts. L’avantage des fonts est qu’elles peuvent s’adapter facilement en taille et en couleur … Des outils existent déjà pour les générer : Font Awesome.\n\nAutre astuce, le differ de chargement des images. Pamela nous propose son outils javascript, qui va permettre de vous simplifier les chargements. Il est aussi possible de ne charger que les images présentes à l’écran et de charger les suivantes lors du scroll. (lazyload).\n\nPour les vidéos la même astuce est possible. Puisque les vidéos sont à présent chargées dans des iFrame, leur contenu peut être chargé de façon différé. Attention, il ne faut pas remplir le href par une url blank, sinon on perd en temps de chargement.\n\nSlide de la présentation\n\n\n\nMicroservices - What an Ops Team Needs to Know\n\nSlides: Microservices - What an Ops Team Needs to Know\n\n\n\nLe buzzword est lâché. Le propos n’était pas ici de troller autour de la notion de micro-services, de l’implémentation ou de leur utilisation, mais plutôt du changement que cela implique pour les équipes d’exploitation.\n\nSouvent considéré comme le goulot d’étranglement de la chaîne de mise en prod, l’exploit’ regarde les architectures de micro-services avec circonspection : en plus d’avoir des dépendances entre eux, les composants sont mis à jour indépendamment et régulièrement, on peut donc vite tout casser en prod.\nPourtant en fournissant des services de bases et des outils aux équipes de développement, on peut augmenter leur autonomie et la disponibilité des infras.\n\nCela passe par:\n\n\n  automatiser les VMs ou les containers en prod comme en dev\n  un système de métriques “As a Service” (similaire à graphite / statsd)\n  un service de log central (logstash/heka/fluentd)\n  un outil de déploiement (capistrano/deployinator)\n\n\nCes outils et services ainsi fournis vont permettre à l’exploitation de se concentrer sur des problématiques plus complexes. En effet les microservices ont besoin d’outils de diagnostics plus poussés (on citera au passage Zipkin), d’alerting et de monitoring spécialisés par exemple.\n\n\n\nQui dit droits, dit devoirs, et là je paraphraserai notre orateur Michael Brunton-Spall:\n\n\n  Give developers pagers too !\n\n\n\n  Developers should be exposed to the pain they cause\n\n\nCela s’inscrit totalement dans le mouvement “You build it, you run it”, où les équipes de développement sont responsables de leur code depuis la conception jusqu’à la maintenance en production.\n\n\n\nIt’s 3AM, Do You Know Why You Got Paged ?\n\nSlides: It’s 3AM, Do You Know Why You Got Paged ?\n\nRyan Frantz nous a rappellé quelques éléments de bon sens concernant les alertes:\n\n\n  un contexte: quel hôte ? serveur ? service ? l’impact front / back ?\n  l’historique de l’alerte et de la métrique: état de la métrique il y a 5min, 15min, 1 jour, 1 semaine, combien de fois a sonné l’alerte aujourd’hui ?\n  la raison d’être du check (rédigée par le créateur du check)\n  des couleurs et mise en forme permettant de trouver visuellement l’information le plus rapidement possible (rappel il est 3 heure du matin, et peut être daltonien, pensez donc bien à vos codes couleurs)\n\n\n\n\n\nSeule une alerte critique doit vous faire lever à 3H du matin, un volume disque à 80% plein n’est pas réellement grave, cependant si son taux de remplissage est passé de 1% par heure à 300% par heure, cela peut devenir problématique.\n\nRyan nous a ensuite présenté nagios-herald. Ce plugin nagios permet de multiplexer une alerte dans différent services (cf schéma) ci dessous.\n\n\n\nPour ma part je préfère Sensu qui intègre de base ce type de mécanisme. On peut affecter à une alerte un groupe de handlers (alerte hipchat + graphite + logstash par exemple)\n\n\n\nCustomizing Chef for Fun and Profit\n\n\n\nSlides: Customizing Chef for Fun and Profit\n\nEn suivant les étapes d’application d’une recette Chef, Jon Cowie a distillé son savoir sur la personnalisation de Chef.\n\nIl nous a par exemple démontré qu’il était très simple de développer son propre plugin ohai et ses propres handlers.\n\nJ’ai apprécié le passage sur la gestion des événements Chef, en effet la sortie en ligne de commande n’est qu’une des façons de récupérer les logs, les événements sont basés sur un système de pub/sub, on pourrait très bien imaginer la publication en live stream dans un redis ou autre.\n\nPar ailleurs Jon vient de publier un livre sur le sujet:\nO’Reilly - Customizing Chef\n\n\n\nMega quiz Velocity\n\nPerry Dyball et Stephen Thair avaient préparé un quiz interactif avec les participants à la conférence. Des questions diverses et variées défilaient sur le grand écran et une application web permettaient à chacun d’y répondre. Un moment fun animé par deux animateurs survoltés.\n Malheuresement il semble que l’application n’aient pas tenu la charge et personne n’a pu voté après la seconde question (la prochaine fois ils devraient nous confier le projet :) ), mais un système de fallback a été prévu, basé sur des feuilles de papier de couleur à brandir bien haut pour répondre aux questions.\n\n\n  merci le papier ! :)\n\n\n\n\nConclusion\n\nFin des conférences et direction les soirées offertes par Facebook (où nous avons pu discuter avec Santosh Janardhan, responsable des infrastructures de Facebook ^^ !) et Dyn.\n\nLe résumé de la première journée est également disponible.\n"
} ,
  
  {
    "title"    : "Retour sur la Velocity Barcelone - premier jour",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2014-day-1",
    "date"     : "November 19, 2014",
    "excerpt"  : "Velocity Barcelone, premier jour\n\nBaptiste, François et Olivier ont eu la chance de participer à la Vélocity Conférence Europe 2014 qui avait lieu cette année à Barcelone.\n\nVoici le compte rendu des conférences et des moments qui les ont marqués.\n...",
  "content"  : "Velocity Barcelone, premier jour\n\nBaptiste, François et Olivier ont eu la chance de participer à la Vélocity Conférence Europe 2014 qui avait lieu cette année à Barcelone.\n\nVoici le compte rendu des conférences et des moments qui les ont marqués.\n\nMorning Keynotes\n\nLes keynotes du matin semblaient être scénarisées sur différents points que les organisateurs de la conférence voulaient mettre en avant.\n\nLife after human error - Steven Shorrock (EUROCONTROL)\n\nSteven Shorrock n’est pas un homme de l’IT, mais travaille autour de la sécurité aérienne. Il se définit comme un ergonomiste des systèmes. Il a présenté comment, autour des erreurs humaines, “les mots créaient le monde” et entrainaient immédiatement un jugement social (“négligence” est évidement plus connoté que “erreur d’attention”). Il peut y avoir des erreurs dans la définition d’une erreur. Qualifier une erreur demandait une définition précise de standards et de contextes. \nIl a également conseillé d’étudier les cas de fonctionnement normaux ; ne pas faire seulement des post-mortem mais des pre et des no mortem.\n\n\n\nUne présentation intéressante sur l’incident et l’erreur.\n\n\n\nMaximize the Return of Your Digital Investments - Aaron Rudger (Keynote Systems)\n\nUne présentation sponsorisée bien faite, montrant les difficultés de communication entre deux populations (IT et biz en l’occurence) et comment un outil performant et agréable peut aider à combler ce gap.\nChez M6Web nous utilisons grafana, et il est vrai que cet outil pourrait largement sortir du périmètre de l’IT.\n\nSlides : Maximize the Return of Your Digital Investments\n\n\n\nAlways Keep an Eye on Your Website Performance - PerfBar Khalid Lafi (WireFilter)\n\nUne rapide démonstration d’un outil en javascript à installer sur les postes de vos développeurs et permettant d’afficher des alertes si un site en production (ou ailleurs) dépasse un certain seuil.\n\nA découvrir : PerfBar\n\n\n\nThe Impatience Economy, Where Velocity Creates Value - Monica Pal (Aerospike Inc.)\n\nIl y a une génération on attendait 10 jours un échange de courrier postal, aujourd’hui un adolescent vérifie son téléphone toutes les 10 secondes ! Nous sommes moins attentifs, plus impatients.\nDe ce constat Monica Pal explique comment les backend web doivent s’adapter et servir de plus en plus d’informations contextualisées : search, sort, recommand, personalize.\n\nSlides : The Impatience Economy\n\n\n\nRecruiting for Diversity in Tech - Laine Campbell (Pythian)\n\nUn thème récurrent de la velocity de cette année. Laine explique comment l’ascenseur méritocratique est cassé et que seule une démarche volontaire permettra d’augmenter la diversité dans les entreprises.\n\nSlides : Recruiting for Diversity in Tech\n\n\n\nBetter Performance Through Better Design - Mark Zeman (SpeedCurve)\n\nLa dernière keynote était vraiment excellente. Mark Zeman, venu de Nouvelle Zélande, a expliqué comment le processus créatif pouvait aider à améliorer la performance. Dans ce but il a proposé de redesign the design process.\n\n  se fixer certains principes/objectifs de performance dès le départ\n  ajouter les designers dans la feature team et itérer via des prototypes\n  de partager le savoir sous forme d’informations visuelles (graphique mais aussi sous forme d’un bookmarklet indiquant quelle partie d’un site met du temps à charger)\n\n\n\n\n\n\nJe vous invite vivement à regarder sa vidéo :\n\n\n\n\n\nIT Janitor, How to Tidy Up - Mark Barnes (Financial Times)\n\nCe manager au Financial Times a expliqué comment le journal a été touché de plein fouet par la révolution du web mobile et a dû s’adapter très rapidement.\n\n\n\nIl a expliqué quelle stratégie il a adoptée pour tuer ou refaire les vieux systèmes et comment, en premier lieu, il a vendu le projet à ses supérieurs.\nIl a tout d’abord présenté le TCO de ce qu’il a appelé la version ”classic” de ft.com (la carotte) puis a appuyé sur la peur de l’incident et les problèmes de sécurité (le bâton ; le journal ayant été la cible des pirates syriens).\n\nAprès une analyse fine du traffic il a ensuite appliqué ces stratégies :\n\n  tuer directement une application inutile (il y en avait), quitte à la rallumer si quelqu’un finalement en à l’usage :) (et couper un serveur Solaris avec 1833 jours d’uptime !)\n  reécrire l’application et la redéployer sur le nouveau système\n  tuer une application et écrire plusieurs autres (découpage en micro services)\n\n\nSon crédo était ”try to make the right thing easier.” Ainsi les projets basés sur la nouvelle stack disposait out of the box de fonctionnalités de monitoring et de log. Cela a beaucoup motivé les équipes de développements.\n\nAu final la purge du legacy a apporté :\n\n\n  un gain de 30% de performance\n  un meilleur TTM\n  de substantiels retours sur investissement\n\n\nSlides : IT Janitor - How to Tidy Up\n\n\n\nMansplaining 101: Cisadmin Edition - Marni Cohen (Puppet Labs)\n\nLa conférence la plus geek de la journée. La conférencière a ouvert un terminal et a tapé\n\nbrew install feminism\n\n\n\n\nLa conférence était très sincère et didactique sur comment mieux intégrer les femmes dans l’IT.\n\nVoici les scripts et les ressources qu’elle a présentés : https://gitlab.com/marni/mansplaining\n\n\n\nBuilding the FirefoxOS Homescreen - Kevin Grandon (Mozilla)\n\n\n\nSlides : Building the FirefoxOS Homescreen\n\nConférence de présentation de l’OS pour smartphone de Firefox.\n\nLors de cette présentation, Kevin Grandon Ingénieur chez Mozilla nous a présenté le nouvel OS, et nous a initié à la programmation sur ce dernier.\nCe nouvel OS est donc basé sur des langages simples : HTML / CSS / Javascript.\n\nLe développement est donc assez facile à prendre en main, le débugage aussi car on peux monitorer tout ce qu’il se passe sur le device de test via un firebug dédié.\n\n\n\nDon’t Kill Yourself : Mobile Web Performance Tricks that Aren’t Worth it, and Somme that Are - Lyza Gardner (Cloud Four)\n\nOptimisations pour le web mobile.\n\nLyza Gardner nous a présenté sa vision de l’optimisation sur web mobile. Elle nous a tout d’abord fait un compte rendu sur son expérience personnelle. Liza a cherché via différentes analyses (speedIndex…) à trouver une relation entre temps de chargement, nombres d’assets etc… Et la conclusion qu’elle mettait en avant, c’est qu’il n’y avait pas de recette magique. \nElle a ensuite fait la parallèle entre le web lors de ces débuts qui était limité par le débit de nos connexions de l’époque, et le web mobile tel qu’il est actuellement. Ainsi certaines optimisations de l’époque sont adaptables, et même toujours valables, à nos problématiques actuelles.\nSelon elle, il ne faut pas optimiser un site pour le mobile, mais l’optimiser tout court. Elle propose de se fixer des objectifs, par exemple se fixer une limite de nombre d’appel asset. Mais surtout d’optimiser / limiter les images puisque 62% du trafic d’un site correspond a ces dernières.\n\n\n\nWhat are the Third-party Components Doing to Your Site’s Performance? - Andy Davies, Simon Hearne (NCC Group)\n\nSlides : Third-party components and site performance?\n\nNous utilisons tous des « Third-Party » sur nos sites, mais est-ce une bonne idée ?\n\nUn Third-Party est un script que nous chargeons depuis un autre site. Par exemple : Google Analitycs. Il existe différents type de Third-Party : la publicité, les analyseurs de trafic … La problématique est que nous ne pouvons pas controller ces outils. Nous n’avons pas la main sur le temps de chargement, la disponibilité de l’outils, et cela peut influer sur l’expérience utilisateur et la qualité de nos services.\n\nPour conclure, il faut trouver le bon compromis entre ce que nous apporte le Third-Party et ce qu’il peut nous coûter …\n\n\n\nGuide to Survive a World Wide Event - Almudena Vivanco, Mateus Bartz (Telefónica\n\nSlides : Survive a World Wide Event\n\nRetour d’expérience de Movistar TV, une chaîne payante multi-support qui a diffusé la coupe du monde en Espagne, au Brésil et en Argentine.\n\nCette société s’est confrontée à une problématique de traffic avec des pics de connexions importants en peu de temps. La société devait diffuser la coupe du monde FIFA 2014 dans plusieurs pays et sur plusieurs devices différents. Après des tests en condition réelles avant le début de la compétition, ils se sont aperçus qu’ils ne pouvaient pas gérer le pic de connexion qui arrivait entre 5 minutes avant le coup d’envoi et 5 minutes après, ainsi qu’à la reprise du match et début de deuxième mi-temps.\nIl a donc fallu tout refaire à plusieurs niveaux :\n\n  Création d’un CDN en interne\n  Refonte globale du système de connexion pour pouvoir supporter les pics.\n  Mise en place de monitoring via Graphite\n  Mise en place de Tests\n\n\nMise en avant de beaucoup de problématiques :\n\n  Multi plateforme\n  Déploiement sur plusieurs continents (Amérique du Sud, Europe)\n  Rassembler 11 outils de monitoring en un seul.\n\n\n\n\nIs TLS Fast yet ?\n\nSlides : Is TLS Fast yet ?\n\nTL;DR = Oui, il pourrait l’être !\n\nLe talent d’Ilya pour les conférences techniques a une fois de plus fait ses preuves. \nTout en détaillant l’utilité de TransportLayerSecurity (compression, vérification d’erreurs, authentification, chiffrement…) Ilya nous prouve que dans le meilleur des cas, un RTT supplémentaire est nécessaire et l’impact CPU très faible.\n\nOutre l’utilisation des dernières versions du Kernel, d’OpenSSL et de votre OS serveur, la performance de TLS passe aussi par la réutilisation d’éléments négociés lors de la première (et coûteuse) poignée de main. Cette optimisation se fait coté serveur en conservant les “sessions identifiers” coté serveur ou coté client avec un “cookie” chiffré, le “session ticket”. Il faudra bien entendu ajuster la durée de cache et/ou les timeouts (~ 1 jour).\n\nUne erreur fréquemment commise consiste à ne pas intégrer le certificat intermédiaire (peu de CA s’autorise à signer votre certificat avec leur CA Root) dans le certificat serveur ce qui a pour conséquence de stopper le render, ouvrir une nouvelle connexion tcp et https pour récupérer ce dernier chez l’autorité de certification.\n\nL’OSCP stappling permet lui d’inclure directement la réponse OCSP et ainsi éviter le même problème de blocage du rendu, connexion à un tiers etc…\n\nL’utilisation hasardeuse de redirection 301 peut considérablement augmenter le Time To First Byte de votre site, il est donc fortement conseillé de bien analyser ses chaînes de redirections (ex: https://domain.com =&amp;gt; https://www.domain.com =&amp;gt; https://www.domain.com) et d’utiliser HSTS. Ce header émis par le serveur permettra au navigateur de mettre en cache la décision de redirection vers https.\n\nLe talk s’est terminé par un tableau comparatif fort intéressant des serveurs HTTP et des CDNs concernant tous ces aspects.\n\nQuelques liens supplémentaires:\n\n\n  https://www.ssllabs.com/ssltest/\n  https://www.feistyduck.com/books/bulletproof-ssl-and-tls/\n\n\n\n\nMonitoring: the math behind bad behavior\n\nSlides : the math behind bad behavior\n\nLa détection d’anomalies dans les flux continus de données de type Time Series n’est pas une chose aisée.\n\nSe baser sur un percentile, une moyenne ou une mediane uniquement ne permet pas de capturer les phénomènes de saisonnalité et d’anomalies.\n\nThéo nous a proposé une méthode de détection de ces dernières appelée “lurching windows”. Sur des fenêtres de temps glissantes, on applique la méthode CUSUM (Cumulative Sum), qui somme les données en affectant un poids relatif (en réalité la probabilité que cette valeur existe).\n\nA voir: https://en.wikipedia.org/wiki/CUSUM\n\n\n\nWhat ops can learn from design - Robert Treat (OmniTI)\n\nSlides : What ops can learn from design\n\n“Un designer est quelqu’un qui design”.\n\nDerrière cette lapalissade se cache en réalité plusieurs concepts importants à intégrer pour toutes personnes produisant un code, un service utilisé par un tiers.\n\nNous sommes tous des designers. Il est donc indispensable de mettre en oeuvre 3 mécanismes simples pour faciliter l’utilisation de votre code/service.\n\nLe “Feedback”: le bon code de retour lors de l’échec d’un script, un message intelligible et contextualisé dans un log d’erreur applicatif, le “natural Mapping”: -d dans une option en ligne de commande pour indiquer –database, et le “force functions”: sous Unix, kill est par défaut non destructif, il faut forcer avec kill -9 pour tuer définitivement un processus, tout comme on vous force à fermer la porte de votre micro-onde pour le mettre en marche.\n\nConférence intéressante qui vous fera sentir moins coupable de ne pas savoir si il fallait pousser ou tirer une porte :)\n\n\n\nStatistical Learning-based Automatic Anomaly Detection @Twitter\n\nArun Kejariwal est maintenant un habitué de la Velocity, j’avais particulièrement apprécié sa présentation l’année dernière à Londres sur la détection d’anomalies chez twitter.\n\nL’ojectif est toujours le même: prédire la capacité pour ajouter du matériel en datacenter, détecter des événements particulier, distinguer le spam du trafic normal etc…\n\nLeur méthode est relativement identique à ce qui avait été présenté l’année dernière: sur deux semaines de données on applique un traitement du signal pour décomposer et filtrer la saisonnalité. Il “suffit” ensuite d’appliquer une regression ou un ESD sur les résidus pour détecter d’éventuelles anomalies.\n\nChose à savoir: Twitter va publier un package R contenant ces fonctions et algorithmes, qui seront donc utilisables par le commun des mortels !\n\nConclusion\n\nUne première journée intéressante et intense, sous le soleil de Barcelone !\n\n\n\nLe résumé de la seconde journée est également disponible.\n"
} ,
  
  {
    "title"    : "Configuration dynamique avec Symfony ExpressionLanguage",
    "category" : "",
    "tags"     : " configuration, symfony, cytron",
    "url"      : "/symfony-expression-language",
    "date"     : "November 17, 2014",
    "excerpt"  : "Grâce à notre bundle MonologExtra, nous avons la possibilité d’inclure des informations statiques dans le contexte de nos logs.\nNous souhaiterions maintenant avoir aussi d’autres informations plus dynamiques comme le nom de l’utilisateur.\n\nPour ce...",
  "content"  : "Grâce à notre bundle MonologExtra, nous avons la possibilité d’inclure des informations statiques dans le contexte de nos logs.\nNous souhaiterions maintenant avoir aussi d’autres informations plus dynamiques comme le nom de l’utilisateur.\n\nPour cela, nous avons donc ajouté la possibilité de configurer une expression qui sera évaluée par le composant ExpressionLanguage de Symfony de cette manière :\n\nm6_web_monolog_extra:\n    processors:\n        userProcessor:\n            type: ContextInformation\n            config:\n                env: expr(container.getParameter(&#39;kernel.environment&#39;))\n                user: expr(container.get(&#39;security.context&#39;).getToken() ? container.get(&#39;security.context&#39;).getToken().getUser().getUsername() : &#39;anonymous&#39;)\n\nPour interpréter cette expression, nous avons injecté dans notre processeur Monolog une instance de ExpressionLanguage ainsi que le container :\n\nservices:\n  m6_web_monolog_extra.expression_language:\n    class: Symfony\\Component\\ExpressionLanguage\\ExpressionLanguage\n    public: false\n  m6_web_monolog_extra.processor.contextInformation:\n    abstract: true\n    class: M6Web\\Bundle\\MonologExtraBundle\\Processor\\ContextInformationProcessor\n    arguments:\n      - @service_container\n      - @m6_web_monolog_extra.expression_language\n    calls:\n      - [ setConfiguration, []]\n\nNous utilisons une définition de service abstraite qui sert de modèle pour les services qui sont générés à partir de la configuration sémantique gérée par l’extension du bundle :\n\n&amp;lt;?php\nforeach ($config[&#39;processors&#39;] as $name =&amp;gt; $processor) {\n    $serviceId = sprintf(&#39;%s.processor.%s&#39;, $alias, is_int($name) ? uniqid() : $name);\n\n    $definition = clone $container-&amp;gt;getDefinition(sprintf(&#39;%s.processor.%s&#39;, $alias, $processor[&#39;type&#39;]));\n    $definition-&amp;gt;setAbstract(false);\n\n    $tagOptions = [];\n    if (array_key_exists(&#39;channel&#39;, $processor)) {\n        $tagOptions[&#39;channel&#39;] = $processor[&#39;channel&#39;];\n    }\n    if (array_key_exists(&#39;handler&#39;, $processor)) {\n        $tagOptions[&#39;handler&#39;] = $processor[&#39;handler&#39;];\n    }\n    $definition-&amp;gt;addtag(&#39;monolog.processor&#39;, $tagOptions);\n\n    if (array_key_exists(&#39;config&#39;, $processor)) {\n        if ($definition-&amp;gt;hasMethodCall(&#39;setConfiguration&#39;)) {\n            $definition-&amp;gt;removeMethodCall(&#39;setConfiguration&#39;);\n            $definition-&amp;gt;addMethodCall(&#39;setConfiguration&#39;, [$processor[&#39;config&#39;]]);\n        } else {\n            throw new InvalidConfigurationException(sprintf(&#39;&quot;%s&quot; processor is not configurable.&#39;, $processor[&#39;type&#39;]));\n        }\n    }\n\n    $container-&amp;gt;setDefinition($serviceId, $definition);\n}\n\nEt l’expression est finalement évaluée par le processeur en utilisant le composant quand la valeur est de la forme expr(...), ceci permettant de garder une compatibilité ascendante avec les configurations statiques précédentes.\n\n&amp;lt;?php \nprotected function evaluateValue($value)\n{\n    if (preg_match(&#39;/^expr\\((.*)\\)$/&#39;, $value, $matches)) {\n        return $this-&amp;gt;expressionLanguage-&amp;gt;evaluate($matches[1], [&#39;container&#39; =&amp;gt; $this-&amp;gt;container]);\n    }\n    return $value;\n}\n\nAvec la configuration présentée au début, nous récupérons ainsi l’environnement et l’utilisateur connecté dans le contexte de nos logs.\n\nMonologExtraBundle est disponible en open-source sur le compte GitHub de M6Web.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #7",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2014/11/13/m6web-dev-facts-7.html",
    "date"     : "November 13, 2014",
    "excerpt"  : "Ça faisait un moment ! Voici le retour des devfacts !\n\nReproduction\n\n  Je ne sais pas si ça corrige le bug qu’on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nAu moins, c’est efficace\n\n  Quand je fais un “echo $id”, ça affich...",
  "content"  : "Ça faisait un moment ! Voici le retour des devfacts !\n\nReproduction\n\n  Je ne sais pas si ça corrige le bug qu’on ne reproduit pas mais en tout cas je ne le reproduis toujours pas\n\n\nAu moins, c’est efficace\n\n  Quand je fais un “echo $id”, ça affiche l’id\n\n\nProprement sale\n\n  C’est pas forcément plus propre, mais c’est moins sale\n\n\nA une vache près !\n\n  C’est à peu près approximatif …\n\n\nIl faut savoir ce qu’on veut\n\n  C’est pas prévu pour être utile\n\n\nC’est louche.\n\n  Commençons par comprendre pourquoi le code fonctionne\n\n\nToi aussi fais du marketing …\n\n  On pourrait leverager le ROI du big data avec de l’analytics predictif.\n\n\nPour une fois ….\n\n  Pour une fois c’est pas un bug ! c’est un truc qui marche.\n\n\nPooh\n\n  Au sujet d’une sombre histoire d’expression de besoin \n“Je ne peux pas toujours les aider à faire leurs besoins…”\n\n\nComprends moi !\n\n  “Comprends mon incompréhension !”\n\n\nROI !\n\n  Y a autant d’utilisateurs que de jour homme pour ce projet !\n\n\n"
} ,
  
  {
    "title"    : "Retour sur le forum PHP 2014 organisé par l&#39;AFUP",
    "category" : "",
    "tags"     : " afup, php, forumphp, conference",
    "url"      : "/2014/11/12/retour-sur-le-forumphp2014.html",
    "date"     : "November 12, 2014",
    "excerpt"  : "M6Web était présent en force avec 5 collaborateurs présent à l’évènement. Voici un retour des conférences qui nous ont le plus marquées.\n\nVers des applications “12 factor” avec Symfony et Docker\n\nCette session avait pour objectif de nous présenter...",
  "content"  : "M6Web était présent en force avec 5 collaborateurs présent à l’évènement. Voici un retour des conférences qui nous ont le plus marquées.\n\nVers des applications “12 factor” avec Symfony et Docker\n\nCette session avait pour objectif de nous présenter la méthodologie du “twelve-factor app”, à travers des exemples concrets pour PHP à l’aide de Symfony et Docker.\n\n“The twelve-factor app” est une suite de recommandations, indépendante d’un langage de programmation particulier et pouvant s’appliquer à toutes sortes de logiciels développés en tant que service.\n\nSans revenir sur l’ensemble de la présentation, voici un retour sur les 12 facteurs :\n\n\n  Codebase : une app = un repo (ou équivalent) servant de source à tous les déploiements (dev / preprod / recette / prod  etc.). Exemples : git, mercurial etc.\n  Dependencies : déclaration explicite et complète de l’arbre de dépendances, utilisé uniformément pour tous les environnements. Exemples : composer, npm etc.\n  Config : séparation stricte config/code (Resources, Backing services, Credentials, Hostname etc.). Exemples : parameters.yml pour Symfony 2 ou utilisation de variables d’environnement avec Docker notamment. Utilisation de fig pour l’orchestration des containers docker.\n  Backing Services : tous les services utilisés par l’application sont accessibles par le réseau. Il n’y a pas de distinction entre les ressources locales et distantes car toutes sont accessibles via URL et/ou Credentials. Exemples : MySQL, RabbitMQ, Postfix, Redis, S3 etc.\n  Build, release, run : séparation stricte entre\n    \n      “build stage” : téléchargement d’une version du code et des dépendances. Exemples : “docker build”\n      “release stage“ : utilise le “build” et le combine avec la configuration du déploiement (une version sur un environnement). Exemple : “docker push”, utilisation de capistrano…\n      “run stage” : lancement de la “release” sur l’environnement cible. Exemple “docker run” ou “fig run”\n    \n  \n  Processes : chaque composant de l’application est ‘sans état’ et ne doit pas partager directement des données. Tout doit être partagé en “backing service”.\n  Port binding : les services doivent être disponibles en mettant à disposition un port d’accès, directement accessible. Cela permet une utilisation aisée en environnement de dev mais également de réutiliser les services.\n  Concurrency : une application respectant les “12 factor” est facilement scalable, quel que soit son type (web, worker, etc.) car elle repose sur des composants systèmes pour son pilotage.\n  Disposability : robustesse par le lancement et l’arrêt rapide des services, pour rendre chacune de ses services scalables.\n  Dev/Pro parity : homogénéité des environnements dev/prod et gain de temps pour la prise en main d’un projet. (mais le développeur n’aura pas une vision précise de la configuration… boite noire ?)\n  Logs : traitement des logs en tant que flux, utilisés par des services. Exemples : ELK, StatsD/Grafana etc.\n  Admin process : Exécuter les tâches de maintenance sur les mêmes environnements/containers. Exemples : docker exec\n\n\nslides\n\nPersonnellement, j’ai trouvé cette conférence vraiment riche et instructive. Peut-être un peu plus d’exemples de configuration fig/docker aurait pu illustrer d’avantage.\n\nLa mesure ce n’est pas que pour le devops\n\nLes conférenciers ont commencé leur présentation sur un rappel de ce qu’est le Lean Startup, héritier de la méthode Lean mise au point par Toyota. Nous connaissions la démarche Lean mais pas du tout son approche spécifique au lancement d’un produit.\n\nLe concept pourrait se résumer à : la base du lean startup est de savoir écouter -ses utilisateurs- car le succès dépend d’un feedback mesurable.\n\nLe processus d’application est très simple : un cycle construit/mesure/apprend.\n\nS’en est logiquement suivi une énumération des manières de mettre en oeuvre le processus en sachant prendre en compte les mesures qui importent (AAA, AARRR), plutôt que des “mesures de vanité” (followers, nombre de visite,..).\n\nEnfin pour appliquer ces mesures, une présentation des outils a disposition a été faite.\n\n\n\nPHP dans les distributions RPM\n\nSlides\n\nCette session avait comme objectif de faire un état de PHP dans les distributions RPM RHEL/Centos/Fedora.\n\nRHEL / Centos :\n\n\n  Objectif de stabilité à 10 ans\n  Stabilité binaire et de configuration sur la durée de vie de la distribution\n  RHEL : version payante avec support (contacts avec les ingénieurs RedHat, ressources en ligne, cycles de mises à jour garantis etc.).\n  Centos : même code que RHEL (juste recompilé) mais uniquement un support communautaire (comme fedora, ubuntu, suse…).\n  RHEL 5 : PHP 5.1 / RHEL 6 : PHP 5.3 / RHEL 7 : PHP 5.4\n  Application des patchs de sécurités sur les versions anciennes de PHP pendant 10 ans.\n  Possibilité d’utiliser des repos tiers pour choisir une version plus récente spécifique (comme ceux de Remi Collet) - mais pas de support officiel.\n  Distributions plutôt destinées à des applicatifs maintenus sur le long terme.\n\n\nFedora 21+ :\n\n\n  3 sous distributions : Workstation / Server / Cloud\n  Dernière version de PHP (PHP5.5 pour f20 et PHP5.6 pour f21)\n  Intégration continue de PHP dans les cycles de Fédora. Permet d’éviter les régressions.\n\n\nA venir : Software Collections (scl) permet d’avoir TOUTES les versions de PHP souhaitées simultanément sur la même installation de Fedora. Vraiment prometteur !\n\nExemple d’utilisation des SCL en cli :\n\nscl enable php56 -f myscript56.php\nscl enable php56 bash\nscl enable php53 -f myscript53only.php\nscl enable php53 bash\n\nDans une config apache :\n\n&amp;lt;VirtualHost *:80&amp;gt;\n    ServerName php56scl\n    \n    # Redirect to FPM server in php56 SCL\n    &amp;lt;FilesMatch \\.php$&amp;gt;\n    SetHandler &quot;proxy:fcgi://127.0.0.1:9006&quot;\n    &amp;lt;/FilesMatch&amp;gt;\n&amp;lt;/VirtualHost&amp;gt;\n\nFrameworks: A History of Violence\n\n\n\nFrancois Zaninotto nous a offert un vrai show en se mettant dans la peau d’un homme politique candidat à la présidence du parti des développeurs. Avec beaucoup d’humour il a fait un retour sur l’évolution (sa propre évolution ?) du développement web et son futur hypothétique, tout en distillant (son programme) de précieux conseils pour être un meilleur développeur.\n\nSon programme :\n\n\n  Le domaine d’abord : lier son développement métier à un minimum de tierce partie (pas facile à faire !),\n  Dites non au full-stack (ça se discute !),\n  L’application plurielle : ne pas hésiter à mélanger différents langages et différents projets dialoguant via http sur une même application,\n  Repenser le temps : passons aux 32h pour nous permettre de faire de la veille.\n\n\nA la communauté PHP nous pourrions proposer une synthèse (entendu ailleurs) : “soyons plus des développeurs web que des développeurs PHP, soyons plus des développeurs que des développeurs web”.\n\n\n\nRetour d’expérience ARTE GEIE : développement d’API\n\nUne conférence donnée par un de nos confrères d’ARTE sur des problématiques très actuelles pour nous. François Dume a expliqué la stratégie de mise en place d’une API autour de JSON API et des microservices. L’utilisation de OpenResty et du langage Lua couplé à un serveur oAuth en Symfony2 gérant la validation des tokens et le throttling.\n\nIl a ensuite expliqué en détail l’implémentation de {json:api} dans Symfony2, en mettant en avant de nombreuses contributions open-source.\n\n\n\nUne conférence didactique et claire.\n\n\n\nVDM, DevOps malgré moi\n\nMaxime Valette nous expliqué comment il a (à 20 ans à peine) crée un business incroyable sur Internet et a surtout réussi à gérer une augmentation de 30 à 40K visiteurs de plus chaque jour avec pratiquement juste sa b* et son c*.\n\n- Comment on fait ? \n- Comme on peut ! \n\n\nDe vrai qualité d’orateur pour Maxime et une conf très rafraichissante. Une démonstration de lean startup par l’exemple. Même si ce choix n’a pas été discuté, PHP était un choix naturel pour lui à l’époque.\n\n\n\nAn introduction to the Laravel Framework for PHP.\n\nBien que Symfony soit très largement majoritaire en Europe, Laravel est très populaire en Amérique du Nord, c’est donc avec curiosité que nous avons assisté à cette présentation du framework faite par Dayle Rees, core developer.\n\nUne fois passé la très longue présentation des livres et autres activités du conférencier, nous avons eu droit à une présentation générale du framework qui nous as fortement rappelé Symfony1 : utilisation de singleton à outrance, MagicBox (équivalent du sfContext), beaucoup de magie pour réduire la configuration (nom des contrôleurs).\n\nAu final, l’impression laissée est mitigée : certes, le seuil d’entrée est relativement réduit, tout est simple d’apparence, mais c’était la même chose pour Symfony1, et l’expérience nous a montré que lorsque l’on essayait de sortir le framework des sentiers battus, cette simplicité devenait un vrai obstacle.\n\nAu final, Laravel est sûrement une alternative intéressante pour les nostalgiques de Symfony1, puisque le projet est actif et maintenu. Mais, pour les projets que nous développons, Symfony2 reste une solution tout à fait adaptée.\n\nLaisse pas trainer ton log !\n\nOlivier Dolbeau nous a fait un retour sur la problématique d’accès et l’interprétation des logs sur les serveurs de production.\n\n\n\nIl nous as donc présenté la solution qu’il utilise, à savoir la stack ELK, pour ElasticSearch/LogStash/Kibana, qui permet à chaque serveur d’envoyer ses logs vers un serveur central, qui a pour charge de les agréger, et de permettre leur utilisation avancée.\nFini la recherche dans des fichiers textes plats qu’il faut commencer par comprendre, désormais vos applicatifs peuvent enrichir leurs logs, les envoyer sur un système dédié à la gestion des logs disposant de vraies interfaces de recherche et de consultation.\n\nNous avons été confortés dans notre idée, puisque nous mettons également en oeuvre cette solution.\n\nTable ronde “Etat des lieux et avenir de PHP”\n\nPascal Martin a animé d’une main de maître une table ronde sur l’avenir de PHP. Avec Jordi Boggiano, lead developer de Composer, Pierre Joye, core dev de PHP, Julien Pauli, release manager de PHP 5.5 et co-RM de PHP 5.6.\n\nLa communauté se pose beaucoup de questions sur le devenir de l’engine PHP et comment va évoluer le langage. \nLes débats ont été intenses et les invités ont pu répondre à des questions posées via Twitter. Au final peu de conclusions définitives. On peut déduire que malgré les alternatives proposées par HHVM et HippyVM, la communauté reste majoritairement sur PHP et est toujours très friande d’évolutions du langage et de sa performance. Les invités de la table ronde ont exhortés les participants à contribuer au code de PHP en nous fournissant pas mal de conseils.\n\n\n\nSlideshow Karaoké\n\nUne honte ! En plus les slides n’avaient aucun sens ! :) Bravo à Mc Kenny pour l’animation.\n\n\n\n\n\nUn grand merci à l’AFUP pour ce joli évènement ! Retrouvez pas mal de ressources partagés pendant l’event sur eventifier.\n"
} ,
  
  {
    "title"    : "Github Team Reviewer pour gagner la course aux Pull Requests",
    "category" : "",
    "tags"     : " outil, github, pull-requests, cytron, open-source",
    "url"      : "/github-team-reviewer-pull-requests.html",
    "date"     : "November 7, 2014",
    "excerpt"  : "Les PR, c’est le bien\n\nChez M6Web, nous utilisons Github Enterprise en interne pour nos projets privés et Github pour nos projets open-source. Grâce à ces outils, nous avons adopté de manière systématique l’usage des Pull Requests pour faire relir...",
  "content"  : "Les PR, c’est le bien\n\nChez M6Web, nous utilisons Github Enterprise en interne pour nos projets privés et Github pour nos projets open-source. Grâce à ces outils, nous avons adopté de manière systématique l’usage des Pull Requests pour faire relire et valider notre code par nos collaborateurs. La qualité de nos développements a ainsi été grandement améliorée au fil du temps.\n\nOui, mais…\n\nNous utilisons aussi HipChat pour communiquer au quotidien et rapidemment au sein de nos équipes. Chaque création de Pull Request émet une notification sur HipChat. Cependant, le nombre de pull requests initiées augmente avec le temps et chacun tend à ignorer peu à peu les notifications ou y fait moins attention. Les Pull Requests s’accumulent sur certains projets et nous n’avions, jusqu’à présent, pas vraiment de moyen pour lister par équipe toutes les PR en cours. Cela nous permettrait d’avoir une vue globale et d’être plus réactifs et rigoureux.\n\nIl y a bien le nouveau Pull Requests Dashboard de Github avec ses filtres de recherche avancée qui permet de répertorier toutes ses PR ou celles d’une organisation. Cette mise à jour n’est pas encore entrée en application dans Github Enterprise. Mais surtout, nous avons plusieurs équipes au sein d’une même organisation et nous voulons pouvoir les gérer de manière indépendante : cette fonctionnalité ne résoud pas notre problématique.\n\nGTR !\n\nNous avons donc développé Github Team Reviewer, un outil ultra simple mais efficace qui permet en un coup d’œil de voir toutes les PR de ses équipes et leur statut, qu’elles soient sur un Github Entreprise interne ou sur Github. Le projet utilise AngularJS et l’API fournit par Github. L’installation se fait sur n’importe quel serveur web et requiert npm (via Node.js) pour builder l’application grâce à Bower et Gulp.js.\n\nL’application propose volontairement un nombre limité de paramètres de configuration éditables dans le fichier config/config.json:\n\n\n  l’intervalle de rafraichissement de la liste des PR,\n  la liste des équipes en définissant pour chacune :\n    \n      son nom,\n      les utilisateurs Github concernés,\n      les organisations Github concernées,\n      l’url de l’API à interroger (pour Github Enterprise, par défaut l’url de l’API public de Github est utilisée),\n      un token utilisateur (utile pour augmenter le rate limit de l’API public).\n    \n  \n\n\nUne select box permet de basculer d’une équipe à une autre très facilement.\n\nGithub Team Reviewer est disponible en open-source sur le compte Github de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Providers AngularJS et configuration dynamique",
    "category" : "",
    "tags"     : " configuration, angular, cytron",
    "url"      : "/surcharger-un-provider-angular",
    "date"     : "October 9, 2014",
    "excerpt"  : "Nous avons eu besoin de surcharger un provider AngularJS – AnalyticsProvider – pour le rendre configurable dynamiquement en fonction d’un paramètre de la route. Le service $route n’étant pas disponible dans la phase de configuration d’AngularJS, i...",
  "content"  : "Nous avons eu besoin de surcharger un provider AngularJS – AnalyticsProvider – pour le rendre configurable dynamiquement en fonction d’un paramètre de la route. Le service $route n’étant pas disponible dans la phase de configuration d’AngularJS, il a fallu ruser…\n\nLe but est donc de changer la méthode $get de ce provider afin de lui ajouter notre dépendance et ainsi finir notre configuration.\n\nIl existe bien une méthode decorator() dans le service d’injection de dependance d’AngularJS, mais celle-ci ne permet que de décorer des services et pas leurs providers.\n\nNous allons donc mettre les mains dans l’$injector pour récupérer et modifier à la volée le provider :\n\nangular.module(&#39;myModule&#39;)\n  .config(function ($injector) {\n    var AnalyticsProvider = $injector.get(&#39;AnalyticsProvider&#39;);\n    var $get              = AnalyticsProvider.$get;\n    // ...\n  });\n\nMaintenant que nous avons le $get, il faut le modifier pour ajouter notre dépendance. Et c’est assez simple vu qu’il utilise l’annotation sous forme de tableau :\n\n// https://github.com/revolunet/angular-google-analytics/blob/e821407fe0436677cb42eafd5b338d767990b723/src/angular-google-analytics.js#L99\nthis.$get = [&#39;$document&#39;, &#39;$rootScope&#39;, &#39;$location&#39;, &#39;$window&#39;, function($document, $rootScope, $location, $window) {\n\nNous devons modifier ce tableau en ajoutant nos dépendances et en remplaçant la fonction :\n\n// la fonction d&#39;origine est le dernier élément du tableau\nvar origFn = $get[$get.length - 1];\n// on la remplace par notre dépendance\n$get[$get.length - 1] = &#39;$route&#39;;\n// on ajoute notre nouvelle fonction à la fin du tableau\n$get[$get.length] = function () {\n    // $route est le dernier argument\n    var $route = arguments[arguments.length - 1];\n    // on fait notre traitement\n    AnalyticsProvider.setAccount($route.current.params.partner ? &#39;partner-account&#39; : &#39;own-account&#39;);\n    // et qui rappelle la fonction originale\n    return origFn.apply(AnalyticsProvider, arguments);\n};\n\nOn peut noter l’utilisation de l’objet arguments qui permet de rester générique et de garder la compatibilité en cas de changement des dépendances du module surchargé.\n\nGrâce à cette astuce, notre service Analytics est maintenant configuré dynamiquement selon nos souhaits avant son utilisation.\n"
} ,
  
  {
    "title"    : "Améliorer la webperf de son application JS avec GruntJs",
    "category" : "",
    "tags"     : " webperf, angular, grunt, performance",
    "url"      : "/2014/09/30/ameliorer-la-webperf-de-son-application-js-avec-gruntjs.html",
    "date"     : "September 30, 2014",
    "excerpt"  : "L’un des principaux problèmes que nous rencontrons sur nos développement chez M6Web est la tenue en charge.\nQuand elles sont liées à des sites à fort trafic ou à une émission télé (#effetcapital), nos applications doivent être conçues pour support...",
  "content"  : "L’un des principaux problèmes que nous rencontrons sur nos développement chez M6Web est la tenue en charge.\nQuand elles sont liées à des sites à fort trafic ou à une émission télé (#effetcapital), nos applications doivent être conçues pour supporter des pics de charge plus ou moins importants.\n\nC’est une problématique qu’on croit souvent liée uniquement aux backends (scripts serveurs, bases de données etc), en oubliant souvent que le front-end est aussi, voir tout autant concerné.\n\nC’est notamment le cas pour une “Single Page Application” Angular.Js que nous développons en ce moment.\n\nL’objectif ici est d’avoir une application qui exécutera le moins de requêtes possible pour s’afficher, et qui ensuite sera quasiment autonome en ne faisant que le minimum de requêtes HTTP. Ceci afin de garantir, que lorsque quelqu’un charge l’application, l’expérience est quasi parfaite, même si entre temps, le CDN ou l’hébergement connaît une surcharge temporaire.\n\nL’autre avantage de diminuer le nombre d’appels HTTP, c’est aussi de limiter l’impact de la latence réseau, encore plus imposante dans notre cas, car notre cible est majoritairement mobile.\n\nPour les applications “Client-Side”, nous utilisons Grunt.Js pour automatiser toutes les tâches de développement, build, déploiement … (Nul doute que la même chose existe avec Gulp pour les plus Hipsters d’entre vous). Grunt regorge de plugins en tout genre pour automatiser énormément de choses coté WebPerf, commençons par le plus évident et le plus simple.\n\nP.S : Je passe volontairement l’installation/initialisation de Grunt ainsi que de ses plugins. Le web regorgeant de ressources là dessus.\n\nMinification HTML\n\nAfin de gagner quelques octets, nous allons minifier (suppression des espaces, retours charriot, et commentaires HTML) notre code HTML généré.\nPour ceci, nous utilisons le plugin grunt-contrib-htmlmin.\n\noptions: {\n        collapseWhitespace: true,\n        collapseBooleanAttributes: true,\n        removeCommentsFromCDATA: true,\n        removeOptionalTags: true,\n        removeComments: true\n      }\n\nMinification CSS\n\nMême chose au niveau des feuilles de styles avec grunt-contrib-cssmin.\n\nCompression des images\n\nAfin d’éviter d’avoir des images « brutes » de taille trop importante, on utilise grunt-contrib-imagemin pour compresser au build nos différentes images, afin de gagner quelques ko toujours précieux.\n\nInlining des images d’interface\n\nDans notre cas, où nous souhaitons réduire le nombre de requêtes HTTP superflues, nous avons opté pour l’inlining des images dites d’interface (boutons d’actions, picto etc).\n\nNous utilisons aussi le pré-compilateur CSS Less, par simplicité et pour éviter le DRY CSS.\nNous avons donc un premier fichier .less qui va contenir toutes les images d’interface sous cette forme :\n@facelessImg: url(&#39;images/faceless.jpg’);\n\nLe plugin Grunt grunt-css-url-embed sera configuré pour remplacer les urls présentes dans ce fichier par la version data-uri (=source de l’image encodée en base64).\nIl est important de se concentrer uniquement sur les images « d’interface », car le poids des images sera ici augmenté d’environ 30% (à cause du base64).\n\nDans notre CSS principale, on pourra ensuite mettre cette image en background d’une classe CSS :\n\n.faceless {\n  background-image: @facelessImg;\n}\n\nEt dans notre code HTML, on pourra placer l’image de la manière suivante :\n&amp;lt;span class=&quot;faceless&quot;&amp;gt;&amp;lt;/span&amp;gt;\n\nGrâce à cet ajout, nous économiserons une requête HTTP pour chacune des images.\n\nVersionning des assets\n\nUne autre bonne pratique est de versionner les assets en production. Cela signifie, donner un nom unique à chaque fichier statique (JS, CSS, image), ne changeant pas, tant que le fichier en question n’aura pas subi de modification, dans le but de pouvoir mettre un cache navigateur (Expire) et un cache CDN/Proxy Cache le plus long possible (Cache-control).\nNous passerons de /images/info.jpg à /images/a21992d7.info.jpg par exemple.\n\nNous utilisons ici le plugin grunt-rev (en combinaison avec grunt-usemin), qui va d’abord versionner les assets ayant changés, et ensuite, mettre à jour les références vers les fichiers en question dans tous vos fichiers HTML, CSS, JS.\n\nConcaténation des fichiers JS\n\nDirectement dans le code HTML, toujours avec le plugin grunt-usemin, vous allez pouvoir mettre des commentaires HTML pour définir quels ensembles de fichiers devront être concaténés.\nLa bonne pratique est d’avoir un fichier app.js avec son code maison, un fichier vendor.js avec les librairies tierces, et potentiellement un fichier de config.js\nEtant donné que dans notre cas, 99% du poids Js est concentré dans “Vendor”, nous avons décidé de concaténer l’ensemble dans un seul fichier.\n\n&amp;lt;!-- build:js(.tmp) scripts/risingstar.js --&amp;gt;\n  &amp;lt;script src=&quot;bower_components/jquery/dist/jquery.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;bower_components/angular/angular.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;config.js&quot;&amp;gt;&amp;lt;/script&amp;gt;\n  &amp;lt;script src=&quot;app.js&quot;&amp;gt;&amp;lt;/script&amp;gt; \n….\n&amp;lt;!-- endbuild —&amp;gt;\n\nInlining des templates\n\nPour finir, vous aurez peut-être remarqué, si vous développez des SPA avec Angular, ou un autre framework moderne, un changement de route (ou d’état) de votre application (ou l’affichage d’une directive) va impliquer des appels XHR pour charger les nouveaux templates à afficher. La bonne pratique ici étant de découper au maximum tous les templates dans des fichiers distincts.\nCela ne pose pas de problème en temps normal, mais dans notre cas, cela ne respecte pas nos ambitions de départ.\n\nAngular a la particularité de permettre d’utiliser la balise script pour charger des templates :\n\n&amp;lt;script type=&quot;text/ng-template&quot; id=&quot;views/info.html&quot;&amp;gt;Code HTML du template&amp;lt;/script&amp;gt;\n\nSi votre routeur ou une directive demande un template, avant de vérifier si le fichier existe, Angular vérifiera si une balise &amp;lt;script type=’text/ng-template’&amp;gt; a été déclarée avec l’identifiant correspondant au chemin demandé.\n\nGrunt via le plugin grunt-angular-inline-templates, nous permet d’automatiser cette tâche au build, afin de regrouper dans le index.html du build, tous les templates dans un script avec l’id correspondant au chemin du fichier html original. De cette manière, nous n’avons plus aucun appel HTTP à faire pendant toute l’utilisation de l’application.\nAttention toutefois, cela signifie que le poids du fichier HTML original va forcément augmenter.\n\nConclusion\n\nComme vous avez pu le voir, nous avons grandement optimisé notre application, en utilisant simplement des plugins Grunt à notre disposition. Nous travaillons donc sur un espace de développement respectant toutes les bonnes pratiques (découpages des fichiers JS, CSS, HTML au maximum, code commenté …) et toutes les opérations d’optimisation sont automatiquement effectuées au build, fait avant chaque déploiement.\n\nAttention, cela signifie aussi que votre projet en production devient relativement différent de celui que vous testé en développement. Il devient donc important de mettre en place des tests fonctionnels sur le build de production (avec Protractor par exemple, ou même Behat), et de tester régulièrement la bonne génération et le bon fonctionnement du build de prod.\n"
} ,
  
  {
    "title"    : "Tests E2E sur son application AngularJS avec Protractor",
    "category" : "",
    "tags"     : " qualite, tests, javascript, angular, protractor, cytron",
    "url"      : "/tests-e2e-application-angularjs-protractor.html",
    "date"     : "September 24, 2014",
    "excerpt"  : "Familier des tests fonctionnels avec Behat et Atoum pour des applications majoritairement PHP, nous l’étions beaucoup moins avec les tests end-to-end pour des applications pures Javascript, qui plus est, sous AngularJS. Les tests end-to-end ou tes...",
  "content"  : "Familier des tests fonctionnels avec Behat et Atoum pour des applications majoritairement PHP, nous l’étions beaucoup moins avec les tests end-to-end pour des applications pures Javascript, qui plus est, sous AngularJS. Les tests end-to-end ou tests e2e ne sont autres que des tests fonctionnels dans la domaine du Javascript. L’objectif de cet article est de montrer le cheminement que nous avons emprunté pour mettre en place ces tests sur une de nos applications et pour gérer les difficultés qui en ont découlé.\n\nLe contexte\n\nIl s’agit d’une application web présentant des écrans différents à l’utilisateur en fonction des données contenues dans un fichier distant requêté à intervalle régulier court (quelques secondes). L’utilisateur est invité ou non à agir avec les vues, principalement en appuyant sur des boutons, qui changent l’état interne de l’application et peut, a posteriori, influer sur les écrans suivants.\n\nMettre en place Protractor\n\nLa première étape consiste à installer Protractor, framework de tests e2e dédié à AngularJS et utilisant Node.js. Si vous utilisez Grunt pour gérer les tâches de build de votre projet, il suffit d’exécuter la commande :\n\nnpm install grunt-protractor-runner --save-dev\n\nPuis on crée le fichier de configuration dans le projet :\n\n/* protractor-local.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;\n};\n\nTous les tests e2e de notre application sont écrits dans des fichiers javascript dont le nom est suffixé par .e2e.js. Nous avons en effet fait le choix d’une architecture modulaire se retrouvant dans l’organisation des dossiers de notre projet : les fichiers de tests e2e se trouvent dans les mêmes répertoires que les controllers auxquels ils sont rattachés 1.\n\nUn navigateur pour mes tests\n\nPour exécuter ses tests dans les conditions réelles de son application, il faut un navigateur. Nous développons sur un serveur distant en SSH. Le seul navigateur utilisable est donc un browser headless, le plus connu et utilisé étant PhantomJS. Cependant, combiné à Protractor, ce dernier est particulièrement instable pour le moment et il n’est pas recommandé de l’utiliser. Nous optons donc pour Chrome (via le plugin chromedriver). Nécessitant une interface graphique, nous ne pourrons donc pas lancer nos tests sur le serveur de développement mais nous devrons le faire en local sur nos machines.\n\n/* protractor-local.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;,\n  maxSessions: 1,\n  multiCapabilities: [\n    { browserName: &#39;chrome&#39; }\n  ]\n};\n\nOn installe les binaires nécessaires au lancement de Chrome via Protractor :\n\n./node_modules/grunt-protractor-runner/node_modules/.bin/webdriver-manager update\n\nPuis on ajoute les tâches Grunt :\n\n/* Gruntfile.js */\ngrunt.initConfig({\n  connect: {\n    dist: {\n      options: {\n        port: 9000,\n        hostname: &#39;localhost&#39;,\n        base: &#39;dist&#39;\n      }\n    }\n  },\n  protractor: {\n    local: {\n      options: {\n        configFile: &quot;protractor-local.conf.js&quot;\n      }\n    }\n  }\n});\n\ngrunt.registerTask(&#39;test&#39;, [\n  &#39;build&#39;,\n  &#39;connect:dist&#39;,\n  &#39;protractor:local&#39;\n]);\n\nIntégration continue\n\nL’ensemble de nos projets joue automatiquement leurs tests sur un serveur Jenkins commun qui ne dispose pas de navigateurs graphiques. Nous aurions pu mettre en place au sein de notre infrastructure un serveur Selenium pour répondre à cette problèmatique. Mais les contraintes du projet ne nous autorisaient pas à y consacrer le temps nécessaire. Nous avons donc opté pour une solution tiers plus rapide à mettre en œuvre : SauceLabs, plateforme de tests hébergée dans le “cloud”.\n\nUne fois enregistré sur le site, on crée un nouveau fichier de configuration Protractor :\n\n/* protractor-saucelabs.conf.js */\nexports.config =  {\n  specs: [&#39;app/**/*.e2e.js&#39;],\n  baseUrl: &#39;https://localhost:9000/&#39;,\n  allScriptsTimeout: 30000,\n  jasmineNodeOpts: {\n    defaultTimeoutInterval: 60000\n  },\n  maxSessions: 1,\n  sauceUser: &#39;mySauceUser&#39;,\n  sauceKey: &#39;mySauceKey&#39;,\n  multiCapabilities: [\n    {\n      browserName: &#39;chrome&#39;,\n      platform: &#39;Linux&#39;\n    },\n    {\n      browserName: &#39;firefox&#39;,\n      platform: &#39;Linux&#39;\n    },\n    {\n      browserName: &#39;safari&#39;,\n      platform: &#39;OS X 10.9&#39;\n    },\n    {\n      browserName: &#39;chrome&#39;,\n      platform: &#39;Windows 8.1&#39;\n    }\n  ]\n};\n\nNotons que l’on peut lancer ses tests sur autant de couples OS/navigateurs que l’on souhaite en remplissant le tableau multiCapabilities. Le fichier de configuartion Grunt doit être adapté pour lancer SauceConnect, l’interface entre SauceLabs et l’application, avant le démarrage des tests :\n\n/* Gruntfile.js */\ngrunt.initConfig({\n  connect: {\n    dist: {\n      options: {\n        port: 9000,\n        hostname: &#39;localhost&#39;,\n        base: &#39;dist&#39;\n      }\n    }\n  },\n  protractor: {\n    local: {\n      options: {\n        configFile: &#39;protractor-local.conf.js&#39;\n      }\n    },\n    saucelabs: {\n      options: {\n        configFile: &#39;protractor-saucelabs.conf.js&#39;\n      }\n    }\n  },\n  run: {\n    installsc: {\n      options: {\n        wait: true\n      },\n      cmd: &#39;bash&#39;,\n      args: [\n        &#39;-c&#39;,\n        &#39;test -d sc-4.2-linux || (wget https://saucelabs.com/downloads/sc-4.2-linux.tar.gz &amp;amp;&amp;amp; tar xvf sc-4.2-linux.tar.gz)&#39;\n      ]\n    },\n    sauceconnect: {\n      options: {\n        wait: false,\n        quiet: true,\n        ready: /Sauce Connect is up/\n      },\n      cmd: &#39;./sc-4.2-linux/bin/sc&#39;,\n      args: [\n        &#39;-u&#39;,\n        &#39;mySauceUser&#39;,\n        &#39;-k&#39;,\n        &#39;mySauceKey&#39;\n      ]\n    }\n  }\n});\n  \n\ngrunt.registerTask(&#39;test-e2e&#39;, function (target) {\n  var tasks = [\n    &#39;build&#39;,\n    &#39;connect:dist&#39;\n  ];\n\n  if (target === &#39;local&#39;) {\n    tasks.push(&#39;protractor:local&#39;);\n  } else {\n    tasks.push(&#39;run:installsc&#39;);\n    tasks.push(&#39;run:sauceconnect&#39;);\n    tasks.push(&#39;protractor:saucelabs&#39;);\n    tasks.push(&#39;stop:sauceconnect&#39;);\n  }\n\n  grunt.task.run(tasks);\n});\n\nAvec cette configuration, nous lançons les tests en local sur notre machine avec la commande grunt test-e2e:local ou à distance sur SauceLabs avec grunt test-e2e.\n\nNotre premier test\n\nLe premier test que nous avons écrit pour valider l’architecture est plutôt basique :\n\ndescribe(&#39;Controller: MainCtrl&#39;, function () {\n  it(&#39;should work&#39;, function () {\n    browser.get(browser.baseUrl);\n    expect(true).toBe(true);\n  })\n});\n\nOn remarque que l’écriture d’un test e2e utilise, comme les tests unitaires, la syntaxe du framework Jasmine : un bloc describe regroupe une suite de tests définis dans des blocs it. Les variables de configuration définies dans les fichiers de configuration Protractor sont utilisables via la variable globale browser, variable qui nous permettra d’entretenir le lien entre nos tests et le code exécuté dans le navigateur. Pour mieux appréhender les étapes du processus et les erreurs qui se produisent, il est en effet très important de bien comprendre la séparation entre le code Javascript exécuté dans Node.js via Protractor, qui correspond au déroulement des tests, et le code Javascript de notre application qui lui est exécuté dans le browser et avec lequel on ne peut interagir depuis les tests que par certaines fonctions du framework (element, executeScript, addMockModule, etc.)2. Ce sont deux univers d’exécution bien distincts.\n\nDébugger avec Protractor\n\nLorsque vous lancerez les tests en local, vous remarquerez que Chrome est réellement exécuté mais vous ne verrez pas grand chose car l’affichage est bien trop rapide. Il est possible de mettre des points d’arrêt dans ses tests pour y voir plus clair et pour, par exemple, consulter la console Javascript du navigateur. Pour cela, il faut utiliser la fonction browser.debugger() comme point d’arrêt et ajouter l’option debug dans la configuration Grunt :\n\n/* Gruntfile.js */\nprotractor: {\n  local: {\n    options: {\n      configFile: &#39;protractor-local.conf.js&#39;,\n      debug: true\n    }\n  }\n}\n\nPour passer d’un point d’arrêt à l’autre, on saisit c comme continue. Notez que cela ne fonctionnera pas si vous avez plus d’un navigateur dans le tableau multiCapabilities de votre configuration.\n\nOn peut également ajouter l’option --debug à la commande grunt test-e2e:local pour afficher l’ensemble des requêtes lancées par l’application.\n\nMocker sa config\n\nComme souvent dans les projets AngularJS, nous utilisons un module pour définir nos variables de configuration :\n\nangular.module(&quot;config&quot;, [])\n  .constant(&quot;config&quot;, {\n    &#39;ma_variable&#39;: &#39;une_valeur&#39;\n  });\n\nDans les tests e2e, on veut tout tester, en particulier les comportements qui diffèrent en fonction des valeurs de configuration. Comment faire puisque ce module est chargé une fois pour toute au lancement de l’application ? Protractor introduit la fonction addMockModule qui permet de bouchonner à la volée un module Angular.\n\nit(&#39;comportement avec une autre valeur&#39;, function () {\n  browser.addMockModule(&#39;config&#39;, function () {\n  \tangular.module(&#39;config&#39;, []).constant(&#39;config&#39;, {\n    \t\t&#39;ma_variable&#39;: &#39;une_autre_valeur&#39;\n  \t});\n  });\n\n  // mon test\n  \n  browser.removeMockModule(&#39;config&#39;);\n});\n\nMocker le service $http\n\nDans notre application, un fichier externe est requêté régulièrement via le service Angular $http. AngularJS fournit déjà un mock complet de ce service nommé $httpBackend. Pour y avoir accès, il faut ajouter la dépendance angular-mocks en devDependencies dans son fichier bower.json et inclure le fichier bower_components/angular-mocks/angular-mocks.js dans l’application en développement. $httpBackend permet de définir quels appels HTTP doivent être interceptés et quelles réponses doivent être renvoyées.\n\nLa difficulté dans notre cas réside dans le fait de pouvoir simuler le changement d’état du fichier distant dans un même test pour pouvoir vérifier les changements de vue qui en découlent. Il est possible de le faire directement via $httpBackend moyennant quelques acrobaties, mais la librairie HttpBackend simplifie grandement son utilisation pour ce type de tests 3.\n\nvar HttpBackend = require(&#39;httpbackend&#39;);  \nvar backend;\n\ndescribe(&#39;Test workflow&#39;, function() {  \n  beforeEach(function() {\n    backend = new HttpBackend(browser);\n  });\n\n  afterEach(function() {\n    backend.clear();\n  });\n\n  it(&#39;should display result when status is changed to RESULT&#39;, function(done) {\n    backend.whenJSONP(/status.json/).respond({status: &#39;initial&#39;});\n\n    browser.get(&#39;/&#39;);\n\n    var result = element(by.binding(&#39;result&#39;));\n    expect(result.getText()).toEqual(&#39;no result&#39;);\n    \n    backend.whenJSONP(/status.json/).respond({status: &#39;result&#39;, percentage: 70});\n    \n    browser.wait(function () {\n      return browser.getLocationAbsUrl().then(function (currentUrl) {\n        return currentUrl === &#39;https://localhost:9000/#/result&#39;;\n      });\n    }, 5000).then(function () {\n      expect(result.getText()).toEqual(&#39;70 %&#39;);\n      done();\n    });\n  });\n});\n\nOui, mais…\nProtractor nous a été indispensable pour implémenter les tests fonctionnels sur notre application car son intégration avec AngularJS offre des possibilités que les autres frameworks de tests fonctionnels n’ont pas. On pense principalement à la synchronisation qui est mise en œuvre entre les tests et l’initialisation d’Angular dans la page (“wait for angular”). Cependant, avec le recul que l’on peut avoir sur notre projet :\n\n\n  il faut l’avouer, Protractor n’est pas aussi simple à mettre en place que Behat par exemple,\n  le debuggage est assez pénible car les messages d’erreur sont souvent peu verbeux et, c’est l’inconvénénient de tester du javascript avec du javascript, on ne sait pas toujours où se situe l’erreur (dans les tests ou dans le code applicatif ?),\n  Protractor est parfois instable avec les webdrivers utilisés, ce qui nous oblige à relancer les tests manuellement,\n  nos tests dans SauceLabs sont (très) lents, ce qui nous a contraint à la longue à réduire le nombre de navigateurs testés (améliorant par la même occasion la stabilité des tests).\n\n\n\n  \n    \n      Scalable code organization in AngularJS &amp;#8617;\n    \n    \n      Protractor API &amp;#8617;\n    \n    \n      Angular e2e tests, Mock your backend. &amp;#8617;\n    \n  \n\n"
} ,
  
  {
    "title"    : "Contrôlez facilement votre cohérence de code sur votre projet Symfony2 avec coke",
    "category" : "",
    "tags"     : " code sniffing, coke, Symfony2",
    "url"      : "/2014/08/05/verifier-la-coherence-du-code-d-un-projet-symfony2-avec-coke.html",
    "date"     : "August 5, 2014",
    "excerpt"  : "Pour qu’un projet persiste dans le temps, il est important que le style de codage soit le même. Et quand vous vous reposez sur des outils, autant faire en sorte que le style de codage retenu soit proche, si ce n’est le même, que les briques que vo...",
  "content"  : "Pour qu’un projet persiste dans le temps, il est important que le style de codage soit le même. Et quand vous vous reposez sur des outils, autant faire en sorte que le style de codage retenu soit proche, si ce n’est le même, que les briques que vous utilisez. Et dans le cas où vous utilisez un framework, c’est d’autant plus important.\n\nAvec Symfony2, c’est d’autant plus facile que l’architecture des bundles est très marquée, et qu’un coding guide est publié.\n\nÇa, c’est pour la théorie, mais en pratique, si ce n’est pas super simple, automatique, une somme de toutes petites erreurs apparaissent et le sentiment d’abandon s’installe rapidement.\n\nCoke\n\nIl y a un peu plus d’un an, chez M6Web, nous avons développé coke pour configurer simplement l’exécution de PHP_CodeSniffer.\n\nDepuis quelques mois, il est possible d’installer coke via Composer :\n\n{\n  &quot;require&quot;: {\n    &quot;m6web/coke&quot;: &quot;~1.2&quot;\n  }\n}\n\nL’avantage de passer par Composer, c’est que coke va lui-même installer PHP_CodeSniffer en tant que dépendance Composer (dans le dossier vendor), permettant de ne pas avoir à suivre la fastidieuse procédure d’installation via PEAR.\n\nInstaller un coding standard via Composer\n\nLorsque nous voulons utiliser un coding standard qui n’est pas inclus par défaut avec PHP_CodeSniffer, il est possible de l’installer en utilisant Composer\n\nSymfony2-coding-standard\n\nChez M6Web, nous maintenons le standard Symfony2-coding-standard qui permet de valider que le code d’un projet respecte les coding standard de Symfony2.\n\nPour rendre à César ce qui appartient à César, nous avons récupéré la base du standard telle que créé par opensky.\n\nSi nous avons décidé de le forker, c’est que la structure ne correspondait pas à ce qui est nécessaire pour une installation de ce standard via Composer\n\nProcédure complète, pas-à-pas\n\nCréer le fichier composer.json suivant :\n\n{\n  &quot;require-dev&quot;: {\n    &quot;m6web/coke&quot;                       : &quot;~1.2&quot;,\n    &quot;m6web/symfony2-coding-standard&quot;   : &quot;~1.1&quot;,\n  }\n}\n\nInstaller les dépendances Composer :\n\ncomposer install\n\nCréer le fichier .coke suivant :\n\n# Standard used by PHP CodeSniffer (required)\nstandard=vendor/m6web/symfony2-coding-standard/Symfony2\n\nIl est désormais possible d’appeler la commande suivante pour valider le style de codage de votre projet\n\n./vendor/bin/coke\n\nConclusion\n\nAvec cette technique, il est très simple de valider le style de codage d’un projet. Du coup, plus d’excuse pour ne pas le faire ;)\n\nBonus\n\nL’idéal, pour ne jamais commiter un code ne respectant pas les conventions de codage, est d’utiliser les hooks de commit pour que cette vérification soit faite automatiquement.\n\nLa manière la plus simple de le faire est d’ajouter la ligne ./vendor/bin/coke dans le fichier .git/hooks/pre-commit, mais cette méthode a le défaut de vérifier tout le projet, et pas uniquement le code modifié et à commiter.\n\nPour aller plus loin, vous pouvez vous inspirer du script suivant qui ne lance coke que sur les fichiers dans le “staging” de Git (les fichiers à commiter).\n\n"
} ,
  
  {
    "title"    : "M6Web était présent au PHPTour Lyon 2014",
    "category" : "",
    "tags"     : " afup, phptour, conference, video",
    "url"      : "/2014/06/25/m6web-etait-au-phptour-lyon-2014.html",
    "date"     : "June 25, 2014",
    "excerpt"  : "Le Lundi 23 et Mardi 24 juin a eu lieu l’événement PHP de l’année à Lyon : le PHPTour Lyon.\nÀ cette occasion, les équipes d’M6Web ont présenté un talk, dont voici les slides et vidéos :\n\n#Nouveau socle pour une nouvelle vie, chez M6Web (par Kenny ...",
  "content"  : "Le Lundi 23 et Mardi 24 juin a eu lieu l’événement PHP de l’année à Lyon : le PHPTour Lyon.\nÀ cette occasion, les équipes d’M6Web ont présenté un talk, dont voici les slides et vidéos :\n\n#Nouveau socle pour une nouvelle vie, chez M6Web (par Kenny Dits)\n\nLa seconde conférence de @techM6Web a été tenue par Kenny Dits (@kenny_dee) : “Nouveau socle pour une nouvelle vie, chez M6Web”.\n\n\n\n\n\nVoir les commentaires sur Joind.in\n\nNous avons aussi retrouvé une bonne partie des développeurs de l’équipe (anciens ou actuels) qui ont joué le jeu de la borne photo Pixiway mise à disposition :\n\n\n\n#Conclusion\n\nL’Afup a encore réalisé un boulot considérable cette année, pour accoucher sans aucun doute, du meilleur PHP Tour jamais fait.\nBravo à toute la team pour l’organisation sans faille, et aux autres speakers pour la qualité de leurs talks.\n"
} ,
  
  {
    "title"    : "M6Web sera présent au PHPTour Lyon 2014",
    "category" : "",
    "tags"     : " afup, phptour",
    "url"      : "/2014/05/15/m6web-sera-present-au-phptour-lyon-2014.html",
    "date"     : "May 15, 2014",
    "excerpt"  : "M6Web sera bien représenté au PHPTour 2014 organisé par l’AFUP et est très heureux de soutenir l’évènement en étant sponsor Argent.\n\n\n\nVenez nombreux augmenter votre pilosité faciale, tel un vrai sysadmin.\n\nFaites le plein d’anecdotes croustillant...",
  "content"  : "M6Web sera bien représenté au PHPTour 2014 organisé par l’AFUP et est très heureux de soutenir l’évènement en étant sponsor Argent.\n\n\n\nVenez nombreux augmenter votre pilosité faciale, tel un vrai sysadmin.\n\nFaites le plein d’anecdotes croustillantes et découvrez l’histoire de M6Web Lyon avec Kenny Dits.\n"
} ,
  
  {
    "title"    : "Babitch, the story behind our table soccer web application",
    "category" : "",
    "tags"     : " opensource, babyfoot, angularjs, d3js, symfony",
    "url"      : "/2014/04/23/babitch-the-story-behind-our-table-soccer-web-application.html",
    "date"     : "April 23, 2014",
    "excerpt"  : "At M6Web, we love playing foosball!\nWe have one old (incredibly strong) soccer table in our « fun room », and at lunch time, a part of us enjoy playing it.\n\nThe soccer table in enterprise is awesome for a lot of things:\n\n\n  Team building between e...",
  "content"  : "At M6Web, we love playing foosball!\nWe have one old (incredibly strong) soccer table in our « fun room », and at lunch time, a part of us enjoy playing it.\n\nThe soccer table in enterprise is awesome for a lot of things:\n\n\n  Team building between each players,\n  Don’t think about work (almost) when we are playing,\n  Fun! a lot of!\n  Attract good people …\n\n\nOur rules are simple : Doubles (4 players) only, and the first team at ten win.\n\nSo, as programmers, we tend to have software ideas for each questions in our life ! and the questions we had with foosball were:\n\n\n  Who is the best player?\n  How many goals did I score ?\n  Who won the most games?\n  …\n\n\nSo, we begin to talk several months ago about a foosball app, allowing us to record each games, and each goals, to compute lot of stats about our games.\nEveryone had good ideas about it, but someone had many more than us. Even more that it ruined all motivation of the other folks wanting to do work on this webapp, because the first steps to begin the app with all the features we had in mind was too big for all of us …\n\nSo before having started the project, it was over !\n\nFew months later, an undercover part of the team began the development of a more simple and stupid foosball application : it just allowed to select 4 players, and to register matches by telling who scored and what kind of goal it was (normal or own goal).\nThis was ugly as possible, but it worked ! And it was an awesome start for improving it and giving back all the motivation developpers had lost before !\n\nBabitch was born :)\n\nArchitecture\n\nAt the beggining, there was only one project, with the server API, and the client part.\nThis was bad. It was a good way to start fast, but a bad way to allow each project to evolve on its own side.\nSo we decided to divide the Babitch Project into two parts, Babitch, the server API, and BabitchClient, a client to consume Babitch Api data.\n\nBabitch, the API\n\nThe Babitch API is a simple PHP/MySQL project, based on Symfony2, Doctrine, FosRestBundle, and NelmioApiDocBundle.\nThe documentation generated by the NelmioApiDocBundle is available at /api/doc and allows to view each Route of the API, and to send requests on them with the Sandbox menu.\nThe API is functionnaly tested by Behat.\nA Vagrant file is available if you want to try it easily. (More information in the Readme.md)\n\nBabitch, the Client\n\nThe Babitch Client, is the “official” client for the API.\nThe client side doesn’t require any webserver, it’s just an Angular.js app, doing REST queries to the Babitch API thanks to Restangular.\n\nWe used Yeoman to bootstrap the project because it helps in many ways:\n\n\n  adds grunt configuration and support for serving, building and testing the project,\n  have generators for controllers, service, etc …\n\n\nFor development on this client, we are heavily using Grunt, Karma for Unit Testing, and the new Protactor for E2E testing.\n\nThe client is divided into four major parts:\n\nNew Game\n\n\n\nThis is the main feature, it allows to begin a new game, choose 4 players, and assign each goals to the right players.\nThe game is saved only when the last goal is made.\nEach player is represented by his Gravatar for a nicer UI :)\n\nLive\n\n\n\nThe table soccer is not at the same floor than we are, so we are using our monitoring screen to show at lunch time the live state of the table score !\n\nWith a screen on this feature, we could see:\n\n\n  if a game is played right now,\n  who’s playing,\n  the live score,\n  for each goal, in live, who scored, and on which side :)\n\n\nThe live part use a Faye server, which you can host freely on Heroku (more information on Readme.md). You configure a channel name, and all actions done on the new game view are forwarded to the Faye server, forwarded back to the client listening on the Live view. It just rocks !\n\nStats\n\n\n\nAll of this would be useless if you don’t have any way to compare your … stats to others competitors, right ?\nSo stats section is here for that purpose.\nIt shows you :\n\n\n  the last played match,\n  a sortable table by each stat of each player,\n  data visualization on each type of stats,\n  an individual card by player,\n  a sortable table by each stat of each team.\n\n\nAnd for each player and team, you have access to a lot of stats:\n\n\n  Elo Ranking (According to the Bonzini Usa Player Ranking/Rating System),\n  Percentage of goals per ball played,\n  Percentage of victory/loose per game,\n  Number of games played,\n  Team Goalaverage,\n  …\n\n\nThere was some long debate about how stats have to be computed : on the server side ? (not really the goal of a REST Api …), or on the client side ?\nAfter some successfull tries, we decided to compute stats on the client side, in an Angular.Js Service.\nThe service loads the last 300 games, and computes team and player stats fastly.\nWe also use the awesome D3.Js framework for data visualization.\n\nAdmin\n\nProbably the first screen you will need, for a simple way to add, modify, or delete players.\n\nConclusion\n\nWorking on our free (or lunch) time on a side-project like this is awesome!\nIt allows us to use several technologies or tools we don’t use often, to improve our knowledge on tons of other things, to view the project on the product owner side and to mix teams who don’t work a lot together.\n\nOne other thing interesting to remember about that project: Keep things as simple and small as possible (according to KISS principles) ! And only when your simple project is done, iterate by adding more and more features.\n\nTry and contribute?\n\nSo, if like us, you love foosball and play at work, give it a try, and give us feedback if you use it :)\n\nAlso, it’s open-source, so you’re welcome to contribute on BabitchClient and BabitchService, by posting/reading/commenting issue and PR.\n\nThanks ! :)\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un Lead Developpeur / Architecte web (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/2014/04/11/m6web-lyon-recherche-un-lead-developpeur-architecte-web-h-f-en-cdi.html",
    "date"     : "April 11, 2014",
    "excerpt"  : "\n\nM6Web Lyon recrute, en CDI, un Lead Développeur LAMP, avec une très forte expertise sur les technologies PHP 5.5, MySQL, Symfony2, GIT, et capable d’encadrer une petite équipe de développement.\n\nNous recherchons quelqu’un de très passionné, enth...",
  "content"  : "\n\nM6Web Lyon recrute, en CDI, un Lead Développeur LAMP, avec une très forte expertise sur les technologies PHP 5.5, MySQL, Symfony2, GIT, et capable d’encadrer une petite équipe de développement.\n\nNous recherchons quelqu’un de très passionné, enthousiaste, et mordu de veille technologique : un missionnaire de l’open source, un intégriste de la qualité de code, des tests unitaires et fonctionnels, et un architecte de projets aguerri avec une première approche en méthodologie de développement agile, et une expérience de management de développeurs.\n\nSi, en plus, vous êtes un malade de l’optimisation back-end et front-end, que des technologies comme Node.js vous émoustillent, que, malgré la qualité de MySQL, vous envisagez dans certains cas des solutions NoSQL alternatives (Mongo, Redis…), votre profil nous intéresse !\n\nVenez apporter vos compétences aux équipes techniques de M6Web en travaillant sur des sites à très forte charge (6Play, m6.fr, clubic.com, jeuxvideo.fr …), et partagez-les grâce à des conférences internes ou externes et des articles sur notre blog.\n\nSi vous avez les qualités requises et l’envie de nous rejoindre, allez sur le lien ci-dessous et faites nous part de votre CV, de votre compte github, et d’une lettre attrayante pour nous motiver à vous rencontrer.\n\nSi vous souhaitez postuler ou avoir plus d’infos : https://www.groupem6.fr/ressources-humaines/offres-emploi/lead-developpeur-architecte-web-h-f-229879.html\n"
} ,
  
  {
    "title"    : "Conférence au Symfony Live 2014 : Symfony à la télé",
    "category" : "",
    "tags"     : " symfony, conference",
    "url"      : "/2014/04/10/SfLive2014-symfony-a-la-tele.html",
    "date"     : "April 10, 2014",
    "excerpt"  : "Invité par SensioLabs au Symfony Live 2014, j’ai pu présenter le travail des équipes de M6Web, et de nos partenaires, autour de Symfony 2.\n\nVoici les slides de la conférence :\n\n \n\nL’enregistrement audio (avec les slides) est disponible ici :\n\n\n\nJe...",
  "content"  : "Invité par SensioLabs au Symfony Live 2014, j’ai pu présenter le travail des équipes de M6Web, et de nos partenaires, autour de Symfony 2.\n\nVoici les slides de la conférence :\n\n \n\nL’enregistrement audio (avec les slides) est disponible ici :\n\n\n\nJe tiens à remercier toutes les personnes avec qui j’ai pu échanger autour des thématiques de la conférence. J’ai eu beaucoup de plaisir à m’apercevoir que de nombreux conférenciers citaient le travail de M6Web pendant leur talk !\n\nRendez vous au phptour à Lyon pour les prochaines conférences techniques M6Web.\n"
} ,
  
  {
    "title"    : "Utilisation du StatsdBundle avec le composant Console",
    "category" : "",
    "tags"     : " statsd, php, symfony, console, monitoring, cytron",
    "url"      : "/2014/03/04/utilisation-du-statsdbundle-avec-la-console.html",
    "date"     : "March 4, 2014",
    "excerpt"  : "Le StatsdBundle\n\nChez M6Web, nous utilisons StatsD et nous avons créé un bundle pour cela.\nCe bundle permet d’ajouter facilement des incréments et des timings dans StatsD sur des événements Symfony2.\n\nDe la Request à la console\n\nOr pour des raison...",
  "content"  : "Le StatsdBundle\n\nChez M6Web, nous utilisons StatsD et nous avons créé un bundle pour cela.\nCe bundle permet d’ajouter facilement des incréments et des timings dans StatsD sur des événements Symfony2.\n\nDe la Request à la console\n\nOr pour des raisons de performances, lors des événements Symfony, les incréments et timings sont seulement stockés dans une variable et ne sont envoyés réellement à StatsD que pendant le kernel.terminate qui se déroule après l’envoi de la réponse HTTP au client.\nCeci pose un problème pour les événements lancés depuis une commande Symfony puisque en console, il n’y pas de Request et donc pas de kernel.terminate.\nNous avons envisagé d’utiliser l’événement console.terminate pour palier à cela, mais cela pose deux problèmes :\n\n\n  pour une commande qui est censée tourner indéfiniment (par exemple un consumer), on ne veut pas attendre la fin de la commande pour envoyer les données,\n  dans le cas d’une exception pendant la commande, l’événement console.terminate est lancé avant console.exception.\n\n\nLa première solution était donc d’appeler manuellement $container-&amp;gt;get(&#39;m6_statsd&#39;)-&amp;gt;send() dans la commande ou dans un ConsoleExceptionListener mais cela nous fait perdre le principal intérêt du StatsdBundle à savoir le découplage entre la commande et le client StatsD.\n\nLa seconde solution a donc été de modifier le StatsdBundle et d’ajouter une configuration au niveau de l’événement pour forcer l’envoi instantané des données.\n\nAinsi, avec la configuration suivante :\n\nclients:\n    event:\n        console.exception:\n            increment:      mysite.command.&amp;lt;command.name&amp;gt;.exception\n            immediate_send: true\n        m6kernel.exception:\n            increment: mysite.errors.&amp;lt;status_code&amp;gt;\n\nL’incrément mysite.command.&amp;lt;command.name&amp;gt;.exception sera envoyé en temps réel, alors que les autres comme mysite.errors.&amp;lt;status_code&amp;gt; continueront à être envoyés pendant kernel.terminate.\n"
} ,
  
  {
    "title"    : "Refonte de notre système de vote",
    "category" : "",
    "tags"     : " api, symfony, redis, monitoring, qualite, cytron",
    "url"      : "/2014/02/18/refonte-de-notre-systeme-de-vote.html",
    "date"     : "February 18, 2014",
    "excerpt"  : "Notre système de vote est utilisé d’une part pour gérer l’ensemble des questions et des réponses associées utilisées dans nos quizz et d’autre part pour récolter le nombre de votes des internautes lors des jeux concours.\n\nActuellement, le trafic g...",
  "content"  : "Notre système de vote est utilisé d’une part pour gérer l’ensemble des questions et des réponses associées utilisées dans nos quizz et d’autre part pour récolter le nombre de votes des internautes lors des jeux concours.\n\nActuellement, le trafic généré par cette fonctionnalité varie entre quelques votes par minute la nuit à quelques dizaines de votes par seconde lors des premières parties de soirée.\n\nHistorique\n\nComme souvent, les besoins ont régulièrement évolué depuis la mise en place initiale du système en 2009, faisant parfois prendre des chemins tortueux à l’implémentation technique. Au fil des demandes, notre système a par exemple dû stocker ses données dans nos forums pour une fonctionnalité qui a ensuite été rapidement abandonnée.\n\nL’année 2012 a vu l’arrivée du second écran : à l’aide de l’application gratuite adéquate, les périphériques mobiles peuvent désormais se synchroniser avec l’émission en cours de visionnage, en direct ou en différé, sur la TV ou sur le web (la synchronisation se fait par la bande son). Cette synchronisation nous permet de pusher instantanément sur les périphériques mobiles du contenu adapté à ce que le téléspectateur regarde : le détail de la recette que le cuisinier prépare dans Top Chef ou un sondage concernant la dernière trouvaille linguistique d’un ch’tit face à sa ch’tite.\n\nLe second écran s’annonçait alors comme une source importante de trafic supplémentaire pour notre système de vote. Effectivement, en plus du trafic historique généré par les sites web, nous allions aussi recevoir tous les votes provenant des périphériques mobiles.\n\nCe nouveau trafic a une saisonnalité très marquée : il est principalement présent en début de soirée et reste très dépendant du programme diffusé et de la contribution apportée.\n\nProblématique\n\nLa principale problématique venait de l’architecture des bases de données MySQL. Étant fortement couplées sur l’ensemble de la plateforme, la moindre défaillance de l’une d’elles, due à une surcharge sur un sondage, risquait de pénaliser les internautes de tous nos autres sites (un sondage du second écran pouvait donc impacter l’expérience utilisateur de Clubic).\n\nLe code était aussi fortement couplé entre nos différentes applications : l’action PHP d’un vote était exécutée sur la même plateforme que notre BO permettant à tous nos web services de fonctionner ainsi qu’aux contributeurs d’ajouter du contenu. Une surchage sur les votes aurait donc pu entrainer des perturbations sur le fonctionnement global du site m6.fr et de ses web services, donc de beaucoup de produits par extension.\n\nPour résumer, l’imbrication du code et des bases de données dans l’usine logiciel ne permettait pas de calibrer le système de vote pour qu’il puisse recevoir la charge attendue par le second écran.\n\nC’est donc début 2013 que Kenny Dits m’a contacté pour que nous trouvions une solution permettant de découpler le système de vote tout en faisant évoluer son architecture interne afin qu’il puisse facilement s’adapter à la charge inconstante du second écran.\n\nSolution\n\nNous avons alors conçu un nouveau service dédié uniquement à la gestion des questions, réponses et votes des utilisateurs. Ce nouveau service Polls est autonome, ce qui nous permet de le découpler complètement de notre usine logicielle avec laquelle il communique via une API REST.\n\nConcernant le stockage des données, nous avons simplement choisi un moteur très performant qui supporterait la charge sur une seule machine bien calibrée. Cela nous évitait alors les problématiques complexes de clustering. Mais nous devions tout de même stocker quelques informations relationnelles : il fallait donc avoir accès à quelques primitives nous permettant d’émuler les relations minimum entre nos données. Redis s’est donc imposé comme la solution adéquate. Cela reste malgré tout une solution théoriquement insatisfaisante, car non réellement scalable. Mais en pratique, les très bonnes performances de Redis permettent de répondre à (bien plus que) nos attentes.\n\nLe code se trouve, pour sa part, complètement isolé sur son propre serveur.\nComme ce service est complètement stateless et que notre base de donnée est centralisée et suffisamment performante, nous pouvons donc facilement ajouter ou supprimer des serveurs web selon la charge attendue : on peut dire qu’en pratique le service Polls est scalable horizontalement.\n\nLorsque l’architecture mise en place permet de répartir la charge sur un nombre variable de machines, le contrat est rempli : ce n’est plus qu’une question d’argent pour supporter n’importe quelle charge. Et comme tout le monde le sait : l’argent n’est pas un problème, c’est une solution.\n\nDéveloppement\n\nLe service Polls a été développé en PHP avec Symfony et le FOSRestBundle. Nous avons d’abord suivi certaines références, puis nous avons ensuite développé un micro ORM maison pour faire persister nos données dans Redis et enfin nous avons monitoré tous ce que l’on pouvait à l’aide de notre bundle dédié.\n\nUne attention toute particulière a été portée à la qualité avec des tests unitaires couvrant un maximum de code et des tests fonctionnels couvrant la plupart des cas d’utilisation des clients. Les nombreuses mises en production journalières pendant la phase d’optimisation ont ainsi été grandement facilitées, notamment grâce à la sérénité apportée par l’intégration continue.\n\nMise en production\n\nL’intégration de ce nouveau service Polls a cependant été bien plus longue que son développement. Nous l’avons d’abord mis en production en doublon de l’ancien système : toutes les écritures étaient faites sur les deux systèmes, mais l’ancien était encore la référence lors de la lecture des résultats par les clients.\n\nPuis après deux semaines, lorsque nous avons validé l’exacte corrélation entre les deux courbes du nombre de votes par minute à l’aide de Graphite, nous avons alors changé les clients pour qu’ils viennent lire les résultats sur le service Polls.\n\nEncore deux semaines plus tard, lorsque tout était validé et que nous avions développé et exécuté un script d’import de l’historique, nous avons débranché l’ancien système.\n\nL’intégration a donc été au moins trois fois plus longue, et donc couteuse, que le développement du service en lui-même.\n\nOptimisation\n\nLa première optimisation est simplement conceptuelle : nous avons concentré la criticité sur une seule route, celle qui est utilisée par chaque client pour voter. Il est ainsi plus simple de mesurer et donc d’améliorer les performances du service Polls. Cette route est critique parce qu’elle est utilisée par tous les clients, qu’elle ne peut pas être cachée et qu’il faut écrire des données en base lors de chaque appel.\n\nIl existait plusieurs pistes d’optimisation connues (système de queue, node.js, etc.) mais dans une optique KISS, nous avons d’abord opté pour l’utilisation des technologies en place pour ensuite interpréter les résultats récupérés lors des tests de charge et s’adapter si besoin.\n\nDans un premier temps, nous avons légèrement ajusté notre modèle de données pour limiter le nombre d’action à réaliser sur la base de données : nous avons seulement deux instructions Redis de complexité constante O(1) à réaliser pour chaque vote. Puis nous avons utilisé les transactions pour grouper ces deux instructions et éviter la latence d’une connexion supplémentaire vers notre serveur Redis.\n\nNous avons enfin supprimé la vérification de deux contraintes d’intégrité sans importance. Le code retour en cas d’erreur est juste un peu moins cohérent (400 au lieu de 422) mais cela n’impacte ni l’intégrité des votes ni la sécurité du service.\n\n\n\n\n\nAfin de savoir si nous n’avions pas complètement pris une mauvaise direction dans notre utilisation de Symfony, nous avons alors fait appel à Alexandre Salomé, consultant SensioLabs, pour auditer notre code.\n\nLors de cette journée, durant laquelle nous avons beaucoup appris, nous avons simplement désactivé tous les bundles que nous n’utilisions pas réellement en production : principalement Twig. Cela a occasionné une légère modification de notre code car le FOSRestBundle nécessite Twig pour afficher les erreurs même lorsque celles-ci sont en JSON.\n\nUne fois cette modification apportée, nous avons gagné les ultimes millisecondes nous permettant de passer sous la barre symbolique des 10ms de temps de réponse sur notre route critique.\n\n\n\nVous remarquerez que nous avons d’abord déployé le système en production avant de chercher à l’optimiser : nous pouvions ainsi mesurer en temps réel l’impact de nos développements sur une multitude d’indicateurs dont le temps de réponse.\n\nMise en pratique\n\nLe service Polls a facilement tenu la charge pour la première émission mettant en avant le second écran : un épisode de Hawaï 5-0 durant lequel les internautes pouvaient choisir le coupable avec un sondage (sur leur téléphone, tablette ou PC).\n\n\n\nPlus précisément, nous sommes montés à 150 requêtes par secondes (ce qui est évidemment bien moins que nos tests de charge), mais nous savons que nous pourrons maintenant nous adapter très simplement à une charge beaucoup plus forte en ajoutant des serveurs web. Notamment lors d’émissions faisant grandement appel au second écran.\n\nDans le pire des cas, si le service Polls devient indisponible, aucune autre partie de notre infrastructure ne sera compromise.\n\nLeçons\n\nAu cours du développement, de la mise en production et de la maintenance de ce service, j’ai appris plusieurs choses que j’essaierai de ne pas oublier trop vite :\n\n\n  Yes we can! Il est possible de combler petit à petit la dette technique, mais uniquement si c’est la volonté des décideurs,\n  la sérénité apportée par les tests automatisés est sans égale pour le confort de développement,\n  Redis est très performant.\n\n\nHa ? Attendez ! On me dit dans l’oreillette que certains doutaient encore qu’il était possible de faire du code performant avec un framework full stack comme Symfony.\n\nPas moi :-)\n"
} ,
  
  {
    "title"    : "How we use StatsD",
    "category" : "",
    "tags"     : " statsd, graphite, php, nodejs, monitoring",
    "url"      : "/2014/01/28/how-we-use-statsd.html",
    "date"     : "January 28, 2014",
    "excerpt"  : "What we want\n\nAs developers, we (M6Web) want to keep our eyes open on what is going on in production.\n\nOur local CMO (chief monitoring officier ;) ) did a nice presentation about this (in french).\n\nAs someone very wise (Theo Schlossnagle) said: “I...",
  "content"  : "What we want\n\nAs developers, we (M6Web) want to keep our eyes open on what is going on in production.\n\nOur local CMO (chief monitoring officier ;) ) did a nice presentation about this (in french).\n\nAs someone very wise (Theo Schlossnagle) said: “It’s not in production unless it’s monitored”. Another cool mantra is: “I am wondering what to monitor ? everything dude !”. Finally “if you can not measure it, you can not improve it.” (Lord Kelvin).\n\nWe ship new apps very often, so we have to industrialise this practice.\n\nWhat is it?\n\nStatsD is a Node.Js daemon allowing you to send metrics (increment values and timers) over UDP. The fire and forget feature of UDP is great for reducing risks of introducing latency or crashes in your application.\n\nStatsD is open sourced by etsy. In our configuration, we use several StatsD deamons and aggregate metrics on Graphite - one point per minute. Many servers allows us to scale, because we don’t sample the data at all.\n\nOn client side, we use a simple consistent hashing algorithm to dispatch metrics overs StatsD nodes on the same server.\n\nCollecting metrics\n\nFrom raw PHP\n\nWe’ve created a simple PHP lib to dispatch metrics over UDP. Check it out on Github or Packagist.\n\nThe usage is pretty straightforward :\n\n&amp;lt;?php\n// client creation\n$client = new Statsd\\Client(\n                    array(\n                        &#39;serv1&#39; =&amp;gt; array(&#39;address&#39; =&amp;gt; &#39;udp://200.22.143.12&#39;),\n                        &#39;serv2&#39; =&amp;gt; array(&#39;port&#39; =&amp;gt; 8125, &#39;address&#39; =&amp;gt; &#39;udp://200.22.143.12&#39;)\n                    )\n                );\n// usage\n$client-&amp;gt;increment(&#39;a.graphite.node&#39;);\n$client-&amp;gt;timing(&#39;another.graphite.node&#39;, (float) $timing);\n\nFrom Symfony2\n\nAs basic Symfony2 fanboys, we’ve built a bundle on top of the StatsD component.\nIt provides these features:\n\n\n  manage multiple Symfony services with different configurations\n  bind any event to increment nodes and collect timers\n\n\nDuring Symfony 2 execution, metrics are collected and sent only at the kernel shutdown. A nice feature is that you can easily collect basic metrics based on events without touching your code.\n\nFor example, in conjunction with the M6Web\\HttpKernelBundle, just dropping this in config.yml is enough:\n\nm6_statsd:\n    clients:\n        default:\n            servers: [&#39;all&#39;]\n            events:\n              m6.terminate:\n                increment:     request.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;\n                timing:        request.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;\n                custom_timing: { node: memory.yourapp.&amp;lt;status_code&amp;gt;.&amp;lt;route_name&amp;gt;, method: getMemory }\n              m6kernel.exception:\n                increment: errors.&amp;lt;status_code&amp;gt;.yourapp\n\n\n\nOffering this to the tech team means that I am now pretty sure that almost all new PHP apps pop with those metrics out of the box.\n\nPlease checkout the bundle documentation on github.\n\nFrom anywhere else\n\nFrom Flex, mobile app or JS applications we’ve developed a simple Node.js app, translating an HTTP call to a StatsD UDP one. Like the PHP implementation, this application shards the metrics over multiple servers.\n\nPlease consider sending metrics asynchronously and add a timeout to this HTTP call.\n\nLiving with metrics\n\nAbout 120K metrics are collected on our platform. That’s a lot.\n\nGraphite dashboards are quite rustic. But surprisingly lots of non-techs people use this tool: SEO experts, advertising managers, contributors, …\n\n\n\n\n\nFor now we keep using Graphite. We try to keep our dashboards organised and well named.\n\nFor alerting purpose, a tool based on Graphite JSON output has been developed. It sends emails when it reaches some user defined conditions. Honestly, it does the job, but frankly we are still looking for something else, more flexible with more notification systems than emails.\n\nIf you use such a tool, and you’re happy with it, please let us know in the comments.\n\nFound a typo or bad english langage, just propose a pull request.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #6",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/2014/01/20/m6web-dev-facts-6.html",
    "date"     : "January 20, 2014",
    "excerpt"  : "Parce que nous en avons encore une quantité incroyable en stock, voici une nouvelle sélection des meilleures phrases entendues dans nos bureaux !\n\nTo code or not to code\n\n\n  \n    Je veux faire du code qui marche, et qui sert à quelque chose !\n    ...",
  "content"  : "Parce que nous en avons encore une quantité incroyable en stock, voici une nouvelle sélection des meilleures phrases entendues dans nos bureaux !\n\nTo code or not to code\n\n\n  \n    Je veux faire du code qui marche, et qui sert à quelque chose !\n    C’est soit l’un, soit l’autre, mec …\n  \n\n\nImage trompeuse\n\nUn dév teste une appli sur ipad\n\n\n  \n    Ah mais l’image est toute moche\n    C’est ton reflet que tu vois\n  \n\n\nUne histoire de chocolat\n\n\n  \n    Qui a testé la Kindle HDX ?\n    Moi j’ai pas trop aimé\n    Pourquoi ?\n    Je préfére le Kindle Surprise\n  \n\n\nc’est automatique je te dis\n\n\n  Mais je te dis que c’est automatique … à 90%\n\n\nUne évidence évidente\n\n\n  C’est pas toujours simple parce que c’est compliqué\n\n\nTout tout tout\n\n\n  Ca couvre la quasi totalité de tout\n\n\nVers l’infini et au delà\n\n\n  Il faut que le forum soit caché intégralement autant que possible\n\n\nFichier introuvable\n\n\n  Je t’envoie par mail l’url du fichier\n\n\nPar mail :\n\n\n  file://C:/Users/**/Desktop/Sanstitre-1.html\n\n\nUnknown Notice\n\n\n  Les notices c’est tabous, on en viendra tous à bout\n\n\nLa minute de 30 secondes - le retour\n\n\n  Tu as pris six mois d’année sabatique\n\n"
} ,
  
  {
    "title"    : "Vagrant &amp; Cie, du Dév à la Prod avec Julien Bianchi",
    "category" : "",
    "tags"     : " lft, vagrant, video",
    "url"      : "/2014/01/18/vagrant-julien-bianchi.html",
    "date"     : "January 18, 2014",
    "excerpt"  : "Un grand merci à Julien Bianchi qui, à notre demande, est venu nous parler un peu de vagrant lors d’un de nos fameux Last Friday Talk.\n\nSes slides : https://speakerdeck.com/jubianchi/vagrant-and-cie-du-dev-a-la-prod.\n\nMalheureusement, la vidéo n’e...",
  "content"  : "Un grand merci à Julien Bianchi qui, à notre demande, est venu nous parler un peu de vagrant lors d’un de nos fameux Last Friday Talk.\n\nSes slides : https://speakerdeck.com/jubianchi/vagrant-and-cie-du-dev-a-la-prod.\n\nMalheureusement, la vidéo n’est plus disponible…\n"
} ,
  
  {
    "title"    : "API à consommer avec modération",
    "category" : "",
    "tags"     : " outil, api, symfony, doctrine, cytron, open-source",
    "url"      : "/2014/01/08/api-a-consommer-avec-moderation.html",
    "date"     : "January 8, 2014",
    "excerpt"  : "Après avoir travaillé pendant plusieurs mois sur la création et les tests de nos API avec Symfony, le moment de leur publication est enfin arrivé !\n\nOr, les clients de nos API sont multiples : il peut s’agir d’applications mobiles, de sites web ma...",
  "content"  : "Après avoir travaillé pendant plusieurs mois sur la création et les tests de nos API avec Symfony, le moment de leur publication est enfin arrivé !\n\nOr, les clients de nos API sont multiples : il peut s’agir d’applications mobiles, de sites web mais aussi d’un back office interne. Chacun de ces clients peut nécessiter des “vues” différentes de l’API.\n\nEffectivement, alors que le BO devra pouvoir accéder à la totalité des ressources disponibles, l’application mobile ne devra avoir accès qu’aux ressources publiées. De la même manière, la gestion du cache ainsi que la disponibilité des routes doit pouvoir s’adapter facilement aux clients qui consomment l’API.\n\nNous avons opté pour l’utilisation d’un sous-domaine par client afin de l’identifier et ainsi de lui appliquer des configurations particulières. Ex :\n\n\n  https://bo.api.monservice.fr pour le BO,\n  https://mobile.api.monservice.fr pour l’application mobile.\n\n\n#### Authentification\n\nNous utilisons le composant sécurité de Symfony, qui permet de créer un utilisateur authentifié à la volée et de charger la configuration spécifique à celui-ci.\n\nNous avons tout d’abord besoin de créer une classe User implémentant Symfony\\Component\\Security\\Core\\User\\UserInterface, et contenant les informations de configuration spécifique.\n\nLes différents Users sont ensuite créés à l’aide d’un fournisseur d’utilisateurs implémentant Symfony\\Component\\Security\\Core\\User\\UserProviderInterface.\nDans notre cas, chaque utilisateur possède son propre fichier de configuration yml. Le fournisseur d’utilisateur vérifie donc que l’utilisateur demandé possède un fichier de configuration et instancie un objet User avec cette configuration. Ce UserProvider est défini comme service dans notre bundle et configuré dans security.yml.\n\nIl faut ensuite créer notre propre fournisseur d’authentification pour avoir une authentification par nom de domaine. Pour cela nous avons suivi et adapté le cookbook de Symfony. Cette authentification s’articule autour de 2 classes : un FirewallListener et un AuthenticationProvider. Pour que notre FirewallListener puisse facilement récupérer le client associé, nous avons ajouté un paramètre au routing Symfony :\n\nhost: {client}.api.monservice.fr\n\nLe FirewallListener utilise donc ce paramètre du routing comme nom d’utilisateur et le transmet à notre AuthenticationProvider. Celui-ci récupère le User grâce au UserProvider et profite de cette phase pour vérifier que l’adresse IP du client est bien autorisée dans sa configuration grâce au FirewallBundle.\n\nEffectivement, nous avons ajouté un filtrage initial (mais optionnel) sur les IPs pour chaque client, dans le fichiers app/config/users/{username}.yml :\n\nfirewall:\n    user_access:\n        default_state: false\n        lists:\n            m6_prod: true\n            m6_preprod: true\n            m6_dev: true\n            m6_lan: true\n            m6_local: true\n            m6_public: true\n\nPour plus de précisions, voir la documentation du FirewallBundle.\n\nAutorisation\n\nPour gérer les autorisations d’accès des utilisateurs aux différentes routes, nous avons créé un EventListener qui écoute kernel.request et qui décide de laisser passer la requête ou non en fonction de la configuration de l’utilisateur.\n\nallow:\n    default: true\n    methods:\n        delete: false\n    resources:\n        exam: false\n    routes:\n        get_articles: false\n\nDans cet exemple, l’utilisateur a accès par défaut à toutes les routes sauf les méthodes DELETE, les routes concernant les exams et la route spécifique get_articles.\n\n#### Durée de cache\n\nLes temps de cache sont différents en fonction de l’utilisation des données. Les données du backoffice ne seront pas cachées, tandis que les données de l’application mobile auront un temps de cache de 300s.\nNous avons là-aussi créé un EventListener qui écoute cette fois kernel.response et qui modifie les headers de cache de la réponse en fonction de la configuration utilisateur qui peut contenir une durée par défaut de cache et des durées de cache par route.\n\nFiltrage automatique avec Doctrine\n\nNous pouvons offrir une “vue” différente de nos données à chaque client en définissant des critères de filtrage pour Doctrine (ex: date de publication, ressource activée, etc.) dans les fichiers de configuration des clients :\n\nentities:\n    article:\n        active: true\n        publication: false\n\nAfin de ne pas modifier le comportement par défaut de Doctrine, nous avons ajouté une méthode findWithContext à nos repositories qui reprend les mêmes paramètres que la méthode findBy en injectant le SecurityContext. Cette méthode permet donc de récupérer des entités filtrées en fonction des paramètres d’un client :\n\n&amp;lt;?php\n$article = $this\n    -&amp;gt;get(&#39;m6_contents.article.manager&#39;)\n    -&amp;gt;getRepository()\n    -&amp;gt;findWithContext($this-&amp;gt;container-&amp;gt;get(&#39;security.context&#39;), [&#39;id&#39; =&amp;gt; $id]);\n\n#### Personnalisation avancée\n\nGrâce à l’utilisation du Bundle Security de Symfony, toute la configuration spécifique à un sous-domaine est stockée dans l’utilisateur courant. Et dans Symfony, l’utilisateur courant est facilement récupérable à partir du service security_context. Il est ainsi possible de personnaliser n’importe quelle brique de l’application en y injectant la dépendance sur ce service.\n\nDomainUserBundle\n\nAfin d’implémenter facilement ce fonctionnement sur nos API, nous avons développé un bundle dédié. Il peut donc aussi vous permettre de gérer l’authentification et la configuration de vos API par nom de domaine.\n\nDomainUserBundle est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Qui a bouchonné mon Redis ?",
    "category" : "",
    "tags"     : " qualite, outil, redis, cytron, open-source",
    "url"      : "/redismock-qui-a-bouchonne-mon-redis",
    "date"     : "December 11, 2013",
    "excerpt"  : "Les tests fonctionnels tiennent un rôle majeur dans la réussite et la pérennité d’un projet web, d’autant plus s’il est déployé continuellement. Nous nous étions donc déjà intéressés à cette problématique dans le cas d’un service proposant une API...",
  "content"  : "Les tests fonctionnels tiennent un rôle majeur dans la réussite et la pérennité d’un projet web, d’autant plus s’il est déployé continuellement. Nous nous étions donc déjà intéressés à cette problématique dans le cas d’un service proposant une API REST et utilisant MySQL et Doctrine. Mais nous développons aussi des services du même type utilisant d’autres systèmes de stockage de données comme Redis.\n\nAfin de tester fonctionnellement ces services, nous avons d’abord eu l’idée d’installer une instance Redis sur nos serveurs de tests. Mais nous allions inéluctablement retomber sur les mêmes obstacles qu’avec MySQL :\n\n\n  il n’est pas toujours possible de monter une instance Redis dédiée aux tests,\n  mais surtout une telle architecture n’est pas viable dans un système de tests concurrentiels.\n\n\nLa librairie RedisMock\n\nNous nous sommes alors penchés sur la possibilité de bouchonner Redis, chose qui parait au premier abord plus aisée que de bouchonner Doctrine : Redis propose une API simple et bien documenté (même si abondante). Nous pensions trouver une librairie PHP déjà existante mais nos recherches sont restées vaines.\n\nNous avons donc créé la librairie RedisMock qui reprend simplement les commandes de l’API de Redis et simule leur comportement grâce aux fonctions natives de PHP. Évidemment, toutes les commandes Redis n’ont pas encore été implémentées, seules celles qui sont utilisées dans nos tests sont présentes. Vous pouvez nous proposer l’implémentation de nouvelles fonctions Redis, selon vos besoins, via des Pull Requests sur le projet.\n\nToutes les commandes exposées par le mock sont testées unitairement via atoum en reprenant pour chaque cas les spécifications énoncées dans la documentation Redis.\n\nUtiliser RedisMock dans vos tests sur Symfony\n\nTout d’abord, il faut rajouter la dépendance à la librairie dans le composer.json et mettre à jour les vendors :\n\n\n\nL’utilisation du mock reste très simple dans un projet Symfony. Chez M6Web, nous utilisons notre propre composant Redis, lui même basé sur Predis. Afin que le mock puisse complètement se faire passer pour la librairie Redis lors de l’execution des tests, nous avons implémenté une factory qui crée à la volée un adapteur héritant de la classe à bouchonner. La méthode getAdpaterClass permet de récupérer le nom de la classe à instancier.\n\n\n\nPour simplifier la création de l’adapteur et son injection dans l’application via le fichier config_test.yml, on peut utiliser la méthode getAdapter qui instancie directement l’objet sans paramètre. Il nous suffit alors de modifier la définition du service Redis dans l’environnement de test.\n\n\n\nEt voilà, le tour est joué ! Les tests utilisent maintenant le mock à la place du véritable Redis. Attention cependant, si votre librairie utilise des fonctionnalités non implémentées dans RedisMock, vous pourriez faire face à des comportements aléatoires indésirables.\n\nRedisMock  est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Composer installation without github.com (nor packagist) dependency - like a boss !",
    "category" : "",
    "tags"     : " satis, composer, aws, s3, github, packagist, cloud",
    "url"      : "/composer-installation-without-github",
    "date"     : "December 2, 2013",
    "excerpt"  : "\n\nFirst a thought about github, composer, packagist : we like / adore / thanks the contributors, for those great services and all the open source people dropping great software on it.\n\nThat said, picture yourself operating an online PHP service, g...",
  "content"  : "\n\nFirst a thought about github, composer, packagist : we like / adore / thanks the contributors, for those great services and all the open source people dropping great software on it.\n\nThat said, picture yourself operating an online PHP service, generating hundreds euros per hour (cool isn’t it ?).\n\nIf you use Symfony2 and other public packages, like us, you’re probably deploying your application using composer.\n\n\n\nSuddenly the service is dealing with more and more and more traffic (maybe someone talk about on national tv … or Justin Bieber tweet something … maybe :) ). No problem, says the system administrator (yes our sysadmins are cool), lets pop more virtual machines and deploy more instance of the service !\n\nAnd then :\n\n\n\nboum ! =&amp;gt;composer install command can’t download distant packages on api.github.com (website is down, or the network connection or whatever).\n\nGood luck explaining to your boss that you rely on free hosting service to deploy your business critical website !\n\nThis is our situation. So here how we deal with that.\n\nPrinciples.\n\n\n\nWe chose to use Satis - a great tool provided by the Composer team. The main idea is, regulary download packages and their informations on our local servers. We (at M6Web) deployed services on our local infrastructure and on S3 servers in Amazon Web Services.\n\nHow to ? For your local network.\n\nWe set 2 different satis instance. One for our private packages, and another for all the dependencies we use (basically around Symfony2). The first one (satis-private) will build every 5 minutes, the second (satis-public) every half hour.\n\nfor example :\n\n\n  satis-private.yourcompany.com\n  satis-public.youcompany.com\n\n\nSatis for private package configuration (data/satis.json) :\n\n{\n    &quot;name&quot;: “satis-private&quot;,\n    &quot;homepage&quot;: &quot;https://satis-private.yourcompany.com&quot;,\n    &quot;archive&quot;: {\n        &quot;directory&quot;: &quot;dist&quot;,\n        &quot;absolute-directory&quot; : &quot;/srv/data/satis-private/dist&quot;,\n        &quot;format&quot;: &quot;zip&quot;,\n        &quot;skip-dev&quot;: true\n    },\n    &quot;repositories&quot;: [\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/great-bundle&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/great-component&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/awesomelib&quot; },\n        { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git://git.youcompany.com/raoul&quot; },\n…\n    ],\n    &quot;require-all&quot;: true\n}\n\n\nSatis for public package configuration (data/satis.json) :\n\n{\n    &quot;name&quot;: “satis-public&quot;,\n    &quot;homepage&quot;: &quot;https://satis-public.yourcompany.com&quot;,\n    &quot;archive&quot;: {\n        &quot;directory&quot;: &quot;dist&quot;,\n        &quot;format&quot;: &quot;zip&quot;,\n        &quot;skip-dev&quot;: false,\n        &quot;absolute-directory&quot; : &quot;/srv/data/satis-public/dist&quot;\n    },\n    &quot;repositories&quot;: [\n        { &quot;type&quot;: &quot;composer&quot;, &quot;url&quot;: &quot;https://packagist.org&quot; }\n    ],\n    &quot;require&quot;: {\n\n        &quot;m6web/firewall-bundle&quot; : &quot;*&quot;,\n        &quot;m6web/statsd-bundle&quot;   : &quot;*&quot;,\n\n        &quot;doctrine/orm&quot;               : &quot;~2.3&quot;,\n        &quot;doctrine/common&quot;            : &quot;~2.4&quot;,\n        &quot;doctrine/dbal&quot;              : &quot;~2.3&quot;,\n        &quot;doctrine/doctrine-bundle&quot;   : &quot;~1.2&quot;,\n\n        &quot;naderman/composer-aws&quot;      : &quot;~0.2.3&quot;,\n…\n    },\n    &quot;require-dependencies&quot;: true\n}\n\n\nOn the crontab, add this command for each satis instance :\n\nphp -d memory_limit=xx bin/satis build data/satis.json web\n\n\n(increasing memory_limit was mandatory for us with satis-public).\n\nPlease note the require-dependencies directive. It tell satis to digg on sub-dependencies on the required packages. And yes, it can take a while. You will probably hit the Github API rate limit. To increase it, add a Github key on your composer configuration file on the satis servers.\n\n$ cat .composer/config.json\n{\n    &quot;config&quot;: {\n        &quot;github-oauth&quot;: {\n            &quot;github.com&quot;: “xxxxxx&quot;\n        }\n    }\n\n}\n\n\nIn your projects, edit the composer.json and replace the repositories entry by\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://satis-private.yourcompany.com&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://satis-public.yourcompany.com&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nRemove your composer.lock and vendors then run composer update on the project.\n&quot;packagist&quot;: false&quot; mean : “do not search missing packages on packagist.com”. If a package is missing during install, you have to add it in satis-public configuration file then try again.\n\nthat’s it :)\n\nHow to ? For AWS.\n\nSync our 2 satis servers with an S3 bucket.\n\n\n\nOn satis servers, use s3cmd to keep in sync the S3 bucket. Let’s say : yourcloud-satis.\n\nAdd some commands after the build script of satis :\n\nphp -d memory_limit=xx bin/satis build data/satis.json web\ncd web\nsed &#39;s#https://satis-private\\.yourcompany\\.com#s3://yourcloud-satis/satis-private#&#39; packages.json &amp;gt; packages-cloud.json\ns3cmd put index.html s3://yourcloud-satis/satis-private/index.html\ns3cmd put packages-cloud.json s3://yourcloud-satis/satis-private/packages.json\ncd /srv/data/satis-private/\ns3cmd sync ./dist s3://6cloud-satis/satis-private/\n\n\n(do the same for satis-public).\n\nupdate your projects\n\nIn your projects, edit the composer.json and replace the repositories entry by\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-private/&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-public/&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nEnable the AWS plugin in EC2 servers\n\nAdd our repositories in ~./composer/composer.json file of the user used to deploy your code.\n\n&quot;repositories&quot;: [\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-private/&quot;\n    },\n    {\n        &quot;type&quot;: &quot;composer&quot;,\n        &quot;url&quot;: &quot;https://s3-eu-west-1.amazonaws.com/yourcloud-satis/satis-public/&quot;\n    },\n    {\n        &quot;packagist&quot;: false\n    }\n],\n\n\nYou have to install the S3 plugin for composer on your EC2 instance.\n\n$ composer global require &quot;naderman/composer-aws:~0.2.5&quot;\n\n\nIf you don’t use IAM roles, add the following composer config on your EC2 servers (~/.composer/config.json) :\n\n{\n    &quot;config&quot;: {\n        &quot;amazon-aws&quot;: {\n            &quot;key&quot;:    &quot;KEYYYYYY&quot;,\n            &quot;secret&quot;: &quot;seeeeecret&quot;\n        }\n    }\n}\n\n\ncomposer install --prefer-dist command will now download all the packages files from S3 !\n\nThanks to Pierre and Jeremy for their help.\n\nFound a typo or bad english langage, just propose a pull request.\n"
} ,
  
  {
    "title"    : "JenkinsLight, mettez en lumière vos jobs Jenkins",
    "category" : "",
    "tags"     : " outil, jenkins, ci, cytron, open-source",
    "url"      : "/jenkinslight-mettez-en-lumiere-vos-jobs-jenkins",
    "date"     : "November 20, 2013",
    "excerpt"  : "L’idée de JenkinsLight a germé lorsque nous nous sommes fait taper sur les doigts pour la troisième fois (à juste titre) parce que l’on avait désactivé la publicité sur nos sites de chaîne lors d’une mise en production. Or la publicité est un poin...",
  "content"  : "L’idée de JenkinsLight a germé lorsque nous nous sommes fait taper sur les doigts pour la troisième fois (à juste titre) parce que l’on avait désactivé la publicité sur nos sites de chaîne lors d’une mise en production. Or la publicité est un point critique car directement reliée au chiffre d’affaires. Le pire est que nous testions déjà le bon fonctionnement de la publicité en intégration continue sur nos serveurs de preprod, avant la mise en production. Mais une configuration légèrement différente sur les serveurs de prod rendait le nouveau code instable. Cette situation rend donc impossible la détection de certaines anomalies avant la mise en production…\n\nD’où notre besoin d’avoir un tableau de bord nous permettant de vérifier chaque instant la disponibilité des fonctionnalités névralgiques de nos sites en production afin de réagir au plus vite en cas de problèmes. Et ce, avant même que l’anomalie ne nous soit remontée par les autres secteurs. Nous avions déjà nos tests dans Jenkins que nous avons alors fait pointer vers la prod. Il nous manquait donc juste une sorte de “Panic Board” sur un écran placé au sein de nos bureaux nous remontant rapidement le moindre problème sur nos sites en production.\n\nNous avons créé JenkinsLight qui permet d’afficher distinctement le statut des jobs d’une vue Jenkins en quasi temps réel. Le projet utilise AngularJS et l’API de Jenkins pour récupérer les informations nécessaires. L’installation se fait sur n’importe quel serveur web et requiert uniquement Bower pour installer les composants. Afin de permettre à l’API d’être appelée en crossdomain (CORS), il est également nécessaire d’installer un plugin spécifique sur votre serveur Jenkins.\n\nL’application propose quelques variables de configuration éditables dans le fichier “app/scripts/config.js” permettant de spécifier :\n\n\n  l’url du serveur Jenkins,\n  l’identification au serveur (si nécessaire),\n  la vue Jenkins par défault,\n  les types de jobs affichés,\n  une regexp pour exclure certains jobs,\n  le nombre maximum de jobs par ligne sur l’écran,\n  l’intervalle de rafraîchissement (en millisecondes),\n  une image de fond quand il n’y a aucun job à afficher.\n\n\nJenkinsLight est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 3",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-3",
    "date"     : "November 19, 2013",
    "excerpt"  : "\n\nDernière journée de cette Velocity Europe, avec en plus du track Performance et Ops, l’ouverture d’un track Culture.\n\nPour rappel, si vous les avez ratés, les CR des journées précédentes sont à retrouver ici :\n\n\n  CR Velocity Europe 2013 - Day 1...",
  "content"  : "\n\nDernière journée de cette Velocity Europe, avec en plus du track Performance et Ops, l’ouverture d’un track Culture.\n\nPour rappel, si vous les avez ratés, les CR des journées précédentes sont à retrouver ici :\n\n\n  CR Velocity Europe 2013 - Day 1\n  CR Velocity Europe 2013 - Day 2\n\n\nPour ce “Day 3”, nous nous retrouvons tous dans la grande salle avec la vidéo “Slow Motion Water Balloon Fight” en guise d’introduction :\n\n\n\nExtreme Image Optimisation: WebP &amp;amp; JPEG XR\n\nIdo Safruti (Akamai) @safruti\n\n\n\nA ce jour, d’après HTTP Archive, 62% du poids des pages Web correspond aux images sur desktop, 65% sur mobile.\n\nNous utilisons toujours des technos vieilles de plus de 15 ans ! Jpg, Png, Gif …\n\n\n  “Deploying new image formats on the web is HARD (but doable)” Ilya Grigorik\n\n\nLa conférence traite de deux formats bien plus récents :\n\n\n  WebP (2011)\n  JXR : Jpeg eXtended Range (2009)\n\n\nqui supportent le lossless et lossy, ainsi que la transparence (en lossless et lossy aussi)\n\nOn retrouve un tableau très intéressant sur une comparaison taille entre les différents formats sur un même niveau de qualité :\n\n\n\nAttention, certaines fois (quelques %), l’image peut être plus grosse qu’en jpg. Si vous partez du Jpg pour la compression, comparez les tailles et affichez le Jpg s’il est plus petit.\n\nLe support de ces formats reste toutefois minime :\n\n\n  WebP : Chrome &amp;gt;= 23, Opera &amp;gt;= 12, …\n  Jpeg XR : IE =&amp;gt; 10, …\n\n\nJXR, gère notamment le progressive, ce que ne gère pas encore date WebP. Plus d’infos sur les “progressives Jpeg” sur le blog de Patrick Meenan (Créateur de WebPageTest).\n\nUne petite anecdote intéressante sur WebP aussi. Facebook avait mis en place WebP mais est revenu en arrière car les utilisateurs râlaient ! Quand ils enregistraient ou partageait une photo de Chrome (donc en WebP), les utilisateurs IE notamment, ne pouvaient pas la consulter …\n\nIdo fera un article sur l’incontournable calendrier de l’avant sur le sujet : Performance Calendar en décembre 2013 !\n\n\n\nSLOWING DOWN TO GO FASTER: Responsive Web Design And The Problem Of Agility vs Robustness\n\nTom Maslen (BBC News) @tmaslen\n\n\n\nGros retour d’expérience des équipes de BBC News sur leur approche du “Responsive Web Design”, et sur la manière dont cela a impacté leurs workflows, ainsi que leur culture.\n\nLe RWD prend du temps, beaucoup plus de temps (3x), sur le Design, le développement, le test.\n\nTom parcourt les optimisations “classiques”, ainsi que la manière dont ils enrichissent l’expérience : Ils délivrent une “Core Experience” à tous, et une “Enhanced Experience” aux navigateurs qui le supportent, utilisent Grunt pour certaines automatisations (pour fournir les bonnes images à la bonne taille https://github.com/BBC-News/Imager.js/, versionner les assets https://github.com/kswedberg/grunt-version …).\n\nVous pouvez aussi découvrir Wraith, leur outil de comparaison de screeshot Responsive.\n\nBref, une excellente conférence avec un très bon speaker (très drôle sur la fin).\n\n\n  “Don’t do whoopsies on other people things” Tom Maslen\n\n\n\n\nAn Introduction to Code Club\n\nJohn Wards (White October)\n\n\n\nCode Club est un projet leadé par des bénévoles pour créer des clubs de coding dans les écoles, pour des enfants entre 9 et 11 ans. Plus de 1400 clubs ont déjété créés en Angleterre !\n\nLes enfants utilisent le projet Scratch, et qui permet via un langage de programmation assez simple, de programmer des jeux.\n\nLeurs vidéos de présentation sont de plus assez fun, notamment celle ci, qui nous à été montrée, à 6mn50 dans la vidéo ci dessous :\n\n\n\nLightning Demo: Automating WebPagetest with wpt-script\n\nJonathan Klein (Etsy) @jonathanklein\n\n\n\nAfin d’automatiser la prise de mesure synthétique à l’aide de WebPageTest (notamment si vous avez installé une instance privée), les gars d’Etsy ont développé un wrapper Php à l’Api de Webpagetest. Le wrapper permet aussi de pousser les résultats dans un Graphite ou un Splunk.\n\nL’outil est dispo sur Github : Wpt-Script\n\n\n\nLightning Demo: Introducing a New RUM Resource From SOASTA\n\nBuddy Brewer (SOASTA)\n\n\n\nSOASTA, société connue notamment pour avoir racheté LogNormal (outil de R.U.M. l’année dernière), propose aujourd’hui un outil de R.U.M. nommé : mPulse.\n\nIls ont publié de nombreuses statistiques sur leur site, sur les performances, suivant le navigateur, la localité etc : https://www.soasta.com/summary/\n\n\n\nLightning Demo: Automating The Removal Of Unused CSS\n\nAddy Osmani (Google Chrome) @addyosmani\n\n\n\nL’un des petits problèmes récurrents du développement web est situé dans nos fichiers Css. A force d’ajout de fonctionnalités ou de framework (notamment les fameux frameworks Css, Bootstrap &amp;amp; co), on finit par obtenir des fichiers CSS gigantesques, dans lesquels il devient très compliqué de savoir ce qui est utilisé ou pas sur votre site.\n\nAddy présente des solutions qu’on peut retrouver :\n\n\n  pour un nettoyage mono page, dans la DevTools de Chrome (Onglet Audit puis Run puis “Remove Unused Css Rules”)\n  pour un nettoyage d’un site complet, via des outils autour de Grunt, notamment Grunt Uncss https://github.com/addyosmani/grunt-uncss fait par Addy en personne, basé sur le module Uncss de Giakki\n\n\n\n\nLearning from the Worst of WebPagetest\n\nRick Viscomi (Google)\n\n\n\nRick travaille pour YouTube chez Google, comme WebDéveloppeur Front-end orienté performance.\n\nSa passion, se moquer des mauvais résultats sur les historiques du WPT public :-)\n\nC’est d’ailleurs pour lui, l’une des bonnes sources pour découvrir les “anti-patterns” de la perf, et les choses à ne pas faire.\n\nIl présente une Pull Request en cours sur WebPageTest avec le Multi Variate Testing, permettant de tester tout un site, sur plusieurs localités. Plus d’infos sur l’article de son blog sur le sujet : https://jrvis.com/blog/wpt-mvt/\n\nAre today’s good practices … tomorrows performance anti-patterns\n\nAndy Davies @andydavies​\n\n\n\nAvec l’arrivée d’HTTP 2.0, on se demande, si les optimisations WebPerf que nous réalisons aujourd’hui ne seront pas gênantes demain : Les dataURI, le JS inline, le domain sharding, les sprites …\n\nLes réponses ne sont pas aussi simples, et nous “développeurs” nous devons de nous poser les questions afin d’avoir les bonnes réponses avant l’arrivée d’HTTP 2.0. Andy à le mérite de lancer le débat, via des protocoles de test pour chacun des cas. en comparant HTTP 1.0 et SPDY.\n\n\n\nProvisioning the Future - Building and Managing High Performance Compute Clusters in the Cloud\n\nMarc Cohen, Mandy Waite (Google)\n\nMarc et Mandy nous ont présenté le Google Cloud, alternative AWS. Basé sur du KVM hautement modifié par les équipes de Google, on retrouve grossièrement des services identiques (stockage élastique persistent, SDN pour le réseau, load-balancing, des profils de machines highmem ou highcpu). Toutefois on notera l’absence d’un marketplace pour les images des VMs et seules Debian et Centos sont disponibles. Énorme avantage par rapport AWS: la facturation la minute au bout de 15min ! Mandy nous a fait la démo du lancement de 1000 Vms en 2min15. Google fournit une api complète et un outil en ligne de commande pour piloter absolument tout: gcutil.\n\nSecurity Monitoring (With Open Source Penetration Testing Tools)\n\nGareth Rushgrove (Government Digital Service)\n\nCombien d’entre nous testent la sécurité de leur applicatif en continu ? Elle devrait pourtant faire partie de l’assurance qualité du développement d’un logiciel. Gareth propose donc d’ajouter des tests de sécurité via Jenkins et des tests unitaires dans notre pipeline de développement. Parmis la liste d’outils (rkhunter, naxsi, logstash, fail2ban, auditd, la distrib BackTrack, clamav, Arachni) certains sont aisément intégrables au workflow. A tester le très bon OWASP ZAP (et https://www.dvwa.co.uk/ pour se faire la main !).\n\nLes slides :\n\n\n\nBeyond Pretty Charts…. Analytics for the cloud infrastructure\n\nToufic Boubez (Metafor Software) @tboubez\n\nToufic travaille depuis 20 ans dans la gestion des données des datacenters et la détection d’anomalies. Comme lors de la présentation de Twitter il explique qu’on ne peut pas appliquer ces données temporelles des méthodes statistiques classiques (holt winter forecast, régression linéaire, smooth splines), car elles sont non stationnaires (violant ainsi le principe d’homogénéité) et la plupart du temps elles ne sont pas distribués normalement (principal pré-requis). Il nous a donc présenté le test de Kolmogorov-Smirnov couplé aux techniques de bootstraping qui permet d’avoir des prédictions assez fiables. Comme les modèles ARIMA, il fait partie de la famille des méthodes statistiques non paramétriques, qui ne présupposent pas de la distribution des données)\n\nLes slides :\n\n\n\nAutomated Multi-Platform Golden Image Creation, Unlocking New Potential\n\nMitchell Hashimoto (HashiCorp) @mitchellh\n\nAvoir un environnement stable, clonable depuis la dev vers la prod est le rêve conjoint des développeurs et sysadmins. L’utilisation des images pour le déploiement de machines et de code peut être fastidieux, au moindre changement de version il faut instancier l’image, faire la modification, recréer l’image etc…Ce qui n’est pas forcement adapté au cloud computing et la virtualisation. A l’inverse n’utiliser que des logiciels de gestion de la configuration (Cfengine, Puppet, Chef) ne certifie pas qu’un serveur vierge aura le même comportement qu’un serveur sur lequel on aura passé 10.000 modifications. Il m’arrive régulièrement d’avoir des erreurs de run puppet cause d’une dépendance non satisfaite (packages) ou de problèmes réseau, (le plus souvent on souffre de la lenteur d’application d’un profil puppet).\n\nDe plus passé du VMWare du AWS ou Vagrant demande d’avoir autant d’images que de plateforme ! Ce saint graal du “serveur immuable” et agnostique de la plateforme est possible en utilisant une méthode intermédiaire : Packer permet de créer des images (Aws, Virtualbox, VMWare) partir d’une source et avec l’aide d’un chef/puppet/cfengine. Le workflow proposé est le suivant: commit dans le repository =&amp;gt; build avec Packer =&amp;gt; CI (Jenkins) =&amp;gt; Image ready !\n\nCela permet de garder la flexibilité d’un Puppet avec l’idempotence des images et leur facilité, rapidité de déploiement.\n\nL’orchestration peut être réalisée avec Serf, il implémente un protocole Gossip (tout le monde se parle, mais pas en même temps). C’est un agent installé sur le serveur qui gère des messages et des handlers (scripts personnalisés dans le langage de votre choix). Dans l’exemple de déploiement de multiples load balancer, l’image va permettre d’avoir un système fonctionnel rapidement, les utilisateurs, les logiciels et c’est Serf qui récupérera la configuration appliquer au load balancer.\n\nOn peut aller plus loin en déployant les code avec Docker, ce qui ajoute une couche d’abstraction supplémentaire extrêmement puissante.\n\nDOM to Pixels: Accelerate Your Rendering Performance\n\nPaul Lewis (Google - Team Chrome) @aerotwist\n\nPaul Lewis explique quelques principes mis en oeuvre lors du rendu graphique dans Chrome tel que la gestion des calques qui permet d’utiliser plus intensément la puissance du GPU mais dont la multiplication peut s’avérer contre productive : la gestion de trop nombreux calques par le CPU contrebalance la performance du rendu par le GPU (évidemment, sinon cela serait trop simple).\n\nPour bien appréhender cette présentation, il m’a semblé nécessaire d’avoir un petit background dans la programmation graphique : savoir, par exemple, pourquoi une ombre ou un flou sont couteux pour le rendu (cause des calculs entre les différentes zones de mémoire contenant les informations graphiques superposer).\n\nPaul présente ensuite en détail l’outil de debug du rendu dans les WebTools : comment enregistrer en temps réel les différentes frames affichées par Chrome et visualiser les différents temps de calculs.\n\nCette présentation, bien que très intéressante par son contenu fut aussi mise en valeur par l’interprétation de Paul Lewis : toujours précise mais simple, sérieuse et fun la fois.\n Ce fût pour moi, la meilleure présentation (show!) de la Vélocity.\n\nEt n’oubliez pas :\n\n\n  “Tools, not rules” Paul Lewis\n\n\nConclusion :\n\nEpuisé par trois jours de conférence d’une densité incroyable, La Velocity a encore aisément tenu toute ses promesses.\n\nNous espérons que ces comptes-rendus vous auront été utile, autant que les présentations nous l’ont été.\n\nN’hésitez pas à commenter l’un des CR pour donner votre avis, sur le CR, ou sur certains points couvert par les Talks.\n\nMerci.\n\nVous pouvez retrouver :\n\n\n  quelques vidéos de la conférence sur Youtube\n  les slides sur le site d’Oreilly\n  et les photos ici sur Flickr : https://www.flickr.com/photos/oreillyconf/sets/72157637657689424/\n\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 2",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-2",
    "date"     : "November 16, 2013",
    "excerpt"  : "\n\nDe retour à l’hôtel Hilton de Londres, afin de commencer cette deuxième journée qui s’annonce très chargée : jusqu’4 tracks en parallèle. Performance, Mobile, Ops, et Sponsors.\n\nMaking Government digital services fast\n\nPaul Downey @psd\n\n\n\nPaul e...",
  "content"  : "\n\nDe retour à l’hôtel Hilton de Londres, afin de commencer cette deuxième journée qui s’annonce très chargée : jusqu’4 tracks en parallèle. Performance, Mobile, Ops, et Sponsors.\n\nMaking Government digital services fast\n\nPaul Downey @psd\n\n\n\nPaul est un “Technical Architect” pour le gouvernement anglais. Il nous explique comment ils gèrent et priorisent les problématiques de performances pour offrir des services internet centrés sur les besoins des utilisateurs avant ceux du gouvernement.\n\nAvec notamment une réduction drastique du nombre de pages, ce qui leur a permis d’obtenir plus de visites au final !\n\nLe tout est entièrement documenté en ligne, en accès public, et regorge d’informations intéressante que vous pouvez retrouver sur le Gov.Uk Service Manual.\n\n\n\nStand down your smartphone testing army\n\nMitun zavery (keynote)\n\n\n\nMitun travaille chez Keynote, et fait une démonstration de deux de leurs outils :\n\nKeynote DA Free (DA = Device Anywhere)\n\nL’outil très intéressant propose un grand nombre d’appareils mobiles qu’on peut acquérir pendant 10 minutes, afin de lancer des tests. Le gros intérêt est qu’on parle ici de vrais appareils, pas de simulateurs.\n\nLe service est disponible sur : https://dafree.keynote.com après vous êtres inscrit gratuitement sur cette url https://www.keynotedeviceanywhere.com/da-free-register.html\n\n(Le service ne fonctionne pas sur Chrome mac pour ma part)\n\nC’est plutôt impressionnant techniquement, on lance les applications que l’on souhaite, rentre du texte, change l’orientation … ! A mémoriser.\n\nMITE, le deuxième outil qui à l’air très complet permet d’aller beaucoup plus loin, mais avec des simulateurs cette fois : https://mite.keynote.com/download.php\n\nDommage que l’on oublie les Macs dans l’histoire.\n\n\n\nTesting all the way to production\n\nSam adams (lmax exchange) @LMAX\n\n\n\nPour ceux qui ne seraient pas encore convaincu de l’intérêt des tests automatisés, Lmax exchange (site sur l’univers boursier gèrant des sommes d’argent assez phénoménales) présente le workflow de développement basé sur les tests pour déployer du code le plus souvent possible en évitant au maximum les régressions.\n\n\n\nMost of the time we measure the performance of others\n\nklaus enzenhofer (compuware) @kenzenhofer\n\n\n\nCourte présentation de Compuware qui édite des solutions de monitoring, et aussi Dynatrace Ajax avec une étude de cas assez simple sur la détection d’anomalies sur un site (142 domaines de 3rd party chargés !).\n\nLe blog de Compuware regorge d’article en tout genre sur la Webperf.\n\n\n\nMaking performance personal at Ft labs\n\nAndrew Betts @triblondon\n\n\n\nLes équipes du Financial Times sont très actifs dans le domaine de la webperf, avec notamment l’outil FastClick qui permet d’enlever le delay du touch sur mobile (entre 100 et 300 ms!). Ils developpent aussi une webapp html5 très riche, et expliquent comment rendre la problématique de performance importante aux yeux du “produit”. On apprend pas mal d’astuces pour mesurer la performance, gérer le appcache, éviter les sprites etc …\n\nVous pouvez retrouvez les slides ici : https://triblondon.github.io/talk-makingperfpersonal/#/\n\n\n\nLightning demo : Global web page performance\n\nJames Smith (Devopsguys) @thedevmgr\n\n\n\nJames est venu présenter lors d’une lightning démo : worldwidepagetest.com\n\nUn outil permettant de tester partout dans le monde les performances de son site, basé sur Webpagetest, et les locations et browsers disponible.\n\nEn plus de l’échec total de la démo (bug/plantage … “worst demo ever” d’après le speaker lui même), l’outil qui parait intéressant sur le papier me semble une fausse bonne idée et le risque de saturer les instances de WPT mondial à cause de ce type d’outil me parait bien plus gênant que les avantages qu’il apporte.\n\nLightning demo: HTTP Archive, BigQuery, and you!\n\nIlya Grigorik @igrigorik\n\n\n\nL’impressionnante quantité de données agrégées par HTTP Archive est maintenant disponible dans Google BigQuery : toutes les données statistiques sur les requêtes et réponses HTTP de plusieurs centaines de milliers de site différents sont donc simplement requêtables et disponibles la vitesse de la lumière (c’est une expression à la mode en ce moment par ici).\n\nUn article d’Ilya explique la marche suivre pour utiliser les données de HTTP Archive stockées sur BigQueries : https://www.igvita.com/2013/06/20/http-archive-bigquery-web-performance-answers/\n\nNous pouvons ainsi aisément effectuer quelques comparaisons avec nos “concurrents” et néanmoins amis présents la conférence :-) : https://denisroussel.fr/httparchive-bigquery-french-test.html\n\nIlya présente aussi le site communautaire BigQueri.es (Powered by Discourse), permettant de partager les réponses des questions statistiques sur les bases présentes dans BigQuery (notamment celle de HTTP Archive)\n\n\n\nGimme More! Enabling User Growth in a Performant and Efficient Fashion\n\nArun Kejariwal (Twitter) @arun_kejariwal, Winston Lee (Twitter Inc.) @winstl\n\nLa planification de la capacité (“capacity planning”) chez Twitter passe par l’utilisation de modèles statistiques et la prédiction sur des données temporelles. C’est absolument nécessaire pour le dimensionnement des plateformes techniques.\n\nL’utilisation d’une simple régression linéaire capture la tendance globale, mais ne prends pas en compte la saisonnalité ni les pics de trafic (positifs ou négatifs). Un modèle “smooth splines” correctement paramétré, de part son design, ne le permet pas non plus. Idem pour le Holt Winters (que vous pouvez tester avec graphite). Ils utilisent donc le modèle ARIMA (Autoregressive integrated moving average), qui permet d’effectuer des prédictions partir de données temporelles non stationaires (c’est dire que la moyenne et la variance change = pics de trafic). Le nettoyage des données et la vérification du modèle représente la majorité du travail. Les données journalières permettent de prédire jusqu’90 jours, et les données la minute un trimestre. Les prédictions sur 1 mois des métriques systèmes (cpu, ram) sont considérées comme fiables alors que les métriques business (nombre d’utilisateurs, nombres de photos ou vidéos stockées) le sont pour 3 ou 4 mois. Pour les évènements exceptionnels (superbowl, nouvelle année) ces prédictions ne sont pas assez fiables, ils se basent donc simplement sur les années précédentes.\n\nWhen dynamic becomes static: the next step in web caching techniques\n\nWim Godden (Cu.Be Solutions)\n\nLe monsieur effectue d’abord un récapitulatif des pratiques de cache dans le web depuis son commencement : sans cache, avec du cache applicatif, avec du reverse proxy cache et enfin avec beaucoup trop de systèmes de cache qui rende l’architecture très complexe (tiens tiens).\n\nPuis apparaissent les ESI, c’est en gros du reverse proxy cache par bloc (en réalité, ils sont parmi nous depuis bien longtemps). Mais une limitation conceptuelle évidente borne leur utilisation : les sites sont très souvent personnalisés en fonction de l’utilisateur (affichage du nom de l’utilisateur connecté par exemple). Et du coup, les blocs personnalisés, même simples, ne peuvent bénéficier du reverse proxy cache. Ce qui défie un peu le concept.\n\nPour pallier à ce problème, Wim et son équipe ont développé un langage spécifique dans Nginx (qui est aussi un reverse proxy en plus d’être un serveur http) permettant au serveur web de gérer des variables directement dans le reverse proxy afin que celui-ci les stock dans son propre memcache et puisse y accéder pour retourner la page au client sans faire un appel supplémentaire au serveur web : SCL.\n\nAlors oui. C’est pas forcément l’idéal de commencer poser des variables dans le reverse proxy. Mais n’ayons crainte, ce n’est pas pour tout de suite : la release publique ne devrait pas arriver avant mi-2014 :-)\n\nLes slides\n\nDeveloper-Friendly Web Performance Testing in Continuous Integration\n\nMichael Klepikov (Google, Inc)\n\nIntégrer les mesures/tests de régressions de performance dans nos outils d’intégrations continues et une tâche très compliqué. Michael présente une approche assez maligne consistant utiliser les tests fonctionnels déjen place, pour récolter les mesures des outils de R.U.M. déjà présent sur le site (soit parce que les mesures sont présentes dans l’url d’appel de l’outil de R.U.M.), soit en récupérant les valeurs dans les DevTools de Chrome.\n\nL’outil TSviewDB permet d’avoir une interface qui agrège plusieurs time-series sur une seule time-series (plus d’infos dans le Readme du projet).\n\nPas mal d’informations à creuser dans les slides, comme l’envoi de donnée directe WebPageTest pour utiliser l’UI sur le résultat, ou la façon de récupérer les infos de la DevTools de Chrome en vidéo\n\nLes slides :\n\n\n\nIntegrating multiple CDN providers at Etsy\n\nMarcus Barczak (Etsy), Laurie Denness (Etsy)\n\nPour des raisons de haute disponibilité, de résilience et de balance des coûts, Etsy a mis en place depuis 2012 un système qui leur permet d’utiliser de multiples CDN (parmi eux Akamai, Fastly, et EdgeCast). Leur critères d’évaluations sont le hit ratio et la décharge de trafic des serveurs origine, le reporting, le pilotage via des APIs, la personalisation et l’accès aux logs HTTP. Ils ont pour cela dû faire du ménage dans leur codes et dans leurs headers (cache-control, expires, etag, last-modified)\n\nIls ont commencé par les images (1% puis 100% du trafic), et se sont servis des CDNs pour effectuer leur tests A/B. L’equilibrage de charge entre les CDNs se fait manuellement via une interface web ou via un outil en ligne de commande qu’ils ont mis disposition de la communauté (cdncontrol sur leur github). L’inconvénient de cette solution est la multiplication des requetes DNS (puisqu’ils utilisent des CNAME type img1.etsystatic.com =&amp;gt; global-ssl.fastly.net par exemple), la non atomicité et le delais des modifications DNS qui engendrent une long tail importante et le debug plus complexe. Les requetes depuis les CDNs vers les origines sont trackés via un header HTTP et sont monitorés dans un graphite surveillé par un nagios selon un seuil déterminé.\n\n\n  “If you can do it at the origin, do it !”.\n\n\n\n\nWhat is the Velocity of an Unladen Swallow? A quest for the Holy Grail.\n\nPerry Dyball (Seatwave Ltd) @perrydyball\n\nRetour d’XP très utile, plein d’honnêteté et d’humilité, de SeatWave (site permettant d’acheter des tickets concerts/spectacle etc), sur l’effet douloureux de la première pub télé qu’ils ont acheté pour promouvoir leur service. On découvre la façon dont ils ont su optimiser leur application pour supporter les publicités suivantes, grâce un système de queuing avec un décompte en cas de fortes charges, ainsi que les impacts sur une multitude de métriques et le coté financier.\n\nLe phénomène est presque un running gag chez nous (ou même sur Twitter), quand votre (ou même l’un de nos …) site est présenté dans une pub ou Capital, et que votre application et/ou serveur ne supporte pas la charge.\n\nBref, encore un bel exemple de culture d’entreprise, qui démontre que la performance n’est pas un projet ou une feature “one shot”, mais une culture et une mentalité constante.\n\n\n  “Performance it’s not just for today, it’s for every day” Peter Dyball\n\n\n\n\nGetting 100B metrics to disk\n\nJonathan Thurman (New Relic)\n\nNew relic a présenté l’architecture MySQL qui stocke leur 196 milliards de métriques journalières. Elle est basée sur des shards MySQL, propulsée par de puissants serveurs (12 actuellement) équipés de SSD Intel et de shelf de disques Dell. Les shards sont fait (via leur API shardGuard) par numéro de client, et les tables MySQL sont construites sur le modèle numéroClient_year_julianDay_metricResolution. Il y a environ 200.000 tables par databases. Les métriques sont régulierement (toutes les heures), purgées et aggrégées, en utilisant le innodb_lazy_drop_table de percona 5.5 (et surtout par delete from ou drop table).\n\nLe code initialement en Ruby est passé Java/Jetty. Les inserts se font séquentiellement en batch de 5000 et sont buffurisés en RAM (ils doivent se faire en moins d’une minute, résolution minimale du produit). Ils prévoient d’utilisé de multiples instances MySQL par serveurs et de gonfler leur capacité hardware (SSD 800G, 96G RAM)\n\n\n\nHigh Velocity Migration\n\nJoshua Hoffman (SoundCloud) @oshu\n\nJoshua nous a conté l’histoire d’une startup (fictive mais pas vraiment car c’est celle de Tumblr), qui a commencé en 2006 entre deux amis souhaitant partager des images de parties d’échecs et comment en 2012 elle a dû, en 6H de maintenance, basculer 1200 serveurs et les données de 50M d’utilisateurs. Il nous a détaillé l’évolution année par année de l’infrastructure et du nombre de devs/sysadmins. Les ingrédients pour gérer une croissance comme celle ci sont selon lui : le provisionning automatique (ipxe, kickstart), la gestion de la configuration (puppet/chef/ansible), le monitoring et l’alerting et les outils de déploiement de code. Il précisé qu’il faut accepter l’imperfection de ses outils, ne pas chercher réinventer la roue mais plutôt utiliser l’open source, ne pas hésiter tuer les projets Zombies (ceux qui durent depuis trop longtemps et qui n’ont pas les fonctionnalités attendues !) et surtout respecter le principe KISS (keep it simple and stupid).\n\nLa migration en 2012 de leur plateforme gérée par une société tierce vers leur propre infrastructure a commencé 120 jours plus tôt avec l’installation des serveurs, machines, systèmes de déploiements, l’acquisition de leur numéro d’AS, et l’utilisation en front d’un proxy pour plus tard pouvoir rediriger le trafic de façon transparente vers la nouvelle infrastructure. Le jour J la fenetre de maintenance du site de 6H été suffisante pour synchroniser les données utilisateurs entre les deux infras, tester et mettre en production.\n\n\n\nCode is Evil\n\nDan Rathbone (British Sky Broadcasting)\n\nFace aux problèmes de performance du site https://www.skybet.com/, qui doit probablement attirer une quantité impressionnante de parieurs tout en devant afficher des données très fraîches (cela fait partie du business model), Dan a remis plat toute la logique de développement du site.\n\nLorsque la performance passe seule au premier plan, il est ainsi possible de renverser le paradigme du développement dans son ensemble : alors qu’en général, les données sont stockées structurées puis extraites pour peupler du code métier puis affichées par des templates élaborés, dans ce cas particulier, les données sont directement stockées de manières dénormalisées, directement prêtes être affichées par des templates simplistes. Le code métier est en amont et sert pré-calculé les données qui sont stockées en base.\n\nIl est ainsi possible de minimiser drastiquement la quantité de code critique. Et cela ouvre beaucoup de portes : peu de code = peu de maintenance, aucun framework nécessaire, aucun cache nécessaire, etc.\n\nC’était une présentation assez polémique mais particulièrement intéressante (ce qui n’était pas l’avis de l’audience, semblerait-il) et rafraîchissante car elle permet de sortir des cas standards du monde du web. Entre nous, tous ces principes étaient déjen vogue dans le développement des jeux vidéo dans les années 90 : nous devions constamment contourner la limitation du matériel (les optimisations étaient au cycle processeur près).\n\nBreaking 1000ms Mobile Barrier\n\nIlya Grigorik (Google)\n\n\n\nComment arriver afficher sa page web sans dépasser la barrière de 1000 ms ! Un dur challenge dont les épreuves sont détaillés par Ilya.\n\n\n\nDes problèmatiques de latence sur le “Touch” mobile, sur les communications 3G/4G, du fonctionnement TCP, du critical rendering path au niveau CSS et JS, mod_page_speed et ngx_page_speed, ainsi que des évolutions venir sur Page Speed Insights, c’est un panel ultra complet de la WebPerf qui été couvert sur cette heure ultra dense, mais oh combien indispensable. C’est donc, comment souvent avec Ilya Grigorik, un must read absolu pour ceux que la Performance Front-End et Mobile, ainsi que la latence, passionne.\n\nLes slides sont ici https://docs.google.com/presentation/d/1wAxB5DPN-rcelwbGO6lCOus_S1rP24LMqA8m1eXEDRo/present#slide=id.p19\n\nLive Sketching !\n\nAvant de conclure, petit hommage Natalia Talkowska , qui, sur chaque conférence, réalisait un live sketching d’une qualité incroyable\n\n@Natalka_Design #livesketching is back with @allspaw @souders and @courtneynash opening up #velocityconf, let&amp;#39;s go! pic.twitter.com/FYBQIVk8tr&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @psd at #velocityconf as first #keynote! pic.twitter.com/9zAMXJZNWW&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @keynotesystems at #velocityconf pic.twitter.com/S8XYaNFKzU&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @LMAX at #velocityconf pic.twitter.com/UYLYDTSuPO&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @kenzenhofer at #velocityconf pic.twitter.com/l2ndwj8V1G&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nMust follow: @NatiTal: #livesketching @psd at #velocityconf as first #keynote! pic.twitter.com/W8xTTjC581 #Awesomeness&amp;mdash; Mike Hendrickson (@mikehatora) November 14, 2013\n\n\n#livesketching @triblondon at #velocityconf pic.twitter.com/VTF2gZFEsH&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @thedevmgr at #velocityconf pic.twitter.com/iaaZgRxVwN&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nNot a bad likeness! “@NatiTal: #livesketching @triblondon at #velocityconf pic.twitter.com/Pq4NiAPEW7”&amp;mdash; Andrew Betts (@triblondon) November 14, 2013\n\n\n#livesketching @igrigorik at #velocityconf pic.twitter.com/5rBPelS9an&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\n#livesketching @edgecast at #velocityconf last #keynote pic.twitter.com/rZ5Pa1MvhQ&amp;mdash; Natalia Talkowska (@NatiTal) November 14, 2013\n\n\nConclusion :\n\nC’est complètement lessivé que nous sortons de cette journée, avec une quantité d’idées / projets tester incroyable.\n\nVous pouvez retrouver le compte rendu de la première journée ainsi que de la dernière sur notre Blog.\n\nN’hésites pas donner vos retours (positifs ou négatifs en commentaire). Merci :-)\n\n© des photos : Flickr officiel O’Reilly\n\nCR rédigé par Baptiste, Denis Roussel et Kenny Dits\n"
} ,
  
  {
    "title"    : "Velocity Europe 2013 - Day 1",
    "category" : "",
    "tags"     : " conference, velocity, webperf",
    "url"      : "/velocity-europe-2013-day-1.html",
    "date"     : "November 14, 2013",
    "excerpt"  : "Introduction :\n\n\n\nNous voici de retour à Londres pour la troisième édition de la Vélocity Europe, qui se déroule, pour la deuxième fois à Londres (la précédente était à Berlin).\n\nPour rappel, la Vélocity est la conférence autour de la performance ...",
  "content"  : "Introduction :\n\n\n\nNous voici de retour à Londres pour la troisième édition de la Vélocity Europe, qui se déroule, pour la deuxième fois à Londres (la précédente était à Berlin).\n\nPour rappel, la Vélocity est la conférence autour de la performance web. Qu’elle soit Front-End, Back-End, Dév ou Ops. C’est l’événement de l’année à ne pas manquer en Europe, ou aux US (ou Chine) pour les plus chanceux\n\nCette première journée (ayant eu lieu le 13 novembre 2013) est axée sur le signe des “Tutorials”. De looongues conférences de 90 minutes dont voici le compte rendu écrit à 6 mains.\n\nLa conférence “classique” commence le 14 et se déroulera sur deux journées.\n\nGone in 60 frames per second\n\nAddy Osmani (Google Chrome) @addyosmani\n\n\n\nAddy est une figure incontournable du web. Créateur de TodoMVC, Lead dév de Yeoman et travail dans la Google Chrome Team sur les outils à destination des développeurs autour du navigateur.\n\nAprès la génération du code html par les serveurs et le transfert de ce code par les réseaux, le rendu graphique de la page par le navigateur est le dernier évènement significatif du chargement de la page lors de la consultation d’un site par un client.\n\nVoici donc un résumé des bonnes pratiques permettant d’obtenir un meilleur framerate (nombre de rafraîchissement de la page par seconde) et ainsi une meilleure fluidité lors de la navigation :\n\n\n  disposer des images à la bonne taille pour éviter les redimenssionnements à la volée,\n  limiter les handlers sur l’événement onScroll(),\n  limiter tous les éléments ‘fixed’ car cela force le navigateur à recalculer constamment la zone affichée (ou utiliser l’astuce translateZ(0)),\n  \n    limiter les directives CSS qui nécessites un calcul supplémentaire (lorsque tout est déjà affiché) :\n  \n  les ombres,\n  les flous,\n  et les dégradés : (Bootstrap a supprimé tous les dégradés sur ses boutons : +100% de rapidité l’affichage).\n\n\nEnsuite, il reste quelques conseils plus généraux :\n\n\n  Il faut se souvenir que les performances des téléphones ne sont pas celles des PC,\n  un framerate de 60 fps est parfait (c’est dû au matériel), mais un framerate de 30 fps peut aussi être suffisant pour peu qu’il soit constant,\n\n\nEnfin, comme souvent, tous les outils pour comprendre et améliorer le rendu graphique de ses pages web sont disponible dans tous les navigateurs. Dans Chrome, il suffit d’aller dans la section “Frames” de l’onglet “Timeline” des DevTools.\n\nLes slides sont disponible ici : https://speakerdeck.com/addyosmani/velocityconf-rendering-performance-case-studies\n\nLa présentation de la conférence par l’auteur lui-même : https://addyosmani.com/blog/making-a-site-jank-free/\n\n\n\n\n\nVidéo de la même conférence (donnée à la Smashing Conf 2013)\n\nBring the noise : Making effective use of a quarter million metrics\n\nJon Cowie (Etsy) @jonlives\n\n\n\nJon est “Ops Engineer” chez Etsy (Dont le VP, John Allspaw, co-organise avec Steve Souders, la Vélocity).\n\nQuelques données sur Etsy :\n\n\n  Ils font du déploiement continu\n  1.5 milliards de pages vues\n  250 contributeurs (tout le monde déploie du code, même les chiens)\n  ils utilisent Deployinator pour déployer leur code avec un unique “bouton”, et schemanator pour les migrations SQL\n  60 déploiements par jour / 8 commit par deploiement\n  ¼ millions de métriques !\n\n\n\n  “We optimize for quick recovery by anticipating problems instead of fearing human error” John Cowie\n\n\n\n  “Can’t Fix what you don’t measure” W. Edwards Deming\n\n\nLeurs outils pour le monitoring :\n\n\n  \n    Not homemade :\n  \n  Ganglia\n  Graphite\n  Nagios\n  \n    Homemade :\n  \n  StatsD : Simple Daemon for easy stats integration\n  Supergrep : Real time log streamer\n  Skyline : A real time anomaly detection system\n  Oculus : A metric correlation component\n\n\n\n  “Not All things that break throw errors” Oscar Wilde\n\n\n\n  “If it moves, graph it ! If it doesn’t move, graph it anyway” Jon Cowie\n\n\nLa présentation s’axe ensuite plus particulièrement sur la stack “Kale”, qui englobe deux outils que l’on va détailler : Skyline et Oculus. Voir l’article sur le blog technique de Etsy https://codeascraft.com/2013/06/11/introducing-kale/\n\nL’objectif de Skyline, est de détecter les comportements anormaux (gros pics par exemple), avec pour principal challenge, la récupération des données (via le “relay agent” de Graphite, ils envoient en continue les données dans Redis via redis.append() ), le stockage de 250 000 métriques (dans Redis) au format MessagePack. Oculus quand lui permet de corréler les métriques, en utilisant les données brutes de l’api de Graphite, car il est bien plus efficace de comparer des chiffres, que des images …\n\nIl n’y a pas un mais huit algorithmes de détections d’anomalies qui sont utilisés dans un vote à majorité, déterminant ainsi si l’anomalie est avérée (parmi ceux ci, OLS, Grubb’s test, l’histogramme bining etc…). La détection se fait sur une fenêtre d’une heure et une seconde de 24 heures. Skyline souffre encore de quelques faiblesses: l’absence de prise en compte de la saisonnalité, les pics qui peuvent en cacher d’autres plus faibles, le postulat pas toujours vrai que les données sont normalement distribuées et les corrélations négatives.\n\nIls comparent donc la distance euclidienne (slide 99), en gérant aussi le décalage temporel (dynamic time warping / DTW) (voir slide 100).\n\nLa partie la plus intéressante est la simplification d’une métrique temporelle, en la normalisant sur une courbe échelle réduite (de 0 à 25), et en la transformant en une chaine textuelle comportant cinq valeurs :\n\n\n  sharpdecrement\n  decrement\n  flat\n  increment\n  sharpincrement\n\n\nEt ceci en fonction de la valeur en cours par rapport à la valeur précedente.\n\nIls poussent toutes ces métriques normalisées dans Elastic Search dans un champ non tokenisé en réalisant des recherches de phrases afin de corréler les métriques ayant le même pattern et en scorant via un plugin codé par leurs soins (incluant une version “rapide” du DTW).\n\nUne fois les métriques corrélées affichées, il est possible de sauvegarder un snapshot de ces dernières et d’inclure des commentaires dans une “collection”. Cela permet notamment de construire une base de données de connaissance sur les incidents ou les comportements anormaux mais explicables.\n\nSkyline est visible par tous dans leur bureaux, sur l’un des 6 écrans de dashboards, devant lesquels on peut notamment lire le nombre de requêtes HTTP par seconde, le top 10 des pages, les temps de générations et d’affichage etc…\n\nLes slides sont disponible ici : https://www.slideshare.net/jonlives/bring-the-noise\n\n\n\nResponsive images Technique and Beyond\n\nYoav Weiss (WL Square) @yoavweiss\n\n\n\nYoav est un spécialiste de la WebPerf et travaille sur les problématiques des images liées au Responsive Web Design. Il est aussi Technical Lead au RICG (Responsive images community Group)\n\nLe principal problème des images responsive, c’est de charger l’image correctement dimensionnée par rapport à une page, de manière efficace.\n\n72% des sites RWD servent les mêmes ressources entre les résolutions petites et grandes …\n\nOn peut économiser 72% en taille d’image en compressant correctement (voir https://timkadlec.com/2013/06/why-we-need-responsive-images/).\n\nYoav a développé un outil utilisant PhantomJs, permettant de mesurer la différence entre les images chargées, et celle qui seraient correctement dimensionnées : Sizer Soze\n\nOn aborde ensuite les deux cas principaux gênant :\n\n\n  Servir une dimension différentes de l’image à différents support. (et les Retina uniquement aux devices le supportant)\n  et le “Art direction”, avoir une image qui correspond au layout\n\n\nAinsi que l’intérêt du Pre-loader, souvent peu connu. Beaucoup plus d’infos sur cet article d’Andy Davies (https://andydavies.me/blog/2013/10/22/how-the-browser-pre-loader-makes-pages-load-faster/)\n\nYoav parcours ensuite toutes les techniques des images responsive avec avantages/inconvénients et exemple pour chacune, que vous pouvez retrouvez dès la slide 57 de la présentation ci après : https://yoavweiss.github.io/velocity-eu-13-presentation/#/\n\nL’étude et les retours sont extrêmement complet, et immanquable, si vous travaillez ou allez travaillez sur le sujet. Il aborde aussi une approche en cours d’étude, qui verra peut être le jour prochainement (Responsive Image Container).\n\nPerformance Analysis of JVM components for non-specialists\n\nBen Evans (JClarity) @kittylyst\n\n\n\nLa performance et la complexité des applications qui fonctionnent sur la JVM ont suivi l’évolution de la loi de Moore. Malgré que nous ayons gagné de la puissance et des transistors, notre code s’est complexifié d’année en année et d’autant plus avec le boom d’Internet.\n\nLe tuning de la JVM est indispensable pour avoir une application performante et doit se faire de façon rigoureuse et scientifique, il faut comprendre, mesurer, tester, vérifier et répéter ce processus jusqu’ce que l’on considère la performance comme bonne.\n\nBen a ensuite détaillé l’anatomie de la JVM, les spécificités du langage Java, les “mid 90’s decisions design” qui ont été faites, comment est géré l’allocation mémoire, la heap, et le fonctionnement du garbage collector (mark and sweep, stop the world). La durée du “stop the world” est ridicule comparé aux temps de latence réseau, ceux engendrés par l’hyperviseur etc…\n\nIl a présenté quelques optimisations indispensables selon lui, et a insisté sur le fait que l’optimisation prématurée pouvait être la source de bien des soucis coté code.\n\nTuning Network Performance to Eleven\n\nIlya Grigorik (Google) @igrigorik\n\n\n\nAKA comment condenser un livre dans un tutorial d’1H30. Exercice encore plus difficile lorsqu’il faut résumer le résumé d’un livre aussi dense et complet. Ilya en tant que spécialiste de la webperf a examiné les mécanismes de la latence et de la bande passante, le fonctionnement du protocole TCP, la gestion de congestion, les problèmes structurels de HTTP 1.0 et HTTP 1.1, l’impact de TLS (le chiffrement) sur les performances. Il a donné ses recommandations pour optimiser TCP et bien utilisé TLS.\n\n“bandwidth + latence =~ performance”\n\n\n  “Video streaming is bandwidth limited, web browsing is latency limited” Ilya Grigorik\n\n\nIl a ensuite expliqué comment fonctionne le réseau radio 2G/3G/4G et les contraintes que ces architectures exercent sur les temps de chargement et la durée de vie des batteries pour les appareils mobiles.\n\nLe tutorial s’est achevé sur les défauts de HTTP 1.1 et les nouveautés (nombreuses et sexys) d’HTTP 2.0. Ce fut extrêmement plaisant d’assister à cette présentation, tant Ilya est pointu techniquement, précis et didactique dans ses démonstrations. Le livre est un MUST-READ !\n\nIl est d’ailleurs disponible gratuitement ici : https://chimera.labs.oreilly.com/books/1230000000545\n\nLes slides sont disponible ici\n\nBe Mean to your code with Gauntlt and the Rugged Way\n\nJames wickett (Mentor Graphics) @wickett\n\n\n\nCette présentation fut le seul et unique vrai “Workshop” du jour, dans le sens où une machine virtuelle (monter avec Vagrant) était fournie pour réaliser l’atelier au fur et mesure de la présentation sur sa machine.\n\nGauntlt est un framework autour de la sécurité, qui fournie des hooks pour de nombreux outils d’attaques (Xss, Sql injection etc …).\n\nAprès une introduction un peu longue autour de la place de la “sécurité” aux seins de nos services.\n\nL’approche de Gauntlt est basée sur le “Rugged Manifesto”\n\nGauntlt permet donc d’automatiser au sein de son système d’intégration continue, des tests autour de la sécurité de son applicatif et de son infra, basés sur Cucumber, utilisant le langage Gherkin (que certains connaissent peut être mieux dans le monde php via Behat), et interfaçant des outils tels que :\n\n\n  Garmr\n  Nmap\n  Arachni\n  Sqlmap\n  …\n\n\nSi vous voulez tester l’outil, qui à l’air très prometteur, vous pouvez suivre ce tutoriel : https://bit.ly/gauntlt-demo-instructions qui vous fourni la Virtual Box, les consignes d’installations, et les exemples ayant été réalisés pendant la conférence, ainsi qu’une application de test en Ruby Railsgoat pour servir de cible à vos tests.\n\nLes slides sont disponible ici\n\n\n\nHands-on Web Performance Optimization Workshop\n\nAndy Davies (Asteno) @andydavies , Tobias Baldauf (Freelancer) @tbaldauf\n\n\n\nDernière session de la journée, avec Andy et Tobias, sur un workshop axé Performance Web.\n\nOn commence par une présentation général d’un outil qu’on ne devrait plus présenter : WebPageTest, l’outil principal pour les problématiques de performances front-end.\n\nAndy aborde ensuite quelques autres outils :\n\n\n  PhantomJs (un headless browser)\n  Simple Website Speed Test\n  et surtout Phantomas, un module PhantomJs pour collecter les métriques de Webperf.\n  le wrapper Node.Js pour WebPageTest de Marcel Duran\n  SiteSpeed.io pour monitorer toutes les pages de son site, basé notamment sur Yslow\n  HttpArchive, l’excellent service de Steve Souders qui tracke le web avec une multitude de stats intéressante, que vous pouvez d’ailleurs installer pour une instance privée afin de tracker vos sites : https://bbinto.wordpress.com/2013/03/25/setup-your-own-http-archive-to-track-and-query-your-site-trends/ \\o/\n\n\nLa suite de la conférence consister a analyser en live certains sites dont quelques uns assez hilarant au niveau performance :\n\n\n  Dailymail.co.uk avec ces +de 800 requêtes HTTP et 7 mo !\n  Wildbit.com qui consomme un CPU énorme cause de l’animation sur le logo qu’on ne voit quasiment pas :)\n\n\nLes slides :\n\n\n\nConclusion :\n\nBonne première journée avec ce format “Tutorials” un peu trop touffu (90 minutes par conférence …). Déjà des tonnes d’idées qui ressortent, on a hâte de voir la suite.\n\nRetrouvez les autres CR :\n\n\n  Compte rendu du jour 2 \n  Compte rendu du jour 3 \n\n\n© des photos : Flickr officiel O’Reilly\n\nCR rédigé par Baptiste, Denis Roussel et Kenny Dits\n"
} ,
  
  {
    "title"    : "Tester fonctionnellement une API REST",
    "category" : "",
    "tags"     : " qualite, symfony, atoum, tests fonctionnels",
    "url"      : "/2013/10/tester-fonctionnellement-une-api-rest-symfony-doctrine-atoum",
    "date"     : "October 14, 2013",
    "excerpt"  : "Un des enjeux des tests fonctionnels est de pouvoir être joués dans un environnement complètement indépendant, dissocié de l’environnement de production, afin de ne pas être tributaires de données versatiles qui pourraient impacter leur résultat. ...",
  "content"  : "Un des enjeux des tests fonctionnels est de pouvoir être joués dans un environnement complètement indépendant, dissocié de l’environnement de production, afin de ne pas être tributaires de données versatiles qui pourraient impacter leur résultat. Il faut, cependant, que cet environnement soit techniquement similaire à celui de production pour que les tests aient une réelle validité fonctionnelle.\n\nAvec la Team Cytron, nous sommes tombés face à cette problématique lorsque nous avons voulu tester fonctionnellement un service agnostique de contenu mettant à disposition une API REST et utilisant Symfony2, MySQL, Doctrine et atoum.\n\nMonter un serveur de données dédié aux tests\n\nDans le cas d’une application utilisant MySQL, on pense alors monter un serveur applicatif de test relié à une base de données de test. Plusieurs problèmes peuvent alors découler d’un tel système :\n\n\n  il faut être en mesure de pouvoir mettre en œuvre un serveur MySQL dédié uniquement aux tests,\n  mais surtout cette architecture n’est pas exploitable pour exécuter des tests de manière concurrentielle (ce qui pose problème pour l’intégration continue). En effet, des collisions apparaîtraient en base de données et le résultat des tests ne seraient plus exploitables.\n\n\nMocker Doctrine\n\nNotre seconde réaction a été de vouloir mocker Doctrine pour devenir indépendant de MySQL. Lourde tâche.\n\nTant bien que mal, nous sommes arrivés à un résultat plutôt satisfaisant car notre API réalise des opérations simples : ajout, modification, suppression et consultation avec un filtrage élémentaire.\n\nLa première chose à faire est de s’assurer que notre serveur de test n’accède pas aux données de production dans MySQL en changeant la configuration Doctrine dans le fichier config_test.yml.\n\n\n\nEnsuite, nous avons créé une classe abstraite dont héritent toutes nos classes de test, et qui permet d’initialiser le mock de Doctrine.\n\nAutant vous dire que le développement de cette classe a été fastidieux car incrémental : chaque nouveau besoin de manipulation de données dans nos tests, il a fallu modifier le mock pour prendre en compte des méthodes ou des fonctionnalités de méthodes qui n’avaient pas été encore mockées (comme le filtrage par critères dans la fonction findBy).\n\nLes possibilités de ce mock reste limitées. Nous sommes, par exemple, tombés sur le cas où deux managers de données en relation (des recettes et leurs ingrédients) dépendaient d’un même EntityManager Doctrine : tel que nous l’avons développé, le mock ne sait pas gérer cette situation et engendre des erreurs à l’exécution. Il aurait fallu refactoriser le code pour parvenir à nos fins et passer encore plus de temps sur ce projet… et nous n’en avions pas beaucoup !\n\nAutre problème : nous utilisons des fonctionnalités de la librairie Gedmo/DoctrineExtensions pour la gestion automatique des dates de création et de modification. Évidemment, elles ne sont pas opérationnelles avec notre mock et nous aurions encore dû développer pour faire passer nos tests.\n\nUtiliser les transactions de Doctrine\n\nIl a donc fallu nous rendre l’évidence : cette solution ne correspondait pas à nos besoins ! Nous avons alors émis l’hypothèse d’une alternative qui nous permettrait peut-être de nous passer d’une config spécifique MySQL pour nos tests : l’utilisation des transactions via Doctrine.\n\nAu début de chaque test, nous aurions ouvert une transaction mais qui n’aurait jamais été commitée par la suite, évitant toute interaction avec la base de données de production. Mais avec cette solution, dangereuse à mettre en place et à maintenir, nous aurions couru le risque de modifier des données de production.\n\nRemplacer MySQL par un autre SGBD uniquement pour les tests\n\nFinalement, nous sommes partis sur une autre piste, celle qui fait actuellement tourner nos tests fonctionnels sur ce projet. Nous utilisons SQLite dans notre environnement de test la place de MySQL. Ce SGBD est très léger et simple mettre en œuvre : pas besoin d’une installation sur un serveur dédié, il suffit simplement d’activer une extension de PHP. SQLite se base sur des fichiers physiques pour gérer le stockage des données. Ainsi, chaque build de test peut avoir ses propres fichiers de BDD dans son répertoire évitant toute collision dans le cas de tests concurrentiels.\n\nNous avons donc configuré Doctrine, pour qu’il utilise SQLite lors de son exécution en environnement de test en modifiant le config_test.yml\n\n\n\nComme pour le mock de Doctrine, nous avons mis en place une classe abstraite qui permet de gérer la réinitialisation de la base pour chaque test.\n\nNous pouvons donc maintenant tester unitairement et fonctionnellement notre API REST développée en PHP l’aide de Symfony2 et Doctrine. Et nous ne nous en privons pas : notre API est couverte par bientôt 5.000 assertions.\n\nGénération des données de test\n\nAprès avoir trouvé une solution pour l’accès à la structure de données en environnement de test, nous nous sommes penchés sur la question du contenu de ces données de tests. Pas longtemps.\n\nNotre service REST permettant des opérations CRUD, nous partons pour chaque test d’un contenu vide que nous remplissons à l’aide de notre propre service. Cela permet de tester beaucoup plus de cas d’utilisation. Mais surtout cela permet aussi de tester des cas plus réels, plus proches de son utilisation par nos clients.\n"
} ,
  
  {
    "title"    : "Distribuez votre vidéo partout avec 3 euros en poche et devenez millionaire. Ou presque.",
    "category" : "",
    "tags"     : " lft, video",
    "url"      : "/2013/10/distribuez-votre-video-partout-avec-3-euros-en-poche-et-devenez-millionaire-ou-presque.html",
    "date"     : "October 9, 2013",
    "excerpt"  : "“Comment gagner des millions, sans sortir de chez vous, en robe de chambre, en distribuant des vidéos de chats sur les internets, grâce à ffmeg, h264, dash, tous pleins de buzz word, justin bieber” (Merci ! Toute l’équipe SEO).\n\nUne présentation d...",
  "content"  : "“Comment gagner des millions, sans sortir de chez vous, en robe de chambre, en distribuant des vidéos de chats sur les internets, grâce à ffmeg, h264, dash, tous pleins de buzz word, justin bieber” (Merci ! Toute l’équipe SEO).\n\nUne présentation de Ludovic Bostral, notre ex valeureux responsable R&amp;amp;D en charge - jusqu’il y a peu de temps - de la fabrication de toutes nos vidéos et du SI associé.\n\nSi vous vous posez des questions ce sujet, n’hésitez pas à venir lui faire un petit coucou virtuel, ou sur Nantes. Ca marche aussi pour discuter zombie ou nanar. Ou mieux, un nanar avec des zombies !\n\nRetrouvez Ludovic sur son site : https://digibos.com.\n\n"
} ,
  
  {
    "title"    : "Le NoSQL, Focus sur MongoDB par Cédric Derue (Altran)",
    "category" : "",
    "tags"     : " lft, nosql, mongodb, video",
    "url"      : "/le-nosql-focus-sur-mongodb-par-cedric-derue-altran",
    "date"     : "October 8, 2013",
    "excerpt"  : "Porte-étandard des bases de données NoSQL de type document, MongoDB nous a été présenté cet été par Cédric Derue (@cderue) , de la société Altran, lors de nos conférences internes.\n\nDans cette présentation d’environ une heure, il aborde un tour d’...",
  "content"  : "Porte-étandard des bases de données NoSQL de type document, MongoDB nous a été présenté cet été par Cédric Derue (@cderue) , de la société Altran, lors de nos conférences internes.\n\nDans cette présentation d’environ une heure, il aborde un tour d’horizon des différentes catégories de bases de données NoSQL, pour s’attacher ensuite sur un focus assez complet de MongoDB, agrémenté de quelques démonstrations.\n\nMerci à Altran et Cédric pour le partage de cette présentation.\n\nVous pouvez aussi retrouver d’autres sessions de nos Last Friday Talk :\n\n\n  Introduction Drupal par Claire Roubey (Clever Age)\n  Redis on Fire\n  La POO Canada Dry\n\n\nMalheureusement, la vidéo n’est plus disponible…\n"
} ,
  
  {
    "title"    : "Vigo, le fléau des Carpates",
    "category" : "",
    "tags"     : " outil, qualite, javascript, tests fonctionnels",
    "url"      : "/vigo-le-fleau-des-carpates-la-tristesse-de-moldavie",
    "date"     : "August 13, 2013",
    "excerpt"  : "CasperJS permet d’écrire des scripts javascript qui vont automatiser des tests fonctionnels de pages web. Il exécute ces tests dans une instance de PhantomJS qui est un navigateur scriptable et sans interface graphique (“Headless” dit-on dans le m...",
  "content"  : "CasperJS permet d’écrire des scripts javascript qui vont automatiser des tests fonctionnels de pages web. Il exécute ces tests dans une instance de PhantomJS qui est un navigateur scriptable et sans interface graphique (“Headless” dit-on dans le milieu).\n\nAfin de mieux structurer nos tests, de faciliter leur écriture et de pouvoir les lancer avec une commande unique, nous avons créé VigoJS, une surcouche pour CasperJS.\n\nFonctionnalités\n\nToutes les fonctionnalités de base de CasperJS sont accessibles. Nous y avons simplement ajouté un mécanisme de configuration contenant plusieurs paramètres de base dont l’URL de test par défaut, l’authentification HTTP éventuelle ou encore la taille de l’écran virtuel. Il est également possible de spécifier des environnements (dev, preprod, prod…) pour différencier les comportements de certains tests. Ainsi, en fonction de l’environnement demandé dans la ligne de commande, les tests peuvent être joués sur des URL différentes avec la bonne authentification HTTP.\n\nQuelques fonctions utilitaires sont aussi disponibles pour réaliser rapidement certaines vérifications récurrentes et ainsi faciliter le développement des tests. On peut, par exemple, rechercher aisément la présence d’erreurs ou warnings PHP dans une page. Il est aussi possible de faire un retry lorsqu’un test a échoué afin d’être certain que ce n’est pas une erreur du type “MySql server has gone away” qui peut se produire de temps en temps sur les serveurs de tests. Par ailleurs, quand un test échoue, VigoJS exporte une capture d’écran qui s’avère très pratique pour comprendre ce qu’il s’est passé !\n\nTous les paramètres ajoutés à la ligne de commande et dans la configuration sont injectés et accessibles dans la classe de test. On garde, de cette manière, une certaine flexibilité. Cela peut permettre, par exemple, de découper les tests avec de la pagination :\n\n\n\n\n\nAffichage dans le terminal\n\nNous avons aussi amélioré l’affichage des résultats des tests. Il est ainsi possible de préciser pour chaque test : un titre et une description personnalisés afin de rendre les comptes-rendus plus compréhensible pour les utilisateurs. De même des commentaires utilisateurs peuvent être ajoutés plus simplement dans le déroulement des tests.\n\n\n\nIntégration continue\n\nCasperJS génère nativement des rapports xUnit. VigoJS intègre donc cette fonctionnalité pour être utilisé sur une plateforme d’intégration continue comme Jenkins. Il est aussi possible de modifier le paramètre classPath dans le fichier xUnit pour améliorer la lisibilité des résultats :\n\n\n\nLe chemin dans lequel est généré le rapport est configurable par l’option –buildPath (ou dans la configuration) :\n\n\n\nIl suffit ensuite de configurer le job Jenkins pour qu’il récupère le rapport de test dans ce dossier. Sans oublier de faire un job pour tester les Pull Requests de votre projet.\n\nVigoJS est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Introduction à la qualité logicielle avec Node.js",
    "category" : "",
    "tags"     : " nodejs, javascript, qualite",
    "url"      : "/introduction-qualite-logicielle-avec-node-js",
    "date"     : "August 12, 2013",
    "excerpt"  : "\n\n(Source : https://www.flickr.com/photos/dieselbug2007/414348333/)\n\nChez M6Web, nous avons récemment réecrit un de nos projets Node.js.\nLe monde Node.js évolue très rapidement et a fait d’énorme progrès dans le domaine de la qualité logicielle.\nN...",
  "content"  : "\n\n(Source : https://www.flickr.com/photos/dieselbug2007/414348333/)\n\nChez M6Web, nous avons récemment réecrit un de nos projets Node.js.\nLe monde Node.js évolue très rapidement et a fait d’énorme progrès dans le domaine de la qualité logicielle.\nNous avons donc decidé de monter en qualité sur nos projets Node.js en utilisant les derniers outils proposés par la communauté.\n\nPour cela, nous mesurons maintenant différentes métriques sur nos projets Node:\n\n\n  la qualité du code (checkstyle)\n  des tests unitaires et fonctionnels\n\n\net tout ceci est lancé par notre serveur d’intégration continue: Jenkins.\n\nTests unitaires\n\nPour tout ce qui est “tests”, nous avons choisi le très bon duo :\n\n\n  Mocha\n  Chai\n\n\n\n\nMocha c’est un “test-runner” javascript qui fonctionne aussi bien sur Node que dans un navigateur web. Plus simplement mocha est l’outil qui va contenir nos tests: il va exécuter les tests et afficher les résultats.\n\n\n\nChai est une librairie d’assertion assez complète, permettant plusieurs syntaxe :\n\n\n  assert.equal(foo, ‘raoul’);\n  foo.should.equal(‘raoul’);\n  expect(foo).to.equal(‘bar’);\n\n\nCes deux outils fonctionnent aussi bien pour tester vos javascripts Node que front.\n\nCe duo permet une écriture de test simple et très lisible, dont voici un exemple :\n\n\n\nTests fonctionnels\n\nPour les tests fonctionnels, nous avons choisi d’utiliser Supertest, un package Node.js qui permet de simplifier l’écriture de requête HTTP (une surcouche au package http disponible dans Node.js).\n\nCi-dessous, un exemple de tests fonctionnels :\n\n\n\nCheckstyle\n\nEn javascript, on peut aussi écrire du code propre et respecter des conventions de codage.\n Afin de vérifier que notre code respecte les standards en vigueur, nous utilisons JsHint.\n\nIntégration continue\n\nToutes ces métriques sont récoltées grâce à Jenkins-CI à l’aide du fichier Ant suivant :\n\n\n\n\n\nLe résultat de l’intégration continue dans jenkins.\n\nConclusion\n\nNode.js propose des outils très performants pour la qualité logicielle, et écrire des tests avec le duo “Mocha + Chai” devient vite quelque chose de simple. Et même les développeurs les plus réfractaires aux tests devraient apprécier.\n\nN’hésitez pas à commenter cet article et à indiquez la solution que vous utilisez pour vos projets Node.\n\n"
} ,
  
  {
    "title"    : "Introduction à Drupal par Claire Roubey (Clever Age)",
    "category" : "",
    "tags"     : " lft, drupal, video",
    "url"      : "/introduction-%C3%A0-drupal-par-claire-roubey-clever-age",
    "date"     : "July 19, 2013",
    "excerpt"  : "Drupal, le CMS très très connu mais que nous on connait pas ! A notre demande Clever Age, par l’intermédiaire de Claire Roubey, est venue nous présenter cet outil lors d’un de nos fameux Last Friday Talk.\n\nMalheureusement, la vidéo est coupée à en...",
  "content"  : "Drupal, le CMS très très connu mais que nous on connait pas ! A notre demande Clever Age, par l’intermédiaire de Claire Roubey, est venue nous présenter cet outil lors d’un de nos fameux Last Friday Talk.\n\nMalheureusement, la vidéo est coupée à environ la moitié de sa durée (fort dommage car les questions étaient très intéressantes). Les slides sont toutefois disponibles : https://fr.slideshare.net/claire_/drupal-m6-web310513.\n\nUn énorme merci Clever Age et Claire !\n\nMalheureusement, la vidéo n’est plus disponible…\n"
} ,
  
  {
    "title"    : "Lâche moi la branch !",
    "category" : "",
    "tags"     : " qualite, jenkins, github",
    "url"      : "/lache-moi-la-branch",
    "date"     : "July 15, 2013",
    "excerpt"  : "Test continu des Pull Requests\n\nMaintenant que nous utilisons GitHub Enterprise chez M6Web, nous avons la joie de pouvoir utiliser les Pull Requests de façon abusive. Mais leur puissance n’est maximale que lorsqu’elles peuvent être testées individ...",
  "content"  : "Test continu des Pull Requests\n\nMaintenant que nous utilisons GitHub Enterprise chez M6Web, nous avons la joie de pouvoir utiliser les Pull Requests de façon abusive. Mais leur puissance n’est maximale que lorsqu’elles peuvent être testées individuellement avant d’être mergées sur le master.\n\n\n\nPour ce faire, nous avons utilisé le plugin GitHub Pull Request Builder de Jenkins, qui après une configuration assez simple, nous a permis de créer un job qui lance automatiquement un build lorsqu’une Pull Request est modifiée. Ce build se positionne sur la branch pointée par la Pull Request et exécute les tests.\n\n\n\nIl est donc nécessaire de créer un job dédié au test des Pull Requests pour chaque projet dont nous souhaitons voir les Pull Request automatiquement testées. Ça peut paraître évident, mais lorsqu’on a plus de 200 repositories, c’est tout de suite moins trivial.\n\nConfiguration du plugin\n\nLe fonctionnement par défaut du plugin GitHub Pull Request Builder est assez restrictif. Il nécessite qu’un contributeur ajoute un commentaire sur la Pull Request en demandant un test puis qu’un admin (parmi une liste à configurer) réponde avec un deuxième commentaire acceptant de lancer les tests (le tout avec des phrases types configurables). C’est uniquement ensuite que Jenkins lancera un build.\n\nOr dans notre contexte d’entreprise, nous souhaitons que l’automatisation soit totale, comme dans Travis : chaque modification d’une Pull Request lance l’ensemble des tests. Pour arriver ce fonctionnement, il suffit de cocher “Build every pull request automatically without asking (Dangerous!)” dans la section “Avancée” des options de lancement de build par “Github pull requests builder”.\n\nTest continu du master\n\nNous essayons tant que possible de suivre le workflow de déploiement de GitHub : on développe une fonctionnalité par branch, on fait une Pull Request sur le master et on ne merge que lorsque tout le monde est d’accord et que les tests sont passés. Cela nous permet de garder le master toujours déployable.\n\nNous avons donc, pour chaque projet, un second job qui lance l’ensemble des tests lors de chaque modification du master. Cela n’arrive normalement que lors du merge des nouvelles fonctionnalités contenues dans les Pull Requests, qui ont déjà été individuellement testées. Nous sommes donc sereins sur l’intégration croisée de toutes les nouvelles fonctionnalités sur le master.\n\nDéploiement\n\nAvant de déployer à l’aide de Capistrano, nous vérifions que les tests passent (résultat de l’intégration continue + lancement manuel des tests). Le manque d’automatisation concernant ces mises en production fait apparaitre une faille assez large. Pour la résorber, nous pourrions par exemple accepter le déploiement d’un service, uniquement si ses tests sont passés et si aucun autre n’est en cours ou en attente. Même si cela ajoute une dépendance aux serveurs d’intégration continue, cela sécurise les déploiements.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #5",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-5",
    "date"     : "July 12, 2013",
    "excerpt"  : "On est dredi, et on a encore pas mal de phrases chocs de nos développeurs, entendues dans nos locaux partager.\n\nEn voici une nouvelle sélection avec les Dev Facts #5\n\nFaille de sécu ?\n\n\n  C’est un peu complexe, c’est une back-office front !\n\n\nLa b...",
  "content"  : "On est dredi, et on a encore pas mal de phrases chocs de nos développeurs, entendues dans nos locaux partager.\n\nEn voici une nouvelle sélection avec les Dev Facts #5\n\nFaille de sécu ?\n\n\n  C’est un peu complexe, c’est une back-office front !\n\n\nLa boucle est bouclée\n\n\n  Il finit là où il s’est arrêté\n\n\nEnvoi impossible\n\n\n  Mais tu n’étais pas en pièce jointe !\n\n\nC’est l’histoire d’une fille\n\n\n  C’est un peu comme “Un gars, une fille” mais sans le gars\n\n\nLa minute de 30 secondes\n\n\n  J’ai pris une année sabatique de 6 mois.\n\n\nL’année des deux mains\n\n\n  Ca dépend si c’est une année ambidextre !\n\n\nLes dents de la neige\n\n\n  \n    Vous avez entendu, une championne de snowboard est morte.\n    Elle s’est faite manger par un requin ?\n  \n\n\nLe vrai du faux\n\n\n  Ils ont trouvé une faille infaillible\n\n\nLe roi de la combine\n\n\n  \n    En France, un truc comme ca, t’en as pour 280 euros !\n    Et tu l’as eu a combien ?\n    280 euros.\n  \n\n\nA bientôt pour un prochain épisode\n\n"
} ,
  
  {
    "title"    : "Benchmarking WebSockets avec NodeJs",
    "category" : "",
    "tags"     : " nodejs, websockets, benchmark, open-source",
    "url"      : "/benchmarking-websockets-avec-nodejs",
    "date"     : "July 5, 2013",
    "excerpt"  : "Nous avons récemment eu à repenser une application Node.js de timeline temps réel, basée sur les WebSockets afin de tenir une charge plus élevée.\n\nL’application timeline\n\nFonctionnellement, l’application timeline est relativement simple: elle cons...",
  "content"  : "Nous avons récemment eu à repenser une application Node.js de timeline temps réel, basée sur les WebSockets afin de tenir une charge plus élevée.\n\nL’application timeline\n\nFonctionnellement, l’application timeline est relativement simple: elle consiste à afficher un flux de message publiés par des contributeurs en temps réel pour les internautes présent sur la page. Pour cela l’application se base sur socket.io pour la partie websocket, et supporte à peu près 15 000 connexions simultanées.\n\nAfin d’augmenter la capacité de l’application, nous avons décidé de la rendre scalable horizontalement. C’est dire, répartir la charge sur un nombre X de serveurs communiquant entre eux, par exemple, par le biais de Redis.\n\n\n\nPour cela socket.io propose un store redis qui permet aux différentes instances de communiquer entre elles. Malheureusement les performances de ce store sont plutôt désastreuses car le store que propose socket.io est beaucoup trop verbeux et écrit absolument tous les évènements que reçoit un serveur sur un seul channel redis. L’application devenait inutilisable autour de 8 000 connexions. Il était donc inenvisageable de l’utiliser en production.\n\nNous avons donc décidé rapidement de passer une autre solution que socket.io. Après pas mal de recherche nous avons fait notre choix sur Faye, une implémentation du protocole de Bayeux, bien documenté et proposant aussi d’utiliser redis comme “store”. Après test, cette solution s’est révélée bien plus performante que socket.io.\n\nTests de charge\n\nUne des problématiques rapidement rencontrée sur ce projet a été de tester la charge de notre application: comment simuler 15 000 connexions simultanées ?\n\nEn faisant le tour des solutions de benchmark de websocket (thor, …) ,nous n’avons pas trouvé la solution qui nous permettait de faire les tests que nous souhaitions. Siege, ab ne le propose pas encore,Gatling, Jmeter, Tsung ont des plugins web-socket mais l’utilisation et le reporting ne sont pas des plus clair.\n\nLa solution ?\n\nWebsocket-bench\n\nNous avons donc décidé de développer notre propre outil de benchmark de websocket (Socket.io ou Faye), au nom très original : websocket-bench.\n\nCet outil se base sur les clients Node que proposent Faye et Socket.io. Il peut être facilement étendu à l’aide de “generator” (module Node), afin de rajouter la logique de votre application. Par exemple dans le cas de notre application, en se connectant, un client doit envoyer un message au serveur pour valider la connexion.\n\nCi dessous un exemple de générateur qu’on a pu utiliser lors de nos tests de charge.\n\n\n\nCet outil, lancé sur des instances Amazon, nous a permis d’exécuter nos tests de charge.\n\nUn exemple : la commande ci dessous va lancer 25 000 connexions, à raison de 1000 connexions par seconde en utilisant le generateur “generator.js” :\n\n\n\n\n\nNombre de clients connectés sur Graphite\n\nAu delà de 25 000 connexions, l’instance Amazon (large) qui lançait les tests ne tenait plus. Une solution pour tester un nombre plus élevés de connexions serait d’utiliser plusieurs machine de tests, peut être à l’aide de bees with machin guns et ainsi d’utiliser plusieurs instances pour lancer les tirs de charge.\n\nBonnes pratique de test de charge\n\nLors de votre test de charge (et pour la prod), n’oubliez pas d’augmenter le nombre maximal de descripteurs de fichiers coté client ET coté injecteur (ulimit -n 256000 par exemple dans la conf de supervisor, et dans le terminal avant de lancer le benchmark).\n\nSurveillez votre conntrack (si firewall iptables), augmentez votre plage locale de port, et si vous êtes amenés à tester plus de 25K connexions, utilisez plusieurs machines et/ou plusieurs IP sources différentes.\n\nComment contribuer au projet ?\n\nN’hésitez pas à remonter d’éventuels bug via les issues ou à contribuer au projet l’aide de pull request github (https://github.com/BedrockStreaming/websocket-bench)\n\n"
} ,
  
  {
    "title"    : "Performances web et &quot;Disaster case&quot; sur applications mobile native",
    "category" : "",
    "tags"     : " webperf, mobile",
    "url"      : "/performances-web-disaster-case-applications-mobile-native",
    "date"     : "July 2, 2013",
    "excerpt"  : "La performance Web (ou grossièrement temps de chargement) est devenue aujourd’hui une problématique majeure dans tout développement Web.\n\nLes outils pour mesurer / comprendre sont plutôt reconnus désormais et arrivent a une certaine maturité. Il y...",
  "content"  : "La performance Web (ou grossièrement temps de chargement) est devenue aujourd’hui une problématique majeure dans tout développement Web.\n\nLes outils pour mesurer / comprendre sont plutôt reconnus désormais et arrivent a une certaine maturité. Il y a toutefois encore un créneau plutôt peu documenté (à mon goût) dans le domaine, celui permettant de mesurer les temps de chargement dans des applications mobiles natives (Android / iOs …)\n\nVoici un retour des méthodes que nous utilisons pour mesurer les performances (notamment de chargement) de nos applications natives et générer des Waterfall Charts, mais aussi sur la mise en place de tests “disaster case” en cas d’indisponibilité de services utilisés par l’application.\n\nPour les besoins de ce tutoriel, nous allons prendre comme configuration, un Mac, avec une application native sur un iPhone 4 (relié au même réseau Wi-Fi que le Mac), ainsi que la version d’essai du logiciel CharlesProxy installé. (Mais la configuration et procédure est la même sur un autre OS, ou un autre mobile, et fonctionne aussi pour tester des webapps ou sites mobiles)\n\nCharlesProxy\n\nNous allons donc utiliser le logiciel payant CharlesProxy, qui est un proxy HTTP ou Reverse Proxy permettant de capturer le traffic HTTP de son ordinateur. Il existe une version d’essai sans limite de 30 jours. Il y a peut être des alternatives libres, mais Charles étant plutôt une référence, c’est l’outil que nous utilisons.\n\nCommencez donc par aller sur le site et installez CharlesProxy.\n\nUne fois installé, lancez le, il devrait automatiquement commencer à capturer le trafic réseau.\n\nConnexion Wi-Fi et récupération IP\n\nLa deuxième étape consiste à connecter votre Ordinateur et votre Téléphone sur le même réseau Wi-Fi.\n\nRécupérons ensuite notre adresse Ip via les “Préférences Système”, section “Internet et sans fil” et icone “Réseau” sur votre configuration Wi-Fi :\n\n\n\nConfiguration du proxy sur son iPhone\n\nPassons ensuite sur le téléphone, dans vos préférences Wi-Fi.\n\nCliquez ensuite sur la flèche bleu à droite du nom de la connexion sur le paramétrage Wi-Fi de notre iPhone, et descendre tout en bas du paramétrage pour configurer manuellement notre proxy HTTP :\n\nConfigurez le proxy de cette connexion pour passer par le Proxy Charles, avec l’adresse IP récupérée plus haut, et le port par défaut de Charles 8888.\n\nUne fois la connexion lancée avec le Proxy activé et Charles bien lancé sur votre Mac, une popup d’activation devrait apparaitre :\n\n\n\nAller ensuite sur un site mobile via Safari pour vérifier que le trafic est bien capturé par votre Proxy.\n\nA ce stade, tout est prêt pour commencer les mesures.\n\nPrise de mesure avec Charles\n\nPour prendre une mesure avec Charles, allez dans le menu “Proxy”, décochez le “MAC OS X Proxy” afin de ne pas parasiter vos mesures, et nettoyez l’écran de Charles pour commencer une “session” propre.\n\n\n\nVous n’avez ensuite plus qu’a lancer une application pour mesurer la liste des requêtes HTTP nécessaire à son démarrage.\n\nDans la partie Structure, un clic sur un domaine vous donnera plus d’infos (nombre de requête, et détails de chacune) …\n\nSélectionnez toutes les requêtes, puis cliquez sur “Chart” sur la droite, pour obtenir un premier Waterfall (made in Charles)\n\n\n\nGénération de Waterfall (plus complet)\n\nToujours sous Charles, avec toutes les structures sélectionnées, Fichier / Export puis selectionner le format Http Archive (.har)\n\nNous allons ensuite utiliser l’outil harviewer, pour visualiser le waterfall sous une forme plus complète que dans Charles.\n\nRendez vous ici (avec Firefox, plutôt que Chrome dont le rendu est buggé sur cet outil) : https://www.softwareishard.com/har/viewer/\n\nDécochez la case “Validate data before processing?” pour être moins embêté par des problèmes de compatibilité surement liés à l’export de Charles.\n\n\n\nEnsuite, faites un Drag &amp;amp; Drop de votre fichier .har dans le textarea de HarViewer pour obtenir votre waterfall, très proche de l’onglet Réseau de Firebug ou Network de la console de Chrome.\n\nVous retrouvez donc pour chaque requête tous les élements classique, avec détail des réponses, code HTTP de retour, taille etc, et le tout sur une timeline très précise.\n\n\n\nThrottling\n\nPour le moment, nous avons donc testé notre application sur notre connexion Wi-Fi, cas plutôt idéal. Mais comment simuler une connexion 3g par exemple, peut être plus proche de la réalité des utilisateurs de l’applications ?\n\nPour cela, il vous suffit d’aller dans Charles, puis le menu “Proxy” et “Throttle Settings”.\n\nLa latence par défaut configurée est un peu élevée (600ms), mais vous pouvez la modifier et affiner vos tests pour se rapprocher de conditions plus réelles.\n\nEnsuite, toujours dans le menu “Proxy”, activé l’option “Throttle” et vous pourrez tester sur une connexion différente.\n\n\n\nDisaster Case ?\n\nComment savoir comment se comporte votre application si vos Webservices sont injoignables ? ou si l’un des services tiers que vous utilisez est down ? Comment trouver les SPOF (Single Point Of Failure) de vos apps ?\n\nToujours dans Charles, Allez dans “Tools”, puis “Map Remote”.\n\nIci, vous allez pouvoir rediriger les domaines de vos choix, vers un domaine de type Blackhole.\n\nC’est à dire que le domaine choisi réagira comme si votre serveur web était dans un état de mort cérébrale ! Pas celui où il rejette la connexion immédiatement (trop facile), celui où il végète sans arriver à acquitter la réponse (le fameux “en attente de https:// ….”)\n\nPour ce besoin, nous allons utiliser le Blackhole fourni par Patrick Meenan pour l’outil de mesure de performance web : WebPageTest : https://blackhole.webpagetest.org\n\n\n\nVous pouvez ensuite jouer avec les domaines, et regarder comment se comporte votre application dans le cas où l’un d’entre eux est inaccessible.\n\nSur notre iPhone 4 de test, on remarque d’ailleurs un timeout sur les requêtes de 75 secondes ! Imaginez le cas, où le développement et l’appel à ce service est synchrone ? 75 secondes de loading dans votre application avant de passer aux requêtes suivantes …\n\n\n\nVoilà, vous avez désormais une solution vous permettant de générer des Waterfall Charts pour vos apps natives, et de tester des conditions de mauvaises connexions, ou d’indisponibilité de service.\n\nSi vous avez d’autres méthodes, plus simples ou plus complètes, ou tout autre remarque sur cette article, n’hésitez pas à le faire dans les commentaires ci-dessous.\n\nMerci.\n\nP.s: pour complément, n’hésitez pas à creuser le blogpost de Steve Souders sur les waterfall mobile, qui utilise une méthode très différente avec tcpdump et pcapperf https://www.stevesouders.com/blog/2013/03/26/mobile-waterfalls/\n\n"
} ,
  
  {
    "title"    : "Coke, pour bien sniffer son code",
    "category" : "",
    "tags"     : " outil, qualite, php, open-source",
    "url"      : "/coke-pour-bien-sniffer-son-code",
    "date"     : "June 27, 2013",
    "excerpt"  : "Afin d’uniformiser nos développements, nous avons décidé de suivre des conventions de code. Les projets deviennent ainsi plus homogènes et la revue de code, comme la maintenance, s’en trouvent simplifiées. Comme la majorité de nos services sont en...",
  "content"  : "Afin d’uniformiser nos développements, nous avons décidé de suivre des conventions de code. Les projets deviennent ainsi plus homogènes et la revue de code, comme la maintenance, s’en trouvent simplifiées. Comme la majorité de nos services sont en PHP, nous utilisons PHP CodeSniffer.\n\nLe manque\n\nCependant, l’éventail des frameworks utilisés en interne (Symfony, ZF, homemade) ne nous permet pas d’employer une seule et même convention. De plus, l’organisation des projets est assez hétérogène (ex: les répertoires de test ne se nomment pas tous de la même manière). Nous avions donc besoin de pouvoir configurer spécifiquement PHP CodeSniffer pour chacun de nos projets.\n\nLe deal\n\nA la manière de Travis, nous avons opté pour la méthode dite “du fichier .truc posé à la racine de chaque projet” (tm). Nous avons donc développé Coke, un script de sniff, qui lance PHP CodeSniffer avec la configuration contenu dans le fichier “.coke” la racine du projet :\n\n\n\nAinsi, lorsque le fichier est paramétré et que le script coke est correctement installé sur le système, il suffit d’exécuter la commande “coke” depuis la racine du projet sniffer.\n\nLe fix\n\nDans l’optique d’automatiser le plus possible nos processus, nous avons inséré la vérification des coding styles à l’aide de Coke, dans un hook git de pre-commit.\n\nCoke est disponible en open-source sur le compte GitHub de M6Web.\n\nEnjoy !\n"
} ,
  
  {
    "title"    : "Encodage - packaging - DRM - tout sur la vidéo",
    "category" : "",
    "tags"     : " video, codec, drm, lft",
    "url"      : "/encodage-packaging-drm-tout-sur-la-vid%C3%A9o",
    "date"     : "June 26, 2013",
    "excerpt"  : "\n\nUne nouvelle vidéo de l’année dernière provenant d’un Last Friday Talk.\n\nSouvent le monde de vidéo est source d’imprécision, cette vidéo met à plat l’ensemble des termes qui sont utilisés dans le domaine :\n\n\n  encodage, transcodage\n  packaging (...",
  "content"  : "\n\nUne nouvelle vidéo de l’année dernière provenant d’un Last Friday Talk.\n\nSouvent le monde de vidéo est source d’imprécision, cette vidéo met à plat l’ensemble des termes qui sont utilisés dans le domaine :\n\n\n  encodage, transcodage\n  packaging (transformation du conteneur vidéo)\n  DRM\n\n\nMalheureusement, la vidéo n’est plus disponible…\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #4",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-4",
    "date"     : "June 14, 2013",
    "excerpt"  : "Parceque c’est dredi et que ca nous fait toujours rire de partager les phrases chocs entendues dans nos bureaux, voici les Dev Facts #4.\n\nCaptain Obvious\n\n\n  \n    “Vous avez oubliez quelque chose ?”\n    “Le problème, c’est que quand t’oublie, t’y ...",
  "content"  : "Parceque c’est dredi et que ca nous fait toujours rire de partager les phrases chocs entendues dans nos bureaux, voici les Dev Facts #4.\n\nCaptain Obvious\n\n\n  \n    “Vous avez oubliez quelque chose ?”\n    “Le problème, c’est que quand t’oublie, t’y penses pas”\n  \n\n\nLa minute de 30 secondes\n\n\n  Si je gagne cette somme, je me prends 6 mois d’année sabatique\n\n\n$i++\n\n\n  Avec un ticket resto, tu peux manger 2, avec toute ta famille\n\n\n???\n\n\n  On t’a pas sonné les oreillettes\n\n\nla mémoire\n\n\n  La musique est mémorable, mais je m’en souviens plus\n\n\nAnonymous Proxy Land\n\n\n  Pour poster en anonyme, il faut être loggué.\n\n\nJésus multipliait les pains\n\n\n  On va dédupliquer les clics par quatre\n\n\nAbsent coupable!\n\n\n  “Dès que je ne suis pas là, j’ai toujours tord … :(“\n “Bein oui! Les innocents ont toujours tord”\n\n\nTransgiving\n\n\n  1 mec sur 3 qui regardent des pornos sur Internet sont des femmes\n\n\nFatal error never die\n\n\n  PHP : Fatal error: date() [function.date]: Timezone database is corrupt - this should never happen! in ///**/error.php on line 105\n\n\n"
} ,
  
  {
    "title"    : "Firewall applicatif PHP et bundle Symfony",
    "category" : "",
    "tags"     : " outil, php, symfony, open-source",
    "url"      : "/firewall-applicatif-php-et-bundle-symfony",
    "date"     : "May 30, 2013",
    "excerpt"  : "Nous publions aujourd’hui notre firewall applicatif sur notre compte GitHub. Il se compose :\n\n\n  d’un composant PHP (5.4+) gérant les IPs (V4 et V6), plages, wildcards, white/black lists, etc.\n  d’un bundle Symfony permettant d’utiliser le composa...",
  "content"  : "Nous publions aujourd’hui notre firewall applicatif sur notre compte GitHub. Il se compose :\n\n\n  d’un composant PHP (5.4+) gérant les IPs (V4 et V6), plages, wildcards, white/black lists, etc.\n  d’un bundle Symfony permettant d’utiliser le composant Firewall dans les controllers à l’aide des annotations et de retourner une réponse HTTP personnalisée.\n\n\nIls utilisent tous les deux Composer et sont disponibles sur Packagist.\n\nQu’est ce qu’un Firewall applicatif ?\n\nUn Firewall applicatif permet de restreindre l’accès de certaines IPs à certaines parties d’une application. Vous pouvez par exemple définir la liste des IPs autorisées dans la section d’administration ou au contraire celles que vous souhaitez bloquer dans un forum.\n\nPourquoi cette implémentation ?\n\nNous souhaitions éviter de redéfinir l’ensemble des IPs chaque point de restriction. Nous avons donc cherché centraliser la configuration. Le FirewallBundle permet de mettre en place des listes hiérarchisées ainsi que des configurations prédéfinies que nous pouvons réutiliser et adapter chaque besoin.\n\nComment contribuer ?\n\nSi notre firewall applicatif répond certaines de vos problématiques, mais que vous souhaitez le voir évoluer, n’hésitez pas participer son développement :\n\n\n  forkez les projets sur GitHub,\n  faites une branche par fonctionnalité,\n  proposez-nous vos évolutions et optimisations via les Pull Requests.\n\n\nVous pouvez également nous remonter les problèmes rencontrés lors de son utilisation dans les issues du composant ou les issues du bundle.\n\nEnfin, n’hésitez pas utiliser les commentaires de cet article pour nous faire part de vos réactions.\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #3",
    "category" : "",
    "tags"     : " devfacts, humour",
    "url"      : "/m6web-dev-facts-3",
    "date"     : "May 24, 2013",
    "excerpt"  : "Episode 3 des devfacts ! Parce qu’on ne s’en lasse pas.\n\nFort Boyaux\n\n\n  Cassandra tête de tigre !\n\n\nla dev même en prod\n\n\n  C’est un environnement de dev même en prod !\n\n\nPCF\n\n\n  \n    J’adore le prénom de Staline !\n    Sylvester ?\n  \n\n\nglory and ...",
  "content"  : "Episode 3 des devfacts ! Parce qu’on ne s’en lasse pas.\n\nFort Boyaux\n\n\n  Cassandra tête de tigre !\n\n\nla dev même en prod\n\n\n  C’est un environnement de dev même en prod !\n\n\nPCF\n\n\n  \n    J’adore le prénom de Staline !\n    Sylvester ?\n  \n\n\nglory and consequences\n\n\n  OH PUTAIN j’ai été RT par un participant de la belle &amp;amp; ses princes ! : jour de gloire :\n\n\nl’optimiste\n\n\n  C’est moins pire que rien\n\n\nle blagueur\n\n\n  T’as trois poussins sur une table ; comment tu fais pour en avoir plus que deux ? … T’en pousses un.\n\n\nI can haz root access !\n\n\n  \n    Tu peux me donner les accès MySQL ?\n    Ils sont en root.\n    Et ils arrivent quand ?\n  \n\n\nle jimmy cliff\n\n\n  I can see clearly now, the regex’s gone.\n\n\nla honte\n\n\n  \n    j’ai un peu honte de ce que je fais là …\n    quoi tu fais du javascript ?\n  \n\n\nl’aveuglement\n\n\n  Tu peux être valide w3c, l’aveugle, il verra toujours rien !\n\n\nle choix dans la date\n\n\n  Date de sortie (jj/dd/yyyy)\n\n"
} ,
  
  {
    "title"    : "Redis on fire !",
    "category" : "",
    "tags"     : " redis, nosql, lft, video",
    "url"      : "/redis-on-fire",
    "date"     : "May 22, 2013",
    "excerpt"  : "On continue la diffusion de quelques LFT triés sur le volet.\n\nCette fois ci c’est Kenny Dits qui s’y colle avec une présentation de Redis et des cas d’utilisation de cette technologie.\n\nMalheureusement, la vidéo n’est plus disponible…\n",
  "content"  : "On continue la diffusion de quelques LFT triés sur le volet.\n\nCette fois ci c’est Kenny Dits qui s’y colle avec une présentation de Redis et des cas d’utilisation de cette technologie.\n\nMalheureusement, la vidéo n’est plus disponible…\n"
} ,
  
  {
    "title"    : "CR Conférence Agora Cms du 15 mai 2013",
    "category" : "",
    "tags"     : " conference, cms",
    "url"      : "/cr-conference-agora-cms-du-15-mai-2013",
    "date"     : "May 20, 2013",
    "excerpt"  : "\n\nLe 15 mai 2013 avait lieu à Paris, la première édition de l’AgoraCMS, Conférence axée sur les CMS et la gestion de contenu Web.\n\nCette conférence est organisée par des acteurs importants du milieu, de chez Microsoft, Epitech, Oxalide, Cap Gemini...",
  "content"  : "\n\nLe 15 mai 2013 avait lieu à Paris, la première édition de l’AgoraCMS, Conférence axée sur les CMS et la gestion de contenu Web.\n\nCette conférence est organisée par des acteurs importants du milieu, de chez Microsoft, Epitech, Oxalide, Cap Gemini …\n\nAu rendez-vous, des sujets sur Drupal, Wordpress, le Responsive, les Réseaux Sociaux d’entreprise, et des retours d’expérience de différents acteurs français sur leurs utilisations de CMS public, “home made” ou propriétaire.\n\nLes CMS - écosystème - état des lieux et tendance, par Marine Soroko (CoreTechs) et Frédéric Bon (Clever Age)\n\nPremière conférence, et bonne introduction en la matière avec une présentation de la typologie des CMS.\n\nOn nous met en garde sur les “éditeurs de contenu” notamment, ou un CMS ne doit pas permettre de générer du contenu dans les pages. Un CMS de nos jours, doit permettre de gérer un référentiel de contenu, que nos pages doivent pouvoir requêter, sinon nous devenons vite confrontés a des problèmes de ré-usabilité.\n\nLes différents acteurs du marché sont présentés via les fameux “Quadrant magic” qu’on nous annonce finalement très loin de la réalité.\n\nOn finit sur une projection du futur des CMS qui devront répondre aux problématiques suivantes :\n\n\n  multi-canal (support tablette / site tiers etc)\n  multi-source\n  personnalisation\n  intégration e-commerce\n  Analyses et statistique\n  interactions et e-services\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nVotre CMS intelligent grâce l’analyse des logs, par Jérome Renard (Belogik)\n\nJérome Renard, ancien développeur EzPublish, a monté ces derniers mois une start-up proposant du Log As a Service, avec une solution du nom de Belogik, un peu comparable un Loggly.\n\nLa conférence présente l’intérêt d’analyser les logs (ici sortant d’un CMS, mais transposable tout site/service web) :\n\n\n  Incident de production\n  Service de la preuve\n  SEO\n  Performances\n  Gestion applicative\n  Sécurité\n  Développement\n\n\nMais aussi la difficulté à traiter des logs de formats différents, pas forcément disposition, quand vous ne maitrisez pas ou peu l’hébergement.\n\nPour la recherche dans ses logs, on a des solutions comme SolR ou les différents produits basés sur Lucene comme ElasticSearch (je rajouterais aussi le couple LogStash + Kibana utilisant aussi ElasticSearch)\n\nBref, une excellente présentation, dans la veine de celles que nous avions pu présenter chez m6web au niveau du Monitoring, un sujet très complémentaire avec le Logging.\n\nPour plus d’informations, belogik.com, ou sur leur compte twitter : @belogikCom.\n\nSinon vous pouvez consulter les slides sur le lien ci-dessous.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nVotre CMS intelligent grâce l’analyse des logs\n\nhttps://jrenard.info/talks/agoracms2013/\n\nTendances du design et nouveaux usages, par Patrick Maruejouls (Think Think)\n\nTrès bonne présentation sur l’importance du design dans le premier sens du terme : un outil pouvant et devant permettre de servir les intérêts stratégiques d’une entreprise. Il en découle qu’il serait bon de placer le design en amont des décisions et non en aval comme c’est souvent le cas.\n\nDe ses nombreuses expériences, nous pouvons retenir qu’une des plus importantes tendances pour les années venir est l’adaptation du design l’utilisateur. Ainsi, une grande enseigne de prêt-à-porter masculin a inséré une puce RFID dans l’étiquette de ses vêtements pour que l’ambiance des cabines d’essayage s’adapte aux vêtements essayés (ex: une petite musique des îles se déclenche lorsque le client essaye une chemise hawaïenne).\n\nDe même, cette tendance d’adaptation du design l’utilisateur pourra se voir magnifiée par la TV connectée et le second écran.\n\n\n\n\n\nLe second écran chez M6Web\n\nLes meilleurs thèmes et modules Drupal, par Dorian Marchan (Kernel 42) et Romain Jarraud (Trained People)\n\nPetite introduction Drupal, présenté comme un CMS, mais surtout un CMF (Content Management Framework), avec la présentation de quelques modules très intéressants pour des développeurs ou pour réaliser son “Usine à Site”.\n\nOn retiendra GCC dont une démo très intéressante sera faite, Drupal Commerce, étant une distribution de Drupal avec un assemblage de modules et personnalisation pour orienter son drupal vers le e-commerce, mais aussi la présentation d’un des thèmes les plus évolués de Drupal : Omega, thème ultra complet, Responsive avec un back-office assez puissant.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nLes Réseaux sociaux d’entreprise, par Edouard Ly (Oxalide) et Marine Soroko (Core Techs)\n\nTerme la mode depuis ces dernières années, les RSE commencent envahir les entreprises.\n Présentation (sans démo ou screenshot malheureusement), des différents acteurs du marché (très peu d’acteurs open source d’ailleurs :( ) :\n\n\n  Jive (leader du marché)\n  BlueKiwi\n  Telligent\n  BuddyPress : extension Wordpress\n  Yammer\n  Elgg\n  Chatter\n  Sharepoint\n  Liferay\n  Drupal commons\n  …\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nResponsive design : un site mobile en moins d’une heure, par Raphael Goetter (AlsaCreations)\n\nRaphael Gotter ( @goetter), le créateur d’AlsaCréations, est complètement incontournable pour tout ce qui touche l’intégration HTML, ou le RWD (Responsive Web Design) dans la communauté francophone, nous explique les approches à avoir pour réaliser un site RWD.\n Notamment que le Mobile First, paraît toujours être la bonne approche, ainsi qu’un rappel sur le fait que le RWD doit être pensé et prévu en amont.\n\nRaphael nous démontre aussi que son titre, et la réalisation d’un site RWD en moins d’une heure est infaisable. En prenant l’exemple du site d’Agora CMS, et en nous présentant l’approche pour transformer sa HP en Responsive.\n Plus de 15 jours de boulot au final, et des slides très intéressantes découvrir ci-dessous, remplies d’astuces, de checklists, et autres notions à bien comprendre avant de s’intéresser et de se lancer dans le RWD :\n\n\n  Comprendre les surfaces d’affichage\n  Connaître les Media Queries css 3\n  le Box-sizing\n  Halte aux débordements\n\n\nJe vous invite à tester sur un mobile Mobitest.me pour bien comprendre la différence entre viewport, largeur en pixel réelle, …\n\nPrésentation de la propriété hyphens aussi pour gérer les débordements de texte, coupler avec la propriété word-wrap, le framework / css de base Knacss.com, les tailles de typo en “rem”.\n\nBref, conférence très riche, drôle et a creuser impérativement pour tout ceux qui travaillent de près ou de loin sur ces problématiques.\n\nCompte rendu par Raphael lui même : https://blog.goetter.fr/post/50567713227/conference-un-site-responsive-en-une-heure avec le résultat voir ici : https://kiwi.gg/rg/agora/\n\nEt les slides ci-dessous :\n\n\n\n20 minutes: gérer le multi-canal, apps, web, devices, par Arnaud Limbourg (20 minutes)\n\nArnaud ( @arnaudlimbourg) a rapidement expliqué la stratégie mise en place par 20minutes pour permettre le multi-support : centralisation des données dans un référentiel accessible via une API permettant aux différents devices de se fournir en données, chacun leur manière.\n\nIl a ensuite donné quelques conseils afin de fournir une bonne API, en précisant qu’il était très difficile d’en réaliser une bonne :\n\n\n  architecture REST en HTTP,\n  faibles temps de réponse,\n  bon monitoring,\n  facilité d’apprentissage et d’utilisation,\n  concepts simples,\n  documentation.\n\n\nEnfin, Arnaud explique qu’il n’y a pas de solution miracle entre l’internalisation ou l’externalisation des développements (l’app Android a été développée en interne alors que le développement de l’app iOS a été externalisée).\n De la même manière, le choix entre le HTML5 et le code natif dépends des besoins et des ressources.\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nRetour d’expérience : France Télévisions, par Léo Poiroux (France Télévisions)\n\nNos confrères de chez France Télévisions nous présentent leur retour d’expérience sur leurs usines site, avec au micro, Léo ( @Leo_Px).\n\nAu départ, sur Spip, il nous explique pourquoi ils ont migré (douloureusement au début) sur Drupal, et qu’est ce que cela leur a apporté.\n\nUne conférence très transparente, drôle, agréable et intéressante.\n\nL’avenir de leur côté tend vers une transformation de l’usine à site, vers une “usine à interface” notamment pour gérer les multi-écrans, et une ouverture de leurs api/services vers de l’openApi, OpenData.\n\nFT c’est une très grosse DT (une centaine de personnes), une équipe d’expert transverse surnommée SWAT (composée d’Expert frontend JS/WebPerf, Expert Archi/Varnish, Expert Drupal, et de deux Coach Agile).\n\nD’autres excellentes idées comme leurs Dojo et Safari.\n Les Dojo sont des sessions d’une heure où les développeurs se relaient toutes les 5 minutes sur un même code, pour avancer un développement..\n Les safaris sont une sorte de “Vis ma vie” avec une journée en immersion dans une autre équipe de développement par exemple ou dans une équipe de journaliste utilisant l’un des sites qu’ils ont développés.\n\n\n  Pourquoi Drupal ? \n “La maison blanche utilise Drupal”\n\n\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\n\n\nTable Ronde. Quel modèle choisir : créer mon propre CMS ? utiliser un CMS existant ? OpenSource / propriétaire ?\n\nLa journée se termine sur une table ronde composée de différents responsables techniques :\n\n\n  Olivier Grange Labat – @ogrange DT @ Le Monde interactif : Olivier est en charge de la technique de Lemonde.fr. Il a fait le choix de développer son propre outil de gestion de contenu\n  Damien Cirotteau – @cirotix : Damien est CTO chez Rue89 qui utilise et est spécialisé en Drupal.\n  Olivier Fouqueau – DSI de la mairie d’Aulnay-sous-bois : Olivier est en charge des Systèmes d’informations et de l’innovation la Mairie d’Aulnay-sous-bois. Il fait le choix du CMS Ametys.\n  Galdric Pons – @hebiflux Chef de projet digital @ BNP Paribas : Galdric est Chef de projet digital au pôle innovation de BNP Paribas ou il a mis en place une usine site avec WordPress.\n\n\nChacun des participants au cours d’une interview animée par Cyril Pierre de Geyer ( @cyrilpdg) , va expliquer son choix de CMS, les avantages et les inconvénients et donner de précieux conseils aux personnes dans la même situation.\n\n\n(Source: https://photos.silberman.fr/Other/AgoraCMS/ )\n\nConclusion\n\nUne journée haute en couleur, avec de très bonnes conférences et des retours d’XP toujours aussi intéressants.\n On salue une organisation impeccable et le tout pour un prix très accessible (20€).\n\nCe CR ne rend-compte finalement que d’une mince partie des conférences (4 conférences en parallèle pour cause), mais vous pouvez retrouver d’autres slides sur le site officiel de l’évenement.\n\nN’hésitez pas commenter ce CR si vous avez des remarques ;-)\n\nP.s: Merci @nsilberman pour les photos que vous pouvez retrouver en intégralité ici : https://photos.silberman.fr/Other/AgoraCMS/\n"
} ,
  
  {
    "title"    : "La POO Canada Dry",
    "category" : "",
    "tags"     : " php, poo, lft, video",
    "url"      : "/la-poo-canada-dry",
    "date"     : "May 6, 2013",
    "excerpt"  : "Nous vous avions parlé, il y’a quelques mois, de nos conférences interne, les Last Friday Talk.\n\nVoici une première vidéo de l’une de ces sessions sur “la POO Canada Dry”.\n\nLa POO (Programmation Orienté Objet) ne consiste pas à mettre du code dans...",
  "content"  : "Nous vous avions parlé, il y’a quelques mois, de nos conférences interne, les Last Friday Talk.\n\nVoici une première vidéo de l’une de ces sessions sur “la POO Canada Dry”.\n\nLa POO (Programmation Orienté Objet) ne consiste pas à mettre du code dans des classes, elle fait appel à des concepts vitaux pour le développeur moderne. Olivier nous présente quelques mauvais exemples tirés de code legacy et quelques bonnes pratiques pour faire de la POO (mais en fait c’est surtout du troll).\n\nMalheureusement, la vidéo n’est plus disponible…\n"
} ,
  
  {
    "title"    : "CR Real Time Conférence Europe 2013 - Day 2",
    "category" : "",
    "tags"     : " conference, nodejs, realtime",
    "url"      : "/cr-real-time-conference-europe-2013-day-2",
    "date"     : "April 26, 2013",
    "excerpt"  : "\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8679275805/in/set-72157633306379029\n\nAprès la première journée, on continue avec la deuxième journée de conférence. Toujours sur le format de 20 minutes pour présenter le sujet.\n\nWOOT, Arial B...",
  "content"  : "\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8679275805/in/set-72157633306379029\n\nAprès la première journée, on continue avec la deuxième journée de conférence. Toujours sur le format de 20 minutes pour présenter le sujet.\n\nWOOT, Arial Balkan\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326600103475425281/photo/1\n\n@aral a travaillé sur une solution d’édition partagée de contenu, une solution sans transformation opérationnelle (OT sur Wikipedia) : WOOT qui signifie Without Operational Transformation.\n\nLe concept exposé est de ne pas faire de suppression des caractères d’une chaîne, mais plutôt de travailler sur la notion de visible / invisible et de position du caractère au sein de la chaîne. L’objectif étant de garder la convergence et de préserver les intentions des utilisateurs.\n\nIl nous indique différentes ressources pour approfondir le sujet :\n\n\n  une vidéo de Google Tech Talk :Issues and Experiences in Designing Real-time Collaborative Editing Systems\n  une librairie JS : ShareJS\n  pour aller plus loin dans la gestion du travail collaboratif, l’Inria rend disponible différents travaux de recherches sur le sujet\n\n\nConvention-Driven JSON, Steve Klabnik\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326607298556489728/photo/1\n\n@steveklabnik avait déjà parlé la veille sur un autre sujet. Aujourd’hui, il nous expose la problématique de passer des objets (quelque soit le langage) JSON. Bien souvent, on utilise JSON pour la communication entre différents services (l’un en PHP et l’autre en Python par ex, ou deux services en PHP).\n\nPour un site web, on arrive souvent au résultat suivant :\n\nObjet -&amp;gt; Template -&amp;gt; HTML\n\nPour éviter certains problèmes lors des échanges de données, il propose par exemple en ruby d’utiliser active_model_serializers ce qui permet d’obtenir le résultat suivant :\n\nObjects -&amp;gt; Serializer -&amp;gt; HTML\n\nEn résumé, il recommande de passer par un outil de serialisation des données afin de ne pas perdre la structure de l’objet et donc d’avoir une plus grande réactivité entre le client et le serveur.\n\nRealtime vs Real world, Tyler Mac Mullen\n\n@tbmcmullen travaille pour Fastly. Sa société propose des solutions d’optimisation au sein des infrastructures de type CDN.\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326613272969228288/photo/1\n\nIl commence sa présentation en définissant les deux termes :\n\n\n  Realtime = réduire la latence\n  Realworld = notion d’infrastructure\n\n\nIl indique également l’impossibilité de construire des infrastructures en temps-réel. Seul les CDNs ont la possibilité de s’approcher du temps-réel. La notion de purge est également essentielle.\n\nTyler présente ensuite 3 possibilités de purge :\n\n\n  Utilisation de Rsyslog : soit via TCP (problème : lenteur), soit via UDP (problème : pas de retour d’erreur). Un noeud notifie tous les autres.\n  Love triangle : pas de serveur central mais notion de peer-to-peer. Chaque noeud interagit avec 2/3 autres noeuds. Problème : il n’y a pas d’état global ni de possibilité de scalabilité avec ce type d’infrastructure\n  Hybride : on met en place des switchs au niveau des noeuds. Les switchs interagissent entre eux, puis redistribuent l’information au niveau de ces serveurs.\n\n\nLa société a déjà fait d’autres sessions lors d’autres conférences tel que Velocity qui pourrait fortement intéressé les adminsys ;-)\n\nDiscoRank : optimizing discoverability on SoundCloud, par Amélie Anglade\n\n\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8680450402/in/set-72157633306379029\n\n\n\n@utstikkar est une française qui travaille pour Soundcloud en tant que MIR Software Engineer.\n\nElle nous a expliqué l’évolution effectuée au sein de leur moteur de recherche : Discorank. Ce système peut être assimilé au PageRank de Google.\n\nIls utilisent pour cela : MySQL puis HDFS et enfin tout est re-manipulé dans ElasticSearch\n\nBuddyCloud - Rethinking Social, par Simon Tennant\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326635400175161344/photo/1\n\nSimon Tennant est CEO de la société BuddyCloud. Il a tenté de faire passer les informations suivantes :\n\n\n  fédérer ou mourir\n  travailler sur les protocoles non sur les APIs\n  pour construire du social dans un produit, il recommande de se baser sur l’open source, les standards et protocoles ouverts\n\n\nL’objectif de sa société est de permettre aux personnes de construire un réseau social fédéré et bien entendu temps-réel.\n\nN’hésitez pas à fouiller dans leur source sur Github) qui fourmille de fonctionnalités.\n\nRealtime at Microsoft, Pierre Couzy\n\nPierre Couzy travaille depuis plus de 10 ans chez Microsoft. Il nous présente un projet réalisé par des développeurs US : SerialR (Github). Le projet utilise les Websockets.\n\n\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8679336549/in/set-72157633306379029\n\nLe projet possède deux dépendances principales :\n\n\n  Json.net côté serveur\n  Jquery côté client\n\n\nIl est noter que la négociation exacte entre le client et le serveur dépend du navigateur utilisé.\n\nA noter que cette présentation est l’une des rares qui n’a pas été réalisée avec un MacBook ;-)\n\nLearning from Past Mistakes, a new node http layer, par Tim Caswell\n\n@creationix était un des anciens core dev de NodeJS. Il nous expose les différents points cause desquels il a quitté le projet.\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326670519371980800/photo/1\n\nUn des autres problèmes avec NodeJS est que le changement est difficile :\n\n\n  utilisé en production par différentes sociétés\n  difficulté à modifier les APIs\n\n\nIl a donc développé Luvit basé sur la technologie Lua (légère, rapide et permettant les co-routines).\n Cette nouvelle couche HTTP donne la possibilité :\n\n\n  de suspendre et de reprendre la fibre actuelle\n  lorsque l’on a des fibres on peut faire d’autres choses\n  écrire sur les objets stream avec .write(item)\n  lire sur les objets stream avec .read()\n  de terminer un stream avec false item\n\n\nVous pouvez retrouver l’ensemble des slides sur Github\n\nHTTP Proxy, par Nuno Job\n\n\n\nCrédit : https://www.flickr.com/photos/andyet-photos/8680421388/in/set-72157633306379029\n\n@dscape nous propose un bon article sur le load balancing avec nodejs.\n Pour le speaker nodejs c’est : net protocols &amp;amp;&amp;amp; libuv &amp;amp;&amp;amp; v8 &amp;amp;&amp;amp; npm\n\nVous pourrez retrouver l’ensemble des slides sur Github\n\nLearning How To Let Go, par Kyle Drake\n\n@kyledrake introduit d’autres solutions en remplacement de JSON : basés sur des données en binaire.\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326684373090963456/photo/1\n\nCependant, tout le monde n’utilise pas correctement les échanges binaires. Le speaker nous fait un très bon résumé de la situation pour effectuer des pushs sur la plateforme d’Apple (Apple Push Notification Service) et des types de retours effectués par Apple. Ceci résume assez bien la situation.\n\n\n\nCrédit : https://twitter.com/noel_olivier/status/326686457257406464\n\nJS ne propose pas d’API “native” mais un projet permet de traiter du binaire : binaryjs (du binaire via websockets).\n\nKyle effectue différents benchs sur la taille des contenus envoyés : l’un en JSON, l’autre en Binary JSON et le dernier via MessagePack. Bien entendu, ce sont les contenus en binaire qui sont les plus légers, mais il reste voir l’impact du téléchargement du JS associé et du traitement côté client.\n\nUn sujet donc à étudier qui rappelle AMF pour échanger des informations au format binaire entre PHP et Flash.\n\nSecuring socket applications, par James Coglan\n\nDans un premier temps, @jcoglan nous indique que la sécurité c’est difficile et que cela concerne :\n\n\n  l’authentification\n  la vie privée\n  les XSS\n  les CSRF\n\n\nPour répondre aux différentes problématiques, il nous présente Faye un système simple de message pub/sub pour le web. Ses slides sont disponibles.\n\nReal-time design, par Jan-Christoph Borchardt\n\n\n\nCrédit : https://twitter.com/OriPekelman/status/326711206209527809/photo/1\n\n@jancborchardt n’est pas un développeur, mais il nous rappelle quelques concepts importants :\n\n\n  Plus l’utilisateur s’ennuie, plus la confusion augmente\n  Sous 0,1 ms, l’utilisateur considère cela comme du temps réel\n  Attention aux transitions\n  Ne pas tuer la fluidité\n  “Interruptification” (exemple flagrant sur l’image ci-dessous)\n  Pas de notifications pendant l’utilisation (ex batterie faible 20% sur un mobile)\n\n\n\n\nEn résumé, l’interface et le design sont importants également pour s’approcher d’une expérience utilisateur temps-réel.\n\nFin de la seconde journée\n\nPour les plus courageux, le livestream est également disponible.\n\n\n\nCet événement était très intéressant :\n\n\n  par son avance de phase, la majorité des présentations correspondaient des résultats de R&amp;amp;D, voire d’innovation.\n  par l’ambiance\n  par le networking que l’on pouvait y faire\n\n\nOn peut en revanche peut être un peu regretter le nombre de français (la fois côté speaker et également côté public).\n\nPour terminer, un grand merci Julien Genestoux qui a organisé l’événement.\n\nRendez-vous pour la prochaine édition.\n"
} ,
  
  {
    "title"    : "CR Real Time Conférence Europe 2013 - Day 1",
    "category" : "",
    "tags"     : " conference, nodejs, zeromq, rabbitmq, realtime",
    "url"      : "/cr-real-time-conference-europe-2013-day-1",
    "date"     : "April 25, 2013",
    "excerpt"  : "Les 22 et 23 Avril 2013, ont eu lieu, la Real Time Conférence en version Européenne.\n\nPour cette première édition, les festivités se déroulaient à Lyon, à la Plateforme, une péniche posée sur les quais du Rhône très sympathique.\n\nPassé l’accueil “...",
  "content"  : "Les 22 et 23 Avril 2013, ont eu lieu, la Real Time Conférence en version Européenne.\n\nPour cette première édition, les festivités se déroulaient à Lyon, à la Plateforme, une péniche posée sur les quais du Rhône très sympathique.\n\nPassé l’accueil “la Titanic” avec l’orchestre dans le hall d’entrée, nous descendons au sous-sol pour commencer suivre la première journée de conférence, qui s’annonce déjà très chargée.\n\n\n\nLa vue de la salle de conférence (Crédit : https://twitter.com/frescosecco/status/326302218515017729/photo/1 )\n\nWebSuckets, par Arnout Kazemier\n\nPremière conférence autour des Websockets et des bugs ou difficultés d’implémentation que l’on peut rencontrer.\n\nOn parle notamment de Firefox qui en prend pour son grade : Si l’on appuie sur ESC après que la page soit chargée, toutes les connexions sont fermées … Firefox ne peut pas se connecter non plus sur une Websocket non sécurisée en HTTPS.\n\nCoté Safari Mobile, écrire dans une Websocket fermée plante votre téléphone, et cela arrive quand on revient sur un onglet qui utilisait des Websocket, ou lorsque l’on réouvre un safari précédemment réduit.\n\nBref, en gros, ca démotive un petit peu sur l’utilisation des Websockets !\n\nArnout ( @3rdEden ) déconseille aussi l’utilisation des Websocket sur mobile, et indique de ne les utiliser que quand c’est vraiment nécessaire sur desktop.\n\nQuelques présentations d’outillages :\n\n\n  HA Proxy\n  HTTP-Proxy\n  Nginx-devel\n\n\nVous pouvez retrouver une battle sur les perfs de ces proxys ici : github.com/observing/balancerbattle\n\nOn aborde aussi les problématiques de tirs de charge sur les Websocket avec :\n\n\n  wsbench\n  websocketbenchmark\n\n\nLes deux étant, d’après Arnout, incomplets ou dépassés …\n\nIl a donc développé son propre outil : Thor, “smasher of Websockets” à tester de toute urgence : https://github.com/observing/thor\n\nLes frameworks mentionnés pour en simplifier l’implémentation :\n\n\n  Faye\n  Signalr\n  xsockets\n  sockjs\n  socket.io\n\n\nAttention aussi aux éléments perturbateurs : firewall, extensions de browsers, antivirus, ou proxy qui peuvent bloquer les ports utilisés par les Websockets.\n\nBref, une première entrée en matière très complète et intéressante qui couvre vraiment toute la partie moins glamour des Websockets.\n\nJe vous invite aussi à consulter son blog si le sujet vous intéresse : https://blog.3rd-eden.com/\n\n\n\n(Crédit : https://twitter.com/hintjens/status/326243158109347841/photo/1 )\n\n\n\nSocketStream 0.4, par Owen Barnes\n\nVoici l’un des frameworks pour l’implémentation des Websockets, où son créateur ( @socketstream ) nous partagé ses idées de la conception et de l’utilisation d’un framework : découplage, simplicité, modularité etc.\n\nLe framework est “Transport Agnostics” et peut donc utiliser sockJs, Engine.io, ou Websockets native juste en changeant une simple ligne.\n\nLe FW est basé sur Prism, un module de serveur realtime, lui aussi open-sourcé sur github.com/socketstream/prism.\n\nLa 0.4 présentée est en cours de finalisation, et sera disponible prochainement en version finale sur le github https://github.com/socketstream/\n\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8672407195/in/set-72157633306379029/ )\n\nXSockets, par Magnus Thor\n\nMagnus ( @dathor ), nous présente son framework Xsockets pour l’utilisation des Websockets avec une démo “live coding” peut-être intéressante, mais tentée “online” et avec une connexion bien foireuse (comme dans toutes les conférences techniques, non ?) …\n\nBref, un peu douloureux à regarder, mais la démo avait l’air d’avoir du potentiel : une application web utilisant WebRPC pour partager en mode Peer To Peer la Webcam de l’utilisateur.\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673638532/in/set-72157633306379029/ )\n\nZeroMQ as scriptable sockets, par Lourens Naudé (Bear Metal)\n\nLourens ( @methodmissing) est l’un des “co-maintainer” de ZeroMq.\n\nIl nous présente ZeroMq comme une solution de messagerie instantanée pour les apps. Ca n’est pas un serveur, ni un broker, mais une librairie sur la communication et gestion de la concurrence.\n\nOn parcourt ensuite les différents types de sockets supportés :\n\n\n  Req / Rep\n  Pub / Sub\n  Push / Pull\n\n\nVoir la présentation ci dessous :\n\n\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8672695927/in/set-72157633306379029/ )\n\nWebRTC, par Sam Dutton (Google)\n\nSam Dutton, Developper Advocate chez Google ( @sw12 ), qu’on à déja vu/entendu par le passé à la Vélocity Conf (voir précédent CR) nous parle de WebRPC.\n\nOn parcourt les différentes API disponible, le support des navigateurs (Chrome, Firefox Nightly et IE Chrome Frame …).\n\nOn découvre ensuite de nombreuses démos très sympa :\n\n\n  Ascii Caméra\n  GetUserMedia\n  Webcam Toy\n  Magic Xylophone\n  Screen Capture (nécessite Chrome Canary)\n  …\n\n\nPour débugger plus facilement, utilisez le chrome://webrtc-internals\n\nLibs, apps et frameworks pour XML RPC :\n\n\n  easyRTC : full stack\n  conversat.io built with SimpleWebRTC\n  PeerJS : API abstraction\n  webRTC.io\n  Sharefest\n\n\nPlus d’infos/codes ou démos sur les slides : https://samdutton.net/realtime2013/\n\n\n  “WebRTC and HTML5 could enable the same transformation for real-time communications that the original browser did for information.” Phil Edholm / Nojitter\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673907134/in/set-72157633306379029 )\n\nEnemy of the state : An introduction to functional reactive programming with Bacon.Js, par Phiilip Roberts (Float)\n\n\n\nL’une des présentations les plus intéressantes de la journée par Philip Roberts ( @philip_roberts ), CTO et co-founder de Float avec l’introduction Bacon.Js et la “Functional Reactive Programming” en Javascript. Une façon différente de coder pour éviter les “callback hell” notamment.\n\nLe projet répond aussi à une problématique très courante des dév JS, avec l’exemple du “Check Username Availibility” qui lance une requête Ajax chaque KeyPress et dont l’ordre n’est pas maitrisé. (partir de la slide 38)\n\nBacon.Js est dispo sur Github : https://github.com/raimohanska/bacon.js\n\nP.s: la visualisation des streams sur ses slides était très sympa : https://latentflip.com/bacon-examples/\n\nPlus d’infos sur les slides : https://latentflip.com/bacon-talk-realtimeconfeu/\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673684632/in/set-72157633306379029 )\n\nQuick Wins with Redis for your website, par Cathering Jung\n\nCatherine Jung ( @bufferine ) travaille sur des services de paris en ligne. Elle explique les problématiques de temps réel qu’elle doit affronter, et comment Redis lui permet de mieux supporter la charge.\n\nAu final, on parle un peu plus de Scala que de Redis, mais tout retour d’expérience est toujours bon prendre.\n\nRetrouvez les slides ici :\n\nhttps://docs.google.com/file/d/0By6ZH5wplIR-MzgyOEJCMEkyWmc/edit?usp=sharing \n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673926016/in/set-72157633306379029/ )\n\nRealtime and Go : Leaving the frameworks behind, par Paddy Foran\n\nPaddy ( @paddyforan ) nous présente le language Go.\n\nA la question : “What is Go ?” la réponse est :\n\n\n  A better C, from the guys that didn’t bring you C++\n\n\nhttps://goonaboat.com/\n\nBref, Go c’est :\n\n\n  Compiled\n  Static typed\n  Fast\n  Elegant\n  Concurrent\n\n\nLes slides sont disponibles ici : https://goonaboat.com/ et le code de la présentation : https://github.com/paddyforan/goonaboat\n\nPlus d’infos sur le langage ici : https://golang.org/ avec un “Tour” qui parait très bien fait : https://tour.golang.org/#1\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673939104/in/set-72157633306379029 )\n\nCloud Messaging with Node.Js and RabbitMQ, par Alvaro Videla\n\nAlvaro ( @oldsound ) est le co-auteur de “Rabbit Mq In action”.\n\nIl a présenté l’intérêt d’utiliser un rabbitMQ dans un projet qui est un fork d’Instagram, mais Real Time : CloudStagram, sur une stack “Cloud Foundy”, Rabbit MQ, Redis, MongoDB et SockJS\n\nNotamment le concept de tout gérer via événement (slide 54 ci-dessous).\n\nBref, pas mal de bonnes idées à retenir et pas mal de projets intéressants sur son github : https://github.com/videlalvaro , comme le RabbitMqSimulator pour présenter clairement le fonctionnement des RabbitMQ\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8673976982/in/set-72157633306379029/ )\n\n\n\nOffline first!, par Jan Lehnardt\n\nJan ( @janl ) bosse sur CouchDb. Apache CouchDB est une base de données de type document basée sur le format JSON et utilisant Javascript (notamment pour les MapReduce).\n\nIl commence sa présentation par un “You are all doing it wrong !”. En réexpliquant que le réseau est toujours rapide, mais que c’est la latence qui est problématique. (Voir l’excellent article de 2010 de @edasfr sur le sujet toujours aussi pertinent ).\n\nIl faut aujourd’hui travailler Offline First ! (un peu l’équivalent d’un Mobile First coté apps), et prend pas mal d’exemples de bonne ou mauvaise implémentation (de la gestion hors connexion, du passage dans un tunnel, en se moquant de la mauvaise couverture française dans le TGV).\n\nOn aborde ensuite les :\n\n\n  CouchDB\n  PouchDB : Javascript database that syncs!\n  TouchDB : CouchDB-compatible embeddable database engine for mobile &amp;amp; desktop apps\n\n\net la présentation du framework Hoodie : https://hood.ie/, basé sur le Offline par défaut.\n\n\n  “Think of CouchDB as Git for your application data” Jan Lehnardt\n\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8672884965/in/set-72157633306379029/ )\n\nBuilding Realtime HTML5 apps for Android and Firefox OS, par Anthony Ricaud\n\nPrésentation par Anthony Ricaud ( @rik24d ) des fonctionnalités HTML5 implémentées par les équipes de Mozilla, notamment pour connaitre l’état de la batterie, l’orientation, la gestion des apps supportant la sélection de photos par exemple…\n\nChaque site peut être une apps, à condition de mettre les lignes nécessaires dans un fichier manifest. Beaucoup de débats aussi autour des systèmes fermés de MarketPlace.\n\nPlus d’infos dans les slides ci dessous :\n\n\n\n(Crédit : https://www.flickr.com/photos/andyet-photos/8672894401/in/set-72157633306379029 )\n\n\n\nNodeCopter + Hackathon\n\nJe laisse la parole à Olivier Mansour pour la présentation du NodeCopter :\n\nRomain Huet ( @romainhuet ) nous a présenté et fait une petite démonstration du pilotage d’un AR Drone avec NodeJs.\n\nIssu du projet nodecopter https://nodecopter.com/, un ensemble de librairies Node.js est disponible et rend le pilotage du drone complètement accessible. Mouvements en vol, stream de la caméra, Romain nous a fait une démonstration fun et captivante de l’engin.\n\nEt faire voler un drone dans un bateau … on avait jamais vu ça !\n\nFin de la 1ère journée :\n\nUne première journée très sympathique, bourrée d’idées et d’outils en tout genre. L’organisation est vraiment au poil, et on repart en voulant refaire le monde techniquement :-)\n\nLe compte rendu de la deuxième journée est ici : https://tech.bedrockstreaming.com/cr-real-time-conference-europe-2013-day-2\n\nP.s : Merci &amp;amp;Yet pour la plupart des photos présentes ici : https://www.flickr.com/photos/andyet-photos/sets/72157633306379029/\n\nPour les plus motivés, la conférence été enregistrée en vidéo :\n\n\nVidéo de la première journée de la RealTime Conf Europe\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #2",
    "category" : "",
    "tags"     : " humour, devfacts",
    "url"      : "/m6web-dev-facts-2",
    "date"     : "April 9, 2013",
    "excerpt"  : "On continue la série des Dév Facts, phrases oh combien cultes prononcées par nos chers développeurs lors d’oublis cérébraux :-)\n\nPour ceux qui avaient raté la première série, c’est ici : https://tech.bedrockstreaming.com/m6web-dev-facts-1\n\nEnjoy\n\n...",
  "content"  : "On continue la série des Dév Facts, phrases oh combien cultes prononcées par nos chers développeurs lors d’oublis cérébraux :-)\n\nPour ceux qui avaient raté la première série, c’est ici : https://tech.bedrockstreaming.com/m6web-dev-facts-1\n\nEnjoy\n\nCaptain Obvious\n\n\n  Chef de Projet : “Vous avez oublié quelque chose ?”\n\n  Développeur : “Le probléme c’est que quand t’oublie, t’y pense pas”\n\n\nPaye ta culture\n\n\n  C’est une citation de la bible … et de Civilisation IV\n\n\nSeigneur non !\n\n\n  \n    Avant j’encodais les vidéos pour l’émission “Le jour du Seigneur”\n    C’est quoi ? une émission porno ?\n    Ca dépend comment t’écris “seigneur”\n  \n\n\nLes congés du fantastique\n\n\n  Le 4, je suis en RPG … euh RTT\n\n\nDouble compétence …\n\n\n  Je viens de recevoir le CV d’un gars, il a fait une formation “Maîtrise en patisserie” … il doit maîtriser les cookies non ? :)\n\n\nIncomparable pour ne rien comparer\n\n\n  \n    Ah bon ? C’est les TCL qui font le plus souvent grève en France ? Plus que la SNCF ?\n    Oui, à titre de comparaison, je crois que c’est incomparable …\n  \n\n\nRetour vers le futur\n\n\n  Donc vous savez que je ne suis pas là la semaine dernière\n\n\nTrouvé !\n\n\n  Je viens de trouver une découverte !\n\n\nLa preuve par dix\n\n\n  On a doublé la bande passante par dix\n\n\nLe flegme illustré\n\n\n  De toute facon, y’a pas de conséquence : au pire, on meurt.\n\n"
} ,
  
  {
    "title"    : "M6Web Dev Facts #1",
    "category" : "",
    "tags"     : " humour, devfacts",
    "url"      : "/m6web-dev-facts-1",
    "date"     : "March 21, 2013",
    "excerpt"  : "Depuis de nombreuses années, toutes les “phrases chocs” dites par les équipes techniques de M6Web sont archivées, loggées, historisées. Pas moyen de sortir une ânerie sans qu’elle soit gravée dans le marbre.\n\nDu coup, nous avons décidé de profiter...",
  "content"  : "Depuis de nombreuses années, toutes les “phrases chocs” dites par les équipes techniques de M6Web sont archivées, loggées, historisées. Pas moyen de sortir une ânerie sans qu’elle soit gravée dans le marbre.\n\nDu coup, nous avons décidé de profiter de cette période très calme sur le blog pour vous faire partager une petite sélection en vrac de quelques Dev Facts entendus dans les locaux d’M6Web Lyon :\n\ninvalider le cache, nommer les choses …\n\n\n  \n    On purge le cache infini tous les jours.\n    Non c’est le cache 7 jours qu’on purge tous les jours !\n  \n\n\nJ’ajoute -2\n\n\n  J’ai réduit de fois 10\n\n\nwork - not - flow\n\n\n  On a des workflows, on a aussi des worknotflows\n\n\nLa bonne affaire\n\n\n  Pour le même prix, t’as bien moins cher ailleurs !\n\n\nEt pour arrêter ? tu cliques sur démarrer !\n\n\n  La vidéo a été mise en erreur avec succès.\n\n\nBig or what ?\n\n\n  Il y a une librairie Rennes, elle est énorme! Tu y rentres, c’est tout petit… !\n\n\nNiveau CE2\n\n\n  je fais un mail de réponse avec une explication niveau CE2\n\n\nBref\n\n\n  \n    \n      J’ai demandé Pierre, qui m’a dit qu’il ne savait pas mais que si j’avais l’info, il la voulait bien. Du coup, j’ai demandé Kenny, qui m’a dit de demander a Antony… qui ne savait pas et m’a dit de demander Pierre\n    \n    \n      Bref, j’ai posé une question un admin\n    \n  \n\n\nCDP Junior\n\n\n  Raa mais chui un cdp junior moi je sais rien faire de mes 10 doigts :(\n\n\nLa suite dans un prochain épisode ;-)\n\n"
} ,
  
  {
    "title"    : "M6Web Lyon recherche un Lead Developpeur / Architecte web (H/F) en CDI",
    "category" : "",
    "tags"     : " recrutement",
    "url"      : "/m6web-lyon-recherche-un-lead-developpeur-architecte-web-h-f-en-cdi",
    "date"     : "February 5, 2013",
    "excerpt"  : "\n\nMise jour : Le poste n’est à plus pourvoir. Merci\n\nM6Web Lyon recrute, en CDI, un Lead Développeur LAMP, avec une très forte expertise sur les technologies PHP 5.4, MySQL, Symfony2, GIT, et capable d’encadrer une petite équipe de développement.\n...",
  "content"  : "\n\nMise jour : Le poste n’est à plus pourvoir. Merci\n\nM6Web Lyon recrute, en CDI, un Lead Développeur LAMP, avec une très forte expertise sur les technologies PHP 5.4, MySQL, Symfony2, GIT, et capable d’encadrer une petite équipe de développement.\n\nNous recherchons quelqu’un de très passionné, enthousiaste, et mordu de veille technologique : un missionnaire de l’open source, un intégriste de la qualité de code, des tests unitaires et fonctionnels, et un architecte de projets aguerri avec une première approche en méthodologie de développement agile, et une expérience de management de développeurs.\n\nSi, en plus, vous êtes un malade de l’optimisation back-end et front-end, que des technologies comme Node.js vous émoustillent, que, malgré la qualité de MySQL, vous envisagez dans certains cas des solutions NoSQL alternatives (Mongo, Redis…), votre profil nous intéresse !\n\nVenez apporter vos compétences aux équipes techniques de M6Web en travaillant sur des sites très forte charge (m6.fr, clubic.com, jeuxvideo.fr …), et partagez-les grâce des conférences internes ou externes et des articles sur notre blog.\n\nSi vous avez les qualités requises et l’envie de nous rejoindre, allez sur le lien ci-dessous et faites nous part de votre CV, de votre compte github, et d’une lettre attrayante pour nous motiver vous rencontrer.\n\nSi vous souhaitez postuler ou avoir plus d’infos : https://www.groupem6.fr/ressources-humaines/offres-emploi/lead-developpeur-architecte-web-h-f-229879.html\n\n"
} ,
  
  {
    "title"    : "Organiser des conférences technique en interne",
    "category" : "",
    "tags"     : " conference, culture, lft",
    "url"      : "/organiser-des-conferences-technique-en-interne",
    "date"     : "December 5, 2012",
    "excerpt"  : "\n\nLes “Last Friday Talk” : Le concept\n\nDepuis 10 mois désormais, chez M6Web, nous organisons chaque “dernier vendredi” du mois, une manifestation que nous avons nommée “Last Friday Talk”.\n\nLe concept : 2 heures de 13h30 15h30, où 4 sessions “type ...",
  "content"  : "\n\nLes “Last Friday Talk” : Le concept\n\nDepuis 10 mois désormais, chez M6Web, nous organisons chaque “dernier vendredi” du mois, une manifestation que nous avons nommée “Last Friday Talk”.\n\nLe concept : 2 heures de 13h30 15h30, où 4 sessions “type conférence” de 25 minutes, suivies de 5 minutes de questions, sont présentées par des personnes de la Direction Technique. La participation (orateur ou public) est bien entendue facultative.\n\nL’idée, est que chacun a quelque chose dire dans le web de nos jours, quelque chose présenter/partager aux autres. Soit un retour d’experience sur une techno, un outil, une méthodologie, soit même une présentation de ses développements ou projets passés.\n\nDes exemples ?\n\nQuelques exemples de présentations qui ont été faites :\n\n\n  ZéroMq, la bibliothèque réseau\n  Présentation du langage Python\n  VIM pour les nuls\n  Les méthodes agiles\n  Doctrine 2\n  La sécurité SQL\n  Le déploiement chez Facebook\n  La WebPerf avancée\n  Présentation de CoffeeScript\n  …\n\n\nL’interêt ?\n\nLes apports pour vos équipes sont nombreux :\n\n\n  Toute la direction technique participe et partage une partie de la veille technologique de chacun.\n  Les conférenciers améliorent leur communication de “groupe”.\n  Ils deviennent souvent le référent sur le sujet dans l’entreprise.\n  C’est du Team Building, et un rendez-vous mensuel avec vos équipes.\n  Et cela donne des idées à tous les autres développeurs pour de futurs projets (perso ou d’entreprise bien sûr), et attise leur curiosité.\n\n\nNous avons donc tous les mois entre 5 et 10 présentations proposées pour n’en choisir que 4, et une trentaine de participants par session au niveau du public, et nous filmons toutes les conférences, et les archivons sur un site interne.\n\nLes “Karaoké Slideshow” pour plus de fun !\n\nPour combler les mois les plus creux, nous avons aussi tenté une session “Karaoké Slideshow” qui fut hilarante (dans l’esprit des “Ignite Karaoké” pour ceux qui connaissent) ! \n Le concept : Entre 5 et 10 volontaires font une présentation tour tour, sur une série de 5 slides qu’ils n’ont jamais vus, dont chaque slide défile toutes les 15 secondes. Le thème est libre et est improvisé en fonction des slides !\n\nC’est un bel exercice d’improvisation et le fun est garanti :-)\n\nConclusion\n\nAlors si votre entreprise ou votre univers le permet, nous vous conseillons vraiment de tenter l’aventure. C’est très formateur, instructif, et intéressant pour tout le monde, et cela apporte une vraie culture de la veille dans votre environnement ;-)\n\n\n\nOlivier Mansour nous présente la Programmation Orienté Objet “Canada Dry” !\n\n"
} ,
  
  {
    "title"    : "M6Web au banquet de la cuisine du web",
    "category" : "",
    "tags"     : " conference, lcdw",
    "url"      : "/m6web-au-banquet-de-la-cuisine-du-web",
    "date"     : "November 23, 2012",
    "excerpt"  : "Une partie de l’équipe de M6Web était présente au banquet de la cuisine du web avec la fine fleur du web Lyonnais !\n\n\n\n",
  "content"  : "Une partie de l’équipe de M6Web était présente au banquet de la cuisine du web avec la fine fleur du web Lyonnais !\n\n\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conférence Europe 2012 : Day 3",
    "category" : "",
    "tags"     : " conference, velocity, webperf, mobile, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-3",
    "date"     : "October 9, 2012",
    "excerpt"  : "\n\nTroisième et dernière journée la Vélocity Europe.\n\nOn arrive déjà fatigué et gavé d’informations et idées en tout genre, mais on a hâte de démarrer cette journée ! :-)\n\n[Mobile] The Performance of Web Vs Apps, par Ben Galbraith et Dion Almaer (W...",
  "content"  : "\n\nTroisième et dernière journée la Vélocity Europe.\n\nOn arrive déjà fatigué et gavé d’informations et idées en tout genre, mais on a hâte de démarrer cette journée ! :-)\n\n[Mobile] The Performance of Web Vs Apps, par Ben Galbraith et Dion Almaer (Walmart.com)\n\nBen (@bgalbs) et Dion (@dalmaer) nous reprennent dans les grandes lignes, la conférence faite la Vélocity Us (voir le CR + la vidéo de ce talk : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-3-devops-webperf )\n\nL’idée est de comparer les experiences possibles sur WebApp et Apps Native, avec toujours cette comparaison très drôle entre le mode de distribution des apps natives ce que cela donnerait si les show tv devraient être distribués de la même manière en prenant l’exemple de la série Friends : Hilarant !\n\nVoir la vidéo ci-dessous vers 17min 30 :\n\n\n\nBen Galbraith et Dion Almaer (Source : https://royal.pingdom.com/2012/10/05/report-from-velocity-europe-day-3/ )\n\n\n\n[WebPerf] Lightning Démos, par Steve Souders et Patrick Meenan (Google)\n\nPatrick nous a montré les derniers ajouts intéressants de WebPageTest.org, avec notamment le “Block Ads Feature”, l’onglet “SPOF” dans les paramètres avancés pour tester si nos scripts tiers sont des SPOF sur nos sites (j’y reviendrai) … L’outil s’enrichit progressivement et reste toujours LA référence ultime du domaine !\n\n\n\nSteve Souders de son côté, est revenu sur un cas étudié la veille, à savoir l’implémentation d’un LazyLoader sur un caroussel, afin de déterminé via Browserscope.js si cela repoussait le OnLoad event, et c’est le cas !\n\nPetite parenthèse sur les caroussels, je vous invite lire cet article : Don’t Use Automatic Image Sliders or Carousels, Ignore the Fad\n\n\n\n[WebPerf] Do All Users Benefit Equally from Web Performance Optimizations? , par Arnaud Becart (ip-label)\n\nTalk sponsorisé assez intéressant, qui étudie les données récoltées par Ip-label afin de voir si tout le monde profite de la WebPerf de la même manière. La réponse est évidente, mais c’est intéressant de rappeler qu’il faut toujours comparer tests synthétiques au réel, et qu’en fonction du navigateur, du terminal, de la puissance de votre machine, …, des optimisations WebPerf auront un impact différent, du négatif au très positif.\n\n\n\n[DevOps] From DevOps to Operation Science, par Christopher Brown (Opscode)\n\nSon twitter : @skeptomai\n\nTalk orienté “Culture” intéressant par l’un des créateurs de EC2. Mon moment d’absence de la journée :)\n\n\n\n[WebPerf] Performance and Metrics on lonelyplanet.com, par Mark Jennings et Dave Nolan (Lonely Planet)\n\nRetour d’experience des gars de Lonelyplanet (sorte de Routard) très enrichissant. Notamment sur la façon de communiquer à des équipes non techniques, les différentes expérimentations réalisées, et le changement de culture opéré, l’utilisation de Graphite avec notamment les Holt-Winters …\n\n\n  “Being Right isn’t Always Enough” !\n\n\n\n  “Give your metrics a public presence”\n\n\nLes slides : https://fr.slideshare.net/mbjenn/performance-and-metrics-at-lonely-planet-14589911\n\n\n\nMark Jennings et Dave Nolan (Source : https://twitter.com/smcinnes/status/253805752312033280/photo/1 )\n\n\n\n[WebPerf] Third-Party Scripts and You, par Patrick Meenan (Google)\n\nPatrick Meenan nous parle ici de SPOF (Single Point Of Failure ou Point Individuel De Defaillance en français …) et des 3rd party scripts.\n\nL’idée est de montrer comment suivant l’intégration Javascript de scripts tiers, vous pouvez rendre l’affichage de votre site dépendant du bon fonctionnement des serveurs du script tiers.\n\nLes navigateurs mettent en général 20 secondes (45 sous mac et linux) avant de rejeter une connexion sur un script tiers en rade. Vous pouvez voir des vidéos de l’effet que ça peut avoir sur vos pages dans les slides.\n\nAfin de les détecter, il existe l’extension SPOF-O-Matic :\n\nhttps://chrome.google.com/webstore/detail/spof-o-matic/plikhggfbplemddobondkeogomgoodeg\n\nEn surfant, vous saurez rapidement si un SPOF est présent sur votre site ou non et combien de contenu il bloque, et pourrez générer un WebPageTest comparatif en simulant le plantage du script en question (Redirection sur domaine blackhole.webpagetest.org)\n\nPour régler ces problèmes, plusieurs solutions : Script à charger dynamiquement via Js de manière asynchrone, script avec async et defer, ou au pire, script avant le /body.\n\nLes slides : https://www.slideshare.net/patrickmeenan/velocity-eu-2012-third-party-scripts-and-you\n\n\n\n[Ops] How Draw Something Absorbed 50 Million New Users, in 50 Days, with Zero Downtime, par J Chris Anderson (Couchbase)\n\nNous arrivons à l’entrée de la salle où a lieu cette présentation, barré par des commerciaux CouchBase, nous empéchant de rentrer sans prendre le prospectus CouchBase et sans se faire scanner son QRcode présent sur nos badges … ça commence mal …\n\nAu bout de 2 minutes de talk par J Chris Anderson ( @jchris ) , co-fondateur de Couchbase, le malaise est confirmé : on ne parlera pas ici de Draw Something, mais de Couchbase 2.0 uniquement, le nom Draw Something n’étant là que pour appâter du client potentiel, et ça marche, la salle est comble …\n\nDifficile du coup d’être concentré dans cette approche plus que douteuse … les questions au final seront aussi assez violentes sur le sujet : “pourquoi appâter les gens avec Draw Something, si ça n’est que pour parler de CouchBase ?” La réponse est évasive … Nous n’avons pas eu le droit d’en parler …\n\nBref, le produit Couchbase a tout de meme l’air très intéressant, et plutôt costaud, avec de très chouettes Dashboard de monitoring temps réels built-in.\n\nJ’en sors quand même avec l’impression très désagréable de m’être fait piéger …\n\nLes slides (non dispo) ressemblait fortement à cette autre présentation de J Chris : https://speakerdeck.com/u/jchris/p/nosql-landscape-speed-scale-and-json\n\n\n\n[WebPerf] WebPagetest - Beyond the Basics, par Aaron Peters (Turbobytes), Andy Davies (Asteno)\n\nPas mal de conférences parlaient de WebPageTest, mais celle-ci promettait d’aller en profondeur. Le créateur n’a jamais caché son manque de talent pour les interfaces, et WPT regorge de richesses en tout genre cachées dans les méandres de ses pages :-)\n\nEnormement d’informations et tips sont donc présents dans les slides de cette conférence.\n\nPour rappel : instruction d’Andy pour monter une instance privée de WPT : https://andydavies.me/blog/2012/09/18/how-to-create-an-all-in-one-webpagetest-private-instance/\n\nLes slides : https://www.slideshare.net/AndyDavies/web-page-test-beyond-the-basics\n\n\n\nAndy Davies et Aaron Peters (Source : https://royal.pingdom.com/2012/10/05/report-from-velocity-europe-day-3/ )\n\n\n\n[DevOps] What HTTP/2.0 Will* Do For You, par Mark Nottingham (Akamai)\n\nL’une des conférences les plus intéressantes de la Vélocity pour ma part avec notamment l’annonce que HTTP/2.0 sera basé sur SPDY déj…\n\nMark Nottingham ( @mnot ), Chair of the IETF HTTPbis Working Group, excellent conférencier, nous explique donc ce que sera HTTP/2.0 :\n\n\n  Aucun changement la sémantique HTTP\n  Basé sur Speedy\n  Multiplexing (voir slide 22)\n  Header Compression, technique très intéressante, pour éviter de ré-envoyer les memes headers pour chaque requête HTTP\n\n\nPas mal de ressources sont disponibles sur son site : https://www.mnot.net/\n\nLes slides sont un modèle du genre, simples et efficaces : https://www.slideshare.net/mnot/what-http20-will-do-for-you\n\n\n\n[DevOps] Web &amp;amp; Native Cross-Platform Multiplayer, par Ashraf Samy Hegab (Orange)\n\nComment développer une expérience de Gaming multi-plateforme : Web, Android, Iphone ? C’est la question à laquelle Ashraf a essayé de répondre, sur cette dernière conférence avec un humour et une énergie très communicative.\n\nPas mal de bonnes ideés applicables au web traditionnel, sur une stack NodeJs/Mongo/Socket.io, pour faire la majorité du travail et communiquer avec les parties natives d’app Android et Ios.\n\nNous avons aussi fait une démo live sur le jeu Phone Wars (Disponible sur Appstore et Google Play) d’une éxperience Gaming Multi-plateforme.\n\nTrès rafraîchissant pour finir ces 3 journées marathons !\n\nConclusion :\n\nÇa y’est, la Vélocity Europe est finie pour cette année. Les bouchées doubles ont été mises par rapport à la Vélocity Berlin de l’année dernière, et cette conférence reste vraiment la conf incontournable pour tous ceux que la Webperf, les Devops, et les sites fort traffic intéressent !\n\nOn regrette simplement que seule la grande salle ait été filmée, que la chasse aux slides soit toujours aussi tordue (trop peu renseigné sur le site de la Vélocity). Le reste est juste parfait !\n\nA la prochaine, et merci pour vos retours.\n\nP.s: Merci aussi aux équipes de Pingdom pour leurs Twitt Live et les chouettes photos prises ( https://royal.pingdom.com/ )\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-2\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-1\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conférence Europe 2012 : Day 2",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-2",
    "date"     : "October 8, 2012",
    "excerpt"  : "\n\nDeuxième journée, avec le début “officiel” de cette conférence, où l’on nous donne rendez vous dans l’immense King’s Room.\n\nPour bien démarrer, on commence avec en quelque sorte l’hymne de la Vélocity : Speed &amp;amp; Velocity !\n\n\n\n[DevOps] Move Fa...",
  "content"  : "\n\nDeuxième journée, avec le début “officiel” de cette conférence, où l’on nous donne rendez vous dans l’immense King’s Room.\n\nPour bien démarrer, on commence avec en quelque sorte l’hymne de la Vélocity : Speed &amp;amp; Velocity !\n\n\n\n[DevOps] Move Fast and Ship Things, par Girish Patangay (Facebook)\n\nPremière présentation de la journée par l’un des Manager chez Facebook, (maintenant dans les bureaux londoniens), sur la capacité de Facebook, et leur volonté, d’ évoluer et de déployer rapidement.\n\nIl nous raconte les débuts de Facebook, avec peu de serveurs, des changements inférieurs 5Mb, Rsync pour pusher en prod etc … puis la migration vers HipHop.\n\nDésormais chaque changement nécessite de recompiler un gros binaire de 1.2Go, et d’y envoyer sur plus de 10 000 serveurs, et ce plusieurs fois par jour !\n\nAvec Bittorent, ils envoient 500Mb en moins d’une minute sur les 10 000 serveurs.\n\nOn a eu le droit une présentation de GateKeeper, outil interne, permettant de faire du feature flipping géolocalisé. La timeline a par exemple plus d’une centaine de GateKepper.\n\nAujourd’hui, Facebook cherche trouver le moyen de scaler de plus de 1000 développeurs à 10000, et d’évoluer sur ce système de “Move Fast” dans le mobile natif.\n\nPour ceux que ça intéresse, en plus de la vidéo çi dessous, Quora est une mine d’or d’infos sur FB : https://www.quora.com/Girish-Patangay\n\n\n\n(Source https://royal.pingdom.com/2012/10/03/report-from-velocity-europe-day-2/ )\n\n\n\n[WebPerf] Keynote KITE and MITE, par Robert Castley (Keynote)\n\nVient ensuite la conférence de Keynote (Sponsorisée), qui nous présente deux outils intéressant :\n\n\n  KITE pour Keynote Internet Testing Environnement : https://kite.keynote.com/\n  MITE pour Mobile Internet Testing Environnement : https://mite.keynote.com/\n\n\nMITE est d’ailleurs utilisé par le site de Google Howtogomo.com\n\nBref, si vous êtes sur Windows (…), jetez y un oeil. Plus d’infos en vidéo :\n\n\n\n[WebPerf] Lightning Démos\n\nDémo 1 : Chrome Dev Tools :\n\nPremière démo de Iliya Grigorik ( @igrigorik) sur les capacités avancées de la chrome Dev Toolbar.\n\nOn peut par exemple faire un clic droit sur l’onglet Network pour récupérer le har (voir le format HTTP Archive) en json, et utiliser d’autres outils avec ce har, notamment Yslow dont je parlais dans le compte rendu suivant, qui permet d’ajouter les régressions possible WebPerf dans votre CI Jenkins.\n\nOn parle aussi du chrome://tracing , du débugger mobile, que la devtools est une WebApp avec une url propre et scriptable, du Chrome Benchmarking (extension) …\n\nVoir la présentation suivante pour plus d’infos, bien plus compléte : https://www.igvita.com/slides/2012/devtools-tips-and-tricks/\n\n\n\nDémo 2 : Box Anemometer :\n\nGavin Towey ( @gtowey), DBA MySql chez Box.com nous présente une interface pour visualiser et traiter correctement le slow log de Mysql. Basé sur Php 5.3, les outils Percona et Bootstrap pour l’interface, l’outil est un vrai bonheur pour tous ceux qui font un peu d’optimisation MySql au quotidien, développeur, sysadmin ou dba. Nous l’avions déjà découvert aux DevOpsDays Mountain View cet été, et l’utilisons massivement depuis.\n\nLe projet est sur Github : https://github.com/box/Anemometer\n\nVoir aussi le projet Rain Gauge dans la lignée de Anemometer, toujours par Gavin Towey : https://github.com/box/RainGauge\n\nVoici la démo en vidéo :\n\n\n\n[WebPerf] Emerging Markets / Growth Markets, par Jeff Kim (CDnetworks)\n\nJeff Kim, Chief Operating Officer chez CDnetworks nous a partagé quelques données et chiffres intéressants sur les marchés émergeants comme l’Inde, Indonésie, les Philippines, le Brésil etc\n\nOn apprend que Chrome a une part de marché de 62% au Brésil, 51% en Inde, et Opéra de 26% en Russie. Que l’e-commerce au final n’a pas vraiment encore démarré en Inde, Brésil et Russie…\n\nAprès des études d’Eye Tracking View, on remarque aussi entre la population chinoise et américaine, que sur une page de résultats de recherche, les américains ne regardent que le coin haut gauche de la page pour se contenter des premiers résultats, alors que les chinois consultent vraiment toute la hauteur de la page, pour regarder tous les résultats.\n\n\n\n[WebPerf] Why page speed isn’t enough, par Tim Morrow (Betfair)\n\nSon twitter : @timmorrow\n\nAncien de ShopZilla, et déja présent Berlin l’année dernière, Tim a partagé son retour d’expérience sur la refonte de BetFair (très gros site de pari en ligne). Les gens se plaignaient d’une mauvaise expérience (alors que les pages refondues étaient plus rapides), mais nécessitaient à priori beaucoup plus de temps pour parvenir au pari final que dans l’ancienne version. En gros, le temps de chargement de vos pages n’est pas suffisant, il faut aussi regarder les scénarios fonctionnels de vos sites. Ils sont passés sur une navigation typée Ajax pour ne pas rafraîchir dans certains cas l’intégralité de la page.\n\nVoir les slides et la vidéo : https://fr.slideshare.net/timmorrow/why-page-speed-isnt-enough-tim-morrow-velocity-europe-2012\n\n\n\n\n[WebPerf] W3C Status on Web Performance, par Alois Reitbauer (Compuware, Dynatrace)\n\nBeau récapitulatif du status du “W3C performance working group” sur la performance Web, avec des rappels sur la NavTiming et les différents standards, quelques échanges autour de la NavTimingV2, d’une Resource time Measurement etc …\n\nSon twitter : @compuware et @AloisReitbauer\n\nPlus d’infos dans la vidéo ci dessous :\n\n\n\n[WebPerf] 3.5s Dash for attention and other stuff we found about RUM, par Philipp Tellis (Log Normal)\n\nSon twitter : @bluesmoon\n\nLe transcript et détail complet de la présentation est présent sur un blogPost très complet de Philip Tellis, créateur de Log Normal (racheté par SOASTA) :\n\nhttps://www.lognormal.com/blog/2012/10/03/the-3.5s-dash-for-attention/\n\nBeaucoup de chiffres et d’infos intéressantes autour de la WebPerf, avec notamment cette métrique basée sur un Bounce Rate &amp;gt;= 50%\n\nLes slides sont disponibles : https://fr.slideshare.net/bluesmoon/the-35s-dash-for-user-attention-and-other-things-we-found-in-rum\n\n\n\n[Mobile] Escaping the uncanny valley, par Andrew Betts (FT Labs)\n\nSon twitter : @triblondon\n\nConférence très intéressante par Andrew, le directeur d’FT Labs (Financial Times), qui a parcouru l’ensemble des travaux que ces équipes ont effectués autour des capacités mobile pour faire une webapp HTML5 intégrée dans une appli native avec la meilleure experience possible.\n\nAu menu, rappel sur la lourdeur de parcours du DOM en JS, les incohérences du SetTimeout (notamment sur IOs), les problématiques de l’AppCache ou localStorage. Le fait d’utiliser au maximum l’accélération matérielle sur les CSS, l’optimisation des paint, les spinners et loading bar, le progressive rendering …\n\nRappel de la latence sur les “clics” tactiles avec le projet Fastclick, qui enlève les 300ms de délai sur le clic mobile : https://github.com/ftlabs/fastclick/\n\n[Mobile] Make your mobile web apps fly, par Sam Dutton (Google)\n\nSon twitter : @sw12\n\nSam Dutton, Developers Advocate (?) chez Google, nous fait un parcours assez complet des bases d’optimisations WebPerf pour le mobile. Un peu de redondance versus d’autres confs vues plus tôt, mais le sujet est bien maîtrisé et bien couvert.\n\nQuelques petits outils intéressants, notamment le Multires pour la gestion des images Retina : https://fhtr.org/multires/\n\nA noter aussi une phrase que j’ai beaucoup aimé sur l’ergnonomie mobile, et la position des boutons de contrôle :\n\n\n  “Controls should be beneath content: think calculator”\n\n\nLes slides : https://www.samdutton.com/velocity2012/ \n\n[DevOps] Scaling Instagram, par Mike Krieger (Instagram)\n\nSon twitter : @mikeyk\n\nPetit debrief orienté Ops de la success story d’Instragram. Le rythme de la présentation était vraiment pushy, donc plutôt dur suivre pour nous autre francophones …\n\nEn plus d’avoir l’impression de voir un demi milliard bouger sur scène, nous avons quand même appris certaines choses sur la Stack Instagram : EC2, Python Django, Postgres, Gearman, RabbitMQ …\n\nLa présentation n’étant pas nouvelle, est disponible ici : https://fr.slideshare.net/iammutex/scaling-instagram\n\n\n\nMike Krieger (Source : https://royal.pingdom.com/2012/10/03/report-from-velocity-europe-day-2/ )\n\n\n\n[WebPerf] Bringing HTML5 into Nativelandia: A Tale of Caution, par Jackson Gabbard (Facebook)\n\nL’une de conférences que j’attendais beaucoup, de Jackson Gabbard, Mobile Engineer chez Facebook, qui nous explique le passage du HTML5 au native pour les applications mobile (IOs pour le moment?).\n\nPassé les stats toujours aussi hallucinantes, il explique tout ce qui n’était pas convainquant sur l’ancienne webApp HTML5, et que ce qui a vraiment échoué, est le marriage entre la WebView et le natif. Que l’expérience “native” est bien plus concluante pour l’utilisateur par rapport ce qu’ils souhaitent obtenir niveau fluidité, performances, efficacité, utilisation réseau etc …\n\nIls ont du coup redeveloppé pas mal de leurs outils pour s’adapter a ce mode de fonctionnement (un GateKeeper plus light) etc …\n\nConférence vraiment passionnante, avec un gars plutôt très transparent sur Facebook et les raisons qui ont poussées ce changement.\n\nConclusion :\n\nDu lourd encore une fois, avec énormement de sujets très intéressants, même si l’on commence à tourner un peu en rond autour de la WebPerf Mobile.\n\nA noter aussi qu’un grand nombre de livre Oreilly ont été donné et dédicacé par Steve Souders et John Allspaw ;-)\n\nLes salles sont déjà bien plus sympa, et l’organisation toujours au top ! Vivement demain.\n\nPour finir, voici quelques vidéos diffusées pendant les breaks la Vélocity :\n\n\nHot Wheels World Record: Double Loop Dare at the 2012 X Games Los Angeles\n\n\nJeb Corliss “ Grinding The Crack”\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-3\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-1\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conférence Europe 2012 : Day 1",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops",
    "url"      : "/cr-velocity-conference-europe-2012-day-1",
    "date"     : "October 4, 2012",
    "excerpt"  : "Nous voici de retour pour une Vélocity Conference, le paradis de la WebPerf et des Devops !\n\nAprès l’excellente moisson de la Vélocity US de Santa Clara, dont voici nos 3 CR :\n\n\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-da...",
  "content"  : "Nous voici de retour pour une Vélocity Conference, le paradis de la WebPerf et des Devops !\n\nAprès l’excellente moisson de la Vélocity US de Santa Clara, dont voici nos 3 CR :\n\n\n  Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-2-devops-webperf\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-3-devops-webperf\n\n\nEt après la session de l’année dernière qui avait lieu à Berlin, nous nous retrouvons cette fois dans la capitale anglaise Londres, au Hilton hôtel.\n\nVoici le compte rendu des conférences de la première journée, journée un peu à part placée sous le signe des “Tutorials” (2 octobre 2012)\n\n[OPS] Monitoring and Observability in complex architecture, par Theo Schlossnagle (OmniTI)\n\nPremière conférence de la journée, avec Theo, habitué des Velocity, et plutôt expert dans les domaines “infra” et “monitoring”. Créateur de Omniti, MessageSystems et Circonus.\n\nSon twitter : @postwait\n\nTheo nous explique comment monitorer et observer des archi complexes avec une présentation très bas niveau.\n\nLes outils de collectes de statistiques qu’il cite :\n\n\n  Metrics.js : https://github.com/mikejihbe/metrics\n  Resmon : https://labs.omniti.com/labs/resmon\n  Folsom : https://github.com/boundary/folsom\n  Metrics : https://metrics.codahale.com/\n  Metrics-net : https://github.com/danielcrenna/metrics-net\n  StatsD : https://github.com/etsy/statsd\n\n\nEt pour le stockage :\n\n\n  Reconnoiter : https://labs.omniti.com/labs/reconnoiter\n  Graphite : https://graphite.wikidot.com/\n  OpenTSDB : https://opentsdb.net/\n  Circonus : https://circonus.com/\n  Librato : https://metrics.librato.com/\n\n\nOnt suivi ensuite des démos tcpdump assez poussées plutôt intéressante :\n Par exemple, pour voir les nouvelles connexions entrantes (récupération des packets SYN) : tcpdump -nnq -tttt -s384 ‘tcpport 80 and(tcp[13] &amp;amp; (2|16) == 2)’\n\n\n  d’exemples sont disponibles dans les slides, avec aussi d’autres exemples live sur “strace” et “dtrace”.\n\n\nBref, ca commence très (trop?) fort, surtout pour nous, pas forcément de nature très “OPS” !\n\nLes slides sont disponible ici et plutôt parlantes pour ceux qui voudraient creuser le sujet : https://www.slideshare.net/postwait/monitoring-and-observability\n\n\n\n\n  “You cannot correct what you cannot measure” Theo Schlossnagle\n\n\n\n\nTheo Schlossnagle (Source : https://img.ly/o0Ht )\n\n[DevOps] Escalading Scenario : a deep dive into outage falls, par John Allspaw (Etsy)\n\nOn prend toujours autant de plaisir écouter John Allspaw, VP d’Esty et Co-organisateur de la Vélocity avec Steve Souders, nous parler d’incident, et de la meilleure manière de les gérer.\n\nSon Twitter : @allspaw\n\nBeaucoup de parallèles sont faits avec des incidents dans l’aviation, l’industrie, voir l’armée, avec des références aussi au PostMortem d’appolo 13 … Un joli condensé de bouquins “Must Read” d’après Allspaw comme : Normal Accidents ou encore Naturalistic Decision Making, nous sont présentés.\n\nDifficile d’en faire un résumé, tellement la conférence était bourrée d’informations, graphiques et anecdotes en tout genre ! Malgré tout, dans les “choses” retenir, en vrac :\n\n\n  Attention au contexte de vos graphiques : un graph qui parait anormal sur une heure, peut s’avérer normal sur une échelle de temps d’une journée par exemple\n  Des bons conseils sur la résolution d’incident en équipe, notamment au niveau de la communication avec des exemples de conversation pendant des incidents chez Etsy trop ambigus. Il est important de confirmer les réponses, corriger la communication des autres, afin d’éviter tout soucis de compréhension\n  Ne pas hésiter à demander des “pre-mortem”. Dire à l’auteur du projet par exemple, que cela va planter dans plusieurs mois, et lui demander d’essayer de trouver la ou les raisons qui pourraient amener le projet au plantage.\n  …\n\n\nJe vous invite encore consulter les slides pour plus d’informations et de références : https://fr.slideshare.net/jallspaw/velocity-eu-2012-escalating-scenarios-outage-handling-pitfalls\n\n\n\n\n\nJohn Allspaw (Source : https://twitter.com/lozzd/status/253074489540239360 )\n\n[WebPerf] Running a WebPerf Dashboard in 90 minutes, par Jeroen Tjepkema\n\nSon twitter : @jeroentjepkema\n\nL’objectif de cette conférence était de proposer en 90 minutes, les étapes nécessaire pour monter un dashboard orienté WebPerf.\n\nOn re-parcourt du coup un peu tout le classique de la performance web, en présentant déjà quelques exemples de Dashboard (rien de très sexy …, hormis peut-être celui de Nrc.nl orienté “audience éditoriale” plutot intéressant), la pertinence de certains types de “graphs” comme les “heatmap” et aussi en comparant les différentes solutions pour mesurer la performance web, avec avantages et inconvénients :\n\n\n  Synthetic Monitoring (Gomez, Keynote, IpLabel, Pingdom etc) (slide 104-105)\n  Real User Monitoring (LogNormal dont l’acquisition par Soasta a aussi été annoncé ce jour, Boomerang.js, Torbit, Google Analytics …) (slide 121-122)\n  Real User Benchmarking (WebPageTest) (slide 134)\n\n\nQuelques idées sympa de design sont disséminées tout au long de cette longue présentation, on regrette simplement de survoler toujours un peu tous les concepts, mais malgrés tout, cela reste l’une des rares tentatives de faire un dashboard WebPerf accessible à des “non-techniciens”. Chapeau pour cela.\n\nUne démo du dashboard est testable ici : https://app.measureworks.nl/secured/dashboard (Login : demo@measureworks.nl , password: performance )\n\nLes slides sont disponibles ici : https://www.slideshare.net/MeasureWorks/measureworks-velocity-conference-europe-2012-a-web-performance-dashboard-final\n\n\n\n\n\nJeroen Tjepkema (Source : https://twitter.com/pingdom/status/253135289327951872 )\n\n[WebPerf] Deep Dive into Performance analysis, par Steve Souders et Patrick Meenan (Google)\n\nLeurs Twitter : @souders et @patmeenan\n\nDernier “Tutorials” du jour avec Steve Souders (Chief performance Officer chez Google), et Patrick Meeman (aussi chez Google désormais, créateur de Webpagetest).\n\nHistoire de s’adapter au public Anglais, les deux compères ont décidés de s’attaquer aux sites des équipes de Premier League du Foot Anglais :-)\n\nComme imaginé, ca n’est pas “folichon”, et on étudiera en profondeur en live les sites de Chelsea et Tottenham, qui chacun enchaine un nombre d’aberrations plus grandes les une que les autres !\n\nLe tableur utilisé pendant la présentation avec les liens vers les tests WebPageTest : goo.gl/YfbRn\n\nQuelques exemples sur le site de Chelsea : https://www.chelseafc.com/\n\nLes tests WPT annoncent un Load Time 21 secondes, 203 requêtes HTTP et 3mo4 téléchargés !\n\nLe Waterfall que vous pouvez voir ici, est on ne peut plus parlant, avec une mention spéciale pour la liste des images présentes sur la HP : https://velocity.webpagetest.org/pageimages.php?test=120925_0_13&amp;amp;run=2&amp;amp;cached=0\n\nJ’imagine que le problème va vous sauter aux yeux, entre les fonds énormissimes et images non compréssées, les images de chacun des joueurs de l’équipe (oui, le pauvre carroussel en bas de page …), et le nombre incroyable de logo et ou picto en tout genre, on voit vite comment améliorer la page :-)\n\nLe carroussel du haut donne aussi un effet assez comique sur le “FilmStrip View” où l’ont voit vers les 10 secondes, un début d’image se charger, pour s’effacer car le carroussel passe déja au panel suivant … Merci au passage au jCaroussel qui charge bêtement toutes les images …\n\nOn remarque aussi un nombre conséquent de JS qui retarde grandement le Start Render. Optez autant que possible pour les positionner juste avant le /body, ou les charger en async/defer ou via un chargement asynchrone en Js.\n\nPas mal de petites astuces sont partagées par Patrick et Steve, notamment sur l’utilisation de la courbe de Bande Passante, pour voir les parties pouvant être optimisées (celle ou la bande passante n’est pas utilisée fond par exemple), on remarque aussi quelques ajouts récents comme l’affichage des évenements Paint sur la FilmStrip View (Screenshot encadré de Orange), ou encore la possibilité via clic droit dans la Chrome Dev Tools de vider le cache et les cookies rapidement etc …\n\nNous avons aussi eu la confirmation que Google prenait le Onload Time comme référence pour ses algorithmes de ranking.\n\nBref, superbe application de tous les concepts WebPerf avec des cas concrets d’étude et une conférence très interactive avec suffisament d’astuces pour combler aussi les habitués de WPT.\n\n\n\nPatrick Meenan &amp;amp; Steve Souders (Source : https://twitter.com/simonox/status/253146271156670464 )\n\n[DevOps] Ignite Talk\n\nPour finir cette journée, rendez vous dans l’immense salle (dans laquelle aura lieu la majorité des conférences suivantes), pour un Ignite Talk combiné entre la Vélocity Conférence et Strata Conférence qui ont lieu au même moment dans l’Hilton hôtel Londres.\n\n\n\nSalle King’s Room (Source : https://twitter.com/cmsj/status/253139957093367808/photo/1 )\n\nNous suivrons une série de 7 ou 8 Ignite Talk dont le concept est de présenter un sujet sur 20 slides défilant automatiquement toutes les 15 secondes. C’est assez décalé, fun, et l’exercice parait très “sport” … :)\n\nPas mal de sujets tournaient autour de l’Open Data ou Big Data, les DataViz …\n\nVoici par exemple un talk sympa sur les “Dataviz as interface” par @makoto_inoue : https://fr.slideshare.net/inouemak/data-viz-asinterfacemakotoinoue\n\n\n\n\n\nMakoto Inoue, de l’énergie, de la danse, et de la bière ! (Source: https://royal.pingdom.com/2012/10/02/velocity-europe-1/ )\n\n[WebPerf] Step by Step Mobile Optimization, par Guy Podjamy (Akamai)\n\nConférence auquelle je n’ai pas pu assisté : https://fr.slideshare.net/guypod/step-by-step-mobile-optimization\n\n\n\nConclusion :\n\nBonne première journée avec déjpas mal de choses retenir et appliquer quotidiennement !\n\nOn regrette le fait d’avoir été dans des petites salles (sûrement à cause de la dernière journée de la Strata Conférence), et du coup d’avoir alterné le manque de place, avec la chaleur des salles … Et je n’ai pas l’impression que les sessions du jour étaient filmées malheuresement !\n\nC’est en tout cas un très bon avant-gout de ce qui nous attend demain ;-)\n\nEnjoy !\n\nP.s : N’hésitez pas nous faire des retours sur ce CR ! :)\n\nRappel : les CR des autres jours sont disponible :\n\n\n  Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-2\n  Day 3 : https://tech.bedrockstreaming.com/cr-velocity-conference-europe-2012-day-3\n\n"
} ,
  
  {
    "title"    : "Tentative d&#39;explication des Fast-Forward sous Git",
    "category" : "",
    "tags"     : " git",
    "url"      : "/tentative-d-explication-des-fast-forward-sous-git",
    "date"     : "July 13, 2012",
    "excerpt"  : "\n\nTous les projets M6Web sont passés récemment sous le système de gestion de contenu Git.\n\nGit, c’est super cool ! On peut faire facilement des branches, les “merger” les unes aux autres et “switcher” d’une branche une autre. Pratique donc (dans l...",
  "content"  : "\n\nTous les projets M6Web sont passés récemment sous le système de gestion de contenu Git.\n\nGit, c’est super cool ! On peut faire facilement des branches, les “merger” les unes aux autres et “switcher” d’une branche une autre. Pratique donc (dans l’idée) !\n\nIl a été finalement assez facile de se faire au vocabulaire et au fonctionnement de git. Je ne dis pas que je ne fais pas non plus mes commit sur la bonne branche chaque fois, mais on arrive tout de même assez facilement à s’en sortir (“git reset –help” si vous êtes dans ce cas) !\n\nLe très intéressant article “A successful Git branching model” a mis en avant la gestion des fast-forward, cela dit l’utilité m’est resté assez floue et ne couvrait pas l’ensemble de mes questions.\n\nJ’ai donc fouillé la documentation de Git afin de débroussailler les “fast-forward”.\n\nQue fait git lors d’un merge\n\nGit “merge” deux branches lorsque :\n\n\n  La commande “merge” est utilisée, par exemple git merge feature-myfeature,\n  la commande pull est utilisée, exemple git pull origin master\n\n\nGit “fast-forward”\n\nMettons que Bob fasse une modification sur une branche, il crée un commit Y.\n\nIl fait une autre modification qu’il commit, il crée alors un commit Z.\n\nAutomatiquement git “fast-forward”, c’est dire qu’il fait pointer la branche, qui pointait sur le commit Y, vers le commit Z. Sur les graphiques de git log, les deux commit sont liés par un trait continu.\n\n- Y - Z\n\nTant que Bob continue faire des modifications + commit sans toucher au fast-forward, git va automatiquement “fast-forwarder”. On aura donc un enchaînement de commit qui sont liés par un trait continu.\n\n- Y - Z - AA - AB - AC - ...\n\nGit ne “fast-forward” pas\n\nNous sommes maintenant sur une branche qui en est la révision X.\n\nAlice travaille sur son projet et crée la révision A.\n\nCela dit, Bob travaille aussi sur le projet et crée la révision B.\n\nAlice pousse ses modifications :\n\nLe commit A a pour parent le commit X, qui est le dernier commit connu par la branche, Git peut donc “fast-forwarder”.\n\n- X - A\n\nBob pousse ses modifications :\n\nLe commit de bob ne connaît pas de commit A dans son historique (son commit parent est le commit X).\n\n\n- X - A\n   \\\n    B\n\nSi git “fast-forwardait” ici, il ferait pointer la branche sur le commit B, et perdrait le commit A. Comme on ne souhaite pas perdre les modifications d’Alice, il va donc passer en mode “no fast-forward” automatiquement.\n\nGit va donc récupérer les modifications de A et les mélanger (merge) aux modifications de B en créant un commit C.\n\nLe commit C a pour parent les commit** B** et** A, le pointeur de dernier commit peut donc être placé sur **C sans risque de perte d’historique.\n\n- X - A\n   \\   \\\n    B - C\n\nOption* –no-ff*\n\nL’auteur de l’article cité précédemment conseille d’utiliser l’option –no-ff sur les merge.\n\nCette option force git a créer un commit de “merge” qui aura pour parents notre commit de modification, et le dernier commit connu sur la branche, même si il n’y a pas eu de modification sur cette dernière.\n\nCela permet de revenir facilement à la version antérieure de la branche, sans avoir à fouiller dans les nombreux commits ayant pu amener un bug : on revient à la version initiale avant de passer plus de temps pour corriger le bug.\n\n- M1 -- -- -- -- M2\n    \\            /\n     B1 - B2 - B3\n\nDans l’exemple ci-dessus, on peut facilement revenir au commit M1, et exclure ainsi toute la branche B. Si on avait “fast-forwardé”, il nous aurait fallut retrouver le commit M1 en regardant tous les commit précédent.\n\nMode auto contre –no-ff\n\nForcer le *–no-ff *ne sera finalement utile que lorsque vous développez une fonctionnalité pour laquelle vous allez beaucoup “commiter”, sans que personne d’autre ne commit entre-temps.\n\nA vous de l’utiliser de manière intelligente !\n\nSources\n\n\n  nvie.com : A successful Git Branching Model\n  git push –help\n\n\n"
} ,
  
  {
    "title"    : "Retrouvez l&#39;intervention du CTO de M6 Web, Martin Boronski, à la table ronde du Forum PHP 2012",
    "category" : "",
    "tags"     : " forumphp, afup, video, php",
    "url"      : "/retrouvez-l-intervention-du-cto-de-m6-web-martin-boronski-a-la-table-ronde-du-forum-php-2012",
    "date"     : "July 11, 2012",
    "excerpt"  : "\n\n",
  "content"  : "\n\n"
} ,
  
  {
    "title"    : "Intégration continue avec Jenkins et Atoum",
    "category" : "",
    "tags"     : " php, atoum, jenkins, ci",
    "url"      : "/integration-continue-avec-jenkins-et-atoum",
    "date"     : "July 11, 2012",
    "excerpt"  : "\n\nChez M6 Web nous tentons de créer une approche open-source intra entreprise. L’objectif est que certains composants génériques adaptés notre métier puissent être crées et diffusés largement parmis les dizaines de projets gérés chaque année. Un p...",
  "content"  : "\n\nChez M6 Web nous tentons de créer une approche open-source intra entreprise. L’objectif est que certains composants génériques adaptés notre métier puissent être crées et diffusés largement parmis les dizaines de projets gérés chaque année. Un prochain post traitera de cette problématique.\n\nDans cette optique, il faut nous assurer de la qualité et la non régréssion de ces composants. Pour cela nous avons mis en place Jenkins afin d’assurer l’intégration continue de nos tests unitaires. Voici un exemple d’intégration avec Atoum (ce n’est pas forcement la meilleur méthode, n’hésitez pas à la commenter).\n\nStructure du composant :\n\n\n  ./src contient les classes du composants au format PSR-0\n  ./tests contient les TU Atoum\n  ./build-tools/jenkins contient les fichiers de configuration pour Atoum et Ant\n  ./vendor contient les dépendances externes du projet (gérées avec Composer)\n\n\nVoici le composer.json utilisé.\n\n\n\nVoici le fichier de configuration de Atoum : build-tools/jenkins/atoum.ci.php et celui de jenkins build-tools/jenkins/build.xml\n\n\n\n(cette configuration inclut l’ensemble des outils d’analyse statique que l’on utilise)\n\nEnfin, voici la configuration faire sur Jenkins (en image).\n\n\n\n\n\n\n\nVia cette conf on obtient le résultat des tests (naturellement) ainsi que la couverture de code des tests avec la coloration des lignes couvertes ou non couvertes dans les classes testées.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 3 (DevOps/WebPerf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops, mobile",
    "url"      : "/cr-velocity-conference-2012-day-3-devops-webperf",
    "date"     : "June 28, 2012",
    "excerpt"  : "Dernière journée de cette monstrueuse conférence qu’est la Vélocity Conférence.\n\nOn commence dans la joie et la bonne humeur avec la “Seven Databases Song” :D\n\n\n\n[Mobile WebPerf] The Performance of Web vs. Apps, par Ben Galbraith (Walmart.com) &amp;am...",
  "content"  : "Dernière journée de cette monstrueuse conférence qu’est la Vélocity Conférence.\n\nOn commence dans la joie et la bonne humeur avec la “Seven Databases Song” :D\n\n\n\n[Mobile WebPerf] The Performance of Web vs. Apps, par Ben Galbraith (Walmart.com) &amp;amp; Dion Almaer (Walmart.com)\n\n\n\nPetit sujet assez trollesque sur les WebApps vs Apps. Conférence hyper énérgétique et très drôle ! Notamment le passage à 12mn dans la vidéo, où l’on compare le mode de distribution des apps natives à ce que cela donnerait si les show tv devraient être distribués de la même manière en prenant l’exemple de la série Friends : Hilarant !\n\nL’idée intéressante sur la fin du talk, concerne le rendu de l’application, qui grâce Node.js (dispo désormais en v0.8.0 enfin) peut être aussi bien fait coté client que serveur suivant le client qui demande. A creuser.\n\n\n\n[WebPerf] Akamai Internet Insights, Stephen Ludin (Akamai)\n\n\n\nPetit talk de Stephen Ludin “Chief Architect for Akamai’s Site Acceleration and Security group”.\n\nAprès une présentation assez hallucinante en quelques chiffres du traffic et des données qui passent chez Akamai :\n\nToutes les 60 secondes =&amp;gt; 1 milliard 3 de logs, + de 6200 heures de vidéos streamés …\n\nIl a aussi partagé une initiative louable et très intéressante sur un projet de partage des données récoltées chez Akamai : https://www.akamai.com/io\n\nOn y observe quelques statistiques (relativement peu date) sur les browsers notamment. On voit d’ailleurs quelque chose d’assez fun sur les IE8 : chaque weekend, on apercoit une baisse de présence sur IE8 (qui se retrouve sur d’autres navigateurs plus récent) … Bref, on voit encore que c’est le monde de l’entreprise qui ralenti la propagation des navigateurs récents !\n\nSource : \nhttps://www.akamai.com/html/io/io_dataset.html#stat=browser_ver&amp;amp;top=5&amp;amp;type=line&amp;amp;start=20120601&amp;amp;end=20120626&amp;amp;net=n\n\nEt slides ici : \nhttps://assets.en.oreilly.com/1/event/79/Akamai%20Internet%20Insights%20%20Presentation.pptx\n\nLightning Demos, par Marcel Duran (Twitter Inc.), Nat Duca (Google), Lindsey Simon (Twist)\n\n\n\nEnsuite, viennent trois sessions de Lightning Talk : 5 minutes pour présenter un sujet.\n\nOn commence par Marcel Duran, créateur de Yslow, célèbre extension WebPerf de Firebug à l’origine, qui fait son petit bonhomme de chemin depuis :\n\n\n  Disponible dans quasiment tous les browsers\n  Ruleset personnalisable (cf C3PO voir plus bas)\n  Une version en ligne de commande (en Node.Js) pour extraire les données YSlow partir d’un HAR : https://github.com/marcelduran/yslow/wiki/Command-Line-%28HAR%29\n  Un serveur Node.js que vous pouvez tester ici nécessitant aussi un HAR : https://yslow.nodester.com/\n  et le meilleur pour la fin, une version pour Phantom.Js (Projet très impressionnant d’Headless Browser) qui vous permet de simplement mentionner l’url et d’avoir le résultât en sortie ! Avec en plus la possibilité via le format TAP (Test Any Protocol), d’intégrer les résultats dans votre Intégration Continue pour éviter les régressions. Juste ultime, tout est expliqué sur ce Github : https://github.com/marcelduran/yslow/wiki/PhantomJS\n\n\nJ’ai hâte d’implémenter tout ca chez M6Web :) Une vidéo à voir donc absolument :\n\n\n\n\n\nOn continue dans le lourd, avec Nat Duca qui travaille sur le développement du navigateur Chrome et qui nous démontrer une feature très bas niveau mais au combien intéressante : le chrome://tracing/\n\nCette fonctionnalité va vous permettre de profiler les actions du navigateur au plus bas niveau possible. Encore un excellent nouvel ajout au niveau du panel d’outillage du browser Chrome destination des développeurs. Voir vidéo ci dessous :\n\n\n\n\n\nEt pour finir cette jolie session de Lightning Talk, Lindsey Simon, nous présenté Browserscope : https://www.browserscope.org/\n\nOutil dont la puissance et l’interêt pour tout développeurs Front-end Desktop ou Mobile n’est plus démontrer.\n\nSi vous ne connaissez pas, passez 5 minutes de votre temps sur cette vidéo :\n\n\n\n[WebPerf] Browsers, par Luz Caballero (Opera Software), Tony Gentilcore (Google), Taras Glek (Mozilla Corporation)\n\nPetite déception sur cette classique des Vélocity, où les talks ce sont plutôt concentré sur les nouveautés des browsers mobile de Google et Opéra Mini, et où le gars de Mozilla n’a pas jouer le jeu et préféré parler de la lenteur du SetTimeout Javascript ainsi que de l’api LocalStorage …\n\nSlide Mozilla : \nhttps://people.mozilla.com/~tglek/velocity2012/#/step-1\n\nSlide Opéra mini avec notes : \nhttps://www.slideshare.net/gerbille/speed-in-the-opera-mobile-browsers-13476236\n\nConcernant Google, la conférence par Tony Gentilcore (créateur de FasterFox pour ceux qui ce souviennent) était plus intéressante, déja par l’annonce suivante :\n\n\n  Chrome for Android will be the default browser starting with Jelly Bean\n\n\nTony Gentilcore\n\nIl a aussi parlé du fonctionnement de WebKit, du Compositor Thread, ainsi que du Chrome Remote Debugging\n\nPour info, Google a peu de temps après annoncé la présence de Chrome sur iOs !\n\n[DevOps] Simple log analysis and trending, par Mike Brittain (Etsy)\n\n\n\nOn retrouve Mike sur un sujet un peu différent : Comment analyser des logs Apache pour en sortir des graphites. Quelques astuces sur la fonction PHP apache_note() sont mentionnées, sur le traitement des logs avec les commandes linux “awk” et “sed”, et l’utilisation assez étonnante de Gnuplot pour grapher : https://www.gnuplot.info/ !\n\nLes slides sont dispos ici : https://www.mikebrittain.com/blog/2012/06/22/velocity-2012/ , et les codes d’exemples sur Github : https://github.com/mikebrittain/presents\n\nEncore pas mal d’idées piocher ! (Ca commence faire beaucoup d’idées …)\n\n\n\n[WebPerf] Social Button BFFs, par Stoyan Stefanov (Facebook)\n\nStoyan n’est plus présenter dans l’industrie des performances web. Il est désormais chez Facebook, travailler sur les performances des plugins, dont le “Like” ! Suivez le sur Twitter, c’est bourré de superbes infos @stoyanstefanov ainsi que son blog : https://www.phpied.com/ !\n\nL’idée du talk est de faire en sorte que les boutons sociaux (et widgets tiers) en général, deviennent vos BFF ! (Best Friend Forever :D) : https://www.phpied.com/social-button-bffs/\n\nIl explique de quel manière doit-on intégrer ces widgets sur nos sites, et vous permet de le vérifier par l’extension Chrome qu’il a développé 3PO#Fail (3PO = 3rd Party Optimization) ou via une extension de RuleSet pour YSlow.\n\nLes slides ici : https://www.slideshare.net/stoyan/social-button-bffs\n\n\n  “Friends don’t let friends do document.write” Stoyan Stefanov\n\n\n[WebPerf] 5 Essential Tools for UI Performance, par Nicole Sullivan (Stubbornella)\n\nEncore un excellent talk pour ce dernier jour avec Nicolas Sullivan, Experte et consultante dans l’optimisation CSS, sur le fonctionnement très précis de la gestion des CSS par vos navigateurs et toutes les optimisations récentes qu’ils y ont apportées, ainsi qu’une démo (qui fait toujours son petit effet dans une salle Geek) de Tilt sur Firefox\n\nLes slides ne sont malheuresement pas encore en ligne, mais cela ne devrait tarder sur son Slideshare.\n\nVous pouvez retrouvez l’idée du talk sur l’interview ci dessous réalisée elle aussi lors de la Vélocity.\n\n\n\nConclusion\n\nVoil, c’est terminé pour ce compte rendu en 3 actes de ce que j’ai vécu et retenu cette Vélocity Conférence 2012. François prendra le relais pour présenter sa vision d’autres talks, mais orientés Ops (Sysadmin).\n\nJ’espère que ces comptes rendu auront servi à partager quelques outils, liens ou best practices qui vous donnerons des tonnes d’idées de nouvelles choses à faire coté Web dans votre société. De mon coté, comme la Vélocity Berlin l’année dernière, j’ai appris beaucoup et apprécié une grande partie des conférences. Cette conf reste pour moi (et nous chez M6Web) la plus importante au monde sur les aspects de Performance.\n\nPour finir, je vous remercie pour vos retours (et je vous invite à continuer m’en faire un maximum) et lectures. Vous pouvez en connaitre d’avantage sur les autres talks avec quelques vidéos gratuite disponible sur \nhttps://www.youtube.com/playlist?list=PL80D9129677893FD8, Ainsi que les slides qui continuent d’arriver sur\nhttps://velocityconf.com/velocity2012/public/schedule/proceedings\n\nEt pour ceux que ca intéresse, sachez que Oreilly mettra disposition un pack complet des vidéos pour généralement un tarif autour des 400$, et qu’une Vélocity Europe aura lieu Londres les 3 et 4 octobre 2012.\n\nMerci tous !\n\n\n  CR Vélocity Day 1 : https://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n  CR Vélocity Day 2 : https://tech.bedrockstreaming.com/cr-velocity-conference-2012-day-2-devops-webperf\n\n\n(Crédit photo : https://www.flickr.com/photos/oreillyconf/sets/72157630300659948/)\n\n\nLe Job Board assez hallucinant !\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 2 (DevOps/WebPerf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, devops, mobile",
    "url"      : "/cr-velocity-conference-2012-day-2-devops-webperf",
    "date"     : "June 28, 2012",
    "excerpt"  : "Compte rendu des tracks DevOps/WebPerf de cette deuxième journée de cette Vélocity Conférence à Santa Clara (Californie) qui marque l’ouverture “officielle” de la conférence, la veille étant considérée comme des conférences bonus orientées Tutoria...",
  "content"  : "Compte rendu des tracks DevOps/WebPerf de cette deuxième journée de cette Vélocity Conférence à Santa Clara (Californie) qui marque l’ouverture “officielle” de la conférence, la veille étant considérée comme des conférences bonus orientées Tutoriaux.\n\nLa matinée offrait un track unique dans une salle gigantesque.\n\n\n\nVidéo d’intro à la Vélocity !\n\n\n\nL’ouverture officielle est donc présentée par Steve Souders (Google) et John Allspaw (Etsy), toujours dans un show l’américaine, et même déguisés. \n\n\nS’enchaine ensuite un condensé de session plutôt courte par des acteurs très prestigieux du web.\n\n(Crédit photo : https://instagr.am/p/MV5xAAJLSt/ )\n\n[DevOps] Building for a billion Users, par Jay Parikh (Facebook)\n\nPremière conférence du matin, avec une présentation du “VP of Infrastructure Engineering at Facebook”.\n\nOn suis avec attention, une présentation très dense de l’infrastructure de Facebook, avec quelques chiffres hors normes.\n\nLa philosophie Facebook est présentée en 4 points :\n\n\n  Focus on Impact\n  Move Fast\n  Be Bold\n  Be Open\n\n\nAvec une explication sur les fameux Bootcamp cher Facebook, formation obligatoire auquelle tout le monde participe en rentrant chez Facebook.\n\nUne présentation très brève des outils internes utilisés et développés par Facebook : Perflab, GateKeeper (sorte de Feature Flipping), Claspin, Tasks, SevManager …\n\nUne explication sur les procédures de déploiement chez Facebook et leur gestion du cache, sur leur volonté de constamment tout refaire, pour toujours être meilleur.\n\n\nEt pour finir, une anecdote assez drôle sur un incident ayant eu lieu chez Facebook, où toutes les fonctionnalités “non terminées” se sont un jour retrouvées en production.\n\nBref, une conférence intéressante, mais très dense, dont je vous conseille de regarder la vidéo çi dessous.\n\n\n  “Fix More, Whine less.” Jay Parikh\n\n\n\n\n[DevOps] Investigating Anomalies, par John Rauser (Amazon)\n\nBelle surprise de la journée, avec cette conférence sur la gestion d’incident, qui raconte l’histoire de l’épidémie de Cholera ayant eu lieu à Londres en 1854, et comment John Snow, trouver l’origine de l’épidémie, en se concentrant sur les données, et non pas seulement sur les chiffres.\n\n\n  “Explaining anomalies often makes your theroy bulletproof” John Rauser\n\n\nUne deuxième partie était concentrée sur le fait d’étudier les extremités sur vos échantillons de manière à trouver ce qui n’allait pas. Point de vue très instructif.\n\nLa vidéo ci dessus est un “must-see” de la Vélocity.\n\n\n  “Look at the extremes and you’ll find things that are broken” John Rauser\n\n\n\n\n[DevOps] Building Resilient User Experiences, par Mike Brittain (Etsy)\n\nLe message autour de cette présentation, est que votre application DOIT s’adapter aux incidents. Si possible faire en sorte que cela ne soit même pas percu par la plupart de vos internautes. En découpant chacune des fonctionnalités de votre site, vous devez pouvoir ne pas afficher celle qui ne fonctionne pas correctement sans que cela impact vos utilisateurs (Graceful Degradation).\n\nLes slides sont disponible ici : https://www.slideshare.net/mikebrittain/building-resilient-user-experiences-13461063\n\n\n\n[WebPerf] Predicting User Activity to Make the Web Fast, par Arvind Jain (Google), Dominic Hamon (Google)\n\nLa présentation commence avec un rappel sur “How Fast is the web today”. \n En quelques chiffres :\n\n\n  Chrome ~2.3s/5.4s page load time (median/mean)\n  Google Analytics ~2.9s/6.9s page load time (median/mean)\n  Mobile ~4.3s/10.2s page load time (median/mean)\n\n\nD’autres infos sont partagées venant du très utile HttpArchive …\n\nOn assiste ensuite la présentation des fonctionnalités de “Prefetch” de google et du Prerendering en place dans la barre de recherche de Chrome : “Omnibox”, ceci ayant pour but de rendre le web encore plus rapide.\n\nTout cela donne des idéées sur la façon de prédire ce que vont faire les internautes, et sur nos gestions d’“autocomplete”.\n\n\n\n\n\n[WebPerf] Performance Implications of Responsive Web Design, par Jason Grigsby (Cloud Four)\n\nUne autre conférence que j’attendais grandement, sur le Responsive Web Design. Le sujet est parfaitement maitrisé, et tous les reproches que je peux faire cette techno en ce moment, sont mentionnés, expliqués, et certaines solutions ou idées sont données ! Du tout bon.\n\nA retenir, la méthode conseillée qui est de faire du Mobile First Responsive Web design, c’est dire commencer par la version mobile, puis faire la version web, et non l’inverse.\n\nLes slides ici : https://speakerdeck.com/u/grigs/p/performance-implications-of-responsive-design\n\nLa conférence n’étant pas disponible en vidéo, vous pouvez déjécouter Jason Grisby lors d’une interview suite sa conférence en vidéo si dessous.\n\n(Crédit photo : https://www.flickr.com/photos/stuart-dootson/4024407198/ )\n\n\nJason Grigsby interview la Vélocity Conf 2012\n\n\n\n[WebPerf] RUM for Breakfast - Distilling Insights From the Noise, par Buddy Brewer (LogNormal), Philip Tellis (LogNormal, Inc) &amp;amp; Carlos Bueno (Facebook)\n\nRUM aka Real User Monitoring est un terme qui est revenu très régulièrement durant cette Velocity. Nous avions pour cette conférence notamment, deux personnes de LogNormal dont le créateur de Boomerang.js : https://yahoo.github.com/boomerang/doc/ et Carlos Bueno de Facebook.\n\nLa présentation expliquait comment mesurer des métriques de performances venant d’utilisateurs réel, comment analyser toutes les données, en filtrer le “bruit”, et quoi en tirer. Le tout était très instructif, surtout sur la partie filtrage de données (Band Pass Filtering, IQR Filtering ..).\n\nSlides : https://www.slideshare.net/buddybrewer/rum-for-breakfast-distilling-insights-from-the-noise\n\n\n\n[WebPerf] Rendering Slow? Too Much CSS3? Ask RSlow, par Marcel Duran (Twitter Inc.), David Calhoun (CBS Interactive)\n\nOn retrouve ici une présentation assez fun du résumé de la conf sous forme de Waterfall (voir photo).\n\nLe talk a ensuite abordé les notions de rendering au niveau CSS avec au départ un cas d’étude : Réaliser le logo de ySlow en CSS3 entièrement. On observe de manière assez drôle le rendu finale dans les différents navigateurs (éclat de rire général sur IE off course). Vous pouvez les retrouver sur les slides ci dessous.\n\nLa conférence part ensuite sur quelques tests réalisés sur Chrome uniquement (à prendre donc avec des pincettes) sur les performances CSS3 de chacun de ces cas :\n\n\n  background-image vs css3 gradient\n   vs css background-image\n  @font-face vs  vs sans-serif\n  animated gif vs css3 spinner\n\n\nL’étude est intéressante, et mériterais d’être creusée sur d’autres navigateurs, mais cela est rendu très difficile par le fait que seul Chrome sait exporter les données de rendu de sa Timeline …\n\nLes slides sont dispo ici : \nhttps://docs.google.com/presentation/d/1b7rdeXYdmL3lmT24GAaC14eOSkq5qt6FM-yLSeFykQk/edit?pli=1\n\n[WebPerf] Time To First Tweet, par Dan Webb, par (Twitter Inc) &amp;amp; Rob Sayre (Twitter)\n\nDan et Rob nous parle performances coté client chez Twitter, et la réecriture du Front-End.\n\nLa notion de Time To First Tweet, correspond au temps de navigation jusqu’a l’affichage du premier twiit sur la Timeline. Cette mesure est prise grace à la Navigation Timing Api, supportée dans IE&amp;gt;=9, Firefox &amp;amp; Chrome notamment.\n\nTwitter à aussi abandonné progressivement, l’utilisation des hashbangs (les #! dans l’url), pour utiliser la PushState Api, ainsi que le templating coté client (Mustache.js et Hogan.js) pour repasser sur un templating serveur avec leur migration de Ruby vers Java, avec au final 75% de temps client gagné sur le 95th Percentile !\n\nConférence très intéressante, notamment sur la manière de charger les Javascripts.\n\nPlus de détail sur le blog technique de Twitter : https://engineering.twitter.com/2012/05/improving-performance-on-twittercom.html\n\nLes slides sont disponible : https://speakerdeck.com/u/danwrong/p/time-to-first-tweet\n\nConclusion Day 2 :\n\nEncore une journée riche en informations et idées. Le rythme étant beaucoup plus soutenu, et la fatigue s’accumulant, il n’était pas évident d’être à 100% dans chaque talk :-)\n\nEn attendant le CR Ops, et celui du Day 3, vous pouvez relire le CR du Day 1 : \nhttps://tech.bedrockstreaming.com/cr-velocity-conference-day-1-dev-webperf\n\nP.S : Retrouvez moi sur twitter : @kenny_dee\n\nPlaylist Youtube Velocity US 2012\n"
} ,
  
  {
    "title"    : "CR Velocity Conference 2012 : Day 1 (Dev/Webperf)",
    "category" : "",
    "tags"     : " velocity, conference, webperf, mobile",
    "url"      : "/cr-velocity-conference-day-1-dev-webperf",
    "date"     : "June 27, 2012",
    "excerpt"  : "Nous voici à Santa Clara, CA, ce lundi 25 juin pour notre première Velocity Conference (Web Performance &amp;amp; Operations) aux states (nous avions déjà pu avoir un aperçu l’année dernière avec la première Velocity Europe Berlin).\n\n\n\nL’évenement se ...",
  "content"  : "Nous voici à Santa Clara, CA, ce lundi 25 juin pour notre première Velocity Conference (Web Performance &amp;amp; Operations) aux states (nous avions déjà pu avoir un aperçu l’année dernière avec la première Velocity Europe Berlin).\n\n\n\nL’évenement se situe au Convention Center, et la première chose que nous remarquons, c’est la taille démesurée du lieu ! Et pour cause, 800 personnes sont attendues !\n\nPour cette première journée sous le signe des tutoriaux, il y avait entre trois et quatre tracks parallèles de 90 minutes chacun, dont un réservé aux sponsors. Dur de faire des choix parmi toutes les confs et le programme alléchant de la journée !\n\nJe m’oriente donc sur le coté Dév / WebPerf / Monitoring, pendant que mon collègue, Francois, part sur le coté operations qu’il couvrira dans une autre série de CR.\n\n[WebPerf] Understanding and Optimizing Web Performance Metrics, par Bryan McQuade de chez Google\n\nAu menu :\n\n\n  Explication des métriques de performance orientées réseau\n  Fonctionnement du parser HTML5\n  Explication des métriques de performance orientées rendu\n  Démo de Critical Path Explorer (PageSpeed Online)\n  Optimisation de l’affichage perçu utilisateur\n\n\nLes slides parlent d’elles même et sont disponibles ici : \nhttps://perf-metrics-velocity2012.appspot.com .\n\nElles parcourent l’intégralité des notions de WebPerf existantes “à jour”, dont certaines peu connues comme la Speculative loading, et permettent surtout de comprendre ce qu’elles signifient très précisément.\n\nA retenir aussi le SSL Server Test ici https://www.ssllabs.com/ssltest/\n\nC’est donc un Must Read pour tout ceux que la WebPerf intéresse.\n\n**Page Speed Insights : **\n\nNous avons eu droit ensuite une démo très intéressante de la fonctionnalité Critical Path Explorer (déjà entraperçue en version béta à la Velocity Europe), et qui sera je pense lancée officiellement demain.\n\nEn attendant et pour la tester : \nhttps://developers.google.com/speed/pagespeed/insights?velocity=1\n\nCette fonctionnalité permet, comme son nom l’indique, de montrer le chemin critique de votre page. Sur les quelques tests que j’ai pu effectués, c’est très efficace. On apprécie le détail au niveau du waterfall sur l’exécution des javascripts, l’affichage de “qui bloque quoi”, ou le render css. A approfondir de toute urgence !\n\n\n\nBryan McQuade (Google)\n\n\nLa Lightning Démo de Page Speed ayant eu lieu le lendemain\n\n[WebPerf] A Web Perf Dashboard: Up &amp;amp; Running in 90 Minutes, par Cliff Crocker et Aaron Kulick.\n\nL’idée ici était de montrer en 90 minutes avec quels outils obtenir un dashboard orienté WebPerf, qui sera fourni comme une VM à la fin de la session.\n\nAprès une longue présentation orale d’outils plutôt connus désormais comme :\n\n\n  Boomerang.Js\n  WebPageTest instance privée + API\n  Piwik (clone de Google Analytics)\n  StatsD (collecteur pour Graphite)\n  Graphite\n  REDbot.org\n  cUrl\n  ShowSlow\n  …\n\n\nLes deux conférenciers nous présentent un site réalisé pour l’occasion : “Sally Squirrel’s Dance Emporium”, hommage aux gifs animés d’écureuils dansant, et nous font une démo (un peu capricieuse) de leur dashboard basé sur Piwik, alternative Google Analytics avec un système de “plugin” visiblement pour aggréger un peu le tout.\n\nL’idée est clairement bonne, le résultat ne m’a pas convaincu titre personnel. On vante au départ de la présentation, le faite qu’une image bien choisie suffit au monitoring, et qu’un dashboard ne doit pas être complexe, et au final, on se retrouve avec un dashboard remplis d’images en tout genre, de données tabulaires, … complexe quoi …\n\nPiwik en alternative Analytics ?\n\nJe doute aussi de la robustesse de Piwik que nous avions déjà étudier, et voir des pages listant les temps de latence ou de chargement utilisateur par utilisateur, me fait réellement peur avec une audience dépassant la centaine de personnes la journée …\n\nPour l’anecdote, sur le wiki de Piwik, on lit cette phrase que je vous laisse apprécier : “If your website has more than a few hundreds visits per day (bravo!), waiting for Piwik to process your data may take a few minutes”\n\nJe vous invite tout de même à lire les slides : \n[https://assets.en.oreilly.com/1/event/79/A%20Web%20Perf%20Dashboard_%20%20Up%20%20Running%20in%2090%20Minutes%20Presentation.pptx](https://assets.en.oreilly.com/1/event/79/A%20Web%20Perf%20Dashboard%20%20Up%20_%20Running%20in%2090%20Minutes%20Presentation.pptx)\n\nAinsi qu’à tester la VM mise à disposition, car le travail derrière est conséquent, et peut correspondre à certains, ou peut au moins donner des idées pour d’autres : https://t.co/uLv1fX1A\n\n**A retenir : **\n\nA retenir aussi dans cette présentation, tout le bien qui a été dit de Graphite (même si je ne suis plus convaincre la dessus), et de quelques features pour lesquelles j’étais passée travers :\n\n\n  Support du SVG ( &amp;amp;format=svg) qui va enfin nous permettre de tester l’enrichissement des graphs par du contenu “connexe” (liste des erreurs 404 sur le graph lorsque l’on clic sur un point, nom du développeur ayant fait la mise en production etc)\n  Les fonctions de HoltWinter afin d’avoir des tendances hautes et basses pour mieux savoir quand alerter par exemple.\n\n\n[WebPerf] How to Walk Away From Your Outage Looking Like a HERO par Teresa Dietrich (WebMD), Derek Chang (WebMD)\n\nL’une des conférences que j’attendais beaucoup : le titre annonçait un talk sur la gestion d’incident avec un coté humoristique.\n\nLes deux conférenciers ont présentés des templates très complet de gestion d’incident qu’ils réalisent pour des posts-mortems qu’on peut retrouver ici : https://www.teresadietrich.net/?page_id=37\n\nPersonnellement, il me parait très important de réaliser des posts mortems. Mais si c’est pour passer plus de temps à rédiger des rapports d’incidents trop complet qu’on ne relira jamais qu’a en tirer un quelconque bénéfice, cela me parait inutile.\n\nDu coup, la première demi-heure a consisté à présenter ces templates, lire et expliquer quelques incidents ayant eu lieu chez WebMD.\n\nJ’ai, comme une bonne partie de la salle, fait l’impasse rapidement : entre le sujet dans lequel je ne suis jamais rentré ainsi que des slides avec beaucoup de texte illisible passés les premiers rangs de la très grande salle, je n’ai pas accroché.\n\nA revoir tête reposée : https://velocityconf.com/velocity2012/public/schedule/detail/23615 (slides non dispo à l’heure actuelle)\n\n\n\nTeresa Dietrich (WebMD), Derek Chang (WebMD)\n\n[WebPerf] The 90-minutes Mobile optimization life cycle par Hooman Beheshti (VP strangeloop)\n\nConférence orientée WebPerf Mobile.\n\nIci, on retrouve tout ce que j’aime dans les conférences Vélocity :\n\n\n  Un conférencier avec un grand talent d’orateur : précis, drôle et captivant\n  Un sujet très maitrisé\n  Des slides propres et parlantes même sans avoir assisté au talk\n  Des débats lancés …\n\n\nL’idée était de partir d’un site, au hasard Oreilly.com, puis le site mobile velocityconf.com par la suite, et de démontrer les étapes d’optimisation WebPerf, étape par étape, avec à chaque fois, ce que l’on souhaite obtenir, ce que l’on obtient réellement, et une comparaison vidéo du changement.\n\nDes points ont été approfondis comme la gestion du cache, via LocalStorage, du fonctionnement des CDN (pour le mobile), du Pipellining HTTP, de la congestion TCP etc …\n\nBeaucoup d’outils ont aussi été mentionnés pour la WebPerf mobile :\n\n\n  Chrome remote debugging : Http://developers.google.com/chrome/mobile/docs/debugging/\n  iWebInspector for IOS simulator : www.iwebinspector.com\n  Weinre : Remote debugging from the desktop for what the phone is doing : https://people.apache.org/~pmuellr/weinre/\n  Aardwolf : Remote js debugging lexandera.com/aardwolf\n  Mobile Perf Bookmarklet : stevesouders.com/mobileperf/mobileperfbkm.php\n  Pcap2har : Turn packet captures to waterfalls https://pcapperf.appspot.com\n  …\n\n\nBref, un très bon panorama pour la WebPerf mobile avec deux cas concret d’étude, chacun avec deux approches différentes :\n\n\n  Améliorer les métriques de WebPerf pour le site d’Oreilly\n  Améliorer la perception utilisateur sans regarder les métriques pour le site mobile de la Vélocityconf\n\n\nSlides dispo ici : https://www.strangeloopnetworks.com/blog/the-90-minute-mobile-optimization-life-cycle/\n\nJe vous invite aussi à regarder son interview ci dessous.\n\n\n\n\n\n[Event] Akamai Pool Party\n\nCette journée touche à sa fin avec une Pool Party extérieure par Akamai avec un orchestre (qui nous joué notamment le thème de Mario ! Très fun), beaucoup à boire, et beaucoup à manger (légumes tremper dans du brie chaud, WTF ?). L’occasion de rencontrer quelques sponsors et 2 autres français. :-)\n\n\nDevOps drinking session / @jstinson\n\n[Event] Ignite Sessions\n\nA 19h30 avait lieu les Ignite sessions, des confs “lightning talks” de 5 minutes sur des sujets divers, certains très intéressant comme :\n\n\n  le “Perceptual Diff” par un ingénieur de chez Google, pour être alerté (par l’intégration continue) lorsque la page du Service customers de chez Google change. Photos de la présentation ici + https://pdiff.sourceforge.net/\n  les #lolops, avec une série de Twitt orientée Devops à mourir de rire : Voir ici https://www.slideshare.net/cwestin63/lolops-a-years-worth-of-humorous-engineering-tweets\n  …\n\n\nLe concept est vraiment efficace avec des speakers ultra dynamique et pour la majorité très drôle.\n\nMention spéciale pour la partie centrale, où 11 personnes de la salle (dont certains speakers, Allspaw et Souders en tête), avait 1 à 2 minutes pour improviser sur des slides plutôt très drôle qu’ils n’avaient jamais vu.\n\nJ’espère que les vidéos seront disponible car c’était juste hilarant au possible. Je ne m’attendais pas à pleurer de rire non stop ici :-)\n\n\n\nConclusion\n\nExcellente première journée, déjà des tonnes d’infos à condenser / retenir, et ce n’était que le premier jour !\n\nSinon l’organisation est impeccable, lieu exceptionnel, wifi public qui fonctionne, repas de très bonne qualité (et table qui plus est), pas mal de multiprises dans les salles, à boire à volonté … La grande classe !\n\nLes comptes rendus des prochaines journées et des sessions orientés Ops suivre ;-)\n\nN’hésitez pas à faire un maximum de retour sur ce compte rendu, cela nous aidera et nous motivera pour les prochains ;-)\n\nP.S: Retrouvez moi sur Twitter : @kenny_dee\n\n"
} ,
  
  {
    "title"    : "Monitoring applicatif : Pourquoi et comment ?",
    "category" : "",
    "tags"     : " monitoring, graphite, statsd, conference",
    "url"      : "/monitoring-applicatif-pourquoi-et-comment",
    "date"     : "June 26, 2012",
    "excerpt"  : "Voici les slides de la présentation que j’ai donnée au Forum PHP 2012, et au WebEvent 4 :\n\nVous êtes développeur, chef de projet technique ou même responsable et vous souhaitez avoir de la visibilité sur le fonctionnement de vos applicatifs, ou su...",
  "content"  : "Voici les slides de la présentation que j’ai donnée au Forum PHP 2012, et au WebEvent 4 :\n\nVous êtes développeur, chef de projet technique ou même responsable et vous souhaitez avoir de la visibilité sur le fonctionnement de vos applicatifs, ou sur la plateforme sur laquelle ils sont hébérgés ?\n\nNous étudierons comment, grâce à des outils simples (StatD / Graphite / Log BDD) et nos expériences chez M6Web, mettre en place un monitoring applicatif ultra complet.\n\nCe monitoring vous permettra de retrouver la vue sur vos projets, pour mieux anticiper la charge, detecter la root cause en cas d’incident et connaitre l’état de chacun de vos services ..\n\nMonitoring applicatif : Pourquoi et comment ?\n\n \n"
} ,
  
  {
    "title"    : "M6Web au Web Event Lyon #4",
    "category" : "",
    "tags"     : " webevent",
    "url"      : "/m6web-au-web-event-lyon-4",
    "date"     : "June 20, 2012",
    "excerpt"  : "\nUne partie de l’équipe de M6 Web au webevent de La ferme du Web.\n\n",
  "content"  : "\nUne partie de l’équipe de M6 Web au webevent de La ferme du Web.\n\n"
} ,
  
  {
    "title"    : "M6 Web était présent au Forum PHP 2012",
    "category" : "",
    "tags"     : " afup, forumphp, conference",
    "url"      : "/post/24732185644/m6-web-tait-pr-sent-au-forum-php-2012",
    "date"     : "June 9, 2012",
    "excerpt"  : "Voici quelques photos des membres d’M6Web prises lors du Forum PHP 2012.\n\nKenny a animé une conférence sur le monitoring applicatif.\n\n\n\nMartin, notre CTO, a participé une table ronde.\n\n\n\nOlivier était également présent en tant que membre de l’Afup...",
  "content"  : "Voici quelques photos des membres d’M6Web prises lors du Forum PHP 2012.\n\nKenny a animé une conférence sur le monitoring applicatif.\n\n\n\nMartin, notre CTO, a participé une table ronde.\n\n\n\nOlivier était également présent en tant que membre de l’Afup\n\n\n\nAinsi que Didier et Julien, développeurs dans nos équipes R&amp;amp;D, et trop préssés d’assiter toutes les conférences pour être photographiés.\n\n"
} ,
  
  {
    "title"    : "M6Web au Forum PHP 2012 et au WebEvent #4",
    "category" : "",
    "tags"     : " php, afup, monitoring, conference",
    "url"      : "/post/24184111542/m6web-au-forum-php-2012-et-au-webevent-4",
    "date"     : "June 1, 2012",
    "excerpt"  : "Cette année, M6Web sponsorise deux événements français majeurs dans le monde du Web :\n\n\n  le Web Event à Lyon #4 (https://event.lafermeduweb.net/ au centre de congrès de Lyon le 15 juin 2012)\n  et le forumPHP 2012 (https://afup.org/pages/forumphp2...",
  "content"  : "Cette année, M6Web sponsorise deux événements français majeurs dans le monde du Web :\n\n\n  le Web Event à Lyon #4 (https://event.lafermeduweb.net/ au centre de congrès de Lyon le 15 juin 2012)\n  et le forumPHP 2012 (https://afup.org/pages/forumphp2012/ la Cité universitaire à Paris le 5 &amp;amp; 6 juin 2012) !\n\n\nA cette occasion, M6Web sera bien représenté :\n\n\n  je (Kenny Dits) présenterais chaque évènement des sessions sur le monitoring applicatif en regard de ce que nous faisons au quotidien chez M6Web. https://afup.org/pages/forumphp2012/sessions.php#632\nhttps://event.lafermeduweb.net/les-sessions#c6\n  Olivier Mansour en tant que Vice Président de l’Afup parlera lors de la Keynote de cloture du Forum Php le 6 juin. https://afup.org/pages/forumphp2012/sessions.php#732\n  Martin Boronski, notre directeur technique participera également à la table ronde DSI organisée par l’AFUP Paris le 6 Juin.\n\n\nRendez-vous là bas ? ;-)\n\nBannière du Forum PHP 2012\n\n[\n\nBannière du Web Event Lyon\n\n[\n\n"
} ,
  
  {
    "title"    : "Performances PHP chez M6Web",
    "category" : "",
    "tags"     : " graphite, monitoring, nodejs, php, varnish, webperf, conference",
    "url"      : "/post/23671071384/performances-php-chez-m6web",
    "date"     : "May 24, 2012",
    "excerpt"  : "Voici les slides de la présentation du 23 mai réalisée à l’Epitech de Lyon.\n\nC’est un retour d’expérience, qui survole un peu tous les axes sur lesquels nous travaillons chez m6web, ayant trait aux optimisations de nos sites.\n\nJ’espère que certain...",
  "content"  : "Voici les slides de la présentation du 23 mai réalisée à l’Epitech de Lyon.\n\nC’est un retour d’expérience, qui survole un peu tous les axes sur lesquels nous travaillons chez m6web, ayant trait aux optimisations de nos sites.\n\nJ’espère que certains points feront l’objet d’autres articles dans le futur ;-)\n\nPerformances php chez M6Web\n&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;&amp;lt;/param&amp;gt;\n\n"
} ,
  
  {
    "title"    : "Lancement du blog technique d&#39;M6Web",
    "category" : "",
    "tags"     : " ",
    "url"      : "/post/23664141031/lancement-du-blog-technique-dm6web",
    "date"     : "May 24, 2012",
    "excerpt"  : "Bienvenue sur le blog de la direction technique de M6 Web.\n\nVous retrouverez ici, une fréquence qu’on espère des plus régulières, quelques articles et autres retours d’expérience de nos équipes technique.\n\nAttendez vous à manger du PHP, Mysql, Nod...",
  "content"  : "Bienvenue sur le blog de la direction technique de M6 Web.\n\nVous retrouverez ici, une fréquence qu’on espère des plus régulières, quelques articles et autres retours d’expérience de nos équipes technique.\n\nAttendez vous à manger du PHP, Mysql, Node.js, entendre parler de performance, monitoring, vidéo, html5 etc ;-)\n\nBonne lecture à tous.\n\n"
} 
  
  
  
]
