---
layout: ../../layouts/post.astro
title: "Bonnes pratiques pour la maintenance d'une application web"
description: "Comment ne pas jeter son projet au bout de 2 ans ? Retour d'exp√©rience bas√© sur les bonnes pratiques appliqu√©es √† la plateforme web d√©velopp√©e chez Bedrock Streaming."
author: a_caron
category:
tags: [js, react, web, frontend]
feature-img: "../../../../images/posts/bonnes-pratiques-web/bedrock.jpg"
comments: true
canonical: "https://slashgear.github.io/fr/comment-ne-pas-jeter-son-application-au-bout-de-deux-ans/"
language: fr
redirect_from:
  - /bonnes-pratiques-web/
---

> Comment ne pas jeter son application tous les deux ans ?

_Retour d'exp√©rience bas√© sur les bonnes pratiques appliqu√©es √† la plateforme web d√©velopp√©e chez [Bedrock Streaming](https://www.bedrockstreaming.com/)_

## Un peu de contexte

Chez Bedrock Streaming de nombreuses √©quipes d√©veloppent et maintiennent des applications _frontend_ pour nos clients et utilisateurs.
Certaines ne sont pas toute jeune.
En effet, l'application sur laquelle je travaille principalement est un site web dont les d√©veloppements ont commenc√© en 2014.
Je l'ai d'ailleurs d√©j√† √©voqu√©e dans diff√©rents articles de [ce blog](https://slashgear.github.io/).

![impression d'√©cran du nombre de commit sur master de notre projet 15668](../../../../images/posts/bonnes-pratiques-web/commit-count.png)

Vous pourriez vous dire: _"Oh les pauvres maintenir une application vieille de presque 10 ans √ßa doit √™tre un enfer !"_

Rassurez-vous, ce n'est pas le cas !
J'ai travaill√© sur des projets bien moins vieux mais sur lesquels le d√©veloppement de nouvelles fonctionnalit√©s √©tait bien plus p√©nible.

Aujourd'hui le projet reste √† jour techniquement, on doit √™tre sur la derni√®re version de React alors que celui-ci avait commenc√© sur une version _0.x.x_.
Dans ce monde des technologies web souvent d√©cri√© (ex: les nombreux articles sur la _Javascript Fatigue_) dont les outils et les pratiques √©voluent constamment, conserver un projet "√† jour" reste un vrai challenge.

![nombre de versions de l'application 1445](../../../../images/posts/bonnes-pratiques-web/releases.png)

De plus, dans le contexte de ce projet, en presque 10 ans, nous avons connu une centaine de contributeurs.
Certains ne sont rest√©s que quelques mois/ann√©es.
Comment garder au maximum la connaissance sur "Comment on fait les choses et comment √ßa marche ?" dans un contexte humain si mouvant ?

![liste des 100 contributeurs du projet](../../../../images/posts/bonnes-pratiques-web/contributors.png)

C'est ce que je vous propose de vous pr√©senter.

Avec l'aide de mes coll√®gues, j'ai rassembl√© la liste des bonnes pratiques qui nous permettent encore aujourd'hui de maintenir ce projet en √©tat.
Avec [Florent Dubost](https://twitter.com/fooragnak), on s'est souvent dit qu'il serait int√©ressant de la publier.
Nous esp√®rons que cela vous sera utile.

## S'imposer des r√®gles et les automatiser

Un projet qui r√©siste au temps c'est tout d'abord un ensemble de connaissances qu'on empile les unes sur les autres.
C'est en quelque sorte la tour de Kapla que vous assembliez petit en essayant d'aller le plus haut possible.
Une base solide sur laquelle on esp√®re pouvoir ajouter le plus possible avant une potentielle chute.

D√®s le d√©but d'un projet on est donc amen√© √† prendre des d√©cisions importantes sur "Comment on souhaite faire les choses ?".
On pense par exemple √† "Quel format pour nos fichiers ? Comment on nomme telle ou telle chose ?"
√âcrire une documentation pr√©cise de "Comment on fait les choses" pourrait paraitre une bonne id√©e.

Cependant la documentation c'est cool, mais √ßa a tendance √† p√©rimer tr√®s vite.
Nos d√©cisions √©voluent mais pas la documentation.

> "Les temps changent mais pas les README."
>
> [_Olivier Mansour (deputy CTO √† Bedrock)_](https://twitter.com/omansour)

Automatiser la v√©rification de chacune des r√®gles qu'on s'impose (sur notre codebase ou nos process) est bien plus p√©renne.
Pour faire simple, on √©vite dans la mesure du possible de dire "On devrait faire les choses comme cela", et on pr√©f√®re "on va coder un truc qui nous le v√©rifie √† notre place".
En plus de √ßa, cot√© JS on est vraiment bien √©quip√© avec des outils comme [Eslint](https://eslint.org/) qui nous permettent d'impl√©menter nos propres r√®gles.

Le r√©flexe qu'on essaie donc d'adopter est donc le suivant:

- "On devrait essayer de faire comme cela √† pr√©sent !"
- "Ok c'est int√©ressant, mais comment peut-on s'assurer qu'on fasse comme cela automatiquement avec notre CI (Int√©gration continue) ?"

L'int√©gration continue d'un projet est la solution parfaite pour ne rien louper sur chacune des _Pull Request_ que nous proposons.
Les reviews n'en sont que plus simples car vous n'avez plus √† vous soucier de l'ensemble des r√®gles qui sont d√©j√† automatis√©es.
Dans ce mod√®le, la review sert donc plus au partage de connaissance qu'au flicage de typo et autre non respect des conventions du projet.

Dans ce principe, il faut donc essayer de bannir les r√®gles orales.
Le temps des druides est termin√©, s'il faut transmettre oralement toutes les bonnes pratiques d'un projet, l'accompagnement de nouveaux d√©veloppeurs dans votre √©quipe n'en sera que plus long.

![la recette de la potion magique de panoramix est perdue car secr√®te](../../../../images/posts/bonnes-pratiques-web/panoramix.gif)

Un projet n'est pas fig√©. Ces r√®gles √©voluent donc avec le temps.
On pr√©f√®rera alors l'ajout de r√®gles qui poss√®dent un script qui _autofixera_ toute la codebase intelligemment.
De nombreuses r√®gles Eslint le proposent, et cela est vraiment un crit√®re de s√©lection tr√®s important dans nos choix de nouvelles conventions.

```shell
eslint --fix
```

Une r√®gle tr√®s stricte qui vous obligera √† modifier votre code manuellement avant chaque push est p√©nible √† la longue et √©nervera vos √©quipes.
Alors qu'une r√®gle (m√™me tr√®s stricte) qui peut s'autofixer automatiquement au moment du commit ne sera pas per√ßue comme g√™nante.

**Comment d√©cider d'ajouter de nouvelles r√®gles ?**

Cette question peut paraitre √©pineuse, prenons par exemple le cas des `<tab>` / `<space>` dans les fichiers.
Pour cela, on essaie d'√©viter les d√©bats sempiternels et on se plie √† la tendance et aux r√®gles de la communaut√©.
Par exemple, [notre base de configuration Eslint](https://github.com/BedrockStreaming/eslint-tools)) est bas√©e sur celle d'Airbnb qui semble avoir un certain succ√®s dans la communaut√© JS.
Mais si la r√®gle qu'on souhaite s'imposer n'est pas disponible dans Eslint ou d'autres outils, il nous arrive de pr√©f√©rer ne pas suivre la r√®gle plut√¥t que de se dire "On le fait sans CI qui v√©rifie".

### La liste _presque_ exhaustive ü§û

![Notre workflow d'int√©gration continue](../../../../images/posts/bonnes-pratiques-web/ci-workflow.png)

- Le format des fichiers est suivi g√©r√© par [Editorconfig](https://editorconfig.org/), [prettier](https://prettier.io/) et [Eslint](https://eslint.org/).
  Nous avons opensourc√© [notre propre configuration](https://github.com/BedrockStreaming/eslint-tools), si jamais celle-ci peut vous √™tre utile.
- Nous utilisons un [nommage de commit bien sp√©cifique](https://www.conventionalcommits.org/en/v1.0.0/) pour g√©n√©rer nos changelog.
  Pour s'assurer que les devs le respectent, une simple √©tape de notre CI le v√©rifie.
- On ne souhaite pas qu'un dev fasse grossir √©norm√©ment nos bundles JS en production, c'est pourquoi nous suivons et mesurons leur taille dans la CI.
  On utilise un outil maison mais on peut vous recommander l'outil [BuildTracker](https://buildtracker.dev/).
- La couverture de tests n'est pas un indicateur pour l'√©quipe, toutes les lignes n'ont pas la m√™me n√©cessit√© pour nous d'√™tre test√©es.
  Certaines √©quipes √† Bedrock suivent cependant cet indicateur qui a au moins l'int√©r√™t de donner une tendance.
- Nos tests unitaires tournent bien √©videmment sur la CI, ceux-ci doivent passer.
- Nos tests fonctionnels (End to end: E2E) tournent sur Chrome Headless, ils doivent √™tre au vert.
- Les logs de nos tests E2E sont r√©cup√©r√©s et pars√©s afin d'√©viter l'introduction d'erreur ou de React warning (Le script de parsing est cependant compliqu√© √† maintenir)
- Les tests fonctionnels fonctionnent dans une _sandbox_ o√π tout le r√©seau est proxyfi√©.
  Nous surveillons que nos tests ne d√©pendent pas d'une API non mock√©e qui pourrait ralentir leur ex√©cution.
- Durant les tests E2E nous v√©rifions qu'aucune requ√™te d'image n'a g√©n√©r√© une 404.
- On r√©alise quelques [v√©rifications d'accessibilit√© avec Axe](https://www.deque.com/axe/) durant nos tests E2E.
- On v√©rifie quelques r√®gles sur le CSS avec [Stylelint](https://stylelint.io/) et [bemlinter](https://github.com/BedrockStreaming/bemlinter) (on n'utilise plus BEM aujourd'hui mais il reste encore un peu de style g√©r√© en SCSS qu'on migre petit √† petit en StyledComponent)
- Le projet est un monorepo sur lequel nous essayons de maintenir les m√™mes versions de d√©pendances pour chaque package.
  Pour cela nous avons d√©velopp√© un outil qui permet de faire cette v√©rification _[monorepo-dependencies-check](https://www.npmjs.com/package/monorepo-dependencies-check)_
- On v√©rifie que notre fichier `yarn.lock` n'a pas √©t√© modifi√© par inadvertance ou bien qu'il a √©t√© mis √† jour par rapport aux modifications du `package.json`.
- [Terraform](https://www.terraform.io/) est utilis√© pour la gestion de nos ressources cloud, nous v√©rifions que le format des fichiers est correct.

## Tester, tester, tester

J'esp√®re qu'en 2021 il n'est plus n√©cessaire d'expliquer pourquoi tester automatiquement son application est indispensable pour la rendre p√©renne.
En JS on est plut√¥t bien √©quip√© en terme d'outils pour tester aujourd'hui.
Il reste cependant l'√©ternelle question:

> "Qu'est-ce qu'on veut tester ?"

Globalement si on recherche sur internet cette question, on voit que des besoins diff√©rents font √©merger des pratiques et des outils de testing bien diff√©rents.
Ce serait tr√®s pr√©somptueux de penser qu'il y a une bonne mani√®re de tester automatiquement son application.
C'est pourquoi il est pr√©f√©rable de d√©finir une ou plusieurs strat√©gies de test qui r√©pondent √† des besoins d√©finis et limit√©s.

Nos strat√©gies de tests reposent sur deux volont√©s bien distinctes:

- Automatiser la v√©rification des fonctionnalit√©s propos√©es aux utilisateurs en se mettant √† sa place.
- Nous fournir des solutions efficaces pour specifier la mani√®re dont nous impl√©mentons nos solutions techniques pour nous permettre de les faire √©voluer plus facilement.

Pour cela, nous r√©alisons deux "types de tests" que je propose de vous pr√©senter ici.

### Nos tests E2E

On les appelle "tests fonctionels", ce sont des tests End-to-end (E2E) sur une stack technique tr√®s efficace compos√©e de [CucumberJS](https://cucumber.io/docs/installation/javascript/), [WebdriverIO](https://webdriver.io/) avec [ChromeHeadless](https://developers.google.com/web/updates/2017/04/headless-chrome)
Il s'agit d'une stack technique mise en place au d√©but du projet (√† l'√©poque avec [PhantomJS](https://phantomjs.org/) pour les plus anciens d'entre-vous)

Cette stack nous permet d'automatiser le pilotage de tests qui contr√¥lent un navigateur.
Ce navigateur va r√©aliser des actions qui se rapprochent le plus de celles que nos vrais utilisateurs peuvent faire tout en v√©rifiant comment le site r√©agit.

Il y a quelques ann√©es, cette stack technique √©tait plut√¥t compliqu√©e √† mettre en place, mais aujourd'hui il est plut√¥t simple de le faire.
[Le site qui h√©berge cet article de blog](https://github.com/Slashgear/slashgear.github.io) en est lui-m√™me la preuve.
Il ne m'a fallu qu'une dizaine de minutes pour mettre en place cette stack avec [le WebdriverIo CLI](https://webdriver.io/docs/gettingstarted) pour v√©rifier que mon blog fonctionne comme pr√©vu.

J'ai d'ailleurs r√©cemment publi√© [un article pr√©sentant la mise en place de cette stack](https://slashgear.github.io/how-to-setup-e2e-tests-with-webdriverio/).

Voici donc un exemple de fichier de test E2E pour vous donner une id√©e:

```gherkin
Feature: Playground

  Background: Playground context
    Given I use "playground" test context

  Scenario: Check if playground is reachable
    When As user "toto@toto.fr" I visit the "playground" page
    And I click on "playground trigger"
    Then I should see a "visible playground"
    And I should see 4 "playground tab" in "playground"

    When I click on "playground trigger"
    Then I should not see a "visible playground"

    # ...
```

Et √ßa donne √ßa en local avec mon navigateur Chrome !

![Exemple d'ex√©cution de test fonctionnel](../../../../images/posts/bonnes-pratiques-web/e2e-example.gif)

Voil√† un sch√©ma qui explique comment cette stack fonctionne:

![sch√©ma qui explique le fonctionnement de notre stack](../../../../images/posts/bonnes-pratiques-web/e2e-archi.png)

Aujourd'hui, l'application web de Bedrock poss√®de plus de 800 sc√©narios de tests E2E qui tournent sur chacune de nos _Pull Request_ et sur la branche `master`.
Ils nous assurent que nous n'introduisons pas de r√©gression fonctionnelle et c'est juste g√©nial !

üëç Les points positifs

- WebdriverIO nous permet √©galement de lancer de mani√®re journali√®re ces m√™mes tests sur des vrais devices en passant par le service payant SAAS [Browserstack](https://www.browserstack.com/).
  On a donc tous les jours un _job_ qui s'assure que notre site fonctionne correctement sur un Chrome derni√®re version sur Windows 10 et Safari sur MacOs.
- Ces tests nous permettent de facilement documenter les fonctionnalit√©s de l'application gr√¢ce au langage Gherkin.
- Ils nous permettent de reproduire des cas qui sont loin d'√™tre nominaux.
  Dans une logique _TDD_, ils permettent d'avancer sur le d√©veloppement sans avoir √† cliquer pendant des heures.
- Ces tests nous ont permis de ne pas casser l'ancienne version du site qui est toujours en production pour quelques clients alors que nos efforts se concentrent sur la nouvelle.
- Ils nous apportent une vraie confiance.
- Gr√¢ce notre librairie [_superagent-mock_](https://www.npmjs.com/package/superagent-mock), nous pouvons _fixturer_ (bouchonner, mocker) toutes les API dont on d√©pend et ainsi m√™me v√©rifier les cas d'erreurs.
  De plus, mocker la couche XHR du navigateur permet une am√©lioration significative du temps d'ex√©cution des tests. üöÄ
- Ils nous donne acc√®s √† des usages √©tendus comme :
    - v√©rification de r√®gles d'accessibilit√©
    - check les logs de la console navigateur (pour ne pas introduire d'erreur ou de React Warning par exemple)
    - surveiller tous les appels r√©seaux du site gr√¢ce √† un proxy
    - et j'en passe...

üëé Les complications

- Maintenir cette stack est compliqu√© et co√ªteux.
  √âtant donn√© que peu de ressources sont publi√©es sur ce domaine, on se retrouve parfois √† devoir creuser pendant plusieurs jours pour les r√©parer üòÖ.
  Il nous arrive de nous sentir parfois bien seul √† avoir ces soucis.
- Il est tr√®s facile de coder un test E2E dit _flaky_ (ie: un test qui peut √©chouer al√©atoirement).
  Ils nous font croire que quelque chose est cass√©.
  Ils nous prennent parfois du temps √† les stabiliser.
  Il reste cependant **bien meilleur de supprimer un test qui ne vous donnera pas un r√©sultat stable.**
- Faire tourner tous les tests prend un temps important sur notre int√©gration continue.
  Il faut r√©guli√®rement travailler sur leur optimisation pour que le feedback qu'ils vous apportent soit le plus rapide possible.
  Ces temps importants coutent √©galement de l'argent, il faut en effet bien faire tourner ces tests sur des machines.
  Pour information, l'infrastructure du site web (√† lui seul, juste l'h√©bergement de nos servers Node + fichiers statiques + CDN) coutent bien moins cher que notre int√©gration continue.
  Cela fait bien √©videmment sourire nos Ops ! üòä
- Les nouvelles recrues de nos √©quipes n'ont souvent jamais r√©alis√© ce genre de tests, il y a donc une phase ~~de gal√®re~~ d'apprentissage..
- Certaines fonctionnalit√©s sont parfois trop compliqu√©es √† tester avec notre stack E2E (par exemple, les parcours de paiement qui d√©pendent de tiers).
  Il nous arrive alors de nous rabattre sur d'autres techniques avec Jest notamment en ayant un scope moins unitaire.

### Nos tests "unitaires"

Pour compl√©ter nos tests fonctionnels nous avons √©galement une stack de tests √©crits avec [Jest].
On qualifie ces tests d'unitaires car nous avons comme principe d'essayer de toujours tester nos modules JS en ind√©pendance des autres.

_Ne d√©battons pas ici sur "Est-ce que ce sont des vrais tests unitaires ?", suffisamment d'articles sur internet traitent de ce sujet._

On utilise ces tests pour diff√©rentes raisons qui couvrent des besoins que nos tests fonctionnels ne couvrent pas:

- nous aider √† d√©velopper nos modules JS avec des pratiques TDD.
- documenter et d√©crire comment fonctionne un module JS.
- tester des cas limites tr√®s/trop compliqu√©s √† tester avec nos tests E2E.
- faciliter le refactoring de notre application en nous montrant les impacts techniques de nos modifications.

Avec ces tests, on se met au niveau d'une fonction utilitaire, d'une action Redux, d'un reducer, d'un composant React.
On se base essentiellement sur [la fonctionnalit√© d'`automock` de Jest](https://slashgear.github.io/discover-jest-hidden-feature-automock/) qui nous propose d'isoler nos modules JS lorsqu'on teste.

![repr√©sentation visuelle de l'automock](../../../../images/posts/bonnes-pratiques-web/mocked-modules.jpg)

L'image pr√©c√©dente repr√©sente la m√©taphore qui nous permet d'expliquer notre strat√©gie de tests unitaires aux nouveaux arrivant.

> "Il faut s'imaginer que l'application est un mur compos√© de briques unitaires (nos modules ecmascript), nos tests unitaires doivent tester une √† une les briques en ind√©pendance totale des autres.
> Nos tests fonctionnels sont l√† pour tester le ciment entre les briques."

**Pour r√©sumer, on pourrait dire que nos tests E2E testent _ce que notre application doit faire_, et nos tests unitaires s'assurent eux de v√©rifier _comment √ßa marche._**

Aujourd'hui ce sont plus de 6000 tests unitaires qui couvrent l'application et permettent de limiter les r√©gressions.

üëç

- [Jest] est vraiment une librairie g√©niale, rapide, compl√®te, bien document√©e.
- Les tests unitaires nous aident beaucoup √† comprendre _plusieurs ann√©es apr√®s_ comment tout cela fonctionne.
- On arrive toujours √† tester unitairement notre code, et cela compl√®te bien nos tests E2E.
- L'`automock` est vraiment pratique pour le d√©coupage de tests par modules.

üëé

- Parfois, nous nous sommes trouv√©s limit√©s par notre stack de tests E2E et nous ne pouvions pas uniquement nous baser sur les tests unitaires.
  Il nous manquait quelque chose pour pouvoir s'assurer que le _ciment entre les briques_ fonctionnait comme on le souhaitait.
  Pour cela, il a √©t√© mis en place une deuxi√®me stack de tests [Jest] nomm√© "test d'int√©gration" ou l'`automock` est d√©sactiv√©.
- L'abus de [_Snapshot_](https://jestjs.io/docs/snapshot-testing) est dangereux pour la sant√©.
  L'usage du _"Snapshot testing"_ peut faire gagner du temps sur l'impl√©mentation de vos tests mais peuvent en r√©duire la qualit√©.
  Avoir √† review un object de 50 lignes en _Snapshot_ est ni facile, ni pertinent.
- Avec la d√©pr√©ciation d'[EnzymeJS], nous sommes contraints de migrer sur [React Testing Library].
  Il est bien √©videmment possible de tester unitairement des composants avec cette nouvelle librairie.
  Malheureusement, ce n'est pas vraiment l'esprit et la fa√ßon de faire.
  [React Testing Library] nous pousse [√† ne pas jouer avec le _shallow rendering_](https://kentcdodds.com/blog/why-i-never-use-shallow-rendering).

### Nos principes

Nous essayons de toujours respecter les r√®gles suivantes lorsqu'on se pose la question "Dois-je ajouter des tests ?".

1. Si notre _Pull Request_ introduit des nouvelles fonctionnalit√©s utilisateurs, il faut int√©grer des scenarios de test E2E.
   Des tests unitaires avec Jest peuvent les compl√©ter / remplacer en fonction.
2. Si notre _Pull Request_ a pour but de corriger un bug, cela signifie qu'il nous manque un cas de test.
   On doit donc essayer de rajouter un test E2E ou √† d√©faut un test unitaire.

_C'est en √©crivant ces lignes que je me dis que ces principes pourraient tr√®s bien faire l'objet d'une automatisation._ ü§£

## Le projet reste, les fonctionnalit√©s non

> "La seconde √©volution d'une fonctionnalit√© est tr√®s souvent sa suppression."

Par principe, nous souhaitons faire en sorte que chaque nouvelle fonctionnalit√© de l'application ne base pas son activation sur le simple fait d'√™tre dans la codebase.
Classiquement, le cycle de vie d'une "feature" dans un projet peut √™tre le suivant (dans un [Github Flow](https://guides.github.com/introduction/flow/)):

- une personne impl√©mente sur une branche
- la fonctionnalit√© est _merg√©e_ sur master
- elle est d√©ploy√©e en production
- vis sa vie de fonctionnalit√© (avec parfois des bugs et des correctifs)
- la fonctionnalit√© n'est plus n√©cessaire
- une personne d√©tricote le code et l'enl√®ve
- nouveau d√©ploiement

Pour simplifier certaines √©tapes, il a √©t√© mis en place du _feature flipping_ sur le projet.

**Comment √ßa marche ?**

Dans notre config il y a une _map_ cl√©/valeur qui liste toutes les fonctionnalit√©s de l'application associ√©es √† leur statut d'activation.

```js
const featureFlipping = {
  myAwesomeFeature: false,
  anotherOne: true,
}
```

Dans notre code, nous avons donc impl√©ment√© des traitements conditionnels qui disent "Si cette feature est activ√©e alors...".
Cela peut changer le rendu d'un composant, changer l'impl√©mentation d'une action Redux ou bien d√©sactiver une route de notre _react-router_.

**Mais √† quoi √ßa sert ?**

- On peut d√©velopper des nouvelles √©volutions progressivement en les cachant derri√®re une cl√© de configuration.
  On livre des fonctionnalit√©s en production sans les activer.
- En environnement de test, on peut surcharger cette config pour tester des features qui ne sont pas encore activ√©es en production.
- Dans le cas d'un site en marque blanche, on peut proposer ces fonctionnalit√©s √† nos clients comme des options possibles.
- Avant de supprimer le code d'une feature, on la d√©sactive puis on fait le m√©nage sans risque.
- Gr√¢ce √† un outil maison nomm√© l'_Applaunch_, cette config de feature flipping est surchargeable dans une interface graphique √† chaud sans d√©ploiement.
  Cela nous permet d'activer des fonctionnalit√©s sans faire de mise en production du code.
  En cas d'incident, on peut d√©sactiver des fonctionnalit√©s qui sont d√©grad√©es.

Pour vous donner un exemple plus concret, entre 2018 et 2020 nous avons compl√®tement refondu l'interface de l'application.
Cette √©volution graphique n'√©tait qu'une cl√© de featureFlipping.
La refonte graphique n'a donc pas √©t√© la remise √† z√©ro du projet, on continue encore aujourd'hui de vivre avec les deux versions (tant que la bascule de tous nos clients n'est pas termin√©e).

![screenshot comparatif v4 / v5 sur 6play](../../../../images/posts/bonnes-pratiques-web/compare-v4-v5.jpg)

### L'A/B testing

Gr√¢ce au super travail des √©quipes backend et data, on a pu m√™me √©tendre l'usage du _feature flipping_ en rendant cette configuration modifiable pour des sous groupes d'utilisateurs.

Cela permet de d√©ployer des nouvelles fonctionnalit√©s sur une portion plus r√©duite des utilisateurs afin de comparer nos [KPI].

Prise de d√©cision, am√©lioration des performances techniques ou produit, exp√©rimentations, les possibilit√©s sont nombreuses et nous les exploitons de plus en plus.

### Le _futur flipping_

> Sur une id√©e originale de [Florent Lepretre](https://twitter.com/SuperFlaw).

Nous avions r√©guli√®rement le besoin d'activer des feature √† des heures ~~tr√®s~~ trop matinales dans le futur.
Pour cela nous devions √™tre connect√© √† une heure pr√©cise sur notre poste pour modifier la configuration √† chaud.

Afin d'√©viter d'oublier de le faire, ou de le faire en retard, nous avons fait en sorte qu'une cl√© de configuration puisse √™tre activ√©e √† partir d'une certaine date.
Pour cela, nous avons fait √©voluer notre _selector redux_ qui indiquait si une feature √©tait activ√©e pour qu'il puisse g√©rer des formats de date et les comparer √† l'heure courante.

```js
const featureFlipping = {
  myAwesomeFeature: {
    offDate: '2021-07-12 20:30:00',
    onDate: '2021-07-12 19:30:00',
  },
}
```

> De nombreux caf√©s ‚òïÔ∏è √† 9h ont √©t√© sauv√©s gr√¢ce au _futur flipping_

## Monitorer, Mesurer, Alerter

Pour maintenir un projet aussi longtemps que l'application web de bedrock, des tests, de la documentation et de la rigueur ne suffisent pas.
Il faut √©galement de la visibilit√© sur ce qui marche en production.

> "Comment sais-tu que l'application que tu as en production en ce moment m√™me fonctionne comme pr√©vu ?"

On part du principe qu'aucune fonctionnalit√© ne marche tant qu'elle n'est pas monitor√©e.
Aujourd'hui le monitoring √† Bedrock cot√© Frontend se mat√©rialise par diff√©rents outils et diff√©rentes stacks.
Je pourrais vous citer [NewRelic](https://newrelic.com/), un [Statsd](https://github.com/statsd/statsd), une stack [ELK](https://www.elastic.co/fr/what-is/elk-stack) ou bien encore [Youbora](https://youbora.nicepeopleatwork.com/) pour la vid√©o.

Pour vous donner un exemple, √† chaque fois qu'un utilisateur commence une session de navigation on envoie un _Hit_ de monitoring anonyme pour incr√©menter un compteur dans Statsd.
On a alors plus qu'√† d√©finir un dashboard qui affiche dans un graphique l'√©volution de ce nombre.
Si on observe une variation trop importante, cela peut nous permettre de d√©tecter un incident.

![exemple de dashboard de suivi](../../../../images/posts/bonnes-pratiques-web/grafana-monitoring-example.png)

Le monitoring nous offre aussi des solutions pour comprendre et analyser un bug qui s'est produit dans le pass√©.
Comprendre un incident, l'expliquer, en trouver sa _root cause_ sont les possibilit√©s qui s'offrent √† vous si vous monitorez votre application.
Le monitoring peut √©galement permettre de mieux communiquer avec les clients sur les impacts d'un incident et √©galement d'estimer le nombre d'utilisateurs impact√©s.

Avec la multiplication de nos clients, bien monitorer nos plateformes n'est plus suffisant.
Trop de donn√©es, trop de dashboards √† surveiller, il devient tr√®s facile de louper quelque chose.
Nous avons donc commenc√© √† compl√©ter notre suivi des mesures par de l'_alerting_ automatique.
Une fois que les mesures nous apportent suffisamment de confiance, on peut facilement mettre en place des alertes qui vont nous pr√©venir en cas de valeur incoh√©rente.

Nous essayons cependant de toujours d√©clencher des alertes uniquement quand celle-ci est actionnable.
Dans d'autres termes, si une alerte sonne, nous avons quelque chose √† faire.
Faire sonner des alertes qui ne n√©cessitent aucune action imm√©diate humaine g√©n√®rent du bruit et de la perte de temps.

![alerte g√©n√©rale](../../../../images/posts/bonnes-pratiques-web/alerte-taxi.gif)

## Limiter, surveiller et mettre √† jour ses d√©pendances

Ce qui p√©rime plus vite que votre ombre dans un projet web bas√© sur des technologies javascript, ce sont vos d√©pendances.
L'√©cosyst√®me √©volue rapidement et vos d√©pendances peuvent vite se retrouver non maintenues, plus √† la mode ou bien compl√®tement refondues avec de gros _breaking changes_.

On essaye donc dans la mesure du possible de limiter nos d√©pendances et d'√©viter d'en ajouter inutilement.
Une d√©pendance, c'est souvent tr√®s facile √† ajouter mais elle peut devenir un vrai casse-t√™te √† enlever.

Les librairies de composants graphiques (exemple React bootstrap, Material Design) sont un bel exemple de d√©pendance que nous tenons √† ne pas introduire.
Elles peuvent faciliter l'int√©gration dans un premier temps mais celles-ci bloquent souvent la version de votre librairie de composant par la suite.
Vous ne voulez pas figer la version de React dans votre application pour deux composants de formulaires.

La surveillance fait aussi partie de nos routines de gestion de nos d√©pendances.
Depuis l'ajout du [signalement de failles de s√©curit√© dans un package NPM](https://docs.npmjs.com/auditing-package-dependencies-for-security-vulnerabilities), il est possible de savoir si un projet int√®gre une d√©pendance qui contient une faille de s√©curit√© connue par une simple commande.
Nous avons donc des jobs journaliers sur nos projets qui lancent la commande `yarn audit` afin de nous forcer √† appliquer les correctifs.

> La maintenance de d√©pendances est grandement facilit√© par notre stack de tests E2E qui sonnent direcement si la mont√©e de version g√©n√®re une regression.

Aujourd'hui, hors failles de s√©curit√©, nous mettons √† jour nos d√©pendances "quand on a le temps", souvent en fin de _sprint_.
Cela ne nous satisfait pas car certaines d√©pendances peuvent se retrouver oubli√©es.
J'ai personnellement l'habitude d'utiliser des outils comme [`yarn outdated`](https://classic.yarnpkg.com/en/docs/cli/outdated/) et [Dependabot](https://dependabot.com/) sur mes projets personels pour automatiser la mise √† jour de mes d√©pendances.

## Accepter sa dette technique

Un projet accumulera toujours de la dette technique.
**C'est un fait.**
Que ce soit de la dette volontaire ou involontaire, un projet qui r√©siste aux ann√©es va forc√©ment accumuler de la dette.
D'autant plus, si pendant toutes ces ann√©es vous continuez d'ajouter des fonctionnalit√©s.

Depuis 2014, nos bonnes pratiques, nos fa√ßons de faire ont bien √©volu√©.
Parfois nous avons d√©cid√© ces changements mais parfois nous les avons subi (un exemple, l'arriv√©e des composants fonctionnels avec React et l'api des Hooks).

**Notre projet n'est pas compl√®tement _"state of art"_ et on l'assume.**

![√ßa tiendra !](../../../../images/posts/bonnes-pratiques-web/leak.gif)

Nous essayons de prioriser nos sujets de _refactoring_ sur les parties de l'application sur lequel on a le plus de souci, le plus de peine.
On consid√®re qu'une partie de l'application qui ne nous pla√Æt pas mais sur laquelle on n'a pas besoin de travailler (apporter des √©volutions) ne m√©rite pas qu'on la refactorise.

Je pourrais vous citer de nombreuses fonctionnalit√©s de notre application qui n'ont pas √©volu√© fonctionnellement depuis plusieurs ann√©es.
Mais comme nous avons couvert ces fonctionnalit√©s de tests E2E depuis le d√©but, nous n'avons pas vraiment eu √† y retoucher.

Comme dit plus haut, la prochaine √©volution d'une feature de code est parfois sa d√©sactivation.
Alors pourquoi passer son temps √† r√©-√©crire toute l'application ?

- Le code devient dans tous les cas du "legacy".
- Tant que les fonctionnalit√©s sont test√©es, rien ne nous oblige √† tout refactorer en permanence pour que toute notre codebase soit _state of art_.
- On se focalise sur nos _pain points_, on re-factorise ce qu'on a vraiment besoin de faire √©voluer.

## Pour r√©sumer

Les bonnes pratiques pr√©sent√©es ici restent bien √©videmment subjectives et ne s'appliqueront pas parfaitement/directement dans vos contextes.
Je suis cependant convaincu qu'elles peuvent probablement vous aider √† identifier ce qui peut faire passer votre projet de fun √† p√©rim√©.
√Ä Bedrock nous avons mis en place d'autres pratiques que je n'ai pas list√©es ici mais ce sera l'occasion de faire un nouvel article un jour.

Enfin, si vous souhaitez que je revienne plus en d√©tail sur certains chapitres pr√©sent√©s ici, n'h√©sitez pas √† me le dire, je pourrais essayer d'y d√©dier un article sp√©cifique.

[jest]: https://jestjs.io/fr/
[enzymejs]: https://enzymejs.github.io/enzyme/
[react testing library]: https://testing-library.com/docs/react-testing-library/intro/
[kpi]: https://www.journaldunet.fr/business/dictionnaire-du-marketing/1198189-kpi-key-performance-indicator-marketing-definition-exemples-okr/
