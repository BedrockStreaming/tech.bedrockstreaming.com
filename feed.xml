<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://tech.bedrockstreaming.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tech.bedrockstreaming.com/" rel="alternate" type="text/html" /><updated>2022-10-07T09:59:21+00:00</updated><id>https://tech.bedrockstreaming.com/feed.xml</id><title type="html">Bedrock Tech Blog</title><subtitle>Blog technique de Bedrock</subtitle><entry><title type="html">API Platform Conference 2022</title><link href="https://tech.bedrockstreaming.com/2022/10/07/api-platform-conference-2022.html" rel="alternate" type="text/html" title="API Platform Conference 2022" /><published>2022-10-07T00:00:00+00:00</published><updated>2022-10-07T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/10/07/api-platform-conference-2022</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/10/07/api-platform-conference-2022.html">&lt;p&gt;En cette période de rentrée, Bedrock participait à l’&lt;a href=&quot;https://api-platform.com/con/2022&quot;&gt;API Platform Conference 2022&lt;/a&gt;, où nous avons eu le plaisir d’assister à une partie des conférences proposées. Un grand merci à toutes les personnes chez &lt;a href=&quot;https://les-tilleuls.coop/&quot;&gt;Les-Tilleuls.coop&lt;/a&gt; pour l’organisation de cet évènement !
Pour cette seconde édition, le programme était réparti sur deux jours, les 15 &amp;amp; 16 septembre 2022.&lt;/p&gt;

&lt;p&gt;En introduction à cette conférence, Kévin Dunglas, créateur d’&lt;a href=&quot;https://api-platform.com/&quot;&gt;API Platform&lt;/a&gt;, a mis en ligne la version 3.0.0 du framework en nous présentant certaines nouvelles fonctionnalités développées telles que le support natif de XDebug. Il a profité de l’occasion pour présenter un petit historique d’API Platform.&lt;/p&gt;

&lt;h2 id=&quot;domain-driven-design-with-api-platform-3&quot;&gt;Domain-driven design with API Platform 3&lt;/h2&gt;

&lt;p&gt;Lors de cette conférence, &lt;a href=&quot;https://twitter.com/chalas_r&quot;&gt;Robin Chalas&lt;/a&gt; et &lt;a href=&quot;https://twitter.com/matarld&quot;&gt;Mathias Arlaud&lt;/a&gt; nous ont parlé de l’utilisation d’API Platform dans le cadre du Domain Driven Development et de l’Architecture Hexagonale.
Les présentateurs ont commencé cette présentation par plusieurs rappels et présentations sur des sujets comme :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Domain Driven Design&lt;/li&gt;
  &lt;li&gt;Structures hexagonales&lt;/li&gt;
  &lt;li&gt;CQRS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ces rappels ont permis d’enchainer sur l’utilisation du framework dans ce contexte à travers un &lt;a href=&quot;https://github.com/mtarld/apip-ddd&quot;&gt;exemple de projet&lt;/a&gt; DDD utilisant API Platform 3 et suivant l’architecture hexagonale.&lt;/p&gt;

&lt;p&gt;Ils expliquent comment implémenter API Platform dans notre code en détaillant plusieurs points :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;L’implémentation des providers côté query&lt;/li&gt;
  &lt;li&gt;L’implémentation des processors côté command&lt;/li&gt;
  &lt;li&gt;Le système d’opération&lt;/li&gt;
  &lt;li&gt;Les providers/processors qui appellent l’app via les bus&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;comment-alice-gardens-gère-t-elle-son-code-métier-via-les-évènements&quot;&gt;Comment Alice Garden’s gère-t-elle son code métier via les évènements&lt;/h2&gt;

&lt;p&gt;Nous avons pu assister à la conférence “Comment Alice Garden’s gère-t-elle le code métier via des évènements ?” proposée par leur technical architect &lt;a href=&quot;https://twitter.com/epatwon&quot;&gt;Nicolas Lemahieu&lt;/a&gt;. Tout d’abord, il nous a présenté le contexte de son entreprise Alice Garden’s qui fait de la vente de mobilier d’extérieur. En se basant sur la stack technique déjà présente : Symfony, RabbitMQ, MariaDB et sans tout refondre, comment faire pour mieux gérer le code métier actuellement éparpillé un peu partout dans le code.&lt;/p&gt;

&lt;p&gt;Ils utilisaient beaucoup de subscribers Doctrine, ce qui entraîne plusieurs problèmes :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Des subscribers nombreux = plus de logique au flush&lt;/li&gt;
  &lt;li&gt;Code plus difficile à maintenir et à comprendre&lt;/li&gt;
  &lt;li&gt;L’augmentation du risque de boucles infinies implique que chaque changement entraîne des boucles sur le UnitOfWork&lt;/li&gt;
  &lt;li&gt;Duplication de code&lt;/li&gt;
  &lt;li&gt;Code fortement couplé à Doctrine et manque de typage&lt;/li&gt;
  &lt;li&gt;Du côté du profiler Symfony, cela devient compliqué aussi dès qu’on commence à en avoir beaucoup&lt;/li&gt;
  &lt;li&gt;Les tests sont compliqués :
    &lt;ul&gt;
      &lt;li&gt;Unitaires quasi impossibles,&lt;/li&gt;
      &lt;li&gt;Fonctionnels possibles, mais demandent beaucoup de ressources en temps et donc d’argent&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nicolas Lemahieu a donc présenté les différentes solutions envisagées ainsi que leurs avantages et inconvénients :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Domain Driven Development : séparation très nette du métier et de l’infra, mais demande beaucoup trop d’effort à mettre en place, car aucune correspondance avec les bundles déjà existants (risque de régression trop haut, coût de développement trop grand…)&lt;/li&gt;
  &lt;li&gt;Garder les évènements et s’affranchir de Doctrine : c’est la solution qui a été retenue parce qu’elle permettait de réutiliser un maximum de l’existant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Puis, nous avons pu apprendre comment implémenter cette solution :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Création d’une abstraction supplémentaire “BusinessObject” : nouveau dossier Business dans src où :
    &lt;ul&gt;
      &lt;li&gt;Entité = objet métier&lt;/li&gt;
      &lt;li&gt;Méthodes des entités = règles métier&lt;/li&gt;
      &lt;li&gt;Toutes les interfaces entité étendent BusinessObjectInterface&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Classes abstraites dans Business&lt;/li&gt;
  &lt;li&gt;Implémentation d’events custom pour chaque objet métier et par type d’event&lt;/li&gt;
  &lt;li&gt;Event provider : fournit les évènements qui sont mis dans une collection puis tag dans les services&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;En conclusion, chez Alice Garden’s, ils ont réussi à n’avoir qu’un seul Doctrine Subscriber, donc une seule boucle de UnitOfWork et des tests facilités, car c’est seulement du PHP. La dépendance à Doctrine est éliminée et il suffira de déplacer le provider pour adapter le code à une autre infrastructure.&lt;/p&gt;

&lt;h2 id=&quot;réutiliser-et-partager-vos-opérations-personnalisées-avec-api-platform&quot;&gt;Réutiliser et partager vos opérations personnalisées avec API Platform&lt;/h2&gt;

&lt;p&gt;Grâce à &lt;a href=&quot;https://mobile.twitter.com/jean_beru&quot;&gt;Hubert Lenoir&lt;/a&gt; et &lt;a href=&quot;https://mobile.twitter.com/jjarrie&quot;&gt;Jérémy Jarrié&lt;/a&gt; de l’entreprise SensioLabs, nous avons pu apprendre à réutiliser et partager les opérations avec API Platform. Ils utilisent 3 API REST faites avec API Platform v3. Des opérations génériques comme “liker” peuvent s’appliquer sur des ressources différentes (articles, photos, pages, etc.), on peut donc réutiliser du code.&lt;/p&gt;

&lt;p&gt;Les principes pour faire cette utilisation générique de code sont simples :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Un seul contrôleur pour plusieurs ressources = un contrôleur de comportement&lt;/li&gt;
  &lt;li&gt;Une interface pour que les entités puissent adopter ce comportement&lt;/li&gt;
  &lt;li&gt;Trait pour les méthodes du comportement (1 à n comportements, donc classe abstraite ou interface impossible)&lt;/li&gt;
  &lt;li&gt;Ajouter des services intermédiaires&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Il survient un seul problème avec ce pattern : la duplication des annotations API Platform. La solution est d’ajouter des décorations (design pattern decorator) sur les métadatas d’API Platform. Il est donc simple de créer des comportements indépendants des ressources qui pourront être facilement réutilisés. Ce projet était encore à l’état de POC, mais Jérémy et Hubert allaient mettre à jour la version plus aboutie sur le &lt;a href=&quot;https://github.com/JJarrie/reuse-behaviour&quot;&gt;repository GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;la-revue-de-code-est-un-art&quot;&gt;La revue de code est un art&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/SmaineDev&quot;&gt;Smaine Milianni&lt;/a&gt; a proposé une conférence sur les conseils à suivre afin d’effectuer une revue de code. Nous avons pu réaliser qu’au sein de Bedrock nous appliquions déjà de nombreux principes.&lt;/p&gt;

&lt;p&gt;Nous appliquons déjà :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Un template de PR pour décrire les caractéristiques du bug ou de l’US :
    &lt;ul&gt;
      &lt;li&gt;Quoi, pourquoi, comment, comment tester, etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Des noms de commits explicites&lt;/li&gt;
  &lt;li&gt;La bienveillance&lt;/li&gt;
  &lt;li&gt;Le challenge du code des autres personnes&lt;/li&gt;
  &lt;li&gt;La connaissance des nombreux concepts de code (SOLID, KISS…)&lt;/li&gt;
  &lt;li&gt;Faire du pair review ou mob review&lt;/li&gt;
  &lt;li&gt;Un request bot déjà en place&lt;/li&gt;
  &lt;li&gt;Tester et ne pas se fier uniquement à la lecture du code&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nous avons aussi pu prendre du recul et noter des conseils à appliquer. Chacun a pu transmettre ses idées à son équipe. Le rappel qu’une revue c’est aussi souligner le positif et pas seulement challenger le code. Cette conférence est très concrète et facilement applicable à Bedrock.&lt;/p&gt;

&lt;h2 id=&quot;fighting-impostor-syndrome-a-practical-handbook&quot;&gt;Fighting impostor syndrome: a practical handbook&lt;/h2&gt;

&lt;p&gt;Lors de cette conférence, &lt;a href=&quot;https://twitter.com/mupsigraphy&quot;&gt;Marine Gandy&lt;/a&gt; commence sa présentation par définir ce que le syndrome de l’imposteur est : une peur de l’échec, la crainte que quelqu’un dise que nous ne sommes pas capables, mais aussi le sentiment de ne pas mériter de réussir.
Elle nous explique que ce terme était tout d’abord attribué exclusivement aux femmes, mais qu’après une étude montrant que 70% de la population était touchée, il se serait généralisé à tous les genres.
Marine Gandy nous énonce que la tech est très touchée par ce phénomène pour plusieurs raisons comme le fait qu’il y ait beaucoup de renouveau dans ce corps de métier et que nous avons vite l’impression de retourner à nos débuts lorsque nous changeons de techno, créant ainsi un sentiment d’instabilité. 
Dans ce contexte, Marine nous parle de &lt;a href=&quot;https://www.universite-paris-saclay.fr/sites/default/files/media/2020-02/erreur-fondamentale-d-attribution-atelierfbjip2018.pdf&quot;&gt;l’effet Julien Lepers&lt;/a&gt; en prenant pour exemple le fait de se trouver dans une équipe de personnes bien plus expérimentées que nous où la situation influe sur la personne.
Pour finir, la conférencière nous présente plusieurs pistes à suivre pour éviter ou minimiser ce genre de sentiment :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Arrêter de se comparer aux autres&lt;/li&gt;
  &lt;li&gt;Se challenger sur de nouveaux domaines pour se rendre compte qu’on peut toujours apprendre&lt;/li&gt;
  &lt;li&gt;Travailler sur ses faiblesses pour permettre de se sentir plus compétent&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mon-combat-contre-larachnophobie&quot;&gt;Mon combat contre l’arachnophobie&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/Deuchnord&quot;&gt;Jérôme Tanghe&lt;/a&gt;, par son arachnophobie, nous a expliqué comment il est arrivé à contribuer à API Platform afin d’ajouter une option pour cacher la mascotte Webby. Et c’est ce dont il nous a parlé, à savoir comment bien démarrer sa première pull request pour contribuer au logiciel libre. La première étape étant de trouver le bon repository qui nous conviendrait parmi les projets existants. Dans le but d’identifier un sujet sur lequel contribuer, il ne faut pas hésiter à utiliser la fonctionnalité des tags sur les issues, par exemple le tag hacktoberfest dans le cadre d’API Platform. Une fois le sujet trouvé, il faut maintenant identifier la branche de base à partir de laquelle faire sa pull request, cela peut s’agir d’une version spécifique choisie ou même de la branche principale. La contribution au logiciel libre ou à l’open source ne passe pas uniquement par des pull requests uniquement basés sur le code. Il est également possible de tester les préversions (release-candidate), signaler des bugs, faire des suggestions, améliorer la documentation ou encore rédiger des traductions. Enfin, il est conseillé de prendre en compte chaque retour sur d’autres pull requests, cela permet notamment de découvrir les principes et standards du projet.&lt;/p&gt;

&lt;h2 id=&quot;pourquoi-je-nutilise-pas-api-platform&quot;&gt;Pourquoi je n’utilise pas API Platform&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/FredBouchery&quot;&gt;Frédéric Bouchery&lt;/a&gt;, s’est décidément perdu en se retrouvant à présenter cette conférence à l’API Platform conference 2022. Malgré tout, cela lui a permis de nous partager son introspection : Mais au fait, pourquoi il ne s’en sert pas ?&lt;br /&gt;
Dans cette première partie de sa conférence, Frédéric n’hésite pas à utiliser beaucoup de sarcasme. Il nous explique qu’il ne s’en sert pas, car API Platform est écrit en PHP et pour lui, c’est une technologie vieillissante qui ne devrait pas tarder à rejoindre Cobolt. Également parce qu’API Platform utilise Symfony, alors que tout le monde le sait très bien, enfin surtout les Google Trend, Laravel est plus utilisé dans le monde. De plus, Frédéric n’aime pas la magie et API Platform en est rempli : sérialisation et désérialisation à tout va alors que lui est capable de faire une API en seulement quelques lignes avec du PHP pur sans artifice. Enfin, il reproche à API Platform de devenir compliqué à utiliser si le projet qui se base dessus est complexe, trop de personnalisation et de configurations doivent être effectuées. &lt;br /&gt;
Dans cette deuxième partie, Frédéric fait tomber son masque sarcastique et décide de revenir sur les points qu’il a abordés précédemment :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Les tendances concernant un langage ne sont pas des bons indicateurs, en effet imaginer le futur ou la mort de PHP via des statistiques dont certaines basées sur l’opinion publique n’est pas une bonne façon de faire&lt;/li&gt;
  &lt;li&gt;PHP, c’est aujourd’hui 75% des sites du monde entier et 54% parmi le top 1000 des sites internet fréquemment utilisés&lt;/li&gt;
  &lt;li&gt;Malgré tout, il nous conseille de ne pas être mono technologique non plus. S’intéresser à d’autres langages est une bonne chose&lt;/li&gt;
  &lt;li&gt;API Plateform a quand même de bonnes performances : 99% de réponses avec une moyenne de 91 ms sur 5 000 requêtes comparées à son code PHP pur avec une moyenne de 21 ms pour 5 000 requêtes également sachant que son code ne prend pas en compte la sécurité&lt;/li&gt;
  &lt;li&gt;Le framework Laravel utilisant des composants Symfony, il est dès lors difficile de les comparer. Même si factuellement Laravel est plus utilisé dans le monde, on n’utilise pas du Laravel ou du Symfony pour les mêmes raisons et c’est une bonne chose que les deux coexistent ensemble&lt;/li&gt;
  &lt;li&gt;Il pensait qu’avec la complexité de ses projets, il était trop dur de passer à API Platform, mais il s’est rendu compte que ce n’était pas nécessairement vrai&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;En conclusion de sa conférence et sans aucun sarcasme, Frédéric n’hésite pas à se livrer à nous et finit par nous dire qu’il va finalement utiliser API Platform 3 pour un projet.&lt;/p&gt;

&lt;h2 id=&quot;whats-new-in-caddy-the-webserver-of-api-platform&quot;&gt;What’s New in Caddy, the webserver of API Platform&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/_francislavoie&quot;&gt;Francis Lavoie&lt;/a&gt; nous a présenté plusieurs nouveautés dans &lt;strong&gt;Caddy&lt;/strong&gt;, un webserver écrit en Go ayant beaucoup de fonctionnalités activées par défaut et fourni avec API Platform dans &lt;a href=&quot;https://api-platform.com/docs/distribution/caddy/&quot;&gt;l’installation de base&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Parmi les nouveautés présentées, il nous a notamment parlé d’améliorations au niveau des &lt;strong&gt;request matchers&lt;/strong&gt; avec des matchers réutilisables, des expressions et des fonctions. Il a ensuite parlé d’une gestion native de &lt;a href=&quot;https://www.authelia.com&quot;&gt;Authelia&lt;/a&gt; permettant de déléguer facilement l’authentification depuis la configuration du serveur.
Enfin, la directive &lt;strong&gt;file_server&lt;/strong&gt; peut maintenant servir des fichiers provenant d’autres sources que le système de fichiers local, par exemple depuis un &lt;strong&gt;bucket S3&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Certaines fonctionnalités sont désormais activées par défaut comme &lt;strong&gt;HTTP/3&lt;/strong&gt; et la suppression du log des headers d’authentification où il est également possible de créer des filtres pour retirer d’autres informations.&lt;/p&gt;

&lt;h2 id=&quot;webauthn--se-débarrasser-des-mots-de-passe-définitivement&quot;&gt;WebAuthn : se débarrasser des mots de passe. Définitivement.&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://mobile.twitter.com/florentmorselli&quot;&gt;Florent Morselli&lt;/a&gt; nous fait une proposition : il est de plus en plus possible aujourd’hui de se passer complètement des mots de passe.&lt;/p&gt;

&lt;p&gt;Il a commencé par nous rappeler les problèmes récurrents : mots de passe trop faibles et/ou trop courts, réutilisation sur plusieurs sites, la multiplication des fuites de bases de données, etc.&lt;/p&gt;

&lt;p&gt;La solution proposée : &lt;a href=&quot;https://webauthn.io&quot;&gt;WebAuthn&lt;/a&gt;, un standard d’authentification multifacteur permettant d’identifier les utilisateurs via des données biométriques, des clés physiques ou sans aucune information après la première authentification sur un appareil.&lt;/p&gt;

&lt;p&gt;Côté implémentation, Florent nous a présenté deux projets : un &lt;a href=&quot;https://github.com/web-auth/webauthn-symfony-bundle&quot;&gt;bundle Symfony&lt;/a&gt; pour le côté backend et un &lt;a href=&quot;https://github.com/web-auth/webauthn-stimulus&quot;&gt;composant Symfony UX&lt;/a&gt; pour le frontend.&lt;/p&gt;

&lt;h2 id=&quot;php-websockets-or-how-to-communicate-with-clients-in-real-time&quot;&gt;PHP WebSockets, or how to communicate with clients in real-time&lt;/h2&gt;

&lt;p&gt;Habituellement connue pour faire des conférences sur Git, &lt;a href=&quot;https://twitter.com/vanamerongen&quot;&gt;Pauline Vos&lt;/a&gt; nous a fait une démo en live de l’utilisation des &lt;strong&gt;WebSockets&lt;/strong&gt; en PHP.&lt;/p&gt;

&lt;p&gt;Elle a commencé par une rapide explication de différents protocoles de communication en temps réel existant : &lt;em&gt;WebRTC&lt;/em&gt; chez Google, &lt;em&gt;Mercure&lt;/em&gt; chez Symfony et &lt;em&gt;Livewire&lt;/em&gt; chez Laravel.
Les WebSockets étant de simples tunnels à données, ces protocoles permettent de les enrichir de diverses fonctionnalités : identification, structures de messages, reconnexion auto, etc.&lt;/p&gt;

&lt;p&gt;Vient ensuite la démo qui consistait en une mini webapp de tombola en ligne. Elle a été découpée en différentes étapes (préparées dans des branches Git) avec, pour chaque étape, une présentation du code et des tests en live via l’outil &lt;a href=&quot;https://websocketking.com&quot;&gt;WebSocketKing&lt;/a&gt;.
Pour l’étape finale, un QR code a été affiché à l’écran pour permettre aux spectateurs et spectatrices de participer en live. Le hasard a voulu que le nom tiré soit &lt;a href=&quot;https://twitter.com/s0yuka&quot;&gt;Antoine Bluchet&lt;/a&gt;, le contributeur principal d’API Platform !&lt;/p&gt;

&lt;h2 id=&quot;comment-remettre-la-tech-au-service-du-bien-commun-&quot;&gt;Comment (re)mettre la tech au service du bien commun ?&lt;/h2&gt;

&lt;p&gt;Pour conclure ces deux jours de conférences intenses en savoir et en émotions, animé par &lt;a href=&quot;https://twitter.com/gregcop1&quot;&gt;Grégory Copin&lt;/a&gt; : &lt;a href=&quot;https://twitter.com/HeleneMaitre&quot;&gt;Hélène Marchois&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/paulandrieux&quot;&gt;Paul Andrieux&lt;/a&gt; et &lt;a href=&quot;https://twitter.com/dunglas&quot;&gt;Kévin Dunglas&lt;/a&gt; nous ont proposé une excellente table ronde riche en idées et porteuse (d’un peu) d’espoir. Le sujet étant de savoir s’il est possible de faire évoluer la tech dans le but de rejoindre les objectifs de départ du logiciel libre.&lt;/p&gt;

&lt;p&gt;L’apparition du mouvement du logiciel libre puis celle du web se sont bâties sur de grands espoirs et de beaux objectifs : émancipation des individus, partage des connaissances à l’échelle planétaire, liberté d’expression, constructions de bien commun appartenant à toutes et tous et maintenues collectivement. Malheureusement, force est de constater que le web comme le logiciel libre ont été détournés de leurs objectifs de base et que les idéaux qu’ils portaient ont été bien mis à mal : surveillance de masse, capitalisation de ces biens communs et précarisation des individus et des libertés.&lt;/p&gt;

&lt;p&gt;Kévin nous explique ensuite la différence entre logiciel libre et open source : API Platform est un logiciel libre plus qu’open source, même si techniquement, c’est les deux. Historiquement, le logiciel libre est apparu dans le but de créer un bien commun pour l’humanité et s’est élargi avec, notamment, la notion de commons via Wikipédia. La différence avec l’open source est que si le code est disponible, ce n’est pas uniquement pour bâtir tout et n’importe quoi avec, mais c’est un code qui porte des valeurs et qui a pour but de faire en sorte que tout le monde sans distinction puisse facilement créer de nouveaux outils qui puissent être partagés, qui appartiennent à un ensemble de personnes et qui vont socialiser le travail qui est réalisé en commun là-dessus. Le but du logiciel libre à la base, c’est de faire en sorte que ces valeurs de transparence, de démocraties, de partage de connaissance s’étendent via le logiciel à l’ensemble de la société. Donc si vous aussi, vous voulez utiliser un logiciel libre, la condition est que, vous aussi, vous devez faire quelque chose qui sert l’humanité : créer un bien commun et mettre aussi à disposition le code source du logiciel. En l’occurrence, API Platform, est une licence permissive, c’est-à-dire qu’il est possible de faire tout et n’importe quoi avec, mais ce n’est pas le cas pour le logiciel &lt;a href=&quot;https://github.com/dunglas/mercure&quot;&gt;Mercure&lt;/a&gt; par exemple, où si vous l’utilisez et le modifiez, vous êtes obligé de redistribuer les éléments.&lt;/p&gt;

&lt;p&gt;Quant à l’open source, c’est une initiative qui est arrivée bien après le logiciel libre et est une offensive de multinationale de la technologie qui veut dépolitiser le mouvement du logiciel libre. Le point de départ étant que, techniquement, c’est très intéressant de créer du logiciel ensemble, de partager les coûts de maintenance entre différentes entreprises ou personnes et c’est surtout très intéressant d’avoir accès au secret de fabrication pour les choses qui ont peu de valeur ajoutée. Mais l’objectif final étant de capitaliser, faire du business et capter la valeur sur ce qui a une très forte valeur ajoutée. Par exemple, pour macOS, toutes les briques de bases sont complètement libres, développées par une communauté de personne, d’entreprise et essentiellement beaucoup de bénévoles et d’hobbyiste. Et dans ce cas-là, ce qui a une extrême valeur ajoutée, c’est l’UI au-dessus du matériel ou encore les jolis outils qui coûtent une fortune. Ce qui permet à Apple d’être la boite la plus riche du monde en réutilisant le travail de personnes qui n’ont pas fait ça pour macOS à la base.&lt;/p&gt;

&lt;p&gt;Les trois personnes intervenantes représentant chacune une SCOP, la table ronde s’est ensuite naturellement tournée vers le lien entre le logiciel libre et le mouvement coopératif. Le lien étant la vision politique du logiciel libre via son socle de valeur : liberté, transparence, gouvernance partagée et coopération. On retrouve cet esprit de transparence, de fonctionnement démocratique et de fonctionnement par coopération à l’intérieur de la SCOP et entre les différentes SCOP.
S’en est ajoutée la question du sens par rapport à son travail. Effectivement, le logiciel libre, comme le mouvement coopératif, redonne du sens, principalement car cela ouvre le champ des possibles en vue des enjeux climatiques et sociaux actuels. Même si l’on vit dans une société qui est régie par le profit, la compétition féroce et le pouvoir, il existe des possibilités de s’organiser autrement et qui fonctionne quand même à une échelle conséquente, bien qu’encore insuffisante. Des actions individuelles existent et sont possibles. Pour cela, nous vous recommandons de regarder &lt;a href=&quot;https://www.youtube.com/watch?v=XpY7p062zIo&amp;amp;list=PL3hoUDjLa7eSo7-CAyiirYfhJe4h_Wxs4&quot;&gt;la conférence d’Hélène&lt;/a&gt; à l’API Platform Conference de l’année dernière qu’elle résume et étoffe lors de cette table ronde.&lt;/p&gt;

&lt;p&gt;Et bien sûr, quand cela sera possible, nous vous encourageons fortement de regarder le replay de cette conférence (&lt;a href=&quot;https://www.youtube.com/c/Les-tilleulsCoop&quot;&gt;sur la chaine des Tilleuls&lt;/a&gt;) qui redonne un peu d’espoir quant aux futurs des organisations démocratiques de nos métiers.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Merci à toutes et tous les speakers, à API Platform ainsi qu’aux Tilleuls-coop pour cet évènement ! Nous avons pu en apprendre plus sur API Platform et revenir la tête pleine d’idées pour nos projets futurs et présents ! À l’année prochaine peut-être !&lt;/p&gt;</content><author><name>backend</name></author><category term="conferences" /><category term="backend" /><category term="api" /><category term="php" /><summary type="html">En cette période de rentrée, Bedrock participait à l’API Platform Conference 2022, où nous avons eu le plaisir d’assister à une partie des conférences proposées. Un grand merci à toutes les personnes chez Les-Tilleuls.coop pour l’organisation de cet évènement ! Pour cette seconde édition, le programme était réparti sur deux jours, les 15 &amp;amp; 16 septembre 2022.</summary></entry><entry><title type="html">Subtitles, open captions, closed captions, SDH, oh my!</title><link href="https://tech.bedrockstreaming.com/2022/09/20/captioning-in-the-streaming-world.html" rel="alternate" type="text/html" title="Subtitles, open captions, closed captions, SDH, oh my!" /><published>2022-09-20T00:00:00+00:00</published><updated>2022-09-20T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/09/20/captioning-in-the-streaming-world</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/09/20/captioning-in-the-streaming-world.html">&lt;h1 id=&quot;subtitles-open-captions-closed-captions-sdh-oh-my&quot;&gt;Subtitles, open captions, closed captions, SDH, oh my!&lt;/h1&gt;

&lt;p&gt;Wait, what? Subtitles and captions are not the same? Have you noticed the popular « cc » logo in a player you use, standing for « Closed Captions » but you have never heard of Open Captions? In this article we are going to dive into the different textual representations used in the streaming world.&lt;/p&gt;

&lt;h2 id=&quot;subtitles-vs-captions-a-matter-of-accessibility&quot;&gt;Subtitles vs Captions: a matter of accessibility&lt;/h2&gt;

&lt;p&gt;Since both terms are often mixed up, in this post we are going to explain in details the different types of subtitles and captions.&lt;/p&gt;

&lt;p&gt;Subtitles exist in order to help the viewer understand the spoken language in the content being watched, assuming that the viewer can hear. This is the important part. You can think of subtitles as the closest translation of what is being said, textually represented on the screen.&lt;/p&gt;

&lt;h3 id=&quot;closed-captions&quot;&gt;Closed Captions&lt;/h3&gt;

&lt;p&gt;Closed Captions on the other hand, assume that the viewer is deaf (or hard of hearing) hence cannot understand what is being said, regardless of the spoken language. For this reason and contrary to subtitles, it will describe spoken dialogues as well as all important audio information such as music, sounds, speaker information when it makes sense (for example narrated content). In terms of appearance, closed captions are usually white text on a black background. An important note is that they are not supported through digital connections such as HDMI.&lt;/p&gt;

&lt;p&gt;Subtitles and closed captions are separate files that provide information for the receiver to decode. They’re not part of the stream and both can be turned off.&lt;/p&gt;

&lt;h3 id=&quot;subtitles-for-the-deaf-and-hard-of-hearing&quot;&gt;Subtitles for the Deaf and Hard of Hearing&lt;/h3&gt;

&lt;p&gt;Subtitles for the Deaf and Hard of Hearing, known as SDH, are a combination between subtitles and closed captions. They can be in the same language of the video original audio and bring some additional non-spoken information (speaker identification, sound effects, etc.) and/or be translated. This makes the content accessible for the deaf and hard of hearing who can read and understand foreign languages.&lt;/p&gt;

&lt;h3 id=&quot;open-captions&quot;&gt;Open Captions&lt;/h3&gt;

&lt;p&gt;The most important difference between Closed and Open Captions is that Open Captions are always visible and cannot be turned off. Think of them as « burned » in the video stream: they are not a separate file. Because of this, quality and readability may be affected. Open captions are widely used on social media for retention. Since there is a high chance that the end user is scrolling through content without sound, ensuring the display of text on the video helps catch and retain attention. In other cases where closed captions cannot be used for example if you have no control on the media player that will play your file, you may provide open captions to be sure to display a textual translation. The downside might be that part of the audience dislikes the superfluous text burned in the stream.&lt;/p&gt;

&lt;h3 id=&quot;forced-subtitles&quot;&gt;Forced subtitles&lt;/h3&gt;

&lt;p&gt;There is often a misconception around forced subtitles (sometimes referred as forced narratives) as they are mistaken with open captions. The name « forced » might suggest that they are burned in the video stream like open captions but there is a difference. Forced subtitles are actually distributed in a separate file and, despite their name, are not necessarily displayed. On our platform, if a subtitle or closed captions track is selected by the user, forced subtitles will not show up. We will come back to this later.&lt;/p&gt;

&lt;p&gt;Actually, forced subtitles are a text representation of a communication element like a spoken dialogue, specify a character ID that are not described in the original (or dubbed) audio stream. A common example would be to translate alien language. Despite watching a movie in your native (or any language that does not require you to activate subtitles), you would not be able to understand. That’s where forced subtitles come into play and ensure that you have a textual representation of what is being said even if you set subtitles to off, hence the « forced » attribute.&lt;/p&gt;

&lt;p&gt;However, imagine you are French and watch a Spanish show for instance. If you set the subtitles to French in order to be able to understand the content, forced subtitles won’t show up since you already have a textual representation of the content. Same goes for closed captions: if set to off, forced subtitles will display, if any. Otherwise, they won’t show up. To ensure better user experience, forced subtitles content should be included in all other tracks (regular subtitles, SDH, CC).&lt;/p&gt;

&lt;p&gt;If available, forced subtitles should be displayed in the preferred language set up by the user.&lt;/p&gt;

&lt;p&gt;To sum up, here is a table comparing the different technologies:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Subtitles&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SDH&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Closed Captions&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Open Captions&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Forced Subtitles&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Can be turned off&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;*&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Appearance&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Varies&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Varies&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Usually white text / black background&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Varies&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Varies&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Position&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Bottom third, centered&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Bottom third, centered&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Varies&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Varies&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Bottom third, centered&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HDMI Supported&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Describes music and sounds&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Describes speaker ID&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Available in source language&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;✔️&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;*: Forced subtitles do not show up in the track selection tool making them impossible to turn on or off. However, the business rules of the media platform will take care of displaying them, if needed.&lt;/p&gt;</content><author><name>Hugo Riffiod</name></author><category term="streaming" /><category term="subtitles" /><category term="captions" /><category term="video" /><category term="player" /><summary type="html">Subtitles, open captions, closed captions, SDH, oh my!</summary></entry><entry><title type="html">Monitoring at scale with Victoria Metrics</title><link href="https://tech.bedrockstreaming.com/2022/09/06/monitoring-at-scale-with-victoriametrics.html" rel="alternate" type="text/html" title="Monitoring at scale with Victoria Metrics" /><published>2022-09-06T00:00:00+00:00</published><updated>2022-09-06T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/09/06/monitoring-at-scale-with-victoriametrics</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/09/06/monitoring-at-scale-with-victoriametrics.html">&lt;h1 id=&quot;monitoring-at-bedrock-&quot;&gt;Monitoring at Bedrock :&lt;/h1&gt;
&lt;p&gt;At &lt;a href=&quot;https://www.bedrockstreaming.com/&quot;&gt;Bedrock Streaming&lt;/a&gt;, a large part of our applications are hosted on Kubernetes clusters, others use the EC2 service from AWS and a small part are hosted on “OnPremise” servers.&lt;/p&gt;

&lt;p&gt;From 2018 until January 2022, we used Prometheus to monitor all these platforms, because Prometheus met all our needs: keeping control over our monitoring solution and supporting service discovery, which is essential in environments such as Kubernetes or AWS EC2. Prometheus also supports custom exporters we developed internally.&lt;/p&gt;

&lt;p&gt;Over the years, our business has grown significantly, so the load on our platforms has increased. Indirectly, the load on our Prometheus instances has also increased, to the point where certain limitations have become too much for us. This is why we have changed our monitoring/alerting stack.&lt;/p&gt;

&lt;h1 id=&quot;limits-of-prometheus&quot;&gt;Limits of Prometheus&lt;/h1&gt;
&lt;p&gt;Prometheus does not have a native High Availability mode: to have high availability, we had to duplicate our Prometheus instances. This implies that our targets were “scrapped” by all our Prometheus instances (same for our rules and records).
To avoid this, we had to use sharding, but this made the infrastructure more complex. More information on this subject in this &lt;a href=&quot;https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/high-availability.md&quot;&gt;documentation&lt;/a&gt; from the Prometheus operator&lt;/p&gt;

&lt;p&gt;Prometheus is not designed to store metrics on a long-term basis, as mentioned in the &lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/storage/#operational-aspects&quot;&gt;documentation&lt;/a&gt; :&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;gt; Prometheus’s local storage is not intended to be durable long-term storage; external solutions offer extended retention and data durability.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Prometheus’s local storage is not intended to be durable long-term storage; external solutions offer extended retention and data durability.
We worked around this limitation by using Victoria Metrics (VMCluster) as a LongTermStorage via the remote_write protocol&lt;/p&gt;

&lt;p&gt;All processes (scrapping, ingest, storage, etc.) were, until now, managed in the same “prometheus” instance, which implied a less flexible and vertical scaling only (since recently a &lt;a href=&quot;https://prometheus.io/blog/2021/11/16/agent/&quot;&gt;Prometheus agent&lt;/a&gt; is available for the “scrapping” part).&lt;/p&gt;

&lt;p&gt;The RAM and CPU usage of a Prometheus instance is correlated to the number of metrics (and their cardinality) it has to manage. In our case, several Prometheus instances consumed more than 64 GB of RAM and 26 CPUs each, in order to absorb our peak loads. In a Kubernetes cluster, this high resources consumption can cause problems, especially for scheduling.&lt;/p&gt;

&lt;p&gt;The Write-Ahead Log (WAL) system can cause rather slow restarts if the Prometheus instance runs out of RAM and can cause the Prometheus instance to hang for varying lengths of time. During the replay of the WAL, Prometheus doesn’t scrape anything, thus there is no alerting and no way of knowing if something is going on.&lt;/p&gt;

&lt;h2 id=&quot;the-cardinality-of-metrics&quot;&gt;The cardinality of metrics&lt;/h2&gt;
&lt;p&gt;When our Kubernetes clusters manage a large number of pods, a constraint quickly appears: cardinality.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;gt; The cardinality of a metric is the number of TimeSeries of that metric with single-valued labels.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/cardinality-example.png&quot; alt=&quot;schema of cardinality&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the example above, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;status_code&lt;/code&gt; label has a cardinality of 5, app has a cardinality of 2 and the overall cardinality of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server_reponses&lt;/code&gt; metric is 10.&lt;/p&gt;

&lt;p&gt;In this example, any Prometheus instance can handle this cardinality, but if you add for example the label &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pod_name&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;client_IP&lt;/code&gt; (or both) to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server_reponses&lt;/code&gt; metric, the cardinality increases for each different clients calls and for each pod.&lt;/p&gt;

&lt;p&gt;You should read the excellent &lt;a href=&quot;https://www.robustperception.io/cardinality-is-key/&quot;&gt;article&lt;/a&gt; from “Robust Perception” for more details on this subject.&lt;/p&gt;

&lt;p&gt;At Bedrock the high cardinality metrics come from our HAProxy ingress. For our needs, we retrieve several labels like the name of the ingress pod as well as its IP address, but more importantly the name and IP address of the destination pod. In a cluster that can grow to more than 15,000 pods, the combination of unique labels (cardinality) is very significant for some of our ingress metrics.&lt;/p&gt;

&lt;p&gt;We found that Prometheus performed poorly when we had multiple metrics with high cardinalities (&amp;gt; 100,000), and resulted in over-consumption of RAM.&lt;/p&gt;

&lt;p&gt;During a high load event, Prometheus could consume up to 200 GB of RAM before being OOMKilled. When this happened, we would go completely blind as we had no metrics or alerting.
This also impacts us on scalability in our Kubernetes clusters, as we use &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics&quot;&gt;CustomMetrics&lt;/a&gt; very heavily in HPAs to scale the number of pods in our applications.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;RAM and CPU consumption of our prometheus instances (the red lines represent the reboots of our instances, we also see a loss of metrics)&lt;/em&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/prometheus-ram.png&quot; alt=&quot;RAM consumption of prometheus &quot; /&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/prometheus-cpu.png&quot; alt=&quot;RAM consumption of prometheus &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Prometheus is still a good solution, which has served its purpose well for several years, but we have reached its limits in our production environments.&lt;/p&gt;

&lt;h1 id=&quot;replacing-prometheus&quot;&gt;Replacing Prometheus?&lt;/h1&gt;

&lt;p&gt;We spent time optimizing Prometheus to absorb the amount of metrics and their cardinality, in particular by either directly removing high cardinality metrics if they were totally unused, or by removing the labels of certain metrics that caused high cardinalities.
We have also optimized the Prometheus configuration directly, as well as the maximum IOPS of our EBS. The RAM and CPU consumption of Prometheus is linked to the number of metrics to manage and their cardinality. But we always have more traffic and therefore always more pods in our clusters: we should have perpetually increased Prometheus instances resources. This was a problem for scalability and costs.&lt;/p&gt;

&lt;p&gt;Can we replace a critical tool like this? What are our short, medium and long term needs? How can we optimize costs? And especially in what timeframe?
The emergency of recent incidents forced us to exclude solutions such as &lt;a href=&quot;https://thanos.io/&quot;&gt;Thanos&lt;/a&gt; and &lt;a href=&quot;https://cortexmetrics.io/&quot;&gt;Cortex&lt;/a&gt;. Testing these solutions completely would have required too much time, which we did not have.&lt;/p&gt;

&lt;p&gt;It is also important to consider that we were already using Victoria Metrics, but only for the Long Term Storage part, without any problems.
Could replacing Prometheus with a stack based entirely on Victoria Metrics overcome the limitations we had with Prometheus?
High availability and fault tolerance is well-supported, their &lt;a href=&quot;https://docs.victoriametrics.com/guides/k8s-ha-monitoring-via-vm-cluster.html&quot;&gt;documentation&lt;/a&gt; explains how to manage this.&lt;/p&gt;

&lt;p&gt;Managing long-term data is possible, as we were already doing it.
Victoria Metrics is built around a set of microservices. Each one is built in order to serve a specific job, and each supports vertical and especially horizontal scaling (with sharding). A very important point when used in a Kubernetes environment.&lt;/p&gt;

&lt;p&gt;In addition, Victoria Metrics seemed to handle high cardinality metrics better (see &lt;a href=&quot;https://valyala.medium.com/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b&quot;&gt;article&lt;/a&gt; on this subject). It is also possible to do &lt;a href=&quot;https://docs.victoriametrics.com/vmagent.html#cardinality-limiter&quot;&gt;rate limiting&lt;/a&gt; on the number of Time Series to be ingested:&lt;/p&gt;

&lt;p&gt;CPU and RAM consumption is lower with better performance than with Prometheus and even other TSDBs, several comparative articles on this subject have already been published:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://promcon.io/2019-munich/talks/remote-write-storage-wars/&quot;&gt;Remote Write Storage Wars&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f&quot;&gt;Prometheus vs VictoriaMetrics benchmark on node_exporter metrics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://valyala.medium.com/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4&quot;&gt;When size matters — benchmarking VictoriaMetrics vs Timescale and InfluxDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/faun/comparing-thanos-to-victoriametrics-cluster-b193bea1683&quot;&gt;Comparing Thanos to VictoriaMetrics cluster&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also wanted to keep the Prometheus language: &lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/querying/basics/&quot;&gt;PromQL&lt;/a&gt; in order to keep our Grafana dashboards and all our Prometheus alerts. Even though Victoria Metrics offers its own MetricsQL language, it is perfectly compatible with PromQL.&lt;/p&gt;

&lt;p&gt;You can see the &lt;a href=&quot;https://docs.victoriametrics.com/#prominent-features&quot;&gt;main features&lt;/a&gt; of Victoria Metrics as well as various &lt;a href=&quot;https://docs.victoriametrics.com/#case-studies-and-talks&quot;&gt;case studies&lt;/a&gt; in their documentation.&lt;/p&gt;

&lt;h1 id=&quot;poc-of-victoria-metrics&quot;&gt;POC of Victoria Metrics&lt;/h1&gt;
&lt;p&gt;We wanted to validate the performance and consumption of a stack entirely based on Victoria Metrics, the results were really encouraging.&lt;/p&gt;

&lt;p&gt;Test environment :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1500 web app pods&lt;/li&gt;
  &lt;li&gt;250 Haproxy Ingress pods (metric with high cardinality enabled)&lt;/li&gt;
  &lt;li&gt;3700 scrapped targets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Comparative table between Prometheus and Victoria Metrics :&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Prometheus&lt;/th&gt;
      &lt;th&gt;Victoria Metrics&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CPU consumption&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RAM consumption&lt;/td&gt;
      &lt;td&gt;30Go&lt;/td&gt;
      &lt;td&gt;11Go&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;New TimeSeries / min&lt;/td&gt;
      &lt;td&gt;50K&lt;/td&gt;
      &lt;td&gt;6.5M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max active TimeSeries&lt;/td&gt;
      &lt;td&gt;7M&lt;/td&gt;
      &lt;td&gt;91M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max cardinality&lt;/td&gt;
      &lt;td&gt;4 metrics &amp;gt; 100K&lt;/td&gt;
      &lt;td&gt;10+ metrics &amp;gt; 1M&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Graph on the CPU consumption of Victoria Metrics components&lt;/em&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/cpu-poc-vm.png&quot; alt=&quot;cpu-usage-poc-vm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Number of active “TimeSeries” in Victoria Metrics&lt;/em&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/active-ts-poc-vm.png&quot; alt=&quot;active-ts-poc-vm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our benchmark persuaded us to use Victoria Metrics as a replacement for Prometheus.&lt;/p&gt;

&lt;h1 id=&quot;implementation-of-victoria-metrics-&quot;&gt;Implementation of Victoria Metrics :&lt;/h1&gt;
&lt;p&gt;We used the official &lt;a href=&quot;https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-k8s-stack/README.md&quot;&gt;victoria-metrics-k8s-stack&lt;/a&gt; Helm chart which is based on an &lt;a href=&quot;https://github.com/VictoriaMetrics/helm-charts/tree/master/charts/victoria-metrics-operator&quot;&gt;operator&lt;/a&gt;. This chart Helm permits to deploy a complete monitoring and alerting stack in a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;A VMCluster (Insert, Select, Storage) is deployed to manage access to metrics. The collection of metrics (push/pull) from exporters in Prometheus format is handled by the VMagent. Its configuration is done in the form of a Prometheus configuration file. It is able to :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Manage the relabeling of metrics.&lt;/li&gt;
  &lt;li&gt;Temporarily store the metrics it has collected if the VMCluster is unavailable or not able to send the metrics to the VMCluster.&lt;/li&gt;
  &lt;li&gt;Limit the cardinality of metrics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the advantages of using this Helm chart is that it will deploy essential components to properly monitor a Kubernetes cluster such as Kube-state-metrics or prometheus-node-exporter, but also scraping configurations for services such as Kubelet, KubeApiServer, KubeControllerManager, KubeDNS, KubeEtcd, KubeScheduler, KubeProxy&lt;/p&gt;

&lt;p&gt;Alerting is also managed via a VMAlert component, which will execute the alerting and recording rules set by VictoriaMetrics. Notifications are managed by an Alertmanager which is also deployable via this chart.&lt;/p&gt;

&lt;p&gt;One of the advantages of using this Helm chart is that it will deploy essential components to properly monitor a Kubernetes cluster such as &lt;em&gt;Kube-state-metrics&lt;/em&gt; or &lt;em&gt;prometheus-node-exporter&lt;/em&gt;, but also scraping configurations for services such as &lt;em&gt;Kubelet, KubeApiServer, KubeControllerManager, KubeDNS, KubeEtcd, KubeScheduler, KubeProxy&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is what our monitoring and alerting stack based on this Helm chart looks like.&lt;/em&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/stack-vm.png&quot; alt=&quot;stack-vm&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;resumption-of-the-history&quot;&gt;Resumption of the history&lt;/h1&gt;
&lt;p&gt;We wanted to keep historical metrics of our Kubernetes clusters. Victoria Metrics provides a tool to manage the export and import of data from different TSDB: &lt;a href=&quot;https://docs.victoriametrics.com/vmctl.html&quot;&gt;vmctl&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order not to overload our monitoring stack, we splitted the exports into smaller or larger time ranges, depending on the history of the cluster. For clusters with little activity and therefore few metrics, exports/imports were split day by day, for others we had to use smaller time slots.
A home-made bash script launched several kubernetes jobs simultaneously and took care of restarting one of them as soon as another one ended.&lt;/p&gt;

&lt;p&gt;Below an extract of the definition of our Kubernetes job with the arguments we used to do our history transfer by time range:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vmctl&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;victoriametrics/vmctl&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vm-native&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--vm-native-src-addr=http://victoria-metrics-cluster-vmselect.monitoring.svc.cluster.local.:8481/select/001/prometheus&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--vm-native-dst-addr=http://vminsert-victoria-metrics-k8s-stack.monitoring.svc.cluster.local.:8480/insert/000/prometheus&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--vm-native-filter-match={__name__!~&quot;_vm.*&quot;}&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--vm-native-filter-time-start=&quot;{ { start } }&quot;&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--vm-native-filter-time-end=&quot;{ { end } }&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;restartPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Never&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;feedback-after-months-of-use&quot;&gt;Feedback after months of use&lt;/h1&gt;
&lt;p&gt;Since we have been using our new monitoring stack, we have encountered a few bugs (as with all solutions).
Most of the time, these were not impactful, except for one that caused us a production incident.
We had an overconsumption of RAM of VMStorage which was fixed in version 1.76. I would like to highlight the responsiveness of the VictoriaMetrics team, whether on slack or on GitHub: I have had several discussions with them on various subjects, and they have always been reactive&lt;/p&gt;

&lt;p&gt;Victoria Metrics regularly releases new versions, including performance improvements and new features. The &lt;a href=&quot;https://docs.victoriametrics.com/CHANGELOG.htm&quot;&gt;changelog&lt;/a&gt; will give you an idea of the latest improvements and their frequency.&lt;/p&gt;

&lt;p&gt;Victoria Metrics has an &lt;a href=&quot;https://victoriametrics.com/products/enterprise/&quot;&gt;Enterprise&lt;/a&gt; version that adds some features, including one that we are interested in but have not yet tested: downsampling.
We have configured a one-year retention for each of our Kubernetes clusters, and on some clusters that’s mean more than 7 TB of data per VMStorage.&lt;/p&gt;

&lt;p&gt;The downsampling allows you to configure how many metrics you want to keep per time interval.&lt;/p&gt;

&lt;p&gt;In this example: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-downsampling.period=24h:10s,1w:30s,30d:1m,360d:5m&lt;/code&gt;, (assuming we collect metrics every 5 seconds) we only keep:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;one measurement point every 10 seconds beyond 24 hours (instead of one point every 5 seconds)&lt;/li&gt;
  &lt;li&gt;one measurement point every 30 seconds beyond 7 days&lt;/li&gt;
  &lt;li&gt;one measurement point every minute beyond 30 days&lt;/li&gt;
  &lt;li&gt;one measurement point every 5 minutes beyond one year&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is rarely necessary to keep all the measurements of our metrics on such a long scale, when we want to retrieve measurements that are several months old, it is usually to see a trend and not all the measurements.
With this option, we could greatly reduce the storage used by our metrics.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Through this article, you have discovered why and how we migrated our monitoring stack of our Kubernetes clusters at Bedrock from Prometheus to Victoria Metrics.&lt;/p&gt;

&lt;p&gt;This was an important and critical subject for us, as monitoring is a critical need.
Now our monitoring stack, based entirely on Victoria Metrics, is robust and capable of absorbing large load peaks.&lt;/p&gt;

&lt;p&gt;Here are some indicators of the victoria metrics stack performance of one of our Kubernetes clusters during last 6 months:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;active time series: up to 39 million (average 7.4M)&lt;/li&gt;
  &lt;li&gt;total number of datapoints: 12 trillion&lt;/li&gt;
  &lt;li&gt;ingestion rate : up to 1.3 million new samples per second (average 227K)&lt;/li&gt;
  &lt;li&gt;churn rate : up to 117 Million new time series per day (average 30.6 Million)&lt;/li&gt;
  &lt;li&gt;disk usage (data + index): 15 TB&lt;/li&gt;
  &lt;li&gt;sample rate : up to 4.99M (average 343K)&lt;/li&gt;
  &lt;li&gt;scrape target : up to 49K (average 4.4K)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/total-datapoint-vm-last-6m.png&quot; alt=&quot;total-datapoint-vm-last-6m&quot; /&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/active-ts-vm-last-6m.png&quot; alt=&quot;active-ts-vm-last-6m&quot; /&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/ingestion-rate-vm-last-6m.png&quot; alt=&quot;ingestion-rate-vm-last-6m&quot; /&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/cpu-usage-vm-last-6m.png&quot; alt=&quot;cpu-usage-vm-last-6m&quot; /&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/ram-usage-vm-last-6m.png&quot; alt=&quot;ram-usage-vm-last-6m&quot; /&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/churn-rate-vm-last-6m.png&quot; alt=&quot;churn-rate-vm-last-6m&quot; /&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/sample-rate-vm-last-6m.png&quot; alt=&quot;sample-rate-vm-last-6m&quot; /&gt;
&lt;img src=&quot;/images/posts/2022-09-06-monitoring-at-scale-with-victoriametrics/scrape-target-vm-last-6m.png&quot; alt=&quot;scrape-target-vm-last-6m&quot; /&gt;&lt;/p&gt;</content><author><name>Julien Menan</name></author><category term="k8s" /><category term="kubernetes" /><category term="monitoring" /><category term="prometheus" /><category term="scaling" /><category term="victoriametrics" /><category term="cardinality" /><summary type="html">Monitoring at Bedrock : At Bedrock Streaming, a large part of our applications are hosted on Kubernetes clusters, others use the EC2 service from AWS and a small part are hosted on “OnPremise” servers.</summary></entry><entry><title type="html">Is machine learning a unicorn hiding a series of if and else?</title><link href="https://tech.bedrockstreaming.com/2022/09/05/machine-learning-if-else.html" rel="alternate" type="text/html" title="Is machine learning a unicorn hiding a series of if and else?" /><published>2022-09-05T00:00:00+00:00</published><updated>2022-09-05T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/09/05/machine-learning-if-else</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/09/05/machine-learning-if-else.html">&lt;p&gt;Recently, a colleague asked me:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;All good with your if and else machine learning system?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It was a joke but this one made me think.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is a running gag: machine learning is only a series of if and else.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-05-machine-learning-if-else/unicorn_forest.jpg&quot; alt=&quot;unicorn in the forest&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Beyond the joke, it is true?&lt;/p&gt;

&lt;p&gt;Yes! …and no. As always, it depends.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quick answer&lt;/strong&gt;: Machine learning is a bunch of mathematical and statistical operations. Sometimes, the operations you use can be translated into &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt; clauses, and sometimes not. But you never write the series of &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt; yourself.&lt;/p&gt;

&lt;h2 id=&quot;a-recap-of-machine-learning&quot;&gt;A recap of machine learning&lt;/h2&gt;
&lt;p&gt;The idea of machine learning is: you have some data, and you apply an algorithm to these to detect a pattern. You put this pattern into a function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-05-machine-learning-if-else/ML%20recap.png&quot; alt=&quot;machine learning representation schema&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, you’ll be able to use this function on new data to extract new information.&lt;/p&gt;

&lt;h2 id=&quot;a-decision-tree-with-a-series-of-if-and-else&quot;&gt;A decision tree with a series of if and else&lt;/h2&gt;

&lt;p&gt;There are different types of machine learning. If you decide to build a decision tree (a famous way to do machine learning) to know the form of a diamond, you’ll get something like that:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-05-machine-learning-if-else/decision_tree.png&quot; alt=&quot;decision tree&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you translate it with code, you’ll get something like that:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;carat&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diamond&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;red&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;red&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grey&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pentagon&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Then, yes, you can see that here, you have a series of &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;And decision trees are used a lot in machine learning. Most of the time, you don’t use decision trees directly but forests of decision trees in the Random Forest algorithm or a series of decision trees in the Gradient Boosted Trees algorithm.&lt;/p&gt;

&lt;h2 id=&quot;but-many-algorithms-in-machine-learning-are-just-the-generation-of-plain-mathematical-formulas&quot;&gt;But, many algorithms in machine learning are just the generation of plain mathematical formulas&lt;/h2&gt;

&lt;p&gt;Let’s take another famous way to do machine learning: a neural network. What you’ll get at the end is more something like that:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a*10+b*15+c*16+20…
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, the process doesn’t try to find a series of &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt;, but a mathematical formula.&lt;/p&gt;

&lt;p&gt;I would like to finish with a last example: recommender systems. There are many ways to build a recommendation system. One which is well known is matrix factorization.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Matrix factorization, what?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I won’t explain deeply what it is about, but as a sum up, it’s a manipulation of matrices. It comes from linear algebra.
Here is a definition: &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_decomposition&quot;&gt;Matrix decomposition&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The result is something like that:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Vector A * Vector B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;As a result, yes, you have types of machine learning that will generate a series of &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt;. But, you have also plenty of algorithms that try to find the variables of an equation or vectors.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;you-never-write-the-series-of-if-and-else-yourself&quot;&gt;You never write the series of if and else yourself&lt;/h2&gt;

&lt;p&gt;Let’s go back to the decision tree. As you’ve seen previously, the result could be translated as a series of &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;But, you don’t write directly this code. You generate it using… mathematical operations. Yes, again!&lt;/p&gt;

&lt;p&gt;As an example, you can get the result of a decision tree using an optimisation algorithm with the Shannon Entropy formula:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-05-machine-learning-if-else/formula.png&quot; alt=&quot;shannon entropy formula&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s suppose you want to guess the form (pentagon or plate) of a diamond according to its attributes. You have three diamonds:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;carat&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;size&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;form&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;high&lt;/td&gt;
      &lt;td&gt;small&lt;/td&gt;
      &lt;td&gt;plate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;low&lt;/td&gt;
      &lt;td&gt;high&lt;/td&gt;
      &lt;td&gt;plate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;low&lt;/td&gt;
      &lt;td&gt;small&lt;/td&gt;
      &lt;td&gt;pentagon&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The process is the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The data is split randomly: a random &lt;em&gt;if&lt;/em&gt; statement is created like &lt;em&gt;if carat is high&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;The process checks if it helps to generate a more accurate view of the data: by doing this &lt;em&gt;if&lt;/em&gt;, are the data separated correctly? Do we have pentagons mostly from one side and plates from another?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;To be able to know if the data are separated correctly, the Shannon entropy formula is used&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;if yes, the process keeps the &lt;em&gt;if carat is high&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;if not, it generates another one&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then, by keeping the &lt;em&gt;if&lt;/em&gt; you get something like that:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-05-machine-learning-if-else/decision_tree_first_step.png&quot; alt=&quot;decision tree - first step&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The translation with a code is:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;carat&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diamond&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;red&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#The process doesn&apos;t know yet how to handle that
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that you have a branch (below &lt;em&gt;low&lt;/em&gt;) with a plate and a pentagon. It corresponds to the &lt;em&gt;else&lt;/em&gt; where the process doesn’t know what to put yet.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;So, the data below &lt;em&gt;low&lt;/em&gt; is split randomly: another &lt;em&gt;if&lt;/em&gt; is created&lt;/li&gt;
  &lt;li&gt;The process checks if it helps to generate a more accurate view of the data&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;if yes, the process keeps the new &lt;em&gt;if&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;if not, it generates another one&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By keeping the new &lt;em&gt;if&lt;/em&gt;, you get another branch:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-05-machine-learning-if-else/decision_tree_second_step.png&quot; alt=&quot;decision tree - second step&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The translation with a code is:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;red&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grey&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pentagon&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At the end, you get a final tree decision:
&lt;img src=&quot;/images/posts/2022-09-05-machine-learning-if-else/decision_tree_final.png&quot; alt=&quot;decision tree - final&quot; /&gt;&lt;/p&gt;

&lt;p&gt;with a final code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;carat&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diamond&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;red&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;red&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grey&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pentagon&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, in real life, data are more complicated and the process must iterate a lot until getting the perfect tree. The process used is an optimisation algorithm. This is the part called &lt;em&gt;learning&lt;/em&gt; in machine learning.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Mathematical optimization […] is the selection of a best element, with regard to some criterion, from some set of available alternatives (definition from Wikipedia)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you want to know how the Shannon entropy works with mathematical formulas, you’ve got this article: &lt;a href=&quot;https://medium.zenika.com/classification-in-machine-learning-example-of-decision-tree-with-shannon-entropy-945fc8e2a3fb&quot;&gt;Classification in machine learning - Example of Decision Tree with Shannon Entropy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Then as a result, yes, you can have machine learning algorithms that will build a series of &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt;. But to generate it, you’ll use mathematical operations.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note that for other algorithms such as the matrix factorisation or neural networks, you don’t use a process with the Shannon entropy formula, but other optimisation algorithms that don’t generate a series of if and else but, as previously seen, vectors or formulas.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;so-why-do-we-sometimes-say-that-machine-learning-is-a-bunch-of-if-and-else-statements&quot;&gt;So why do we sometimes say that machine learning is a bunch of if and else statements?&lt;/h2&gt;

&lt;p&gt;To my opinion, because of expert systems. They are the ancestors of machine learning in artificial intelligence.&lt;/p&gt;

&lt;p&gt;Artificial intelligence is a way to simulate human cognitive abilities. In the history of artificial intelligence, people thought that they would be able to target that with expert systems. These are big series of hardcoded rules and then… of &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To conclude, most of the time, machine learning is not a series of &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;else&lt;/em&gt;. It’s just mathematics and for some techniques, they are very old. I’m thinking of linear regressions or Bayesian probabilities. These were used long before the existence of computers.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Photo of the unicorn by &lt;a href=&quot;https://unsplash.com/@stephenleo1982?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Stephen Leonardi&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/s/photos/unicorn?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Unsplash&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</content><author><name>Nastasia Saby</name></author><category term="machine learning" /><category term="Data Science" /><summary type="html">Recently, a colleague asked me:</summary></entry><entry><title type="html">Using a circuit breaker to spare the API we are calling</title><link href="https://tech.bedrockstreaming.com/2022/09/02/backend-circuit-breaker.html" rel="alternate" type="text/html" title="Using a circuit breaker to spare the API we are calling" /><published>2022-09-02T00:00:00+00:00</published><updated>2022-09-02T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/09/02/backend-circuit-breaker</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/09/02/backend-circuit-breaker.html">&lt;p&gt;Hi! We’re going to start our &lt;a href=&quot;#from-the-same-series&quot;&gt;fourth article&lt;/a&gt; about Bedrock’s API gateway.
Today we will talk about the circuit breaker pattern, what it is, and how we’re using it.&lt;/p&gt;

&lt;h2 id=&quot;the-circuit-breaker-pattern&quot;&gt;The Circuit Breaker Pattern&lt;/h2&gt;

&lt;p&gt;With this pattern, our API Gateway detects errors when calling its dependencies.
It will stop calling them if a given threshold (ratio of errors) is crossed.&lt;/p&gt;

&lt;p&gt;The circuit breaker allows us to spare the dependencies in difficulty, but also avoid taking time to do something that will most likely fail.&lt;/p&gt;

&lt;p&gt;You’ll find a more detailed explanation about the circuit breaker on Martin FOWLER’s &lt;a href=&quot;https://martinfowler.com/bliki/CircuitBreaker.html&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;where-to-use-it&quot;&gt;Where to use it?&lt;/h2&gt;

&lt;p&gt;As soon as a service call is not mandatory for our BFF to answer something that a frontend application can read, then we can use the circuit breaker pattern.&lt;/p&gt;

&lt;p&gt;If an API cannot handle a sudden increase in traffic (for example: it’s not scaling fast enough or its database starts to throttle), it’s better to stop calling it temporarily.
When the right timeouts are configured, an API throttling will result in an error, as seen &lt;a href=&quot;/2022/08/25/backend-errors-connections.html&quot;&gt;in the previous article&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are some examples:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Video progress information&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Displaying a video progress bar is useful for end users, but it’s better to not display this information instead of risking the entire page to not be displayed!
If the service that stores video viewing sessions is (slowing) down, we can stop asking for this information and stop displaying the video progress bar.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-02-backend-circuit-breaker/progress-bar.png&quot; alt=&quot;a video with a progress bar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;User geolocation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The geolocation service allows us to know where the end user is in the world. Based on this information we lock some area restricted contents.
If this service goes down for some reason, we will stop calling it, and instead use a default area matching the area of our customer as it’s the majority case.&lt;/p&gt;

&lt;h2 id=&quot;implementation-and-configuration&quot;&gt;Implementation and configuration&lt;/h2&gt;

&lt;p&gt;So far we’re only using the circuit breaker pattern with HTTP calls.
This is made possible thanks to the &lt;a href=&quot;https://github.com/ackintosh/ganesha&quot;&gt;Ganesha library&lt;/a&gt;, and its Guzzle middleware.&lt;/p&gt;

&lt;p&gt;The Guzzle middleware is created as a service within the Symfony service definitions.
It’s then injected into our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HttpClientFactory&lt;/code&gt; that will handle the creation of all the different clients.
The responsibility of using the circuit breaker falls on each service that will create a http client.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;Ackintosh\Ganesha\GuzzleMiddleware&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;factory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@...Infrastructure\HttpClient\CircuitBreaker\CircuitBreakerMiddlewareFactory&apos;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;buildWithRateStrategy&apos;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;arguments&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;$timeWindow&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;60&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;$failureRateThreshold&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;40&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;$minimumRequests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;$intervalToHalfOpen&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;60&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;monitoring-the-circuit-breaker&quot;&gt;Monitoring the circuit breaker&lt;/h2&gt;

&lt;p&gt;At Bedrock, we’re used to monitor everything. The circuit breaker makes no exception to this rule.
Usually we store time spent and response code for every outgoing http call.
To see when the circuit breaker is open, we catch the ganesha’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RejectedException&lt;/code&gt; to save a dedicated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;666&lt;/code&gt; http status.&lt;/p&gt;

&lt;p&gt;This allows us to look for the exact number of calls avoided.
Below lies an example of a monitoring chart showing some errors happening during a usual night.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-02-backend-circuit-breaker/monitoring-1.png&quot; alt=&quot;monitoring excluding less reliable services&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also have to query slower services that often trigger our circuit breaker because they cannot answer in the short timeout we impose.
Thereafter, the same monitoring chart including such services.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-02-backend-circuit-breaker/monitoring-2.png&quot; alt=&quot;monitoring including less reliable service&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;going-further&quot;&gt;Going further&lt;/h2&gt;

&lt;p&gt;So far, we have identified two areas for improvements described below.&lt;/p&gt;

&lt;h3 id=&quot;different-configurations&quot;&gt;Different configurations&lt;/h3&gt;

&lt;p&gt;We’re only using a single configuration for the circuit breaker.
We should allow each service to choose from a named list of configurations when creating a client, &lt;a href=&quot;/2022/08/25/backend-errors-connections.html&quot;&gt;similarly to the different guzzle configuration we are using&lt;/a&gt;.
The main obstacle is a lack of hindsight which prevent us to have fine-tuned values.
This is something that will definitively be improved over time as we monitor over long period.&lt;/p&gt;

&lt;h3 id=&quot;staled-cache-when-the-circuit-breaker-is-open&quot;&gt;Staled cache when the circuit breaker is open&lt;/h3&gt;

&lt;p&gt;For many editorial contents, we’re using a staled cache version of the data as a fallback.
To do so, we’re using &lt;a href=&quot;https://github.com/Kevinrob/guzzle-cache-middleware&quot;&gt;another guzzle middleware&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sadly, the two middlewares don’t work together. We have to chose which one to use based on the criticality of the content and the API behind. 
This is something that we aim at solving with a bit of R&amp;amp;D.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In today’s post we’ve seen our usage of the circuit breaker pattern.
It allows us to spare the services we are calling, and avoid slowing us down in case of throttling.&lt;/p&gt;

&lt;p&gt;Next time, we will talk about our ultimate layer of protection to ensure the BFF always responds something readable to frontend applications.&lt;/p&gt;

&lt;h2 id=&quot;from-the-same-series&quot;&gt;From the same series&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/06/10/backend-bff-intro.html&quot;&gt;What’s a BFF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/08/12/backend-fallbacks.html&quot;&gt;Handling API failures in a gateway&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/08/25/backend-errors-connections.html&quot;&gt;What’s an error, and handling connexion to multiple APIs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/09/02/backend-circuit-breaker.html&quot;&gt;Using a circuit breaker&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Valentin CLARAS</name></author><category term="backend" /><category term="php" /><category term="api" /><category term="api-gateway" /><category term="back-for-front" /><category term="resiliency" /><category term="circuit-breaker" /><summary type="html">Hi! We’re going to start our fourth article about Bedrock’s API gateway. Today we will talk about the circuit breaker pattern, what it is, and how we’re using it.</summary></entry><entry><title type="html">Prescaling pods in Kubernetes, we open source our solution</title><link href="https://tech.bedrockstreaming.com/2022/09/01/kubernetes-prescaling-we-open-source-our-solution.html" rel="alternate" type="text/html" title="Prescaling pods in Kubernetes, we open source our solution" /><published>2022-09-01T00:00:00+00:00</published><updated>2022-09-01T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/09/01/kubernetes-prescaling-we-open-source-our-solution</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/09/01/kubernetes-prescaling-we-open-source-our-solution.html">&lt;p&gt;Previously we &lt;a href=&quot;https://tech.bedrockstreaming.com/2022/02/03/prescaling.html&quot;&gt;discussed&lt;/a&gt; how we manage the load of our Kubernetes clusters and how we can anticipate our needs with prescaling. Today, we are here to share our solution that we have reworked and open sourced! 
&lt;img src=&quot;/images/posts/2022-09-01-kubernetes-prescaling-we-open-source-our-solution/br-opensource.png&quot; alt=&quot;BedrockStreaming Logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At &lt;a href=&quot;https://www.bedrockstreaming.com/&quot;&gt;Bedrock Streaming&lt;/a&gt;, we provide streaming platforms to our customers (6play, Salto, Videoland and many others), we have a good knowledge of the daily load peaks and we know in advance the programs that are likely to generate a lot of traffic. We can therefore rely not only on reactive scaling, which has its limits (cf. &lt;a href=&quot;https://tech.bedrockstreaming.com/2022/02/03/prescaling.html&quot;&gt;prescaling article&lt;/a&gt;) but also on prescaling.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;gt; &lt;strong&gt;Prescaling&lt;/strong&gt; consists of increasing the number of critical application pods in our clusters in advance in order to be ready to face a sudden traffic peak.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Initially, we developed an in-house solution in Python for a simple reason: it was the language that most people in the team knew. Since we had time to test our solution, we thought it would be great to share it with everyone. But to do so, we had to make some adjustments.&lt;/p&gt;

&lt;h2 id=&quot;we-rewrote-everything-in-go&quot;&gt;We rewrote everything in go&lt;/h2&gt;

&lt;p&gt;Many open source projects we use are written in Golang. In addition, the DevOps/Cloud world is mostly focused on Go. So, we decided to rewrite our prescaling solution in Go in order to make our teams more skilled in this language. The other goal was to make it cloud agnostic. In the Python version, we had an API part that stored prescaling events in a DynamoDB table, which made the solution dependent on AWS. Since prescaling is Kubernetes oriented, we had thought in the first versions in Python to store these events in Custom Resources (CRD) but due to lack of time, we did not implement it. We took advantage of the redesign to implement it and remove the dependency with AWS DynamoDB.&lt;/p&gt;

&lt;p&gt;We also wanted to simplify the project. In the first versions, we had two bricks: one containing the exporter and another the API. We merged the two applications into one monolith. The API is CRUD and can handle CRD events.&lt;/p&gt;

&lt;h2 id=&quot;here-we-go-we-open-source-it&quot;&gt;Here we go, we open source it&lt;/h2&gt;

&lt;p&gt;The great moment has come. Our prescaling solution is now available on GitHub in its alpha version: &lt;a href=&quot;https://github.com/BedrockStreaming/prescaling-exporter&quot;&gt;https://github.com/BedrockStreaming/prescaling-exporter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is the version we currently use in all our clusters. Let’s quickly see how to implement the solution (you can find more details in the repo README).&lt;/p&gt;

&lt;p&gt;The prescaling-exporter is distributed with helm charts in order to install it in kubernetes cluster.&lt;/p&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;

&lt;p&gt;The following bricks must be installed in the k8s cluster:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Prometheus&lt;/code&gt; Stack or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Victoria Metrics Stack&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Prometheus Adapter&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is possible to use another metrics stack but we do not provide an example at this time.&lt;/p&gt;

&lt;p&gt;Clone the repo and run the following command with Helm3:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;prescaling-exporter ./helm/prescaling-exporter &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; prescaling-exporter &lt;span class=&quot;nt&quot;&gt;--create-namespace&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It’s required to add the following configuration to Prometheus adapter:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- &quot;metricsQuery&quot;: &quot;avg(&amp;lt;&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;})&quot;
    &quot;name&quot;:
      &quot;as&quot;: &quot;prescale_metric&quot;
    &quot;resources&quot;:
      &quot;overrides&quot;:
        &quot;namespace&quot;:
          &quot;resource&quot;: &quot;namespace&quot;
    &quot;seriesQuery&quot;: &quot;prescale_metric&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;daily-prescaling-event&quot;&gt;Daily prescaling event&lt;/h3&gt;

&lt;p&gt;We have chosen to manage the configuration of daily events directly on the HPA (HorizontalPodAutoscaler) of the applications. Here is how to activate it, through annotations:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;autoscaling/v2beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;HorizontalPodAutoscaler&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;annotations.scaling.exporter.replica.min&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;annotations.scaling.exporter.time.end&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;annotations.scaling.exporter.time.start&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;External&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;external&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;metricName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;prescaling_metric&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;metricSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;deployment&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;targetValue&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We are able to control the start and end time of the prescaling and the minimum number of pods we want during this window. Please note that if the number of pods we want for prescaling is less than the current number of pods, the solution will not downscale the application and the HPA will continue to behave as usual.&lt;/p&gt;

&lt;h3 id=&quot;one-time-events&quot;&gt;One-time events&lt;/h3&gt;

&lt;p&gt;We can also record one-off events. For example, at Bedrock Streaming, during an important soccer match, we will record a special event in a Custom Resource Definition. 
One-time events allow to prescale all applications having annotations on their HPA by multiplying their prescaling minimum replicas (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;annotations.scaling.exporter.replica.min&lt;/code&gt;) by the multiplier of the event in question.&lt;/p&gt;

&lt;p&gt;To record a one-time event, an OpenAPI UI (formerly known as Swagger) is exposed by the prescaling exporter at the url &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/swagger/index.html&lt;/code&gt;. We can also register a new event from here or directly by making an api call to the following address &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/api/v1/events/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-09-01-kubernetes-prescaling-we-open-source-our-solution/post-prescaling-event.png&quot; alt=&quot;Screenshot POST prescaling event&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;We will continue to improve the solution. For example, we are thinking about removing annotations on HPAs and replacing them with a new dedicated CRD.&lt;/p&gt;

&lt;p&gt;All contributions are welcome, don’t hesitate to come and exchange with us on GitHub if you want to use the solution, we would be delighted.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Authors:&lt;a href=&quot;https://www.linkedin.com/in/jeremy-planckeel-44426112b/&quot;&gt; Jérémy Planckeel&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/valentin-chabrier-180937142/&quot;&gt;Valentin Chabrier&lt;/a&gt;&lt;/p&gt;</content><author><name>[&quot;j_planckeel&quot;, &quot;v_chabrier&quot;]</name></author><category term="k8s" /><category term="kubernetes" /><category term="pods" /><category term="prometheus" /><category term="scaling" /><category term="hpa" /><category term="resiliency" /><category term="go" /><category term="prescaling" /><category term="opensource" /><summary type="html">Previously we discussed how we manage the load of our Kubernetes clusters and how we can anticipate our needs with prescaling. Today, we are here to share our solution that we have reworked and open sourced!</summary></entry><entry><title type="html">BFF’s error definition, and handling connections to multiple API</title><link href="https://tech.bedrockstreaming.com/2022/08/25/backend-errors-connections.html" rel="alternate" type="text/html" title="BFF’s error definition, and handling connections to multiple API" /><published>2022-08-25T00:00:00+00:00</published><updated>2022-08-25T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/08/25/backend-errors-connections</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/08/25/backend-errors-connections.html">&lt;p&gt;A &lt;em&gt;quick&lt;/em&gt; sidetrack in &lt;a href=&quot;#from-the-same-series&quot;&gt;our series&lt;/a&gt; about Bedrock’s API gateway.
This piece defines what are we talking about when we say “an error”, and explains how we handle the numerous connections to services we are calling.&lt;/p&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&quot;/2022/08/12/backend-fallbacks.html&quot;&gt;the previous article&lt;/a&gt;, we’ve seen how we handle errors.
This was mainly from a business point of view, and how it’s done in our domain.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But what is “an error”?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This term is a bit generic, and the definition will be too: &lt;em&gt;an error is anything unexpected by the application&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In our context of an API Gateway, we are restricting this to the services we are calling.&lt;/p&gt;

&lt;p&gt;This can be, but not exhaustively, a service not responding because:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it’s offline;&lt;/li&gt;
  &lt;li&gt;it’s taking too much time to answer;&lt;/li&gt;
  &lt;li&gt;it’s responding with a 5** error (when talking about an API);&lt;/li&gt;
  &lt;li&gt;it’s giving us an invalid or unexpected content.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-are-the-consequences-of-those-errors&quot;&gt;What are the consequences of those errors?&lt;/h2&gt;

&lt;p&gt;The first issue is: we won’t be able to display some part of the application as intended.
We’ve &lt;a href=&quot;https://tech.bedrockstreaming.com/2022/08/12/backend-fallbacks.html#handling-failures&quot;&gt;talked about this previously&lt;/a&gt; already.&lt;/p&gt;

&lt;p&gt;The second error, more insidious, is that it can slow down our BFF terribly.&lt;/p&gt;

&lt;p&gt;The BFF response time is, on average, equals to the slowest service the BFF is calling.
If a service that usually responds in 200ms starts slowing down to an average response time of 1s and also times out half the time, it will increase the BFF response time to 1,5s (1s average, and 50% retry).&lt;/p&gt;

&lt;p&gt;That’s why we must be careful when configuring those timeouts.
The BFF exposes a response-time Service Level Objective (SLO), and frontend applications will cut any connection that takes too long.
Losing some parts of the responses is better than slowing the BFF down to a point where frontend won’t get any response at all.&lt;/p&gt;

&lt;h2 id=&quot;how-are-we-mitigating-the-errors&quot;&gt;How are we mitigating the errors?&lt;/h2&gt;

&lt;p&gt;For any remote service, we configure short timeouts, and retry when we must.
A short timeout is a timeout that usually match the SLO of the called services, and that will match 99% of our calls.
When the SLO of the called service is higher than ours, we use a shorter timeout and accept that a larger parts of the calls will be cut.
The values are tailored according to our usages.
We use our monitoring to adapt those values in order to reduce the number of errors, while minimizing the impact on the BFF response time.
We are also constantly challenging our colleagues to improve the average response time of their services that we are calling.&lt;/p&gt;

&lt;p&gt;The choice of using retries is based on the information criticality.
For example, retrieving the user’s previous viewing sessions, is important for his/her experience, so we’re using a retry here.
On the opposite, analytics are less important, so we don’t use any retry there.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;na&quot;&gt;app.http_client_configs.best_effort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;retry&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.6&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app.http_client_configs.fast_fail&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;retry&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.6&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app.http_client_configs.long_fail&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;retry&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app.http_client_configs.reliant&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;retry&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;60&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Above, you can see the yaml configuration our Symfony application uses to build its Guzzle clients.&lt;/p&gt;

&lt;p&gt;Each configuration can cascade onto the clients, making variants available for our Symfony services.&lt;/p&gt;

&lt;p&gt;Below lies a Symfony configuration example:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We have an interface &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BFF\Domain\Content\Repository&lt;/code&gt; from the domain for a content repository.&lt;/li&gt;
  &lt;li&gt;The interface is linked to an implementation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BFF\Infra\HttpContentClient&lt;/code&gt; inside the infrastructure.&lt;/li&gt;
  &lt;li&gt;The implementation is built with variants (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;best_effort&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fast_fail&lt;/code&gt;) from a factory using the matching Guzzle configurations.&lt;/li&gt;
  &lt;li&gt;Other services use a chosen repository &lt;em&gt;according to their needs and criticality&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;c1&quot;&gt;# Service definition with its aliases.&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;BFF\Domain\Content\Repository&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@BFF\Domain\Content\Repository.fast_fail&apos;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;BFF\Domain\Content\Repository.best_effort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@BFF\Infra\HttpContentClient.best_effort&apos;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;BFF\Domain\Content\Repository.fast_fail&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@BFF\Infra\HttpContentClient.fast_fail&apos;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Concrete implementations&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;BFF\Infra\HttpContentClient.best_effort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;BFF\Infra\HttpContentClient&apos;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;factory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@BFF\Infra\ContentClientFactory&apos;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;create&apos;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;bind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;$clientConfig&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;%app.http_client_configs.best_effort%&apos;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;BFF\Infra\HttpContentClient.fast_fail&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;BFF\Infra\HttpContentClient&apos;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;factory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@BFF\Infra\ContentClientFactory&apos;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;create&apos;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;bind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;$clientConfig&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;%app.http_client_configs.fast_fail%&apos;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Other services using the Repository&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;BFF\Domain\Navigation\NavBarResolver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;$content&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@BFF\Domain\Content\Repository.best_effort&apos;&lt;/span&gt;

    &lt;span class=&quot;na&quot;&gt;BFF\Domain\Layout\BlockResolver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;$content&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@BFF\Domain\Content\Repository.fast_fail&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;This is an over simplified example as we have more layers and wrappers used for things like caching, monitoring, logging, etc.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article, we’ve clarified what an error is, and explained that we cannot generalize the configuration and usage of our APIs. Timeouts and retries, especially, must be tailored depending on the criticality of each call.&lt;/p&gt;

&lt;p&gt;This was a deviation on the road to our next article, where we will talk about monitoring the errors and stopping calls to failing APIs by implementing the circuit-breaker pattern.&lt;/p&gt;

&lt;h2 id=&quot;from-the-same-series&quot;&gt;From the same series&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/06/10/backend-bff-intro.html&quot;&gt;What’s a BFF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/08/12/backend-fallbacks.html&quot;&gt;Handling API failures in a gateway&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/08/25/backend-errors-connections.html&quot;&gt;What’s an error, and handling connection to multiple APIs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/09/02/backend-circuit-breaker.html&quot;&gt;Using a circuit breaker&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Valentin CLARAS</name></author><category term="backend" /><category term="php" /><category term="api" /><category term="api-gateway" /><category term="back-for-front" /><category term="error" /><category term="timout" /><category term="retry" /><category term="slo" /><category term="guzzle" /><summary type="html">A quick sidetrack in our series about Bedrock’s API gateway. This piece defines what are we talking about when we say “an error”, and explains how we handle the numerous connections to services we are calling.</summary></entry><entry><title type="html">Les spikes : quand, comment, pour quoi faire ?</title><link href="https://tech.bedrockstreaming.com/how-to-spike" rel="alternate" type="text/html" title="Les spikes : quand, comment, pour quoi faire ?" /><published>2022-08-23T00:00:00+00:00</published><updated>2022-08-23T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/how-to-spike</id><content type="html" xml:base="https://tech.bedrockstreaming.com/how-to-spike">&lt;p&gt;C’est une histoire bien connue, dans la vie de n’importe quel développeur : un ticket arrive dans le backlog, décrivant une problématique relativement complexe. C’est parfois une question de technologie inconnue, ou parfois simplement un chantier un peu trapu. Je pense que toutes les équipes ont, au moins une fois dans leur vie, fait face à ce genre de tâche impossible : c’est l’occasion des regards désespérés, alors qu’un junior se lamente en disant « Mais par où est-ce qu’il faut commencer ? ». Et c’est là qu’on répond : « Essaye de faire un spike ».&lt;/p&gt;

&lt;p&gt;Faire un spike ? Quelle excellente idée ! Encore faudrait-il savoir ce qu’est un spike, comment ça marche, et à quoi ça sert.&lt;/p&gt;

&lt;p&gt;Je vous propose donc ensemble de voir dans cet article : qu’est-ce qu’un spike, quand l’utiliser, et comment considérer qu’il est réussi ?&lt;/p&gt;

&lt;h1 id=&quot;spike-help-&quot;&gt;spike –help 📚&lt;/h1&gt;

&lt;p&gt;Si je devais citer &lt;a href=&quot;https://en.wikipedia.org/wiki/Spike_(software_development)&quot;&gt;Wikipedia&lt;/a&gt;, je dirais qu’un Spike, c’est “une méthode de développement de produit, dérivée de l’extrême programming, et qui cherche à créer le code le plus simple possible pour obtenir des solutions potentielles”.&lt;/p&gt;

&lt;p&gt;En gros, le but d’un spike, c’est de répondre à la question &lt;em&gt;“Comment on fait ?”&lt;/em&gt; avec un prototype de code réalisé grâce à une série de petites étapes simples. Un spike n’est pas une formule magique qui va vous permettre de réaliser la tâche impossible que votre client vous a donné. En revanche, le spike va vous permettre de savoir si la tâche impossible ou compliquée à première vue est en fait possible, et si oui, comment.
Il arrivera également que votre spike vous permette de constater qu’une tâche donnée peut être réalisée de plusieurs manières : que ce soit en passant par des librairies différentes, avec une implémentation changeante, ou autre chose encore. Dans ces cas, le spike va également vous servir à essayer ces différentes possibilités, et à choisir celle qui est la plus appropriée !&lt;/p&gt;

&lt;p&gt;Le moyen le plus simple est de procéder morceau par morceau. Alors je vous propose qu’on s’y mette maintenant, et qu’on regarde quoi faire !&lt;/p&gt;

&lt;h1 id=&quot;kowalski-analysis--&quot;&gt;“kowalski, analysis !” 📊&lt;/h1&gt;

&lt;p&gt;Avant toute chose, il faut savoir exactement ce que vous souhaitez faire. Rien ne sert de mettre la charrue avant les bœufs.&lt;/p&gt;

&lt;p&gt;Si ce n’est pas fait, écrivez noir sur blanc les lignes exactes qui vont définir votre tâche comme finie. Que ce soit connecter votre utilisateur de façon sécurisée, afficher une vidéo sans heurt, ou juste avoir une page qui clignote en blanc et bleu, il faut que vous ayez une liste de &lt;em&gt;bullet points&lt;/em&gt;, qui définit précisément ce que vous voulez faire.&lt;/p&gt;

&lt;p&gt;Votre objectif final est de réaliser tout ce que vous avez sur cette liste : strictement rien de moins, mais aussi strictement rien de plus ! Pas de demande implicite de type “Ah mais je voulais aussi que l’image soit visible en noir et blanc” : si ce n’est pas sur la liste, ce n’est pas à faire.&lt;/p&gt;

&lt;p&gt;Cette liste peut être écrite selon votre format favori : un cahier des charges, une série de directives &lt;em&gt;Gherkin&lt;/em&gt;, l’important c’est qu’elle soit écrite, claire et précise. En d’autres termes, vous définissez ici votre propre cahier des charges.&lt;/p&gt;

&lt;p&gt;Le résultat final doit donc être quelque chose dans ce style :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;As a client
I want to see my product in 3 dimensions
So that I can know what it looks like

As a client
I want to be able to rotate my product using the arrow keys
So that I can check it out entirely

As a client
I want to be able to zoom on my product
So that I can see even the smallest details
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Une fois que vous savez quoi faire, on peut vraiment commencer à mettre la main dans le code !&lt;/p&gt;

&lt;h1 id=&quot;-todo--make-the-code-below-work-&quot;&gt;// TODO : make the code below work 💻&lt;/h1&gt;

&lt;p&gt;Stop. Lâchez tout.&lt;/p&gt;

&lt;p&gt;Je vous vois déjà, votre liste de points en main, à tenter de la faire rentrer dans votre gros projet à grands coups de burin, de vous gratter la tête à comprendre pourquoi ça ne rentre pas, et qu’est-ce qui a bien pu casser, cette fois.&lt;/p&gt;

&lt;p&gt;Un peu de calme : le but d’un spike n’est pas de faire tout fonctionner, pas du tout. Prenez de la distance, et on va y aller en douceur.&lt;/p&gt;

&lt;p&gt;Pour commencer, isolez une partie de votre projet et de vos points objectifs. Il existe plusieurs moyens de s’y prendre : créer un nouveau projet, créer une nouvelle page avec seulement quelques composants, décharger votre backend… On veut un environnement le plus propre possible.
Beaucoup de projets sont vieux, et si mal conçus qu’il &lt;a href=&quot;/2021/09/01/bonnes-pratiques-web&quot;&gt;aurait fallu les jeter au bout de deux ans&lt;/a&gt;. On cherche ici à se détacher au maximum de cette dette technique.&lt;/p&gt;

&lt;p&gt;N’hésitez pas à utiliser des &lt;em&gt;mocks&lt;/em&gt;, des faux appels et résultats au reste de votre application :  en simulant comment se comporte le reste de votre projet sans véritablement y faire appel, vous diminuez au maximum votre marge d’erreur, et vous assurez que vous contrôlez la moindre information qui transite par votre code.&lt;/p&gt;

&lt;p&gt;Maintenant seulement, vous pouvez prendre votre clavier, et coder. Regardez comment implémenter chacun de ces points dans votre code propre de manière épurée.
Ça fonctionne du premier coup ? Génial, notez comment vous avez fait ! Ça ne marche pas ? Dommage, mais ce n’est pas une raison pour Ctrl+Z et recommencer. Notez bien ce qui n’a pas marché, avant de retenter ! Si ça ne marche toujours pas au bout de 2/3 essais, pas de soucis, n’hésitez pas à laisser ce point de côté et passer à un autre. Mais écrivez tout, car cela va vous servir très bientôt !&lt;/p&gt;

&lt;h1 id=&quot;ifbug--true--deletebug-consolelogit-works---&quot;&gt;if(bug == true) { delete(bug); console.log(“It works !”); } 🤖&lt;/h1&gt;

&lt;p&gt;Il peut cependant arriver que, parfois, tous vos efforts ne mènent à rien. Vous avez déjà passé plusieurs jours sur les différents sujets du spike, et vous n’avez pas encore identifié de solution pour faire fonctionner le tout.
Dans ce cas-là, pas de panique ! Il s’agit également d’un des objectifs du spike. Après tout, si vous n’avez pas pu réaliser votre objectif dans un cadre réduit, il est bien probable que vous n’auriez jamais pu le faire fonctionner dans votre projet lui-même.&lt;/p&gt;

&lt;p&gt;Les mêmes points qu’indiqués ci-dessus continuent de s’appliquer : notez ce que vous avez tenté et les soucis rencontrés avec chaque implémentation. Puis, continuez le processus détaillé ici : ce n’est pas parce que votre code n’as pas fonctionné qu’il ne doit surtout pas être présenté. Peut-être un de vos collègues trouvera-t-il la ligne qui vous manque, ou le point-virgule que vous avez oublié : mais peut-être aussi qu’il vous aidera à comprendre ensemble pourquoi la solution ne fonctionne pas dans votre cadre.
Et puis, vous pourrez alors vous poser la question : est-ce qu’il faut bien faire comprendre que la tâche demandée est irréalisable, ou est-ce qu’il faut prévoir un chantier pour réussir à trouver un moyen de remplir la requête ?&lt;/p&gt;

&lt;h1 id=&quot;linstant-doc-&quot;&gt;L’instant doc 📝&lt;/h1&gt;

&lt;p&gt;Une fois que vous avez terminé de coder, il est temps pour vous de poser votre IDE, et de sortir votre outil de documentation favori : Confluence, Jira, que sais-je. 
Puis, écrivez un compte-rendu de votre aventure. Présentez l’origine de votre spike (Le &lt;strong&gt;Pourquoi&lt;/strong&gt;), ce que vous avez tenté (Le &lt;strong&gt;Comment&lt;/strong&gt;). Expliquez ce qui a marché et ce qui n’a pas marché : cela vous servira lorsque vous implémenterez vraiment la feature !
Enfin, écrivez également les étapes qu’il faudrait suivre pour terminer la feature : ajoutez un maximum de détails techniques. Ce sera autant de problématiques en moins pour le pauvre dev qui va récupérer les US après vous.&lt;/p&gt;

&lt;p&gt;Je vous suggère donc de faire un plan de ce type :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Problème&lt;/strong&gt; - Expliquez ici l’état initial. Qu’est-ce qui était demandé ? Pourquoi avoir choisi de faire un spike ? Quel en est l’objectif ?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observations&lt;/strong&gt;  - Indiquez là vos réflexions et le code que vous avez produit. Expliquez ce que vous avez tenté, les problèmes rencontrés et les solutions établies, vos pistes de réflexion. N’hésitez surtout pas à détailler !&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Actions&lt;/strong&gt; - Enfin, détaillez dans cette dernière partie ce qu’il restera à faire afin de transformer ce spike en une feature fonctionnelle. Quels bugs corriger ? Quels points n’ont pas encore été réalisés, et comment faire pour les réaliser ?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Pour la dernière étape, je vous conseille de réaliser un tableau d’actions &lt;em&gt;SMART&lt;/em&gt; afin de définir au mieux les tâches à réaliser.
Le principe SMART suppose qu’une tâche doit être composées des cinq caractéristiques suivantes afin d’être pertinente :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Elle doit être &lt;strong&gt;Spécifique&lt;/strong&gt;, afin que l’objectif soit clair et concis (Qu’est-ce que je dois faire ? Exemple de réponse : « Il faut que l’image d’un objet soit en 3D »)&lt;/li&gt;
  &lt;li&gt;Elle doit être &lt;strong&gt;Mesurable&lt;/strong&gt;, pour définir un objectif quantifiable (Quant est-ce que ma tâche sera finie ? Exemple de réponse : « Il faut que je puisse faire tourner l’image avec les flèches gauches et droites du clavier  »))&lt;/li&gt;
  &lt;li&gt;Elle doit être &lt;strong&gt;Atteignable&lt;/strong&gt;, sans demander de décrocher les étoiles (Comment réaliser ma tâche ? Exemple de réponse : « Utiliser la méthode &lt;em&gt;Get3D&lt;/em&gt; de la librairie &lt;em&gt;Easy3D&lt;/em&gt; »))&lt;/li&gt;
  &lt;li&gt;Elle doit être &lt;strong&gt;Réaliste&lt;/strong&gt; au sujet en cours, donc nécessaire à l’accomplissement final (Est-ce qu’il est pertinent de prendre du temps pour faire ça ? Exemple de réponse : « Afin que notre client puisse voir l’avant et l’arrière de nos produits »)&lt;/li&gt;
  &lt;li&gt;Elle doit être définie de façon &lt;strong&gt;Temporelle&lt;/strong&gt;, afin de ne pas pouvoir s’éterniser (Pour quand ma tâche doit-elle être réalisée ? Exemple de réponse : « A réaliser avant que la feature soit considérée terminée »)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dans le cas où une des tâches que vous avez devisé ne peut pas répondre à un de ces cinq points, alors il est probable qu’elle ne soit pas suffisamment précise : peut-être la tâche manque-t-elle de cadre ou de contexte, ou le temps nécessaire pour la réaliser ne peut que difficilement être justifié. Je vous invite alors à la supprimer, ou à la fusionner avec une autre jusqu’à enfin pouvoir répondre à ces cinq questions !&lt;/p&gt;

&lt;p&gt;Bien entendu, n’hésitez pas à modifier le plan de cette documentation comme vous l’entendez : vous êtes celui qui allez l’utiliser, après tout !&lt;/p&gt;

&lt;p&gt;La doc est finie ? Il ne reste plus que deux étapes, puis on pourra enfin considérer ce spike comme fini !&lt;/p&gt;

&lt;h1 id=&quot;presentation_spikeppt-&quot;&gt;Presentation_Spike.ppt 🎬&lt;/h1&gt;

&lt;p&gt;Avant de pouvoir clôturer ce spike, il serait bien d’avoir des retours extérieurs. Pour ça, rien de mieux que de le présenter à votre équipe !
Organisez ensemble une réunion, pas très longue. Au sein de mon équipe, une demi-heure suffit. Il vous faudra peut-être un peu moins ou un peu plus de temps.&lt;/p&gt;

&lt;p&gt;Utilisez cette présentation afin de montrer, étape par étape, ce que vous avez réalisé. Rappelez tout d’abord les objectifs du spike, avant d’expliquer votre analyse du problème et les objectifs que vous avez identifiés. Puis, présentez les différentes implémentations que vous avez tentées, avant de conclure en montrant votre documentation et en expliquant les tâches qui restent à accomplir pour réaliser la feature objectif.&lt;/p&gt;

&lt;p&gt;Il est très important que vous ne présentiez pas uniquement le code que vous avez réussi à faire fonctionner, mais aussi vos tentatives échouées, et ce pour plusieurs raisons. Tout d’abord, il est tout à fait possible qu’un de vos collègues, en voyant votre présentation, réalise une de vos erreurs et vous l’indique. Mais surtout, si quelqu’un d’autre que vous récupère une des tâches restantes, il risque de tenter les mêmes pistes que vous, et rencontrer les mêmes problématiques que vous !&lt;/p&gt;

&lt;p&gt;Une fois votre présentation terminée, débattez avec le reste de votre équipe. S’ils sont d’accord avec vous sur le plan d’action que vous avez établi grâce à votre tableau SMART, il vous reste une toute dernière étape à accomplir !&lt;/p&gt;

&lt;h1 id=&quot;happily-ever-after-&quot;&gt;“Happily ever after…” 💭&lt;/h1&gt;

&lt;p&gt;Maintenant que tous vos coéquipiers ont pu constater et valider votre travail, il ne vous reste plus qu’à acter la mise en place : et pour ça, rien de mieux que, aux côtés de votre &lt;em&gt;Product Owner&lt;/em&gt; (Ou de l’équivalent dans votre équipe) de créer des tâches, &lt;em&gt;User Story&lt;/em&gt;, post-its, ou quoi que ce soit, pour que les étapes restantes soient visibles et accessibles par tous !&lt;/p&gt;

&lt;p&gt;N’hésitez pas à le guider pour ajouter encore une fois des détails techniques dans ces US ou tâches : vous avez réalisé l’analyse, il serait dommage de ne pas l’utiliser, et ce sera autant de temps gagné pour votre équipe. Tant que vous y êtes, pensez aussi à ajouter un lien vers votre documentation, ou vers une vidéo de votre présentation… Plus il y aura de détails, mieux ça sera !&lt;/p&gt;

&lt;p&gt;Il est également possible, comme indiqué plus haut, que la tâche qui a entraîné la réalisation de ce spike se découvre être impossible à implémenter. Il s’agit là également d’un point à faire avec votre &lt;em&gt;Product Owner&lt;/em&gt;, afin de décider ensemble de la procédure à suivre : peut-être faudra-t-il redéfinir les critères d’acceptation, ou bien laisser tomber complètement cette idée.&lt;/p&gt;

&lt;h1 id=&quot;return-0&quot;&gt;return 0;&lt;/h1&gt;

&lt;p&gt;Vous avez fini votre spike ! Ce qui était à l’origine une tâche complexe, confuse ou impossible à prévoir, est désormais divisée en une série d’étapes, qui sera désormais bien plus aisée à réaliser pour votre équipe. Alors, satisfait ?&lt;/p&gt;</content><author><name>Etienne Doyon</name></author><category term="spike" /><category term="methodologie" /><category term="cytron" /><category term="tech" /><summary type="html">C’est une histoire bien connue, dans la vie de n’importe quel développeur : un ticket arrive dans le backlog, décrivant une problématique relativement complexe. C’est parfois une question de technologie inconnue, ou parfois simplement un chantier un peu trapu. Je pense que toutes les équipes ont, au moins une fois dans leur vie, fait face à ce genre de tâche impossible : c’est l’occasion des regards désespérés, alors qu’un junior se lamente en disant « Mais par où est-ce qu’il faut commencer ? ». Et c’est là qu’on répond : « Essaye de faire un spike ».</summary></entry><entry><title type="html">Handling dependencies failures in an API gateway</title><link href="https://tech.bedrockstreaming.com/2022/08/12/backend-fallbacks.html" rel="alternate" type="text/html" title="Handling dependencies failures in an API gateway" /><published>2022-08-12T00:00:00+00:00</published><updated>2022-08-12T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/08/12/backend-fallbacks</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/08/12/backend-fallbacks.html">&lt;p&gt;Welcome to our second article about the backend architecture and its api gateway.
In &lt;a href=&quot;/2022/06/10/backend-bff-intro.html&quot;&gt;the first part&lt;/a&gt;, we talked about the BFF and all services it depends on.
Today we’re going to take a look at what to do when one of them (or many), fails to respond.&lt;/p&gt;

&lt;h2 id=&quot;service-dependencies&quot;&gt;Service dependencies&lt;/h2&gt;

&lt;p&gt;As seen previously, the BFF uses multiple data sources and services to create a full layout.&lt;/p&gt;

&lt;p&gt;Those services are used to gather the contents to be displayed in the application:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;getting user personalisation data;&lt;/li&gt;
  &lt;li&gt;advertising and analytics configuration;&lt;/li&gt;
  &lt;li&gt;asking if the user has some authorizations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we don’t want our BFF to become one giant SPOF &lt;a href=&quot;#notes&quot;&gt;(1)&lt;/a&gt;, we need to be resilient to the death &lt;a href=&quot;#notes&quot;&gt;(2)&lt;/a&gt; of those dependencies, any of them, at any time!
You must keep in mind that &lt;strong&gt;our top priority is to always be able to answer something readable&lt;/strong&gt; to the frontend applications.&lt;/p&gt;

&lt;h2 id=&quot;ddd&quot;&gt;DDD&lt;/h2&gt;

&lt;p&gt;First thing first, we are using a DDD &lt;a href=&quot;#notes&quot;&gt;(3)&lt;/a&gt; approach for our modeling.
This means that we focus on the business, as described by our Product Owner. We try not to worry about the various implementation of our backend’s friends and their different services.&lt;/p&gt;

&lt;p&gt;A picture is always easier to understand.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-08-12-backend-fallbacks/ddd-page-min.png&quot; alt=&quot;asking for a layout to the domain means asking a interface for&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above, we can see that when a user ask for a layout A, we are looking to resolve who is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt;.
From the domain point of view, the page collection is only an interface.&lt;/p&gt;

&lt;p&gt;In the picture below, we see the “Page collection implem (Infra)”.
It’s a layer implementing the interface defined in the domain. It uses multiple clients that call the services behind.
It’s its responsibility to chose which service to look on for the page.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-08-12-backend-fallbacks/ddd-page-full.png&quot; alt=&quot;page collection implementation chose the correct data source&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DDD is a too large subjects to be perfectly defined in this article. If you want to dig deeper into it, there are multiple great reads, feel free to check them out!
Now, how does this help us?&lt;/p&gt;

&lt;h2 id=&quot;handling-failures&quot;&gt;Handling failures&lt;/h2&gt;

&lt;p&gt;Failures handling is done by the middle layer seen in the previous example.
Its goal is to catch error &lt;a href=&quot;#notes&quot;&gt;(4)&lt;/a&gt;, and convert them to something expected and defined by the interface.&lt;/p&gt;

&lt;p&gt;That said, its responsibility is not to know what the expected answer is. To do that, we use the domain.&lt;/p&gt;

&lt;p&gt;Let’s see with a small code sample.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: The following example is not a real use-case, but it’s representative and simple enough to illustrate how it works.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the code below, we see a class that represents the subscribing status of a user, which has two properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hasAccess&lt;/code&gt; controls whether the user can read protected contents;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;isSubscribed&lt;/code&gt; is used in analytics, and to show subscription pages.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-php highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;?php&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SubscribeStatus&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__construct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readonly&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$hasAccess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readonly&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$isSubscribed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createAnonymous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;self&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createSubscribed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;self&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To create such an object, we use either one of the two static functions, depending on the status we get from the subscriptions API.
This is done in the middle layer, but the business is kept in the domain.&lt;/p&gt;

&lt;p&gt;To handle the failure, we add a new named constructor, dedicated to this specific case.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    public static function createUnknown(): self
    {
        return new self(true, false);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When an error happens and we can’t retrieve the user subscription status, we now have a fallback option.
With this fallback option, the user will:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;be able to access any content, it’s better to let an anonymous user access a content it should not, that blocking a paying customer;&lt;/li&gt;
  &lt;li&gt;still be reported as not subscribed and will see all available offers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of the time, the answer is even simpler than this one.&lt;/p&gt;

&lt;p&gt;Another example would be user’s viewing statuses. If we can’t retrieve them, we don’t display any progress bar.
Users won’t be able to tell if they have seen a content, but they will still be able to navigate the application.&lt;/p&gt;

&lt;h2 id=&quot;infrastructure-solution-the-stale-cache&quot;&gt;Infrastructure solution, the stale cache&lt;/h2&gt;

&lt;p&gt;In some cases, the above solution doesn’t work.
For example, contents information cannot be replaced by default values. If we don’t know about a video or a program, we cannot guess what it is.&lt;/p&gt;

&lt;p&gt;Luckily, we can rely on the stale cache.
Stale cache is an old cache entry which is expired. When the cache finds such entry, it usually ignores it and asks for a new version of the response.
In case of failure, we can use the available staled version.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-08-12-backend-fallbacks/stale-cache-usage.png&quot; alt=&quot;following first example, when the http fails to answer, we use the stale cached response&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The limitation is that a response must have been cached at least once, in order to have a staled version.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When there is no stale cache, we don’t display the content &lt;a href=&quot;#notes&quot;&gt;(5)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So far, we are only using it with http implementation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;called API must answers with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stale-if-error&lt;/code&gt; cache directive, it allows for the response to be used while stale when an error happens;&lt;/li&gt;
  &lt;li&gt;called API can answer with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stale-while-revalidate&lt;/code&gt; cache directive, for better performances;&lt;/li&gt;
  &lt;li&gt;calling API can query with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-stale&lt;/code&gt; cache directive, to use stale response see &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control#cache_directives&quot;&gt;the mdn for more on those headers&lt;/a&gt;;&lt;/li&gt;
  &lt;li&gt;on the client side, we are using the &lt;a href=&quot;https://github.com/Kevinrob/guzzle-cache-middleware&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Kevinrob/guzzle-cache-middleware&lt;/code&gt;&lt;/a&gt; to do the job.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For an entry cached for up to 10 minutes (answered with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-age&lt;/code&gt;), we allow up to 4 hours of stale cache (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stale-if-error&lt;/code&gt;).
Since we are using a shared cache, we are using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-stale&lt;/code&gt; when querying, with a random value up to 1 hour.
This makes most requests use the last stale response while one of them ask for a fresher response.
Those values are chosen according to our platform usages where peak visitor last for about 2 to 3 hours at night.&lt;/p&gt;

&lt;p&gt;We plan to expand its usage to other kinds of cached entries, such as manually saved data, and database queries.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In today’s post, we have seen how we handle the loss of our dependencies by anticipating their potential failures and preparing default acceptable behaviours.&lt;/p&gt;

&lt;p&gt;Next time, we will see how we can spare some traffic on those dependencies when they’re struggling with traffic.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;SPOF, as &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_point_of_failure&quot;&gt;single point of failure&lt;/a&gt; since all frontend applications have to rely on the BFF, I cannot resist linking this excellent &lt;a href=&quot;https://xkcd.com/2347/&quot;&gt;xkcd&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;By “death”, we mean anything unexpected. It can be a 500 error code, a timeout, a wrong content. We will talk a bit more about this in the next article.&lt;/li&gt;
  &lt;li&gt;DDD, as domain driven design, you can read more about it on &lt;a href=&quot;https://martinfowler.com/bliki/DomainDrivenDesign.html&quot;&gt;Martin FOWLER’s website&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Throwing errors is still allowed, but restricted to domain exceptions, and must be specified in the method’s declaration in the interface (i.e. via a comment).&lt;/li&gt;
  &lt;li&gt;There will be a dedicated article on partial rendering.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;from-the-same-series&quot;&gt;From the same series&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/06/10/backend-bff-intro.html&quot;&gt;What’s a BFF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/08/12/backend-fallbacks.html&quot;&gt;Handling API failures in a gateway&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/08/25/backend-errors-connections.html&quot;&gt;What’s an error, and handling connection to multiple APIs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2022/09/02/backend-circuit-breaker.html&quot;&gt;Using a circuit breaker&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;p&gt;In the meantime, feel free to have a look at other articles available on this blog:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;🇺🇸 &lt;a href=&quot;/2022/07/08/encrypt-aws-amis.html&quot;&gt;Encrypt AWS AMIs: one way to do it wrong&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;🇫🇷 &lt;a href=&quot;/2022/06/13/kubecon-2022-part-1.html&quot;&gt;Bedrock à la kubecon 2022 (4 articles)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Valentin CLARAS</name></author><category term="backend" /><category term="php" /><category term="api" /><category term="api-gateway" /><category term="back-for-front" /><category term="resiliency" /><summary type="html">Welcome to our second article about the backend architecture and its api gateway. In the first part, we talked about the BFF and all services it depends on. Today we’re going to take a look at what to do when one of them (or many), fails to respond.</summary></entry><entry><title type="html">How to ingest 400GB of logs per hour?</title><link href="https://tech.bedrockstreaming.com/2022/08/08/private-cdn-logs.html" rel="alternate" type="text/html" title="How to ingest 400GB of logs per hour?" /><published>2022-08-08T00:00:00+00:00</published><updated>2022-08-08T00:00:00+00:00</updated><id>https://tech.bedrockstreaming.com/2022/08/08/private-cdn-logs</id><content type="html" xml:base="https://tech.bedrockstreaming.com/2022/08/08/private-cdn-logs.html">&lt;p&gt;Bedrock Streaming is a company that sells a white labeled streaming and live platform. Our customers are media groups, TV channels, and streaming companies. Our goal is to deliver a state-of-the-art streaming platform to our customers.&lt;/p&gt;

&lt;p&gt;To achieve this goal, we have our own Content Delivery Network (CDN), made of several bare metal servers racked in our Data Centers. Those servers run Nginx and are designed to output hundreds of Gbps (several tens of Pb per month) to end-users. We use them to cache video content at our infrastructure’s edge.&lt;/p&gt;

&lt;p&gt;This increases efficiency of the platform 96 times out of 100, as video traffic doesn’t have to flow all the way through our infrastructure, and improves user experience as it serves video faster. Also, it diminishes the cost of our Video On Demand (VOD) infrastructure as we need less servers in VOD Stack.&lt;/p&gt;

&lt;p&gt;This in-turn increases end-users (clients of our customers) satisfaction with the service.&lt;/p&gt;

&lt;h2 id=&quot;who-needs-to-ingest-400gb-of-logs-per-hour-anyway&quot;&gt;Who needs to ingest 400GB of logs per hour anyway?&lt;/h2&gt;
&lt;p&gt;Every time someone watches a video, it generates traffic on our CDN, resulting in a lot of access logs. Without filtering, it averages to 400GB uncompressed logs per hour.&lt;/p&gt;

&lt;p&gt;This is why, at first, we chose to not log 2XX or 3XX HTTP codes. We had too many of them, and we considered them not as worth it as 4XX and 5XX. The 4XX and 5XX can be especially useful for debugging a particular situation or, from a broader perspective, improving the user experience.&lt;/p&gt;

&lt;p&gt;This was the kind of Nginx configuration we had deployed:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;map $status $loggable {
    ~^[23]  0;
    default 1;
}
access_log /path/to/access.log combined if=$loggable;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;giving-autonomy-for-all-teams-on-logs&quot;&gt;Giving autonomy for all teams on logs&lt;/h2&gt;

&lt;p&gt;At the end of 2021, the finance team approached us with a challenge: how to bill our customers based on their end-users CDN usage?
This was in fact a need we already anticipated, we tried the nginx module &lt;a href=&quot;https://www.nginx.com/resources/wiki/modules/traffic_accounting/&quot;&gt;Traffic_accounting&lt;/a&gt;, but it did not satisfy us fully. This module calculates and exposes metrics on-the-fly, which is CPU and memory intensive, especially above 50Gbps of traffic per server.&lt;/p&gt;

&lt;p&gt;We also had another objective that wasn’t addressed with the nginx module. We needed to give autonomy to QA, Video, Data, and Finance teams. We wanted to allow them to use CDN logs when they needed without having to ask for it, and ideally in a practical and unified way.&lt;/p&gt;

&lt;p&gt;The company philosophy states that we are user obsessed and that we do not finger point. We work as a team to offer the best user experience, this is why we make all our logs available to all teams. We didn’t come around to do it for the CDN as the volume of logs was too much of a constraint.&lt;/p&gt;

&lt;h2 id=&quot;technical-solution&quot;&gt;Technical Solution&lt;/h2&gt;

&lt;p&gt;At Bedrock, we like to keep things simple. We think our CDN main mission is to serve video as efficiently as possible. Our CDN’s servers can’t keep PetaBytes of logs on their disks. This is why we chose to output logs to Amazon S3.&lt;/p&gt;

&lt;p&gt;The real benefit to using S3 is that you can easily plug it into Glue and Athena which allows you to request TeraBytes of data easily.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2022-08-08-privateCdnLogs/image1.png&quot; alt=&quot;technical Solution&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sending-logs-to-s3-vector&quot;&gt;Sending logs to S3: Vector&lt;/h3&gt;

&lt;p&gt;To send logs from our CDN servers to Amazon S3 bucket, we had many options, but chose to test two approaches: &lt;a href=&quot;https://www.fluentd.org/&quot;&gt;Fluentd&lt;/a&gt; and &lt;a href=&quot;https://vector.dev/&quot;&gt;Vector&lt;/a&gt;. Fluentd is the legacy one, and Vector the new rusty one.&lt;/p&gt;

&lt;p&gt;After a quick evaluation, we decided to go with &lt;a href=&quot;https://medium.com/ibm-cloud/log-collectors-performance-benchmarking-8c5218a08fea&quot;&gt;Vector as it seemed more memory efficient&lt;/a&gt; and output more Logs Per Second under heavy load than Fluentd.&lt;/p&gt;

&lt;center&gt;&lt;img alt=&quot;Log per second&quot; src=&quot;/images/posts/2022-08-08-privateCdnLogs/image4.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;Source: &lt;a href=&quot;https://medium.com/ibm-cloud/log-collectors-performance-benchmarking-8c5218a08fea&quot; target=&quot;blank&quot;&gt;Who is the winner — Comparing Vector, Fluent Bit, Fluentd performance from Ajay Gupta&lt;/a&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We have Nginx and Vector installed on the CDN servers. Nginx now outputs all the access logs to a file. Vector reads the file, compresses logs to GZIP format and every 10Mb sends the logs to S3. Nginx may generate at peak 600GB of logs; we only send 10GB.&lt;/p&gt;

&lt;p&gt;Those logs are then locally cleaned by Logrotate.&lt;/p&gt;

&lt;h3 id=&quot;storing-logs-s3&quot;&gt;Storing logs: S3&lt;/h3&gt;
&lt;p&gt;We chose to store logs on an S3 bucket. We figured it was the most scalable and time efficient. S3 buckets can grow to PetaBytes easily. It is a few terraform lines away, this is convenient as we handle all our infrastructure with Terraform.&lt;/p&gt;

&lt;p&gt;We configured our bucket to use several lifecycle policies. One to automatically clean logs after 365 days, another to remove incomplete uploads, and another one to immediately remove files with a delete marker. Also, we configured the storage class in &lt;em&gt;intelligent tiering mode&lt;/em&gt; to store logs according to their access frequency.&lt;/p&gt;

&lt;p&gt;This will permit us to diminish the cost of our S3 bucket and not have an ever-increasing S3 bill.&lt;/p&gt;

&lt;h3 id=&quot;partitioning-logs-on-s3-lambda-stack&quot;&gt;Partitioning logs on S3: Lambda stack&lt;/h3&gt;

&lt;p&gt;Once logs are stored in S3 bucket, we need to classify and sort them in order to extract valuable intel. At Bedrock, we already use a modified version of a lambda stack, that does just that. Originally designed for Cloudfront, we have been using it also for Fastly and now for our Private CDN. You can find the original version at &lt;a href=&quot;https://github.com/aws-samples/amazon-cloudfront-access-logs-queries&quot;&gt;AWS Sample Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have 2 different parts in this lambda stack.&lt;/p&gt;

&lt;center&gt;&lt;img alt=&quot;Move Acess Logs&quot; src=&quot;/images/posts/2022-08-08-privateCdnLogs/image3.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;source: &lt;a href=&quot;https://github.com/aws-samples/amazon-cloudfront-access-logs-queries/blob/mainline/images/moveAccessLogs.png&quot; target=&quot;blanck&quot;&gt;moveAccessLogs&lt;/a&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
The first part is called by S3 Event when a new file is pushed to a specific path. This lambda moves the file to a path assigned per server and per hour. This way, logs are stored for each server, each month, each day and each hour in a separate prefix.&lt;/p&gt;

&lt;center&gt;&lt;img alt=&quot;Transform Partition&quot; src=&quot;/images/posts/2022-08-08-privateCdnLogs/image2.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;source: &lt;a href=&quot;https://github.com/aws-samples/amazon-cloudfront-access-logs-queries/blob/mainline/images/transformPartition.png&quot; target=&quot;blank&quot;&gt;transformPartition&lt;/a&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
Then, another lambda transforms logs into &lt;a href=&quot;https://parquet.apache.org/&quot;&gt;Parquet format&lt;/a&gt;. Parquet is an open source format from the Apache Foundation. It is commonly used in big data. It takes up little space and is very effective.&lt;/p&gt;

&lt;p&gt;We chose to use AWS glue in order to create a database of our logs. The columns of the table are based on our log format. We can then request everything we want in Athena.&lt;/p&gt;

&lt;center&gt;&lt;img alt=&quot;Athena Query&quot; src=&quot;/images/posts/2022-08-08-privateCdnLogs/image5.png&quot; /&gt;&lt;/center&gt;

&lt;p&gt;We are now capable of extracting the bytes sent from a particular virtual host and sum it over a month for all CDN servers to bill our customers.
Those logs are now available for all the teams who may need them to improve their application or to debug an issue they are facing.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We chose &lt;a href=&quot;https://vector.dev/&quot;&gt;Vector&lt;/a&gt; to transport our private CDN logs to an S3 Bucket. Then, we chose to reuse an AWS Stack using Lambda and Glue to extract information from these logs, asynchronously. This stack is used in production for several months on other projects.
All the teams that needed to extract value from our CDN logs are now autonomous to do so. We are now able to bill our customers based on their CDN usage.&lt;/p&gt;</content><author><name>Arthur Zinck</name></author><category term="onprem" /><category term="cdn" /><category term="logs" /><category term="aws" /><category term="cloud" /><category term="nginx" /><category term="vector" /><category term="lambda" /><category term="s3" /><category term="glue" /><category term="athena" /><summary type="html">At Bedrock, we have a CDN that outputs on average 400GB of uncompressed logs per hour. In this article, we present the architecture we have setup to collect these logs and extract value from them.</summary></entry></feed>